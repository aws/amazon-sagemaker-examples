{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting ONNX models\n",
    "\n",
    "The [Open Neural Network Exchange](https://onnx.ai/) (ONNX) is an open standard format for deep learning models that enables interoperability between deep learning frameworks such as Apache MXNet, Caffe2, Microsoft Cognitive Toolkit (CNTK), PyTorch and more. This means that we can use any of these frameworks to train the model, export these pretrained models in ONNX format and then import them in MXNet for inference.\n",
    "\n",
    "The ONNX community maintains a [ONNX Model Zoo](https://github.com/onnx/models), a collection of pre-trained state-of-the-art models in deep learning, available in the ONNX format. These models can be used with any framework supporting ONNX.\n",
    "\n",
    "In this example, we will use the ResNet101_DUC_HDC (uses ResNet101 as a backend network) from [Understanding Convolution for Semantic Segmentation](https://arxiv.org/abs/1702.08502). Dense Upsampling Convolution (DUC) is a semantic segmentation model, i.e., for an input image the model labels each pixel in the image into a set of pre-defined categories. Since the model is trained on the cityscapes dataset which contains images from urban street scenes, it can be used effectively in self driving vehicle systems.\n",
    "\n",
    "We will use the SageMaker Python SDK to host this ONNX model in SageMaker, and perform inference requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we'll get the IAM execution role from our notebook environment, so that SageMaker can access resources in your AWS account later in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hosting script\n",
    "\n",
    "We'll need to provide a hosting script that can run on the SageMaker platform. This script will be invoked by SageMaker when we perform inference.\n",
    "\n",
    "The script we're using here implements two functions:\n",
    "\n",
    "* `model_fn()` - the SageMaker model server uses this function to load the model\n",
    "* `transform_fn()` - this function is for using the model to take the input and produce the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize resnet-duc-hdc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the model\n",
    "\n",
    "To create a SageMaker Endpoint, we'll first need to prepare the model to be used in SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the model\n",
    "\n",
    "For this example, we will export symbol and parameters saved for a semantic segmentation model. Here, we will download the [ResNet_DUC_HDC model](https://s3.console.aws.amazon.com/s3/object/onnx-model-zoo/duc) trained on ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "mx.test_utils.download('https://s3.amazonaws.com/onnx-model-zoo/duc/ResNet_DUC_HDC_CityScapes-symbol.json')\n",
    "mx.test_utils.download('https://s3.amazonaws.com/onnx-model-zoo/duc/ResNet_DUC_HDC_CityScapes-0020.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import math\n",
    "from collections import namedtuple\n",
    "import cityscapes_labels\n",
    "from mxnet.contrib import onnx as onnx_mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_mxnet.export_model('ResNet_DUC_HDC_CityScapes-symbol.json','ResNet_DUC_HDC_CityScapes-0020.params', [(1,3,224,224)], np.float32, onnx_file_path='ResNet_DUC_HDC.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing the model data\n",
    "\n",
    "Now that we have the model data locally, we will need to compress it and upload the tarball to S3 for the SageMaker Python SDK to create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "from sagemaker.session import Session\n",
    "\n",
    "with tarfile.open('onnx_model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('ResNet_DUC_HDC.onnx')\n",
    "\n",
    "model_data = Session().upload_data(path='onnx_model.tar.gz', key_prefix='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SageMaker Python SDK Model instance\n",
    "\n",
    "With the model data uploaded to S3, we now have everything we need to instantiate a SageMaker Python SDK Model. We'll provide the constructor the following arguments:\n",
    "\n",
    "* `model_data`: the S3 location of the model data\n",
    "* `entry_point`: the script for model hosting that we looked at above\n",
    "* `role`: the IAM role used\n",
    "* `framework_version`: the MXNet version in use, in this case '1.3.0'\n",
    "\n",
    "You can read more about creating an `MXNetModel` object in the [SageMaker Python SDK API docs](https://sagemaker.readthedocs.io/en/stable/sagemaker.mxnet.html#mxnet-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNetModel\n",
    "\n",
    "mxnet_model = MXNetModel(model_data=model_data,\n",
    "                         entry_point='resnet-duc-hdc.py',\n",
    "                         role=role,\n",
    "                         framework_version='1.3.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an inference endpoint\n",
    "\n",
    "Now we can use our `MXNetModel` object to build and deploy an `MXNetPredictor`. This creates a SageMaker Model and Endpoint, the latter of which we can use for performing inference. \n",
    "\n",
    "We pass the following arguments to the deploy method:\n",
    "\n",
    "* `instance_count` - how many instances to back the endpoint.\n",
    "* `instance_type` - which EC2 instance type to use for the endpoint. For information on supported instance, please check here.\n",
    "\n",
    "### How our models are loaded\n",
    "By default, the predefined SageMaker MXNet containers have a default `model_fn`, which determines how your model is loaded. The default `model_fn` loads an MXNet Module object with a context based on the instance type of the endpoint.\n",
    "\n",
    "### Choosing instance types\n",
    "We will deploy this model with instance type `ml.c5.xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictor = mxnet_model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing inference\n",
    "\n",
    "With our Endpoint deployed, we can now send inference requests to it. We'll use one image as an example here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the image\n",
    "\n",
    "First, we'll download the image (and view it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "data = 'https://s3.amazonaws.com/onnx-model-zoo/duc/city1.png'\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test image\n",
    "print ('URL: ' + data)\n",
    "mx.test_utils.download(data)\n",
    "# read image as rgb\n",
    "im = cv.imread(os.path.basename(data))[:, :, ::-1]\n",
    "im = cv.resize(im, (224,224))\n",
    "# set output shape (same as input shape)\n",
    "result_shape = [im.shape[0],im.shape[1]]\n",
    "# set rgb mean of input image (used in mean subtraction)\n",
    "rgb_mean = cv.mean(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display input image\n",
    "Image.fromarray(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will preprocess inference image - subtract RGB mean and converts it to ndarray to input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(im):\n",
    "    # Convert to float32\n",
    "    test_img = im.astype(np.float32)\n",
    "    # Extrapolate image with a small border in order obtain an accurate reshaped image after DUC layer\n",
    "    test_shape = [im.shape[0],im.shape[1]]\n",
    "    cell_shapes = [math.ceil(l / 8)*8 for l in test_shape]\n",
    "    test_img = cv.copyMakeBorder(test_img, 0, max(0, int(cell_shapes[0]) - im.shape[0]), 0, max(0, int(cell_shapes[1]) - im.shape[1]), cv.BORDER_CONSTANT, value=rgb_mean)\n",
    "    test_img = np.transpose(test_img, (2, 0, 1))\n",
    "    # subtract rbg mean\n",
    "    for i in range(3):\n",
    "        test_img[i] -= rgb_mean[i]\n",
    "    test_img = np.expand_dims(test_img, axis=0)\n",
    "    # convert to ndarray\n",
    "    test_img = mx.ndarray.array(test_img)\n",
    "    return test_img\n",
    "\n",
    "pre = preprocess(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending the inference request\n",
    "\n",
    "Now we can use the predictor object to classify the input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "labels = predictor.predict(pre.asnumpy())\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocess\n",
    "\n",
    "Reshape the output to match input image dimensions, generate colorized segmentation map using `colorize()`\n",
    "\n",
    "`get_palette()` : Returns predefined color palette for generating output segmentation map\n",
    "\n",
    "`colorize()` : Generate the segmentation map using output labels generated by the model and color palette from get_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_palette():\n",
    "    # get train id to color mappings from file\n",
    "    trainId2colors = {label.trainId: label.color for label in cityscapes_labels.labels}\n",
    "    # prepare and return palette\n",
    "    palette = [0] * 256 * 3\n",
    "    for trainId in trainId2colors:\n",
    "        colors = trainId2colors[trainId]\n",
    "        if trainId == 255:\n",
    "            colors = (0, 0, 0)\n",
    "        for i in range(3):\n",
    "            palette[trainId * 3 + i] = colors[i]\n",
    "    return palette\n",
    "\n",
    "def colorize(labels):\n",
    "    # generate colorized image from output labels and color palette\n",
    "    result_img = Image.fromarray(labels).convert('P')\n",
    "    result_img.putpalette(get_palette())\n",
    "    return np.array(result_img.convert('RGB'))\n",
    "\n",
    "# get input and output dimensions\n",
    "result_height, result_width = result_shape\n",
    "img_height, img_width, _ = im.shape\n",
    "# set downsampling rate\n",
    "ds_rate = 8\n",
    "# set cell width\n",
    "cell_width = 2\n",
    "# number of output label classes\n",
    "label_num = 19\n",
    "\n",
    "# re-arrange output\n",
    "test_width = int((int(img_width) / ds_rate) * ds_rate)\n",
    "test_height = int((int(img_height) / ds_rate) * ds_rate)\n",
    "feat_width = int(test_width / ds_rate)\n",
    "feat_height = int(test_height / ds_rate)\n",
    "labels = labels.reshape((label_num, 4, 4, feat_height, feat_width))\n",
    "labels = np.transpose(labels, (0, 3, 1, 4, 2))\n",
    "labels = labels.reshape((label_num, int(test_height / cell_width), int(test_width / cell_width)))\n",
    "\n",
    "labels = labels[:, :int(img_height / cell_width),:int(img_width / cell_width)]\n",
    "labels = np.transpose(labels, [1, 2, 0])\n",
    "labels = cv.resize(labels, (result_width, result_height), interpolation=cv.INTER_LINEAR)\n",
    "labels = np.transpose(labels, [2, 0, 1])\n",
    "    \n",
    "# get softmax output\n",
    "softmax = labels\n",
    "    \n",
    "# get classification labels\n",
    "results = np.argmax(labels, axis=0).astype(np.uint8)\n",
    "raw_labels = results\n",
    "\n",
    "# comput confidence score\n",
    "confidence = float(np.max(softmax, axis=0).mean())\n",
    "\n",
    "# generate segmented image\n",
    "result_img = Image.fromarray(colorize(raw_labels)).resize(result_shape[::-1])\n",
    "    \n",
    "# generate blended image\n",
    "blended_img = Image.fromarray(cv.addWeighted(im[:, :, ::-1], 0.5, np.array(result_img), 0.5, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the Endpoint\n",
    "\n",
    "Since we've reached the end, we'll delete the SageMaker Endpoint to release the instance associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] https://github.com/onnx/models/tree/master/models/semantic_segmentation/DUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
  "notice": "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
