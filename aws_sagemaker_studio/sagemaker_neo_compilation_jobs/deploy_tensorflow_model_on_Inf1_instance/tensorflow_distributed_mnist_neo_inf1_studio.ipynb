{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protective-northwest",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [8]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dying-greek",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T00:08:10.968810Z",
     "iopub.status.busy": "2021-05-25T00:08:10.968370Z",
     "iopub.status.idle": "2021-05-25T00:08:10.970726Z",
     "shell.execute_reply": "2021-05-25T00:08:10.970237Z"
    },
    "papermill": {
     "duration": 0.016912,
     "end_time": "2021-05-25T00:08:10.970839",
     "exception": false,
     "start_time": "2021-05-25T00:08:10.953927",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "kms_key = \"arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-title",
   "metadata": {
    "papermill": {
     "duration": 0.010687,
     "end_time": "2021-05-25T00:08:10.992608",
     "exception": false,
     "start_time": "2021-05-25T00:08:10.981921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compile and Deploy a TensorFlow model on Inf1 instances\n",
    "\n",
    "Amazon SageMaker supports Inf1 instances for high performance and cost-effective inferences. Inf1 instances are ideal for large scale machine learning inference applications like image recognition, speech recognition, natural language processing, personalization, and fraud detection. In this example, train a classification model on the MNIST dataset using TensorFlow, compile it using Amazon SageMaker Neo, deploy the model on Inf1 instances on a SageMaker endpoint, and use the Neo Deep Learning Runtime to make inferences in real-time and with low latency. \n",
    "\n",
    "## Inf 1 instances \n",
    "Inf1 instances are built from the ground up to support machine learning inference applications and feature up to 16 AWS Inferentia chips, which are high-performance machine learning inference chips designed and built by AWS. The Inferentia chips are coupled with the latest custom 2nd generation Intel® Xeon® Scalable processors and up to 100 Gbps networking to enable high throughput inference. With 1 to 16 AWS Inferentia chips per instance, Inf1 instances can scale in performance to up to 2000 Tera Operations per Second (TOPS) and deliver extremely low latency for real-time inference applications. The large on-chip memory on AWS Inferentia chips used in Inf1 instances allows caching of machine learning models directly on the chip. This eliminates the need to access outside memory resources during inference, enabling low latency without impacting bandwidth. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* SageMaker Studio with Python 3 (Data Science) kernel\n",
    "* SageMaker SDK version 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-disney",
   "metadata": {
    "papermill": {
     "duration": 0.010598,
     "end_time": "2021-05-25T00:08:11.013952",
     "exception": false,
     "start_time": "2021-05-25T00:08:11.003354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install the required version of SageMaker and TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-rider",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T00:08:11.040440Z",
     "iopub.status.busy": "2021-05-25T00:08:11.039918Z",
     "iopub.status.idle": "2021-05-25T00:08:14.661482Z",
     "shell.execute_reply": "2021-05-25T00:08:14.660968Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 3.636892,
     "end_time": "2021-05-25T00:08:14.661594",
     "exception": false,
     "start_time": "2021-05-25T00:08:11.024702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import sagemaker\n",
    "if sagemaker.__version__ >= '2':\n",
    "    orig_sm_version = sagemaker.__version__\n",
    "    with open('orig_sm_version.txt', \"w\") as f:\n",
    "        f.write(orig_sm_version)\n",
    "    %pip install \"sagemaker>=1.14.2,<2\"\n",
    "\n",
    "if sagemaker.__version__ >= '2':\n",
    "    print(f\"WARNING: The current running version of the SageMaker SDK is {sagemaker.__version__}, which will cause this notebook to fail. \"\n",
    "          f\"Restart the kernel to run the required version of the SDK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acceptable-partnership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (2.42.0)\n",
      "Requirement already satisfied: pathos in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (0.2.7)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (1.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: attrs in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (19.3.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (1.18.1)\n",
      "Requirement already satisfied: google-pasta in /home/ubuntu/.local/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: boto3>=1.16.32 in /home/ubuntu/.local/lib/python3.6/site-packages (from sagemaker) (1.16.36)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ubuntu/.local/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathos->sagemaker) (1.6.6.3)\n",
      "Requirement already satisfied: pox>=0.2.9 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathos->sagemaker) (0.2.9)\n",
      "Requirement already satisfied: dill>=0.3.3 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.11 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathos->sagemaker) (0.70.11.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (2.2.0)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker) (45.2.0.post20200210)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.36 in /home/ubuntu/.local/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (1.19.36)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pandas->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.36->boto3>=1.16.32->sagemaker) (1.25.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relative-destruction",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T00:08:14.694021Z",
     "iopub.status.busy": "2021-05-25T00:08:14.693413Z",
     "iopub.status.idle": "2021-05-25T00:08:35.105352Z",
     "shell.execute_reply": "2021-05-25T00:08:35.105717Z"
    },
    "papermill": {
     "duration": 20.430206,
     "end_time": "2021-05-25T00:08:35.105859",
     "exception": false,
     "start_time": "2021-05-25T00:08:14.675653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install tensorflow==1.15.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "electoral-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-agriculture",
   "metadata": {
    "papermill": {
     "duration": 0.049633,
     "end_time": "2021-05-25T00:08:35.205386",
     "exception": false,
     "start_time": "2021-05-25T00:08:35.155753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Start a SageMaker session and get the excecution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "given-strain",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T00:08:35.315092Z",
     "iopub.status.busy": "2021-05-25T00:08:35.313762Z",
     "iopub.status.idle": "2021-05-25T00:08:35.841724Z",
     "shell.execute_reply": "2021-05-25T00:08:35.841205Z"
    },
    "papermill": {
     "duration": 0.58816,
     "end_time": "2021-05-25T00:08:35.841843",
     "exception": false,
     "start_time": "2021-05-25T00:08:35.253683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-prison",
   "metadata": {
    "papermill": {
     "duration": 0.048188,
     "end_time": "2021-05-25T00:08:35.938671",
     "exception": false,
     "start_time": "2021-05-25T00:08:35.890483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Download the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "retired-thumbnail",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T00:08:36.039901Z",
     "iopub.status.busy": "2021-05-25T00:08:36.039425Z",
     "iopub.status.idle": "2021-05-25T00:08:44.531619Z",
     "shell.execute_reply": "2021-05-25T00:08:44.531215Z"
    },
    "papermill": {
     "duration": 8.545157,
     "end_time": "2021-05-25T00:08:44.531733",
     "exception": false,
     "start_time": "2021-05-25T00:08:35.986576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-6-1b48c8015b33>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Writing data/train.tfrecords\n",
      "WARNING:tensorflow:From /home/ubuntu/SageMaker/batch_fix/import_error/aws_sagemaker_studio/sagemaker_neo_compilation_jobs/deploy_tensorflow_model_on_Inf1_instance/utils.py:31: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "Writing data/validation.tfrecords\n",
      "Writing data/test.tfrecords\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "data_sets = mnist.read_data_sets(\"data\", dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "\n",
    "utils.convert_to(data_sets.train, \"train\", \"data\")\n",
    "utils.convert_to(data_sets.validation, \"validation\", \"data\")\n",
    "utils.convert_to(data_sets.test, \"test\", \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-manner",
   "metadata": {
    "papermill": {
     "duration": 0.049715,
     "end_time": "2021-05-25T00:08:44.632227",
     "exception": false,
     "start_time": "2021-05-25T00:08:44.582512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Upload the data to Amazon Simple Storage Service (Amazon S3)\n",
    "Use the `sagemaker.Session.upload_data` function to upload datasets to an S3 location. The return value is the location, which is used when the training job is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recent-equivalent",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T00:08:44.736208Z",
     "iopub.status.busy": "2021-05-25T00:08:44.735501Z",
     "iopub.status.idle": "2021-05-25T00:08:49.034980Z",
     "shell.execute_reply": "2021-05-25T00:08:49.035345Z"
    },
    "papermill": {
     "duration": 4.352779,
     "end_time": "2021-05-25T00:08:49.035483",
     "exception": false,
     "start_time": "2021-05-25T00:08:44.682704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path=\"data\", key_prefix=\"data/DEMO-mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-sucking",
   "metadata": {
    "papermill": {
     "duration": 0.049165,
     "end_time": "2021-05-25T00:08:49.134555",
     "exception": false,
     "start_time": "2021-05-25T00:08:49.085390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Construct a script for distributed training \n",
    "\n",
    "To see the code for the network model, either browse to `mnist.py` in the File Browser or run the following command to show it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "completed-peninsula",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T00:08:49.237402Z",
     "iopub.status.busy": "2021-05-25T00:08:49.236769Z",
     "iopub.status.idle": "2021-05-25T00:08:49.396514Z",
     "shell.execute_reply": "2021-05-25T00:08:49.396882Z"
    },
    "papermill": {
     "duration": 0.212746,
     "end_time": "2021-05-25T00:08:49.397017",
     "exception": false,
     "start_time": "2021-05-25T00:08:49.184271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "\n",
      "import tensorflow as tf\n",
      "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
      "\n",
      "INPUT_TENSOR_NAME = \"inputs\"\n",
      "SIGNATURE_NAME = \"predictions\"\n",
      "\n",
      "LEARNING_RATE = 0.001\n",
      "\n",
      "\n",
      "def model_fn(features, labels, mode, params):\n",
      "    # Input Layer\n",
      "    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n",
      "\n",
      "    # Convolutional Layer #1\n",
      "    conv1 = tf.layers.conv2d(\n",
      "        inputs=input_layer, filters=32, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu\n",
      "    )\n",
      "\n",
      "    # Pooling Layer #1\n",
      "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
      "\n",
      "    # Convolutional Layer #2 and Pooling Layer #2\n",
      "    conv2 = tf.layers.conv2d(\n",
      "        inputs=pool1, filters=64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu\n",
      "    )\n",
      "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
      "\n",
      "    # Dense Layer\n",
      "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
      "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
      "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n",
      "\n",
      "    # Logits Layer\n",
      "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
      "\n",
      "    # Define operations\n",
      "    if mode in (Modes.PREDICT, Modes.EVAL):\n",
      "        predicted_indices = tf.argmax(input=logits, axis=1)\n",
      "        probabilities = tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
      "\n",
      "    if mode in (Modes.TRAIN, Modes.EVAL):\n",
      "        global_step = tf.train.get_or_create_global_step()\n",
      "        label_indices = tf.cast(labels, tf.int32)\n",
      "        loss = tf.losses.softmax_cross_entropy(\n",
      "            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits\n",
      "        )\n",
      "        tf.summary.scalar(\"OptimizeLoss\", loss)\n",
      "\n",
      "    if mode == Modes.PREDICT:\n",
      "        predictions = {\"classes\": predicted_indices, \"probabilities\": probabilities}\n",
      "        export_outputs = {SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)}\n",
      "        return tf.estimator.EstimatorSpec(\n",
      "            mode, predictions=predictions, export_outputs=export_outputs\n",
      "        )\n",
      "\n",
      "    if mode == Modes.TRAIN:\n",
      "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
      "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
      "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
      "\n",
      "    if mode == Modes.EVAL:\n",
      "        eval_metric_ops = {\"accuracy\": tf.metrics.accuracy(label_indices, predicted_indices)}\n",
      "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
      "\n",
      "\n",
      "def serving_input_fn(params):\n",
      "    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\n",
      "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
      "\n",
      "\n",
      "def read_and_decode(filename_queue):\n",
      "    reader = tf.TFRecordReader()\n",
      "    _, serialized_example = reader.read(filename_queue)\n",
      "\n",
      "    features = tf.parse_single_example(\n",
      "        serialized_example,\n",
      "        features={\n",
      "            \"image_raw\": tf.FixedLenFeature([], tf.string),\n",
      "            \"label\": tf.FixedLenFeature([], tf.int64),\n",
      "        },\n",
      "    )\n",
      "\n",
      "    image = tf.decode_raw(features[\"image_raw\"], tf.uint8)\n",
      "    image.set_shape([784])\n",
      "    image = tf.cast(image, tf.float32) * (1.0 / 255)\n",
      "    label = tf.cast(features[\"label\"], tf.int32)\n",
      "\n",
      "    return image, label\n",
      "\n",
      "\n",
      "def train_input_fn(training_dir, params):\n",
      "    return _input_fn(training_dir, \"train.tfrecords\", batch_size=100)\n",
      "\n",
      "\n",
      "def eval_input_fn(training_dir, params):\n",
      "    return _input_fn(training_dir, \"test.tfrecords\", batch_size=100)\n",
      "\n",
      "\n",
      "def _input_fn(training_dir, training_filename, batch_size=100):\n",
      "    test_file = os.path.join(training_dir, training_filename)\n",
      "    filename_queue = tf.train.string_input_producer([test_file])\n",
      "\n",
      "    image, label = read_and_decode(filename_queue)\n",
      "    images, labels = tf.train.batch(\n",
      "        [image, label], batch_size=batch_size, capacity=1000 + 3 * batch_size\n",
      "    )\n",
      "\n",
      "    return {INPUT_TENSOR_NAME: images}, labels\n",
      "\n",
      "\n",
      "def neo_preprocess(payload, content_type):\n",
      "    import io\n",
      "    import logging\n",
      "\n",
      "    import numpy as np\n",
      "\n",
      "    logging.info(\"Invoking user-defined pre-processing function\")\n",
      "\n",
      "    if (\n",
      "        content_type != \"application/x-image\"\n",
      "        and content_type != \"application/vnd+python.numpy+binary\"\n",
      "    ):\n",
      "        raise RuntimeError(\n",
      "            \"Content type must be application/x-image or application/vnd+python.numpy+binary\"\n",
      "        )\n",
      "\n",
      "    f = io.BytesIO(payload)\n",
      "    image = np.load(f) * 255\n",
      "\n",
      "    return np.reshape(image, (1, 784))\n",
      "\n",
      "\n",
      "### NOTE: this function cannot use MXNet\n",
      "def neo_postprocess(result):\n",
      "    import json\n",
      "    import logging\n",
      "\n",
      "    import numpy as np\n",
      "\n",
      "    logging.info(\"Invoking user-defined post-processing function\")\n",
      "\n",
      "    # Softmax (assumes batch size 1)\n",
      "    for res in result:\n",
      "        if res.size > 1:\n",
      "            result = res\n",
      "            break\n",
      "    result = np.squeeze(result)\n",
      "    result_exp = np.exp(result - np.max(result))\n",
      "    result = result_exp / np.sum(result_exp)\n",
      "\n",
      "    response_body = json.dumps(result.tolist())\n",
      "    content_type = \"application/json\"\n",
      "\n",
      "    return response_body, content_type\n"
     ]
    }
   ],
   "source": [
    "!cat 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-cooperative",
   "metadata": {
    "papermill": {
     "duration": 0.048243,
     "end_time": "2021-05-25T00:08:49.494860",
     "exception": false,
     "start_time": "2021-05-25T00:08:49.446617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This script is an adaptation of the [TensorFlow MNIST example](https://github.com/tensorflow/models/tree/master/official/vision/image_classification). It provides a `model_fn(features, labels, mode)` function that is used for training, evaluation and inference. For more details, see [TensorFlow MNIST distributed training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_training_and_serving/tensorflow_script_mode_training_and_serving.ipynb).\n",
    "\n",
    "At the end of the training script, there are two additional functions that are used with Neo Deep Learning Runtime:\n",
    "\n",
    "* `neo_preprocess(payload, content_type)`: takes the payload and Content-Type of each incoming request and returns a NumPy array.\n",
    "* `neo_postprocess(result)`: takes the prediction results produced by Deep Learning Runtime and returns the response body.\n",
    "\n",
    "LeCun, Y., Cortes, C., & Burges, C. (2010). MNIST handwritten digit databaseATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-favor",
   "metadata": {
    "papermill": {
     "duration": 0.048657,
     "end_time": "2021-05-25T00:08:49.592278",
     "exception": false,
     "start_time": "2021-05-25T00:08:49.543621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create a training job\n",
    "\n",
    "Use the `sagemaker.TensorFlow` estimator to create a training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-criticism",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "authentic-inside",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T00:08:49.697274Z",
     "iopub.status.busy": "2021-05-25T00:08:49.696490Z",
     "iopub.status.idle": "2021-05-25T00:08:49.983552Z",
     "shell.execute_reply": "2021-05-25T00:08:49.982783Z"
    },
    "papermill": {
     "duration": 0.341227,
     "end_time": "2021-05-25T00:08:49.983815",
     "exception": true,
     "start_time": "2021-05-25T00:08:49.642588",
     "status": "failed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "framework_version or py_version was None, yet image_uri was also None. Either specify both framework_version and py_version, or specify image_uri.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c8e71f068413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_instance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ml.c5.xlarge\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msagemaker_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/tensorflow/estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, py_version, framework_version, model_dir, image_uri, distribution, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;34m\"train_instance_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"instance_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"instance_type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         )\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_version_or_image_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframework_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpy_version\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"py2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             logger.warning(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/fw_utils.py\u001b[0m in \u001b[0;36mvalidate_version_or_image_args\u001b[0;34m(framework_version, py_version, image_uri)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframework_version\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpy_version\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimage_uri\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         raise ValueError(\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0;34m\"framework_version or py_version was None, yet image_uri was also None. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m             \u001b[0;34m\"Either specify both framework_version and py_version, or specify image_uri.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: framework_version or py_version was None, yet image_uri was also None. Either specify both framework_version and py_version, or specify image_uri."
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(\n",
    "    entry_point=\"mnist.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.11.0\",\n",
    "    training_steps=10,\n",
    "    evaluation_steps=10,\n",
    "    train_instance_count=2,\n",
    "    train_instance_type=\"ml.c5.xlarge\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cathedral-kelly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/SageMaker/batch_fix/import_error/aws_sagemaker_studio/sagemaker_neo_compilation_jobs/deploy_tensorflow_model_on_Inf1_instance\n"
     ]
    }
   ],
   "source": [
    "!pwd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-incident",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The `fit` method creates a training job in two **ml.c5.xlarge** instances. The logs from `fit` show the instances training, evaluating, and incrementing the number of **training steps**. \n",
    "\n",
    "At the end of the training, the training job generates a saved model for compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-sharing",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Deploy the trained model\n",
    "\n",
    "Deploy the model to an Inf1 instance for real-time inferences. Once the training is complete, compile the model using SageMaker Neo to optimize performance for the desired deployment target. SageMaker Neo enables you to train machine learning models once and run them anywhere in the cloud and at the edge. To compile the trained model for deployment to Inf1 instances, use the  `TensorFlowEstimator.compile_model` method and select `ml_inf1` as the deployment target. The compiled model is deployed on an endpoint that uses Inf1 instances in SageMaker.\n",
    "\n",
    "### Compile the model \n",
    "\n",
    "The `input_shape` is the definition for the model's input tensor and `output_path` is where the compiled model is stored in S3.\n",
    "\n",
    "> Note: If `compile_model` results in a permission error, verify that the execution role returned previously by `get_execution_role()` has access to the Amazon S3 bucket specified in `output_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-money",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = \"/\".join(mnist_estimator.output_path.split(\"/\")[:-1])\n",
    "mnist_estimator.framework_version = \"1.15.0\"\n",
    "\n",
    "optimized_estimator = mnist_estimator.compile_model(\n",
    "    target_instance_family=\"ml_inf1\",\n",
    "    input_shape={\"data\": [1, 784]},  # Batch size 1, 3 channels, 224x224 Images.\n",
    "    output_path=output_path,\n",
    "    framework=\"tensorflow\",\n",
    "    framework_version=\"1.15.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-orange",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Deploy to a SageMaker endpoint\n",
    "\n",
    "Deploy the compiled model to an Amazon SageMaker endpoint. This example uses the Inf1 `ml.inf1.xlarge` instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-graphics",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_predictor = optimized_estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.inf1.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-surprise",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Configure a serializer for `application/vnd+python.numpy+binary` Content-Type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-behalf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def numpy_bytes_serializer(data):\n",
    "    f = io.BytesIO()\n",
    "    np.save(f, data)\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "\n",
    "optimized_predictor.content_type = \"application/vnd+python.numpy+binary\"\n",
    "optimized_predictor.serializer = numpy_bytes_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-personal",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Invoking the endpoint\n",
    "\n",
    "When the endpoint is ready, send requests to it and receive inference results in real time with low latency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-northern",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from IPython import display\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(10):\n",
    "    data = mnist.test.images[i]\n",
    "    # Display image\n",
    "    im = PIL.Image.fromarray(data.reshape((28, 28)) * 255).convert(\"L\")\n",
    "    display.display(im)\n",
    "    # Invoke endpoint with image\n",
    "    predict_response = optimized_predictor.predict(data)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response\n",
    "    print(\"prediction is {}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-special",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-emphasis",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(optimized_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-selling",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Rollback the SageMaker Python SDK version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-certificate",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rollback the SageMaker Python SDK to the kernel's original version\n",
    "if os.path.exists('orig_sm_version.txt'):\n",
    "    with open('orig_sm_version.txt', 'r') as f:\n",
    "        orig_sm_version = f.read()\n",
    "    print(f\"Original version: {orig_sm_version}\")\n",
    "    print(f\"Current version: {sagemaker.__version__}\")\n",
    "    %pip install sagemaker=={orig_sm_version}\n",
    "    os.remove('orig_sm_version.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-exchange",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Restart the kernel to run the updated version of the SDK."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "papermill": {
   "default_parameters": {},
   "duration": 40.654711,
   "end_time": "2021-05-25T00:08:50.790054",
   "environment_variables": {},
   "exception": true,
   "input_path": "tensorflow_distributed_mnist_neo_inf1_studio.ipynb",
   "output_path": "/opt/ml/processing/output/tensorflow_distributed_mnist_neo_inf1_studio-2021-05-25-00-04-04.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-05-25T00:08:10.135343",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
