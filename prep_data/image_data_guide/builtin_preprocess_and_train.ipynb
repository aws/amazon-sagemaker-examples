{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, Structure, and Preprocess Image Data for SageMaker Built-In Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: \n",
    "* This notebook should be used with the conda_amazonei_mxnet_p36 kernel\n",
    "* You can also explore image preprocessing with TensorFlow and PyTorch by running [Download, Structure, and Preprocess Image Data for TensorFlow Models](tensorflow_preprocess_and_train.ipynb) and [Download, Structure, and Preprocess Image Data for PyTorch Models](pytorch_preprocess_and_train.ipynb), respectively.\n",
    "\n",
    "The main purpose of this notebook is to demonstrate how you can preprocess image data to train SageMaker Built-In Algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Part 1: Download the Dataset](#Part-1:-Download-the-Dataset)\n",
    "1. [Part 2: Structure the Dataset](#Part-2:-Structure-the-Dataset)\n",
    "1. [Part 3: Preprocess Images for Built-in Algorithms](#Part-3:-Preprocess-Images-for-Built-in-Algorithms)\n",
    "1. [Part 4: Train the Built-in Image Classification Algorithm](#Part-4:-Train-the-Built-in-Image-Classification-Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Download the Dataset\n",
    "----\n",
    "----\n",
    "In this section, you will use a dataset manifest to download animal images from the COCO dataset for all ten animal classes. You will then download frog images from the CIFAR dataset and add them to your COCO animal images. In order to simulate coming to SageMaker with your own dataset, we will keep the data in an unstructured form until the next notebook where you will learn the best practices for structuring an image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install imageio joblib opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import shutil\n",
    "import urllib\n",
    "import pathlib\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread, imwrite\n",
    "from joblib import Parallel, delayed, parallel_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The COCO and CIFAR Datasets\n",
    "___\n",
    "For this series of notebooks we will be sampling images from the [COCO dataset](https://cocodataset.org) and [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) (before beginning the notebooks in this series, it's a good idea to browse each dataset website to familiaraize youreself with the data). Both are datasets of images, but come formatted very differently. The COCO dataset contains images from Flickr that represent a real-world dataset which isn't formatted or resized specifically for deep learning. This makes it a good dataset for this guide because we want it to be as comprehensive as possible. The CIFAR-10 images, on the other hand, are preprocessed specifically for deep learning as they come cropped, resized and vectorized (i.e. not in a readable image format). This notebooks will show you how to work with both types of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the annotations\n",
    "____\n",
    "The dataset annotation file contains info on each image in the dataset such as the class, superclass, file name and url to download the file. Just the annotations for the COCO dataset are about 242MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "urllib.request.urlretrieve(anno_url, \"coco-annotations.zip\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.unpack_archive(\"coco-annotations.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the annotations into Python\n",
    "The training and validation annotations come in separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"annotations/instances_train2017.json\", \"r\") as f:\n",
    "    train_metadata = json.load(f)\n",
    "\n",
    "with open(\"annotations/instances_val2017.json\", \"r\") as f:\n",
    "    val_metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Extract only the animal annotations\n",
    "___\n",
    "To limit the scope of the dataset for this guide we're only using the images of animals in the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = {\n",
    "    c[\"id\"]: c[\"name\"] for c in train_metadata[\"categories\"] if c[\"supercategory\"] == \"animal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract metadata and image filepaths\n",
    "For the train and validation sets, the data we need for the image labels and the filepaths are under different headings in the annotations. We have to extract each out and combine them into a single annotation in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annos = {}\n",
    "for a in train_metadata[\"annotations\"]:\n",
    "    if a[\"category_id\"] in category_labels:\n",
    "        train_annos[a[\"image_id\"]] = {\"category_id\": a[\"category_id\"]}\n",
    "\n",
    "train_images = {}\n",
    "for i in train_metadata[\"images\"]:\n",
    "    train_images[i[\"id\"]] = {\"coco_url\": i[\"coco_url\"], \"file_name\": i[\"file_name\"]}\n",
    "\n",
    "val_annos = {}\n",
    "for a in val_metadata[\"annotations\"]:\n",
    "    if a[\"category_id\"] in category_labels:\n",
    "        val_annos[a[\"image_id\"]] = {\"category_id\": a[\"category_id\"]}\n",
    "\n",
    "val_images = {}\n",
    "for i in val_metadata[\"images\"]:\n",
    "    val_images[i[\"id\"]] = {\"coco_url\": i[\"coco_url\"], \"file_name\": i[\"file_name\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine label and filepath info\n",
    "Later in this series of guides we'll make our own train, validation and test splits. For this reason we'll combine the training and validation datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, anno in train_annos.items():\n",
    "    anno.update(train_images[id])\n",
    "\n",
    "for id, anno in val_annos.items():\n",
    "    anno.update(val_images[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_annos = {}\n",
    "for k, v in train_annos.items():\n",
    "    all_annos.update({k: v})\n",
    "for k, v in val_annos.items():\n",
    "    all_annos.update({k: v})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sample the dataset\n",
    "___\n",
    "In order to make working with the data easier, we'll select 250 images from each class at random. To make sure you get the same set of cell images for each run of this we'll also set Numpy's random seed to 0. This is a small fraction of the dataset, but it demonstrates how using transfer learning can give you good results without needing very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_annos = {}\n",
    "\n",
    "for category_id in category_labels:\n",
    "    subset = [k for k, v in all_annos.items() if v[\"category_id\"] == category_id]\n",
    "    sample = np.random.choice(subset, size=250, replace=False)\n",
    "    for k in sample:\n",
    "        sample_annos[k] = all_annos[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a download function\n",
    "In order to parallelize downloading the images we must wrap the download and save process with a function for multi-threading with joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, path):\n",
    "    data = imread(url)\n",
    "    imwrite(path / url.split(\"/\")[-1], data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the sample of the dataset (2,500 images, ~5min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = pathlib.Path(\"data_sample_2500\")\n",
    "sample_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_backend(\"threading\", n_jobs=5):\n",
    "    Parallel(verbose=3)(\n",
    "        delayed(download_image)(a[\"coco_url\"], sample_dir) for a in sample_annos.values()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with CIFAR-10 frog data\n",
    "___\n",
    "The COCO dataset doesn't include any images of frogs, but let's say our model must also be able to label images of frogs. To fix this we can download another dataset of images which includes frogs, sample 250 frog images and add them to our existing image data. These images are much smaller (32x32) so they will appear pixelated and blurry when we increase the size of them to (244x244). We'll use the CIFAR-10 dataset to achieve this. As you'll see the CIFAR-10 dataset comes formatted in a very different manner from COCO dataset. We must process the CIFAR-10 data into individual image files so that it's congruent to our COCO images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and extract the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tarfile.open(\"cifar-10-python.tar.gz\")\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open first batch of CIFAR-10 dataset\n",
    "The CIFAR-10 dataset comes in five training batches and one test batch. Each training batch has 10,000 randomly ordered images. Since we only need 250 frog images for our dataset, just pulling from the first batch will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./cifar-10-batches-py/data_batch_1\", \"rb\") as f:\n",
    "    batch_1 = pickle.load(f, encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = batch_1[b\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull 250 sample frog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frog_indices = np.array(batch_1[b\"labels\"]) == 6\n",
    "sample_frog_indices = np.random.choice(frog_indices.nonzero()[0], size=250, replace=False)\n",
    "sample_data = image_data[sample_frog_indices, :]\n",
    "frog_images = sample_data.reshape(len(sample_data), 3, 32, 32).transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View frog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4, figsize=(10, 7))\n",
    "indices = np.random.randint(low=0, high=249, size=12)\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(frog_images[indices[i]])\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write sample frog images to `data_sample_2500` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frog_filenames = np.array(batch_1[b\"filenames\"])[sample_frog_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, filename in enumerate(frog_filenames):\n",
    "    filename = filename.decode()\n",
    "    data = frog_images[idx]\n",
    "    if filename.endswith(\".png\"):\n",
    "        filename = filename.replace(\".png\", \".jpg\")\n",
    "    imwrite(sample_dir / filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir.rename(\"data_sample_2750\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add frog annotations to `sample_annos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels[26] = \"frog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_anno_idx = np.array(list(sample_annos.keys())).max() + 1\n",
    "\n",
    "frog_anno_ids = range(next_anno_idx, next_anno_idx + len(frog_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, frog_id in enumerate(frog_anno_ids):\n",
    "    sample_annos[frog_id] = {\n",
    "        \"category_id\": 26,\n",
    "        \"file_name\": frog_filenames[idx].decode().replace(\".png\", \".jpg\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Structure the Dataset\n",
    "----\n",
    "----\n",
    "\n",
    "In this section, you will properly structure your image files for ingestion by the model. Then, we will use Python to create the new folder structure and copy the files into the correct set and label folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper folder structure\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although most tools can accommodate data in any file structure with enough tinkering, it makes most sense to use the sensible defaults that frameworks like MXNet, TensorFlow and PyTorch all share to make data ingestion as smooth as possible. By default, most tools will look for image data in the file structure depicted below:\n",
    "```\n",
    "+-- train\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|\n",
    "+-- val\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|\n",
    "+-- test\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "```\n",
    "You will notice that the COCO dataset does not come structured like above so we must use the annotation data to help restructure the folders of the COCO dataset so they match the pattern above. Once the new directory structures are created you can use your desired framework's data loading tool to gracefully load and define transformation for your image data. Many datasets may already be in this structure in which case you can skip this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg2.3'></a>\n",
    "### Make train, validation and test splits\n",
    "___\n",
    "We should divide our data into train, validation and test splits. A typical split ratio is 80/10/10. Our image classification algorithm will train on the first 80% (training) and evaluate its performance at each epoch with the next 10% (validation) and we'll give our model's final accuracy results using the last 10% (test). It's important that before we split the data we make sure to shuffle it randomly so that class distribution among splits is roughly proportional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "image_ids = sorted(list(sample_annos.keys()))\n",
    "np.random.shuffle(image_ids)\n",
    "first_80 = int(len(image_ids) * 0.8)\n",
    "next_10 = int(len(image_ids) * 0.9)\n",
    "train_ids, val_ids, test_ids = np.split(image_ids, [first_80, next_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg2.4'></a>\n",
    "### Make new folder structure and copy image files\n",
    "___\n",
    "This new folder structure can then be read by data loaders for SageMaker's built-in algorithms, TensorFlow or PyTorch for easy loading of the image data into your framework of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstruct_dir = Path(\"data_sample_2750\")\n",
    "struct_dir = Path(\"data_structured\")\n",
    "struct_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for name, split in zip([\"train\", \"val\", \"test\"], [train_ids, val_ids, test_ids]):\n",
    "    split_dir = struct_dir / name\n",
    "    split_dir.mkdir(exist_ok=True)\n",
    "    for image_id in tqdm(split):\n",
    "        category_dir = split_dir / f'{category_labels[sample_annos[image_id][\"category_id\"]]}'\n",
    "        category_dir.mkdir(exist_ok=True)\n",
    "        source_path = (unstruct_dir / sample_annos[image_id][\"file_name\"]).as_posix()\n",
    "        target_path = (category_dir / sample_annos[image_id][\"file_name\"]).as_posix()\n",
    "        shutil.copy(source_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Preprocess Images for Built-in Algorithms\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explore the different ways to format your image dataset for SageMaker's built-in algorithms. The first involves creating a manifest file for the train and validations sets and the other has you creating .REC files (RecordIO format) which are single binary files made up of all the images for the train and validation sets. Since the RecordIO format is preferred, we will upload the .REC files to S3 for training in the nedxt notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import boto3\n",
    "import shutil\n",
    "import urllib\n",
    "import pickle\n",
    "import pathlib\n",
    "import sagemaker\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application/x-image format\n",
    "___\n",
    "\n",
    "This format is also referred to as \"Image Format\" or \"LST\" format. The benefit of using this format is that it doesn't require any modification or restructuring of your dataset. Instead, you create a manifest of the images for your training set and validation set. These two manifests are separate `.lst` files which list all the images giving each of them a unique index, the class they belong to and the relative path to the image file from the main training folder. The data in the `.lst` file is in tab separated values.\n",
    "\n",
    "While its the easiest format to use, it requires SageMaker to do more work behind the scenes. For datasets with many images, this will cause training to take longer. For datasets with fewer images, the performance difference isn't as pronounced.\n",
    "\n",
    "Below are two examples of how to create your .LST manifest files. One uses your own code and the other uses a script from MXNet. If you want to create .REC files of your images, you should skip to Option 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Manually generate the .LST files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_ids = {name: idx for idx, name in enumerate(sorted(category_labels.values()))}\n",
    "print(category_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = pathlib.Path(\"./data_structured\").rglob(\"*.jpg\")\n",
    "\n",
    "for idx, p in enumerate(image_paths):\n",
    "    image_id = f\"{idx:010}\"\n",
    "    category = category_ids[p.parts[-2]]\n",
    "    path = p.as_posix()\n",
    "    split = p.parts[-3]\n",
    "    with open(f\"{split}.lst\", \"a\") as f:\n",
    "        line = f\"{image_id}\\t{category}\\t{path}\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the contents of the `train.lst` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head train.lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Use im2rec.py script to generate the .LST files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_url = \"https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py\"\n",
    "urllib.request.urlretrieve(script_url, \"im2rec.py\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python im2rec.py --list --recursive LST_FILE_PREFIX DATA_DIR`\n",
    "* --list - generate an LST file\n",
    "* --recursive - looks inside subfolders for image data\n",
    "* LST_FILE_PREFIX - choose the name you want for the `.lst` file\n",
    "* DATA_DIR - relative path to directory with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python im2rec.py --list --recursive train data_structured/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python im2rec.py --list --recursive val data_structured/val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the contents of the `train.lst` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head train.lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application/x-recordio (preferred format)\n",
    "___\n",
    "This format is commonly referred to as RecordIO. It creates a new file for your each of your training and validation datasets with the `.rec` suffix. The `.rec` file is a single file that contains all of the images in the dataset so it can be streamed directly to the SageMaker training algorithm without the overhead involved with transfering thousands of individual files. For datasets with many images this provides a huge reduction in training time because SageMaker doesn't need to download all the image files before it can run the training algorithm. If you use the `im2rec.py` script, it will also resize the images for you as well. The benefits of resizing the files before saving them in the RecordIO format is that it'll reduce the amount of data you need to transfer to s3 and will also speed up trainging by doing the resizing ahead of time instead of at training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Run Option 2 from application/x-image above and copy LST files\n",
    "Once you've run Option 2 from above then proceed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordio_dir = pathlib.Path(\"./data_recordio\")\n",
    "recordio_dir.mkdir(exist_ok=True)\n",
    "shutil.copy(\"train.lst\", \"data_recordio/\")\n",
    "shutil.copy(\"val.lst\", \"data_recordio/\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate .rec files in the RecordIO Format\n",
    "Once the `.lst` file is generated, the same `im2rec.py` script will also generate the `.rec` file.\n",
    "\n",
    "`python im2rec.py --resize 224 --quality 90 --num-thread 16 LST_FILE_PREFIX DATA_DIR/`\n",
    "* **--resize**: Have the script resize the files before saving them all to a `.rec` file. For the image classification algorithm the default dimensions are 224x224. Resizing now will also reduce the size of your `.rec` file.\n",
    "* **--quality**: Default settings will save the image data uncompressed. Adding some compression will keep the filesize of your `.rec` down especially if you're not resizing them.\n",
    "* **--num_thread**: Set how many threads to parallelize the work\n",
    "* **--LST_FILE_PREFIX**: Name of the `.lst` you're referencing for creating the `.rec` file\n",
    "* **--DATA_DIR**: Relative path directory which holds the data listed in the `.lst` file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python im2rec.py --resize 224 --quality 90 --num-thread 16 data_recordio/train data_structured/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python im2rec.py --resize 224 --quality 90 --num-thread 16 data_recordio/val data_structured/val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data to S3\n",
    "___\n",
    "In order for SageMaker's built-in algrorithms to train on the data, it must be stored in an S3 bucket. Here, we will create a bucket, but you can use an existing bucket if you like by replacing the `bucket_name` variable in the first line of the `else` statement below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "prefix = \"DEMO-sm-preprocess-train-image-data-builtin-algo\"\n",
    "s3 = boto3.resource(\"s3\")\n",
    "region = sagemaker.Session().boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload .rec files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uploader = sagemaker.s3.S3Uploader()\n",
    "\n",
    "data_path = recordio_dir / \"train.rec\"\n",
    "\n",
    "data_s3_uri = s3_uploader.upload(\n",
    "    local_path=data_path.as_posix(), desired_s3_uri=f\"s3://{bucket_name}/{prefix}/data/train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = recordio_dir / \"val.rec\"\n",
    "\n",
    "data_s3_uri = s3_uploader.upload(\n",
    "    local_path=data_path.as_posix(), desired_s3_uri=f\"s3://{bucket_name}/{prefix}/data/val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the Built-in Image Classification Algorithm\n",
    "----\n",
    "----\n",
    "In this section, you will use the SageMaker SDK to create an Estimator for SageMaker's Built-in Image Classification algorithm and train it on a remote EC2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Image Classification algorithm\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create SageMaker training and validation channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f\"s3://{bucket_name}/{prefix}/data/train\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    input_mode=\"Pipe\",\n",
    ")\n",
    "\n",
    "val_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f\"s3://{bucket_name}/{prefix}/data/val\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    input_mode=\"Pipe\",\n",
    ")\n",
    "\n",
    "data_channels = {\"train\": train_data, \"validation\": val_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the algorithm's hyperparameters\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html\n",
    "* **num_layers** - The built-in image classification algrorithm is based off the ResNet architecture. There are many different versions of this architecture differing by how many layers they use. We'll use the smallest one for this guide to speed up training. If the algorithm's accuracy is hitting a plateau and you need better accuracy, increasing the number of layers may help.\n",
    "* **use_pretrained_model** - This will initialize the weights from a pre-trained model for transfer learning. Otherwise weights are initialized randomly.\n",
    "* **augmentation_type** - Allows you to add augmentations to your trainingset to help your model generalize better. For small datasets, augmentation can greatly imporve training.\n",
    "* **image_shape** -  The channel, height, width of all the images\n",
    "* **num_classes** - Number of classes in your dataset\n",
    "* **num_training_samples** - Total number of images in your training set (used to help calculate progres)\n",
    "* **mini_batch_size** - The batch size you would like to use during training. \n",
    "* **epochs** - An epoch refers to one cycle through the training set and having more epochs to train means having more oppotunities to improve accracy. Suitable values range from 5 to 25 epochs depending on your time and budget constraints. Ideally, the right number of epochs is right before your validation accuracy plateaus.\n",
    "* **learning_rate**: After each batch of training we update the model's weights to give us the best possible results for that batch. The learning rate controls by how much we should update the weights. Best practices dictate a value between 0.2 and .001, typically never going higher than 1. The higher the learning rate, the faster your training will converge to the optimal weights, but going too fast can lead you to overshoot the target. In this example, we're using the weights from a pre-trained model so we'd want to start with a lower learning rate because the weights have already been optimized and we don't want move too far away from them.\n",
    "* **precision_dtype** -  Whether you want to use a 32-bit float data type for the model's weights or 16-bit. 16-bit can be used if you're running into memory management issues. However, weights can grow or shrink rapidly so having 32-bit weights make your training more robust to these issues and is typically the default in most frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(category_labels)\n",
    "num_training_samples = len(set(pathlib.Path(\"data_structured/train\").rglob(\"*.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"num_layers\": 18,\n",
    "    \"use_pretrained_model\": 1,\n",
    "    \"augmentation_type\": \"crop_color_transform\",\n",
    "    \"image_shape\": \"3,224,224\",\n",
    "    \"num_classes\": num_classes,\n",
    "    \"num_training_samples\": num_training_samples,\n",
    "    \"mini_batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"precision_dtype\": \"float32\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the type of algorithm and resources to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve(\n",
    "    \"image-classification\", sagemaker.Session().boto_region_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_config = {\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"image_uri\": training_image,\n",
    "    \"role\": sagemaker.get_execution_role(),\n",
    "    \"instance_count\": 1,\n",
    "    \"instance_type\": \"ml.p3.2xlarge\",\n",
    "    \"volume_size\": 100,\n",
    "    \"max_run\": 360000,\n",
    "    \"output_path\": f\"s3://{bucket_name}/data/output\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = sagemaker.estimator.Estimator(**algo_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg4a.3'></a>\n",
    "## Understanding the training output\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[09/14/2020 05:37:38 INFO 139869866030912] Epoch[0] Batch [20]#011Speed: 111.811 samples/sec#011accuracy=0.452381\n",
    "[09/14/2020 05:37:54 INFO 139869866030912] Epoch[0] Batch [40]#011Speed: 131.393 samples/sec#011accuracy=0.570503\n",
    "[09/14/2020 05:38:10 INFO 139869866030912] Epoch[0] Batch [60]#011Speed: 139.540 samples/sec#011accuracy=0.617700\n",
    "[09/14/2020 05:38:27 INFO 139869866030912] Epoch[0] Batch [80]#011Speed: 144.003 samples/sec#011accuracy=0.644483\n",
    "[09/14/2020 05:38:43 INFO 139869866030912] Epoch[0] Batch [100]#011Speed: 146.600 samples/sec#011accuracy=0.664991\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training has begun:\n",
    "* Epoch[0]: One epoch corresponds to one training cycle through all the data. Stochastic optimizers like SGD and Adam improve accuracy by running multiple epochs. Random data augmentations is also applied with each new epoch allowing the training algorithm to learn on modified data.\n",
    "* Batch: The number of batches processed by the training algorithm. We specified one batch to be 64 images in the `mini_batch_size` hyperparameter. For algorithms like SGD, the model get a chance to update itself every batch.  \n",
    "* Speed: the number of images sent to the training algorithm per second. This information is important in determining how changes in your dataset affect the speed of training.\n",
    "* Accuracy: the training accuracy achieved at each interval (in this case, 20 batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "[09/14/2020 05:38:58 INFO 139869866030912] Epoch[0] Train-accuracy=0.677083\n",
    "[09/14/2020 05:38:58 INFO 139869866030912] Epoch[0] Time cost=102.745\n",
    "[09/14/2020 05:39:02 INFO 139869866030912] Epoch[0] Validation-accuracy=0.729492\n",
    "[09/14/2020 05:39:02 INFO 139869866030912] Storing the best model with validation accuracy: 0.729492\n",
    "[09/14/2020 05:39:02 INFO 139869866030912] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first epoch of training has ended (for this example we only train for one epoch). The final training accuracy is reported as well as the accuracy on the validation set. Comparing these two number is important in determining if your model is overfit or underfit as well as the bais/variance trade-off. The saved model uses the learned weights from the epoch with the best validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "2020-09-14 05:39:03 Uploading - Uploading generated training model\n",
    "2020-09-14 05:39:15 Completed - Training job completed\n",
    "Training seconds: 235\n",
    "Billable seconds: 235\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model parameters are saved as a `.tar.gz` in S3 to the directory specified in the `output_path` of `algo_config`. Total billable seconds is also reported to help compute the cost of training since you are only charged for the time the EC2 instance is training on the data. Other costs such as S3 storage also apply, but are not included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
