{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Mode Blog Post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.serializers import NumpySerializer, JSONSerializer, CSVSerializer\n",
    "from sagemaker.deserializers import NumpyDeserializer, JSONDeserializer\n",
    "from sagemaker.predictor import Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your SageMaker version is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Python SDK version 2.x is required\n",
    "original_version = sagemaker.__version__\n",
    "if sagemaker.__version__ != '2.24.1':\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, '-m', 'pip', 'install', 'sagemaker==2.24.1']\n",
    "    )\n",
    "    import importlib\n",
    "    importlib.reload(sagemaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# Useful SageMaker variables\n",
    "try:\n",
    "    # You're using a SageMaker notebook\n",
    "    sess = sagemaker.Session()\n",
    "    bucket = sess.default_bucket()\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # You're using a notebook somewhere else\n",
    "    print('Setting role and SageMaker session manually...')\n",
    "    bucket = 'bobby-demo'\n",
    "    region = 'us-west-2'\n",
    "    \n",
    "    iam = boto3.client('iam')\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20200630T141851')['Role']['Arn']\n",
    "    boto3.setup_default_session(region_name=region, profile_name='default')\n",
    "    sess = sagemaker.Session(sagemaker_client=sagemaker_client, default_bucket=bucket)\n",
    "\n",
    "# Local data paths\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Data paths in S3\n",
    "s3_prefix = 'script-mode-workflow'\n",
    "csv_s3_prefix = f'{s3_prefix}/csv'\n",
    "csv_s3_uri = f's3://{bucket}/{s3_prefix}/csv'\n",
    "numpy_train_s3_prefix = f'{s3_prefix}/numpy/train'\n",
    "numpy_train_s3_uri = f's3://{bucket}/{numpy_train_s3_prefix}'\n",
    "numpy_test_s3_prefix = f'{s3_prefix}/numpy/test'\n",
    "numpy_test_s3_uri = f's3://{bucket}/{numpy_test_s3_prefix}'\n",
    "csv_train_s3_uri = f'{csv_s3_uri}/train'\n",
    "csv_test_s3_uri = f'{csv_s3_uri}/test'\n",
    "\n",
    "# Enable Local Mode training\n",
    "enable_local_mode_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/local_mode_setup.sh\n",
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/daemon.json    \n",
    "!/bin/bash ./local_mode_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Boston Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Boston Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "x, y = boston['data'], boston['target']\n",
    "training_index = math.floor(.8 * boston['data'].shape[0])\n",
    "x_train, y_train = x[:training_index], y[:training_index]\n",
    "x_test, y_test = x[training_index:], y[training_index:]\n",
    "x_train_np = StandardScaler().fit_transform(x_train)\n",
    "x_test_np = StandardScaler().fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas and standardize\n",
    "train_df = pd.DataFrame(data=x_train)\n",
    "train_df['target'] = y_train\n",
    "first_col = train_df.pop('target')\n",
    "train_df.insert(0, 'target', first_col)\n",
    "\n",
    "test_df = pd.DataFrame(data=x_test)\n",
    "test_df = pd.DataFrame(StandardScaler().fit_transform(test_df))\n",
    "test_df['target'] = y_test\n",
    "first_col = test_df.pop('target')\n",
    "test_df.insert(0, 'target', first_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as both CSV and Numpy data types to demonstrate data type flexibility in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "train_df.to_csv(f'{train_dir}/train.csv', header=False, index=False)\n",
    "test_df.to_csv(f'{test_dir}/test.csv', header=False, index=False)\n",
    "\n",
    "# Save as Numpy\n",
    "np.save(os.path.join(train_dir, 'x_train.npy'), x_train_np)\n",
    "np.save(os.path.join(test_dir, 'x_test.npy'), x_test_np)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_resource_bucket = boto3.Session().resource('s3').Bucket(bucket)\n",
    "s3_resource_bucket.Object(os.path.join(csv_s3_prefix, 'train.csv')).upload_file('data/train/train.csv')\n",
    "s3_resource_bucket.Object(os.path.join(csv_s3_prefix, 'test.csv')).upload_file('data/test/test.csv')\n",
    "s3_resource_bucket.Object(os.path.join(numpy_train_s3_prefix, 'x_train.npy')).upload_file('data/train/x_train.npy')\n",
    "s3_resource_bucket.Object(os.path.join(numpy_train_s3_prefix, 'y_train.npy')).upload_file('data/train/y_train.npy')\n",
    "s3_resource_bucket.Object(os.path.join(numpy_test_s3_prefix, 'x_test.npy')).upload_file('data/test/x_test.npy')\n",
    "s3_resource_bucket.Object(os.path.join(numpy_test_s3_prefix, 'y_test.npy')).upload_file('data/test/y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script Mode in SageMaker allows you to take control of the training and inference process without having to go through the trouble of creating and maintaining your own docker containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {'max_depth': 20, 'n_jobs': 4, 'n_estimators': 120}\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = 'local'\n",
    "    inputs = {'train': f'file://{train_dir}',\n",
    "              'test': f'file://{test_dir}'}\n",
    "else:\n",
    "    train_instance_type = 'ml.c5.xlarge'\n",
    "    inputs = {'train':csv_train_s3_uri,\n",
    "              'test': csv_test_s3_uri}\n",
    "\n",
    "estimator_parameters = {'entry_point': 'train_deploy_scikitlearn_without_dependencies.py',\n",
    "                        'source_dir': 'scikitlearn_script',\n",
    "                        'framework_version': '0.23-1',\n",
    "                        'py_version':'py3',\n",
    "                        'instance_type': train_instance_type,\n",
    "                        'instance_count': 1,\n",
    "                        'hyperparameters': hyperparameters,\n",
    "                        'role': role,\n",
    "                        'base_job_name': 'randomforestregressor-model'}\n",
    "\n",
    "estimator = SKLearn(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the estimator finishes training, we can deploy it to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_predictor = estimator.deploy(initial_instance_count=1,\n",
    "                             instance_type='ml.m5.xlarge',\n",
    "                             endpoint_name='randomforestregressor-endpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the SageMaker endpoint to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_predictor.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to come back to this notebook after having already deployed the SageMaker endpoint, you can use the following snippet of code to invoke it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_predictor = Predictor(endpoint_name='randomforestregressor-endpoint',\n",
    "                              sagemaker_session=sess,\n",
    "                              serializer=NumpySerializer(),\n",
    "                              deserializer=NumpyDeserializer())\n",
    "\n",
    "sklearn_predictor.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes keeping your code all in one Python file can be unweidly. Script Mode gives you the flexibility to parse out your code into multiple Python files.\n",
    "\n",
    "In this PyTorch example, we want to separate the actual neural network definition from the rest of the code by putting it into its own file as demonstrated in the `pytorch_script/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': 5, 'batch_size': 128, 'learning_rate': 0.01}\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = 'local'\n",
    "    inputs = {'train': f'file://{train_dir}',\n",
    "              'test': f'file://{test_dir}'}\n",
    "else:\n",
    "    train_instance_type = 'ml.c5.xlarge'\n",
    "    inputs = {'train':numpy_train_s3_uri,\n",
    "              'test': numpy_test_s3_uri}\n",
    "\n",
    "estimator_parameters = {'entry_point':'train_deploy_pytorch_without_dependencies.py',\n",
    "                        'source_dir': 'pytorch_script',\n",
    "                        'instance_type' : train_instance_type,\n",
    "                        'instance_count': 1,\n",
    "                        'hyperparameters': hyperparameters,\n",
    "                        'role' : role,\n",
    "                        'base_job_name':'pytorch-model',\n",
    "                        'framework_version':'1.5',\n",
    "                        'py_version':'py3'}\n",
    "\n",
    "estimator = PyTorch(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, after the estimator finishes training, we can deploy it to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_predictor = estimator.deploy(initial_instance_count=1,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     endpoint_name='pytorch-endpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the endpoint to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_predictor.serializer = JSONSerializer()\n",
    "pytorch_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "pytorch_predictor.predict(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to come back to this notebook after having already deployed the SageMaker endpoint, you can use the following snippet of code to invoke it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_predictor = Predictor(endpoint_name='pytorch-endpoint',\n",
    "                              sagemaker_session=sess,\n",
    "                              serializer=JSONSerializer(),\n",
    "                              deserializer=JSONDeserializer())\n",
    "\n",
    "pytorch_predictor.predict(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the number of Python files you have is becoming unweildy now or you want more organization. In this scenario, you might be tempted to create your own Python library. The good news is Script Mode can support adding custom libraries and those libraries don't have to be in the same directory as your entry point Python script (SageMaker will copy the library folder to the same folder where the entrypoint is located).\n",
    "\n",
    "In this example, we have a custom library to implement k-fold cross validation for an XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'num_round': 6, 'K': 5}\n",
    "\n",
    "if enable_local_mode_training:\n",
    "    train_instance_type = 'local'\n",
    "    inputs = {'train': f'file://{train_dir}'}\n",
    "else:\n",
    "    train_instance_type = 'ml.c5.xlarge'\n",
    "    inputs = {'train': csv_s3_uri}\n",
    "\n",
    "estimator_parameters = {'entry_point':'train_deploy_xgboost_with_dependencies.py',\n",
    "                        'source_dir': 'xgboost_script',\n",
    "                        'dependencies': ['my_custom_library'],\n",
    "                        'instance_type' : train_instance_type,\n",
    "                        'instance_count': 1,\n",
    "                        'hyperparameters': hyperparameters,\n",
    "                        'role' : role,\n",
    "                        'base_job_name':'xgboost-model',\n",
    "                        'framework_version':'1.0-1',\n",
    "                        'py_version':'py3'}\n",
    "\n",
    "estimator = XGBoost(**estimator_parameters)\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train the model with k-fold cross validation, we can deploy it to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_predictor = estimator.deploy(initial_instance_count=1,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     endpoint_name='xgboost-endpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use the endpoint to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_predictor.serializer = CSVSerializer()\n",
    "xgboost_predictor.deserializer = JSONDeserializer()\n",
    "xgboost_predictor.predict(x_test[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to come back to this notebook after having already deployed the SageMaker endpoint, you can use the following snippet of code to invoke it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_predictor = Predictor(endpoint_name='xgboost-endpoint',\n",
    "                              sagemaker_session=sess,\n",
    "                              serializer=CSVSerializer(),\n",
    "                              deserializer=JSONDeserializer())\n",
    "\n",
    "xgboost_predictor.predict(x_test[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "pytorch_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "xgboost_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
