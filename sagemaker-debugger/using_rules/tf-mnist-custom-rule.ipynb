{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker - Debugging With Custom Rules\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed platform to build, train, and deploy machine learning models quickly. Amazon SageMaker Debugger offers the capability to debug machine learning models during training by identifying and detecting problems with the models in near real time. \n",
    "\n",
    "## How does Amazon SageMaker Debugger work?\n",
    "\n",
    "Amazon SageMaker Debugger lets you go beyond just looking at scalars like losses and accuracies during training. It gives you full visibility into all tensors flowing through the graph during training. Furthermore, it helps you monitor your training in near real time using rules. It also provides alerts once it has detected an inconsistency in training flow.\n",
    "\n",
    "### Concepts\n",
    "* **Tensors**: These are the artifacts that define the state of the training job at any particular instant in its lifecycle.\n",
    "* **Debug Hook**: Hook is the construct with which Amazon SageMaker Debugger looks into the training process and captures the tensors requested at the desired step intervals\n",
    "* **Debugging Rule**: A logical construct, implemented as python code, which helps analyze the tensors captured by the hook and report anamolies, if at all\n",
    "\n",
    "With these concepts in mind, let's understand the overall flow of things which the Debugger uses to orchestrate debugging\n",
    "\n",
    "\n",
    "### Saving tensors during training\n",
    "\n",
    "The tensors captured by the debug hook are stored in an S3 location specified by you. There are two ways you can configure the Debugger to save tensors:\n",
    "\n",
    "#### With no changes to your training script\n",
    "If you use one of the SageMaker provided [Deep Learning Containers](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) for 1.15, then you don't need to make any changes to your training script for the tensors to be stored. SageMaker Debugger will use the configuration you provide through the SageMaker SDK's Tensorflow `Estimator` when creating your job to save the tensors in the fashion you specify. You can review the script we are going to use at [src/mnist_zerocodechange.py](src/mnist_zerocodechange.py). You will note that this is an untouched TensorFlow script which uses the Estimator interface. Please note that SageMaker Debugger only supports TF.Keras, Estimator and MonitoredSession interfaces. Full description of support is available at [SMDebug with TensorFlow ](https://github.com/awslabs/sagemaker-debugger/tree/master/docs/tensorflow.md)\n",
    "\n",
    "#### Orchestrating your script to store tensors\n",
    " For other containers, you need to make couple of lines of changes to your training script. The Debugger exposes a library `smdebug` which allows you to capture these tensors and save them for analysis. It's highly customizable and allows to save the specific tensors you want at different frequencies and possibly with other configurations. Refer [DeveloperGuide](https://github.com/awslabs/sagemaker-debugger/tree/master/docs) for details on how to use the Debugger library with your choice of framework in your training script. Here we have an example script orchestrated at [src/mnist_byoc](src/mnist_byoc.py). You also need to ensure that your container has the `smdebug` library installed.\n",
    "\n",
    "### Analysis of tensors\n",
    "\n",
    "Once the tensors are saved, the Debugger can be configured to run debugging ***Rules*** on them. On a very broad level, a rule is a python code used to detect certain conditions during training. Some of the conditions that a data scientist training an algorithm may care about are monitoring for gradients getting too large or too small, detecting overfitting, and so on. Sagemaker-Debugger will come pre-packaged with certain first-party (1P) rules. Users can write their own rules using the Sagemaker-Debugger APIs. You can also analyze raw tensor data outside of the Rules construct in say, a Sagemaker notebook, using Sagemaker-Debugger's full set of APIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training TensorFlow models with Amazon SageMaker Debugger\n",
    "\n",
    "### Amazon SageMaker TensorFlow as a framework\n",
    "\n",
    "Train a TensorFlow model in this notebook with Amazon Sagemaker Debugger enabled and monitor the training jobs with rules. This is done using Amazon SageMaker [TensorFlow 1.15.0](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) Container as a framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries needed for the demo of Amazon SageMaker Debugger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, DebuggerHookConfig, TensorBoardOutputConfig, CollectionConfig, rule_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the entry point for the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the entrypoint script\n",
    "entrypoint_script='src/mnist_zerocodechange.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Estimator\n",
    "\n",
    "Now it's time to setup our TensorFlow estimator. There are new parameters with the estimator to enable your training job for debugging through Amazon SageMaker Debugger. These new parameters are explained below\n",
    "\n",
    "* **debugger_hook_config**: This new parameter accepts a local path where you wish your tensors to be written to and also accepts the S3 Uri where you wish your tensors to be uploaded to. It also accepts CollectionConfigurations which specify which tensors will be saved from the training job.\n",
    "* **rules**: This new parameter will accept a list of rules you wish to evaluate against the tensors output by this training job. For rules, \n",
    "\n",
    "SageMaker Debugger supports two types of rules\n",
    "* **Amazon SageMaker Rules**: These are rules curated by the Amazon SageMaker team and you can choose to evaluate them against your training job.\n",
    "* **Custom Rules**: You can optionally choose to write your own rule as a Python source file and have it evaluated against your training job. To provide SageMaker Debugger to evaluate this rule, you would have to provide the S3 location of the rule source and the evaluator image.\n",
    " \n",
    "#### Using your own custom rule\n",
    " \n",
    "In this example you see how to use your own custom rule logic to be evaluated against your training.\n",
    "\n",
    "##### **Summary of what the custom rule evaluates**\n",
    "For demonstration purposes, we provide here a rule that tries to track whether gradients are too large. The custom rule looks at the tensors in the collection \"gradients\" saved during training and attempt to get the absolute value of the gradients in each step of the training. If the mean of the absolute values of gradients in any step is greater than a specified threshold, mark the rule as 'triggering'. Let us look at how to structure the rule source.\n",
    "\n",
    "Any custom rule logic you want to be evaluated should extend the `Rule` interface provided by Amazon SageMaker Debugger\n",
    "\n",
    "```python\n",
    "from smdebug.rules.rule import Rule\n",
    "\n",
    "class CustomGradientRule(Rule):\n",
    "```\n",
    "\n",
    "Now implement the class methods for the rule. Doing this allows Amazon SageMaker to understand the intent of the rule and evaluate it against your training tensors.\n",
    "\n",
    "##### **Rule class constructor**\n",
    "\n",
    "In order for Amazon SageMaker to instantiate your rule, your rule class constructor must conform to the following signature.\n",
    "```python\n",
    "    def __init__(self, base_trial, other_trials, <other parameters>)\n",
    "```\n",
    "`base_trial (Trial)`: This defines the primary [Trial](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/analysis.md#trial) that your rule is anchored to. This is an object of class type `Trial`.\n",
    "\n",
    "`other_trials (list[Trial])`: *(Optional)* This defines a list of 'other' trials you want your rule to look at. This is useful in the scenarios when you're comparing tensors from the base_trial to tensors from some other trials. \n",
    "\n",
    "`<other parameters>`: This is similar to `**kwargs` where you can pass in however many string parameters in your constructor signature. Note that SageMaker would only be able to support supplying string types for these values at runtime (see how, later).\n",
    "\n",
    "##### `invoke_at_step()`:\n",
    "\n",
    "This defines the logic to invoked for each step. Essentially, this is where you decide whether the rule should trigger or not. In this case, you're concerned about the gradients getting too large. So, get the [tensor reduction]() \"mean\" for each step and see if it's value is larger than a threshold.\n",
    "\n",
    "```python\n",
    "    def invoke_at_step(self, step):\n",
    "        for tname in self.base_trial.tensor_names(collection=\"gradients\"):\n",
    "            t = self.base_trial.tensor(tname)\n",
    "            abs_mean = t.reduction_value(step, \"mean\", abs=True)\n",
    "            if abs_mean > self.threshold:\n",
    "                return True\n",
    "        return False\n",
    "```\n",
    "\n",
    "See the [documentation](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/analysis.md) to learn more about structuring your rules and other related concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_rule = Rule.custom(name='MyCustomRule', # used to identify the rule\n",
    "                          image_uri='490809245908.dkr.ecr.us-west-2.amazonaws.com/sagemaker-debugger-rule-evaluator:latest', # rule evaluator container image\n",
    "                          instance_type='ml.t3.medium', # instance type to run the rule evaluation on\n",
    "                          source='rules/my_custom_rule.py', # path to the rule source file\n",
    "                          rule_to_invoke='CustomGradientRule', # name of the class to invoke in the rule source file\n",
    "                          volume_size_in_gb=30, # EBS volume size required to be attached to the rule evaluation instance\n",
    "                          collections_to_save=[CollectionConfig(\"gradients\")], \n",
    "                          # collections to be analyzed by the rule. since this is a first party collection we fetch it as above\n",
    "                          rule_parameters={\n",
    "                              \"threshold\": \"20.0\" # this will be used to intialize 'threshold' param in your constructor\n",
    "                          })\n",
    "\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    base_job_name='smdebugger-custom-rule-demo-mnist-tensorflow',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m4.xlarge',\n",
    "    entry_point=entrypoint_script,\n",
    "    framework_version='1.15',\n",
    "    py_version='py3',\n",
    "    train_max_run=3600,\n",
    "    script_mode=True,\n",
    "    ## New parameter\n",
    "    rules = [custom_rule]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you proceed and create our training job, explore the new parameters in the TensorFlow estimator above\n",
    "\n",
    "* `name`: This is used to identify this particular rule among the suite of rules you specified to be evaluated.\n",
    "* `image_uri`: This is the image of the container that has the logic of understanding your custom rule sources and evaluating them against the collections you save in the training job. You can get the list of open sourced SageMaker rule evaluator images [here]()\n",
    "* `instance_type`: The type of the instance you want to run the rule evaluation on\n",
    "* `source`: This is the local path or the S3 Uri of your rule source file.\n",
    "* `rule_to_invoke`: This specifies the particular Rule class implementation in your source file which you want to be evaluated. SageMaker supports only 1 rule to be evaluated at a time in a rule job. Your source file can have multiple Rule class implementations, though.\n",
    "* `collections_to_save`: This specifies which collections are necessary for this rule.\n",
    "* `rule_parameters`: This provides the runtime values of the parameter in your constructor. You can still choose to pass in other values which may be necessary for your rule to be evaluated. Any value in this map is available as an environment variable and can be accessed by your rule script using `$<rule_parameter_key>`\n",
    "\n",
    "You can read more about custom rule evaluation in Amazon SageMaker in this [documentation](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/analysis.md)\n",
    "\n",
    "\n",
    "Let's call `fit()` on our estimator to start the training job and the parallel custom rule evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AlgorithmSpecification': {'TrainingInputMode': 'File', 'TrainingImage': '072677473360.dkr.ecr.us-west-2.amazonaws.com/beta-tensorflow-training:1.15.0-py3-cpu-with-horovod-build', 'EnableSageMakerMetricsTimeSeries': True}, 'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-west-2-072677473360/'}, 'TrainingJobName': 'smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}, 'ResourceConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m4.xlarge', 'VolumeSizeInGB': 30}, 'RoleArn': 'arn:aws:iam::072677473360:role/service-role/AmazonSageMaker-ExecutionRole-20190917T111877', 'HyperParameters': {'sagemaker_submit_directory': '\"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210/source/sourcedir.tar.gz\"', 'sagemaker_program': '\"mnist_zerocodechange.py\"', 'sagemaker_enable_cloudwatch_metrics': 'false', 'sagemaker_container_log_level': '20', 'sagemaker_job_name': '\"smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210\"', 'sagemaker_region': '\"us-west-2\"', 'model_dir': '\"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210/model\"'}, 'DebugRuleConfigurations': [{'RuleConfigurationName': 'MyCustomRule', 'RuleEvaluatorImage': '490809245908.dkr.ecr.us-west-2.amazonaws.com/sagemaker-debugger-rule-evaluator:latest', 'InstanceType': 'ml.t3.medium', 'VolumeSizeInGB': 30, 'RuleParameters': {'source_s3_uri': 's3://sagemaker-us-west-2-072677473360/MyCustomRule/5ecc69db-d745-433e-bee5-a093f427678b/my_custom_rule.py', 'rule_to_invoke': 'CustomGradientRule', 'threshold': '20.0'}}], 'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-west-2-072677473360/', 'CollectionConfigurations': [{'CollectionName': 'gradients'}]}}\n",
      "2019-12-01 22:26:36 Starting - Starting the training job...\n",
      "2019-12-01 22:26:59 Starting - Launching requested ML instances\n",
      "********* Debugger Rule Status *********\n",
      "*\n",
      "*       MyCustomRule: InProgress        \n",
      "*\n",
      "****************************************\n",
      "...\n",
      "2019-12-01 22:27:36 Starting - Preparing the instances for training......\n",
      "2019-12-01 22:28:31 Downloading - Downloading input data...\n",
      "2019-12-01 22:29:01 Training - Downloading the training image......\n",
      "2019-12-01 22:30:01 Training - Training image download completed. Training in progress.\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[0m\n",
      "\u001b[34m2019-12-01 22:29:50,527 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2019-12-01 22:29:50,534 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-12-01 22:29:50,869 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-12-01 22:29:50,887 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-12-01 22:29:50,904 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-12-01 22:29:50,916 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_zerocodechange\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_zerocodechange.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist_zerocodechange.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist_zerocodechange\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210/source/sourcedir.tar.gz\",\"module_name\":\"mnist_zerocodechange\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_zerocodechange.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 mnist_zerocodechange.py --model_dir s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-12-01-22-26-35-210/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[0m\n",
      "\u001b[34mDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\u001b[0m\n",
      "\u001b[34m#015    8192/11490434 [..............................] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 2121728/11490434 [====>.........................] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 5857280/11490434 [==============>...............] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01511493376/11490434 [==============================] - 0s 0us/step\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist_zerocodechange.py:105: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n",
      "\u001b[0m\n",
      "\u001b[34m[2019-12-01 22:29:55.018 ip-10-0-160-100.us-west-2.compute.internal:26 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[0m\n",
      "\u001b[34m[2019-12-01 22:29:55.041 ip-10-0-160-100.us-west-2.compute.internal:26 INFO hook.py:151] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2019-12-01 22:29:55.041 ip-10-0-160-100.us-west-2.compute.internal:26 INFO hook.py:196] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist_zerocodechange.py:44: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.keras.layers.Conv2D` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease use `layer.__call__` method instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist_zerocodechange.py:48: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse keras.layers.MaxPooling2D instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist_zerocodechange.py:58: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse keras.layers.Dense instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist_zerocodechange.py:60: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse keras.layers.dropout instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist_zerocodechange.py:78: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.where in 2.0, which has the same broadcast rule as np.where\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist_zerocodechange.py:79: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist_zerocodechange.py:83: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist_zerocodechange.py:84: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\u001b[0m\n",
      "\u001b[34m[2019-12-01 22:29:55.754 ip-10-0-160-100.us-west-2.compute.internal:26 INFO hook.py:325] Monitoring the collections: gradients, losses, metrics, sm_metrics\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py:888: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/smdebug/tensorflow/session.py:310: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.compat.v1.graph_util.extract_sub_graph`\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist_zerocodechange.py:89: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
      "\u001b[0m\n",
      "\u001b[34m[2019-12-01 22:34:35.820 ip-10-0-160-100.us-west-2.compute.internal:26 INFO hook.py:325] Monitoring the collections: gradients, losses, metrics, sm_metrics\u001b[0m\n",
      "\u001b[34m[2019-12-01 22:34:40.162 ip-10-0-160-100.us-west-2.compute.internal:26 INFO utils.py:27] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2019-12-01 22:34:40,593 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[34mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[34mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[34m2019-12-01 22:34:40,594 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-12-01 22:35:06 Uploading - Uploading generated training model\n",
      "2019-12-01 22:35:06 Completed - Training job completed\n",
      "Training seconds: 386\n",
      "Billable seconds: 386\n"
     ]
    }
   ],
   "source": [
    "# After calling fit, Amazon SageMaker spins off one training job and one rule job for you.\n",
    "# The rule evaluation status is visible in the training logs\n",
    "# at regular intervals\n",
    "\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result \n",
    "\n",
    "As a result of calling the `fit()` Amazon SageMaker debugger kicked off a rule evaluation job for our custom gradient logic in-parallel with the training job that was monitoring the tensors output by the training job. As you can see, in the summary, there was no step in the training that reported vanishing gradients in the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "description = client.describe_training_job(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RuleConfigurationName': 'MyCustomRule',\n",
       "  'RuleEvaluationJobArn': 'arn:aws:sagemaker:us-west-2:072677473360:processing-job/smdebugger-demo-mnist-tens-mycustomrule-3ddb0ffc',\n",
       "  'RuleEvaluationStatus': 'NoIssuesFound',\n",
       "  'LastModifiedTime': datetime.datetime(2019, 12, 1, 22, 35, 26, 471000, tzinfo=tzlocal())}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description['DebugRuleEvaluationStatuses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having these kind of analyses run through the Amazon SageMaker Debugger in-parallel with the training job is beneficial. You can react to the status transitions of the rule evaluations by configuring Amazon CloudWatch events."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
