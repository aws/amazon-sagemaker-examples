#adaptive_noise_scale = 0.0 #add `latent mean absolute value * this value` to noise_offset
#additional_parameters = "" #use to provide additional params not supported by GUI
bucket_no_upscale = true #make bucket for each image without upscaling
bucket_reso_steps = 64 #steps of resolution for buckets, divisible by 8 is recommended
cache_latents = true #cache latents to main memory to reduce VRAM usage (augmentations must be disabled)
cache_latents_to_disk = true #cache latents to disk to reduce VRAM usage (augmentations must be disabled)
caption_dropout_every_n_epochs = 0 #Dropout all captions every N epochs
caption_dropout_rate = 0.0 #Rate out dropout caption(0.0~1.0)
#clip_skip = 1 #use output of nth layer from back of text encoder (n>=1)
#color_aug = false #enable weak color augmentation
conv_alpha = 1
conv_dim = 1
enable_bucket = true #enable buckets for multi aspect ratio training
#full_bf16 = false #(experimental) bf16 training including gradients
#full_fp16 = false #(experimental) fp16 training including gradients
gradient_accumulation_steps = 1 #number of updates steps to accumulate before performing a backward/update pass
gradient_checkpointing = true #enable gradient checkpointing
#keep_tokens = 0 #keep heading N tokens when shuffling caption tokens (token means comma separated strings)
learning_rate = 0.0004 #learning rate
#logging_dir = "" #enable logging and output TensorBoard log to this directory
lr_scheduler = "constant" #scheduler to use for learning rate. Choices: linear, cosine, cosine_with_restarts, polynomial, constant (default), constant_with_warmup, adafactor
lr_scheduler_num_cycles = 1 #number of restarts for cosine scheduler with restarts
lr_scheduler_power = 1 #polynomial power for polynomial scheduler
#lr_warmup_steps = 0 #number of steps for the warmup in the lr scheduler (default is 0)
max_bucket_reso = 4096 #maximum resolution for buckets
#max_data_loader_n_workers = 0 #max num workers for DataLoader (lower is less main RAM usage, faster epoch start and slower data loading)
resolution = "1024,1024" #resolution in training ('size' or 'width,height')
max_timestep = 1000 #set maximum time step for U-Net training (1~1000, default is 1000)
max_token_length = 75 #max token length of text encoder (75, 150, or 225)
max_train_epochs = 1 #training epochs (overrides max_train_steps)
#max_train_steps #training steps. Use only if you want to calculate yourself (other variables must add up correctly to the steps or you will get an error). Instead use max_train_epochs to auto-calculate.
mem_eff_attn = false #use memory efficient attention for CrossAttention
min_bucket_reso = 512 #minimum resolution for buckets
#min_snr_gamma = 0
min_timestep = 0 #set minimum time step for U-Net training (0~999, default is 0)
mixed_precision = "bf16" #use mixed precision (choices: no, fp16, bf16)
model_list = "custom" #pick model from list
network_alpha = 1 #alpha for LoRA weight scaling
network_dim = 256 #network rank
#network_dropout = 0
network_module = "networks.lora"
no_token_padding = false
noise_offset = 0.0357 #enable noise offset with this value (if enabled, around 0.1 is recommended). SDXL recommendated value may be different.
noise_offset_type = "Original"
#num_cpu_threads_per_process = 2 #Don't set this value here. This value is set in the "train" file when it calls "accelerate launch"
optimizer_type = "Adafactor" #Optimizer to use. AdamW (default), AdamW8bit, PagedAdamW8bit, Lion8bit, PagedLion8bit, Lion, SGDNesterov, SGDNesterov8bit, DAdaptation, DAdaptAdaGrad, DAdaptAdam, DAdaptAdan, DAdaptAdanIP, DAdaptAdamPreprint, DAdaptLion, DAdaptSGD, Adafactor, Prodigy
optimizer_args = [ "scale_parameter=False", "relative_step=False", "warmup_init=False" ] #additional arguments for optimizer (like "weight_decay=0.01 betas=0.9,0.999 ...")
output_dir = "/opt/ml/model" #directory to output trained model #do not modify the output directory, as it is used by SageMaker training
output_name = "custom_lora_model" #base name of trained model file
persistent_data_loader_workers = false #persistent DataLoader workers (useful for reduce time gap between epoch, but may use more memory)
pretrained_model_name_or_path = "stabilityai/stable-diffusion-xl-base-1.0" #pretrained model to train, directory to Diffusers model or StableDiffusion checkpoint
prior_loss_weight = 1.0 #loss weight for regularization images
random_crop = false #enable random crop (for style training in face-centered crop augmentation)
#reg_data_dir = #TODO: add support for regularization. "" #directory for regularization images. 
#sample_every_n_epochs = #TODO: add support for sampling. 0 #generate sample images every N epochs (overwrites n_steps)
#sample_every_n_steps = #TODO: add support for sampling. 100 #generate sample images every N steps
#sample_prompts= #TODO: add support for sampling. #"<model-directory>/sample/prompt.txt"
#sample_prompts = #TODO: add support for sampling. "<trigger-word> man wearing sunglasses and a leather jacket, natural lighting, highly detailed, cinematic" #file for prompts to generate sample images
#sample_sampler = #TODO: add support for sampling. "dpm_2" #sampler (scheduler) type for sample images. Choices: "ddim", "pndm", "lms", "euler", "euler_a", "heun", "dpm_2", "dpm_2_a", "dpmsolver", "dpmsolver++", "dpmsingle", "k_lms", "k_euler", "k_euler_a", "k_dpm_2", "k_dpm_2_a"
save_every_n_epochs = 1 #save checkpoint every N epochs (specify either save_every_n_epochs or save_every_n_steps argument but not both)
#save_every_n_steps = 0 #save checkpoint every N steps (specify either save_every_n_epochs or save_every_n_steps argument but not both)
#save_last_n_steps = 0 #save checkpoints until N steps elapsed (remove older checkpoints if N steps elapsed)
#save_last_n_steps_state = 0 #save states until N steps elapsed (remove older states if N steps elapsed, overrides --save_last_n_steps)
save_model_as = "safetensors" #format to save the model (default is same to original). Choices: "ckpt", "safetensors", "diffusers", "diffusers_safetensors"
save_precision = "bf16" #precision in saving. Choices: "float", "fp16", "bf16"
save_state = false #save training state additionally (including optimizer states etc.)
sdxl = true
cache_text_encoder_outputs = false #only for sdxl. Cache the outputs of the text encoders. This option is useful to reduce the GPU memory usage. This option cannot be used with options for shuffling or dropping the captions
no_half_vae = true #only for sdxl. Disable the half-precision (mixed-precision) VAE. VAE for SDXL seems to produce NaNs in some cases. This option is useful to avoid the NaNs
# seed = 0 #random seed for training
shuffle_caption = false #shuffle comma-separated caption
text_encoder_lr = 0.0004 #learning rate
train_batch_size = 1 #batch size for training
train_data_dir = "/opt/ml/input/data/train" #directory for train images. This is used by SageMaker training
train_on_input = true
unet_lr = 0.0004 #learning rate
unit = 1
v_parameterization = false #enable v-parameterization training
vae_batch_size = 0 #batch size for caching latents
xformers = "xformers" #use xformers for CrossAttention