{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb44f6d",
   "metadata": {},
   "source": [
    "# Introduction to SageMaker JumpStart - Text Generation with Falcon models\n",
    "\n",
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to fine-tuning and deploy Falcon 7B models for text generation. For fine-tuning, we include two types of fine-tuning: instruction fine-tuning and domain adaption fine-tuning. \n",
    "\n",
    "The Falcon model is a permissively licensed ([Apache-2.0](https://jumpstart-cache-prod-us-east-2.s3.us-east-2.amazonaws.com/licenses/Apache-License/LICENSE-2.0.txt)) open source model trained on the [RefinedWeb dataset](https://huggingface.co/datasets/tiiuae/falcon-refinedweb). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c76151-390e-4592-a192-a857292e4e53",
   "metadata": {},
   "source": [
    "Below is the content of the notebook.\n",
    "\n",
    "1. [Instruction fine-tuning](#1.-Instruction-fine-tuning)\n",
    "   * [1.1. Preparing training data](#1.1.-Preparing-training-data)\n",
    "   * [1.2. Prepare training parameters](#1.2.-Prepare-training-parameters)\n",
    "   * [1.3. Starting training](#1.3.-Starting-training)\n",
    "   * [1.4. Deploying inference endpoints](#1.4.-Deploying-inference-endpoints)\n",
    "   * [1.5. Running inference queries and compare model performances](#1.5.-Running-inference-queries-and-compare-model-performances)\n",
    "   * [1.6. Clean up endpoint](#1.6.-Clean-up-the-endpoint)\n",
    "2. [Domain adaptation fine-tuning](#2.-Domain-adaptation-fine-tuning)\n",
    "   * [2.1. Preparing training data](#2.1.-Preparing-training-data)\n",
    "   * [2.2. Prepare training parameters](#2.2.-Prepare-training-parameters)\n",
    "   * [2.3. Starting training](#2.3.-Starting-training)\n",
    "   * [2.4. Deploying inference endpoints](#2.4.-Deploying-inference-endpoints)\n",
    "   * [2.5. Running inference queries and compare model performances](#2.5.-Running-inference-queries-and-compare-model-performances)\n",
    "   * [2.6. Clean up endpoint](#2.6.-Clean-up-the-endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066d7593-7300-4ed6-8f78-63c4b889da72",
   "metadata": {},
   "source": [
    "Install latest SageMaker and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05b931-992e-4526-978d-f03196874a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker --quiet --upgrade --force-reinstall\n",
    "!pip install ipywidgets==7.0.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460e23d-7011-4639-94a2-c55f53a035c9",
   "metadata": {},
   "source": [
    "## 1. Instruction fine-tuning\n",
    "\n",
    "Now, we demonstrate how to instruction-tune `huggingface-llm-falcon-7b-instruct-bf16` model for a new task. As mentioned in [Section 1.3 About the models](#1.3.-About-the-model),  **Falcon-7b-instruct**/**Falcon-40B-instruct** models are instruction base falcon models fine-tuned on a mixture of chat and instruction datasets. \n",
    "\n",
    "**In this task, given a piece of context, the model is asked to generate questions that are relevant to the text, but `cannot` be answered based on provided information. Examples are given in the inference section of this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d90321-54c7-4572-a3f8-bf1810cfd112",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-llm-falcon-7b-instruct-bf16\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba27120-e9ca-4408-b0f0-80ef5a1a5531",
   "metadata": {},
   "source": [
    "### 1.1. Preparing training data\n",
    "We will use a subset of SQuAD2.0 for supervised fine-tuning. This dataset contains questions posed by human annotators on a set of Wikipedia articles. In addition to questions with answers, SQuAD2.0 contains about 50k unanswerable questions. Such questions are plausible, but cannot be directly answered from the articles' content. We only use unanswerable questions for our task.\n",
    "\n",
    "Citation: @article{rajpurkar2018know, title={Know what you don't know: Unanswerable questions for SQuAD}, author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy}, journal={arXiv preprint arXiv:1806.03822}, year={2018} }\n",
    "\n",
    "License: Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cb8a9-be2f-436b-9444-a52fd0caf518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "\n",
    "# Get current region, role, and default bucket\n",
    "aws_region = boto3.Session().region_name\n",
    "aws_role = sagemaker.session.Session().get_caller_identity_arn()\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "# This will be useful for printing\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "print(f\"{bold}aws_region:{unbold} {aws_region}\")\n",
    "print(f\"{bold}aws_role:{unbold} {aws_role}\")\n",
    "print(f\"{bold}output_bucket:{unbold} {output_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e61aa5-e2cd-4cec-902e-5a4b94bb5817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# We will use the train split of SQuAD2.0\n",
    "original_data_file = \"train-v2.0.json\"\n",
    "\n",
    "# The data was mirrored in the following bucket\n",
    "original_data_location = (\n",
    "    f\"s3://sagemaker-example-files-prod-{aws_region}/datasets/text/squad2.0/{original_data_file}\"\n",
    ")\n",
    "S3Downloader.download(original_data_location, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c061fc5-e11b-4a72-8726-00ab32452769",
   "metadata": {},
   "source": [
    "The training data must be formatted in JSON lines (.jsonl) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple jsonl files. The .jsonl file extension is mandatory. The training folder can also contain a template.json file describing the input and output formats.\n",
    "\n",
    "If no template file is given, the following default template will be used:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}`,\n",
    "    \"completion\": \"{response}\",\n",
    "}\n",
    "```\n",
    "\n",
    "In this case, the data in the JSON lines entries must include `instruction`, `context`, and `response` fields.\n",
    "\n",
    "Different from using the default prompt template, in this demo we are going to use a custom template (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df5174-667c-44ea-ad20-47c44ca0e628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = {\n",
    "    \"prompt\": \"Ask a question which is related to the following text, but cannot be answered based on the text. Text: {context}\",\n",
    "    \"completion\": \"{question}\",\n",
    "}\n",
    "\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdecf3-cb15-4ef9-ab6c-ebe9c43aab1d",
   "metadata": {},
   "source": [
    "Next, we are going to reformat the SQuAD 2.0 dataset. The processed data is saved as `task-data.jsonl` file. Given the prompt template defined in above cell, each entry in the `task-data.jsonl` file include **`context`** and **`question`** fields. For demonstration purpose, we limit the number of training examples to be 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b4216-cd79-4e6a-87a8-0fbcdb4260e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_file = \"task-data.jsonl\"  # any name with .jsonl extension\n",
    "\n",
    "with open(original_data_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "def preprocess_data(local_data_file, data, num_maximum_example):\n",
    "    num_example_idx = 0\n",
    "    with open(local_data_file, \"w\") as f:\n",
    "        for article in data[\"data\"]:\n",
    "            for paragraph in article[\"paragraphs\"]:\n",
    "                # iterate over questions for a given paragraph\n",
    "                for qas in paragraph[\"qas\"]:\n",
    "                    if qas[\"is_impossible\"]:\n",
    "                        # the question is relevant, but cannot be answered\n",
    "                        example = {\"context\": paragraph[\"context\"], \"question\": qas[\"question\"]}\n",
    "                        json.dump(example, f)\n",
    "                        f.write(\"\\n\")\n",
    "                        num_example_idx += 1\n",
    "                        if num_example_idx >= num_maximum_example:\n",
    "                            return\n",
    "\n",
    "\n",
    "preprocess_data(local_data_file=local_data_file, data=data, num_maximum_example=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a170ae-b6d4-45a2-ade7-4595e97a5ad8",
   "metadata": {},
   "source": [
    "Upload the prompt template (`template.json`) and training data (`task-data.jsonl`) into S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11577613-2f64-46d1-9fcd-85c9b11a4087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{output_bucket}/train_data324242\"\n",
    "S3Uploader.upload(local_data_file, training_dataset_s3_path)\n",
    "S3Uploader.upload(\"template.json\", training_dataset_s3_path)\n",
    "print(f\"{bold}training data:{unbold} {training_dataset_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9bf21-1d0f-4ea4-a71a-57b2a341219a",
   "metadata": {},
   "source": [
    "### 1.2. Prepare training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa35d5-4211-4976-929a-11c52edd62f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "my_hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version\n",
    ")\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c74a6e-b971-4a41-a8a5-ffd5fa9b2e29",
   "metadata": {},
   "source": [
    "Overwrite the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698cf73-7fe4-4d50-9fe1-296a43a8592a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_hyperparameters[\"epoch\"] = \"2\"\n",
    "my_hyperparameters[\"per_device_train_batch_size\"] = \"2\"\n",
    "my_hyperparameters[\"gradient_accumulation_steps\"] = \"2\"\n",
    "my_hyperparameters[\"instruction_tuned\"] = \"True\"\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d930ae7",
   "metadata": {},
   "source": [
    "Validate hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244d372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters.validate(\n",
    "    model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d147475-329c-4caf-90c7-da6eb622c021",
   "metadata": {},
   "source": [
    "### 1.3. Starting training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36641d77",
   "metadata": {},
   "source": [
    "Note. The parameter `load_best_model_at_end` (Whether or not to load the best model found during training at the end of training. When this option is enabled, the best checkpoint will always be saved) is set as \"True\" by default. During loading the best model checkpoints at the end of training (HuggingFace will load the best model checkpoints before saving it), there is overhead of memory usage which can lead to Out-Of-Memory error.\n",
    "\n",
    "If setting `load_best_model_at_end`, we recommend to use `ml.g5.48xlarge`; if not, we recommend to use `ml.g5.12xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29866d1a-5e3f-4867-aae6-6d024cd608ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "instruction_tuned_estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    hyperparameters=my_hyperparameters,\n",
    "    instance_type=\"ml.g5.48xlarge\",\n",
    ")\n",
    "instruction_tuned_estimator.fit({\"train\": training_dataset_s3_path}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bddbc6e-1512-4a71-9247-76cf03e9bf4b",
   "metadata": {},
   "source": [
    "Extract Training performance metrics. Performance metrics such as training loss and validation accuracy/loss can be accessed through cloudwatch while the training. We can also fetch these metrics and analyze them within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c2488-b3cf-4b53-954c-aa409969c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "training_job_name = instruction_tuned_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340745b6-ae16-4c7e-83fc-7be0ffd338e3",
   "metadata": {},
   "source": [
    "### 1.4. Deploying inference endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0011f1-aec4-4ca2-8e7d-0a66798713c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_tuned_predictor = instruction_tuned_estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e2a5a-acfa-4712-b491-9b0742f58a6a",
   "metadata": {},
   "source": [
    "We also deploy the pre-trained model for comparision in Section 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8536e3a-b528-4ff1-aaf0-9e4259d43e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "my_model = JumpStartModel(model_id=model_id)\n",
    "predictor = my_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93fafa-eaa3-4535-a7e2-3b42637ebb18",
   "metadata": {},
   "source": [
    "### 1.5. Running inference queries and compare model performances\n",
    "\n",
    "We examine three examples as listed in variable `test_paragraphs`. The prompt as defined in variable `prompt` asks the model to ask a question based on the context and make sure the question **cannot** be answered from the context. \n",
    "\n",
    "We compare the performance of pre-trained Falcon instruct 7b (`huggingface-llm-falcon-7b-instruct-bf16`) that we deployed in [Section 1](#1.-Deploying-Falcon-model-for-inference) and fine-tuned Falcon instruct 7b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af09fe-f636-433b-8f09-b5fa84c2d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Ask a question which is related to the following text, but cannot be answered based on the text. Text: {context}\"\n",
    "\n",
    "# Sources: Wikipedia, AWS Documentation\n",
    "test_paragraphs = [\n",
    "    \"\"\"\n",
    "Adelaide is the capital city of South Australia, the state's largest city and the fifth-most populous city in Australia. \"Adelaide\" may refer to either Greater Adelaide (including the Adelaide Hills) or the Adelaide city centre. The demonym Adelaidean is used to denote the city and the residents of Adelaide. The Traditional Owners of the Adelaide region are the Kaurna people. The area of the city centre and surrounding parklands is called Tarndanya in the Kaurna language.\n",
    "Adelaide is situated on the Adelaide Plains north of the Fleurieu Peninsula, between the Gulf St Vincent in the west and the Mount Lofty Ranges in the east. Its metropolitan area extends 20 km (12 mi) from the coast to the foothills of the Mount Lofty Ranges, and stretches 96 km (60 mi) from Gawler in the north to Sellicks Beach in the south.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "Amazon Elastic Block Store (Amazon EBS) provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices. You can mount these volumes as devices on your instances. EBS volumes that are attached to an instance are exposed as storage volumes that persist independently from the life of the instance. You can create a file system on top of these volumes, or use them in any way you would use a block device (such as a hard drive). You can dynamically change the configuration of a volume attached to an instance.\n",
    "We recommend Amazon EBS for data that must be quickly accessible and requires long-term persistence. EBS volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage. Amazon EBS is well suited to both database-style applications that rely on random reads and writes, and to throughput-intensive applications that perform long, continuous reads and writes.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases. \n",
    "You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. \n",
    "All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input. \n",
    "Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e8682-1dcc-4a4b-a457-08b7b8aef27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.01,\n",
    "}\n",
    "\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "def generate_question(endpoint_name, text):\n",
    "    expanded_prompt = prompt.replace(\"{context}\", text)\n",
    "    payload = {\"inputs\": expanded_prompt, \"parameters\": parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response(query_response)\n",
    "    print(f\"Response: {generated_texts}{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef21052-3e86-4626-9636-37945ea5e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{bold}Prompt:{unbold} {repr(prompt)}\")\n",
    "for paragraph in test_paragraphs:\n",
    "    print(\"-\" * 80)\n",
    "    print(paragraph)\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{bold}pre-trained{unbold}\")\n",
    "    generate_question(predictor.endpoint_name, paragraph)\n",
    "    print(f\"{bold}fine-tuned{unbold}\")\n",
    "    generate_question(instruction_tuned_predictor.endpoint_name, paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a29dc-a10b-40e9-a77d-3f71858aa589",
   "metadata": {},
   "source": [
    "The pre-trained model was not specifically trained to generate unanswerable questions. Despite the input prompt, it tends to generate questions that can be answered from the text. The fine-tuned model is generally better at this task, and the improvement is more prominent for larger models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e51104-2f8d-45d1-827d-4b955546d383",
   "metadata": {},
   "source": [
    "### 1.6. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39f216-5866-47dd-a264-53f677e7eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()\n",
    "instruction_tuned_predictor.delete_model()\n",
    "instruction_tuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed71e3-8fa9-4992-97d4-e038f834bf41",
   "metadata": {},
   "source": [
    "## 2. Domain adaptation fine-tuning\n",
    "\n",
    "We also have domain adaptation fine-tuning enabled for Falcon models. Different from instruction fine-tuning, you do not need prepare instruction-formatted dataset and can directly use unstructured text document which is demonstrated as below. However, the model that is domain-adaptation fine-tuned may not give concise responses as the instruction-tuned model because of less restrictive requirements on training data formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a0204-8ce9-4304-86f6-779ec6638c6b",
   "metadata": {},
   "source": [
    "In this demonstration, we use falcon text generation model `huggingface-llm-falcon-7b-bf16`. This is not an instruction-tuned version of Falcon 7B model. You can also conduct domain adaptation finetuning on top of instruction-tuned model like `huggingface-llm-falcon-7b-instruct-bf16`. However, we generally do not recommend that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f9266-0dd3-4533-a5f7-3cea6764058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"huggingface-llm-falcon-7b-bf16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf488a2-956a-4e70-8bdd-d0fd05646603",
   "metadata": {},
   "source": [
    "We will use financial text from SEC filings to fine tune `huggingface-llm-falcon-7b-bf16` for financial applications. \n",
    "\n",
    "Here are the requirements for train and validation data.\n",
    "\n",
    "- **Input**: A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file.\n",
    "    - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "    - The number of files under train and validation (if provided) should equal to one.\n",
    "- **Output**: A trained model that can be deployed for inference.\n",
    "\n",
    "Below is an example of a TXT file for fine-tuning the Text Generation model. The TXT file is SEC filings of Amazon from year 2021 to 2022.\n",
    "\n",
    "---\n",
    "```\n",
    "This report includes estimates, projections, statements relating to our\n",
    "business plans, objectives, and expected operating results that are “forward-\n",
    "looking statements” within the meaning of the Private Securities Litigation\n",
    "Reform Act of 1995, Section 27A of the Securities Act of 1933, and Section 21E\n",
    "of the Securities Exchange Act of 1934. Forward-looking statements may appear\n",
    "throughout this report, including the following sections: “Business” (Part I,\n",
    "Item 1 of this Form 10-K), “Risk Factors” (Part I, Item 1A of this Form 10-K),\n",
    "and “Management’s Discussion and Analysis of Financial Condition and Results\n",
    "of Operations” (Part II, Item 7 of this Form 10-K). These forward-looking\n",
    "statements generally are identified by the words “believe,” “project,”\n",
    "“expect,” “anticipate,” “estimate,” “intend,” “strategy,” “future,”\n",
    "“opportunity,” “plan,” “may,” “should,” “will,” “would,” “will be,” “will\n",
    "continue,” “will likely result,” and similar expressions. Forward-looking\n",
    "statements are based on current expectations and assumptions that are subject\n",
    "to risks and uncertainties that may cause actual results to differ materially.\n",
    "We describe risks and uncertainties that could cause actual results and events\n",
    "to differ materially in “Risk Factors,” “Management’s Discussion and Analysis\n",
    "of Financial Condition and Results of Operations,” and “Quantitative and\n",
    "Qualitative Disclosures about Market Risk” (Part II, Item 7A of this Form\n",
    "10-K). Readers are cautioned not to place undue reliance on forward-looking\n",
    "statements, which speak only as of the date they are made. We undertake no\n",
    "obligation to update or revise publicly any forward-looking statements,\n",
    "whether because of new information, future events, or otherwise.\n",
    "\n",
    "...\n",
    "```\n",
    "---\n",
    "SEC filings data of Amazon is downloaded from publicly available [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch). Instruction of accessing the data is shown [here](https://www.sec.gov/os/accessing-edgar-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d136eb-47d1-4555-bc4b-3a39d1ef2b6c",
   "metadata": {},
   "source": [
    "### 2.1. Preparing training data\n",
    "\n",
    "The training data of SEC filing of Amazon has been pre-saved in the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c71845-58e1-42ca-891f-b22e44c911d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.utils import get_jumpstart_content_bucket\n",
    "\n",
    "# Sample training data is available in this bucket\n",
    "data_bucket = get_jumpstart_content_bucket(aws_region)\n",
    "data_prefix = \"training-datasets/sec_data\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/train/\"\n",
    "validation_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/validation/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c322e99-d9d3-47b2-8489-0cfff08a1f9d",
   "metadata": {},
   "source": [
    "### 2.2. Prepare training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11534a56-ae96-49e2-a8e2-2852a6dfdbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "my_hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version\n",
    ")\n",
    "\n",
    "my_hyperparameters[\"epoch\"] = \"3\"\n",
    "my_hyperparameters[\"per_device_train_batch_size\"] = \"2\"\n",
    "my_hyperparameters[\"instruction_tuned\"] = \"False\"\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b262c88",
   "metadata": {},
   "source": [
    "Validate hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9877c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters.validate(\n",
    "    model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd26bc12-d008-4ad6-9193-57e002ab9d67",
   "metadata": {},
   "source": [
    "### 2.3. Starting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e682d-586e-464c-b28f-451d036d562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "domain_adaptation_estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    hyperparameters=my_hyperparameters,\n",
    "    instance_type=\"ml.p3dn.24xlarge\",\n",
    ")\n",
    "domain_adaptation_estimator.fit(\n",
    "    {\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path}, logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03751984-5ca5-473a-bf34-8d66295c0e39",
   "metadata": {},
   "source": [
    "Extract Training performance metrics. Performance metrics such as training loss and validation accuracy/loss can be accessed through cloudwatch while the training. We can also fetch these metrics and analyze them within the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960e861-635a-43c0-a60d-6f4a41c105bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "training_job_name = domain_adaptation_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ca2e1-d968-434e-85f4-8605350c0ed4",
   "metadata": {},
   "source": [
    "### 2.4. Deploying inference endpoints\n",
    "\n",
    "We deploy the domain-adaptation fine-tuned and pretrained models separately, and compare their performances.\n",
    "\n",
    "We firstly deploy the domain-adaptation fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da369cd9-e3ad-4981-88c9-ac68022c3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_adaptation_predictor = domain_adaptation_estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86815ad-ef6e-47a3-bfc3-b2bb7ab1886a",
   "metadata": {},
   "source": [
    "Next, we deploy the pre-trained `huggingface-llm-falcon-7b-bf16`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1bf93-34d4-4a47-8d8e-76403759445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = JumpStartModel(model_id=model_id)\n",
    "pretrained_predictor = my_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc6024-87be-495a-a541-4f0132c306e2",
   "metadata": {},
   "source": [
    "### 2.5. Running inference queries and compare model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1c732-636f-4473-8c28-3b1fcbb44453",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "def generate_response(endpoint_name, text):\n",
    "    payload = {\"inputs\": f\"{text}:\", \"parameters\": parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response(query_response)\n",
    "    print(f\"Response: {generated_texts}{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1353e-1afb-4a5a-b6c5-f406ea994476",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paragraph_domain_adaption = [\n",
    "    \"This Form 10-K report shows that\",\n",
    "    \"We serve consumers through\",\n",
    "    \"Our vision is\",\n",
    "]\n",
    "\n",
    "\n",
    "for paragraph in test_paragraph_domain_adaption:\n",
    "    print(\"-\" * 80)\n",
    "    print(paragraph)\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{bold}pre-trained{unbold}\")\n",
    "    generate_response(pretrained_predictor.endpoint_name, paragraph)\n",
    "    print(f\"{bold}fine-tuned{unbold}\")\n",
    "    generate_response(domain_adaptation_predictor.endpoint_name, paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca5bdd",
   "metadata": {},
   "source": [
    "As you can, the fine-tuned model starts to generate responses that are more specific to the domain of fine-tuning data which is relating to SEC report of Amazon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d99e2",
   "metadata": {},
   "source": [
    "### 2.6. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b60c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "pretrained_predictor.delete_model()\n",
    "pretrained_predictor.delete_endpoint()\n",
    "domain_adaptation_predictor.delete_model()\n",
    "domain_adaptation_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
