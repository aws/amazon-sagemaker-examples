{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52467706",
   "metadata": {},
   "source": [
    "# SageMaker Inference Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5816806",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "SageMaker Inference Recommender is a new capability of SageMaker that reduces the time required to get machine learning (ML) models in production by automating performance benchmarking and load testing models across SageMaker ML instances. You can use Inference Recommender to deploy your model to a real-time inference endpoint that delivers the best performance at the lowest cost. \n",
    "\n",
    "Get started with Inference Recommender on SageMaker in minutes while selecting an instance and get an optimized endpoint configuration in hours, eliminating weeks of manual testing and tuning time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fc6da",
   "metadata": {},
   "source": [
    "## 2. Setup \n",
    "\n",
    "Note that we are using the `conda_pytorch_p36` kernel in SageMaker Notebook Instances. This is running Python 3.6 and PyTorch 1.4.0. If you'd like to use the same setup, in the AWS Management Console, go to the Amazon SageMaker console. Choose Notebook Instances, and click create a new notebook instance. Upload the current notebook and set the kernel. You can also run this in SageMaker Studio Notebooks with the `PyTorch Python 3.6 CPU Optimized` kernel.\n",
    "\n",
    "In the next steps, you'll import standard methods and libraries as well as set variables that will be used in this notebook. The `get_execution_role` function retrieves the AWS Identity and Access Management (IAM) role you created at the time of creating your notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ffa395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ce3f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-2\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sagemaker_session = Session()\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9311ed03",
   "metadata": {
    "id": "Nb1UImMZ-uxc"
   },
   "source": [
    "## 4. Create a model archive\n",
    "\n",
    "SageMaker models need to be packaged in `.tar.gz` files. When your SageMaker Endpoint is provisioned, the files in the archive will be extracted and put in `/opt/ml/model/` on the Endpoint. \n",
    "\n",
    "In this step, there are two optional tasks to:\n",
    "\n",
    "   (1) Download a pretrained model from PyTorch Hub pytorch/vision\n",
    "   \n",
    "   (2) Write your inference script (inference.py)\n",
    "   \n",
    "These tasks are provided as a sample reference but can and should be modified when using your own trained models with Inference Recommender. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683cb4f4",
   "metadata": {},
   "source": [
    "### Optional: Download model from PyTorch Hub\n",
    "\n",
    "Let's download the model from PyTorch Hub. By setting the variable download_the_model=False, you can skip the download and provide your own model archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd31b982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config option `kernel_spec_manager_class` not recognized by `EnableNBExtensionApp`.\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --quiet ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d284ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44f7ea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/archive/v0.8.2.zip\" to /home/ec2-user/.cache/torch/hub/v0.8.2.zip\n"
     ]
    }
   ],
   "source": [
    "download_the_model = True\n",
    "\n",
    "if download_the_model:\n",
    "    import torch, tarfile\n",
    "\n",
    "    framework_version = torch.__version__\n",
    "    print(framework_version)\n",
    "    model_name = \"resnet18\"\n",
    "    # Load the model, note you have to change the torchvision library to match your pytorch version\n",
    "    model = torch.hub.load(\"pytorch/vision:v0.8.2\", model_name, pretrained=True)\n",
    "    inp = torch.rand(1, 3, 256, 256)\n",
    "    model_trace = torch.jit.trace(model, inp)\n",
    "    # Save your model\n",
    "    model_trace.save(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bacc6fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 256, 256]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b7ea23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "COMPILED_FILE = \"compiled.pt\"\n",
    "UNCOMPILED_FILE = \"model.pth\"\n",
    "\n",
    "\n",
    "def predict_fn(image, model):\n",
    "    return model(image)\n",
    "\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    iobytes = io.BytesIO(request_body)\n",
    "    img = Image.open(iobytes)\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    input_tensor = preprocess(img)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n",
    "    return input_batch.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3c0cf",
   "metadata": {},
   "source": [
    "### Create a model archive\n",
    "\n",
    "To bring your own PyTorch model, SageMaker expects a single archive file in .tar.gz format, containing a model file (\\*.pth) and the script (\\*.py) for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea3f7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_archive_name = \"pyt-resnet18.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46aee6ae",
   "metadata": {
    "id": "HVcbhBde-uxc"
   },
   "outputs": [],
   "source": [
    "!tar -czf {model_archive_name} model.pth inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0746d1",
   "metadata": {
    "id": "z9kutpTP-uxd"
   },
   "source": [
    "### Upload to S3\n",
    "\n",
    "We now have a model archive ready. We need to upload it to S3 before we can use with Inference Recommender. We'll use the SageMaker Python SDK to handle the upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8816e91b",
   "metadata": {
    "id": "TocwZSw4-uxd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model uploaded to: s3://sagemaker-us-east-2-156991241640/model/pyt-resnet18.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# model package tarball (model artifact + inference code)\n",
    "model_url = sagemaker_session.upload_data(path=model_archive_name, key_prefix=\"model\")\n",
    "print(\"model uploaded to: {}\".format(model_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f606b5c",
   "metadata": {},
   "source": [
    "## 5. Create a sample payload archive\n",
    "\n",
    "We need to create an archive that contains individual files that Inference Recommender can send to your Endpoint. Inference Recommender will randomly sample files from this archive so make sure it contains a similar distribution of payloads you'd expect in production. Note that your inference code must be able to read in the file formats from the sample payload.\n",
    "\n",
    "*Here we are only adding four images for the example. For your own use case(s), it's recommended to add a variety of samples that is representative of your payloads.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dc20ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_archive_name = \"payload.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c2285",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional: download sample images\n",
    "SAMPLES_BUCKET = \"sagemaker-sample-files\"\n",
    "PREFIX = \"datasets/image/pets/\"\n",
    "payload_location = \"./sample-payload/\"\n",
    "\n",
    "if not os.path.exists(payload_location):\n",
    "    os.makedirs(payload_location)\n",
    "    print(\"Directory \", payload_location, \" Created \")\n",
    "else:\n",
    "    print(\"Directory \", payload_location, \" already exists\")\n",
    "\n",
    "sagemaker_session.download_data(payload_location, SAMPLES_BUCKET, PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33149e",
   "metadata": {},
   "source": [
    "### Tar the payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c55d634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxer_dog.jpg\n",
      "british_blue_shorthair_cat.jpg\n",
      "english_cocker_spaniel_dog.jpg\n",
      "shiba_inu_dog.jpg\n"
     ]
    }
   ],
   "source": [
    "!cd ./sample-payload/ && tar czvf ../payload.tar.gz *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e442ea9",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "\n",
    "Next, we'll upload the packaged payload examples (payload.tar.gz) that was created above to S3.  The S3 location will be used as input to our Inference Recommender job later in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a158e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_payload_url = sagemaker_session.upload_data(path=payload_archive_name, key_prefix=\"payload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148a4d1",
   "metadata": {},
   "source": [
    "## 6. Register model in Model Registry\n",
    "\n",
    "In order to use Inference Recommender, you must have a versioned model in SageMaker Model Registry.  To register a model in the Model Registry, you must have a model artifact packaged in a tarball and an inference container image.  Registering a model includes the following steps:\n",
    "\n",
    "\n",
    "1) **Create Model Group:** This is a one-time task per machine learning use case. A Model Group contains one or more versions of your packaged model. \n",
    "\n",
    "2) **Register Model Version/Package:** This task is performed for each new packaged model version. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4317b",
   "metadata": {},
   "source": [
    "### Machine learning model details\n",
    "\n",
    "Inference Recommender uses metadata about your ML model to recommend the best instance types and endpoint configurations for deployment. You can provide as much or as little information as you'd like but the more information you provide, the better your recommendations will be.\n",
    "\n",
    "ML Frameworks: `TENSORFLOW, PYTORCH, XGBOOST, SAGEMAKER-SCIKIT-LEARN`\n",
    "\n",
    "ML Domains: `COMPUTER_VISION, NATURAL_LANGUAGE_PROCESSING, MACHINE_LEARNING`\n",
    "\n",
    "Example ML Tasks: `CLASSIFICATION, REGRESSION, IMAGE_CLASSIFICATION, OBJECT_DETECTION, SEGMENTATION, FILL_MASK, TEXT_CLASSIFICATION, TEXT_GENERATION, OTHER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "624fdb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyT Version 1.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ML framework details\n",
    "framework = \"pytorch\"\n",
    "\n",
    "# Note that only the framework major and minor version is supported for Neo compilation\n",
    "framework_version = \".\".join(torch.__version__.split(\".\")[:-1])\n",
    "\n",
    "# model name as standardized by model zoos or a similar open source model\n",
    "model_name = \"resnet50\"\n",
    "\n",
    "# ML model details\n",
    "ml_domain = \"COMPUTER_VISION\"\n",
    "ml_task = \"IMAGE_CLASSIFICATION\"\n",
    "\n",
    "print(\"PyT Version\", framework_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab48a41",
   "metadata": {},
   "source": [
    "### Container image URL\n",
    "\n",
    "If you don’t have an inference container image, you can use one of the open source AWS [Deep Learning Containers (DLCs)](https://github.com/aws/deep-learning-containers) provided by AWS to serve your ML model. The code below retrieves a DLC based on your ML framework, framework version, python version, and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7b1aeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-2.amazonaws.com/pytorch-inference:1.7-cpu-py3'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_type = \"ml.c5.xlarge\"  # Note: you can use any CPU-based instance type here, this is just to get a CPU tagged image\n",
    "dlc_uri = image_uris.retrieve(\n",
    "    framework,\n",
    "    region,\n",
    "    version=framework_version,\n",
    "    py_version=\"py3\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "dlc_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628651e3",
   "metadata": {},
   "source": [
    "### Create Model Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2d40f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelPackageGroup Arn : arn:aws:sagemaker:us-east-2:156991241640:model-package-group/pytorch-cpu-models-1646848140\n"
     ]
    }
   ],
   "source": [
    "model_package_group_name = \"{}-cpu-models-\".format(framework) + str(round(time.time()))\n",
    "model_package_group_description = \"{} models\".format(ml_task.lower())\n",
    "\n",
    "model_package_group_input_dict = {\n",
    "    \"ModelPackageGroupName\": model_package_group_name,\n",
    "    \"ModelPackageGroupDescription\": model_package_group_description,\n",
    "}\n",
    "\n",
    "create_model_package_group_response = sm_client.create_model_package_group(\n",
    "    **model_package_group_input_dict\n",
    ")\n",
    "print(\n",
    "    \"ModelPackageGroup Arn : {}\".format(create_model_package_group_response[\"ModelPackageGroupArn\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8009aa5e",
   "metadata": {},
   "source": [
    "### Register Model Version/Package\n",
    "\n",
    "In this step, you'll register your pretrained model that was packaged in the prior steps as a new version in SageMaker Model Registry.  First, you'll configure the model package/version identifying which model package group this new model should be registered within as well as identify the initial approval status. You'll also identify the domain and task for your model.  These values were set earlier in the notebook \n",
    "where `ml_domain = 'COMPUTER_VISION'` and `ml_task = 'IMAGE_CLASSIFICATION'`\n",
    "\n",
    "*Note: ModelApprovalStatus is a configuration parameter that can be used in conjunction with SageMaker Projects to trigger automated deployment pipeline.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9466ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_description = \"{} {} inference recommender\".format(framework, model_name)\n",
    "\n",
    "model_approval_status = \"PendingManualApproval\"\n",
    "\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageGroupName\": model_package_group_name,\n",
    "    \"Domain\": ml_domain.upper(),\n",
    "    \"Task\": ml_task.upper(),\n",
    "    \"SamplePayloadUrl\": sample_payload_url,\n",
    "    \"ModelPackageDescription\": model_package_description,\n",
    "    \"ModelApprovalStatus\": model_approval_status,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68f6c7",
   "metadata": {},
   "source": [
    "### Set up inference specification\n",
    "\n",
    "You'll now setup the inference specification configuration for your model version.  This contains information on how the model should be hosted.\n",
    "\n",
    "Inference Recommender expects a single input MIME type for sending requests. Learn more about [common inference data formats on SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html). This MIME type will be sent in the Content-Type header when invoking your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cdebfe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mime_types = [\"application/x-image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c32407",
   "metadata": {},
   "source": [
    "If you specify a set of instance types below (i.e. non-empty list), then Inference Recommender will only support recommendations within the set of instances below. For this example, we provide a list of common instance types used for image classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ebe9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_realtime_inference_types = [\"ml.c5.xlarge\", \"ml.m5.large\", \"ml.inf1.xlarge\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb8846",
   "metadata": {},
   "source": [
    "### Optional: Model optimization\n",
    "\n",
    "[Amazon SageMaker Neo](https://aws.amazon.com/sagemaker/neo) is a capability of SageMaker that automatically optimizes your ML models for any target instance type. With Neo, you don’t need to set up third-party or framework-specific compiler software, or tune the model manually for optimizing inference performance. \n",
    "\n",
    "Inference Recommender compiles your model using SageMaker Neo if the `ModelInput` field is provided. To prepare the inputs for model compilation, specify the input shape for your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b54a17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_configuration = \"[[1,3,256,256]]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbd8a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpackage_inference_specification = {\n",
    "    \"InferenceSpecification\": {\n",
    "        \"Containers\": [\n",
    "            {\n",
    "                \"Image\": dlc_uri,\n",
    "                \"Framework\": framework.upper(),\n",
    "                \"FrameworkVersion\": framework_version,\n",
    "                \"NearestModelName\": model_name,\n",
    "                \"ModelInput\": {\"DataInputConfig\": data_input_configuration},\n",
    "            }\n",
    "        ],\n",
    "        \"SupportedContentTypes\": input_mime_types,  # required, must be non-null\n",
    "        \"SupportedResponseMIMETypes\": [],\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": supported_realtime_inference_types,  # optional\n",
    "    }\n",
    "}\n",
    "\n",
    "# Specify the model data\n",
    "modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\n",
    "    \"ModelDataUrl\"\n",
    "] = model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c3faaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_package_input_dict.update(modelpackage_inference_specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "752455f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelPackage Version ARN : arn:aws:sagemaker:us-east-2:156991241640:model-package/pytorch-cpu-models-1646848140/1\n"
     ]
    }
   ],
   "source": [
    "create_mode_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "model_package_arn = create_mode_package_response[\"ModelPackageArn\"]\n",
    "print(\"ModelPackage Version ARN : {}\".format(model_package_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b230739d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageGroupName': 'pytorch-cpu-models-1646848140',\n",
       " 'ModelPackageVersion': 1,\n",
       " 'ModelPackageArn': 'arn:aws:sagemaker:us-east-2:156991241640:model-package/pytorch-cpu-models-1646848140/1',\n",
       " 'ModelPackageDescription': 'pytorch resnet50 inference recommender',\n",
       " 'CreationTime': datetime.datetime(2022, 3, 9, 17, 49, 13, 220000, tzinfo=tzlocal()),\n",
       " 'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-2.amazonaws.com/pytorch-inference:1.7-cpu-py3',\n",
       "    'ImageDigest': 'sha256:1b6b7276ef97a34269479d73c180775b1fedd31bedaa083d406d7cce9ae633c4',\n",
       "    'ModelDataUrl': 's3://sagemaker-us-east-2-156991241640/model/pyt-resnet18.tar.gz',\n",
       "    'ModelInput': {'DataInputConfig': '[[1,3,256,256]]'},\n",
       "    'Framework': 'PYTORCH',\n",
       "    'FrameworkVersion': '1.7',\n",
       "    'NearestModelName': 'resnet50'}],\n",
       "  'SupportedRealtimeInferenceInstanceTypes': ['ml.c5.xlarge',\n",
       "   'ml.m5.large',\n",
       "   'ml.inf1.xlarge'],\n",
       "  'SupportedContentTypes': ['application/x-image'],\n",
       "  'SupportedResponseMIMETypes': []},\n",
       " 'ModelPackageStatus': 'Completed',\n",
       " 'ModelPackageStatusDetails': {'ValidationStatuses': [],\n",
       "  'ImageScanStatuses': []},\n",
       " 'CertifyForMarketplace': False,\n",
       " 'ModelApprovalStatus': 'PendingManualApproval',\n",
       " 'Domain': 'COMPUTER_VISION',\n",
       " 'Task': 'IMAGE_CLASSIFICATION',\n",
       " 'SamplePayloadUrl': 's3://sagemaker-us-east-2-156991241640/payload/payload.tar.gz',\n",
       " 'ResponseMetadata': {'RequestId': '7bf7ccf8-b7bc-403b-8b0e-8c64e79d0465',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7bf7ccf8-b7bc-403b-8b0e-8c64e79d0465',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1207',\n",
       "   'date': 'Wed, 09 Mar 2022 17:49:14 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.describe_model_package(ModelPackageName=model_package_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b9532",
   "metadata": {},
   "source": [
    "## 5: Create a SageMaker Inference Recommender Default Job\n",
    "\n",
    "Now with your model in Model Registry, you can kick off a 'Default' job to get instance recommendations. This only requires your `ModelPackageVersionArn` and comes back with recommendations within an hour. \n",
    "\n",
    "The output is a list of instance type recommendations with associated environment variables, cost, throughput and latency metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4863ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobArn': 'arn:aws:sagemaker:us-east-2:156991241640:inference-recommendations-job/3f2397c4-9fd1-11ec-bfd5-0ac52cb5fde6', 'ResponseMetadata': {'RequestId': '31338c03-0742-45c1-a3ea-2088311cc9a5', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '31338c03-0742-45c1-a3ea-2088311cc9a5', 'content-type': 'application/x-amz-json-1.1', 'content-length': '120', 'date': 'Wed, 09 Mar 2022 17:49:19 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "client = boto3.client(\"sagemaker\", region)\n",
    "\n",
    "role = get_execution_role()\n",
    "default_job = uuid.uuid1()\n",
    "default_response = client.create_inference_recommendations_job(\n",
    "    JobName=str(default_job),\n",
    "    JobDescription=\"Job Description\",\n",
    "    JobType=\"Default\",\n",
    "    RoleArn=role,\n",
    "    InputConfig={\"ModelPackageVersionArn\": model_package_arn},\n",
    ")\n",
    "\n",
    "print(default_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc75685",
   "metadata": {},
   "source": [
    "## 8. Instance Recommendation Results\n",
    "\n",
    "Each inference recommendation includes `InstanceType`, `InitialInstanceCount`, `EnvironmentParameters` which are tuned environment variable parameters for better performance. We also include performance and cost metrics such as `MaxInvocations`, `ModelLatency`, `CostPerHour` and `CostPerInference`. We believe these metrics will help you narrow down to a specific endpoint configuration that suits your use case. \n",
    "\n",
    "Example:   \n",
    "\n",
    "If your motivation is overall price-performance with an emphasis on throughput, then you should focus on `CostPerInference` metrics  \n",
    "If your motivation is a balance between latency and throughput, then you should focus on `ModelLatency` / `MaxInvocations` metrics\n",
    "\n",
    "| Metric | Description |\n",
    "| --- | --- |\n",
    "| ModelLatency | The interval of time taken by a model to respond as viewed from SageMaker. This interval includes the local communication times taken to send the request and to fetch the response from the container of a model and the time taken to complete the inference in the container. <br /> Units: Milliseconds |\n",
    "| MaximumInvocations | The maximum number of InvokeEndpoint requests sent to an endpoint per minute. <br /> Units: None |\n",
    "| CostPerHour | The estimated cost per hour for your real-time endpoint. <br /> Units: US Dollars |\n",
    "| CostPerInference | The estimated cost per inference for your real-time endpoint. <br /> Units: US Dollars |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d36fb13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In progress\n",
      "In progress\n",
      "In progress\n",
      "In progress\n",
      "In progress\n",
      "Inference recommender job completed\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "finished = False\n",
    "while not finished:\n",
    "    inference_recommender_job = sm_client.describe_inference_recommendations_job(\n",
    "        JobName=str(default_job)\n",
    "    )\n",
    "    if inference_recommender_job[\"Status\"] in [\"COMPLETED\", \"STOPPED\", \"FAILED\"]:\n",
    "        finished = True\n",
    "    else:\n",
    "        print(\"In progress\")\n",
    "        time.sleep(300)\n",
    "\n",
    "if inference_recommender_job[\"Status\"] == \"FAILED\":\n",
    "    print(\"Inference recommender job failed \")\n",
    "    print(\"Failed Reason: {}\".format(inference_recommender_job[\"FailureReason\"]))\n",
    "else:\n",
    "    print(\"Inference recommender job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae5834",
   "metadata": {},
   "source": [
    "### Detailing out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e39feb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>InstanceType</th>\n",
       "      <th>InitialInstanceCount</th>\n",
       "      <th>EnvironmentParameters</th>\n",
       "      <th>CostPerHour</th>\n",
       "      <th>CostPerInference</th>\n",
       "      <th>MaxInvocations</th>\n",
       "      <th>ModelLatency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sm-epc-179e9f77-274b-428b-80da-c8ecd1941749</td>\n",
       "      <td>ml.m5.large</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>547</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sm-epc-d0c77fdf-c4f9-4693-b219-cb3f9fb3f225</td>\n",
       "      <td>ml.inf1.xlarge</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>801</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sm-epc-1cdd499b-304e-4e7d-80f6-6710be5d7b57</td>\n",
       "      <td>ml.c5.xlarge</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>758</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  EndpointName    InstanceType  \\\n",
       "0  sm-epc-179e9f77-274b-428b-80da-c8ecd1941749     ml.m5.large   \n",
       "1  sm-epc-d0c77fdf-c4f9-4693-b219-cb3f9fb3f225  ml.inf1.xlarge   \n",
       "2  sm-epc-1cdd499b-304e-4e7d-80f6-6710be5d7b57    ml.c5.xlarge   \n",
       "\n",
       "   InitialInstanceCount EnvironmentParameters  CostPerHour  CostPerInference  \\\n",
       "0                     1                    []        0.115          0.000004   \n",
       "1                     1                    []        0.297          0.000006   \n",
       "2                     1                    []        0.204          0.000004   \n",
       "\n",
       "   MaxInvocations  ModelLatency  \n",
       "0             547           423  \n",
       "1             801           197  \n",
       "2             758           628  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {**x[\"EndpointConfiguration\"], **x[\"ModelConfiguration\"], **x[\"Metrics\"]}\n",
    "    for x in inference_recommender_job[\"InferenceRecommendations\"]\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(\"VariantName\", inplace=True, axis=1)\n",
    "pd.set_option(\"max_colwidth\", 400)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf74b0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JobName': '175a92e4-9f39-11ec-bfd5-0ac52cb5fde6',\n",
       " 'JobDescription': 'Job Description',\n",
       " 'JobType': 'Default',\n",
       " 'JobArn': 'arn:aws:sagemaker:us-east-2:156991241640:inference-recommendations-job/175a92e4-9f39-11ec-bfd5-0ac52cb5fde6',\n",
       " 'RoleArn': 'arn:aws:iam::156991241640:role/service-role/AmazonSageMaker-ExecutionRole-20210302T095973',\n",
       " 'Status': 'COMPLETED',\n",
       " 'CreationTime': datetime.datetime(2022, 3, 8, 23, 40, 9, 593000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 3, 9, 0, 0, 48, 675000, tzinfo=tzlocal()),\n",
       " 'InputConfig': {'ModelPackageVersionArn': 'arn:aws:sagemaker:us-east-2:156991241640:model-package/pytorch-cpu-models-1646781973/1'},\n",
       " 'InferenceRecommendations': [{'Metrics': {'CostPerHour': 0.11500000208616257,\n",
       "    'CostPerInference': 3.582554427339346e-06,\n",
       "    'MaxInvocations': 535,\n",
       "    'ModelLatency': 1070},\n",
       "   'EndpointConfiguration': {'EndpointName': 'sm-epc-61727a21-9c56-4b3a-ba8e-eb4e96ec667b',\n",
       "    'VariantName': 'sm-epc-61727a21-9c56-4b3a-ba8e-eb4e96ec667b',\n",
       "    'InstanceType': 'ml.m5.large',\n",
       "    'InitialInstanceCount': 1},\n",
       "   'ModelConfiguration': {'EnvironmentParameters': []}},\n",
       "  {'Metrics': {'CostPerHour': 0.20399999618530273,\n",
       "    'CostPerInference': 4.298356543586124e-06,\n",
       "    'MaxInvocations': 791,\n",
       "    'ModelLatency': 459},\n",
       "   'EndpointConfiguration': {'EndpointName': 'sm-epc-af216969-4028-4f0a-a571-1618ad2f441c',\n",
       "    'VariantName': 'sm-epc-af216969-4028-4f0a-a571-1618ad2f441c',\n",
       "    'InstanceType': 'ml.c5.xlarge',\n",
       "    'InitialInstanceCount': 1},\n",
       "   'ModelConfiguration': {'EnvironmentParameters': []}},\n",
       "  {'Metrics': {'CostPerHour': 0.296999990940094,\n",
       "    'CostPerInference': 6.1797754824510776e-06,\n",
       "    'MaxInvocations': 801,\n",
       "    'ModelLatency': 213},\n",
       "   'EndpointConfiguration': {'EndpointName': 'sm-epc-2c94d1cc-1b7e-4f75-90c7-70acfae74517',\n",
       "    'VariantName': 'sm-epc-2c94d1cc-1b7e-4f75-90c7-70acfae74517',\n",
       "    'InstanceType': 'ml.inf1.xlarge',\n",
       "    'InitialInstanceCount': 1},\n",
       "   'ModelConfiguration': {'EnvironmentParameters': []}}],\n",
       " 'ResponseMetadata': {'RequestId': '69fd887e-d5be-4b34-85de-22fea6913216',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '69fd887e-d5be-4b34-85de-22fea6913216',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1715',\n",
       "   'date': 'Wed, 09 Mar 2022 00:05:14 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_recommender_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98e32c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CostPerHour</th>\n",
       "      <th>CostPerInference</th>\n",
       "      <th>MaximumInvocations</th>\n",
       "      <th>ModelLatency</th>\n",
       "      <th>EndpointName</th>\n",
       "      <th>InstanceType</th>\n",
       "      <th>InitialInstanceCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.204</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>648</td>\n",
       "      <td>263596</td>\n",
       "      <td>sm-epc-fd60edf3-4ca5-4095-8e79-36d23a6a86d6</td>\n",
       "      <td>ml.c5.xlarge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>655</td>\n",
       "      <td>826019</td>\n",
       "      <td>sm-epc-745873f4-f0b2-44be-a82a-c987365eefd9</td>\n",
       "      <td>ml.c5d.large</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>570</td>\n",
       "      <td>1085446</td>\n",
       "      <td>sm-epc-cd4c1a2a-af80-4eae-a125-043d82eec7b2</td>\n",
       "      <td>ml.m5.large</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CostPerHour  CostPerInference  MaximumInvocations  ModelLatency  \\\n",
       "0        0.204          0.000005                 648        263596   \n",
       "1        0.115          0.000003                 655        826019   \n",
       "2        0.115          0.000003                 570       1085446   \n",
       "\n",
       "                                  EndpointName  InstanceType  \\\n",
       "0  sm-epc-fd60edf3-4ca5-4095-8e79-36d23a6a86d6  ml.c5.xlarge   \n",
       "1  sm-epc-745873f4-f0b2-44be-a82a-c987365eefd9  ml.c5d.large   \n",
       "2  sm-epc-cd4c1a2a-af80-4eae-a125-043d82eec7b2   ml.m5.large   \n",
       "\n",
       "   InitialInstanceCount  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                     1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [{**x[\"Metrics\"], **x[\"EndpointConfiguration\"]} for x in rv[\"InferenceRecommendations\"]]\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(\"VariantName\", inplace=True, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563875e9",
   "metadata": {},
   "source": [
    "## Step 6: Custom Load Test\n",
    "\n",
    "With an 'Advanced' Inference Recommender job, you can provide your production requirements, select instance types, tune environment variables and perform more extensive load tests. This typically takes 2-3 hours depending on your traffic pattern and number of instance types. \n",
    "\n",
    "The output is a list of endpoint configuration recommendations (instance type, instance count, environment variables) with associated cost, throughput and latency metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca46f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.c5d.xlarge\"\n",
    "advanced_job_name = (\n",
    "    model_name + \"-\" + instance_type.replace(\".\", \"-\") + \"-load-test-\" + str(round(time.time()))\n",
    ")\n",
    "job_description = \"PyT {} on {}\".format(model_name, instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a98b970a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobArn': 'arn:aws:sagemaker:us-east-2:156991241640:inference-recommendations-job/resnet50-ml-c5d-xlarge-load-test-1646853966', 'ResponseMetadata': {'RequestId': '239c5dcd-577d-4355-96b1-569f84ddf496', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '239c5dcd-577d-4355-96b1-569f84ddf496', 'content-type': 'application/x-amz-json-1.1', 'content-length': '127', 'date': 'Wed, 09 Mar 2022 19:26:24 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "rv = client.create_inference_recommendations_job(\n",
    "    JobName=advanced_job_name,\n",
    "    JobDescription=job_description,\n",
    "    JobType=\"Advanced\",\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        \"ModelPackageVersionArn\": model_package_arn,\n",
    "        \"JobDurationInSeconds\": 7200,\n",
    "        \"EndpointConfigurations\": [\n",
    "            {\n",
    "                \"InstanceType\": instance_type,\n",
    "                \"EnvironmentParameterRanges\": {\n",
    "                    \"CategoricalParameterRanges\": [{\"Name\": \"OMP_NUM_THREADS\", \"Value\": [\"2\", \"4\"]}]\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"ResourceLimit\": {\"MaxNumberOfTests\": 2, \"MaxParallelOfTests\": 1},\n",
    "        \"TrafficPattern\": {\n",
    "            \"TrafficType\": \"PHASES\",\n",
    "            \"Phases\": [{\"InitialNumberOfUsers\": 1, \"SpawnRate\": 1, \"DurationInSeconds\": 120}],\n",
    "        },\n",
    "    },\n",
    "    StoppingConditions={\n",
    "        \"MaxInvocations\": 300,\n",
    "        \"ModelLatencyThresholds\": [{\"Percentile\": \"P95\", \"ValueInMilliseconds\": 100}],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eec1b2",
   "metadata": {},
   "source": [
    "### 8. Custom Load Test Results\n",
    "\n",
    "Inference Recommender does benchmarking on both the endpoint configurations and here is the result. \n",
    "\n",
    "Analyzing load test result,    \n",
    "`OMP_NUM_THREADS` = 2 shows ~20% better throughput when compared to `OMP_NUM_THREADS` = 4   \n",
    "`OMP_NUM_THREADS` = 2 shows 25% saving in inference-cost when compared to `OMP_NUM_THREADS` = 4   \n",
    "\n",
    "In all front, `OMP_NUM_THREADS` = 2  is much better endpoint configuration than `OMP_NUM_THREADS` = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d045fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In progress\n",
      "In progress\n"
     ]
    }
   ],
   "source": [
    "finished = False\n",
    "while not finished:\n",
    "    inference_recommender_job = sm_client.describe_inference_recommendations_job(\n",
    "        JobName=advanced_job_name\n",
    "    )\n",
    "    if inference_recommender_job[\"Status\"] in [\"COMPLETED\", \"STOPPED\", \"FAILED\"]:\n",
    "        finished = True\n",
    "    else:\n",
    "        print(\"In progress\")\n",
    "        time.sleep(300)\n",
    "\n",
    "if inference_recommender_job[\"Status\"] == \"FAILED\":\n",
    "    print(\"Inference recommender job failed \")\n",
    "    print(\"Failed Reason: {}\".format(inference_recommender_job[\"FailureReason\"]))\n",
    "else:\n",
    "    print(\"Inference recommender job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a820fbc",
   "metadata": {},
   "source": [
    "### Detailing out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {**x[\"EndpointConfiguration\"], **x[\"ModelConfiguration\"], **x[\"Metrics\"]}\n",
    "    for x in inference_recommender_job[\"InferenceRecommendations\"]\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df.drop(\"VariantName\", inplace=True, axis=1)\n",
    "pd.set_option(\"max_colwidth\", 400)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
