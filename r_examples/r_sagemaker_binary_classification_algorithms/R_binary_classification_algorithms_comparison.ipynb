{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b414f2e8",
   "metadata": {},
   "source": [
    "## Compare built-in Sagemaker classification algorithms for a binary classification problem using Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e333bd",
   "metadata": {},
   "source": [
    "In the notebook tutorial, I try to show how to build 3 classification models using HPO and then compare the AUC using the different methods and also later on real-time deploy the best model among them.\n",
    "\n",
    "IRIS is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. The dataset is built-in by default into R or can also be downloaded from https://archive.ics.uci.edu/ml/datasets/iris\n",
    "\n",
    "The iris dataset, besides its historical importance, is also a fun dataset to play with since it can educate us about various ML techniques such as clustering, classification and regression, all in one dataset.\n",
    "\n",
    "The dataset is built into any base R installation, so no download is required.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "5. Species: Iris Setosa, Iris Versicolour, Iris Virginica\n",
    "\n",
    "The prediction I will perform is Species ~ f(sepal.length,sepal.width,petal.width,petal.lenght)\n",
    "\n",
    "Predicted attribute: Species of iris plant.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62146b8a",
   "metadata": {},
   "source": [
    "### Load required libraries and initialize variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5a01fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Your system is mis-configured: ‘/etc/localtime’ is not a symlink”\n",
      "Warning message:\n",
      "“It is strongly recommended to set envionment variable TZ to ‘Etc/UCT’ (or equivalent)”\n",
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.5     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.3     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.7\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.1.3     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.4.0     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.1\n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "\n",
      "Attaching package: ‘pROC’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rm(list=ls())\n",
    "library(reticulate) # be careful not to install reticulate again. since it can cause problems.\n",
    "library(tidyverse)\n",
    "library(pROC)\n",
    "set.seed(1324)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8fb8f1",
   "metadata": {},
   "source": [
    "Sagemaker needs to be imported using the reticulate library. If this was performed in a local computer, we would have to make sure that Python and appropiate sagemaker libraries are installed, but inside a sagemaker notebook R kernels, these are all pre-loaded and the R user does not have to worry about installing reticulate or Python. \n",
    "\n",
    "Session is the unique session ID associated with each sagemaker call. It remains the same throughout the execution of the program and can be recalled later to close a session or open a new session.\n",
    "\n",
    "The bucket is the S3 bucket where we will be storing our data output. The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "\n",
    "The role is the role of the sagemaker notebook as when it was initially deployed. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89da5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker <- import('sagemaker')\n",
    "session <- sagemaker$Session()\n",
    "bucket <- \"r-sagemaker-examples\"\n",
    "role_arn <- sagemaker$get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5a147",
   "metadata": {},
   "source": [
    "### Input the data and basic pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c38453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 5\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & setosa\\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & setosa\\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & setosa\\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & setosa\\\\\n",
       "\t5 & 5.0 & 3.6 & 1.4 & 0.2 & setosa\\\\\n",
       "\t6 & 5.4 & 3.9 & 1.7 & 0.4 & setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 5\n",
       "\n",
       "| <!--/--> | Sepal.Length &lt;dbl&gt; | Sepal.Width &lt;dbl&gt; | Petal.Length &lt;dbl&gt; | Petal.Width &lt;dbl&gt; | Species &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 5.1 | 3.5 | 1.4 | 0.2 | setosa |\n",
       "| 2 | 4.9 | 3.0 | 1.4 | 0.2 | setosa |\n",
       "| 3 | 4.7 | 3.2 | 1.3 | 0.2 | setosa |\n",
       "| 4 | 4.6 | 3.1 | 1.5 | 0.2 | setosa |\n",
       "| 5 | 5.0 | 3.6 | 1.4 | 0.2 | setosa |\n",
       "| 6 | 5.4 | 3.9 | 1.7 | 0.4 | setosa |\n",
       "\n"
      ],
      "text/plain": [
       "  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n",
       "1 5.1          3.5         1.4          0.2         setosa \n",
       "2 4.9          3.0         1.4          0.2         setosa \n",
       "3 4.7          3.2         1.3          0.2         setosa \n",
       "4 4.6          3.1         1.5          0.2         setosa \n",
       "5 5.0          3.6         1.4          0.2         setosa \n",
       "6 5.4          3.9         1.7          0.4         setosa "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "997a95fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n",
       " Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n",
       " 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n",
       " Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n",
       " Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n",
       " 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n",
       " Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n",
       "       Species  \n",
       " setosa    :50  \n",
       " versicolor:50  \n",
       " virginica :50  \n",
       "                \n",
       "                \n",
       "                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298371d",
   "metadata": {},
   "source": [
    "In above, we see that there are 50 flowers of the setosa species, 50 flowers of the versicolor species, and 50 flowers of the virginica species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d8f82",
   "metadata": {},
   "source": [
    "In this case, the target variable is the Species prediction. We are trying to predict the species of the flower given its numerical measurements of Sepal length, sepal width, petal length, and petal width. Since we are trying to do binary classification, we will only take the flower species setosa and versicolor for simplicity. Also we will perform one-hot encoding on the categorical variable Species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b2dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris1 <- iris %>% \n",
    "    dplyr::select(Species,Sepal.Length,Sepal.Width,Petal.Length,Petal.Width) %>% # change order of columns such that the label column is the first column.\n",
    "    dplyr::filter(Species %in% c(\"setosa\",\"versicolor\")) %>%                     #only select two flower for binary classification.\n",
    "    dplyr::mutate(Species = as.numeric(Species) -1)                              # one-hot encoding,starting with 0 as setosa and 1 as versicolor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ae4200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Species</th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0</td><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>0</td><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0</td><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0</td><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0</td><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>0</td><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 5\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & Species & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 0 & 5.1 & 3.5 & 1.4 & 0.2\\\\\n",
       "\t2 & 0 & 4.9 & 3.0 & 1.4 & 0.2\\\\\n",
       "\t3 & 0 & 4.7 & 3.2 & 1.3 & 0.2\\\\\n",
       "\t4 & 0 & 4.6 & 3.1 & 1.5 & 0.2\\\\\n",
       "\t5 & 0 & 5.0 & 3.6 & 1.4 & 0.2\\\\\n",
       "\t6 & 0 & 5.4 & 3.9 & 1.7 & 0.4\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 5\n",
       "\n",
       "| <!--/--> | Species &lt;dbl&gt; | Sepal.Length &lt;dbl&gt; | Sepal.Width &lt;dbl&gt; | Petal.Length &lt;dbl&gt; | Petal.Width &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 0 | 5.1 | 3.5 | 1.4 | 0.2 |\n",
       "| 2 | 0 | 4.9 | 3.0 | 1.4 | 0.2 |\n",
       "| 3 | 0 | 4.7 | 3.2 | 1.3 | 0.2 |\n",
       "| 4 | 0 | 4.6 | 3.1 | 1.5 | 0.2 |\n",
       "| 5 | 0 | 5.0 | 3.6 | 1.4 | 0.2 |\n",
       "| 6 | 0 | 5.4 | 3.9 | 1.7 | 0.4 |\n",
       "\n"
      ],
      "text/plain": [
       "  Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n",
       "1 0       5.1          3.5         1.4          0.2        \n",
       "2 0       4.9          3.0         1.4          0.2        \n",
       "3 0       4.7          3.2         1.3          0.2        \n",
       "4 0       4.6          3.1         1.5          0.2        \n",
       "5 0       5.0          3.6         1.4          0.2        \n",
       "6 0       5.4          3.9         1.7          0.4        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(iris1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dcf948",
   "metadata": {},
   "source": [
    "We now obtain some basic desriptive statistics of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "216f89b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 2 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Species</th><th scope=col>mean_sepal_length</th><th scope=col>mean_petal_length</th><th scope=col>mean_sepal_width</th><th scope=col>mean_petal_width</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0</td><td>5.006</td><td>1.462</td><td>3.428</td><td>0.246</td></tr>\n",
       "\t<tr><td>1</td><td>5.936</td><td>4.260</td><td>2.770</td><td>1.326</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 2 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " Species & mean\\_sepal\\_length & mean\\_petal\\_length & mean\\_sepal\\_width & mean\\_petal\\_width\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0 & 5.006 & 1.462 & 3.428 & 0.246\\\\\n",
       "\t 1 & 5.936 & 4.260 & 2.770 & 1.326\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 2 × 5\n",
       "\n",
       "| Species &lt;dbl&gt; | mean_sepal_length &lt;dbl&gt; | mean_petal_length &lt;dbl&gt; | mean_sepal_width &lt;dbl&gt; | mean_petal_width &lt;dbl&gt; |\n",
       "|---|---|---|---|---|\n",
       "| 0 | 5.006 | 1.462 | 3.428 | 0.246 |\n",
       "| 1 | 5.936 | 4.260 | 2.770 | 1.326 |\n",
       "\n"
      ],
      "text/plain": [
       "  Species mean_sepal_length mean_petal_length mean_sepal_width mean_petal_width\n",
       "1 0       5.006             1.462             3.428            0.246           \n",
       "2 1       5.936             4.260             2.770            1.326           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris1 %>% group_by(Species) %>% summarize(mean_sepal_length = mean(Sepal.Length),\n",
    "                                         mean_petal_length = mean(Petal.Length),\n",
    "                                         mean_sepal_width = mean(Sepal.Width),\n",
    "                                         mean_petal_width = mean(Petal.Width),\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf43e8",
   "metadata": {},
   "source": [
    "In the summary statistics, we observe that mean sepal length is longer than mean petal length for both flowers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7dbd3",
   "metadata": {},
   "source": [
    "### Prepare for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c46d6b",
   "metadata": {},
   "source": [
    "##### We split the train and test and validate into 70%, 15%, and 15%, using random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0db01ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = c(\"Species\", \"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\")\n",
      "\n",
      "Joining, by = c(\"Species\", \"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\")\n",
      "\n",
      "Joining, by = c(\"Species\", \"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_train <- iris1 %>%\n",
    "                    sample_frac(size = 0.7)\n",
    "iris_test <- anti_join(iris1, iris_train) %>%  \n",
    "                  sample_frac(size = 0.5)\n",
    "iris_validate <- anti_join(iris1, iris_train) %>%\n",
    "                        anti_join(., iris_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa01eb3",
   "metadata": {},
   "source": [
    "##### We do a check of the summary statistics to make sure train, test, validate datasets are appropiately split and have proper class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af6ff5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  1 \n",
       "36 34 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "70"
      ],
      "text/latex": [
       "70"
      ],
      "text/markdown": [
       "70"
      ],
      "text/plain": [
       "[1] 70"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(iris_train$Species)\n",
    "nrow(iris_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f11f922",
   "metadata": {},
   "source": [
    "We see that the class balance between 0 and 1 is almost 50% each for the binary classification. We also see that there are 70 rows in the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "240b03fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "0 1 \n",
       "7 8 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "15"
      ],
      "text/latex": [
       "15"
      ],
      "text/markdown": [
       "15"
      ],
      "text/plain": [
       "[1] 15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(iris_validate$Species)\n",
    "nrow(iris_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585d4f7",
   "metadata": {},
   "source": [
    "We see that the class balance in validation dataset between 0 and 1 is almost 50% each for the binary classification. We also see that there are 15 rows in the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0248f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "0 1 \n",
       "7 8 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "15"
      ],
      "text/latex": [
       "15"
      ],
      "text/markdown": [
       "15"
      ],
      "text/plain": [
       "[1] 15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(iris_test$Species)\n",
    "nrow(iris_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3bc261",
   "metadata": {},
   "source": [
    "We see that the class balance in test dataset between 0 and 1 is almost 50% each for the binary classification. We also see that there are 15 rows in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ba116",
   "metadata": {},
   "source": [
    "### Write the data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c370951",
   "metadata": {},
   "source": [
    "Different algorithms in Sagemaker will have different data formats required for training and for testing. These formats are created to make model production easier. csv is the most well known of these formats and has been used here as input in all algorithms to make it consitent.\n",
    "\n",
    "Sagemaker algorithms take in data from an S3 object and output data to an S3 object, so data has to be stored in S3 as csv,json, proto-buf or any format that is supported by the algorithm that you are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a9268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(iris_train, 'iris_train.csv', col_names = FALSE)\n",
    "write_csv(iris_validate, 'iris_valid.csv', col_names = FALSE)\n",
    "# Remove target from test\n",
    "write_csv(iris_test, 'iris_test.csv', col_names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b344e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train <- session$upload_data(path = 'iris_train.csv', \n",
    "                                bucket = bucket, \n",
    "                                key_prefix = 'data')\n",
    "s3_valid <- session$upload_data(path = 'iris_valid.csv', \n",
    "                                bucket = bucket, \n",
    "                                key_prefix = 'data')\n",
    "\n",
    "s3_test <- session$upload_data(path = 'iris_test.csv', \n",
    "                                bucket = bucket, \n",
    "                                key_prefix = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac7b2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_input <- sagemaker$inputs$TrainingInput(s3_data = s3_train,\n",
    "                                     content_type = 'text/csv')\n",
    "s3_valid_input <- sagemaker$inputs$TrainingInput(s3_data = s3_valid,\n",
    "                                     content_type = 'text/csv')\n",
    "s3_test_input <- sagemaker$inputs$TrainingInput(s3_data = s3_test,\n",
    "                                     content_type = 'text/csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96f2fe",
   "metadata": {},
   "source": [
    "To perform Binary classification on Tabular\tdata, Sagemaker contains following algorithms:\n",
    "\n",
    "- XGBoost Algorithm\n",
    "- Linear Learner Algorithm, \n",
    "- K-Nearest Neighbors (k-NN) Algorithm, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516a4a51",
   "metadata": {},
   "source": [
    "## Create Model 1: an xgboost model in sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857ce26",
   "metadata": {},
   "source": [
    "Use the XGBoost built-in algorithm to build an XGBoost training container as shown in the following code example. You can automatically spot the XGBoost built-in algorithm image URI using the SageMaker image_uris.retrieve API (or the get_image_uri API if using Amazon SageMaker Python SDK version 1). If you want to ensure if the image_uris.retrieve API finds the correct URI, see Common parameters for built-in algorithms and look up xgboost from the full list of built-in algorithm image URIs and available regions.\n",
    "\n",
    "After specifying the XGBoost image URI, you can use the XGBoost container to construct an estimator using the SageMaker Estimator API and initiate a training job. This XGBoost built-in algorithm mode does not incorporate your own XGBoost training script and runs directly on the input datasets.\n",
    "\n",
    "See https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44f3193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Container Image URL:  825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest"
     ]
    }
   ],
   "source": [
    "container <- sagemaker$image_uris$retrieve(framework='xgboost', region= session$boto_region_name, version='latest')\n",
    "cat('XGBoost Container Image URL: ', container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87b58b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output <- paste0('s3://', bucket, '/output_xgboost')\n",
    "estimator1 <- sagemaker$estimator$Estimator(image_uri = container,\n",
    "                                           role = role_arn,\n",
    "                                           train_instance_count = 1L,\n",
    "                                           train_instance_type = 'ml.m5.4xlarge',\n",
    "                                           input_mode = 'File',\n",
    "                                           output_path = s3_output,\n",
    "                                           output_kms_key = NULL,\n",
    "                                           base_job_name = NULL,\n",
    "                                           sagemaker_session = NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa787b",
   "metadata": {},
   "source": [
    "How would an untuned model perform compared to a tuned model? Is it worth the effort? Before going deeper into XGBoost model tuning, let’s highlight the reasons why you have to tune your model. The main reason to perform hyper-parameter tuning is to increase predicatability of our models by choosing our hyperparameters in a well thought manner. There are 3 ways to perform hyperparameter tuning: grid search, random search, bayesian search. Popular packages like scikit-learn use grid search and random search techniques. Sagemaker uses Bayesian search techniques.\n",
    "\n",
    "We need to choose \n",
    "\n",
    "- a learning objective function to optimize during model training\n",
    "- an eval_metric to use to evaluate model performance during validation\n",
    "- a set of hyperparameters and a range of values for each to use when tuning the model automatically\n",
    "\n",
    "Sagemaker XGBoost model can be tuned with many hyperparameters. The hyperparameters that have the greatest effect on optimizing the XGBoost evaluation metrics are: \n",
    "\n",
    "- alpha, \n",
    "- min_child_weight, \n",
    "- subsample, \n",
    "- eta, \n",
    "- num_round.\n",
    "\n",
    "\n",
    "The hyperparameters that are required are num_class (the number of classes if it is a multi-class classification problem) and num_round ( the number of rounds to run the training on). All other hyperparameters are optional and will be set to default values if it is not specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ebc929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to make sure which are required and which are optional\n",
    "estimator1$set_hyperparameters(eval_metric='auc',\n",
    "                              objective='binary:logistic',\n",
    "                              num_round = 6L\n",
    "                              )\n",
    "\n",
    "# Set Hyperparameter Ranges, check to make sure which are integer and which are continuos parameters. \n",
    "hyperparameter_ranges = list('eta' = sagemaker$parameter$ContinuousParameter(0,1),\n",
    "                        'min_child_weight'= sagemaker$parameter$ContinuousParameter(0,10),\n",
    "                        'alpha'= sagemaker$parameter$ContinuousParameter(0,2),\n",
    "                        'max_depth'= sagemaker$parameter$IntegerParameter(0L,10L))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b150d8",
   "metadata": {},
   "source": [
    "The evaluation metric that we will use for our binary classification purpose is validation:auc, but you could use any other metric that is right for your problem. You do have to be careful to change your objective_type to point to the right direction of Maximize or Minimize according to the objective metric you have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95f95fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hyperparamter tuner\n",
    "objective_metric_name = 'validation:auc'\n",
    "tuner1 <- sagemaker$tuner$HyperparameterTuner(estimator1,\n",
    "                                             objective_metric_name,\n",
    "                                             hyperparameter_ranges,\n",
    "                                             objective_type='Maximize',\n",
    "                                             max_jobs=4L,\n",
    "                                             max_parallel_jobs=2L)\n",
    "\n",
    "# Define the data channels for train and validation datasets\n",
    "input_data <- list('train' = s3_train_input,\n",
    "                   'validation' = s3_valid_input)\n",
    "\n",
    "# train the tuner\n",
    "tuner1$fit(inputs = input_data, \n",
    "           job_name = paste('sagemaker-tune-xgboost', format(Sys.time(), '%H-%M-%S'), sep = '-'), \n",
    "           wait=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4c098",
   "metadata": {},
   "source": [
    "The output of the tuning job can be checked in sagemaker if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390be730",
   "metadata": {},
   "source": [
    "### Calculate AUC for the test data on model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985c059",
   "metadata": {},
   "source": [
    "Sagemaker will automatically recognize the training job with the best evaluation metric and load the hyperparameters associated with that training job when we deploy the model. One of the benefits of Sagemaker is that we can easily deploy models in a different instance than the instance in which the notebook is running. So we can deploy into a more powerful instance or a less powerful instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9227cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_endpoint1 <- tuner1$deploy(initial_instance_count = 1L,\n",
    "                                   instance_type = 'ml.t2.medium')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b22ef",
   "metadata": {},
   "source": [
    "The serializer tells sagemaker what format the model expects data to be input in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ade470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_endpoint1$serializer <- sagemaker$serializers$CSVSerializer(content_type='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a1215",
   "metadata": {},
   "source": [
    "We input the `iris_test` dataset without the labels into the model using the `predict` function and check it's AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b35b22e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 15 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>predicted_flower</th><th scope=col>Species</th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>1</td><td>5.7</td><td>2.6</td><td>3.5</td><td>1.0</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.3</td><td>2.5</td><td>4.9</td><td>1.5</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>5.0</td><td>3.5</td><td>1.6</td><td>0.6</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.1</td><td>3.0</td><td>4.6</td><td>1.4</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>5.8</td><td>2.6</td><td>4.0</td><td>1.2</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>4.6</td><td>3.6</td><td>1.0</td><td>0.2</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>5.1</td><td>3.8</td><td>1.9</td><td>0.4</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.6</td><td>2.9</td><td>4.6</td><td>1.3</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>5.6</td><td>3.0</td><td>4.1</td><td>1.3</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>5.9</td><td>3.2</td><td>4.8</td><td>1.8</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>4.8</td><td>3.0</td><td>1.4</td><td>0.1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.2</td><td>2.9</td><td>4.3</td><td>1.3</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>4.8</td><td>3.0</td><td>1.4</td><td>0.3</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 15 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " predicted\\_flower & Species & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1 & 1 & 5.7 & 2.6 & 3.5 & 1.0\\\\\n",
       "\t 1 & 1 & 6.3 & 2.5 & 4.9 & 1.5\\\\\n",
       "\t 0 & 0 & 5.0 & 3.5 & 1.6 & 0.6\\\\\n",
       "\t 0 & 0 & 5.0 & 3.4 & 1.5 & 0.2\\\\\n",
       "\t 1 & 1 & 6.1 & 3.0 & 4.6 & 1.4\\\\\n",
       "\t 1 & 1 & 5.8 & 2.6 & 4.0 & 1.2\\\\\n",
       "\t 0 & 0 & 4.6 & 3.6 & 1.0 & 0.2\\\\\n",
       "\t 0 & 0 & 5.1 & 3.8 & 1.9 & 0.4\\\\\n",
       "\t 1 & 1 & 6.6 & 2.9 & 4.6 & 1.3\\\\\n",
       "\t 1 & 1 & 5.6 & 3.0 & 4.1 & 1.3\\\\\n",
       "\t 1 & 1 & 5.9 & 3.2 & 4.8 & 1.8\\\\\n",
       "\t 0 & 0 & 4.8 & 3.0 & 1.4 & 0.1\\\\\n",
       "\t 1 & 1 & 6.2 & 2.9 & 4.3 & 1.3\\\\\n",
       "\t 0 & 0 & 4.8 & 3.0 & 1.4 & 0.3\\\\\n",
       "\t 0 & 0 & 5.1 & 3.3 & 1.7 & 0.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 15 × 6\n",
       "\n",
       "| predicted_flower &lt;dbl&gt; | Species &lt;dbl&gt; | Sepal.Length &lt;dbl&gt; | Sepal.Width &lt;dbl&gt; | Petal.Length &lt;dbl&gt; | Petal.Width &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 1 | 5.7 | 2.6 | 3.5 | 1.0 |\n",
       "| 1 | 1 | 6.3 | 2.5 | 4.9 | 1.5 |\n",
       "| 0 | 0 | 5.0 | 3.5 | 1.6 | 0.6 |\n",
       "| 0 | 0 | 5.0 | 3.4 | 1.5 | 0.2 |\n",
       "| 1 | 1 | 6.1 | 3.0 | 4.6 | 1.4 |\n",
       "| 1 | 1 | 5.8 | 2.6 | 4.0 | 1.2 |\n",
       "| 0 | 0 | 4.6 | 3.6 | 1.0 | 0.2 |\n",
       "| 0 | 0 | 5.1 | 3.8 | 1.9 | 0.4 |\n",
       "| 1 | 1 | 6.6 | 2.9 | 4.6 | 1.3 |\n",
       "| 1 | 1 | 5.6 | 3.0 | 4.1 | 1.3 |\n",
       "| 1 | 1 | 5.9 | 3.2 | 4.8 | 1.8 |\n",
       "| 0 | 0 | 4.8 | 3.0 | 1.4 | 0.1 |\n",
       "| 1 | 1 | 6.2 | 2.9 | 4.3 | 1.3 |\n",
       "| 0 | 0 | 4.8 | 3.0 | 1.4 | 0.3 |\n",
       "| 0 | 0 | 5.1 | 3.3 | 1.7 | 0.5 |\n",
       "\n"
      ],
      "text/plain": [
       "   predicted_flower Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n",
       "1  1                1       5.7          2.6         3.5          1.0        \n",
       "2  1                1       6.3          2.5         4.9          1.5        \n",
       "3  0                0       5.0          3.5         1.6          0.6        \n",
       "4  0                0       5.0          3.4         1.5          0.2        \n",
       "5  1                1       6.1          3.0         4.6          1.4        \n",
       "6  1                1       5.8          2.6         4.0          1.2        \n",
       "7  0                0       4.6          3.6         1.0          0.2        \n",
       "8  0                0       5.1          3.8         1.9          0.4        \n",
       "9  1                1       6.6          2.9         4.6          1.3        \n",
       "10 1                1       5.6          3.0         4.1          1.3        \n",
       "11 1                1       5.9          3.2         4.8          1.8        \n",
       "12 0                0       4.8          3.0         1.4          0.1        \n",
       "13 1                1       6.2          2.9         4.3          1.3        \n",
       "14 0                0       4.8          3.0         1.4          0.3        \n",
       "15 0                0       5.1          3.3         1.7          0.5        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = 0, case = 1\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "Area under the curve: 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the test sample for input into the model\n",
    "test_sample <- as.matrix(iris_test[-1])\n",
    "dimnames(test_sample)[[2]] <- NULL\n",
    "\n",
    "# Predict using the deployed model\n",
    "predictions_ep <- model_endpoint1$predict(test_sample)\n",
    "predictions_ep <- stringr::str_split(predictions_ep, pattern = ',', simplify = TRUE)\n",
    "predictions_ep <- as.numeric(predictions_ep > 0.5)\n",
    "\n",
    "# Add the predictions to the test dataset.\n",
    "iris_predictions_ep1 <- dplyr::bind_cols(predicted_flower = predictions_ep, \n",
    "                      iris_test)\n",
    "iris_predictions_ep1\n",
    "\n",
    "# Get the AUC\n",
    "auc(roc(iris_predictions_ep1$predicted_flower,iris_test$Species))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b452019",
   "metadata": {},
   "source": [
    "## Create Model 2: linear learner in sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e0928",
   "metadata": {},
   "source": [
    "Linear models are supervised learning algorithms used for solving either classification or regression problems. For input, you give the model labeled examples (x, y). x is a high-dimensional vector and y is a numeric label. For binary classification problems, the label must be either 0 or 1.\n",
    "\n",
    "The linear learner algorithm requires a data matrix, with rows representing the observations, and columns representing the dimensions of the features. It also requires an additional column that contains the labels that match the data points. At a minimum, Amazon SageMaker linear learner requires you to specify input and output data locations, and objective type (classification or regression) as arguments. The feature dimension is also required. You can specify additional parameters in the HyperParameters string map of the request body. These parameters control the optimization procedure, or specifics of the objective function that you train on. For example, the number of epochs, regularization, and loss type.\n",
    "\n",
    "See https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "630062a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Learner Container Image URL:  404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:1"
     ]
    }
   ],
   "source": [
    "container <- sagemaker$image_uris$retrieve(framework='linear-learner', region= session$boto_region_name, version='latest')\n",
    "cat('Linear Learner Container Image URL: ', container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2731d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output <- paste0('s3://', bucket, '/output_glm')\n",
    "estimator2 <- sagemaker$estimator$Estimator(image_uri = container,\n",
    "                                           role = role_arn,\n",
    "                                           train_instance_count = 1L,\n",
    "                                           train_instance_type = 'ml.m5.4xlarge',\n",
    "                                           input_mode = 'File',\n",
    "                                           output_path = s3_output,\n",
    "                                           output_kms_key = NULL,\n",
    "                                           base_job_name = NULL,\n",
    "                                           sagemaker_session = NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d234e78",
   "metadata": {},
   "source": [
    "For the text/csv input type, the first column is assumed to be the label, which is the target variable for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3806e28",
   "metadata": {},
   "source": [
    "predictor_type is the only hyperparameter that is required to be pre-defined for tuning. The rest are optional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e64f4",
   "metadata": {},
   "source": [
    "Normalization, or feature scaling, is an important preprocessing step for certain loss functions that ensures the model being trained on a dataset does not become dominated by the weight of a single feature. Decision trees do not require normalization of their inputs; and since XGBoost is essentially an ensemble algorithm comprised of decision trees, it does not require normalization for the inputs either.\n",
    "\n",
    "However, Generalized Linear Models require a normalization of their input. The Amazon SageMaker Linear Learner algorithm has a normalization option to assist with this preprocessing step. If normalization is turned on, the algorithm first goes over a small sample of the data to learn the mean value and standard deviation for each feature and for the label. Each of the features in the full dataset is then shifted to have mean of zero and scaled to have a unit standard deviation.\n",
    "\n",
    "To make our job easier, we do not have to go back to our previous steps to do normalization. Normalization is built in as a hyper-parameter in sagemaker Linear learner algorithm. So no need to worry about normalization for the training portions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1c0265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator2$set_hyperparameters(predictor_type=\"binary_classifier\",\n",
    "                               normalize_data = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204c119",
   "metadata": {},
   "source": [
    "The tunable hyperparameters for linear learner are:\n",
    "\n",
    "- wd\n",
    "- l1\n",
    "- learning_rate\n",
    "- mini_batch_size\n",
    "- use_bias\n",
    "- positive_example_weight_mult\n",
    "\n",
    "Be careful to check which parameters are integers and which parameters are continuous because that is one of the common sources of errors. Also be careful to give a proper range for hyperparameters that makes sense for your problem. Training jobs can sometimes fail if the mini-batch size is too big compared to the training data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7ab948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameter Ranges\n",
    "hyperparameter_ranges = list('wd'  = sagemaker$parameter$ContinuousParameter(0.00001,1),\n",
    "                             'l1'  = sagemaker$parameter$ContinuousParameter(0.00001,1),\n",
    "                             'learning_rate'  = sagemaker$parameter$ContinuousParameter(0.00001,1),\n",
    "                             'mini_batch_size'  = sagemaker$parameter$IntegerParameter(10L, 50L) \n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f76266",
   "metadata": {},
   "source": [
    "The evaluation metric we will be using in our case to compare the models will be the objective loss and is based on the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e470024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hyperparamter tuner\n",
    "objective_metric_name = 'validation:objective_loss'\n",
    "tuner2 <- sagemaker$tuner$HyperparameterTuner(estimator2,\n",
    "                                             objective_metric_name,\n",
    "                                             hyperparameter_ranges,\n",
    "                                             objective_type='Minimize',\n",
    "                                             max_jobs=4L,\n",
    "                                             max_parallel_jobs=2L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "099c93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tuning job name\n",
    "job_name <- paste('tune-linear-learner', format(Sys.time(), '%H-%M-%S'), sep = '-')\n",
    "\n",
    "# Define the data channels for train and validation datasets\n",
    "input_data <- list('train' = s3_train_input,\n",
    "                   'validation' = s3_valid_input)\n",
    "\n",
    "# Train the tuner\n",
    "tuner2$fit(inputs = input_data, job_name = job_name, wait=TRUE, content_type='csv') # since we are using csv files as input into the model, we need to specify content type as csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3330f95",
   "metadata": {},
   "source": [
    "### Calculate AUC for the test data on model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "538e6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model into an instance of your choosing.\n",
    "model_endpoint2 <- tuner2$deploy(initial_instance_count = 1L,\n",
    "                                   instance_type = 'ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b456ad8",
   "metadata": {},
   "source": [
    "For inference, the linear learner algorithm supports the application/json, application/x-recordio-protobuf, and text/csv formats. For more information, https://docs.aws.amazon.com/sagemaker/latest/dg/LL-in-formats.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ae6f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify what data formats you want the input and output of your model to look like.\n",
    "model_endpoint2$serializer <- sagemaker$serializers$CSVSerializer(content_type='text/csv')\n",
    "model_endpoint2$deserializer <- sagemaker$deserializers$JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f22ab",
   "metadata": {},
   "source": [
    "In Linear Learner the output inference files are in JSON or RecordIO formats. https://docs.aws.amazon.com/sagemaker/latest/dg/LL-in-formats.html  \n",
    "\n",
    "When you make predictions on new data, the contents of the response data depends on the type of model you choose within Linear Learner. For regression (predictor_type='regressor'), the score is the prediction produced by the model. For classification (predictor_type='binary_classifier' or predictor_type='multiclass_classifier'), the model returns a score and also a predicted_label. The predicted_label is the class predicted by the model and the score measures the strength of that prediction. So, for binary classification, predicted_label is 0 or 1, and score is a single floating point number that indicates how strongly the algorithm believes that the label should be 1.\n",
    "\n",
    "To interpret the score in classification problems, you have to consider the loss function used. If the loss hyperparameter value is logistic for binary classification or softmax_loss for multiclass classification, then the score can be interpreted as the probability of the corresponding class. These are the loss values used by the linear learner when the `loss` hyperparameter is set to auto as default value. But if the `loss` is set to `hinge_loss`, then the score cannot be interpreted as a probability. This is because hinge loss corresponds to a Support Vector Classifier, which does not produce probability estimates. In the current example, since our loss hyperparameter is logistic for binary classification, we can interpret it as probability of the corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2b32021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 15 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>predicted_flower</th><th scope=col>Species</th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>1</td><td>5.7</td><td>2.6</td><td>3.5</td><td>1.0</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.3</td><td>2.5</td><td>4.9</td><td>1.5</td></tr>\n",
       "\t<tr><td>1</td><td>0</td><td>5.0</td><td>3.5</td><td>1.6</td><td>0.6</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.1</td><td>3.0</td><td>4.6</td><td>1.4</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>5.8</td><td>2.6</td><td>4.0</td><td>1.2</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>4.6</td><td>3.6</td><td>1.0</td><td>0.2</td></tr>\n",
       "\t<tr><td>1</td><td>0</td><td>5.1</td><td>3.8</td><td>1.9</td><td>0.4</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.6</td><td>2.9</td><td>4.6</td><td>1.3</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>5.6</td><td>3.0</td><td>4.1</td><td>1.3</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>5.9</td><td>3.2</td><td>4.8</td><td>1.8</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>4.8</td><td>3.0</td><td>1.4</td><td>0.1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.2</td><td>2.9</td><td>4.3</td><td>1.3</td></tr>\n",
       "\t<tr><td>1</td><td>0</td><td>4.8</td><td>3.0</td><td>1.4</td><td>0.3</td></tr>\n",
       "\t<tr><td>1</td><td>0</td><td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 15 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " predicted\\_flower & Species & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1 & 1 & 5.7 & 2.6 & 3.5 & 1.0\\\\\n",
       "\t 1 & 1 & 6.3 & 2.5 & 4.9 & 1.5\\\\\n",
       "\t 1 & 0 & 5.0 & 3.5 & 1.6 & 0.6\\\\\n",
       "\t 0 & 0 & 5.0 & 3.4 & 1.5 & 0.2\\\\\n",
       "\t 1 & 1 & 6.1 & 3.0 & 4.6 & 1.4\\\\\n",
       "\t 1 & 1 & 5.8 & 2.6 & 4.0 & 1.2\\\\\n",
       "\t 0 & 0 & 4.6 & 3.6 & 1.0 & 0.2\\\\\n",
       "\t 1 & 0 & 5.1 & 3.8 & 1.9 & 0.4\\\\\n",
       "\t 1 & 1 & 6.6 & 2.9 & 4.6 & 1.3\\\\\n",
       "\t 1 & 1 & 5.6 & 3.0 & 4.1 & 1.3\\\\\n",
       "\t 1 & 1 & 5.9 & 3.2 & 4.8 & 1.8\\\\\n",
       "\t 0 & 0 & 4.8 & 3.0 & 1.4 & 0.1\\\\\n",
       "\t 1 & 1 & 6.2 & 2.9 & 4.3 & 1.3\\\\\n",
       "\t 1 & 0 & 4.8 & 3.0 & 1.4 & 0.3\\\\\n",
       "\t 1 & 0 & 5.1 & 3.3 & 1.7 & 0.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 15 × 6\n",
       "\n",
       "| predicted_flower &lt;dbl&gt; | Species &lt;dbl&gt; | Sepal.Length &lt;dbl&gt; | Sepal.Width &lt;dbl&gt; | Petal.Length &lt;dbl&gt; | Petal.Width &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 1 | 5.7 | 2.6 | 3.5 | 1.0 |\n",
       "| 1 | 1 | 6.3 | 2.5 | 4.9 | 1.5 |\n",
       "| 1 | 0 | 5.0 | 3.5 | 1.6 | 0.6 |\n",
       "| 0 | 0 | 5.0 | 3.4 | 1.5 | 0.2 |\n",
       "| 1 | 1 | 6.1 | 3.0 | 4.6 | 1.4 |\n",
       "| 1 | 1 | 5.8 | 2.6 | 4.0 | 1.2 |\n",
       "| 0 | 0 | 4.6 | 3.6 | 1.0 | 0.2 |\n",
       "| 1 | 0 | 5.1 | 3.8 | 1.9 | 0.4 |\n",
       "| 1 | 1 | 6.6 | 2.9 | 4.6 | 1.3 |\n",
       "| 1 | 1 | 5.6 | 3.0 | 4.1 | 1.3 |\n",
       "| 1 | 1 | 5.9 | 3.2 | 4.8 | 1.8 |\n",
       "| 0 | 0 | 4.8 | 3.0 | 1.4 | 0.1 |\n",
       "| 1 | 1 | 6.2 | 2.9 | 4.3 | 1.3 |\n",
       "| 1 | 0 | 4.8 | 3.0 | 1.4 | 0.3 |\n",
       "| 1 | 0 | 5.1 | 3.3 | 1.7 | 0.5 |\n",
       "\n"
      ],
      "text/plain": [
       "   predicted_flower Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n",
       "1  1                1       5.7          2.6         3.5          1.0        \n",
       "2  1                1       6.3          2.5         4.9          1.5        \n",
       "3  1                0       5.0          3.5         1.6          0.6        \n",
       "4  0                0       5.0          3.4         1.5          0.2        \n",
       "5  1                1       6.1          3.0         4.6          1.4        \n",
       "6  1                1       5.8          2.6         4.0          1.2        \n",
       "7  0                0       4.6          3.6         1.0          0.2        \n",
       "8  1                0       5.1          3.8         1.9          0.4        \n",
       "9  1                1       6.6          2.9         4.6          1.3        \n",
       "10 1                1       5.6          3.0         4.1          1.3        \n",
       "11 1                1       5.9          3.2         4.8          1.8        \n",
       "12 0                0       4.8          3.0         1.4          0.1        \n",
       "13 1                1       6.2          2.9         4.3          1.3        \n",
       "14 1                0       4.8          3.0         1.4          0.3        \n",
       "15 1                0       5.1          3.3         1.7          0.5        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = 0, case = 1\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.833333333333333"
      ],
      "text/latex": [
       "0.833333333333333"
      ],
      "text/markdown": [
       "0.833333333333333"
      ],
      "text/plain": [
       "Area under the curve: 0.8333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the test data for input into the model\n",
    "test_sample <- as.matrix(iris_test[-1])\n",
    "dimnames(test_sample)[[2]] <- NULL\n",
    "\n",
    "# Predict using the test data on the deployed model\n",
    "predictions_ep <- model_endpoint2$predict(test_sample)\n",
    "\n",
    "# Add the predictions to the test dataset.\n",
    "df <- data.frame(matrix(unlist(predictions_ep$predictions), nrow=length(predictions_ep$predictions), byrow=TRUE))\n",
    "df <- df %>% dplyr::rename(score = X1, predicted_label = X2)\n",
    "iris_predictions_ep2 <- dplyr::bind_cols(predicted_flower = df$predicted_label, \n",
    "                      iris_test)\n",
    "iris_predictions_ep2\n",
    "\n",
    "# Get the AUC\n",
    "auc(roc(iris_predictions_ep2$predicted_flower,iris_test$Species))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df7a9e",
   "metadata": {},
   "source": [
    "## Create Model 3: KNN in sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71cc937",
   "metadata": {},
   "source": [
    "Amazon SageMaker k-nearest neighbors (k-NN) algorithm is an index-based algorithm. It uses a non-parametric method for classification or regression. For classification problems, the algorithm queries the k points that are closest to the sample point and returns the most frequently used label of their class as the predicted label. For regression problems, the algorithm queries the k closest points to the sample point and returns the average of their feature values as the predicted value.\n",
    "\n",
    "Training with the k-NN algorithm has three steps: sampling, dimension reduction, and index building. Sampling reduces the size of the initial dataset so that it fits into memory. For dimension reduction, the algorithm decreases the feature dimension of the data to reduce the footprint of the k-NN model in memory and inference latency. We provide two methods of dimension reduction methods: random projection and the fast Johnson-Lindenstrauss transform. Typically, you use dimension reduction for high-dimensional (d >1000) datasets to avoid the “curse of dimensionality” that troubles the statistical analysis of data that becomes sparse as dimensionality increases. The main objective of k-NN's training is to construct the index. The index enables efficient lookups of distances between points whose values or class labels have not yet been determined and the k nearest points to use for inference.\n",
    "\n",
    "See https://docs.aws.amazon.com/sagemaker/latest/dg/k-nearest-neighbors.html for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1729e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Container Image URL:  404615174143.dkr.ecr.us-east-2.amazonaws.com/knn:1"
     ]
    }
   ],
   "source": [
    "container <- sagemaker$image_uris$retrieve(framework='knn', region= session$boto_region_name, version='latest')\n",
    "cat('KNN Container Image URL: ', container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58a25abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output <- paste0('s3://', bucket, '/output_knn')\n",
    "estimator3 <- sagemaker$estimator$Estimator(image_uri = container,\n",
    "                                           role = role_arn,\n",
    "                                           train_instance_count = 1L,\n",
    "                                           train_instance_type = 'ml.m5.4xlarge',\n",
    "                                           input_mode = 'File',\n",
    "                                           output_path = s3_output,\n",
    "                                           output_kms_key = NULL,\n",
    "                                           base_job_name = NULL,\n",
    "                                           sagemaker_session = NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0457bfe",
   "metadata": {},
   "source": [
    "Hyperparameter `dimension_reduction_target` should not be set when `dimension_reduction_type` is set to its default value, which is `None`. If 'dimension_reduction_target' is set to a certain number without setting `dimension_reduction_type`, then Sagemaker will ask us to remove 'dimension_reduction_target' from the specified hyperparameters and try again. In this tutorial, we are not performing dimensionality reduction, since we only have 4 features; so `dimension_reduction_type` is set to its default value of `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ae2d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator3$set_hyperparameters( \n",
    "                              feature_dim = 4L, \n",
    "                              sample_size = 10L, \n",
    "                              predictor_type = \"classifier\"\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc2de6",
   "metadata": {},
   "source": [
    "Amazon SageMaker k-nearest neighbor model can be tuned  with the following hyperparameters:\n",
    "- k \n",
    "- sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d51f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameter Ranges\n",
    "hyperparameter_ranges = list('k' = sagemaker$parameter$IntegerParameter(1L,10L)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "465de020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hyperparamter tuner\n",
    "objective_metric_name = 'test:accuracy'\n",
    "tuner3 <- sagemaker$tuner$HyperparameterTuner(estimator3,\n",
    "                                             objective_metric_name,\n",
    "                                             hyperparameter_ranges,\n",
    "                                             objective_type='Maximize',\n",
    "                                             max_jobs=2L,\n",
    "                                             max_parallel_jobs=2L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d9fc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tuning job name\n",
    "job_name <- paste('tune-knn', format(Sys.time(), '%H-%M-%S'), sep = '-')\n",
    "\n",
    "# Define the data channels for train and validation datasets\n",
    "input_data <- list('train' = s3_train_input,\n",
    "                   'test' = s3_valid_input # KNN needs a test data, does not work without it.\n",
    "                    ) \n",
    "\n",
    "# train the tuner\n",
    "tuner3$fit(inputs = input_data, job_name = job_name, wait=TRUE, content_type='text/csv;label_size=0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0edeb8",
   "metadata": {},
   "source": [
    "### Calculate AUC for the test data on model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3b336c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model into an instance of your choosing.\n",
    "model_endpoint3 <- tuner3$deploy(initial_instance_count = 1L,\n",
    "                                   instance_type = 'ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bcdb29",
   "metadata": {},
   "source": [
    "For inference, the linear learner algorithm supports the application/json, application/x-recordio-protobuf, and text/csv formats. For more information, https://docs.aws.amazon.com/sagemaker/latest/dg/LL-in-formats.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d947300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify what data formats you want the input and output of your model to look like.\n",
    "model_endpoint3$serializer <- sagemaker$serializers$CSVSerializer(content_type='text/csv')\n",
    "model_endpoint3$deserializer <- sagemaker$deserializers$JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba57f2",
   "metadata": {},
   "source": [
    "In KNN, the input formats for inference are:\n",
    "- CSV\n",
    "- JSON\n",
    "- JSONLINES\n",
    "- RECORDIO\n",
    "\n",
    "The output formats for inference are:\n",
    "- JSON\n",
    "- JSONLINES\n",
    "- Verbose JSON\n",
    "- Verbose RecordIO-ProtoBuf\n",
    "\n",
    "Notice that there is no CSV output format for inference.  \n",
    "\n",
    "See https://docs.aws.amazon.com/sagemaker/latest/dg/kNN-inference-formats.html for more details.\n",
    "\n",
    "When you make predictions on new data, the contents of the response data depends on the type of model you choose within Linear Learner. For regression (predictor_type='regressor'), the score is the prediction produced by the model. For classification (predictor_type='binary_classifier' or predictor_type='multiclass_classifier'), the model returns a score and also a predicted_label. The predicted_label is the class predicted by the model and the score measures the strength of that prediction. So, for binary classification, predicted_label is 0 or 1, and score is a single floating point number that indicates how strongly the algorithm believes that the label should be 1.\n",
    "\n",
    "To interpret the score in classification problems, you have to consider the loss function used. If the loss hyperparameter value is logistic for binary classification or softmax_loss for multiclass classification, then the score can be interpreted as the probability of the corresponding class. These are the loss values used by the linear learner when the loss hyperparameter is set to auto as default value. But if the loss is set to hinge_loss, then the score cannot be interpreted as a probability. This is because hinge loss corresponds to a Support Vector Classifier, which does not produce probability estimates. In the current example, since our loss hyperparameter is logistic for binary classification, we can interpret it as probability of the corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f58eafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test data for input into the model\n",
    "test_sample <- as.matrix(iris_test[-1])\n",
    "dimnames(test_sample)[[2]] <- NULL\n",
    "\n",
    "# Predict using the test data on the deployed model\n",
    "predictions_ep <- model_endpoint3$predict(test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc0b2a",
   "metadata": {},
   "source": [
    "We see that the output is of a deserialized JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "528708e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>$predictions</strong> = <ol>\n",
       "\t<li><strong>$predicted_label</strong> = 1</li>\n",
       "\t<li><strong>$predicted_label</strong> = 1</li>\n",
       "\t<li><strong>$predicted_label</strong> = 0</li>\n",
       "\t<li><strong>$predicted_label</strong> = 0</li>\n",
       "\t<li><strong>$predicted_label</strong> = 1</li>\n",
       "\t<li><strong>$predicted_label</strong> = 1</li>\n",
       "\t<li><strong>$predicted_label</strong> = 0</li>\n",
       "\t<li><strong>$predicted_label</strong> = 0</li>\n",
       "\t<li><strong>$predicted_label</strong> = 1</li>\n",
       "\t<li><strong>$predicted_label</strong> = 1</li>\n",
       "\t<li><strong>$predicted_label</strong> = 1</li>\n",
       "\t<li><strong>$predicted_label</strong> = 0</li>\n",
       "\t<li><strong>$predicted_label</strong> = 1</li>\n",
       "\t<li><strong>$predicted_label</strong> = 0</li>\n",
       "\t<li><strong>$predicted_label</strong> = 0</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\textbf{\\$predictions} = \\begin{enumerate}\n",
       "\\item \\textbf{\\$predicted\\_label} = 1\n",
       "\\item \\textbf{\\$predicted\\_label} = 1\n",
       "\\item \\textbf{\\$predicted\\_label} = 0\n",
       "\\item \\textbf{\\$predicted\\_label} = 0\n",
       "\\item \\textbf{\\$predicted\\_label} = 1\n",
       "\\item \\textbf{\\$predicted\\_label} = 1\n",
       "\\item \\textbf{\\$predicted\\_label} = 0\n",
       "\\item \\textbf{\\$predicted\\_label} = 0\n",
       "\\item \\textbf{\\$predicted\\_label} = 1\n",
       "\\item \\textbf{\\$predicted\\_label} = 1\n",
       "\\item \\textbf{\\$predicted\\_label} = 1\n",
       "\\item \\textbf{\\$predicted\\_label} = 0\n",
       "\\item \\textbf{\\$predicted\\_label} = 1\n",
       "\\item \\textbf{\\$predicted\\_label} = 0\n",
       "\\item \\textbf{\\$predicted\\_label} = 0\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "**$predictions** = 1. **$predicted_label** = 1\n",
       "2. **$predicted_label** = 1\n",
       "3. **$predicted_label** = 0\n",
       "4. **$predicted_label** = 0\n",
       "5. **$predicted_label** = 1\n",
       "6. **$predicted_label** = 1\n",
       "7. **$predicted_label** = 0\n",
       "8. **$predicted_label** = 0\n",
       "9. **$predicted_label** = 1\n",
       "10. **$predicted_label** = 1\n",
       "11. **$predicted_label** = 1\n",
       "12. **$predicted_label** = 0\n",
       "13. **$predicted_label** = 1\n",
       "14. **$predicted_label** = 0\n",
       "15. **$predicted_label** = 0\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$predictions\n",
       "$predictions[[1]]\n",
       "$predictions[[1]]$predicted_label\n",
       "[1] 1\n",
       "\n",
       "\n",
       "$predictions[[2]]\n",
       "$predictions[[2]]$predicted_label\n",
       "[1] 1\n",
       "\n",
       "\n",
       "$predictions[[3]]\n",
       "$predictions[[3]]$predicted_label\n",
       "[1] 0\n",
       "\n",
       "\n",
       "$predictions[[4]]\n",
       "$predictions[[4]]$predicted_label\n",
       "[1] 0\n",
       "\n",
       "\n",
       "$predictions[[5]]\n",
       "$predictions[[5]]$predicted_label\n",
       "[1] 1\n",
       "\n",
       "\n",
       "$predictions[[6]]\n",
       "$predictions[[6]]$predicted_label\n",
       "[1] 1\n",
       "\n",
       "\n",
       "$predictions[[7]]\n",
       "$predictions[[7]]$predicted_label\n",
       "[1] 0\n",
       "\n",
       "\n",
       "$predictions[[8]]\n",
       "$predictions[[8]]$predicted_label\n",
       "[1] 0\n",
       "\n",
       "\n",
       "$predictions[[9]]\n",
       "$predictions[[9]]$predicted_label\n",
       "[1] 1\n",
       "\n",
       "\n",
       "$predictions[[10]]\n",
       "$predictions[[10]]$predicted_label\n",
       "[1] 1\n",
       "\n",
       "\n",
       "$predictions[[11]]\n",
       "$predictions[[11]]$predicted_label\n",
       "[1] 1\n",
       "\n",
       "\n",
       "$predictions[[12]]\n",
       "$predictions[[12]]$predicted_label\n",
       "[1] 0\n",
       "\n",
       "\n",
       "$predictions[[13]]\n",
       "$predictions[[13]]$predicted_label\n",
       "[1] 1\n",
       "\n",
       "\n",
       "$predictions[[14]]\n",
       "$predictions[[14]]$predicted_label\n",
       "[1] 0\n",
       "\n",
       "\n",
       "$predictions[[15]]\n",
       "$predictions[[15]]$predicted_label\n",
       "[1] 0\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcf6313c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'list'"
      ],
      "text/latex": [
       "'list'"
      ],
      "text/markdown": [
       "'list'"
      ],
      "text/plain": [
       "[1] \"list\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "typeof(predictions_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "723d7d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 15 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>predicted_flower</th><th scope=col>Species</th><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>1</td><td>5.7</td><td>2.6</td><td>3.5</td><td>1.0</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.3</td><td>2.5</td><td>4.9</td><td>1.5</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>5.0</td><td>3.5</td><td>1.6</td><td>0.6</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.1</td><td>3.0</td><td>4.6</td><td>1.4</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>5.8</td><td>2.6</td><td>4.0</td><td>1.2</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>4.6</td><td>3.6</td><td>1.0</td><td>0.2</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>5.1</td><td>3.8</td><td>1.9</td><td>0.4</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.6</td><td>2.9</td><td>4.6</td><td>1.3</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>5.6</td><td>3.0</td><td>4.1</td><td>1.3</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>5.9</td><td>3.2</td><td>4.8</td><td>1.8</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>4.8</td><td>3.0</td><td>1.4</td><td>0.1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td><td>6.2</td><td>2.9</td><td>4.3</td><td>1.3</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>4.8</td><td>3.0</td><td>1.4</td><td>0.3</td></tr>\n",
       "\t<tr><td>0</td><td>0</td><td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 15 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " predicted\\_flower & Species & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1 & 1 & 5.7 & 2.6 & 3.5 & 1.0\\\\\n",
       "\t 1 & 1 & 6.3 & 2.5 & 4.9 & 1.5\\\\\n",
       "\t 0 & 0 & 5.0 & 3.5 & 1.6 & 0.6\\\\\n",
       "\t 0 & 0 & 5.0 & 3.4 & 1.5 & 0.2\\\\\n",
       "\t 1 & 1 & 6.1 & 3.0 & 4.6 & 1.4\\\\\n",
       "\t 1 & 1 & 5.8 & 2.6 & 4.0 & 1.2\\\\\n",
       "\t 0 & 0 & 4.6 & 3.6 & 1.0 & 0.2\\\\\n",
       "\t 0 & 0 & 5.1 & 3.8 & 1.9 & 0.4\\\\\n",
       "\t 1 & 1 & 6.6 & 2.9 & 4.6 & 1.3\\\\\n",
       "\t 1 & 1 & 5.6 & 3.0 & 4.1 & 1.3\\\\\n",
       "\t 1 & 1 & 5.9 & 3.2 & 4.8 & 1.8\\\\\n",
       "\t 0 & 0 & 4.8 & 3.0 & 1.4 & 0.1\\\\\n",
       "\t 1 & 1 & 6.2 & 2.9 & 4.3 & 1.3\\\\\n",
       "\t 0 & 0 & 4.8 & 3.0 & 1.4 & 0.3\\\\\n",
       "\t 0 & 0 & 5.1 & 3.3 & 1.7 & 0.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 15 × 6\n",
       "\n",
       "| predicted_flower &lt;dbl&gt; | Species &lt;dbl&gt; | Sepal.Length &lt;dbl&gt; | Sepal.Width &lt;dbl&gt; | Petal.Length &lt;dbl&gt; | Petal.Width &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 1 | 5.7 | 2.6 | 3.5 | 1.0 |\n",
       "| 1 | 1 | 6.3 | 2.5 | 4.9 | 1.5 |\n",
       "| 0 | 0 | 5.0 | 3.5 | 1.6 | 0.6 |\n",
       "| 0 | 0 | 5.0 | 3.4 | 1.5 | 0.2 |\n",
       "| 1 | 1 | 6.1 | 3.0 | 4.6 | 1.4 |\n",
       "| 1 | 1 | 5.8 | 2.6 | 4.0 | 1.2 |\n",
       "| 0 | 0 | 4.6 | 3.6 | 1.0 | 0.2 |\n",
       "| 0 | 0 | 5.1 | 3.8 | 1.9 | 0.4 |\n",
       "| 1 | 1 | 6.6 | 2.9 | 4.6 | 1.3 |\n",
       "| 1 | 1 | 5.6 | 3.0 | 4.1 | 1.3 |\n",
       "| 1 | 1 | 5.9 | 3.2 | 4.8 | 1.8 |\n",
       "| 0 | 0 | 4.8 | 3.0 | 1.4 | 0.1 |\n",
       "| 1 | 1 | 6.2 | 2.9 | 4.3 | 1.3 |\n",
       "| 0 | 0 | 4.8 | 3.0 | 1.4 | 0.3 |\n",
       "| 0 | 0 | 5.1 | 3.3 | 1.7 | 0.5 |\n",
       "\n"
      ],
      "text/plain": [
       "   predicted_flower Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n",
       "1  1                1       5.7          2.6         3.5          1.0        \n",
       "2  1                1       6.3          2.5         4.9          1.5        \n",
       "3  0                0       5.0          3.5         1.6          0.6        \n",
       "4  0                0       5.0          3.4         1.5          0.2        \n",
       "5  1                1       6.1          3.0         4.6          1.4        \n",
       "6  1                1       5.8          2.6         4.0          1.2        \n",
       "7  0                0       4.6          3.6         1.0          0.2        \n",
       "8  0                0       5.1          3.8         1.9          0.4        \n",
       "9  1                1       6.6          2.9         4.6          1.3        \n",
       "10 1                1       5.6          3.0         4.1          1.3        \n",
       "11 1                1       5.9          3.2         4.8          1.8        \n",
       "12 0                0       4.8          3.0         1.4          0.1        \n",
       "13 1                1       6.2          2.9         4.3          1.3        \n",
       "14 0                0       4.8          3.0         1.4          0.3        \n",
       "15 0                0       5.1          3.3         1.7          0.5        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = 0, case = 1\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "Area under the curve: 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the predictions to the test dataset.\n",
    "df = data.frame(predicted_flower = unlist(predictions_ep$predictions))\n",
    "iris_predictions_ep2 <- dplyr::bind_cols(predicted_flower = df$predicted_flower, \n",
    "                      iris_test)\n",
    "iris_predictions_ep2\n",
    "\n",
    "# Get the AUC\n",
    "auc(roc(iris_predictions_ep2$predicted_flower,iris_test$Species))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d5908",
   "metadata": {},
   "source": [
    "## Compare the AUC of 3 models for the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62034c82",
   "metadata": {},
   "source": [
    "\n",
    "- AUC of xgboost = 1 \n",
    "\n",
    "- AUC of linear learner = 0.83\n",
    "\n",
    "- AUC of KNN = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2195c4",
   "metadata": {},
   "source": [
    "The deployed model of our choosing can be passed onto production and tracked in CloudWatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd611b4",
   "metadata": {},
   "source": [
    "## Clean up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c737d6",
   "metadata": {},
   "source": [
    "##### We close the endpoints which we created to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb67f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "session$delete_endpoint(model_endpoint1$endpoint)\n",
    "session$delete_endpoint(model_endpoint2$endpoint)\n",
    "session$delete_endpoint(model_endpoint3$endpoint)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
