{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Fraud Detection using Graph Neural Networks with DGL (Deep Graph Library) on Amazon SageMaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note. This notebook should be used with the Python 3 (Data Science) kernel.**\n",
    "\n",
    "In this notebook, we provide following highlights. \n",
    "\n",
    "* An end to end pipeline to train a fraud detection model using graph neural networks and a baseline model using xgboost. \n",
    "\n",
    "* Hyper-Parameter Optimization (HPO) for both graph neural networks and xgboost.\n",
    "\n",
    "* For the training and HPO process, we firstly process the raw dataset to prepare the features and extract the interactions in the dataset that are used to construct the graph. \n",
    "\n",
    "* Then we launch a training job using the SageMaker framework estimator to train a graph neural network model with DGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./sagemaker_graph_fraud_detection/\")\n",
    "\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker_graph_fraud_detection import config\n",
    "\n",
    "role = config.role\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3\n",
    "We go over the specific data schema in subsequent cells but now let's move the raw data to a convenient location in the S3 bucket for this proejct, where it is be picked up by the preprocessing job and training job.\n",
    "\n",
    "If you would like to use your own dataset for this demonstration. Replace the `raw_data_location` with the s3 path or local path of your dataset, and modify the data preprocessing step as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace with an S3 location or local path to point to your own dataset\n",
    "raw_data_location = \"s3://{}/{}/artifacts/data\".format(\n",
    "    config.solution_upstream_bucket, config.solution_name\n",
    ")\n",
    "\n",
    "session_prefix = \"dgl-fraud-detection\"\n",
    "input_data = \"s3://{}/{}/{}\".format(config.solution_bucket, session_prefix, config.s3_data_prefix)\n",
    "\n",
    "!aws s3 cp --recursive $raw_data_location $input_data\n",
    "\n",
    "!mkdir input_raw_data # for data visualization, we also download the datasets into local directory.\n",
    "!aws s3 cp --recursive $raw_data_location input_raw_data\n",
    "\n",
    "# Set S3 locations to store processed data for training and post-training results and artifacts respectively\n",
    "train_data = \"s3://{}/{}/{}\".format(\n",
    "    config.solution_bucket, session_prefix, config.s3_processing_output\n",
    ")\n",
    "train_output = \"s3://{}/{}/{}\".format(\n",
    "    config.solution_bucket, session_prefix, config.s3_train_output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "The dataset we use is a synthetic dataset created to mimic typical examples of financial transactions dataset that many companies have. The dataset consists of two tables:\n",
    "\n",
    "* **Transactions** table: Records transactions and metadata about transactions between two users. Examples of columns include the product code for the transaction and features on the card used for the transaction, and a column indicating whether the corresponded transcation is fraud or not.\n",
    "* **Identity** table: Contains information about the identity users performing transactions. Examples of columns here include the device type and device ids used.\n",
    "\n",
    "The two tables can be joined together using the unique identified-key column **TransactionID**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "Read the tables of transaction.csv and identifity.csv and merge them based on the TransactionID column for better visualization.\n",
    "\n",
    "Besides the unique identifier column (**TransactionID**) to identify each transaction, there are two types of predicting columns and one target column.\n",
    "\n",
    "* **Identity columns** that contain identity information related to a transaction. The corresponded columns include **card_no**, **card_type**, **email_domain**, **IpAddress**, **PhoneNo**, **DeviceID**.\n",
    "\n",
    "* **Categorical or numerical columns** that describes the features of each transaction. The corresponded columns include **ProductCD** and **TransactionAmt**.\n",
    "\n",
    "* Target column **isFraud**.\n",
    "\n",
    "The **goal** is to fully utilize the information in the predicting columns to classify each transaction (each row in the table) to be either fraud or not fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "raw_data_dir = \"input_raw_data\"\n",
    "transactions_df = pd.read_csv(os.path.join(raw_data_dir, \"transaction.csv\"))\n",
    "identity_df = pd.read_csv(os.path.join(raw_data_dir, \"identity.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 5 observations in transaction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transactions_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 5 observations in identity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identity_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the two datasets using the **TransactionID** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_identity_df = transactions_df.merge(identity_df, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "# drop transcations time column as it is not useful for constructing graph.\n",
    "full_identity_df.drop([\"TransactionDT\"], axis=1, inplace=True)\n",
    "\n",
    "# Re-arange the order of column names for better visualization\n",
    "full_identity_df = full_identity_df[\n",
    "    [\n",
    "        \"TransactionID\",\n",
    "        \"card_no\",\n",
    "        \"card_type\",\n",
    "        \"email_domain\",\n",
    "        \"IpAddress\",\n",
    "        \"PhoneNo\",\n",
    "        \"DeviceID\",\n",
    "        \"ProductCD\",\n",
    "        \"TransactionAmt\",\n",
    "        \"isFraud\",\n",
    "    ]\n",
    "]\n",
    "full_identity_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Since the dataset shown above not only contains the features of each transaction such as the purchased product type and transaction amount but also multiple identity information that could be used to identify the relations between the transactions. \n",
    "\n",
    "Those information can be used to construct heterogeneous graphs in graph neural networks. The heterogeneous graphs contain different types of nodes and edges. The different types of nodes and edges tend to have different types of attributes that are designed to capture the characteristics of each node and edge type. \n",
    "\n",
    "In our case, different node types correspond to the categorical columns such as **card_type**, **card_no**, **email_domain**, **IpAddress**, **PhoneNo**, and **DeviceID**.\n",
    "\n",
    "The graph neural networks utilize all the constructed information above to learn a hidden representation (embedding) for each transaction such that the hidden representation is used as input for a linear classification layer to determine whether the transaction is fraud or not fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example heterogeneous graph based on the datasets mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "Image(filename=\"illustration-dgl.png\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Preprocessing job with Amazon SageMaker Processing\n",
    "\n",
    "The script we have defined at `data-preprocessing/graph_data_preprocessor.py` performs data preprocessing and feature engineering transformations on the raw data. We provide a general processing framework to convert a relational table to heterogeneous graph edgelists based on the column types of the relational table. Some of the data transformation and feature engineering techniques include:\n",
    "\n",
    "* Performing numerical encoding for categorical variables and logarithmic transformation for transaction amount\n",
    "* Constructing graph edgelists between transactions and other entities for the various relation types\n",
    "\n",
    "The inputs to the data preprocessing script are passed in as python command line arguments. All the columns in the relational table are classifed into one of 3 types for the purposes of data transformation: \n",
    "\n",
    "* **Identity columns** `--id-cols`: columns that contain identity information related to a user or transaction for example IP address, Phone Number, device identifiers etc. These column types become node types in the heterogeneous graph, and the entries in these columns become the nodes. The column names for these column types need to passed in to the script.\n",
    "\n",
    "* **Categorical columns** `--cat-cols`: columns that correspond to categorical features for a user's age group or whether a provided address matches with an address on file. The entries in these columns undergo numerical feature transformation and are used as node attributes in the heterogeneous graph. The columns names for these column types also needs to be passed in to the script\n",
    "\n",
    "* **Numerical columns**: columns that correspond to numerical features like how many times a user has tried a transaction and so on. The entries here are also used as node attributes in the heterogeneous graph. The script assumes that all columns in the tables that are not identity columns or categorical columns are numerical columns\n",
    "\n",
    "The datasets are divided into training (70% of the entire data), validation (20%), and test datasets (10%). The validation dataset are used for hyper-parameter optimization to select the optimal set of hyper-parameters. And the test dataset is used for the final evaluation to compare various models.\n",
    "\n",
    "In order to adapt the preprocessing script to work with data in the same format, you can simply change the python arguments used in the cell below to a comma seperate string for the column names in your dataset. If your dataset is in a different format, then you also have to modify the preprocessing script at `data-preprocessing/graph_data_preprocessor.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from time import strftime, gmtime\n",
    "\n",
    "processing_job_name = \"{}-processing-job-{}\".format(\n",
    "    config.solution_prefix, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    ")\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    base_job_name=processing_job_name,\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"data-preprocessing/graph_data_preprocessor.py\",\n",
    "    inputs=[ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[ProcessingOutput(destination=train_data, source=\"/opt/ml/processing/output\")],\n",
    "    arguments=[\n",
    "        \"--id-cols\",\n",
    "        \"card_no,card_type,email_domain\",\n",
    "        \"--cat-cols\",\n",
    "        \"ProductCD\",\n",
    "        \"--cat-cols-xgboost\",\n",
    "        \"card_type,ProductCD\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results of Data Preprocessing\n",
    "\n",
    "Once the preprocessing job is complete, we can take a look at the contents of the S3 bucket to see the transformed data. We have a set of bipartite edge lists between transactions and different device id types as well as the features, labels and a set of transactions to validate our graph model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "processed_files = S3Downloader.list(train_data)\n",
    "print(\"===== Processed Files =====\")\n",
    "print(\"\\n\".join(processed_files))\n",
    "\n",
    "# download processed data into local directory preprocessed-data\n",
    "S3Downloader.download(train_data, train_data.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a XGBoost Baseline\n",
    "Before diving into training a graph neural network with DGL, let us firstly train a XGBoost model with HPO as the baseline on the transaction table data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from baselines.utils import get_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and upload to S3\n",
    "The features used for training XGBoost are from **features_xgboost.csv** that are processed in above processing job.\n",
    "The features include categorical columns **productCD**, **card_type** and numerical column **TransactionAmt**. The categorical features are onehot encoded. Other features (categorical features) such as **IpAddress**, **PhoneNO** contain too many categories (~40,000) and thus are not suitable to be used as training features for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_df, valid_data_df, test_data_df = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first 5 observations of the train data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the training and validation data into local directory and then upload them to s3 bucket for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p xgboost_input\n",
    "train_data_df.to_csv(\"xgboost_input/train_xgb.csv\", header=False, index=False)\n",
    "valid_data_df.to_csv(\"xgboost_input/validation_xgb.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "from sagemaker_graph_fraud_detection import config\n",
    "\n",
    "role = config.role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = config.solution_bucket\n",
    "prefix = \"xgboost-fraud-detection\"\n",
    "\n",
    "s3_train_data = S3Uploader.upload(\n",
    "    \"xgboost_input/train_xgb.csv\", \"s3://{}/{}/{}\".format(bucket, prefix, \"train\")\n",
    ")\n",
    "print(\"Uploaded training data location: {}\".format(s3_train_data))\n",
    "\n",
    "s3_validation_data = S3Uploader.upload(\n",
    "    \"xgboost_input/validation_xgb.csv\", \"s3://{}/{}/{}\".format(bucket, prefix, \"validation\")\n",
    ")\n",
    "print(\"Uploaded training data location: {}\".format(s3_validation_data))\n",
    "\n",
    "output_location = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "print(\"Training artifacts are uploaded to: {}\".format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SageMaker XGBoost Estimator with HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving onto training, first we need to specify the locations of the XGBoost algorithm containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"latest\")\n",
    "display(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we create TrainingInputs that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_input_train = TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/train\".format(bucket, prefix), content_type=\"csv\"\n",
    ")\n",
    "s3_input_validation = TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/validation/\".format(bucket, prefix), content_type=\"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SageMaker XGBoost Estimator with HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "train_y = train_data_df.values[:, 0]\n",
    "scale_pos_weight = (len(train_y) - sum(train_y)) / sum(\n",
    "    train_y\n",
    ")  # as this is unbalanced dataset, we need give more weight to the minority class example.\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",  #'ml.g4dn.xlarge',\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    eval_metric=\"auc\",\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=1000,\n",
    "    early_stopping_rounds=10,\n",
    "    silent=0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the hyper-parameters search ranges.\n",
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"gamma\": ContinuousParameter(0, 0.6),\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    \"subsample\": ContinuousParameter(0.2, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "objective_metric_name = \"validation:auc\"\n",
    "objective_type = \"Maximize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "unique_hash = str(uuid.uuid4())[:6]\n",
    "tuning_job_name = f\"{config.solution_prefix}-{unique_hash}-tuning-job\"\n",
    "print(\n",
    "    f\"You can go to SageMaker -> Training -> Hyperparameter tuning jobs -> a job name started with {tuning_job_name} to monitor HPO tuning status and details.\\n\"\n",
    "    f\"Note. You are unable to successfully run the following cells until the tuning job completes. This step may take around 15 min.\"\n",
    ")\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=30,\n",
    "    max_parallel_jobs=3,\n",
    "    objective_type=objective_type,\n",
    "    base_tuning_job_name=tuning_job_name,\n",
    ")\n",
    "\n",
    "tuner.fit({\"train\": s3_input_train, \"validation\": s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Status of HPO tuning jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3.client(\"sagemaker\").describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name\n",
    ")[\"HyperParameterTuningJobStatus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the tuning job name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.Session().client(\"sagemaker\")\n",
    "\n",
    "tuning_job_name = tuner.latest_tuning_job.name\n",
    "tuning_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuning_job_result = sm_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "\n",
    "status = tuning_job_result[\"HyperParameterTuningJobStatus\"]\n",
    "if status != \"Completed\":\n",
    "    print(\"Reminder: the tuning job has not been completed.\")\n",
    "\n",
    "job_count = tuning_job_result[\"TrainingJobStatusCounters\"][\"Completed\"]\n",
    "print(\"%d training jobs have completed\" % job_count)\n",
    "\n",
    "is_maximize = (\n",
    "    tuning_job_result[\"HyperParameterTuningJobConfig\"][\"HyperParameterTuningJobObjective\"][\"Type\"]\n",
    "    != \"Maximize\"\n",
    ")\n",
    "objective_name = tuning_job_result[\"HyperParameterTuningJobConfig\"][\n",
    "    \"HyperParameterTuningJobObjective\"\n",
    "][\"MetricName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tuner_analytics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "\n",
    "full_df = tuner_analytics.dataframe()\n",
    "\n",
    "if len(full_df) > 0:\n",
    "    df = full_df[full_df[\"FinalObjectiveValue\"] > -float(\"inf\")]\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values(\"FinalObjectiveValue\", ascending=False)\n",
    "        print(\"Number of training jobs with valid objective: %d\" % len(df))\n",
    "        print({\"lowest\": min(df[\"FinalObjectiveValue\"]), \"highest\": max(df[\"FinalObjectiveValue\"])})\n",
    "        pd.set_option(\"display.max_colwidth\", -1)  # Don't truncate TrainingJobName\n",
    "    else:\n",
    "        print(\"No training jobs have reported valid results yet.\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy endpoint of the best tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "print(\n",
    "    f\"You can go to SageMaker -> Inference -> Endpoints --> an endpoint with name started with {tuning_job_name} to monitor the deployment status.\"\n",
    ")\n",
    "\n",
    "predictor_hpo = tuner.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    serializer=CSVSerializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(current_predictor, data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = \"\"\n",
    "    for array in split_array:\n",
    "        predictions = \",\".join([predictions, current_predictor.predict(array).decode(\"utf-8\")])\n",
    "    return np.fromstring(predictions[1:], sep=\",\")\n",
    "\n",
    "\n",
    "hpo_raw_preds = predict(\n",
    "    predictor_hpo, test_data_df.values[:, 1:]\n",
    ")  # estimated probability for positive class\n",
    "hpo_preds = np.where(hpo_raw_preds > 0.5, 1, 0)  # generate prediction label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def print_metrics(y_true, y_predicted):\n",
    "    cm = confusion_matrix(y_true, y_predicted)\n",
    "    true_neg, false_pos, false_neg, true_pos = cm.ravel()\n",
    "    cm = pd.DataFrame(\n",
    "        np.array([[true_pos, false_pos], [false_neg, true_neg]]),\n",
    "        columns=[\"labels positive\", \"labels negative\"],\n",
    "        index=[\"predicted positive\", \"predicted negative\"],\n",
    "    )\n",
    "\n",
    "    acc = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
    "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return [f1, precision, recall, acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_xgboost_with_hpo = print_metrics(test_data_df.values[:, 0], hpo_preds)\n",
    "fpr, tpr, _ = roc_curve(test_data_df.values[:, 0], hpo_raw_preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "result_xgboost_with_hpo.append(roc_auc)\n",
    "result_xgboost_with_hpo = pd.DataFrame(\n",
    "    result_xgboost_with_hpo,\n",
    "    index=[\"F1\", \"Precision\", \"Recall\", \"Accuracy\", \"ROC_AUC\"],\n",
    "    columns=[\"XGBoost_With_HPO\"],\n",
    ")\n",
    "\n",
    "print(result_xgboost_with_hpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Graph Neural Network using DGL\n",
    "\n",
    "Graph Neural Networks work by learning representation for nodes or edges of a graph that are well suited for some downstream task. We can model the fraud detection problem as a node classification task, and the goal of the graph neural network would be to learn how to use information from the topology of the sub-graph for each transaction node to transform the node's features to a representation space where the node can be easily classified as fraud or not.\n",
    "\n",
    "Specifically, we use a relational graph convolutional neural network model (R-GCN) on a heterogeneous graph since we have nodes and edges of different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "To train the graph neural network, we need to define a few hyperparameters that determine properties such as the class of graph neural network models, the network architecture and the optimizer and optimization parameters. \n",
    "\n",
    "Here we're setting only a few of the hyperparameters, to see all the hyperparameters and their default values, see `dgl-fraud-detection/estimator_fns.py`. The parameters set below are:\n",
    "\n",
    "* **`nodes`** is the name of the file that contains the `node_id`s of the target nodes and the node features.\n",
    "* **`edges`** is a regular expression that when expanded lists all the filenames for the edgelists\n",
    "* **`labels`** is the name of the file tha contains the target `node_id`s and their labels\n",
    "* **`model`** specify which graph neural network to use, this should be set to `r-gcn`\n",
    "\n",
    "The following hyperparameters can be tuned and adjusted to improve model performance\n",
    "* **batch-size** is the number nodes that are used to compute a single forward pass of the GNN\n",
    "\n",
    "* **embedding-size** is the size of the embedding dimension for non target nodes\n",
    "* **n-neighbors** is the number of neighbours to sample for each target node during graph sampling for mini-batch training\n",
    "* **n-layers** is the number of GNN layers in the model\n",
    "* **n-epochs** is the number of training epochs for the model training job\n",
    "* **optimizer** is the optimization algorithm used for gradient based parameter updates\n",
    "* **lr** is the learning rate for parameter updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edges = \",\".join(\n",
    "    map(lambda x: x.split(\"/\")[-1], [file for file in processed_files if \"relation\" in file])\n",
    ")\n",
    "params = {\n",
    "    \"nodes\": \"features.csv\",\n",
    "    \"edges\": \"relation*\",\n",
    "    \"labels\": \"tags.csv\",\n",
    "    \"model\": \"rgcn\",\n",
    "    \"num-gpus\": 1,\n",
    "    \"batch-size\": 1000,\n",
    "    \"embedding-size\": 1024,\n",
    "    \"n-neighbors\": 100,\n",
    "    \"n-layers\": 2,\n",
    "    \"n-epochs\": 10,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 1e-2,\n",
    "}\n",
    "\n",
    "print(\"Graph is constructed using the following edgelists:\\n{}\".format(\"\\n\".join(edges.split(\",\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit SageMaker Estimator\n",
    "\n",
    "With the hyperparameters defined, we can kick off the training job. We use the Deep Graph Library (DGL), with MXNet as the backend deep learning framework, to define and train the graph neural network. Amazon SageMaker makes it do this with the Framework estimators which have the deep learning frameworks already setup. Here, we create a SageMaker MXNet estimator and pass in our model training script, hyperparameters, as well as the number and type of training instances we want.\n",
    "\n",
    "We can then `fit` the estimator on the the training data location in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "from time import strftime, gmtime\n",
    "\n",
    "estimator = MXNet(\n",
    "    entry_point=\"train_dgl_mxnet_entry_point.py\",\n",
    "    source_dir=\"sagemaker_graph_fraud_detection/dgl_fraud_detection\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    framework_version=\"1.6.0\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters=params,\n",
    "    output_path=train_output,\n",
    "    code_location=train_output,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "training_job_name = \"{}-{}\".format(config.solution_prefix, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "print(\n",
    "    f\"You can go to SageMaker -> Training -> Hyperparameter tuning jobs -> a job name started with {training_job_name} to monitor training job status and details.\"\n",
    ")\n",
    "estimator.fit({\"train\": train_data}, job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is completed, the training instances are automatically stopped and SageMaker stores the trained model and evaluation results (on the test data) to a location in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the prediction output for the test data\n",
    "Current training process is transductive setting where the predicting columns of test dataset (not including the target column) are used to construct the graph and thus the test data are included in the training process. At the end of training, the predictions on the test dataset are generated and saved in the **train_output** in the s3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_output_path = os.path.join(train_output, estimator.latest_training_job.job_name, \"output\")\n",
    "!mkdir -p output_dgl_job\n",
    "!aws s3 cp --recursive $test_output_path output_dgl_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# open file\n",
    "tar = tarfile.open(os.path.join(\"output_dgl_job\", \"output.tar.gz\"), \"r:gz\")\n",
    "tar.extractall(\"output_dgl_job\")\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dgl_output = pd.read_csv(os.path.join(\"output_dgl_job\", \"preds.csv\"))\n",
    "dgl_raw_preds, dgl_preds = dgl_output[\"pred_proba\"], dgl_output[\"pred\"]\n",
    "\n",
    "result_dgl_no_hpo = print_metrics(test_data_df.iloc[:, 0], dgl_preds)\n",
    "fpr, tpr, _ = roc_curve(test_data_df.iloc[:, 0], dgl_raw_preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "result_dgl_no_hpo.append(roc_auc)\n",
    "result_dgl_no_hpo = pd.DataFrame(\n",
    "    result_dgl_no_hpo,\n",
    "    index=[\"F1\", \"Precision\", \"Recall\", \"Accuracy\", \"ROC_AUC\"],\n",
    "    columns=[\"DGL_No_HPO\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the results with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_xgboost_dgl = result_xgboost_with_hpo.join(result_dgl_no_hpo)\n",
    "print(result_xgboost_dgl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit SageMaker Estimator with HPO\n",
    "In this section we fit the SageMaker Estimator using DGL with HPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "# Static hyperparameters we do not tune\n",
    "hyperparameters = {\n",
    "    \"nodes\": \"features.csv\",\n",
    "    \"edges\": \"relation*\",\n",
    "    \"labels\": \"tags.csv\",\n",
    "    \"model\": \"rgcn\",\n",
    "    \"num-gpus\": 1,\n",
    "    \"n-layers\": 2,\n",
    "    \"optimizer\": \"adam\",\n",
    "}\n",
    "\n",
    "# Dynamic hyperparameters we want to tune and their searching ranges. For demonstartion purpose, we skip the architecture search by skipping tunning the hyperparameters such as 'skip_rnn_num_layers', 'rnn_num_layers', and etc.\n",
    "hyperparameter_ranges = {\n",
    "    \"batch-size\": CategoricalParameter([512, 1024, 2048, 10000]),\n",
    "    \"embedding-size\": CategoricalParameter([16, 32, 64, 128, 256, 512]),\n",
    "    \"n-neighbors\": IntegerParameter(800, 1200),\n",
    "    \"n-epochs\": IntegerParameter(10, 17),\n",
    "    \"lr\": ContinuousParameter(0.002, 0.1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "objective_metric_name = \"Validation F1\"\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"Validation F1\", \"Regex\": \"Validation F1 (\\\\S+)\"}\n",
    "]  # Root Relative Squared Error (RSE):\n",
    "objective_type = \"Maximize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "estimator_tuning = MXNet(\n",
    "    entry_point=\"train_dgl_mxnet_entry_point.py\",\n",
    "    source_dir=\"sagemaker_graph_fraud_detection/dgl_fraud_detection\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    framework_version=\"1.6.0\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters=params,\n",
    "    output_path=train_output,\n",
    "    code_location=train_output,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tuning_job_name = config.solution_prefix + \"-tuning-job\"\n",
    "print(\n",
    "    f\"You can go to SageMaker -> Training -> Hyperparameter tuning jobs -> a job name started with {tuning_job_name} to monitor HPO tuning status and details.\\n\"\n",
    "    f\"Note. You are unable to successfully run the following cells until the tuning job completes. This step may take around 2 hour.\"\n",
    ")\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator_tuning,  # using the estimator defined in previous section\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=30,\n",
    "    max_parallel_jobs=3,\n",
    "    objective_type=objective_type,\n",
    "    base_tuning_job_name=tuning_job_name,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tuner.fit({\"train\": train_data})\n",
    "\n",
    "hpo_training_job_time_duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.Session().client(\"sagemaker\")\n",
    "\n",
    "tuning_job_name = tuner.latest_tuning_job.name\n",
    "tuning_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuning_job_result = sm_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "\n",
    "status = tuning_job_result[\"HyperParameterTuningJobStatus\"]\n",
    "if status != \"Completed\":\n",
    "    print(\"Reminder: the tuning job has not been completed.\")\n",
    "\n",
    "job_count = tuning_job_result[\"TrainingJobStatusCounters\"][\"Completed\"]\n",
    "print(\"%d training jobs have completed\" % job_count)\n",
    "\n",
    "is_minimize = (\n",
    "    tuning_job_result[\"HyperParameterTuningJobConfig\"][\"HyperParameterTuningJobObjective\"][\"Type\"]\n",
    "    != \"Minimize\"\n",
    ")\n",
    "objective_name = tuning_job_result[\"HyperParameterTuningJobConfig\"][\n",
    "    \"HyperParameterTuningJobObjective\"\n",
    "][\"MetricName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner_analytics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "\n",
    "full_df = tuner_analytics.dataframe()\n",
    "\n",
    "if len(full_df) > 0:\n",
    "    df = full_df[full_df[\"FinalObjectiveValue\"] > -float(\"inf\")]\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values(\"FinalObjectiveValue\", ascending=False)\n",
    "        print(\"Number of training jobs with valid objective: %d\" % len(df))\n",
    "        print({\"lowest\": min(df[\"FinalObjectiveValue\"]), \"highest\": max(df[\"FinalObjectiveValue\"])})\n",
    "        pd.set_option(\"display.max_colwidth\", -1)  # Don't truncate TrainingJobName\n",
    "    else:\n",
    "        print(\"No training jobs have reported valid results yet.\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the prediction output for the test dataset from the best tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = df[df[\"TrainingJobStatus\"] == \"Completed\"]  # filter out the failed jobs\n",
    "output_path_best_tuning_job = os.path.join(train_output, df[\"TrainingJobName\"].iloc[0], \"output\")\n",
    "print(output_path_best_tuning_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p output_dgl_best_tuning_job\n",
    "!aws s3 cp --recursive $output_path_best_tuning_job output_dgl_best_tuning_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# open file\n",
    "tar = tarfile.open(os.path.join(\"output_dgl_best_tuning_job\", \"output.tar.gz\"), \"r:gz\")\n",
    "tar.extractall(\"output_dgl_best_tuning_job\")\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dgl_output_hpo = pd.read_csv(os.path.join(\"output_dgl_best_tuning_job\", \"preds.csv\"))\n",
    "dgl_hpo_raw_preds, dgl_hpo_preds = dgl_output_hpo[\"pred_proba\"], dgl_output_hpo[\"pred\"]\n",
    "\n",
    "result_dgl_with_hpo = print_metrics(test_data_df.values[:, 0], dgl_hpo_preds)\n",
    "fpr, tpr, _ = roc_curve(test_data_df.values[:, 0], dgl_hpo_raw_preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "result_dgl_with_hpo.append(roc_auc)\n",
    "result_dgl_with_hpo = pd.DataFrame(\n",
    "    result_dgl_with_hpo,\n",
    "    index=[\"F1\", \"Precision\", \"Recall\", \"Accuracy\", \"ROC_AUC\"],\n",
    "    columns=[\"DGL_With_HPO\"],\n",
    ")\n",
    "\n",
    "\n",
    "result_xgboost_dgl_full = result_xgboost_dgl.join(result_dgl_with_hpo)\n",
    "print(result_xgboost_dgl_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you are done using this notebook, delete the model artifacts and other resources to avoid any incurring charges.\n",
    "\n",
    "**Caution**: You need to manually delete resources that you may have created while running the notebook, such as Amazon S3 buckets for model artifacts, training datasets, processing artifacts, and Amazon CloudWatch log groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Endpoint Created from XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_hpo.delete_model()\n",
    "predictor_hpo.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/introduction_to_applying_machine_learning|fraud_detection_using_graph_neural_networks|fraud_detection_using_deep_graph_neural_networks.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
