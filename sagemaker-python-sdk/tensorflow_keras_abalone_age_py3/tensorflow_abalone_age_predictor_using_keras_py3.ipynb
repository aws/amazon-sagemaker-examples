{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Estimators with Keras and TensorFlow backend\n",
    "\n",
    "This tutorial covers how to create your own training script using the building\n",
    "blocks provided in `keras`, which will predict the ages of\n",
    "[abalones](https://en.wikipedia.org/wiki/Abalone) based on their physical\n",
    "measurements. You'll learn how to do the following:\n",
    "\n",
    "*   Construct a custom model function\n",
    "*   Configure a neural network using `keras`\n",
    "*   Define a training op for your model\n",
    "*   Define your model metric\n",
    "*   Generate and return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Abalone Age Predictor\n",
    "\n",
    "It's possible to estimate the age of an\n",
    "[abalone](https://en.wikipedia.org/wiki/Abalone) (sea snail) by the number of\n",
    "rings on its shell. However, because this task requires cutting, staining, and\n",
    "viewing the shell under a microscope, it's desirable to find other measurements\n",
    "that can predict age.\n",
    "\n",
    "The [Abalone Data Set](https://archive.ics.uci.edu/ml/datasets/Abalone) contains\n",
    "the following\n",
    "[feature data](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names)\n",
    "for abalone:\n",
    "\n",
    "| Feature        | Description                                               |\n",
    "| -------------- | --------------------------------------------------------- |\n",
    "| Length         | Length of abalone (in longest direction; in mm)           |\n",
    "| Diameter       | Diameter of abalone (measurement perpendicular to length; in mm)|\n",
    "| Height         | Height of abalone (with its meat inside shell; in mm)     |\n",
    "| Whole Weight   | Weight of entire abalone (in grams)                       |\n",
    "| Shucked Weight | Weight of abalone meat only (in grams)                    |\n",
    "| Viscera Weight | Gut weight of abalone (in grams), after bleeding          |\n",
    "| Shell Weight   | Weight of dried abalone shell (in grams)                  |\n",
    "\n",
    "The label to predict is number of rings, as a proxy for abalone age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environmentÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>WholeWeight</th>\n",
       "      <th>ShuckedWeight</th>\n",
       "      <th>VisceraWeight</th>\n",
       "      <th>ShellWeight</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.435</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.585</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.3545</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.655</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.545</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.545</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.3740</td>\n",
       "      <td>0.1695</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Diameter  Height  WholeWeight  ShuckedWeight  VisceraWeight  \\\n",
       "0   0.435     0.335   0.110        0.334         0.1355         0.0775   \n",
       "1   0.585     0.450   0.125        0.874         0.3545         0.2075   \n",
       "2   0.655     0.510   0.160        1.092         0.3960         0.2825   \n",
       "3   0.545     0.425   0.125        0.768         0.2940         0.1495   \n",
       "4   0.545     0.420   0.130        0.879         0.3740         0.1695   \n",
       "\n",
       "   ShellWeight  age  \n",
       "0       0.0965    7  \n",
       "1       0.2250    6  \n",
       "2       0.3700   14  \n",
       "3       0.2600   16  \n",
       "4       0.2300   13  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/abalone_train.csv', names=['Length','Diameter', 'Height', 'WholeWeight', 'ShuckedWeight', 'VisceraWeight','ShellWeight', 'age'])\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data to a S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-324346001917/abalone_dataset\n"
     ]
    }
   ],
   "source": [
    "s3_input_prefix = sagemaker_session.upload_data(path='data', key_prefix='abalone_dataset')\n",
    "print(s3_input_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-14 21:18:14        312 abalone_dataset/abalone_predict.csv\r\n",
      "2019-12-14 21:18:14      37298 abalone_dataset/abalone_test.csv\r\n",
      "2019-12-14 21:18:14     145915 abalone_dataset/abalone_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive $s3_input_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sagemaker_session.upload_data** will upload the abalone dataset from your machine to a bucket named **sagemaker-{your aws account number}**, if you don't have this bucket yet, sagemaker_session will create it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete source code\n",
    "Here is the full code for the network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 5488 Dec 14 21:12 main_train.py\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 1951 Dec 14 21:12 model_exporter_keras_to_pb.py\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   61 Dec 14 21:12 requirements.txt\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  298 Dec 14 21:12 setup.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*   **`setup.py & requirements.txt`** If you use setup.py and specify the dependencies in a requirements.txt, Sagemaker will pip install them for you when it launches the training job\n",
    "\n",
    "\n",
    "*  **`model_exporter_keras_to_pb.py`** This exports keras model into TensorFlow protobuf format.\n",
    "\n",
    "\n",
    "*  **`main_train.py`** This is the entry point file to start training.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\r\n",
      "This sample shows how to use python 3 with TensorFlow and SageMaker\r\n",
      "\"\"\"\r\n",
      "import argparse\r\n",
      "import logging\r\n",
      "\r\n",
      "import sys\r\n",
      "from keras.models import Sequential\r\n",
      "from keras.layers import Dense\r\n",
      "import numpy\r\n",
      "import os\r\n",
      "\r\n",
      "from model_exporter_keras_to_pb import ModelExporterKerasToProtobuf\r\n",
      "\r\n",
      "\r\n",
      "def input_transformer_load(filename):\r\n",
      "    logger = logging.getLogger(__name__)\r\n",
      "\r\n",
      "    data = numpy.loadtxt(filename, delimiter=\",\")\r\n",
      "    x = data[:, 0:7]\r\n",
      "    y = data[:, 7]\r\n",
      "\r\n",
      "    logger.info(\"Feature shape is {}, target shape is {}\".format(x.shape, y.shape))\r\n",
      "    return x, y\r\n",
      "\r\n",
      "\r\n",
      "def train(training_dir, training_filename, val_dir, val_filename, model_snapshotdir, epochs=10, batch_size=32):\r\n",
      "    \"\"\"\r\n",
      "    This is fully customisable code to train your model.\r\n",
      "    :param training_dir:\r\n",
      "    :param training_filename:\r\n",
      "    :param val_dir:\r\n",
      "    :param val_filename:\r\n",
      "    :param model_snapshotdir:\r\n",
      "    :param epochs:\r\n",
      "    :param batch_size:\r\n",
      "    :return: Returns the trained model\r\n",
      "    \"\"\"\r\n",
      "    # Step 1: Do your training here\r\n",
      "    # Define deep learning architecture\r\n",
      "    # In this this is a fully connected multi-layer perceptron\r\n",
      "    model = Sequential([Dense(10, activation='relu', name='input-layer'),\r\n",
      "                        Dense(10, activation='relu', name='hidden-layer'),\r\n",
      "                        Dense(1, activation='linear', name='output-layer')])\r\n",
      "\r\n",
      "    # For a mean squared error regression problem\r\n",
      "    # Using RMSProp optimiser with mean squared error \r\n",
      "    metrics = ['mse', 'mae', 'mape']\r\n",
      "    model.compile(optimizer='rmsprop',\r\n",
      "                  loss='mse', metrics=metrics)\r\n",
      "\r\n",
      "    # load train & test data\r\n",
      "    train_x, train_y = input_transformer_load(os.path.join(training_dir, training_filename))\r\n",
      "    val_x, val_y = input_transformer_load(os.path.join(val_dir, val_filename))\r\n",
      "    # Start training\r\n",
      "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(val_x, val_y))\r\n",
      "    # model evaluate\r\n",
      "    scores = model.evaluate(val_x, val_y)\r\n",
      "    # model save in keras default format\r\n",
      "    model_keras_path = os.path.join(model_snapshotdir, \"abalone_age_predictor.h5\")\r\n",
      "    model.save(model_keras_path)\r\n",
      "\r\n",
      "    # Step 2: Log your metrics in a special format so that it can be extracted using a regular expression.\r\n",
      "    # This allows SageMaker to report this metrics and allows hyper parameter tuning\r\n",
      "    # Note: Use a special marker for SageMaker to extract the metrics, say ## Metric ##\r\n",
      "    for i, m in enumerate(metrics):\r\n",
      "        print(\"## validation_metric_{} ##: {}\".format(m, scores[1+i]))\r\n",
      "\r\n",
      "    # Step 3: Save your model in pb format so that SageMaker TensorFlow container can serve this\r\n",
      "    model_pb_exporter = ModelExporterKerasToProtobuf()\r\n",
      "    # SageMaker TensorFlow serving container is capable of serving more than one model.\r\n",
      "    # The container expects the model.pb to be within a directory structure <model_name>/<model_version>\r\n",
      "    model_name = \"abalone_age_predictor\"\r\n",
      "    # Note: the name of the version directory should match the regex re.match('^\\d+$', dir)\r\n",
      "    model_version = \"1\"\r\n",
      "    # Follow this exact naming convention\r\n",
      "    model_pb_path = os.path.join(model_snapshotdir, model_name, model_version)\r\n",
      "    model_pb_exporter(model_keras_path, model_pb_path)\r\n",
      "\r\n",
      "    return model\r\n",
      "\r\n",
      "\r\n",
      "if '__main__' == __name__:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    # Train dir files\r\n",
      "    parser.add_argument(\"--traindata\", help=\"The input file wrt to the training directory\", required=True)\r\n",
      "    # The environment variable SM_CHANNEL_TRAIN is defined\r\n",
      "    parser.add_argument('--traindata-dir',\r\n",
      "                        help='The directory containing training artifacts such as training data',\r\n",
      "                        default=os.environ.get('SM_CHANNEL_TRAIN', \".\"))\r\n",
      "    # val dir files\r\n",
      "    parser.add_argument(\"--validationdata\", help=\"The validation input file wrt to the val directory\", required=True)\r\n",
      "    parser.add_argument('--validationdata-dir',\r\n",
      "                        help='The directory containing validation artifacts such as validation data',\r\n",
      "                        default=os.environ.get('SM_CHANNEL_VALIDATION', \".\"))\r\n",
      "\r\n",
      "    # output dir to save any files such as predictions, logs, etc\r\n",
      "    parser.add_argument(\"--outputdir\", help=\"The output dir to save results\",\r\n",
      "                        default=os.environ.get('SM_OUTPUT_DATA_DIR', \"result_data\")\r\n",
      "                        )\r\n",
      "\r\n",
      "    parser.add_argument(\"--model_dir\", help=\"Do not use this.. required by SageMaker\", default=None)\r\n",
      "\r\n",
      "    # This is where the model needs to be saved to\r\n",
      "    parser.add_argument(\"--snapshot_dir\", help=\"The directory to save the snapshot to..\",\r\n",
      "                        default=os.environ.get('SM_MODEL_DIR', \".\"))\r\n",
      "\r\n",
      "    # Additional parameters for your code\r\n",
      "    parser.add_argument(\"--epochs\", help=\"The number of epochs\", default=10, type=int)\r\n",
      "    parser.add_argument(\"--batch-size\", help=\"The mini batch size\", default=30, type=int)\r\n",
      "\r\n",
      "    parser.add_argument(\"--log-level\", help=\"Log level\", default=\"INFO\", choices={\"INFO\", \"WARN\", \"DEBUG\", \"ERROR\"})\r\n",
      "\r\n",
      "    args = parser.parse_args()\r\n",
      "\r\n",
      "    print(\"Arguments passed\", args.__dict__)\r\n",
      "\r\n",
      "    # Set up logging\r\n",
      "    logging.basicConfig(level=logging.getLevelName(args.log_level), handlers=[logging.StreamHandler(sys.stdout)],\r\n",
      "                        format='%(asctime)s %(name)s %(levelname)s %(process)d/%(threadName)s - %(message)s')\r\n",
      "\r\n",
      "    # set up dir\r\n",
      "    if not os.path.isdir(args.outputdir):\r\n",
      "        os.makedirs(args.outputdir)\r\n",
      "\r\n",
      "    train(args.traindata_dir, args.traindata, args.validationdata_dir, args.validationdata, args.snapshot_dir,\r\n",
      "          args.epochs, args.batch_size)\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'source/main_train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*   **`Environment variable: SM_MODEL_DIR `**  This is where the model needs to be saved to in tensorflow protobof format. This is required for the tensorflow serving container.\n",
    "`\n",
    "\n",
    "*   **`Model Saving`** The model must be saved in TensorFlow protobuf format for the default serving container to work. The default setting uses SageMaker TensorFlow serving container, which is capable of serving more than one model. Hence the container expects the saved_model.pb to be within a directory structure model_name/model_version.\n",
    "\n",
    "\n",
    "* **`Model Metric`** Model metric is printed in the console, so a regex can be used to extract the metrics. E.g the regex **`## validation_metric_mse ##: (\\d*[.]?\\d*)`** matches the following print\n",
    "    ```python\n",
    "    print(\"## validation_metric_{} ##: {}\".format(\"mse\", scores[1+i]))\n",
    "    ```\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run local-local no sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Arguments passed {'traindata': 'abalone_train.csv', 'traindata_dir': 'data', 'validationdata': 'abalone_test.csv', 'validationdata_dir': 'data', 'outputdir': 'result_data', 'model_dir': None, 'snapshot_dir': '.', 'epochs': 5, 'batch_size': 10, 'log_level': 'INFO'}\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2019-12-14 21:18:20,013 tensorflow WARNING 20286/MainThread - From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2019-12-14 21:18:20,027 tensorflow WARNING 20286/MainThread - From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2019-12-14 21:18:20,063 __main__ INFO 20286/MainThread - Feature shape is (3320, 7), target shape is (3320,)\n",
      "2019-12-14 21:18:20,071 __main__ INFO 20286/MainThread - Feature shape is (850, 7), target shape is (850,)\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2019-12-14 21:18:20,071 tensorflow WARNING 20286/MainThread - From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2019-12-14 21:18:20,072 tensorflow WARNING 20286/MainThread - From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "2019-12-14 21:18:20,195 tensorflow WARNING 20286/MainThread - From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "2019-12-14 21:18:20,199 tensorflow WARNING 20286/MainThread - From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 3320 samples, validate on 850 samples\n",
      "Epoch 1/5\n",
      "2019-12-14 21:18:20.246146: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-12-14 21:18:20.270365: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\n",
      "2019-12-14 21:18:20.270659: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55605dda3380 executing computations on platform Host. Devices:\n",
      "2019-12-14 21:18:20.270680: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-12-14 21:18:20.270869: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-12-14 21:18:20.351354: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "3320/3320 [==============================] - 2s 499us/step - loss: 82.8225 - mean_squared_error: 82.8225 - mean_absolute_error: 8.4924 - mean_absolute_percentage_error: 85.2094 - val_loss: 40.9161 - val_mean_squared_error: 40.9161 - val_mean_absolute_error: 5.7439 - val_mean_absolute_percentage_error: 55.3077\n",
      "Epoch 2/5\n",
      "3320/3320 [==============================] - 1s 199us/step - loss: 13.3365 - mean_squared_error: 13.3365 - mean_absolute_error: 2.6873 - mean_absolute_percentage_error: 25.4582 - val_loss: 8.2329 - val_mean_squared_error: 8.2329 - val_mean_absolute_error: 2.0400 - val_mean_absolute_percentage_error: 18.8378\n",
      "Epoch 3/5\n",
      "3320/3320 [==============================] - 1s 199us/step - loss: 7.5011 - mean_squared_error: 7.5011 - mean_absolute_error: 1.9749 - mean_absolute_percentage_error: 18.9273 - val_loss: 7.6953 - val_mean_squared_error: 7.6953 - val_mean_absolute_error: 1.9346 - val_mean_absolute_percentage_error: 17.8928\n",
      "Epoch 4/5\n",
      "3320/3320 [==============================] - 1s 198us/step - loss: 7.0485 - mean_squared_error: 7.0485 - mean_absolute_error: 1.9166 - mean_absolute_percentage_error: 18.7752 - val_loss: 7.4022 - val_mean_squared_error: 7.4022 - val_mean_absolute_error: 1.9085 - val_mean_absolute_percentage_error: 18.0125\n",
      "Epoch 5/5\n",
      "3320/3320 [==============================] - 1s 198us/step - loss: 6.8252 - mean_squared_error: 6.8252 - mean_absolute_error: 1.8970 - mean_absolute_percentage_error: 18.9216 - val_loss: 7.2477 - val_mean_squared_error: 7.2477 - val_mean_absolute_error: 1.8981 - val_mean_absolute_percentage_error: 18.1655\n",
      "850/850 [==============================] - 0s 33us/step\n",
      "## validation_metric_mse ##: 7.247718172634349\n",
      "## validation_metric_mae ##: 1.8980544166003956\n",
      "## validation_metric_mape ##: 18.16550599490895\n",
      "2019-12-14 21:18:24,626 model_exporter_keras_to_pb INFO 20286/MainThread - Loading model ./abalone_age_predictor.h5 to save as proto bof format at ./abalone_age_predictor/1\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "2019-12-14 21:18:25,040 tensorflow WARNING 20286/MainThread - From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/f/amazon-sagemaker-examples/sagemaker-python-sdk/tensorflow_keras_abalone_age_py3/source/model_exporter_keras_to_pb.py:39: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\n",
      "2019-12-14 21:18:25,040 tensorflow WARNING 20286/MainThread - From /home/ec2-user/SageMaker/f/amazon-sagemaker-examples/sagemaker-python-sdk/tensorflow_keras_abalone_age_py3/source/model_exporter_keras_to_pb.py:39: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\n",
      "INFO:tensorflow:No assets to save.\n",
      "2019-12-14 21:18:25,040 tensorflow INFO 20286/MainThread - No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "2019-12-14 21:18:25,040 tensorflow INFO 20286/MainThread - No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: ./abalone_age_predictor/1/saved_model.pb\r\n",
      "2019-12-14 21:18:25,129 tensorflow INFO 20286/MainThread - SavedModel written to: ./abalone_age_predictor/1/saved_model.pb\r\n",
      "2019-12-14 21:18:25,129 model_exporter_keras_to_pb INFO 20286/MainThread - Model saved to ./abalone_age_predictor/1/saved_model.pb\r\n"
     ]
    }
   ],
   "source": [
    "!python source/main_train.py  --traindata abalone_train.csv --traindata-dir data --validationdata abalone_test.csv --validationdata-dir data --batch-size 10 --epochs 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting script for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Git config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is optional commit id\n",
    "# commit_id = \"e4f5a6bca3b22da7ccda947d0349bcb7c43af3ca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_config = {'repo': 'https://github.com/elangovana/amazon-sagemaker-examples.git',\n",
    "              'branch': 'master',\n",
    "              # This is optional commit id, when not provided gets the latest\n",
    "              # 'commit': commit_id\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source directory\n",
    " \n",
    "Path relative to the root source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'sagemaker-python-sdk/tensorflow_keras_abalone_age_py3/source'\n",
    "entry_point_file = 'main_train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric definitions\n",
    "Plots these on sagemaker console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_def = [\n",
    " {\"Name\": \"val:mean_squared_error\",\n",
    " \"Regex\": \"## validation_metric_mse ##: (\\d*[.]?\\d*)\"}\n",
    ",{\"Name\": \"val:mean_absolute_error\",\n",
    " \"Regex\": \"## validation_metric_mae ##: (\\d*[.]?\\d*)\"}\n",
    ",{\"Name\": \"val:mean_absolute_percentage_error\",\n",
    " \"Regex\": \"## validation_metric_mape ##: (\\d*[.]?\\d*)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training mode: local vs remote instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type =    \"ml.c4.xlarge\"  # 'local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use spot instances\n",
    "\n",
    "Only valid when **not in** local mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set if you need spot instance\n",
    "use_spot = True\n",
    "train_max_run_secs =   24 * 60 * 60\n",
    "# Max wait time  5 minutes + train time\n",
    "max_wait_time_secs = train_max_run_secs +  5 * 60\n",
    "\n",
    "\n",
    "# During local mode, no spot..\n",
    "if train_instance_type == 'local':\n",
    "    use_spot = False\n",
    "\n",
    "    max_wait_time_secs = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {'traindata' : 'abalone_train.csv',\n",
    "     'validationdata' : 'abalone_test.csv',\n",
    "    'epochs': 10, \n",
    "    'batch-size': 32}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit training job\n",
    "\n",
    "We can use the SDK to run our local training script on SageMaker infrastructure.\n",
    "\n",
    "1. Pass the path to the abalone.py file, which contains the functions for defining your estimator, to the sagemaker.TensorFlow init method.\n",
    "2. Pass the S3 location that we uploaded our data to previously to the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-14 21:18:28 Starting - Starting the training job...\n",
      "2019-12-14 21:18:30 Starting - Launching requested ML instances......."
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from time import gmtime, strftime\n",
    "\n",
    "s3_model_path = \"s3://{}/models\".format(sagemaker_session.default_bucket())\n",
    "\n",
    "job_name = \"ablone-age-py3-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "\n",
    "abalone_estimator = TensorFlow(entry_point='main_train.py',\n",
    "                               source_dir=source_dir,\n",
    "                               role=role,\n",
    "                               py_version=\"py3\",\n",
    "                               git_config = git_config,\n",
    "                               framework_version = \"1.11.0\",\n",
    "                               hyperparameters=hp,\n",
    "                               model_dir = s3_model_path,\n",
    "                               metric_definitions = metric_def,\n",
    "                               train_instance_count=1,\n",
    "                               train_use_spot_instances = use_spot,\n",
    "                               train_max_run =  train_max_run_secs,\n",
    "                               # NOTE: if in spot mode, the train_max_wait  needs to be commented out\n",
    "                               train_max_wait = max_wait_time_secs     ,                         \n",
    "                               train_instance_type=train_instance_type)\n",
    "\n",
    "abalone_estimator.fit( {'train': s3_input_prefix, \n",
    "                        'validation':s3_input_prefix}, \n",
    "                      job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`estimator.fit` will deploy a script in a container for training and returns the SageMaker model name using the following arguments:\n",
    "\n",
    "*   **`entry_point=\"main_train.py\"`** The path to the script that will be deployed to the container.\n",
    "*   **`training_steps=100`** The number of training steps of the training job.\n",
    "*   **`evaluation_steps=100`** The number of evaluation steps of the training job.\n",
    "*   **`role`**. AWS role that gives your account access to SageMaker training and hosting\n",
    "*   **`hyperparameters={'epochs' :10, ''batch-size:32}`**. Training hyperparameters. \n",
    "\n",
    "Running the code block above will do the following actions:\n",
    "* deploy your script in a container with tensorflow installed\n",
    "* Pip install the dependencies in the requirements.txt for you.\n",
    "* copy the data from the bucket to the container\n",
    "* save the estimator model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse training job - Only valid in non-local / mangaged mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download analytics and convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sagemaker.analytics import TrainingJobAnalytics\n",
    "\n",
    "\n",
    "training_job_name = job_name\n",
    "metric_name = 'val:mean_squared_error'\n",
    "\n",
    "metrics_dataframe = TrainingJobAnalytics(training_job_name=training_job_name,metric_names=[metric_name]).dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use matplotlib to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = metrics_dataframe.plot( x='timestamp', y='value', style='b.', legend=False)\n",
    "ax.set_ylabel(metric_name);\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submiting a trained model for hosting\n",
    "\n",
    "The deploy() method creates an endpoint which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_predictor = abalone_estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join('data','abalone_predict.csv'), header=None, names = ['Length', 'Diameter', 'Height', 'WholeWeight', 'ShuckedWeight', 'VisceraWeight', 'ShellWeight', 'Age'])\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = test_data[test_data.columns.difference(['Age'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  abalone_predictor.predict(features.values)['predictions']\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "predictions=list(itertools.chain.from_iterable(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame({'actual':test_data.Age.values, 'predictions':predictions} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_predictor.delete_endpoint(abalone_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
