{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Images for Built-in Algorithms (Part 3/4)\n",
    "Download | Structure | **Preprocess** | Train  \n",
    "\n",
    "\n",
    "**Note**: this notebook should be used with the conda_amazonei_mxnet_p36 kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* #### Dependencies\n",
    "* #### Application/x-image\n",
    "* #### Application/x-recordio\n",
    "* #### Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "___\n",
    "For this guide we'll use the SageMaker Python SDK version 2.9.2. By default, SageMaker Notebooks come with version 1.72.0. Other guides provided by Amazon may be set up to work with other versions of the Python SDK so you may wish to roll-back to 1.72.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "original_sagemaker_version = !conda list | grep -E \"sagemaker\\s\" | awk '{print $2}'\n",
    "!{sys.executable} -m pip install -q \"sagemaker==2.9.2\" \"opencv-python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import boto3\n",
    "import shutil\n",
    "import urllib\n",
    "import pickle\n",
    "import pathlib\n",
    "import sagemaker\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker updated  1.72.1 -> 2.9.2\n"
     ]
    }
   ],
   "source": [
    "print(f'sagemaker updated  {original_sagemaker_version[0]} -> {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Category Labels\n",
    "The `category_labels` file was generated from the first notebook in this series `01_download_data.ipynb`. You will need to run that notebook before running the code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickled_data/category_labels.pickle', 'rb') as f:\n",
    "    category_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## application/x-image\n",
    "___\n",
    "\n",
    "This format is also referred to as \"Image Format\" or \"LST\" format. The benefit of using this format is that it doesn't require any modification or restructuring of your dataset. Instead, you create a manifest of the images for your training set and validation set. These two manifests are separate `.lst` files which list all the images giving each of them a unique index, the class they belong to and the relative path to the image file from the main training folder. The data in the `.lst` file is in tab separated values.\n",
    "\n",
    "While its the easiest format to use, it requires SageMaker to do more work behind the scenes. For datasets with many images, this will cause training to take longer. For datasets with fewer images, the performance difference isn't as pronounced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Manually generate the .LST files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bear': 0, 'bird': 1, 'cat': 2, 'cow': 3, 'dog': 4, 'elephant': 5, 'frog': 6, 'giraffe': 7, 'horse': 8, 'sheep': 9, 'zebra': 10}\n"
     ]
    }
   ],
   "source": [
    "category_ids = {name: idx for idx, name in enumerate(sorted(category_labels.values()))}\n",
    "print(category_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = pathlib.Path('./data_structured').rglob('*.jpg')\n",
    "\n",
    "for idx, p in enumerate(image_paths):\n",
    "    image_id = f'{idx:010}'\n",
    "    category = category_ids[p.parts[-2]]\n",
    "    path = p.as_posix()\n",
    "    split = p.parts[-3]\n",
    "    with open(f'{split}.lst', 'a') as f:\n",
    "        line = f\"{image_id}\\t{category}\\t{path}\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000000\t4\tdata_structured/train/dog/000000110395.jpg\n",
      "0000000001\t4\tdata_structured/train/dog/000000239881.jpg\n",
      "0000000002\t4\tdata_structured/train/dog/000000545720.jpg\n",
      "0000000003\t4\tdata_structured/train/dog/000000174305.jpg\n",
      "0000000004\t4\tdata_structured/train/dog/000000564053.jpg\n",
      "0000000005\t4\tdata_structured/train/dog/000000135748.jpg\n",
      "0000000006\t4\tdata_structured/train/dog/000000530624.jpg\n",
      "0000000007\t4\tdata_structured/train/dog/000000377575.jpg\n",
      "0000000008\t4\tdata_structured/train/dog/000000194540.jpg\n",
      "0000000009\t4\tdata_structured/train/dog/000000380893.jpg\n"
     ]
    }
   ],
   "source": [
    "!head train.lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Use im2rec.py script to generate the .LST files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_url = 'https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py'\n",
    "urllib.request.urlretrieve(script_url, \"im2rec.py\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python im2rec.py --list --recursive LST_FILE_PREFIX DATA_DIR`\n",
    "* --list - generate an LST file\n",
    "* --recursive - looks inside subfolders for image data\n",
    "* LST_FILE_PREFIX - choose the name you want for the `.lst` file\n",
    "* DATA_DIR - relative path to directory with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bear 0\n",
      "bird 1\n",
      "cat 2\n",
      "cow 3\n",
      "dog 4\n",
      "elephant 5\n",
      "frog 6\n",
      "giraffe 7\n",
      "horse 8\n",
      "sheep 9\n",
      "zebra 10\n"
     ]
    }
   ],
   "source": [
    "!python im2rec.py --list --recursive train data_structured/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bear 0\n",
      "bird 1\n",
      "cat 2\n",
      "cow 3\n",
      "dog 4\n",
      "elephant 5\n",
      "frog 6\n",
      "giraffe 7\n",
      "horse 8\n",
      "sheep 9\n",
      "zebra 10\n"
     ]
    }
   ],
   "source": [
    "!python im2rec.py --list --recursive val data_structured/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\t9.000000\tsheep/000000574928.jpg\n",
      "121\t0.000000\tbear/000000359337.jpg\n",
      "474\t2.000000\tcat/000000186635.jpg\n",
      "877\t4.000000\tdog/000000187167.jpg\n",
      "1840\t9.000000\tsheep/000000090683.jpg\n",
      "536\t2.000000\tcat/000000333929.jpg\n",
      "1769\t8.000000\thorse/000000422878.jpg\n",
      "1104\t5.000000\telephant/000000262979.jpg\n",
      "1174\t5.000000\telephant/000000460403.jpg\n",
      "1139\t5.000000\telephant/000000362284.jpg\n"
     ]
    }
   ],
   "source": [
    "!head train.lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## application/x-recordio (preferred)\n",
    "___\n",
    "This format is commonly referred to as RecordIO. It creates a new file for your each of your training and validation datasets with the `.rec` suffix. The `.rec` file is a single file that contains all of the images in the dataset so it can be streamed directly to the SageMaker training algorithm without the overhead involved with transfering thousands of individual files. For datasets with many images this provides a huge reduction in training time because SageMaker doesn't need to download all the image files before it can run the training algorithm. If you use the `im2rec.py` script, it will also resize the images for you as well. The benefits of resizing the files before saving them in the RecordIO format is that it'll reduce the amount of data you need to transfer to s3 and will also speed up trainging by doing the resizing ahead of time instead of at training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Run Option 2 from application/x-image above and copy LST files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_recordio/val.lst'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recordio_dir = pathlib.Path('./data_recordio')\n",
    "recordio_dir.mkdir(exist_ok=True)\n",
    "shutil.copy('train.lst', 'data_recordio/')\n",
    "shutil.copy('val.lst', 'data_recordio/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate .rec files in the RecordIO Format\n",
    "Once the `.lst` file is generated, the same `im2rec.py` script will also generate the `.rec` file.\n",
    "\n",
    "`python im2rec.py --resize 224 --quality 90 --num-thread 16 LST_FILE_PREFIX DATA_DIR/`\n",
    "* **--resize**: Have the script resize the files before saving them all to a `.rec` file. For the image classification algorithm the default dimensions are 224x224. Resizing now will also reduce the size of your `.rec` file.\n",
    "* **--quality**: Default settings will save the image data uncompressed. Adding some compression will keep the filesize of your `.rec` down especially if you're not resizing them.\n",
    "* **--num_thread**: Set how many threads to parallelize the work\n",
    "* **--LST_FILE_PREFIX**: Name of the `.lst` you're referencing for creating the `.rec` file\n",
    "* **--DATA_DIR**: Relative path directory which holds the data listed in the `.lst` file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .rec file from /home/ec2-user/SageMaker/sagemaker-notebooks-WIP/sagemaker_image_data_guide/data_recordio/train.lst in /home/ec2-user/SageMaker/sagemaker-notebooks-WIP/sagemaker_image_data_guide/data_recordio\n",
      "time: 0.2049543857574463  count: 0\n",
      "time: 5.9312732219696045  count: 1000\n",
      "time: 5.552445888519287  count: 2000\n"
     ]
    }
   ],
   "source": [
    "!python im2rec.py --resize 224 --quality 90 --num-thread 16 data_recordio/train data_structured/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .rec file from /home/ec2-user/SageMaker/sagemaker-notebooks-WIP/sagemaker_image_data_guide/data_recordio/val.lst in /home/ec2-user/SageMaker/sagemaker-notebooks-WIP/sagemaker_image_data_guide/data_recordio\n",
      "time: 0.0018703937530517578  count: 0\n"
     ]
    }
   ],
   "source": [
    "!python im2rec.py --resize 224 --quality 90 --num-thread 16 data_recordio/val data_structured/val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a bucket for your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Name: sagemaker-prebuilt-ic-2a789e07-853b-4b89-b0eb-8ea33550520e\n"
     ]
    }
   ],
   "source": [
    "if pathlib.Path('pickled_data/prebuilt_bucket_name.pickle').exists():\n",
    "    with open('pickled_data/prebuilt_bucket_name.pickle', 'rb') as f:\n",
    "        bucket_name = pickle.load(f)\n",
    "        print('Bucket Name:', bucket_name)\n",
    "else:\n",
    "    bucket_name = f'sagemaker-prebuilt-ic-{str(uuid.uuid4())}'\n",
    "    s3 = boto3.resource('s3')\n",
    "    region = sagemaker.Session().boto_region_name\n",
    "    bucket_config = {'LocationConstraint': region}\n",
    "    s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=bucket_config)\n",
    "    \n",
    "    with open('pickled_data/prebuilt_bucket_name.pickle', 'wb') as f:\n",
    "        pickle.dump(bucket_name, f)\n",
    "    print('Bucket Name:', bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload .rec files to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEEDS UPDATING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uploader = sagemaker.s3.S3Uploader()\n",
    "\n",
    "data_path = recordio_dir / 'train.rec'\n",
    "\n",
    "data_s3_uri = s3_uploader.upload(\n",
    "    local_path=data_path.as_posix(), \n",
    "    desired_s3_uri=f's3://{bucket_name}/data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = recordio_dir / 'val.rec'\n",
    "\n",
    "data_s3_uri = s3_uploader.upload(\n",
    "    local_path=data_path.as_posix(), \n",
    "    desired_s3_uri=f's3://{bucket_name}/data/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollback to default version of SDK and TensorFlow\n",
    "Only do this if you're done with this guide and want to use the same kernel for other notebooks with an incompatible version of the SageMaker SDK or TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Original version: {original_sagemaker_version[0]}')\n",
    "# print(f'Current version:  {sagemaker.__version__}')\n",
    "# print('')\n",
    "# print(f'Rolling back to {original_sagemaker_version[0]}')\n",
    "# print('Restart notebook kernel to use changes.')\n",
    "# print('')\n",
    "# s = f'sagemaker=={original_sagemaker_version[0]}'\n",
    "# !{sys.executable} -m pip install -q {s}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
