{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable TensorFlow models with Pipe mode and distributed training\n",
    "\n",
    "New data scientists and machine learning engineers have a treasure trove of examples available on the internet to help them get started. These examples typically leverage small public datasets and demonstrate common use cases and approaches. The data in these examples can be downloaded quickly to a training instance and training can be completed typically in minutes. However, many customers have large scale datasets for machine learning that make the simple approach of downloading the full dataset prohibitive. Imagine your training algorithm waiting for a download of 100GB of images for a travel web site, 100TB of video, or even 10PB of monitoring data from heart patients worldwide. Likewise, large scale training jobs can take days to run on a single instance.\n",
    "\n",
    "Amazon SageMaker provides Pipe mode and distributed training for TensorFlow developers for exactly this purpose. Pipe mode lets you establish a channel to your dataset and feed your training algorithm batches of that data incrementally. Your training can start quickly, and you can train on an infinite size dataset. Distributed training lets you reduce training job durations by adding more training instances to parallelize the training. These scaling capabilities work well with SageMaker's TensorFlow container, allowing data scientists to bring their own TensorFlow scripts without having to do the heavier lifting of building Docker containers or standing up machine learning clusters.\n",
    "\n",
    "While there are several examples available on the use of Pipe mode, not all possible scenarios and use cases are covered. This example notebook provides an end to end example for approaching TensorFlow training with large datasets and projected long training durations. It includes use of the following:\n",
    "\n",
    "1. **Script mode** using SageMaker's TensorFlow container and a custom TensorFlow neural network.\n",
    "2. **Pipe mode** to incrementally stream data to the training algorithm. \n",
    "3. Data stored in **TFRecords** format.\n",
    "4. Multiple data channels each containing **multiple files**.\n",
    "5. Data **sharding by S3 keys**.\n",
    "6. Training across **multiple training instances** using SageMaker's built in **parameter server**, plus proper handling of **saving the model artifacts** from only the master node.\n",
    "7. Definition of **SageMaker training metrics** to support experimentation, visualization, and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple synthetic classification dataset\n",
    "\n",
    "For this example, we use a simple numeric dataset that we will use for binary classification. With the focus of this notebook on quickly and easily demonstrating pipe mode, our synthetic dataset has a configurable number of features and samples. Feel free to scale it up to see the approach in action on large datasets. To get started, we create a synthetic dataset and split it into train, test, and validation. \n",
    "\n",
    "Note that for showcasing distributed training, we use a larger dataset. You may need to use an ml.t3.2xlarge notebook instance to have sufficient memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works in a few different modes. If you are wanting to run through it quickly to get a feel for the code and what it takes to do Pipe mode, leave it at `fast_demo`. If you would like to see how the use of multiple training instances can dramatically speed your training job, run the notebook twice:\n",
    "\n",
    "1. Once using `one_slow_node` to see how long a job would take with a single node and a modest amount of data. This serves as a baseline. \n",
    "2. Then run it in `speedy_cluster` mode to run the same exact scenario with more training instances. You will see a significant improvement in training job duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = 'ml.c5.xlarge' \n",
    "serve_instance_type = 'ml.m4.xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'fast-demo' # 'fast-demo' or 'one-slow-node' or 'speedy-cluster'\n",
    "\n",
    "if MODE == 'fast-demo':  ## 3-minute training job\n",
    "    TRAIN_INSTANCE_COUNT = 1\n",
    "    NUM_SAMPLES  = 10000\n",
    "    NUM_FEATURES = 50\n",
    "    NUM_FILES    = 6\n",
    "    NUM_EPOCHS   = 200\n",
    "    BASE_JOB_NAME = 'tf-pipe-fast-demo'\n",
    "else:\n",
    "    NUM_SAMPLES  = 90000\n",
    "    NUM_FEATURES = 5000\n",
    "    NUM_FILES    = 21 # NOTE: For ideal splicing of data across nodes, make this a multiple of TRAIN_INSTANCE_COUNT.\n",
    "    NUM_EPOCHS   = 5000\n",
    "\n",
    "    if MODE == 'one-slow-node':   ## 80-minute training job\n",
    "        TRAIN_INSTANCE_COUNT = 1\n",
    "        BASE_JOB_NAME = 'tf-pipe-one-slow-node'\n",
    "    else:\n",
    "        TRAIN_INSTANCE_COUNT = 3  ## 30-minute training job\n",
    "        BASE_JOB_NAME = 'tf-pipe-speedy-cluster-' + str(TRAIN_INSTANCE_COUNT)\n",
    "\n",
    "# NOTE: For ideal splicing of data across nodes, make NUM_FILES a multiple of TRAIN_INSTANCE_COUNT.\n",
    "\n",
    "BATCH_SIZE   = 64\n",
    "INPUT_MODE   = 'Pipe' # Can try it with 'File' mode as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training script uses the number of features\n",
    "to define the input shape for a simple TensorFlow neural network. Here we use a `sed` script\n",
    "ensure the input shape is consistent across the training script and the notebook generating the dataset and the \n",
    "training files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed 's/NUM_FEATURES = /NUM_FEATURES = {NUM_FEATURES} \\#/' scripts/train.py > scripts/tmp.py\n",
    "!mv scripts/tmp.py scripts/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize scripts/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the dataset and split it across train, test, and val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "X1, Y1 = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_redundant=0, \n",
    "                             n_informative=1, n_classes=2, n_clusters_per_class=1, \n",
    "                             shuffle=True, class_sep=2.0)\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "val_size  = 0.20\n",
    "test_size = 0.10\n",
    "\n",
    "# Give 70% to train\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X1, Y1, test_size=(test_size + val_size), random_state=seed)\n",
    "# Of the remaining 30%, give 2/3 to validation and 1/3 to test\n",
    "X_test, X_val, y_test, y_val     = \\\n",
    "    train_test_split(X_test, y_test, test_size=(test_size / (test_size + val_size)), \n",
    "                     random_state=seed)\n",
    "\n",
    "print('Train shape: {}, Test shape: {}, Val shape: {}'.format(X_train.shape, \n",
    "                                                              X_test.shape, X_val.shape))\n",
    "print('Train target: {}, Test target: {}, Val target: {}'.format(y_train.shape, \n",
    "                                                                 y_test.shape, y_val.shape))\n",
    "print('\\nSample observation: {}\\nSample target: {}'.format(X_test[0], y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we capture how many samples are in each channel. We will be passing this to the training script to define the number of\n",
    "steps per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = X_train.shape[0]\n",
    "num_val_samples   = X_val.shape[0]\n",
    "num_test_samples  = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data to TFRecord files\n",
    "\n",
    "Pipe mode supports RecordIO, TFRecord, and TextLine. Here we will use TFRecord format, and for each of train, test, and val, we generate a set of files so we can see how Pipe mode is able to deal with sets of files. We divide the dataset into a configurable set of slices and save each slice to a separate file. If we were dealing with a massive dataset, dividing the data into separate files makes it easier to feed the data to your training algorithm, as well as facilitating training across a cluster of machines to reduce training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tfr(x, y, out_file):\n",
    "    with tf.python_io.TFRecordWriter(out_file) as record_writer:\n",
    "      num_samples = len(x)\n",
    "      for i in range(num_samples):\n",
    "        example = tf.train.Example()\n",
    "        example.features.feature['features'].float_list.value.extend(x[i])\n",
    "        example.features.feature['label'].int64_list.value.append(int(y[i]))\n",
    "        record_writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove old data directories and files if they exist. Recreate a data folder with subfolders for each of the three channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('data', ignore_errors=True)\n",
    "os.makedirs('data/train')\n",
    "os.makedirs('data/test')\n",
    "os.makedirs('data/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save each of the datasets into their own folder of files based on the configurable number of files. The data will be split as evenly as possible across that number of files in each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_n_files(x, y, n_files, channel):\n",
    "    _split_x = np.array_split(x, n_files)\n",
    "    _split_y = np.array_split(y, n_files)\n",
    "    for i in range(n_files):\n",
    "        convert_to_tfr(_split_x[i], _split_y[i], \n",
    "                       './data/{}/{}{}.tfrecords'.format(channel, channel, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "save_to_n_files(X_train, y_train, NUM_FILES, 'train')\n",
    "save_to_n_files(X_test,  y_test,  NUM_FILES, 'test')\n",
    "save_to_n_files(X_val,   y_val,   NUM_FILES, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free up memory as these portions of the synthetic dataset are no longer needed in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del y_train\n",
    "del X_val\n",
    "del y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the input data to S3\n",
    "Save the entire data folder hierarchy up to S3. For channels configured with `Pipe` mode, the data will be piped to the training job as the training algorithm progresses. If using `File` mode, the entire set of files for each data channel will be downloaded to the training instance at the start of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "data_prefix = 'data/DEMO-hello-pipe-mode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# clear out any old data\n",
    "s3 = boto3.resource('s3')\n",
    "s3_bucket = s3.Bucket(bucket)\n",
    "s3_bucket.objects.filter(Prefix=data_prefix + '/').delete()\n",
    "\n",
    "# upload the entire set of data for all three channels\n",
    "inputs = sagemaker_session.upload_data(path='data', key_prefix=data_prefix)\n",
    "print('Data was uploaded to s3 at: {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a training job using the `TensorFlow` estimator\n",
    "\n",
    "The `sagemaker.tensorflow.TensorFlow` estimator handles locating the script mode container, uploading your script to a S3 location and creating a SageMaker training job. Note that we provide metric definitions to track validation accuracy and validation loss. By providing these definitions, we will now be able to see these charted on the training job detail page in the console. Likewise, we can navigate to the CloudWatch algorithm metrics for the job for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "hyperparameters = {'epochs'    : NUM_EPOCHS, 'batch_size': BATCH_SIZE,\n",
    "                   'num_train_samples': num_train_samples,\n",
    "                   'num_val_samples'  : num_val_samples,\n",
    "                   'num_test_samples' : num_test_samples}\n",
    "\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                            source_dir='scripts',\n",
    "                            input_mode=INPUT_MODE,\n",
    "                            train_instance_type=train_instance_type,\n",
    "                            train_instance_count=TRAIN_INSTANCE_COUNT,\n",
    "                            distributions={'parameter_server': {'enabled': True}},\n",
    "                            metric_definitions=[\n",
    "                               {'Name': 'validation:acc',  'Regex': '- val_acc: (.*?$)'},\n",
    "                               {'Name': 'validation:loss', 'Regex': '- val_loss: (.*?) '}],\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            role=sagemaker.get_execution_role(),\n",
    "                            framework_version='1.12',\n",
    "                            py_version='py3',\n",
    "                            base_job_name=BASE_JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient distributed training is facilitated by the use of sharding the data by S3 key. The alternative data distribution mechanism is fully replicated. When you fully replicate the data, all the data files will be sent to every training instance. S3 sharding speeds the time to training completion by copying only a subset of data files to each training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "DISTRIBUTION_MODE = 'ShardedByS3Key' # 'FullyReplicated'\n",
    "\n",
    "train_input = sagemaker.s3_input(s3_data=inputs+'/train', \n",
    "                                 distribution=DISTRIBUTION_MODE)\n",
    "test_input  = sagemaker.s3_input(s3_data=inputs+'/test', \n",
    "                                 distribution=DISTRIBUTION_MODE)\n",
    "val_input   = sagemaker.s3_input(s3_data=inputs+'/val', \n",
    "                                 distribution=DISTRIBUTION_MODE)\n",
    "\n",
    "remote_inputs = {'train': train_input, 'val': val_input, 'test': test_input}\n",
    "\n",
    "estimator.fit(remote_inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "                             instance_type=serve_instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a handful of predictions to ensure the model is being served properly and is making accurate predictions. The endpoint should yield similar accuracy to that reported at the end of the training job, as it evaluates the model using the same test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_to_test = 100 # or to use the whole test suite, set this to: len(X_test)\n",
    "num_accurate  = 0\n",
    "\n",
    "for i in range(total_to_test):\n",
    "    result = predictor.predict(X_test[i])\n",
    "    predicted_prob = result['predictions'][0][0]\n",
    "    predicted_label = round(predicted_prob)\n",
    "    if y_test[i] == predicted_label:\n",
    "        num_accurate += 1\n",
    "        print('PASS. Actual: {:.0f}, Prob: {:.4f}'.format(y_test[i], predicted_prob))\n",
    "    else:\n",
    "        print('FAIL. Actual: {:.0f}, Prob: {:.4f}'.format(y_test[i], predicted_prob))\n",
    "print('Acc: {:.2%}'.format(num_accurate/total_to_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the SageMaker-hosted endpoint and avoid additional billing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all the generated data from the notebook instance folders and from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('data', ignore_errors=True)\n",
    "s3 = boto3.resource('s3')\n",
    "s3_bucket = s3.Bucket(bucket)\n",
    "resp = s3_bucket.objects.filter(Prefix=data_prefix + '/').delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free up memory for a subsequent run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_test\n",
    "del y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
