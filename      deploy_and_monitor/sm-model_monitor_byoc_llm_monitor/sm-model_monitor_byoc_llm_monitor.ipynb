{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af3794b",
   "metadata": {},
   "source": [
    "# BYOC LLM Monitoring: Bring Your Own Container Llama2 Multiple Evaluations Monitoring with SageMaker Model Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc5ce1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b1b24",
   "metadata": {},
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy and monitor a JumpStart Llama 2 fine-tuned model for Toxicity, Answer Relevance and Accuracy, and Readability. The container associated with this notebook employs [FMEval](https://github.com/aws/fmeval) for LLM Toxicity evaluation, [LangChain](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/) for Answer Relevance and Accuracy, and [WhyLabs LangKit](https://whylabs.ai/langkit) for Readability.\n",
    "\n",
    "To perform inference on these models, you need to pass custom_attributes='accept_eula=true' as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets custom_attributes='accept_eula=false', so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by '=' and pairs are separated by ';'. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if 'accept_eula=false; accept_eula=true' is passed to the server, then 'accept_eula=true' is kept and passed to the script handler.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e31d9",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "SageMaker Model Monitor allows users to provide images of their own custom-built containers to be run at each monitoring job. This notebook leverages the [BYOC](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-containers.html) feature to monitor the Llama2-7b model for 7 different Toxicity levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79c05c",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "- **IF RUNNING LOCALLY (not SageMaker Studio/Classic)**: An IAM role that gives SageMakerFullAccess. This role must also include the AmazonEC2ContainerRegistryFullAccess permission in order to push container image to ECR and the CloudWatchFullAccess permission to create CloudWatch Dashboards. By default, the SageMaker Execution Role associated with Sagemaker Studio instances do not have these permissions; **you must manually attach them**. For information on how to complete this, see this [documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html)\n",
    "\n",
    "- **IF RUNNING ON SAGEMAKER STUDIO/STUDIO CLASSIC (not locally)**: An IAM role that gives SageMakerFullAccess. This role must also include the AmazonEC2ContainerRegistryFullAccess permission in order to push container image to ECR and the CloudWatchFullAccess permission to create CloudWatch Dashboards. By default, the SageMaker Execution Role associated with Sagemaker Studio instances do not have these permissions; **you must manually attach them**. Please also ensure that Docker access is enabled in your domain and that you have downloaded Docker for this notebook instance. Please follow the [guide](#sagemaker-studio-docker-guide) at the end of this notebook to complete Docker setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642ab2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39994bc",
   "metadata": {},
   "source": [
    "**This notebook is best suited for a kernel of python verion >= 3.11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55e677-3429-4668-b100-bd63d2a4c401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeebb0b",
   "metadata": {},
   "source": [
    "## Retreive your SageMaker Session and Configure Execution Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "# Here, we create a role for SageMaker. The role ARN must be specified when calling the predict() method. If this fails, you can manually specify the role ARN in the except block.\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    # Manually specify the role ARN. Ensure that this role has the 'AmazonSageMakerFullAccess' role. See the linked documentation for help.\n",
    "    role = iam.get_role(RoleName=\"<CustomRoleName>\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d458cf0-02e2-4066-927b-25fa5ef2a07e",
   "metadata": {},
   "source": [
    "***\n",
    "You can continue with the default model or choose a different model: this notebook will run with the following model IDs :\n",
    "- `meta-textgeneration-llama-2-7b-f`\n",
    "- `meta-textgeneration-llama-2-13b-f`\n",
    "- `meta-textgeneration-llama-2-70b-f`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882ae62",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b-f\", \"2.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eef0dd",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd598868",
   "metadata": {},
   "source": [
    "### Set up DataCapture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b865cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket()\n",
    "print(\"Demo Bucket:\", bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f445381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "s3_root_dir = \"byoc-multiple-eval-monitor-llm\"\n",
    "\n",
    "s3_capture_upload_path = f\"s3://{bucket}/{s3_root_dir}/datacapture\"\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2bc731",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s3_capture_upload_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d033889e",
   "metadata": {},
   "source": [
    "### Note: This next cell will take ~10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52afae-868d-4736-881f-7180f393003a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version, role=role)\n",
    "predictor = model.deploy(data_capture_config=data_capture_config)\n",
    "print(model.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7207e-01ba-4ac2-b4a9-c8f6f0e1c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "***\n",
    "### Supported Parameters\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "***\n",
    "### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "- This model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5adf9b4-c7e1-4090-aefe-9cae0d096968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_dialog(payload, response):\n",
    "    dialog = payload[\"inputs\"][0]\n",
    "    for msg in dialog:\n",
    "        print(f\"{msg['role'].capitalize()}: {msg['content']}\\n\")\n",
    "    print(\n",
    "        f\">>>> {response[0]['generation']['role'].capitalize()}: {response[0]['generation']['content']}\"\n",
    "    )\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbb9af",
   "metadata": {},
   "source": [
    "### Example of a single invocation\n",
    "\n",
    "**NOTE**: Read the end-user-license-agreement here https://ai.meta.com/resources/models-and-libraries/llama-downloads/ and accept by setting `accept_eula` to `true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbde5e7-1068-41f9-999a-70ef04e1cbbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"what is the recipe of mayonnaise?\"},\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "try:\n",
    "    response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "    print_dialog(payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7ac9d",
   "metadata": {},
   "source": [
    "### Send artificial traffic to the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c200cf",
   "metadata": {},
   "source": [
    "The following cell will send questions to the endpoint until stopped. Feel free to stop the cell whenever you feel you have captured enough data.\n",
    "\n",
    "**NOTE**: Read the end-user-license-agreement here https://ai.meta.com/resources/models-and-libraries/llama-downloads/ and accept by setting `accept_eula` to `true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "line_count = 0\n",
    "with open(\"./data/questions.jsonl\", \"r\") as datafile:\n",
    "    for line in datafile:\n",
    "        if line_count == 10:\n",
    "            break\n",
    "        line_count += 1\n",
    "        data = json.loads(line)\n",
    "        payload = {\n",
    "            \"inputs\": [\n",
    "                [\n",
    "                    data,\n",
    "                ]\n",
    "            ],\n",
    "            \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "        }\n",
    "        try:\n",
    "            response = predictor.predict(payload, custom_attributes=\"accept_eula=false\")\n",
    "            print_dialog(payload, response)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ab1d3",
   "metadata": {},
   "source": [
    "# Build and Push the Container to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repo_name = \"byoc-llm-multiple-eval\"\n",
    "aws_region = sess.boto_region_name\n",
    "aws_account_id = sess.account_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ebf7fe",
   "metadata": {},
   "source": [
    "#### **IMPORTANT:** If running locally (not on SageMaker Studio), delete ' --network sagemaker'\n",
    "Build the image. This will take ~5 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "!set -Eeuxo pipefail\n",
    "!docker build -t \"{ecr_repo_name}\" . --network sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cbcb3d",
   "metadata": {},
   "source": [
    "Create the repository. Ensure the role you have assumed has the AmazonEC2ContainerRegistryFullAccess permission attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr = boto3.client(\"ecr\")\n",
    "\n",
    "try:\n",
    "    response = ecr.create_repository(\n",
    "        repositoryName=ecr_repo_name,\n",
    "        imageTagMutability=\"MUTABLE\",\n",
    "        imageScanningConfiguration={\"scanOnPush\": False},\n",
    "    )\n",
    "except ecr.exceptions.RepositoryAlreadyExistsException:\n",
    "    print(f\"Repository {ecr_repo_name} already exists. Skipping creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc4260",
   "metadata": {},
   "source": [
    "Push the image to ECR. This will take some time, as the image is ~9GB. Ensure that your AWS credentials are fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0043e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!LATEST_IMAGE_ID=$(docker images --filter=reference='{ecr_repo_name}:latest' --format \"{{.ID}}\" | head -n 1)\n",
    "!echo $LATEST_IMAGE_ID\n",
    "\n",
    "!aws ecr get-login-password --region '{aws_region}' | docker login --username AWS --password-stdin '{aws_account_id}'.dkr.ecr.'{aws_region}'.amazonaws.com\n",
    "\n",
    "!docker tag '{ecr_repo_name}':latest '{aws_account_id}'.dkr.ecr.'{aws_region}'.amazonaws.com/'{ecr_repo_name}':latest\n",
    "\n",
    "!echo 'Pushing to ECR Repo: ''{aws_account_id}'.dkr.ecr.'{aws_region}'.amazonaws.com/'{ecr_repo_name}':latest\n",
    "!docker push '{aws_account_id}'.dkr.ecr.'{aws_region}'.amazonaws.com/'{ecr_repo_name}':latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a9722f",
   "metadata": {},
   "source": [
    "# Set a Monitoring Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa6e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import ModelMonitor\n",
    "\n",
    "image_uri = f\"{aws_account_id}.dkr.ecr.{aws_region}.amazonaws.com/{ecr_repo_name}:latest\"\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "monitor = ModelMonitor(\n",
    "    base_job_name=\"byoc-llm-multiple-eval-monitor\",\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.9xlarge\",\n",
    "    env={\n",
    "        \"bucket\": bucket,\n",
    "        \"TOXICITY\": \"Enabled\",\n",
    "        \"READABILITY\": \"Enabled\",\n",
    "        \"RELEVANCE_AND_ACCURACY\": \"Enabled\",\n",
    "    },  # Change one to DISABLED if metrics not desired.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40b933",
   "metadata": {},
   "source": [
    "**Note**: The following cell sets a **one-time** monitoring schedule for demonstration purposes. A one-time monioring schedule will execute immediately. If you would like to set an hourly schedule, swap out the commented line. It is important to know that hourly schedules will only begin at the start of the next full hour, so you will not see immediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b05c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator, MonitoringOutput, EndpointInput\n",
    "\n",
    "# Do not change\n",
    "container_data_destination = \"/opt/ml/processing/input_data\"\n",
    "container_evaluation_source = \"/opt/ml/processing/output\"\n",
    "s3_report_upload_path = f\"s3://{bucket}/{s3_root_dir}/results\"\n",
    "\n",
    "\n",
    "endpoint_input = EndpointInput(\n",
    "    endpoint_name=predictor.endpoint_name,\n",
    "    destination=container_data_destination,\n",
    ")\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    endpoint_input=endpoint_input,\n",
    "    output=MonitoringOutput(source=container_evaluation_source, destination=s3_report_upload_path),\n",
    "    schedule_cron_expression=CronExpressionGenerator.now(),  # CronExpressionGenerator.hourly()\n",
    "    # data sampling is from 3hrs prior to execution to time of execution\n",
    "    data_analysis_start_time=\"-PT3H\",\n",
    "    data_analysis_end_time=\"-PT0H\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3b7d9",
   "metadata": {},
   "source": [
    "# View Results\n",
    "\n",
    "The following cell prints the output report stored in Amazon S3. It includes evaluations for at most 100 samples of the  captured data.\n",
    "\n",
    "**NOTE:** The report will show up once the job is finished. Please try again in a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import s3\n",
    "\n",
    "try:\n",
    "    execution_output = monitor.list_executions()[-1].output\n",
    "    s3_path_to_toxicity_report = f\"{execution_output.destination}/toxicity_custom_dataset.jsonl\"\n",
    "    s3_path_to_readability_report = f\"{execution_output.destination}/readability_eval_results.jsonl\"\n",
    "    s3_path_to_relevance_and_accuracy_report = (\n",
    "        f\"{execution_output.destination}/relevance_and_accuracy_eval_results.jsonl\"\n",
    "    )\n",
    "    print(\"Toxicity report: \\n\")\n",
    "    print(s3.S3Downloader.read_file(s3_path_to_toxicity_report), \"\\n\")\n",
    "    print(\"Readability report: \\n\")\n",
    "    print(s3.S3Downloader.read_file(s3_path_to_readability_report), \"\\n\")\n",
    "    print(\"Relevance and Accuracy report: \\n\")\n",
    "    print(s3.S3Downloader.read_file(s3_path_to_relevance_and_accuracy_report))\n",
    "except:\n",
    "    print(\"Report not found. Please wait and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f2ca9",
   "metadata": {},
   "source": [
    "### View Cloudwatch Dashboard Graph\n",
    "The following cell will generate a CloudWatch Dashboard for the monitoring schedule you created. For more information on dashboard formatting, see [here](https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/CloudWatch-Dashboard-Body-Structure.html#Dashboard-Body-Overall-Structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ea736",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwClient = boto3.client(\"cloudwatch\")\n",
    "monitoring_schedule_name = monitor.describe_schedule()[\"MonitoringScheduleName\"]\n",
    "endpoint_name = monitor.describe_schedule()[\"EndpointName\"]\n",
    "\n",
    "# Get the metrics for this monitoring schedule\n",
    "metric_list = cwClient.list_metrics(\n",
    "    Dimensions=[\n",
    "        {\"Name\": \"Endpoint\", \"Value\": endpoint_name},\n",
    "        {\"Name\": \"MonitoringSchedule\", \"Value\": monitoring_schedule_name},\n",
    "    ],\n",
    ")\n",
    "metric_names = [metric[\"MetricName\"] for metric in metric_list[\"Metrics\"]]\n",
    "print(metric_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_interpolate_metric = [\n",
    "    {\n",
    "        \"expression\": \"FILL(METRICS(), LINEAR)\",\n",
    "        \"label\": \"Linear Interpolated\",\n",
    "        \"id\": \"e1\",\n",
    "        \"region\": sess.boto_region_name,\n",
    "    }\n",
    "]\n",
    "metrics = [linear_interpolate_metric]\n",
    "for i, metric_name in enumerate(metric_names):\n",
    "    metrics.append(\n",
    "        [\n",
    "            \"aws/sagemaker/Endpoints/data-metrics\",\n",
    "            metric_name,\n",
    "            \"Endpoint\",\n",
    "            endpoint_name,\n",
    "            \"MonitoringSchedule\",\n",
    "            monitoring_schedule_name,\n",
    "            {\"id\": f\"m{i+1}\", \"region\": sess.boto_region_name, \"visible\": False},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "widget_title = \"LLM Multiple Evaluations Graph\"\n",
    "\n",
    "dash_data = json.dumps(\n",
    "    {\n",
    "        \"start\": \"-PT6H\",\n",
    "        \"periodOverride\": \"inherit\",\n",
    "        \"widgets\": [\n",
    "            {\n",
    "                \"type\": \"metric\",\n",
    "                \"x\": 0,\n",
    "                \"y\": 0,\n",
    "                \"width\": 13,\n",
    "                \"height\": 10,\n",
    "                \"properties\": {\n",
    "                    \"metrics\": metrics,\n",
    "                    \"view\": \"timeSeries\",\n",
    "                    \"stacked\": False,\n",
    "                    \"region\": sess.boto_region_name,\n",
    "                    \"stat\": \"Average\",\n",
    "                    \"period\": 300,\n",
    "                    \"title\": widget_title,\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"x\": 13,\n",
    "                \"y\": 0,\n",
    "                \"width\": 11,\n",
    "                \"height\": 11,\n",
    "                \"properties\": {\n",
    "                    \"markdown\": \"# LLM Evaluation Descriptions\\n## Toxicity\\nToxicity is measured in 7 different categories:\\n- `toxicity`\\n- `severe_toxicity`\\n- `obscene`\\n- `threat`\\n- `insult`\\n- `identity_attack`\\n- `sexual_explicit`\\n\\nEach score is a number between 0 and 1, with 1 denoting extreme toxicity. To obtain the toxicity scores, the FMEval library uses the open-source [Detoxify](https://github.com/unitaryai/detoxify) model to grade each LLM output.\\n \\n\\n\\n## Readability\\nReadability is measured in 11 different categories. These measurements are created and aggregating by the WhyLabs LangKit `textstat` module. For information on scoring for each metric, read their documentation [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\\n\\n## Relevance and Accuracy\\nRelevance and accuracy is graded on a single score from 1-10. The prompt and response from the monitored LLM are provided to an evaluator LLM with intructions as follows:\\n\\n> Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. For this evaluation, you should primarily consider the following criteria:\\n> - helpfulness: Is the submission helpful, insightful, and appropriate?\\n> - relevance: Is the submission referring to a real quote from the text?\\n> - correctness: Is the submission correct, accurate, and factual?\\n> - depth: Does the submission demonstrate depth of thought?\\n\\n> Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: '[[rating]]', for example: 'Rating: [[5]]'.\",\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "dashboard_name = \"byoc-llm-multiple-monitoring\"\n",
    "cwClient.put_dashboard(DashboardName=dashboard_name, DashboardBody=dash_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7479b",
   "metadata": {},
   "source": [
    "Click the link from the following cell output to view the created CloudWatch Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd247c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"[CloudWatch Dashboard](https://{aws_region}.console.aws.amazon.com/cloudwatch/home?region={aws_region}#dashboards/dashboard/{dashboard_name})\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2189335-4d40-44bb-bef1-4bd3597801b2",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2391e3-bde2-4a7f-bb5c-7af8d1d1c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Delete monitoring job\n",
    "\n",
    "name = monitor.monitoring_schedule_name\n",
    "monitor.delete_monitoring_schedule()\n",
    "\n",
    "# Waits until monitoring schedule has been deleted to delete endpoint\n",
    "while True:\n",
    "    monitoring_schedules = sess.list_monitoring_schedules()\n",
    "    if any(\n",
    "        schedule[\"MonitoringScheduleName\"] == name\n",
    "        for schedule in monitoring_schedules[\"MonitoringScheduleSummaries\"]\n",
    "    ):\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        print(\"Monitoring schedule deleted\")\n",
    "        break\n",
    "\n",
    "sess.delete_endpoint(endpoint_name=predictor.endpoint_name)  # delete model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d444fa3",
   "metadata": {},
   "source": [
    "# SageMaker Studio Docker Guide\n",
    "\n",
    "To set up docker in your SageMaker studio environment, follow these steps:\n",
    "1. Run the following command in the AWS CLI, inputting your region and SageMaker domain ID:\n",
    "```bash\n",
    "aws --region <region> \\\n",
    "    sagemaker update-domain --domain-id <domain-id> \\\n",
    "    --domain-settings-for-update '{\"DockerSettings\": {\"EnableDockerAccess\": \"ENABLED\"}}'\n",
    "```\n",
    "2. Open a new notebook instance. Only instances created after running this command will have Docker access.\n",
    "3. Open the terminal in this new instance and follow the [installation directions](https://github.com/aws-samples/amazon-sagemaker-local-mode/blob/main/sagemaker_studio_docker_cli_install/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93fb1a",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/deploy_and_monitor|sm-model_monitor_byoc_llm_monitor|sm-model_monitor_byoc_llm_monitor.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
