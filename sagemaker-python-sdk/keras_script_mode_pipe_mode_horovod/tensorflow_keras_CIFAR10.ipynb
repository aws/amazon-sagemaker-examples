{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Host a Keras Sequential Model\n",
    "## Using Pipe Mode datasets and distributed training with Horovod\n",
    "This notebook shows how to train and host a Keras Sequential model on SageMaker. The model used for this notebook is a simple deep CNN that was extracted from [the Keras examples](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) is one of the most popular machine learning datasets. It consists of 60,000 32x32 images belonging to 10 different classes (6,000 images per class). Here are the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "![cifar10](https://maet3608.github.io/nuts-ml/_images/cifar10.png)\n",
    "\n",
    "In this tutorial, we will train a deep CNN to recognize these images.\n",
    "\n",
    "We'll compare trainig with file mode, pipe mode datasets and distributed training with Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the CIFAR-10 dataset\n",
    "Downloading the test and training data takes around 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "Download from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and extract.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From generate_cifar10_tfrecords.py:35: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n",
      "Generating ./data/train/train.tfrecords\n",
      "WARNING:tensorflow:From generate_cifar10_tfrecords.py:69: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From generate_cifar10_tfrecords.py:58: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "Generating ./data/validation/validation.tfrecords\n",
      "Generating ./data/eval/eval.tfrecords\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!python generate_cifar10_tfrecords.py --data-dir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.TensorFlow estimator, running locally\n",
    "To test that the code will work in SageMaker, we'll first use SageMaker local mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "import subprocess\n",
    "instance_type = 'local'\n",
    "\n",
    "if subprocess.call('nvidia-smi') == 0:\n",
    "    ## Set type to GPU if one is present\n",
    "    instance_type = 'local_gpu'\n",
    "    \n",
    "local_hyperparameters = {'epochs': 2, 'batch-size' : 64}\n",
    "\n",
    "source_dir = os.path.join(os.getcwd(), 'source_dir')\n",
    "estimator = TensorFlow(entry_point='cifar10_keras_main.py',\n",
    "                       source_dir=source_dir,\n",
    "                       role=role,\n",
    "                       framework_version='1.12.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=local_hyperparameters,\n",
    "                       train_instance_count=1, train_instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpzydpdw61_algo-1-3kdx6_1 ... \n",
      "\u001b[1BAttaching to tmpzydpdw61_algo-1-3kdx6_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m 2020-04-29 02:25:54,502 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m 2020-04-29 02:25:54,757 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m \n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m \n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"validation\": \"/opt/ml/input/data/validation\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"eval\": \"/opt/ml/input/data/eval\"\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"current_host\": \"algo-1-3kdx6\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"algo-1-3kdx6\"\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"epochs\": 2,\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"batch-size\": 64,\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"model_dir\": \"s3://sagemaker-us-east-1-637338777613/sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291/model\"\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"validation\": {\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"eval\": {\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"job_name\": \"sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"master_hostname\": \"algo-1-3kdx6\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-637338777613/sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"module_name\": \"cifar10_keras_main\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"num_gpus\": 1,\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"current_host\": \"algo-1-3kdx6\",\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m             \"algo-1-3kdx6\"\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m     \"user_entry_point\": \"cifar10_keras_main.py\"\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m \n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m \n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_HOSTS=[\"algo-1-3kdx6\"]\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_HPS={\"batch-size\":64,\"epochs\":2,\"model_dir\":\"s3://sagemaker-us-east-1-637338777613/sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291/model\"}\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_USER_ENTRY_POINT=cifar10_keras_main.py\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-3kdx6\",\"hosts\":[\"algo-1-3kdx6\"]}\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"eval\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"},\"validation\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_CHANNELS=[\"eval\",\"train\",\"validation\"]\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_CURRENT_HOST=algo-1-3kdx6\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_MODULE_NAME=cifar10_keras_main\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_NUM_GPUS=1\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-637338777613/sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1-3kdx6\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-3kdx6\"],\"hyperparameters\":{\"batch-size\":64,\"epochs\":2,\"model_dir\":\"s3://sagemaker-us-east-1-637338777613/sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"},\"validation\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291\",\"log_level\":20,\"master_hostname\":\"algo-1-3kdx6\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-637338777613/sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291/source/sourcedir.tar.gz\",\"module_name\":\"cifar10_keras_main\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-3kdx6\",\"hosts\":[\"algo-1-3kdx6\"]},\"user_entry_point\":\"cifar10_keras_main.py\"}\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_USER_ARGS=[\"--batch-size\",\"64\",\"--epochs\",\"2\",\"--model_dir\",\"s3://sagemaker-us-east-1-637338777613/sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291/model\"]\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_CHANNEL_EVAL=/opt/ml/input/data/eval\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_HP_EPOCHS=2\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_HP_BATCH-SIZE=64\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m SM_HP_MODEL_DIR=s3://sagemaker-us-east-1-637338777613/sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291/model\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m \n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m \n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m /usr/bin/python cifar10_keras_main.py --batch-size 64 --epochs 2 --model_dir s3://sagemaker-us-east-1-637338777613/sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291/model\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m \n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m \n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m Using TensorFlow backend.\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:root:Writing TensorBoard logs to s3://sagemaker-us-east-1-637338777613/sagemaker-tensorflow-scriptmode-2020-04-29-02-25-50-291/model\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:root:Running with MPI=False\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:root:getting data\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:root:Running train in File mode\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:root:Running eval in File mode\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:root:Running validation in File mode\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:root:configuring model\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:root:Starting training\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m Train on 64 samples, validate on 64 samples\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m Epoch 1/2\n",
      "625/625 [==============================] - 39s 63ms/step - loss: 1.7865 - acc: 0.3387 - val_loss: 1.7133 - val_acc: 0.3899\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m Epoch 2/2\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 1.3805 - acc: 0.5005 - val_loss: 1.2432 - val_acc: 0.5437\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:root:Test loss:1.259454676738152\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:root:Test accuracy:0.5438701923076923\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:tensorflow:No assets to save.\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:tensorflow:No assets to save.\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:tensorflow:No assets to write.\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:tensorflow:No assets to write.\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:tensorflow:SavedModel written to: /opt/ml/model/1/saved_model.pb\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:tensorflow:SavedModel written to: /opt/ml/model/1/saved_model.pb\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m INFO:root:Model successfully saved at: /opt/ml/model\n",
      "\u001b[36malgo-1-3kdx6_1  |\u001b[0m 2020-04-29 02:27:20,632 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmpzydpdw61_algo-1-3kdx6_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "local_inputs = {'train' : 'file://'+os.getcwd()+'/data/train', \n",
    "                'validation' : 'file://'+os.getcwd()+'/data/validation', \n",
    "                'eval' : 'file://'+os.getcwd()+'/data/eval'}\n",
    "estimator.fit(local_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on SageMaker cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_location = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-cifar10-tf')\n",
    "display(dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring metrics from the job logs\n",
    "SageMaker can get training metrics directly from the logs and send them to CloudWatch metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_metric_definition = [\n",
    "    {'Name': 'train:loss', 'Regex': '.*loss: ([0-9\\\\.]+) - acc: [0-9\\\\.]+.*'},\n",
    "    {'Name': 'train:accuracy', 'Regex': '.*loss: [0-9\\\\.]+ - acc: ([0-9\\\\.]+).*'},\n",
    "    {'Name': 'validation:accuracy', 'Regex': '.*step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_acc: ([0-9\\\\.]+).*'},\n",
    "    {'Name': 'validation:loss', 'Regex': '.*step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_acc: [0-9\\\\.]+.*'},\n",
    "    {'Name': 'sec/steps', 'Regex': '.* - \\d+s (\\d+)[mu]s/step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_acc: [0-9\\\\.]+'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train image classification based on the cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': 10, 'batch-size' : 256}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "\n",
    "source_dir = os.path.join(os.getcwd(), 'source_dir')\n",
    "estimator = TensorFlow(base_job_name='cifar10-tf',\n",
    "                       entry_point='cifar10_keras_main.py',\n",
    "                       source_dir=source_dir,\n",
    "                       role=role,\n",
    "                       framework_version='1.12.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       train_instance_count=1, train_instance_type='ml.p3.2xlarge',\n",
    "                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'file'}],\n",
    "                       metric_definitions=keras_metric_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-29 02:27:40 Starting - Starting the training job...\n",
      "2020-04-29 02:27:42 Starting - Launching requested ML instances......\n",
      "2020-04-29 02:28:51 Starting - Preparing the instances for training......\n",
      "2020-04-29 02:30:00 Downloading - Downloading input data...\n",
      "2020-04-29 02:30:41 Training - Downloading the training image...\n",
      "2020-04-29 02:31:02 Training - Training image download completed. Training in progress..\u001b[34m2020-04-29 02:31:06,368 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-04-29 02:31:07,522 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"eval\": \"/opt/ml/input/data/eval\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 256,\n",
      "        \"model_dir\": \"s3://sagemaker-us-east-1-637338777613/cifar10-tf-2020-04-29-02-27-40-012/model\",\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"eval\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"cifar10-tf-2020-04-29-02-27-40-012\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-637338777613/cifar10-tf-2020-04-29-02-27-40-012/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"cifar10_keras_main\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"cifar10_keras_main.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":256,\"epochs\":10,\"model_dir\":\"s3://sagemaker-us-east-1-637338777613/cifar10-tf-2020-04-29-02-27-40-012/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=cifar10_keras_main.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"eval\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=cifar10_keras_main\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-637338777613/cifar10-tf-2020-04-29-02-27-40-012/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":256,\"epochs\":10,\"model_dir\":\"s3://sagemaker-us-east-1-637338777613/cifar10-tf-2020-04-29-02-27-40-012/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-tf-2020-04-29-02-27-40-012\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-637338777613/cifar10-tf-2020-04-29-02-27-40-012/source/sourcedir.tar.gz\",\"module_name\":\"cifar10_keras_main\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"cifar10_keras_main.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"256\",\"--epochs\",\"10\",\"--model_dir\",\"s3://sagemaker-us-east-1-637338777613/cifar10-tf-2020-04-29-02-27-40-012/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_EVAL=/opt/ml/input/data/eval\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-east-1-637338777613/cifar10-tf-2020-04-29-02-27-40-012/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python cifar10_keras_main.py --batch-size 256 --epochs 10 --model_dir s3://sagemaker-us-east-1-637338777613/cifar10-tf-2020-04-29-02-27-40-012/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[34mINFO:root:Writing TensorBoard logs to s3://sagemaker-us-east-1-637338777613/cifar10-tf-2020-04-29-02-27-40-012/model\u001b[0m\n",
      "\u001b[34mINFO:root:Running with MPI=False\u001b[0m\n",
      "\u001b[34mINFO:root:getting data\u001b[0m\n",
      "\u001b[34mINFO:root:Running train in File mode\u001b[0m\n",
      "\u001b[34mINFO:root:Running eval in File mode\u001b[0m\n",
      "\u001b[34mINFO:root:Running validation in File mode\u001b[0m\n",
      "\u001b[34mINFO:root:configuring model\u001b[0m\n",
      "\u001b[34mINFO:root:Starting training\u001b[0m\n",
      "\u001b[34mTrain on 256 samples, validate on 256 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/10\u001b[0m\n",
      "\u001b[34m  1/156 [..............................] - ETA: 37:05 - loss: 4.2140 - acc: 0.0625\n",
      "  3/156 [..............................] - ETA: 12:16 - loss: 3.7609 - acc: 0.1094\n",
      "  5/156 [..............................] - ETA: 7:18 - loss: 3.5850 - acc: 0.1164 \n",
      "  7/156 [>.............................] - ETA: 5:10 - loss: 3.4031 - acc: 0.1272\n",
      "  9/156 [>.............................] - ETA: 3:59 - loss: 3.2459 - acc: 0.1345\n",
      " 11/156 [=>............................] - ETA: 3:14 - loss: 3.1239 - acc: 0.1374\n",
      " 13/156 [=>............................] - ETA: 2:43 - loss: 3.0110 - acc: 0.1421\n",
      " 15/156 [=>............................] - ETA: 2:20 - loss: 2.9198 - acc: 0.1490\u001b[0m\n",
      "\u001b[34m 17/156 [==>...........................] - ETA: 2:02 - loss: 2.8441 - acc: 0.1526\n",
      " 19/156 [==>...........................] - ETA: 1:48 - loss: 2.7782 - acc: 0.1573\n",
      " 21/156 [===>..........................] - ETA: 1:37 - loss: 2.7169 - acc: 0.1652\n",
      " 23/156 [===>..........................] - ETA: 1:28 - loss: 2.6635 - acc: 0.1710\n",
      " 25/156 [===>..........................] - ETA: 1:20 - loss: 2.6186 - acc: 0.1756\n",
      " 27/156 [====>.........................] - ETA: 1:13 - loss: 2.5781 - acc: 0.1814\n",
      " 29/156 [====>.........................] - ETA: 1:07 - loss: 2.5399 - acc: 0.1849\n",
      " 31/156 [====>.........................] - ETA: 1:02 - loss: 2.5073 - acc: 0.1879\n",
      " 33/156 [=====>........................] - ETA: 58s - loss: 2.4814 - acc: 0.1915 \n",
      " 35/156 [=====>........................] - ETA: 54s - loss: 2.4505 - acc: 0.1964\n",
      " 37/156 [======>.......................] - ETA: 50s - loss: 2.4213 - acc: 0.1997\n",
      " 39/156 [======>.......................] - ETA: 47s - loss: 2.3955 - acc: 0.2026\u001b[0m\n",
      "\u001b[34m 41/156 [======>.......................] - ETA: 44s - loss: 2.3731 - acc: 0.2060\n",
      " 43/156 [=======>......................] - ETA: 42s - loss: 2.3554 - acc: 0.2077\n",
      " 45/156 [=======>......................] - ETA: 39s - loss: 2.3379 - acc: 0.2083\n",
      " 47/156 [========>.....................] - ETA: 37s - loss: 2.3239 - acc: 0.2102\n",
      " 49/156 [========>.....................] - ETA: 35s - loss: 2.3070 - acc: 0.2124\n",
      " 51/156 [========>.....................] - ETA: 33s - loss: 2.2929 - acc: 0.2138\n",
      " 53/156 [=========>....................] - ETA: 31s - loss: 2.2783 - acc: 0.2163\n",
      " 55/156 [=========>....................] - ETA: 30s - loss: 2.2656 - acc: 0.2178\n",
      " 57/156 [=========>....................] - ETA: 28s - loss: 2.2522 - acc: 0.2209\n",
      " 59/156 [==========>...................] - ETA: 27s - loss: 2.2388 - acc: 0.2244\n",
      " 61/156 [==========>...................] - ETA: 26s - loss: 2.2263 - acc: 0.2269\n",
      " 63/156 [===========>..................] - ETA: 24s - loss: 2.2160 - acc: 0.2284\u001b[0m\n",
      "\u001b[34m 65/156 [===========>..................] - ETA: 23s - loss: 2.2072 - acc: 0.2293\n",
      " 67/156 [===========>..................] - ETA: 22s - loss: 2.1985 - acc: 0.2309\n",
      " 69/156 [============>.................] - ETA: 21s - loss: 2.1900 - acc: 0.2322\n",
      " 71/156 [============>.................] - ETA: 20s - loss: 2.1804 - acc: 0.2337\n",
      " 73/156 [=============>................] - ETA: 19s - loss: 2.1700 - acc: 0.2362\n",
      " 75/156 [=============>................] - ETA: 18s - loss: 2.1611 - acc: 0.2374\n",
      " 77/156 [=============>................] - ETA: 17s - loss: 2.1525 - acc: 0.2394\n",
      " 79/156 [==============>...............] - ETA: 17s - loss: 2.1461 - acc: 0.2415\n",
      " 81/156 [==============>...............] - ETA: 16s - loss: 2.1377 - acc: 0.2437\n",
      " 83/156 [==============>...............] - ETA: 15s - loss: 2.1295 - acc: 0.2458\n",
      " 85/156 [===============>..............] - ETA: 14s - loss: 2.1226 - acc: 0.2472\n",
      " 87/156 [===============>..............] - ETA: 14s - loss: 2.1150 - acc: 0.2489\u001b[0m\n",
      "\u001b[34m 89/156 [================>.............] - ETA: 13s - loss: 2.1097 - acc: 0.2494\n",
      " 91/156 [================>.............] - ETA: 12s - loss: 2.1031 - acc: 0.2506\n",
      " 93/156 [================>.............] - ETA: 12s - loss: 2.0966 - acc: 0.2514\n",
      " 95/156 [=================>............] - ETA: 11s - loss: 2.0901 - acc: 0.2535\n",
      " 97/156 [=================>............] - ETA: 11s - loss: 2.0838 - acc: 0.2547\n",
      " 99/156 [==================>...........] - ETA: 10s - loss: 2.0786 - acc: 0.2559\u001b[0m\n",
      "\u001b[34m101/156 [==================>...........] - ETA: 10s - loss: 2.0732 - acc: 0.2573\u001b[0m\n",
      "\u001b[34m103/156 [==================>...........] - ETA: 9s - loss: 2.0669 - acc: 0.2585 \u001b[0m\n",
      "\u001b[34m105/156 [===================>..........] - ETA: 9s - loss: 2.0623 - acc: 0.2595\u001b[0m\n",
      "\u001b[34m107/156 [===================>..........] - ETA: 8s - loss: 2.0576 - acc: 0.2606\u001b[0m\n",
      "\u001b[34m109/156 [===================>..........] - ETA: 8s - loss: 2.0524 - acc: 0.2617\u001b[0m\n",
      "\u001b[34m111/156 [====================>.........] - ETA: 7s - loss: 2.0473 - acc: 0.2630\u001b[0m\n",
      "\u001b[34m113/156 [====================>.........] - ETA: 7s - loss: 2.0427 - acc: 0.2646\u001b[0m\n",
      "\u001b[34m115/156 [=====================>........] - ETA: 6s - loss: 2.0383 - acc: 0.2657\u001b[0m\n",
      "\u001b[34m117/156 [=====================>........] - ETA: 6s - loss: 2.0329 - acc: 0.2670\u001b[0m\n",
      "\u001b[34m119/156 [=====================>........] - ETA: 5s - loss: 2.0288 - acc: 0.2678\u001b[0m\n",
      "\u001b[34m121/156 [======================>.......] - ETA: 5s - loss: 2.0247 - acc: 0.2690\u001b[0m\n",
      "\u001b[34m123/156 [======================>.......] - ETA: 5s - loss: 2.0203 - acc: 0.2701\u001b[0m\n",
      "\u001b[34m125/156 [=======================>......] - ETA: 4s - loss: 2.0152 - acc: 0.2717\u001b[0m\n",
      "\u001b[34m127/156 [=======================>......] - ETA: 4s - loss: 2.0102 - acc: 0.2732\u001b[0m\n",
      "\u001b[34m129/156 [=======================>......] - ETA: 4s - loss: 2.0055 - acc: 0.2744\u001b[0m\n",
      "\u001b[34m131/156 [========================>.....] - ETA: 3s - loss: 2.0015 - acc: 0.2757\u001b[0m\n",
      "\u001b[34m133/156 [========================>.....] - ETA: 3s - loss: 1.9969 - acc: 0.2770\u001b[0m\n",
      "\u001b[34m135/156 [========================>.....] - ETA: 3s - loss: 1.9916 - acc: 0.2786\u001b[0m\n",
      "\u001b[34m137/156 [=========================>....] - ETA: 2s - loss: 1.9883 - acc: 0.2794\u001b[0m\n",
      "\u001b[34m139/156 [=========================>....] - ETA: 2s - loss: 1.9853 - acc: 0.2799\u001b[0m\n",
      "\u001b[34m141/156 [==========================>...] - ETA: 2s - loss: 1.9802 - acc: 0.2814\u001b[0m\n",
      "\u001b[34m143/156 [==========================>...] - ETA: 1s - loss: 1.9756 - acc: 0.2828\u001b[0m\n",
      "\u001b[34m145/156 [==========================>...] - ETA: 1s - loss: 1.9731 - acc: 0.2834\u001b[0m\n",
      "\u001b[34m147/156 [===========================>..] - ETA: 1s - loss: 1.9694 - acc: 0.2845\u001b[0m\n",
      "\u001b[34m149/156 [===========================>..] - ETA: 0s - loss: 1.9670 - acc: 0.2853\u001b[0m\n",
      "\u001b[34m151/156 [============================>.] - ETA: 0s - loss: 1.9637 - acc: 0.2864\u001b[0m\n",
      "\u001b[34m153/156 [============================>.] - ETA: 0s - loss: 1.9604 - acc: 0.2873\u001b[0m\n",
      "\u001b[34m155/156 [============================>.] - ETA: 0s - loss: 1.9565 - acc: 0.2882\u001b[0m\n",
      "\u001b[34m156/156 [==============================] - 22s 141ms/step - loss: 1.9552 - acc: 0.2885 - val_loss: 2.0590 - val_acc: 0.2741\u001b[0m\n",
      "\u001b[34mEpoch 2/10\n",
      "\n",
      "  1/156 [..............................] - ETA: 6s - loss: 1.6081 - acc: 0.3906\n",
      "  3/156 [..............................] - ETA: 5s - loss: 1.6594 - acc: 0.3685\u001b[0m\n",
      "\u001b[34m  5/156 [..............................] - ETA: 5s - loss: 1.6913 - acc: 0.3602\n",
      "  7/156 [>.............................] - ETA: 5s - loss: 1.6838 - acc: 0.3661\n",
      "  9/156 [>.............................] - ETA: 5s - loss: 1.6824 - acc: 0.3724\n",
      " 11/156 [=>............................] - ETA: 5s - loss: 1.6834 - acc: 0.3725\n",
      " 13/156 [=>............................] - ETA: 5s - loss: 1.6686 - acc: 0.3774\n",
      " 15/156 [=>............................] - ETA: 5s - loss: 1.6658 - acc: 0.3742\n",
      " 17/156 [==>...........................] - ETA: 5s - loss: 1.6614 - acc: 0.3757\n",
      " 19/156 [==>...........................] - ETA: 5s - loss: 1.6623 - acc: 0.3744\n",
      " 21/156 [===>..........................] - ETA: 5s - loss: 1.6649 - acc: 0.3757\n",
      " 23/156 [===>..........................] - ETA: 5s - loss: 1.6586 - acc: 0.3758\n",
      " 25/156 [===>..........................] - ETA: 5s - loss: 1.6587 - acc: 0.3761\n",
      " 27/156 [====>.........................] - ETA: 5s - loss: 1.6572 - acc: 0.3754\n",
      " 29/156 [====>.........................] - ETA: 4s - loss: 1.6601 - acc: 0.3755\u001b[0m\n",
      "\u001b[34m 31/156 [====>.........................] - ETA: 4s - loss: 1.6556 - acc: 0.3787\n",
      " 33/156 [=====>........................] - ETA: 4s - loss: 1.6537 - acc: 0.3795\n",
      " 35/156 [=====>........................] - ETA: 4s - loss: 1.6483 - acc: 0.3817\n",
      " 37/156 [======>.......................] - ETA: 4s - loss: 1.6476 - acc: 0.3808\n",
      " 39/156 [======>.......................] - ETA: 4s - loss: 1.6459 - acc: 0.3816\n",
      " 41/156 [======>.......................] - ETA: 4s - loss: 1.6460 - acc: 0.3821\n",
      " 43/156 [=======>......................] - ETA: 4s - loss: 1.6437 - acc: 0.3821\n",
      " 45/156 [=======>......................] - ETA: 4s - loss: 1.6414 - acc: 0.3827\n",
      " 47/156 [========>.....................] - ETA: 4s - loss: 1.6439 - acc: 0.3812\n",
      " 49/156 [========>.....................] - ETA: 4s - loss: 1.6445 - acc: 0.3823\n",
      " 51/156 [========>.....................] - ETA: 4s - loss: 1.6401 - acc: 0.3838\n",
      " 53/156 [=========>....................] - ETA: 3s - loss: 1.6376 - acc: 0.3844\n",
      " 55/156 [=========>....................] - ETA: 3s - loss: 1.6379 - acc: 0.3853\u001b[0m\n",
      "\u001b[34m 57/156 [=========>....................] - ETA: 3s - loss: 1.6359 - acc: 0.3869\n",
      " 59/156 [==========>...................] - ETA: 3s - loss: 1.6351 - acc: 0.3874\n",
      " 61/156 [==========>...................] - ETA: 3s - loss: 1.6310 - acc: 0.3890\n",
      " 63/156 [===========>..................] - ETA: 3s - loss: 1.6271 - acc: 0.3916\n",
      " 65/156 [===========>..................] - ETA: 3s - loss: 1.6286 - acc: 0.3908\n",
      " 67/156 [===========>..................] - ETA: 3s - loss: 1.6251 - acc: 0.3921\n",
      " 69/156 [============>.................] - ETA: 3s - loss: 1.6232 - acc: 0.3928\n",
      " 71/156 [============>.................] - ETA: 3s - loss: 1.6213 - acc: 0.3941\n",
      " 73/156 [=============>................] - ETA: 3s - loss: 1.6212 - acc: 0.3943\n",
      " 75/156 [=============>................] - ETA: 3s - loss: 1.6198 - acc: 0.3948\n",
      " 77/156 [=============>................] - ETA: 3s - loss: 1.6195 - acc: 0.3941\n",
      " 79/156 [==============>...............] - ETA: 2s - loss: 1.6195 - acc: 0.3954\n",
      " 81/156 [==============>...............] - ETA: 2s - loss: 1.6197 - acc: 0.3952\u001b[0m\n",
      "\u001b[34m 83/156 [==============>...............] - ETA: 2s - loss: 1.6181 - acc: 0.3948\n",
      " 85/156 [===============>..............] - ETA: 2s - loss: 1.6160 - acc: 0.3953\n",
      " 87/156 [===============>..............] - ETA: 2s - loss: 1.6140 - acc: 0.3964\n",
      " 89/156 [================>.............] - ETA: 2s - loss: 1.6118 - acc: 0.3975\n",
      " 91/156 [================>.............] - ETA: 2s - loss: 1.6102 - acc: 0.3978\n",
      " 93/156 [================>.............] - ETA: 2s - loss: 1.6094 - acc: 0.3977\n",
      " 95/156 [=================>............] - ETA: 2s - loss: 1.6071 - acc: 0.3985\n",
      " 97/156 [=================>............] - ETA: 2s - loss: 1.6058 - acc: 0.3986\n",
      " 99/156 [==================>...........] - ETA: 2s - loss: 1.6048 - acc: 0.3989\u001b[0m\n",
      "\u001b[34m101/156 [==================>...........] - ETA: 2s - loss: 1.6038 - acc: 0.3994\u001b[0m\n",
      "\u001b[34m103/156 [==================>...........] - ETA: 2s - loss: 1.6010 - acc: 0.3997\u001b[0m\n",
      "\u001b[34m105/156 [===================>..........] - ETA: 1s - loss: 1.5992 - acc: 0.4005\u001b[0m\n",
      "\u001b[34m107/156 [===================>..........] - ETA: 1s - loss: 1.5975 - acc: 0.4012\u001b[0m\n",
      "\u001b[34m109/156 [===================>..........] - ETA: 1s - loss: 1.5958 - acc: 0.4018\u001b[0m\n",
      "\u001b[34m111/156 [====================>.........] - ETA: 1s - loss: 1.5944 - acc: 0.4030\u001b[0m\n",
      "\u001b[34m113/156 [====================>.........] - ETA: 1s - loss: 1.5935 - acc: 0.4037\u001b[0m\n",
      "\u001b[34m115/156 [=====================>........] - ETA: 1s - loss: 1.5924 - acc: 0.4042\u001b[0m\n",
      "\u001b[34m117/156 [=====================>........] - ETA: 1s - loss: 1.5915 - acc: 0.4041\u001b[0m\n",
      "\u001b[34m119/156 [=====================>........] - ETA: 1s - loss: 1.5916 - acc: 0.4041\u001b[0m\n",
      "\u001b[34m121/156 [======================>.......] - ETA: 1s - loss: 1.5896 - acc: 0.4047\u001b[0m\n",
      "\u001b[34m123/156 [======================>.......] - ETA: 1s - loss: 1.5878 - acc: 0.4060\u001b[0m\n",
      "\u001b[34m125/156 [=======================>......] - ETA: 1s - loss: 1.5859 - acc: 0.4065\u001b[0m\n",
      "\u001b[34m127/156 [=======================>......] - ETA: 1s - loss: 1.5843 - acc: 0.4071\u001b[0m\n",
      "\u001b[34m129/156 [=======================>......] - ETA: 1s - loss: 1.5817 - acc: 0.4082\u001b[0m\n",
      "\u001b[34m131/156 [========================>.....] - ETA: 0s - loss: 1.5798 - acc: 0.4091\u001b[0m\n",
      "\u001b[34m133/156 [========================>.....] - ETA: 0s - loss: 1.5794 - acc: 0.4093\u001b[0m\n",
      "\u001b[34m135/156 [========================>.....] - ETA: 0s - loss: 1.5785 - acc: 0.4096\u001b[0m\n",
      "\u001b[34m137/156 [=========================>....] - ETA: 0s - loss: 1.5774 - acc: 0.4100\u001b[0m\n",
      "\u001b[34m139/156 [=========================>....] - ETA: 0s - loss: 1.5768 - acc: 0.4102\u001b[0m\n",
      "\u001b[34m141/156 [==========================>...] - ETA: 0s - loss: 1.5753 - acc: 0.4110\u001b[0m\n",
      "\u001b[34m143/156 [==========================>...] - ETA: 0s - loss: 1.5725 - acc: 0.4122\u001b[0m\n",
      "\u001b[34m145/156 [==========================>...] - ETA: 0s - loss: 1.5711 - acc: 0.4126\u001b[0m\n",
      "\u001b[34m147/156 [===========================>..] - ETA: 0s - loss: 1.5691 - acc: 0.4132\u001b[0m\n",
      "\u001b[34m149/156 [===========================>..] - ETA: 0s - loss: 1.5679 - acc: 0.4139\u001b[0m\n",
      "\u001b[34m151/156 [============================>.] - ETA: 0s - loss: 1.5655 - acc: 0.4150\u001b[0m\n",
      "\u001b[34m153/156 [============================>.] - ETA: 0s - loss: 1.5640 - acc: 0.4154\u001b[0m\n",
      "\u001b[34m155/156 [============================>.] - ETA: 0s - loss: 1.5636 - acc: 0.4158\u001b[0m\n",
      "\u001b[34m156/156 [==============================] - 7s 46ms/step - loss: 1.5627 - acc: 0.4161 - val_loss: 2.3014 - val_acc: 0.2880\u001b[0m\n",
      "\u001b[34mEpoch 3/10\n",
      "\n",
      "  1/156 [..............................] - ETA: 6s - loss: 1.5114 - acc: 0.4453\n",
      "  3/156 [..............................] - ETA: 5s - loss: 1.4576 - acc: 0.4518\n",
      "  5/156 [..............................] - ETA: 6s - loss: 1.4952 - acc: 0.4391\n",
      "  7/156 [>.............................] - ETA: 5s - loss: 1.5046 - acc: 0.4302\n",
      "  9/156 [>.............................] - ETA: 5s - loss: 1.4995 - acc: 0.4353\n",
      " 11/156 [=>............................] - ETA: 5s - loss: 1.4925 - acc: 0.4425\n",
      " 13/156 [=>............................] - ETA: 5s - loss: 1.4996 - acc: 0.4411\n",
      " 15/156 [=>............................] - ETA: 5s - loss: 1.4904 - acc: 0.4474\u001b[0m\n",
      "\u001b[34m 17/156 [==>...........................] - ETA: 5s - loss: 1.4913 - acc: 0.4490\n",
      " 19/156 [==>...........................] - ETA: 5s - loss: 1.4833 - acc: 0.4521\n",
      " 21/156 [===>..........................] - ETA: 5s - loss: 1.4808 - acc: 0.4535\n",
      " 23/156 [===>..........................] - ETA: 5s - loss: 1.4780 - acc: 0.4536\n",
      " 25/156 [===>..........................] - ETA: 5s - loss: 1.4681 - acc: 0.4556\n",
      " 27/156 [====>.........................] - ETA: 5s - loss: 1.4726 - acc: 0.4543\n",
      " 29/156 [====>.........................] - ETA: 5s - loss: 1.4696 - acc: 0.4550\n",
      " 31/156 [====>.........................] - ETA: 4s - loss: 1.4625 - acc: 0.4574\n",
      " 33/156 [=====>........................] - ETA: 4s - loss: 1.4579 - acc: 0.4595\n",
      " 35/156 [=====>........................] - ETA: 4s - loss: 1.4525 - acc: 0.4618\n",
      " 37/156 [======>.......................] - ETA: 4s - loss: 1.4524 - acc: 0.4605\n",
      " 39/156 [======>.......................] - ETA: 4s - loss: 1.4467 - acc: 0.4620\n",
      " 41/156 [======>.......................] - ETA: 4s - loss: 1.4485 - acc: 0.4617\u001b[0m\n",
      "\u001b[34m 43/156 [=======>......................] - ETA: 4s - loss: 1.4464 - acc: 0.4622\n",
      " 45/156 [=======>......................] - ETA: 4s - loss: 1.4443 - acc: 0.4619\n",
      " 47/156 [========>.....................] - ETA: 4s - loss: 1.4473 - acc: 0.4612\n",
      " 49/156 [========>.....................] - ETA: 4s - loss: 1.4434 - acc: 0.4629\n",
      " 51/156 [========>.....................] - ETA: 4s - loss: 1.4395 - acc: 0.4643\n",
      " 53/156 [=========>....................] - ETA: 4s - loss: 1.4404 - acc: 0.4651\n",
      " 55/156 [=========>....................] - ETA: 3s - loss: 1.4397 - acc: 0.4651\n",
      " 57/156 [=========>....................] - ETA: 3s - loss: 1.4384 - acc: 0.4656\n",
      " 59/156 [==========>...................] - ETA: 3s - loss: 1.4368 - acc: 0.4668\n",
      " 61/156 [==========>...................] - ETA: 3s - loss: 1.4364 - acc: 0.4669\n",
      " 63/156 [===========>..................] - ETA: 3s - loss: 1.4329 - acc: 0.4683\n",
      " 65/156 [===========>..................] - ETA: 3s - loss: 1.4322 - acc: 0.4675\n",
      " 67/156 [===========>..................] - ETA: 3s - loss: 1.4304 - acc: 0.4687\u001b[0m\n",
      "\u001b[34m 69/156 [============>.................] - ETA: 3s - loss: 1.4293 - acc: 0.4693\n",
      " 71/156 [============>.................] - ETA: 3s - loss: 1.4279 - acc: 0.4698\n",
      " 73/156 [=============>................] - ETA: 3s - loss: 1.4237 - acc: 0.4715\n",
      " 75/156 [=============>................] - ETA: 3s - loss: 1.4216 - acc: 0.4722\n",
      " 77/156 [=============>................] - ETA: 3s - loss: 1.4194 - acc: 0.4739\n",
      " 79/156 [==============>...............] - ETA: 3s - loss: 1.4178 - acc: 0.4744\n",
      " 81/156 [==============>...............] - ETA: 2s - loss: 1.4160 - acc: 0.4749\n",
      " 83/156 [==============>...............] - ETA: 2s - loss: 1.4140 - acc: 0.4759\n",
      " 85/156 [===============>..............] - ETA: 2s - loss: 1.4153 - acc: 0.4758\n",
      " 87/156 [===============>..............] - ETA: 2s - loss: 1.4147 - acc: 0.4760\n",
      " 89/156 [================>.............] - ETA: 2s - loss: 1.4118 - acc: 0.4772\n",
      " 91/156 [================>.............] - ETA: 2s - loss: 1.4103 - acc: 0.4778\n",
      " 93/156 [================>.............] - ETA: 2s - loss: 1.4079 - acc: 0.4789\u001b[0m\n",
      "\u001b[34m 95/156 [=================>............] - ETA: 2s - loss: 1.4073 - acc: 0.4794\n",
      " 97/156 [=================>............] - ETA: 2s - loss: 1.4083 - acc: 0.4792\n",
      " 99/156 [==================>...........] - ETA: 2s - loss: 1.4081 - acc: 0.4794\u001b[0m\n",
      "\u001b[34m101/156 [==================>...........] - ETA: 2s - loss: 1.4074 - acc: 0.4801\u001b[0m\n",
      "\u001b[34m103/156 [==================>...........] - ETA: 2s - loss: 1.4061 - acc: 0.4812\u001b[0m\n",
      "\u001b[34m105/156 [===================>..........] - ETA: 2s - loss: 1.4038 - acc: 0.4821\u001b[0m\n",
      "\u001b[34m107/156 [===================>..........] - ETA: 1s - loss: 1.4031 - acc: 0.4829\u001b[0m\n",
      "\u001b[34m109/156 [===================>..........] - ETA: 1s - loss: 1.3999 - acc: 0.4841\u001b[0m\n",
      "\u001b[34m111/156 [====================>.........] - ETA: 1s - loss: 1.4002 - acc: 0.4844\u001b[0m\n",
      "\u001b[34m113/156 [====================>.........] - ETA: 1s - loss: 1.4003 - acc: 0.4849\u001b[0m\n",
      "\u001b[34m115/156 [=====================>........] - ETA: 1s - loss: 1.3992 - acc: 0.4858\u001b[0m\n",
      "\u001b[34m117/156 [=====================>........] - ETA: 1s - loss: 1.3972 - acc: 0.4865\u001b[0m\n",
      "\u001b[34m119/156 [=====================>........] - ETA: 1s - loss: 1.3960 - acc: 0.4871\u001b[0m\n",
      "\u001b[34m121/156 [======================>.......] - ETA: 1s - loss: 1.3949 - acc: 0.4871\u001b[0m\n",
      "\u001b[34m123/156 [======================>.......] - ETA: 1s - loss: 1.3940 - acc: 0.4877\u001b[0m\n",
      "\u001b[34m125/156 [=======================>......] - ETA: 1s - loss: 1.3919 - acc: 0.4883\u001b[0m\n",
      "\u001b[34m127/156 [=======================>......] - ETA: 1s - loss: 1.3903 - acc: 0.4891\u001b[0m\n",
      "\u001b[34m129/156 [=======================>......] - ETA: 1s - loss: 1.3886 - acc: 0.4899\u001b[0m\n",
      "\u001b[34m131/156 [========================>.....] - ETA: 0s - loss: 1.3873 - acc: 0.4905\u001b[0m\n",
      "\u001b[34m133/156 [========================>.....] - ETA: 0s - loss: 1.3870 - acc: 0.4907\u001b[0m\n",
      "\u001b[34m135/156 [========================>.....] - ETA: 0s - loss: 1.3856 - acc: 0.4913\u001b[0m\n",
      "\u001b[34m137/156 [=========================>....] - ETA: 0s - loss: 1.3856 - acc: 0.4910\u001b[0m\n",
      "\u001b[34m139/156 [=========================>....] - ETA: 0s - loss: 1.3844 - acc: 0.4915\u001b[0m\n",
      "\u001b[34m141/156 [==========================>...] - ETA: 0s - loss: 1.3834 - acc: 0.4917\u001b[0m\n",
      "\u001b[34m142/156 [==========================>...] - ETA: 0s - loss: 1.3827 - acc: 0.4921\u001b[0m\n",
      "\u001b[34m144/156 [==========================>...] - ETA: 0s - loss: 1.3801 - acc: 0.4932\u001b[0m\n",
      "\u001b[34m146/156 [===========================>..] - ETA: 0s - loss: 1.3787 - acc: 0.4938\u001b[0m\n",
      "\u001b[34m148/156 [===========================>..] - ETA: 0s - loss: 1.3785 - acc: 0.4940\u001b[0m\n",
      "\u001b[34m150/156 [===========================>..] - ETA: 0s - loss: 1.3768 - acc: 0.4945\u001b[0m\n",
      "\u001b[34m152/156 [============================>.] - ETA: 0s - loss: 1.3762 - acc: 0.4946\u001b[0m\n",
      "\u001b[34m154/156 [============================>.] - ETA: 0s - loss: 1.3751 - acc: 0.4947\u001b[0m\n",
      "\u001b[34m156/156 [==============================] - 7s 47ms/step - loss: 1.3735 - acc: 0.4950 - val_loss: 1.3411 - val_acc: 0.5037\u001b[0m\n",
      "\u001b[34mEpoch 4/10\n",
      "\n",
      "  1/156 [..............................] - ETA: 6s - loss: 1.2423 - acc: 0.5742\n",
      "  3/156 [..............................] - ETA: 5s - loss: 1.2964 - acc: 0.5247\u001b[0m\n",
      "\u001b[34m  5/156 [..............................] - ETA: 5s - loss: 1.2611 - acc: 0.5352\n",
      "  7/156 [>.............................] - ETA: 5s - loss: 1.2724 - acc: 0.5312\n",
      "  9/156 [>.............................] - ETA: 5s - loss: 1.2887 - acc: 0.5243\n",
      " 11/156 [=>............................] - ETA: 5s - loss: 1.2820 - acc: 0.5241\n",
      " 13/156 [=>............................] - ETA: 5s - loss: 1.2957 - acc: 0.5222\n",
      " 15/156 [=>............................] - ETA: 5s - loss: 1.2840 - acc: 0.5255\n",
      " 17/156 [==>...........................] - ETA: 5s - loss: 1.2816 - acc: 0.5253\n",
      " 19/156 [==>...........................] - ETA: 5s - loss: 1.2842 - acc: 0.5230\n",
      " 21/156 [===>..........................] - ETA: 5s - loss: 1.2861 - acc: 0.5223\n",
      " 23/156 [===>..........................] - ETA: 5s - loss: 1.2829 - acc: 0.5236\n",
      " 25/156 [===>..........................] - ETA: 5s - loss: 1.2737 - acc: 0.5292\n",
      " 27/156 [====>.........................] - ETA: 5s - loss: 1.2762 - acc: 0.5315\u001b[0m\n",
      "\u001b[34m 29/156 [====>.........................] - ETA: 5s - loss: 1.2786 - acc: 0.5322\n",
      " 31/156 [====>.........................] - ETA: 5s - loss: 1.2747 - acc: 0.5329\n",
      " 33/156 [=====>........................] - ETA: 4s - loss: 1.2743 - acc: 0.5346\n",
      " 35/156 [=====>........................] - ETA: 4s - loss: 1.2741 - acc: 0.5345\n",
      " 37/156 [======>.......................] - ETA: 4s - loss: 1.2678 - acc: 0.5363\n",
      " 39/156 [======>.......................] - ETA: 4s - loss: 1.2664 - acc: 0.5360\n",
      " 41/156 [======>.......................] - ETA: 4s - loss: 1.2650 - acc: 0.5371\n",
      " 43/156 [=======>......................] - ETA: 4s - loss: 1.2638 - acc: 0.5383\n",
      " 45/156 [=======>......................] - ETA: 4s - loss: 1.2612 - acc: 0.5391\n",
      " 47/156 [========>.....................] - ETA: 4s - loss: 1.2597 - acc: 0.5388\n",
      " 49/156 [========>.....................] - ETA: 4s - loss: 1.2585 - acc: 0.5405\n",
      " 51/156 [========>.....................] - ETA: 4s - loss: 1.2572 - acc: 0.5411\n",
      " 53/156 [=========>....................] - ETA: 4s - loss: 1.2582 - acc: 0.5413\u001b[0m\n",
      "\u001b[34m 55/156 [=========>....................] - ETA: 4s - loss: 1.2601 - acc: 0.5406\n",
      " 57/156 [=========>....................] - ETA: 3s - loss: 1.2616 - acc: 0.5402\n",
      " 59/156 [==========>...................] - ETA: 3s - loss: 1.2589 - acc: 0.5418\n",
      " 61/156 [==========>...................] - ETA: 3s - loss: 1.2573 - acc: 0.5432\n",
      " 63/156 [===========>..................] - ETA: 3s - loss: 1.2583 - acc: 0.5431\n",
      " 65/156 [===========>..................] - ETA: 3s - loss: 1.2569 - acc: 0.5443\n",
      " 67/156 [===========>..................] - ETA: 3s - loss: 1.2561 - acc: 0.5445\n",
      " 69/156 [============>.................] - ETA: 3s - loss: 1.2529 - acc: 0.5453\n",
      " 71/156 [============>.................] - ETA: 3s - loss: 1.2519 - acc: 0.5456\n",
      " 73/156 [=============>................] - ETA: 3s - loss: 1.2506 - acc: 0.5464\n",
      " 75/156 [=============>................] - ETA: 3s - loss: 1.2513 - acc: 0.5458\n",
      " 77/156 [=============>................] - ETA: 3s - loss: 1.2532 - acc: 0.5457\n",
      " 79/156 [==============>...............] - ETA: 3s - loss: 1.2522 - acc: 0.5454\u001b[0m\n",
      "\u001b[34m 81/156 [==============>...............] - ETA: 2s - loss: 1.2519 - acc: 0.5450\n",
      " 83/156 [==============>...............] - ETA: 2s - loss: 1.2504 - acc: 0.5452\n",
      " 85/156 [===============>..............] - ETA: 2s - loss: 1.2495 - acc: 0.5457\n",
      " 87/156 [===============>..............] - ETA: 2s - loss: 1.2487 - acc: 0.5455\n",
      " 89/156 [================>.............] - ETA: 2s - loss: 1.2473 - acc: 0.5465\n",
      " 91/156 [================>.............] - ETA: 2s - loss: 1.2468 - acc: 0.5465\n",
      " 93/156 [================>.............] - ETA: 2s - loss: 1.2452 - acc: 0.5472\n",
      " 95/156 [=================>............] - ETA: 2s - loss: 1.2458 - acc: 0.5473\n",
      " 97/156 [=================>............] - ETA: 2s - loss: 1.2462 - acc: 0.5472\n",
      " 99/156 [==================>...........] - ETA: 2s - loss: 1.2441 - acc: 0.5479\u001b[0m\n",
      "\u001b[34m101/156 [==================>...........] - ETA: 2s - loss: 1.2438 - acc: 0.5480\u001b[0m\n",
      "\u001b[34m103/156 [==================>...........] - ETA: 2s - loss: 1.2431 - acc: 0.5484\u001b[0m\n",
      "\u001b[34m105/156 [===================>..........] - ETA: 2s - loss: 1.2423 - acc: 0.5486\u001b[0m\n",
      "\u001b[34m107/156 [===================>..........] - ETA: 1s - loss: 1.2409 - acc: 0.5488\u001b[0m\n",
      "\u001b[34m109/156 [===================>..........] - ETA: 1s - loss: 1.2401 - acc: 0.5493\u001b[0m\n",
      "\u001b[34m111/156 [====================>.........] - ETA: 1s - loss: 1.2383 - acc: 0.5499\u001b[0m\n",
      "\u001b[34m113/156 [====================>.........] - ETA: 1s - loss: 1.2377 - acc: 0.5497\u001b[0m\n",
      "\u001b[34m115/156 [=====================>........] - ETA: 1s - loss: 1.2373 - acc: 0.5496\u001b[0m\n",
      "\u001b[34m117/156 [=====================>........] - ETA: 1s - loss: 1.2367 - acc: 0.5501\u001b[0m\n",
      "\u001b[34m119/156 [=====================>........] - ETA: 1s - loss: 1.2347 - acc: 0.5513\u001b[0m\n",
      "\u001b[34m121/156 [======================>.......] - ETA: 1s - loss: 1.2347 - acc: 0.5509\u001b[0m\n",
      "\u001b[34m123/156 [======================>.......] - ETA: 1s - loss: 1.2350 - acc: 0.5511\u001b[0m\n",
      "\u001b[34m125/156 [=======================>......] - ETA: 1s - loss: 1.2339 - acc: 0.5517\u001b[0m\n",
      "\u001b[34m127/156 [=======================>......] - ETA: 1s - loss: 1.2345 - acc: 0.5514\u001b[0m\n",
      "\u001b[34m129/156 [=======================>......] - ETA: 1s - loss: 1.2333 - acc: 0.5521\u001b[0m\n",
      "\u001b[34m131/156 [========================>.....] - ETA: 0s - loss: 1.2328 - acc: 0.5526\u001b[0m\n",
      "\u001b[34m133/156 [========================>.....] - ETA: 0s - loss: 1.2325 - acc: 0.5529\u001b[0m\n",
      "\u001b[34m135/156 [========================>.....] - ETA: 0s - loss: 1.2329 - acc: 0.5531\u001b[0m\n",
      "\u001b[34m137/156 [=========================>....] - ETA: 0s - loss: 1.2322 - acc: 0.5537\u001b[0m\n",
      "\u001b[34m139/156 [=========================>....] - ETA: 0s - loss: 1.2325 - acc: 0.5537\u001b[0m\n",
      "\u001b[34m141/156 [==========================>...] - ETA: 0s - loss: 1.2323 - acc: 0.5543\u001b[0m\n",
      "\u001b[34m143/156 [==========================>...] - ETA: 0s - loss: 1.2319 - acc: 0.5546\u001b[0m\n",
      "\u001b[34m145/156 [==========================>...] - ETA: 0s - loss: 1.2312 - acc: 0.5548\u001b[0m\n",
      "\u001b[34m147/156 [===========================>..] - ETA: 0s - loss: 1.2295 - acc: 0.5556\u001b[0m\n",
      "\u001b[34m149/156 [===========================>..] - ETA: 0s - loss: 1.2300 - acc: 0.5555\u001b[0m\n",
      "\u001b[34m151/156 [============================>.] - ETA: 0s - loss: 1.2302 - acc: 0.5554\u001b[0m\n",
      "\u001b[34m153/156 [============================>.] - ETA: 0s - loss: 1.2296 - acc: 0.5556\u001b[0m\n",
      "\u001b[34m155/156 [============================>.] - ETA: 0s - loss: 1.2294 - acc: 0.5558\u001b[0m\n",
      "\u001b[34m156/156 [==============================] - 7s 47ms/step - loss: 1.2286 - acc: 0.5560 - val_loss: 1.3086 - val_acc: 0.5142\u001b[0m\n",
      "\u001b[34mEpoch 5/10\n",
      "\n",
      "  1/156 [..............................] - ETA: 6s - loss: 1.1380 - acc: 0.5977\n",
      "  3/156 [..............................] - ETA: 5s - loss: 1.1397 - acc: 0.6133\n",
      "  5/156 [..............................] - ETA: 5s - loss: 1.1570 - acc: 0.6047\n",
      "  7/156 [>.............................] - ETA: 6s - loss: 1.1703 - acc: 0.5965\u001b[0m\n",
      "\u001b[34m  9/156 [>.............................] - ETA: 5s - loss: 1.1846 - acc: 0.5890\n",
      " 11/156 [=>............................] - ETA: 5s - loss: 1.1896 - acc: 0.5838\n",
      " 13/156 [=>............................] - ETA: 5s - loss: 1.1872 - acc: 0.5790\n",
      " 15/156 [=>............................] - ETA: 5s - loss: 1.1939 - acc: 0.5786\n",
      " 17/156 [==>...........................] - ETA: 5s - loss: 1.1903 - acc: 0.5802\n",
      " 19/156 [==>...........................] - ETA: 5s - loss: 1.1934 - acc: 0.5802\n",
      " 21/156 [===>..........................] - ETA: 5s - loss: 1.1869 - acc: 0.5824\n",
      " 23/156 [===>..........................] - ETA: 5s - loss: 1.1864 - acc: 0.5800\n",
      " 25/156 [===>..........................] - ETA: 5s - loss: 1.1855 - acc: 0.5806\n",
      " 27/156 [====>.........................] - ETA: 5s - loss: 1.1849 - acc: 0.5809\n",
      " 29/156 [====>.........................] - ETA: 4s - loss: 1.1847 - acc: 0.5793\n",
      " 31/156 [====>.........................] - ETA: 4s - loss: 1.1784 - acc: 0.5819\n",
      " 33/156 [=====>........................] - ETA: 4s - loss: 1.1801 - acc: 0.5813\u001b[0m\n",
      "\u001b[34m 35/156 [=====>........................] - ETA: 4s - loss: 1.1768 - acc: 0.5823\n",
      " 37/156 [======>.......................] - ETA: 4s - loss: 1.1753 - acc: 0.5815\n",
      " 39/156 [======>.......................] - ETA: 4s - loss: 1.1737 - acc: 0.5818\n",
      " 41/156 [======>.......................] - ETA: 4s - loss: 1.1773 - acc: 0.5791\n",
      " 43/156 [=======>......................] - ETA: 4s - loss: 1.1759 - acc: 0.5789\n",
      " 45/156 [=======>......................] - ETA: 4s - loss: 1.1749 - acc: 0.5799\n",
      " 47/156 [========>.....................] - ETA: 4s - loss: 1.1737 - acc: 0.5813\n",
      " 49/156 [========>.....................] - ETA: 4s - loss: 1.1748 - acc: 0.5808\n",
      " 51/156 [========>.....................] - ETA: 4s - loss: 1.1727 - acc: 0.5803\n",
      " 53/156 [=========>....................] - ETA: 4s - loss: 1.1724 - acc: 0.5806\n",
      " 55/156 [=========>....................] - ETA: 3s - loss: 1.1701 - acc: 0.5807\n",
      " 57/156 [=========>....................] - ETA: 3s - loss: 1.1707 - acc: 0.5807\n",
      " 59/156 [==========>...................] - ETA: 3s - loss: 1.1679 - acc: 0.5816\u001b[0m\n",
      "\u001b[34m 61/156 [==========>...................] - ETA: 3s - loss: 1.1653 - acc: 0.5828\n",
      " 63/156 [===========>..................] - ETA: 3s - loss: 1.1647 - acc: 0.5827\n",
      " 65/156 [===========>..................] - ETA: 3s - loss: 1.1664 - acc: 0.5823\n",
      " 67/156 [===========>..................] - ETA: 3s - loss: 1.1642 - acc: 0.5830\n",
      " 69/156 [============>.................] - ETA: 3s - loss: 1.1674 - acc: 0.5823\n",
      " 71/156 [============>.................] - ETA: 3s - loss: 1.1657 - acc: 0.5829\n",
      " 73/156 [=============>................] - ETA: 3s - loss: 1.1651 - acc: 0.5840\n",
      " 75/156 [=============>................] - ETA: 3s - loss: 1.1662 - acc: 0.5832\n",
      " 77/156 [=============>................] - ETA: 3s - loss: 1.1677 - acc: 0.5824\n",
      " 79/156 [==============>...............] - ETA: 2s - loss: 1.1680 - acc: 0.5827\n",
      " 81/156 [==============>...............] - ETA: 2s - loss: 1.1657 - acc: 0.5834\n",
      " 83/156 [==============>...............] - ETA: 2s - loss: 1.1645 - acc: 0.5837\u001b[0m\n",
      "\u001b[34m 85/156 [===============>..............] - ETA: 2s - loss: 1.1653 - acc: 0.5838\n",
      " 87/156 [===============>..............] - ETA: 2s - loss: 1.1655 - acc: 0.5840\n",
      " 89/156 [================>.............] - ETA: 2s - loss: 1.1626 - acc: 0.5850\n",
      " 91/156 [================>.............] - ETA: 2s - loss: 1.1620 - acc: 0.5855\n",
      " 93/156 [================>.............] - ETA: 2s - loss: 1.1606 - acc: 0.5864\n",
      " 95/156 [=================>............] - ETA: 2s - loss: 1.1586 - acc: 0.5870\n",
      " 97/156 [=================>............] - ETA: 2s - loss: 1.1564 - acc: 0.5873\n",
      " 99/156 [==================>...........] - ETA: 2s - loss: 1.1550 - acc: 0.5878\u001b[0m\n",
      "\u001b[34m101/156 [==================>...........] - ETA: 2s - loss: 1.1539 - acc: 0.5876\u001b[0m\n",
      "\u001b[34m103/156 [==================>...........] - ETA: 2s - loss: 1.1540 - acc: 0.5875\u001b[0m\n",
      "\u001b[34m105/156 [===================>..........] - ETA: 1s - loss: 1.1531 - acc: 0.5877\u001b[0m\n",
      "\u001b[34m107/156 [===================>..........] - ETA: 1s - loss: 1.1513 - acc: 0.5878\u001b[0m\n",
      "\u001b[34m109/156 [===================>..........] - ETA: 1s - loss: 1.1511 - acc: 0.5878\u001b[0m\n",
      "\u001b[34m111/156 [====================>.........] - ETA: 1s - loss: 1.1512 - acc: 0.5878\u001b[0m\n",
      "\u001b[34m113/156 [====================>.........] - ETA: 1s - loss: 1.1508 - acc: 0.5883\u001b[0m\n",
      "\u001b[34m115/156 [=====================>........] - ETA: 1s - loss: 1.1511 - acc: 0.5881\u001b[0m\n",
      "\u001b[34m117/156 [=====================>........] - ETA: 1s - loss: 1.1503 - acc: 0.5881\u001b[0m\n",
      "\u001b[34m119/156 [=====================>........] - ETA: 1s - loss: 1.1488 - acc: 0.5885\u001b[0m\n",
      "\u001b[34m121/156 [======================>.......] - ETA: 1s - loss: 1.1479 - acc: 0.5891\u001b[0m\n",
      "\u001b[34m123/156 [======================>.......] - ETA: 1s - loss: 1.1481 - acc: 0.5889\u001b[0m\n",
      "\u001b[34m125/156 [=======================>......] - ETA: 1s - loss: 1.1487 - acc: 0.5887\u001b[0m\n",
      "\u001b[34m127/156 [=======================>......] - ETA: 1s - loss: 1.1492 - acc: 0.5886\u001b[0m\n",
      "\u001b[34m129/156 [=======================>......] - ETA: 1s - loss: 1.1495 - acc: 0.5886\u001b[0m\n",
      "\u001b[34m131/156 [========================>.....] - ETA: 0s - loss: 1.1478 - acc: 0.5892\u001b[0m\n",
      "\u001b[34m133/156 [========================>.....] - ETA: 0s - loss: 1.1477 - acc: 0.5893\u001b[0m\n",
      "\u001b[34m135/156 [========================>.....] - ETA: 0s - loss: 1.1462 - acc: 0.5899\u001b[0m\n",
      "\u001b[34m137/156 [=========================>....] - ETA: 0s - loss: 1.1453 - acc: 0.5904\u001b[0m\n",
      "\u001b[34m139/156 [=========================>....] - ETA: 0s - loss: 1.1447 - acc: 0.5904\u001b[0m\n",
      "\u001b[34m141/156 [==========================>...] - ETA: 0s - loss: 1.1447 - acc: 0.5906\u001b[0m\n",
      "\u001b[34m143/156 [==========================>...] - ETA: 0s - loss: 1.1436 - acc: 0.5911\u001b[0m\n",
      "\u001b[34m145/156 [==========================>...] - ETA: 0s - loss: 1.1426 - acc: 0.5916\u001b[0m\n",
      "\u001b[34m147/156 [===========================>..] - ETA: 0s - loss: 1.1426 - acc: 0.5918\u001b[0m\n",
      "\u001b[34m149/156 [===========================>..] - ETA: 0s - loss: 1.1413 - acc: 0.5924\u001b[0m\n",
      "\u001b[34m151/156 [============================>.] - ETA: 0s - loss: 1.1417 - acc: 0.5922\u001b[0m\n",
      "\u001b[34m153/156 [============================>.] - ETA: 0s - loss: 1.1415 - acc: 0.5923\u001b[0m\n",
      "\u001b[34m155/156 [============================>.] - ETA: 0s - loss: 1.1401 - acc: 0.5927\u001b[0m\n",
      "\u001b[34m156/156 [==============================] - 7s 46ms/step - loss: 1.1391 - acc: 0.5929 - val_loss: 1.0973 - val_acc: 0.6036\u001b[0m\n",
      "\u001b[34mEpoch 6/10\n",
      "\n",
      "  1/156 [..............................] - ETA: 6s - loss: 0.9442 - acc: 0.6641\n",
      "  3/156 [..............................] - ETA: 5s - loss: 1.0231 - acc: 0.6289\n",
      "  5/156 [..............................] - ETA: 5s - loss: 1.0158 - acc: 0.6289\n",
      "  7/156 [>.............................] - ETA: 5s - loss: 1.0084 - acc: 0.6295\n",
      "  9/156 [>.............................] - ETA: 5s - loss: 1.0338 - acc: 0.6168\n",
      " 11/156 [=>............................] - ETA: 5s - loss: 1.0486 - acc: 0.6122\n",
      " 13/156 [=>............................] - ETA: 5s - loss: 1.0533 - acc: 0.6097\n",
      " 15/156 [=>............................] - ETA: 5s - loss: 1.0565 - acc: 0.6112\n",
      " 17/156 [==>...........................] - ETA: 5s - loss: 1.0672 - acc: 0.6096\u001b[0m\n",
      "\u001b[34m 19/156 [==>...........................] - ETA: 5s - loss: 1.0669 - acc: 0.6108\n",
      " 21/156 [===>..........................] - ETA: 5s - loss: 1.0658 - acc: 0.6116\n",
      " 23/156 [===>..........................] - ETA: 5s - loss: 1.0637 - acc: 0.6124\n",
      " 25/156 [===>..........................] - ETA: 5s - loss: 1.0636 - acc: 0.6128\n",
      " 27/156 [====>.........................] - ETA: 5s - loss: 1.0637 - acc: 0.6134\n",
      " 29/156 [====>.........................] - ETA: 4s - loss: 1.0612 - acc: 0.6141\n",
      " 31/156 [====>.........................] - ETA: 4s - loss: 1.0632 - acc: 0.6140\n",
      " 33/156 [=====>........................] - ETA: 4s - loss: 1.0615 - acc: 0.6152\n",
      " 35/156 [=====>........................] - ETA: 4s - loss: 1.0645 - acc: 0.6133\n",
      " 37/156 [======>.......................] - ETA: 4s - loss: 1.0635 - acc: 0.6142\n",
      " 39/156 [======>.......................] - ETA: 4s - loss: 1.0614 - acc: 0.6154\n",
      " 41/156 [======>.......................] - ETA: 4s - loss: 1.0684 - acc: 0.6128\n",
      " 43/156 [=======>......................] - ETA: 4s - loss: 1.0685 - acc: 0.6136\u001b[0m\n",
      "\u001b[34m 45/156 [=======>......................] - ETA: 4s - loss: 1.0706 - acc: 0.6141\n",
      " 47/156 [========>.....................] - ETA: 4s - loss: 1.0701 - acc: 0.6132\n",
      " 49/156 [========>.....................] - ETA: 4s - loss: 1.0708 - acc: 0.6129\n",
      " 51/156 [========>.....................] - ETA: 4s - loss: 1.0714 - acc: 0.6124\n",
      " 53/156 [=========>....................] - ETA: 4s - loss: 1.0713 - acc: 0.6120\n",
      " 55/156 [=========>....................] - ETA: 3s - loss: 1.0716 - acc: 0.6118\n",
      " 57/156 [=========>....................] - ETA: 3s - loss: 1.0719 - acc: 0.6111\n",
      " 59/156 [==========>...................] - ETA: 3s - loss: 1.0706 - acc: 0.6119\n",
      " 61/156 [==========>...................] - ETA: 3s - loss: 1.0721 - acc: 0.6110\n",
      " 63/156 [===========>..................] - ETA: 3s - loss: 1.0731 - acc: 0.6112\n",
      " 65/156 [===========>..................] - ETA: 3s - loss: 1.0747 - acc: 0.6112\n",
      " 67/156 [===========>..................] - ETA: 3s - loss: 1.0723 - acc: 0.6124\u001b[0m\n",
      "\u001b[34m 69/156 [============>.................] - ETA: 3s - loss: 1.0708 - acc: 0.6132\n",
      " 71/156 [============>.................] - ETA: 3s - loss: 1.0717 - acc: 0.6131\n",
      " 73/156 [=============>................] - ETA: 3s - loss: 1.0711 - acc: 0.6137\n",
      " 75/156 [=============>................] - ETA: 3s - loss: 1.0694 - acc: 0.6142\n",
      " 77/156 [=============>................] - ETA: 3s - loss: 1.0677 - acc: 0.6153\n",
      " 79/156 [==============>...............] - ETA: 3s - loss: 1.0690 - acc: 0.6152\n",
      " 81/156 [==============>...............] - ETA: 2s - loss: 1.0664 - acc: 0.6163\n",
      " 83/156 [==============>...............] - ETA: 2s - loss: 1.0649 - acc: 0.6171\n",
      " 85/156 [===============>..............] - ETA: 2s - loss: 1.0622 - acc: 0.6182\n",
      " 87/156 [===============>..............] - ETA: 2s - loss: 1.0623 - acc: 0.6189\n",
      " 89/156 [================>.............] - ETA: 2s - loss: 1.0612 - acc: 0.6191\n",
      " 91/156 [================>.............] - ETA: 2s - loss: 1.0631 - acc: 0.6180\n",
      " 93/156 [================>.............] - ETA: 2s - loss: 1.0634 - acc: 0.6176\u001b[0m\n",
      "\u001b[34m 95/156 [=================>............] - ETA: 2s - loss: 1.0644 - acc: 0.6171\n",
      " 97/156 [=================>............] - ETA: 2s - loss: 1.0638 - acc: 0.6171\n",
      " 99/156 [==================>...........] - ETA: 2s - loss: 1.0654 - acc: 0.6169\u001b[0m\n",
      "\u001b[34m101/156 [==================>...........] - ETA: 2s - loss: 1.0648 - acc: 0.6169\u001b[0m\n",
      "\u001b[34m103/156 [==================>...........] - ETA: 2s - loss: 1.0631 - acc: 0.6172\u001b[0m\n",
      "\u001b[34m105/156 [===================>..........] - ETA: 1s - loss: 1.0627 - acc: 0.6179\u001b[0m\n",
      "\u001b[34m107/156 [===================>..........] - ETA: 1s - loss: 1.0617 - acc: 0.6178\u001b[0m\n",
      "\u001b[34m109/156 [===================>..........] - ETA: 1s - loss: 1.0626 - acc: 0.6173\u001b[0m\n",
      "\u001b[34m111/156 [====================>.........] - ETA: 1s - loss: 1.0633 - acc: 0.6171\u001b[0m\n",
      "\u001b[34m113/156 [====================>.........] - ETA: 1s - loss: 1.0617 - acc: 0.6176\u001b[0m\n",
      "\u001b[34m115/156 [=====================>........] - ETA: 1s - loss: 1.0623 - acc: 0.6172\u001b[0m\n",
      "\u001b[34m117/156 [=====================>........] - ETA: 1s - loss: 1.0603 - acc: 0.6181\u001b[0m\n",
      "\u001b[34m119/156 [=====================>........] - ETA: 1s - loss: 1.0597 - acc: 0.6181\u001b[0m\n",
      "\u001b[34m121/156 [======================>.......] - ETA: 1s - loss: 1.0592 - acc: 0.6183\u001b[0m\n",
      "\u001b[34m123/156 [======================>.......] - ETA: 1s - loss: 1.0597 - acc: 0.6184\u001b[0m\n",
      "\u001b[34m125/156 [=======================>......] - ETA: 1s - loss: 1.0591 - acc: 0.6187\u001b[0m\n",
      "\u001b[34m127/156 [=======================>......] - ETA: 1s - loss: 1.0589 - acc: 0.6188\u001b[0m\n",
      "\u001b[34m129/156 [=======================>......] - ETA: 1s - loss: 1.0585 - acc: 0.6193\u001b[0m\n",
      "\u001b[34m131/156 [========================>.....] - ETA: 0s - loss: 1.0585 - acc: 0.6194\u001b[0m\n",
      "\u001b[34m133/156 [========================>.....] - ETA: 0s - loss: 1.0579 - acc: 0.6202\u001b[0m\n",
      "\u001b[34m135/156 [========================>.....] - ETA: 0s - loss: 1.0581 - acc: 0.6206\u001b[0m\n",
      "\u001b[34m137/156 [=========================>....] - ETA: 0s - loss: 1.0577 - acc: 0.6205\u001b[0m\n",
      "\u001b[34m139/156 [=========================>....] - ETA: 0s - loss: 1.0573 - acc: 0.6207\u001b[0m\n",
      "\u001b[34m141/156 [==========================>...] - ETA: 0s - loss: 1.0569 - acc: 0.6209\u001b[0m\n",
      "\u001b[34m143/156 [==========================>...] - ETA: 0s - loss: 1.0562 - acc: 0.6214\u001b[0m\n",
      "\u001b[34m145/156 [==========================>...] - ETA: 0s - loss: 1.0566 - acc: 0.6216\u001b[0m\n",
      "\u001b[34m147/156 [===========================>..] - ETA: 0s - loss: 1.0565 - acc: 0.6218\u001b[0m\n",
      "\u001b[34m149/156 [===========================>..] - ETA: 0s - loss: 1.0568 - acc: 0.6218\u001b[0m\n",
      "\u001b[34m151/156 [============================>.] - ETA: 0s - loss: 1.0565 - acc: 0.6217\u001b[0m\n",
      "\u001b[34m153/156 [============================>.] - ETA: 0s - loss: 1.0572 - acc: 0.6215\u001b[0m\n",
      "\u001b[34m155/156 [============================>.] - ETA: 0s - loss: 1.0570 - acc: 0.6215\u001b[0m\n",
      "\u001b[34m156/156 [==============================] - 7s 46ms/step - loss: 1.0563 - acc: 0.6216 - val_loss: 0.9659 - val_acc: 0.6465\u001b[0m\n",
      "\u001b[34mEpoch 7/10\n",
      "\n",
      "  1/156 [..............................] - ETA: 6s - loss: 1.0878 - acc: 0.6211\n",
      "  3/156 [..............................] - ETA: 5s - loss: 1.0392 - acc: 0.6211\n",
      "  5/156 [..............................] - ETA: 5s - loss: 1.0517 - acc: 0.6195\u001b[0m\n",
      "\u001b[34m  7/156 [>.............................] - ETA: 5s - loss: 1.0540 - acc: 0.6228\n",
      "  9/156 [>.............................] - ETA: 5s - loss: 1.0401 - acc: 0.6324\n",
      " 11/156 [=>............................] - ETA: 5s - loss: 1.0256 - acc: 0.6371\n",
      " 13/156 [=>............................] - ETA: 5s - loss: 1.0328 - acc: 0.6355\n",
      " 15/156 [=>............................] - ETA: 5s - loss: 1.0479 - acc: 0.6312\n",
      " 17/156 [==>...........................] - ETA: 5s - loss: 1.0364 - acc: 0.6333\n",
      " 19/156 [==>...........................] - ETA: 5s - loss: 1.0400 - acc: 0.6324\n",
      " 21/156 [===>..........................] - ETA: 5s - loss: 1.0383 - acc: 0.6330\n",
      " 23/156 [===>..........................] - ETA: 5s - loss: 1.0369 - acc: 0.6330\n",
      " 25/156 [===>..........................] - ETA: 5s - loss: 1.0379 - acc: 0.6330\n",
      " 27/156 [====>.........................] - ETA: 5s - loss: 1.0331 - acc: 0.6356\n",
      " 29/156 [====>.........................] - ETA: 4s - loss: 1.0396 - acc: 0.6319\n",
      " 31/156 [====>.........................] - ETA: 4s - loss: 1.0329 - acc: 0.6321\u001b[0m\n",
      "\u001b[34m 33/156 [=====>........................] - ETA: 4s - loss: 1.0317 - acc: 0.6326\n",
      " 35/156 [=====>........................] - ETA: 4s - loss: 1.0331 - acc: 0.6317\n",
      " 37/156 [======>.......................] - ETA: 4s - loss: 1.0328 - acc: 0.6314\n",
      " 39/156 [======>.......................] - ETA: 4s - loss: 1.0300 - acc: 0.6321\n",
      " 41/156 [======>.......................] - ETA: 4s - loss: 1.0304 - acc: 0.6321\n",
      " 43/156 [=======>......................] - ETA: 4s - loss: 1.0293 - acc: 0.6325\n",
      " 45/156 [=======>......................] - ETA: 4s - loss: 1.0300 - acc: 0.6322\n",
      " 47/156 [========>.....................] - ETA: 4s - loss: 1.0279 - acc: 0.6332\n",
      " 49/156 [========>.....................] - ETA: 4s - loss: 1.0281 - acc: 0.6330\n",
      " 51/156 [========>.....................] - ETA: 4s - loss: 1.0260 - acc: 0.6343\n",
      " 53/156 [=========>....................] - ETA: 4s - loss: 1.0275 - acc: 0.6334\n",
      " 55/156 [=========>....................] - ETA: 3s - loss: 1.0254 - acc: 0.6335\n",
      " 57/156 [=========>....................] - ETA: 3s - loss: 1.0241 - acc: 0.6344\u001b[0m\n",
      "\u001b[34m 59/156 [==========>...................] - ETA: 3s - loss: 1.0206 - acc: 0.6361\n",
      " 61/156 [==========>...................] - ETA: 3s - loss: 1.0222 - acc: 0.6351\n",
      " 63/156 [===========>..................] - ETA: 3s - loss: 1.0206 - acc: 0.6365\n",
      " 65/156 [===========>..................] - ETA: 3s - loss: 1.0203 - acc: 0.6369\n",
      " 67/156 [===========>..................] - ETA: 3s - loss: 1.0189 - acc: 0.6368\n",
      " 69/156 [============>.................] - ETA: 3s - loss: 1.0183 - acc: 0.6376\n",
      " 71/156 [============>.................] - ETA: 3s - loss: 1.0188 - acc: 0.6377\n",
      " 73/156 [=============>................] - ETA: 3s - loss: 1.0161 - acc: 0.6390\n",
      " 75/156 [=============>................] - ETA: 3s - loss: 1.0153 - acc: 0.6398\n",
      " 77/156 [=============>................] - ETA: 3s - loss: 1.0122 - acc: 0.6413\n",
      " 79/156 [==============>...............] - ETA: 3s - loss: 1.0107 - acc: 0.6417\n",
      " 81/156 [==============>...............] - ETA: 2s - loss: 1.0090 - acc: 0.6422\n",
      " 83/156 [==============>...............] - ETA: 2s - loss: 1.0088 - acc: 0.6417\u001b[0m\n",
      "\u001b[34m 85/156 [===============>..............] - ETA: 2s - loss: 1.0096 - acc: 0.6414\n",
      " 87/156 [===============>..............] - ETA: 2s - loss: 1.0097 - acc: 0.6414\n",
      " 89/156 [================>.............] - ETA: 2s - loss: 1.0090 - acc: 0.6419\n",
      " 91/156 [================>.............] - ETA: 2s - loss: 1.0078 - acc: 0.6424\n",
      " 93/156 [================>.............] - ETA: 2s - loss: 1.0090 - acc: 0.6420\n",
      " 95/156 [=================>............] - ETA: 2s - loss: 1.0075 - acc: 0.6426\n",
      " 97/156 [=================>............] - ETA: 2s - loss: 1.0055 - acc: 0.6435\n",
      " 99/156 [==================>...........] - ETA: 2s - loss: 1.0073 - acc: 0.6429\u001b[0m\n",
      "\u001b[34m101/156 [==================>...........] - ETA: 2s - loss: 1.0066 - acc: 0.6430\u001b[0m\n",
      "\u001b[34m103/156 [==================>...........] - ETA: 2s - loss: 1.0055 - acc: 0.6438\u001b[0m\n",
      "\u001b[34m105/156 [===================>..........] - ETA: 1s - loss: 1.0056 - acc: 0.6435\u001b[0m\n",
      "\u001b[34m107/156 [===================>..........] - ETA: 1s - loss: 1.0044 - acc: 0.6433\u001b[0m\n",
      "\u001b[34m109/156 [===================>..........] - ETA: 1s - loss: 1.0022 - acc: 0.6439\u001b[0m\n",
      "\u001b[34m111/156 [====================>.........] - ETA: 1s - loss: 1.0019 - acc: 0.6433\u001b[0m\n",
      "\u001b[34m113/156 [====================>.........] - ETA: 1s - loss: 1.0015 - acc: 0.6440\u001b[0m\n",
      "\u001b[34m115/156 [=====================>........] - ETA: 1s - loss: 1.0016 - acc: 0.6436\u001b[0m\n",
      "\u001b[34m117/156 [=====================>........] - ETA: 1s - loss: 1.0009 - acc: 0.6437\u001b[0m\n",
      "\u001b[34m119/156 [=====================>........] - ETA: 1s - loss: 0.9992 - acc: 0.6441\u001b[0m\n",
      "\u001b[34m121/156 [======================>.......] - ETA: 1s - loss: 0.9996 - acc: 0.6438\u001b[0m\n",
      "\u001b[34m123/156 [======================>.......] - ETA: 1s - loss: 1.0003 - acc: 0.6435\u001b[0m\n",
      "\u001b[34m125/156 [=======================>......] - ETA: 1s - loss: 1.0002 - acc: 0.6434\u001b[0m\n",
      "\u001b[34m127/156 [=======================>......] - ETA: 1s - loss: 1.0005 - acc: 0.6434\u001b[0m\n",
      "\u001b[34m129/156 [=======================>......] - ETA: 1s - loss: 0.9995 - acc: 0.6436\u001b[0m\n",
      "\u001b[34m131/156 [========================>.....] - ETA: 0s - loss: 0.9991 - acc: 0.6438\u001b[0m\n",
      "\u001b[34m133/156 [========================>.....] - ETA: 0s - loss: 0.9994 - acc: 0.6438\u001b[0m\n",
      "\u001b[34m135/156 [========================>.....] - ETA: 0s - loss: 0.9977 - acc: 0.6444\u001b[0m\n",
      "\u001b[34m137/156 [=========================>....] - ETA: 0s - loss: 0.9971 - acc: 0.6448\u001b[0m\n",
      "\u001b[34m139/156 [=========================>....] - ETA: 0s - loss: 0.9967 - acc: 0.6449\u001b[0m\n",
      "\u001b[34m141/156 [==========================>...] - ETA: 0s - loss: 0.9966 - acc: 0.6449\u001b[0m\n",
      "\u001b[34m143/156 [==========================>...] - ETA: 0s - loss: 0.9957 - acc: 0.6451\u001b[0m\n",
      "\u001b[34m145/156 [==========================>...] - ETA: 0s - loss: 0.9960 - acc: 0.6446\u001b[0m\n",
      "\u001b[34m147/156 [===========================>..] - ETA: 0s - loss: 0.9960 - acc: 0.6446\u001b[0m\n",
      "\u001b[34m149/156 [===========================>..] - ETA: 0s - loss: 0.9952 - acc: 0.6449\u001b[0m\n",
      "\u001b[34m151/156 [============================>.] - ETA: 0s - loss: 0.9957 - acc: 0.6451\u001b[0m\n",
      "\u001b[34m153/156 [============================>.] - ETA: 0s - loss: 0.9948 - acc: 0.6455\u001b[0m\n",
      "\u001b[34m155/156 [============================>.] - ETA: 0s - loss: 0.9935 - acc: 0.6460\u001b[0m\n",
      "\u001b[34m156/156 [==============================] - 7s 46ms/step - loss: 0.9930 - acc: 0.6462 - val_loss: 1.0195 - val_acc: 0.6424\u001b[0m\n",
      "\u001b[34mEpoch 8/10\n",
      "\n",
      "  1/156 [..............................] - ETA: 6s - loss: 1.0614 - acc: 0.5938\n",
      "  3/156 [..............................] - ETA: 6s - loss: 0.9730 - acc: 0.6328\n",
      "  5/156 [..............................] - ETA: 6s - loss: 0.9520 - acc: 0.6469\n",
      "  7/156 [>.............................] - ETA: 6s - loss: 0.9361 - acc: 0.6574\n",
      "  8/156 [>.............................] - ETA: 6s - loss: 0.9369 - acc: 0.6558\n",
      " 10/156 [>.............................] - ETA: 6s - loss: 0.9498 - acc: 0.6523\n",
      " 12/156 [=>............................] - ETA: 6s - loss: 0.9538 - acc: 0.6507\n",
      " 14/156 [=>............................] - ETA: 6s - loss: 0.9552 - acc: 0.6462\n",
      " 16/156 [==>...........................] - ETA: 5s - loss: 0.9657 - acc: 0.6458\n",
      " 18/156 [==>...........................] - ETA: 5s - loss: 0.9782 - acc: 0.6441\n",
      " 20/156 [==>...........................] - ETA: 5s - loss: 0.9743 - acc: 0.6459\u001b[0m\n",
      "\u001b[34m 22/156 [===>..........................] - ETA: 5s - loss: 0.9754 - acc: 0.6479\n",
      " 24/156 [===>..........................] - ETA: 5s - loss: 0.9754 - acc: 0.6486\n",
      " 26/156 [====>.........................] - ETA: 5s - loss: 0.9797 - acc: 0.6477\n",
      " 28/156 [====>.........................] - ETA: 5s - loss: 0.9782 - acc: 0.6476\n",
      " 30/156 [====>.........................] - ETA: 5s - loss: 0.9810 - acc: 0.6465\n",
      " 32/156 [=====>........................] - ETA: 4s - loss: 0.9811 - acc: 0.6471\n",
      " 34/156 [=====>........................] - ETA: 4s - loss: 0.9827 - acc: 0.6468\n",
      " 36/156 [=====>........................] - ETA: 4s - loss: 0.9826 - acc: 0.6483\n",
      " 38/156 [======>.......................] - ETA: 4s - loss: 0.9821 - acc: 0.6477\n",
      " 40/156 [======>.......................] - ETA: 4s - loss: 0.9773 - acc: 0.6498\n",
      " 42/156 [=======>......................] - ETA: 4s - loss: 0.9721 - acc: 0.6501\n",
      " 44/156 [=======>......................] - ETA: 4s - loss: 0.9717 - acc: 0.6501\n",
      " 46/156 [=======>......................] - ETA: 4s - loss: 0.9685 - acc: 0.6512\u001b[0m\n",
      "\u001b[34m 48/156 [========>.....................] - ETA: 4s - loss: 0.9658 - acc: 0.6512\n",
      " 50/156 [========>.....................] - ETA: 4s - loss: 0.9643 - acc: 0.6521\n",
      " 52/156 [=========>....................] - ETA: 4s - loss: 0.9629 - acc: 0.6538\n",
      " 54/156 [=========>....................] - ETA: 4s - loss: 0.9647 - acc: 0.6540\n",
      " 56/156 [=========>....................] - ETA: 3s - loss: 0.9626 - acc: 0.6542\n",
      " 58/156 [==========>...................] - ETA: 3s - loss: 0.9608 - acc: 0.6544\n",
      " 60/156 [==========>...................] - ETA: 3s - loss: 0.9608 - acc: 0.6540\n",
      " 62/156 [==========>...................] - ETA: 3s - loss: 0.9629 - acc: 0.6523\n",
      " 64/156 [===========>..................] - ETA: 3s - loss: 0.9637 - acc: 0.6525\n",
      " 66/156 [===========>..................] - ETA: 3s - loss: 0.9626 - acc: 0.6539\n",
      " 68/156 [============>.................] - ETA: 3s - loss: 0.9633 - acc: 0.6540\n",
      " 70/156 [============>.................] - ETA: 3s - loss: 0.9617 - acc: 0.6552\n",
      " 72/156 [============>.................] - ETA: 3s - loss: 0.9621 - acc: 0.6549\u001b[0m\n",
      "\u001b[34m 74/156 [=============>................] - ETA: 3s - loss: 0.9624 - acc: 0.6550\n",
      " 76/156 [=============>................] - ETA: 3s - loss: 0.9631 - acc: 0.6546\n",
      " 78/156 [==============>...............] - ETA: 3s - loss: 0.9620 - acc: 0.6549\n",
      " 80/156 [==============>...............] - ETA: 3s - loss: 0.9625 - acc: 0.6551\n",
      " 82/156 [==============>...............] - ETA: 2s - loss: 0.9632 - acc: 0.6552\n",
      " 84/156 [===============>..............] - ETA: 2s - loss: 0.9626 - acc: 0.6555\n",
      " 86/156 [===============>..............] - ETA: 2s - loss: 0.9620 - acc: 0.6560\n",
      " 88/156 [===============>..............] - ETA: 2s - loss: 0.9638 - acc: 0.6555\n",
      " 90/156 [================>.............] - ETA: 2s - loss: 0.9639 - acc: 0.6555\n",
      " 92/156 [================>.............] - ETA: 2s - loss: 0.9634 - acc: 0.6558\n",
      " 94/156 [=================>............] - ETA: 2s - loss: 0.9653 - acc: 0.6555\n",
      " 96/156 [=================>............] - ETA: 2s - loss: 0.9645 - acc: 0.6558\u001b[0m\n",
      "\u001b[34m 98/156 [=================>............] - ETA: 2s - loss: 0.9636 - acc: 0.6565\u001b[0m\n",
      "\u001b[34m100/156 [==================>...........] - ETA: 2s - loss: 0.9623 - acc: 0.6570\u001b[0m\n",
      "\u001b[34m102/156 [==================>...........] - ETA: 2s - loss: 0.9625 - acc: 0.6567\u001b[0m\n",
      "\u001b[34m104/156 [===================>..........] - ETA: 2s - loss: 0.9634 - acc: 0.6569\u001b[0m\n",
      "\u001b[34m106/156 [===================>..........] - ETA: 1s - loss: 0.9625 - acc: 0.6571\u001b[0m\n",
      "\u001b[34m108/156 [===================>..........] - ETA: 1s - loss: 0.9611 - acc: 0.6577\u001b[0m\n",
      "\u001b[34m110/156 [====================>.........] - ETA: 1s - loss: 0.9601 - acc: 0.6581\u001b[0m\n",
      "\u001b[34m112/156 [====================>.........] - ETA: 1s - loss: 0.9595 - acc: 0.6584\u001b[0m\n",
      "\u001b[34m114/156 [====================>.........] - ETA: 1s - loss: 0.9577 - acc: 0.6588\u001b[0m\n",
      "\u001b[34m116/156 [=====================>........] - ETA: 1s - loss: 0.9567 - acc: 0.6591\u001b[0m\n",
      "\u001b[34m118/156 [=====================>........] - ETA: 1s - loss: 0.9555 - acc: 0.6595\u001b[0m\n",
      "\u001b[34m120/156 [======================>.......] - ETA: 1s - loss: 0.9565 - acc: 0.6592\u001b[0m\n",
      "\u001b[34m122/156 [======================>.......] - ETA: 1s - loss: 0.9560 - acc: 0.6592\u001b[0m\n",
      "\u001b[34m124/156 [======================>.......] - ETA: 1s - loss: 0.9563 - acc: 0.6592\u001b[0m\n",
      "\u001b[34m126/156 [=======================>......] - ETA: 1s - loss: 0.9554 - acc: 0.6600\u001b[0m\n",
      "\u001b[34m128/156 [=======================>......] - ETA: 1s - loss: 0.9550 - acc: 0.6601\u001b[0m\n",
      "\u001b[34m130/156 [========================>.....] - ETA: 1s - loss: 0.9554 - acc: 0.6597\u001b[0m\n",
      "\u001b[34m132/156 [========================>.....] - ETA: 0s - loss: 0.9553 - acc: 0.6599\u001b[0m\n",
      "\u001b[34m134/156 [========================>.....] - ETA: 0s - loss: 0.9557 - acc: 0.6598\u001b[0m\n",
      "\u001b[34m136/156 [=========================>....] - ETA: 0s - loss: 0.9551 - acc: 0.6599\u001b[0m\n",
      "\u001b[34m138/156 [=========================>....] - ETA: 0s - loss: 0.9550 - acc: 0.6596\u001b[0m\n",
      "\u001b[34m140/156 [=========================>....] - ETA: 0s - loss: 0.9551 - acc: 0.6599\u001b[0m\n",
      "\u001b[34m142/156 [==========================>...] - ETA: 0s - loss: 0.9542 - acc: 0.6603\u001b[0m\n",
      "\u001b[34m144/156 [==========================>...] - ETA: 0s - loss: 0.9542 - acc: 0.6601\u001b[0m\n",
      "\u001b[34m146/156 [===========================>..] - ETA: 0s - loss: 0.9531 - acc: 0.6607\u001b[0m\n",
      "\u001b[34m148/156 [===========================>..] - ETA: 0s - loss: 0.9530 - acc: 0.6609\u001b[0m\n",
      "\u001b[34m150/156 [===========================>..] - ETA: 0s - loss: 0.9524 - acc: 0.6609\u001b[0m\n",
      "\u001b[34m152/156 [============================>.] - ETA: 0s - loss: 0.9514 - acc: 0.6615\u001b[0m\n",
      "\u001b[34m154/156 [============================>.] - ETA: 0s - loss: 0.9512 - acc: 0.6617\u001b[0m\n",
      "\u001b[34m156/156 [==============================] - 7s 46ms/step - loss: 0.9512 - acc: 0.6618 - val_loss: 0.8410 - val_acc: 0.6995\u001b[0m\n",
      "\u001b[34mEpoch 9/10\n",
      "\n",
      "  1/156 [..............................] - ETA: 6s - loss: 1.0582 - acc: 0.6328\n",
      "  3/156 [..............................] - ETA: 5s - loss: 0.9555 - acc: 0.6628\n",
      "  5/156 [..............................] - ETA: 5s - loss: 0.9614 - acc: 0.6547\n",
      "  7/156 [>.............................] - ETA: 5s - loss: 0.9506 - acc: 0.6618\u001b[0m\n",
      "\u001b[34m  9/156 [>.............................] - ETA: 5s - loss: 0.9457 - acc: 0.6649\n",
      " 11/156 [=>............................] - ETA: 5s - loss: 0.9410 - acc: 0.6708\n",
      " 13/156 [=>............................] - ETA: 5s - loss: 0.9373 - acc: 0.6749\n",
      " 15/156 [=>............................] - ETA: 5s - loss: 0.9391 - acc: 0.6721\n",
      " 16/156 [==>...........................] - ETA: 5s - loss: 0.9364 - acc: 0.6729\n",
      " 18/156 [==>...........................] - ETA: 5s - loss: 0.9429 - acc: 0.6714\n",
      " 20/156 [==>...........................] - ETA: 5s - loss: 0.9409 - acc: 0.6705\n",
      " 22/156 [===>..........................] - ETA: 5s - loss: 0.9467 - acc: 0.6674\n",
      " 24/156 [===>..........................] - ETA: 5s - loss: 0.9344 - acc: 0.6711\n",
      " 26/156 [====>.........................] - ETA: 5s - loss: 0.9269 - acc: 0.6734\n",
      " 28/156 [====>.........................] - ETA: 5s - loss: 0.9208 - acc: 0.6740\n",
      " 30/156 [====>.........................] - ETA: 4s - loss: 0.9242 - acc: 0.6733\n",
      " 32/156 [=====>........................] - ETA: 4s - loss: 0.9221 - acc: 0.6736\n",
      " 34/156 [=====>........................] - ETA: 4s - loss: 0.9226 - acc: 0.6737\u001b[0m\n",
      "\u001b[34m 36/156 [=====>........................] - ETA: 4s - loss: 0.9268 - acc: 0.6727\n",
      " 38/156 [======>.......................] - ETA: 4s - loss: 0.9295 - acc: 0.6708\n",
      " 40/156 [======>.......................] - ETA: 4s - loss: 0.9286 - acc: 0.6716\n",
      " 42/156 [=======>......................] - ETA: 4s - loss: 0.9289 - acc: 0.6707\n",
      " 44/156 [=======>......................] - ETA: 4s - loss: 0.9324 - acc: 0.6696\n",
      " 46/156 [=======>......................] - ETA: 4s - loss: 0.9328 - acc: 0.6703\n",
      " 48/156 [========>.....................] - ETA: 4s - loss: 0.9312 - acc: 0.6703\n",
      " 50/156 [========>.....................] - ETA: 4s - loss: 0.9260 - acc: 0.6726\n",
      " 52/156 [=========>....................] - ETA: 4s - loss: 0.9250 - acc: 0.6738\n",
      " 54/156 [=========>....................] - ETA: 3s - loss: 0.9219 - acc: 0.6750\n",
      " 56/156 [=========>....................] - ETA: 3s - loss: 0.9220 - acc: 0.6746\n",
      " 58/156 [==========>...................] - ETA: 3s - loss: 0.9219 - acc: 0.6751\u001b[0m\n",
      "\u001b[34m 60/156 [==========>...................] - ETA: 3s - loss: 0.9229 - acc: 0.6745\n",
      " 62/156 [==========>...................] - ETA: 3s - loss: 0.9229 - acc: 0.6743\n",
      " 64/156 [===========>..................] - ETA: 3s - loss: 0.9213 - acc: 0.6749\n",
      " 66/156 [===========>..................] - ETA: 3s - loss: 0.9211 - acc: 0.6756\n",
      " 68/156 [============>.................] - ETA: 3s - loss: 0.9202 - acc: 0.6757\n",
      " 70/156 [============>.................] - ETA: 3s - loss: 0.9209 - acc: 0.6752\n",
      " 72/156 [============>.................] - ETA: 3s - loss: 0.9211 - acc: 0.6754\n",
      " 74/156 [=============>................] - ETA: 3s - loss: 0.9176 - acc: 0.6763\n",
      " 76/156 [=============>................] - ETA: 3s - loss: 0.9161 - acc: 0.6768\n",
      " 78/156 [==============>...............] - ETA: 3s - loss: 0.9150 - acc: 0.6776\n",
      " 80/156 [==============>...............] - ETA: 2s - loss: 0.9152 - acc: 0.6774\n",
      " 82/156 [==============>...............] - ETA: 2s - loss: 0.9161 - acc: 0.6771\n",
      " 84/156 [===============>..............] - ETA: 2s - loss: 0.9134 - acc: 0.6777\u001b[0m\n",
      "\u001b[34m 86/156 [===============>..............] - ETA: 2s - loss: 0.9135 - acc: 0.6775\n",
      " 88/156 [===============>..............] - ETA: 2s - loss: 0.9137 - acc: 0.6779\n",
      " 90/156 [================>.............] - ETA: 2s - loss: 0.9137 - acc: 0.6779\n",
      " 92/156 [================>.............] - ETA: 2s - loss: 0.9130 - acc: 0.6779\n",
      " 94/156 [=================>............] - ETA: 2s - loss: 0.9116 - acc: 0.6789\n",
      " 96/156 [=================>............] - ETA: 2s - loss: 0.9115 - acc: 0.6786\n",
      " 98/156 [=================>............] - ETA: 2s - loss: 0.9116 - acc: 0.6788\u001b[0m\n",
      "\u001b[34m100/156 [==================>...........] - ETA: 2s - loss: 0.9132 - acc: 0.6787\u001b[0m\n",
      "\u001b[34m102/156 [==================>...........] - ETA: 2s - loss: 0.9114 - acc: 0.6793\u001b[0m\n",
      "\u001b[34m104/156 [===================>..........] - ETA: 2s - loss: 0.9100 - acc: 0.6796\u001b[0m\n",
      "\u001b[34m106/156 [===================>..........] - ETA: 1s - loss: 0.9104 - acc: 0.6794\u001b[0m\n",
      "\u001b[34m108/156 [===================>..........] - ETA: 1s - loss: 0.9109 - acc: 0.6795\u001b[0m\n",
      "\u001b[34m110/156 [====================>.........] - ETA: 1s - loss: 0.9093 - acc: 0.6799\u001b[0m\n",
      "\u001b[34m112/156 [====================>.........] - ETA: 1s - loss: 0.9093 - acc: 0.6798\u001b[0m\n",
      "\u001b[34m114/156 [====================>.........] - ETA: 1s - loss: 0.9097 - acc: 0.6797\u001b[0m\n",
      "\u001b[34m116/156 [=====================>........] - ETA: 1s - loss: 0.9094 - acc: 0.6798\u001b[0m\n",
      "\u001b[34m118/156 [=====================>........] - ETA: 1s - loss: 0.9094 - acc: 0.6797\u001b[0m\n",
      "\u001b[34m120/156 [======================>.......] - ETA: 1s - loss: 0.9078 - acc: 0.6805\u001b[0m\n",
      "\u001b[34m122/156 [======================>.......] - ETA: 1s - loss: 0.9071 - acc: 0.6813\u001b[0m\n",
      "\u001b[34m124/156 [======================>.......] - ETA: 1s - loss: 0.9069 - acc: 0.6813\u001b[0m\n",
      "\u001b[34m126/156 [=======================>......] - ETA: 1s - loss: 0.9053 - acc: 0.6820\u001b[0m\n",
      "\u001b[34m128/156 [=======================>......] - ETA: 1s - loss: 0.9045 - acc: 0.6829\u001b[0m\n",
      "\u001b[34m130/156 [========================>.....] - ETA: 1s - loss: 0.9048 - acc: 0.6826\u001b[0m\n",
      "\u001b[34m132/156 [========================>.....] - ETA: 0s - loss: 0.9034 - acc: 0.6830\u001b[0m\n",
      "\u001b[34m134/156 [========================>.....] - ETA: 0s - loss: 0.9021 - acc: 0.6834\u001b[0m\n",
      "\u001b[34m136/156 [=========================>....] - ETA: 0s - loss: 0.9018 - acc: 0.6832\u001b[0m\n",
      "\u001b[34m138/156 [=========================>....] - ETA: 0s - loss: 0.9015 - acc: 0.6834\u001b[0m\n",
      "\u001b[34m140/156 [=========================>....] - ETA: 0s - loss: 0.9017 - acc: 0.6830\u001b[0m\n",
      "\u001b[34m142/156 [==========================>...] - ETA: 0s - loss: 0.9010 - acc: 0.6833\u001b[0m\n",
      "\u001b[34m144/156 [==========================>...] - ETA: 0s - loss: 0.9008 - acc: 0.6835\u001b[0m\n",
      "\u001b[34m146/156 [===========================>..] - ETA: 0s - loss: 0.9013 - acc: 0.6834\u001b[0m\n",
      "\u001b[34m148/156 [===========================>..] - ETA: 0s - loss: 0.9020 - acc: 0.6833\u001b[0m\n",
      "\u001b[34m150/156 [===========================>..] - ETA: 0s - loss: 0.9019 - acc: 0.6831\u001b[0m\n",
      "\u001b[34m152/156 [============================>.] - ETA: 0s - loss: 0.9023 - acc: 0.6828\u001b[0m\n",
      "\u001b[34m154/156 [============================>.] - ETA: 0s - loss: 0.9020 - acc: 0.6830\u001b[0m\n",
      "\u001b[34m156/156 [==============================] - 7s 46ms/step - loss: 0.9015 - acc: 0.6831 - val_loss: 0.8740 - val_acc: 0.6882\u001b[0m\n",
      "\u001b[34mEpoch 10/10\u001b[0m\n",
      "\u001b[34m  1/156 [..............................] - ETA: 6s - loss: 0.9277 - acc: 0.6836\n",
      "  3/156 [..............................] - ETA: 6s - loss: 0.9369 - acc: 0.6745\n",
      "  5/156 [..............................] - ETA: 6s - loss: 0.9183 - acc: 0.6781\n",
      "  7/156 [>.............................] - ETA: 5s - loss: 0.8908 - acc: 0.6864\n",
      "  9/156 [>.............................] - ETA: 5s - loss: 0.8976 - acc: 0.6888\n",
      " 11/156 [=>............................] - ETA: 5s - loss: 0.9149 - acc: 0.6811\n",
      " 13/156 [=>............................] - ETA: 5s - loss: 0.9039 - acc: 0.6833\n",
      " 15/156 [=>............................] - ETA: 5s - loss: 0.8916 - acc: 0.6867\n",
      " 17/156 [==>...........................] - ETA: 5s - loss: 0.8956 - acc: 0.6847\n",
      " 19/156 [==>...........................] - ETA: 5s - loss: 0.8984 - acc: 0.6840\n",
      " 21/156 [===>..........................] - ETA: 5s - loss: 0.8942 - acc: 0.6851\n",
      " 23/156 [===>..........................] - ETA: 5s - loss: 0.8862 - acc: 0.6894\u001b[0m\n",
      "\u001b[34m 25/156 [===>..........................] - ETA: 5s - loss: 0.8887 - acc: 0.6870\n",
      " 27/156 [====>.........................] - ETA: 5s - loss: 0.8909 - acc: 0.6869\n",
      " 29/156 [====>.........................] - ETA: 4s - loss: 0.8892 - acc: 0.6860\n",
      " 31/156 [====>.........................] - ETA: 4s - loss: 0.8872 - acc: 0.6862\n",
      " 33/156 [=====>........................] - ETA: 4s - loss: 0.8843 - acc: 0.6886\n",
      " 35/156 [=====>........................] - ETA: 4s - loss: 0.8852 - acc: 0.6879\n",
      " 37/156 [======>.......................] - ETA: 4s - loss: 0.8804 - acc: 0.6906\n",
      " 39/156 [======>.......................] - ETA: 4s - loss: 0.8754 - acc: 0.6913\n",
      " 41/156 [======>.......................] - ETA: 4s - loss: 0.8719 - acc: 0.6927\n",
      " 43/156 [=======>......................] - ETA: 4s - loss: 0.8732 - acc: 0.6916\n",
      " 45/156 [=======>......................] - ETA: 4s - loss: 0.8747 - acc: 0.6918\n",
      " 47/156 [========>.....................] - ETA: 4s - loss: 0.8746 - acc: 0.6920\n",
      " 49/156 [========>.....................] - ETA: 4s - loss: 0.8735 - acc: 0.6919\u001b[0m\n",
      "\u001b[34m 51/156 [========>.....................] - ETA: 4s - loss: 0.8740 - acc: 0.6921\n",
      " 53/156 [=========>....................] - ETA: 4s - loss: 0.8738 - acc: 0.6925\n",
      " 55/156 [=========>....................] - ETA: 3s - loss: 0.8756 - acc: 0.6925\n",
      " 57/156 [=========>....................] - ETA: 3s - loss: 0.8783 - acc: 0.6924\n",
      " 59/156 [==========>...................] - ETA: 3s - loss: 0.8780 - acc: 0.6921\n",
      " 61/156 [==========>...................] - ETA: 3s - loss: 0.8787 - acc: 0.6918\n",
      " 63/156 [===========>..................] - ETA: 3s - loss: 0.8782 - acc: 0.6917\n",
      " 65/156 [===========>..................] - ETA: 3s - loss: 0.8773 - acc: 0.6922\n",
      " 67/156 [===========>..................] - ETA: 3s - loss: 0.8759 - acc: 0.6925\n",
      " 69/156 [============>.................] - ETA: 3s - loss: 0.8766 - acc: 0.6927\n",
      " 71/156 [============>.................] - ETA: 3s - loss: 0.8742 - acc: 0.6932\n",
      " 73/156 [=============>................] - ETA: 3s - loss: 0.8740 - acc: 0.6929\n",
      " 75/156 [=============>................] - ETA: 3s - loss: 0.8739 - acc: 0.6924\u001b[0m\n",
      "\u001b[34m 77/156 [=============>................] - ETA: 3s - loss: 0.8720 - acc: 0.6932\n",
      " 80/156 [==============>...............] - ETA: 2s - loss: 0.8727 - acc: 0.6933\n",
      " 84/156 [===============>..............] - ETA: 2s - loss: 0.8710 - acc: 0.6936\n",
      " 88/156 [===============>..............] - ETA: 2s - loss: 0.8713 - acc: 0.6939\n",
      " 92/156 [================>.............] - ETA: 2s - loss: 0.8714 - acc: 0.6939\n",
      " 96/156 [=================>............] - ETA: 2s - loss: 0.8734 - acc: 0.6931\u001b[0m\n",
      "\u001b[34m100/156 [==================>...........] - ETA: 1s - loss: 0.8703 - acc: 0.6941\u001b[0m\n",
      "\u001b[34m104/156 [===================>..........] - ETA: 1s - loss: 0.8710 - acc: 0.6941\u001b[0m\n",
      "\u001b[34m108/156 [===================>..........] - ETA: 1s - loss: 0.8700 - acc: 0.6944\u001b[0m\n",
      "\u001b[34m112/156 [====================>.........] - ETA: 1s - loss: 0.8680 - acc: 0.6953\u001b[0m\n",
      "\u001b[34m116/156 [=====================>........] - ETA: 1s - loss: 0.8673 - acc: 0.6959\u001b[0m\n",
      "\u001b[34m120/156 [======================>.......] - ETA: 1s - loss: 0.8675 - acc: 0.6959\u001b[0m\n",
      "\u001b[34m124/156 [======================>.......] - ETA: 0s - loss: 0.8650 - acc: 0.6964\u001b[0m\n",
      "\u001b[34m128/156 [=======================>......] - ETA: 0s - loss: 0.8681 - acc: 0.6954\u001b[0m\n",
      "\u001b[34m132/156 [========================>.....] - ETA: 0s - loss: 0.8694 - acc: 0.6950\u001b[0m\n",
      "\u001b[34m136/156 [=========================>....] - ETA: 0s - loss: 0.8678 - acc: 0.6953\u001b[0m\n",
      "\u001b[34m140/156 [=========================>....] - ETA: 0s - loss: 0.8682 - acc: 0.6949\u001b[0m\n",
      "\u001b[34m144/156 [==========================>...] - ETA: 0s - loss: 0.8666 - acc: 0.6952\u001b[0m\n",
      "\u001b[34m148/156 [===========================>..] - ETA: 0s - loss: 0.8660 - acc: 0.6955\u001b[0m\n",
      "\u001b[34m152/156 [============================>.] - ETA: 0s - loss: 0.8664 - acc: 0.6956\u001b[0m\n",
      "\u001b[34m156/156 [==============================] - 5s 34ms/step - loss: 0.8651 - acc: 0.6961 - val_loss: 0.7889 - val_acc: 0.7235\u001b[0m\n",
      "\u001b[34mINFO:root:Test loss:0.8098232394609696\u001b[0m\n",
      "\u001b[34mINFO:root:Test accuracy:0.7193509615384616\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:No assets to save.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:No assets to save.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:SavedModel written to: /opt/ml/model/1/saved_model.pb\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:SavedModel written to: /opt/ml/model/1/saved_model.pb\u001b[0m\n",
      "\u001b[34mINFO:root:Model successfully saved at: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2020-04-29 02:32:50,983 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-04-29 02:33:09 Uploading - Uploading generated training model\n",
      "2020-04-29 02:33:09 Completed - Training job completed\n",
      "Training seconds: 189\n",
      "Billable seconds: 189\n"
     ]
    }
   ],
   "source": [
    "remote_inputs = {'train' : dataset_location+'/train', 'validation' : dataset_location+'/validation', 'eval' : dataset_location+'/eval'}\n",
    "estimator.fit(remote_inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the job training metrics\n",
    "SageMaker used the regular expression configured above, to send the job metrics to CloudWatch metrics.\n",
    "You can now view the job metrics directly from the SageMaker console.  \n",
    "\n",
    "login to the [SageMaker console](https://console.aws.amazon.com/sagemaker/home) choose the latest training job, scroll down to the monitor section.  \n",
    "Using CloudWatch metrics, you can change the period and configure the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "CloudWatch metrics: [link](https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metricsV2:query=%7B/aws/sagemaker/TrainingJobs,TrainingJobName%7D%20cifar10-tf-2020-04-29-02-27-40-012)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "After you choose a metric, change the period to 1 Minute (Graphed Metrics -> Period)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import Markdown\n",
    "\n",
    "link = 'https://console.aws.amazon.com/cloudwatch/home?region='+sagemaker_session.boto_region_name+'#metricsV2:query=%7B/aws/sagemaker/TrainingJobs,TrainingJobName%7D%20'+estimator.latest_training_job.job_name\n",
    "display(Markdown('CloudWatch metrics: [link]('+link+')'))\n",
    "display(Markdown('After you choose a metric, change the period to 1 Minute (Graphed Metrics -> Period)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on SageMaker with Pipe Mode input\n",
    "SageMaker Pipe Mode is a mechanism for providing S3 data to a training job via Linux fifos. Training programs can read from the fifo and get high-throughput data transfer from S3, without managing the S3 access in the program itself.\n",
    "Pipe Mode is covered in more detail in the SageMaker [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container-inputdataconfig)\n",
    "\n",
    "in out script, we enabled Pipe Mode using the following code:\n",
    "```python\n",
    "from sagemaker_tensorflow import PipeModeDataset\n",
    "dataset = PipeModeDataset(channel=channel_name, record_format='TFRecord')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "\n",
    "source_dir = os.path.join(os.getcwd(), 'source_dir')\n",
    "estimator_pipe = TensorFlow(base_job_name='pipe-cifar10-tf',\n",
    "                       entry_point='cifar10_keras_main.py',\n",
    "                       source_dir=source_dir,\n",
    "                       role=role,\n",
    "                       framework_version='1.12.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       train_instance_count=1, train_instance_type='ml.p3.2xlarge',\n",
    "                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'pipe'}],\n",
    "                       metric_definitions=keras_metric_definition,\n",
    "                       input_mode='Pipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we set ```wait=False``` if you want to see the output logs, change this to ```wait=True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_inputs = {'train' : dataset_location+'/train/train.tfrecords', 'validation' : dataset_location+'/validation', 'eval' : dataset_location+'/eval'}\n",
    "estimator_pipe.fit(remote_inputs, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we specified the exact filename of training data rather than just the folder name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training with Horovod\n",
    "Horovod is a distributed training framework based on MPI. Horovod is only available with TensorFlow version 1.12 or newer. You can find more details at [Horovod README](https://github.com/horovod/horovod/blob/master/README.rst).\n",
    "\n",
    "To enable Horovod, we need to add the following code to our script:\n",
    "```python\n",
    "import horovod.keras as hvd\n",
    "hvd.init()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "K.set_session(tf.Session(config=config))\n",
    "```\n",
    "\n",
    "Add the following callbacks:\n",
    "```python\n",
    "hvd.callbacks.BroadcastGlobalVariablesCallback(0)\n",
    "hvd.callbacks.MetricAverageCallback()\n",
    "```\n",
    "\n",
    "Configure the optimizer:\n",
    "```python\n",
    "opt = Adam(lr=learning_rate * size, decay=weight_decay)\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "```\n",
    "Choose to save checkpoints and send TensorBoard logs only from the ```python hvd.rank() == 0``` instance.\n",
    "\n",
    "To start a distributed training job with Horovod, configure the job distribution:\n",
    "```python\n",
    "distributions = {'mpi': {\n",
    "                    'enabled': True,\n",
    "                    'processes_per_host': # Number of Horovod processes per host\n",
    "                        }\n",
    "                }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "train_instance_type='ml.p3.8xlarge'\n",
    "train_instance_count = 1\n",
    "gpus_per_host = 4\n",
    "\n",
    "num_of_shards = gpus_per_host * train_instance_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = {'mpi': {\n",
    "                    'enabled': True,\n",
    "                    'processes_per_host': gpus_per_host\n",
    "                        }\n",
    "                }\n",
    "\n",
    "keras_metric_definition = [\n",
    "    {'Name': 'train:loss', 'Regex': '.*loss: ([0-9\\\\.]+) - acc: [0-9\\\\.]+.*'},\n",
    "    {'Name': 'train:accuracy', 'Regex': '.*loss: [0-9\\\\.]+ - acc: ([0-9\\\\.]+).*'},\n",
    "    {'Name': 'validation:accuracy', 'Regex': '.*step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_acc: ([0-9\\\\.]+).*'},\n",
    "    {'Name': 'validation:loss', 'Regex': '.*step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_acc: [0-9\\\\.]+.*'},\n",
    "    {'Name': 'sec/steps', 'Regex': '.* - \\d+s (\\d+)[mu]s/step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_acc: [0-9\\\\.]+'}\n",
    "]\n",
    "\n",
    "hyperparameters = {'epochs': 20, 'batch-size' : 256}\n",
    "\n",
    "input_mode = 'File'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from shard import do_shard\n",
    "\n",
    "def shard_data_and_upload(local_data_dir, num_of_shards):\n",
    "    do_shard(local_data_dir, num_of_shards)\n",
    "    dataset_location = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-cifar10-tf')\n",
    "    display(dataset_location)\n",
    "    \n",
    "    remote_inputs = {}\n",
    "\n",
    "    for idx in range(num_of_shards):\n",
    "        train_channel_name = 'train_{}'.format(idx)\n",
    "        train_channel_location = '{}/train/{}'.format(dataset_location, idx)\n",
    "        remote_inputs[train_channel_name] = train_channel_location\n",
    "        \n",
    "        remote_inputs['validation_{}'.format(idx)] = '{}/validation'.format(dataset_location)\n",
    "\n",
    "    remote_inputs['validation'] = '{}/validation'.format(dataset_location)\n",
    "    remote_inputs['eval'] = '{}/eval'.format(dataset_location)\n",
    "    \n",
    "    return remote_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split for Horovod and upload to S3\n",
    "\n",
    "For Horovod, we need a dedicated input channel for each Horovod worker. In this example, we will use a instance with 4 GPUs (**ml.p3.8xlarge**). So we will shard the train data into four tfrecord files as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/efs/tfkeras-horovod-pipemode-muhyun/keras_script_mode_pipe_mode_horovod/shard.py:9: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/efs/tfkeras-horovod-pipemode-muhyun/keras_script_mode_pipe_mode_horovod/shard.py:11: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "Generating ./data/train/0/train_0.tfrecords\n",
      "Generating ./data/train/1/train_1.tfrecords\n",
      "Generating ./data/train/2/train_2.tfrecords\n",
      "Generating ./data/train/3/train_3.tfrecords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "remote_inputs = shard_data_and_upload('./data', num_of_shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_0': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/train/0',\n",
       " 'validation_0': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/validation',\n",
       " 'train_1': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/train/1',\n",
       " 'validation_1': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/validation',\n",
       " 'train_2': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/train/2',\n",
       " 'validation_2': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/validation',\n",
       " 'train_3': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/train/3',\n",
       " 'validation_3': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/validation',\n",
       " 'validation': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/validation',\n",
       " 'eval': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/eval'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = os.path.join(os.getcwd(), 'source_dir')\n",
    "estimator_dist = TensorFlow(base_job_name='horovod-cifar10-tf',\n",
    "                       entry_point='cifar10_keras_main-tf2.py',\n",
    "                       source_dir=source_dir,\n",
    "                       role=role,\n",
    "                       framework_version='2.1.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       train_instance_count=train_instance_count,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'horovod'}],\n",
    "                       metric_definitions=keras_metric_definition,\n",
    "                       distributions=distributions,\n",
    "                       input_mode=input_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we set ```wait=False``` if you want to see the output logs, change this to ```wait=True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-29 02:34:12 Starting - Starting the training job...\n",
      "2020-04-29 02:34:15 Starting - Launching requested ML instances......\n",
      "2020-04-29 02:35:18 Starting - Preparing the instances for training......\n",
      "2020-04-29 02:36:38 Downloading - Downloading input data\n",
      "2020-04-29 02:36:38 Training - Downloading the training image.........\n",
      "2020-04-29 02:38:08 Training - Training image download completed. Training in progress..\u001b[34m2020-04-29 02:38:12,424 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-04-29 02:38:13,346 sagemaker-containers INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2020-04-29 02:38:13,346 sagemaker-containers INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2020-04-29 02:38:13,352 sagemaker-containers INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2020-04-29 02:38:13,352 sagemaker-containers INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:4'] process_per_hosts: 4 num_processes: 4\u001b[0m\n",
      "\u001b[34m2020-04-29 02:38:13,353 sagemaker-containers INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2020-04-29 02:38:13,398 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 4,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"eval\": \"/opt/ml/input/data/eval\",\n",
      "        \"train_2\": \"/opt/ml/input/data/train_2\",\n",
      "        \"train_1\": \"/opt/ml/input/data/train_1\",\n",
      "        \"validation_3\": \"/opt/ml/input/data/validation_3\",\n",
      "        \"train_0\": \"/opt/ml/input/data/train_0\",\n",
      "        \"validation_2\": \"/opt/ml/input/data/validation_2\",\n",
      "        \"validation_1\": \"/opt/ml/input/data/validation_1\",\n",
      "        \"validation_0\": \"/opt/ml/input/data/validation_0\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train_3\": \"/opt/ml/input/data/train_3\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 256,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"epochs\": 20\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"eval\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train_2\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train_1\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation_3\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train_0\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation_2\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation_1\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation_0\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train_3\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"horovod-cifar10-tf-2020-04-29-02-34-12-249\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-637338777613/horovod-cifar10-tf-2020-04-29-02-34-12-249/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"cifar10_keras_main-tf2\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"cifar10_keras_main-tf2.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":256,\"epochs\":20,\"model_dir\":\"/opt/ml/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=cifar10_keras_main-tf2.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train_0\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train_1\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train_2\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train_3\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation_0\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation_1\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation_2\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation_3\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"eval\",\"train_0\",\"train_1\",\"train_2\",\"train_3\",\"validation\",\"validation_0\",\"validation_1\",\"validation_2\",\"validation_3\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=cifar10_keras_main-tf2\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-637338777613/horovod-cifar10-tf-2020-04-29-02-34-12-249/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train_0\":\"/opt/ml/input/data/train_0\",\"train_1\":\"/opt/ml/input/data/train_1\",\"train_2\":\"/opt/ml/input/data/train_2\",\"train_3\":\"/opt/ml/input/data/train_3\",\"validation\":\"/opt/ml/input/data/validation\",\"validation_0\":\"/opt/ml/input/data/validation_0\",\"validation_1\":\"/opt/ml/input/data/validation_1\",\"validation_2\":\"/opt/ml/input/data/validation_2\",\"validation_3\":\"/opt/ml/input/data/validation_3\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":256,\"epochs\":20,\"model_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train_0\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train_1\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train_2\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train_3\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation_0\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation_1\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation_2\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation_3\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"horovod-cifar10-tf-2020-04-29-02-34-12-249\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-637338777613/horovod-cifar10-tf-2020-04-29-02-34-12-249/source/sourcedir.tar.gz\",\"module_name\":\"cifar10_keras_main-tf2\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"cifar10_keras_main-tf2.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"256\",\"--epochs\",\"20\",\"--model_dir\",\"/opt/ml/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_EVAL=/opt/ml/input/data/eval\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN_2=/opt/ml/input/data/train_2\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN_1=/opt/ml/input/data/train_1\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION_3=/opt/ml/input/data/validation_3\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN_0=/opt/ml/input/data/train_0\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION_2=/opt/ml/input/data/validation_2\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION_1=/opt/ml/input/data/validation_1\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION_0=/opt/ml/input/data/validation_0\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN_3=/opt/ml/input/data/train_3\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:4 -np 4 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to socket -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.6/dist-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_EVAL -x SM_CHANNEL_TRAIN_2 -x SM_CHANNEL_TRAIN_1 -x SM_CHANNEL_VALIDATION_3 -x SM_CHANNEL_TRAIN_0 -x SM_CHANNEL_VALIDATION_2 -x SM_CHANNEL_VALIDATION_1 -x SM_CHANNEL_VALIDATION_0 -x SM_CHANNEL_VALIDATION -x SM_CHANNEL_TRAIN_3 -x SM_HP_BATCH-SIZE -x SM_HP_MODEL_DIR -x SM_HP_EPOCHS -x PYTHONPATH /usr/bin/python3 -m mpi4py cifar10_keras_main-tf2.py --batch-size 256 --epochs 20 --model_dir /opt/ml/model\n",
      "\n",
      "\n",
      " Data for JOB [4479,1] offset 0 Total slots allocated 4\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-228-95#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [4479,1] App: 0 Process rank: 0 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [4479,1] App: 0 Process rank: 1 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [4479,1] App: 0 Process rank: 2 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [4479,1] App: 0 Process rank: 3 Bound: UNBOUND\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mWARNING: Open MPI tried to bind a process but failed.  This is a\u001b[0m\n",
      "\u001b[34mwarning only; your job will continue, though performance may\u001b[0m\n",
      "\u001b[34mbe degraded.\n",
      "\n",
      "  Local host:        ip-10-0-228-95\n",
      "  Application name:  /usr/bin/python3\n",
      "  Error message:     failed to bind memory\n",
      "  Location:          rtc_hwloc.c:447\n",
      "\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Writing TensorBoard logs to s3://sagemaker-us-east-1-637338777613/horovod-cifar10-tf-2020-04-29-02-34-12-249/output\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:Writing TensorBoard logs to s3://sagemaker-us-east-1-637338777613/horovod-cifar10-tf-2020-04-29-02-34-12-249/output\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:Writing TensorBoard logs to s3://sagemaker-us-east-1-637338777613/horovod-cifar10-tf-2020-04-29-02-34-12-249/output\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Using TensorFlow backend.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Using TensorFlow backend.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Using TensorFlow backend.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:Writing TensorBoard logs to s3://sagemaker-us-east-1-637338777613/horovod-cifar10-tf-2020-04-29-02-34-12-249/output\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Using TensorFlow backend.\u001b[0m\n",
      "\u001b[34m[ip-10-0-228-95.ec2.internal:00053] 3 more processes have sent help message help-orte-odls-default.txt / memory not bound\u001b[0m\n",
      "\u001b[34m[ip-10-0-228-95.ec2.internal:00053] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:Starting training\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:Starting training\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:Starting training\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Starting training\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:38:19.212 algo-1:61 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:38:19.212 algo-1:59 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:38:19.212 algo-1:61 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:38:19.212 algo-1:61 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:38:19.213 algo-1:59 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:38:19.213 algo-1:59 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:38:19.240 algo-1:60 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:38:19.241 algo-1:60 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:38:19.241 algo-1:60 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:38:19.249 algo-1:58 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:38:19.250 algo-1:58 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:38:19.250 algo-1:58 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:38:19.254 algo-1:61 INFO keras.py:68] Executing in TF2.x eager mode.TF 2.x eager doesn't provide gradient and optimizer variable values.SageMaker Debugger will not be saving gradients and optimizer variables in this case\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:38:19.254 algo-1:59 INFO keras.py:68] Executing in TF2.x eager mode.TF 2.x eager doesn't provide gradient and optimizer variable values.SageMaker Debugger will not be saving gradients and optimizer variables in this case\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:38:19.268 algo-1:60 INFO keras.py:68] Executing in TF2.x eager mode.TF 2.x eager doesn't provide gradient and optimizer variable values.SageMaker Debugger will not be saving gradients and optimizer variables in this case\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:38:19.982 algo-1:58 INFO keras.py:68] Executing in TF2.x eager mode.TF 2.x eager doesn't provide gradient and optimizer variable values.SageMaker Debugger will not be saving gradients and optimizer variables in this case\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:38:22.884 algo-1:61 INFO hook.py:364] Monitoring the collections: metrics, losses, sm_metrics\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:38:22.896 algo-1:59 INFO hook.py:364] Monitoring the collections: sm_metrics, metrics, losses\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:38:22.907 algo-1:60 INFO hook.py:364] Monitoring the collections: losses, metrics, sm_metrics\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:38:23.478 algo-1:58 INFO hook.py:364] Monitoring the collections: metrics, losses, sm_metrics\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.228.95<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.4.7+cuda10.1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.228.95<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.228.95<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.228.95<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 00 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 01 :    0   2   1   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 02 :    0   3   1   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 03 :    0   3   2   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 04 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 05 :    0   2   1   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 06 :    0   3   1   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 07 :    0   3   2   1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 00 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO Ring 01 : 1[1] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 01 : 0[0] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 01 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO Ring 01 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 02 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO Ring 02 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 02 : 3[3] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO Ring 02 : 2[2] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO Ring 03 : 1[1] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 03 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO Ring 03 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 03 : 3[3] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO Ring 04 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 04 : 0[0] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 04 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO Ring 04 : 2[2] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO Ring 05 : 1[1] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 05 : 0[0] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 05 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO Ring 05 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 06 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO Ring 06 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 06 : 3[3] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO Ring 06 : 2[2] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO Ring 07 : 1[1] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 07 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 07 : 3[3] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO Ring 07 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO comm 0x7f8a40321400 rank 0 nranks 4 cudaDev 0 nvmlDev 0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:213 [1] NCCL INFO comm 0x7fb05c30f1f0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO comm 0x7fc1ec30ed00 rank 3 nranks 4 cudaDev 3 nvmlDev 3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:210 [2] NCCL INFO comm 0x7fb63830f190 rank 2 nranks 4 cudaDev 2 nvmlDev 2 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:38:28.653965: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:38:28.655480: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:ERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.342245). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.342250). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.342250). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.342245). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.342215). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.342215). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:38:28.690248: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.134881). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.134881). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.818665). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.818665). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.410258). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.410258). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00001: saving model to /opt/ml/output/checkpoint-1.ckpt\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:ERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:ERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:ERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 13s - loss: 2.1734 - accuracy: 0.2264 - val_loss: 2.1612 - val_accuracy: 0.1278\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 13s - loss: 2.1967 - accuracy: 0.2264 - val_loss: 2.1612 - val_accuracy: 0.1278\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 13s - loss: 2.1946 - accuracy: 0.2264 - val_loss: 2.1612 - val_accuracy: 0.1278\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 2/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 2/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 2/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 13s - loss: 2.1850 - accuracy: 0.2264 - val_loss: 2.1612 - val_accuracy: 0.1278\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 2/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00002: saving model to /opt/ml/output/checkpoint-2.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 3s - loss: 1.6942 - accuracy: 0.3610 - val_loss: 2.1646 - val_accuracy: 0.2192\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 3s - loss: 1.7004 - accuracy: 0.3610 - val_loss: 2.1646 - val_accuracy: 0.2192\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 3s - loss: 1.6730 - accuracy: 0.3610 - val_loss: 2.1646 - val_accuracy: 0.2192\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 3/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 3/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 3/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.6850 - accuracy: 0.3610 - val_loss: 2.1646 - val_accuracy: 0.2192\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 3/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00003: saving model to /opt/ml/output/checkpoint-3.ckpt\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.5070 - accuracy: 0.4370 - val_loss: 2.1351 - val_accuracy: 0.2925\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 1.5086 - accuracy: 0.4370 - val_loss: 2.1351 - val_accuracy: 0.2925\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 1.5259 - accuracy: 0.4370 - val_loss: 2.1351 - val_accuracy: 0.2925\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 4/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 4/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 4/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.5217 - accuracy: 0.4370 - val_loss: 2.1351 - val_accuracy: 0.2925\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 4/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00004: saving model to /opt/ml/output/checkpoint-4.ckpt\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 1.3463 - accuracy: 0.5049 - val_loss: 1.8127 - val_accuracy: 0.3904\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 1.3437 - accuracy: 0.5049 - val_loss: 1.8127 - val_accuracy: 0.3904\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.3789 - accuracy: 0.5049 - val_loss: 1.8127 - val_accuracy: 0.3904\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 5/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 5/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 5/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.3478 - accuracy: 0.5049 - val_loss: 1.8127 - val_accuracy: 0.3904\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 5/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 5: finished gradual learning rate warmup to 0.004.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 5: finished gradual learning rate warmup to 0.004.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 5: finished gradual learning rate warmup to 0.004.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 5: finished gradual learning rate warmup to 0.004.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00005: saving model to /opt/ml/output/checkpoint-5.ckpt\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.2490 - accuracy: 0.5544 - val_loss: 1.3110 - val_accuracy: 0.5298\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 1.2204 - accuracy: 0.5544 - val_loss: 1.3110 - val_accuracy: 0.5298\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 1.2377 - accuracy: 0.5544 - val_loss: 1.3110 - val_accuracy: 0.5298\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 6/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 6/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 6/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.2207 - accuracy: 0.5544 - val_loss: 1.3110 - val_accuracy: 0.5298\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 6/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00006: saving model to /opt/ml/output/checkpoint-6.ckpt\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 1.1166 - accuracy: 0.5898 - val_loss: 1.5065 - val_accuracy: 0.4762\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 1.1772 - accuracy: 0.5898 - val_loss: 1.5065 - val_accuracy: 0.4762\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.1777 - accuracy: 0.5898 - val_loss: 1.5065 - val_accuracy: 0.4762\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 7/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 7/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 7/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.1455 - accuracy: 0.5898 - val_loss: 1.5065 - val_accuracy: 0.4762\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 7/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00007: saving model to /opt/ml/output/checkpoint-7.ckpt\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.1060 - accuracy: 0.6199 - val_loss: 1.3385 - val_accuracy: 0.5305\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 1.0768 - accuracy: 0.6199 - val_loss: 1.3385 - val_accuracy: 0.5305\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 1.0520 - accuracy: 0.6199 - val_loss: 1.3385 - val_accuracy: 0.5305\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 8/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 8/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 8/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.0512 - accuracy: 0.6199 - val_loss: 1.3385 - val_accuracy: 0.5305\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 8/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00008: saving model to /opt/ml/output/checkpoint-8.ckpt\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.0383 - accuracy: 0.6468 - val_loss: 0.9056 - val_accuracy: 0.6740\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.9982 - accuracy: 0.6468 - val_loss: 0.9056 - val_accuracy: 0.6740\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.9657 - accuracy: 0.6468 - val_loss: 0.9056 - val_accuracy: 0.6740\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 9/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 9/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 9/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.9857 - accuracy: 0.6468 - val_loss: 0.9056 - val_accuracy: 0.6740\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 9/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00009: saving model to /opt/ml/output/checkpoint-9.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.9560 - accuracy: 0.6682 - val_loss: 1.6915 - val_accuracy: 0.4695\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.9525 - accuracy: 0.6682 - val_loss: 1.6915 - val_accuracy: 0.4695\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.9349 - accuracy: 0.6682 - val_loss: 1.6915 - val_accuracy: 0.4695\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 10/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 10/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 10/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.9336 - accuracy: 0.6682 - val_loss: 1.6915 - val_accuracy: 0.4695\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 10/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00010: saving model to /opt/ml/output/checkpoint-10.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.9164 - accuracy: 0.6821 - val_loss: 0.9135 - val_accuracy: 0.6745\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.8904 - accuracy: 0.6821 - val_loss: 0.9135 - val_accuracy: 0.6745\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.9230 - accuracy: 0.6821 - val_loss: 0.9135 - val_accuracy: 0.6745\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 11/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 11/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 11/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.9067 - accuracy: 0.6821 - val_loss: 0.9135 - val_accuracy: 0.6745\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 11/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00011: saving model to /opt/ml/output/checkpoint-11.ckpt\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.8469 - accuracy: 0.6960 - val_loss: 0.9746 - val_accuracy: 0.6679\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.8599 - accuracy: 0.6960 - val_loss: 0.9746 - val_accuracy: 0.6679\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.8901 - accuracy: 0.6960 - val_loss: 0.9746 - val_accuracy: 0.6679\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 12/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 12/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 12/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.8756 - accuracy: 0.6960 - val_loss: 0.9746 - val_accuracy: 0.6679\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 12/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00012: saving model to /opt/ml/output/checkpoint-12.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.8376 - accuracy: 0.7082 - val_loss: 0.9402 - val_accuracy: 0.6724\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.8224 - accuracy: 0.7082 - val_loss: 0.9402 - val_accuracy: 0.6724\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 13/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.8527 - accuracy: 0.7082 - val_loss: 0.9402 - val_accuracy: 0.6724\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 13/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 13/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.8338 - accuracy: 0.7082 - val_loss: 0.9402 - val_accuracy: 0.6724\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 13/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00013: saving model to /opt/ml/output/checkpoint-13.ckpt\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.8272 - accuracy: 0.7214 - val_loss: 0.7588 - val_accuracy: 0.7415\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.7970 - accuracy: 0.7214 - val_loss: 0.7588 - val_accuracy: 0.7415\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.8028 - accuracy: 0.7214 - val_loss: 0.7588 - val_accuracy: 0.7415\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 14/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 14/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 14/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.7808 - accuracy: 0.7214 - val_loss: 0.7588 - val_accuracy: 0.7415\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 14/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00014: saving model to /opt/ml/output/checkpoint-14.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.7744 - accuracy: 0.7328 - val_loss: 0.9374 - val_accuracy: 0.6756\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.7852 - accuracy: 0.7328 - val_loss: 0.9374 - val_accuracy: 0.6756\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.7481 - accuracy: 0.7328 - val_loss: 0.9374 - val_accuracy: 0.6756\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 15/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 15/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 15/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.7779 - accuracy: 0.7328 - val_loss: 0.9374 - val_accuracy: 0.6756\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 15/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00015: saving model to /opt/ml/output/checkpoint-15.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.7569 - accuracy: 0.7426 - val_loss: 1.1064 - val_accuracy: 0.6199\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.7692 - accuracy: 0.7426 - val_loss: 1.1064 - val_accuracy: 0.6199\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.7403 - accuracy: 0.7426 - val_loss: 1.1064 - val_accuracy: 0.6199\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 16/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 16/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 16/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.7413 - accuracy: 0.7426 - val_loss: 1.1064 - val_accuracy: 0.6199\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 16/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00016: saving model to /opt/ml/output/checkpoint-16.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.7138 - accuracy: 0.7466 - val_loss: 0.8189 - val_accuracy: 0.7204\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.7435 - accuracy: 0.7466 - val_loss: 0.8189 - val_accuracy: 0.7204\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.7297 - accuracy: 0.7466 - val_loss: 0.8189 - val_accuracy: 0.7204\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 17/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 17/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 17/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.7223 - accuracy: 0.7466 - val_loss: 0.8189 - val_accuracy: 0.7204\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 17/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00017: saving model to /opt/ml/output/checkpoint-17.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.7256 - accuracy: 0.7573 - val_loss: 0.8286 - val_accuracy: 0.7221\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.6833 - accuracy: 0.7573 - val_loss: 0.8286 - val_accuracy: 0.7221\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.7211 - accuracy: 0.7573 - val_loss: 0.8286 - val_accuracy: 0.7221\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 18/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 18/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 18/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.7015 - accuracy: 0.7573 - val_loss: 0.8286 - val_accuracy: 0.7221\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 18/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00018: saving model to /opt/ml/output/checkpoint-18.ckpt\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.6945 - accuracy: 0.7645 - val_loss: 0.7769 - val_accuracy: 0.7240\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.6808 - accuracy: 0.7645 - val_loss: 0.7769 - val_accuracy: 0.7240\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.6868 - accuracy: 0.7645 - val_loss: 0.7769 - val_accuracy: 0.7240\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 19/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 19/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 19/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.6619 - accuracy: 0.7645 - val_loss: 0.7769 - val_accuracy: 0.7240\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 19/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00019: saving model to /opt/ml/output/checkpoint-19.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 1s - loss: 0.6894 - accuracy: 0.7659 - val_loss: 0.6917 - val_accuracy: 0.7569\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 1s - loss: 0.6962 - accuracy: 0.7659 - val_loss: 0.6917 - val_accuracy: 0.7569\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 1s - loss: 0.6713 - accuracy: 0.7659 - val_loss: 0.6917 - val_accuracy: 0.7569\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 20/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 20/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 20/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 1s - loss: 0.6701 - accuracy: 0.7659 - val_loss: 0.6917 - val_accuracy: 0.7569\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 20/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00020: saving model to /opt/ml/output/checkpoint-20.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 1s - loss: 0.6454 - accuracy: 0.7804 - val_loss: 0.7362 - val_accuracy: 0.7486\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 1s - loss: 0.6288 - accuracy: 0.7804 - val_loss: 0.7362 - val_accuracy: 0.7486\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 1s - loss: 0.6463 - accuracy: 0.7804 - val_loss: 0.7362 - val_accuracy: 0.7486\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:39:11.139 algo-1:59 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:39:11.140 algo-1:60 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:39:11.141 algo-1:61 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 1s - loss: 0.6627 - accuracy: 0.7804 - val_loss: 0.7362 - val_accuracy: 0.7486\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 1s - loss: 0.7840 - accuracy: 0.7296\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Test loss:0.7840161552795997\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Test accuracy:0.729567289352417\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:39:14.308795: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:If using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:If using Keras pass *_constraint arguments to layers.[1,0]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/assets\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/assets\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:39:16.174 algo-1:58 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-04-29 02:39:17,206 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving containers. The SavedModel bundle is under directory \"model\", not a numeric name.\u001b[0m\n",
      "\u001b[34m2020-04-29 02:39:17,206 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-04-29 02:39:27 Uploading - Uploading generated training model\n",
      "2020-04-29 02:39:27 Completed - Training job completed\n",
      "Training seconds: 185\n",
      "Billable seconds: 185\n"
     ]
    }
   ],
   "source": [
    "estimator_dist.fit(remote_inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training with Horovod and Pipe Mode input\n",
    "Ditributed training with Horovod can also utilize SageMaker Pipe Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "train_instance_type='ml.p3.8xlarge'\n",
    "train_instance_count = 1\n",
    "gpus_per_host = 4\n",
    "\n",
    "num_of_shards = gpus_per_host * train_instance_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = {'mpi': {\n",
    "                    'enabled': True,\n",
    "                    'processes_per_host': gpus_per_host\n",
    "                        }\n",
    "                }\n",
    "\n",
    "keras_metric_definition = [\n",
    "    {'Name': 'train:loss', 'Regex': '.*loss: ([0-9\\\\.]+) - acc: [0-9\\\\.]+.*'},\n",
    "    {'Name': 'train:accuracy', 'Regex': '.*loss: [0-9\\\\.]+ - acc: ([0-9\\\\.]+).*'},\n",
    "    {'Name': 'validation:accuracy', 'Regex': '.*step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_acc: ([0-9\\\\.]+).*'},\n",
    "    {'Name': 'validation:loss', 'Regex': '.*step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_acc: [0-9\\\\.]+.*'},\n",
    "    {'Name': 'sec/steps', 'Regex': '.* - \\d+s (\\d+)[mu]s/step - loss: [0-9\\\\.]+ - acc: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_acc: [0-9\\\\.]+'}\n",
    "]\n",
    "\n",
    "hyperparameters = {'epochs': 20, 'batch-size' : 256}\n",
    "\n",
    "input_mode = 'Pipe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from shard import do_shard\n",
    "\n",
    "def shard_data_and_upload(local_data_dir, num_of_shards):\n",
    "    do_shard(local_data_dir, num_of_shards)\n",
    "    dataset_location = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-cifar10-tf')\n",
    "    display(dataset_location)\n",
    "    \n",
    "    remote_inputs = {}\n",
    "\n",
    "    for idx in range(num_of_shards):\n",
    "        train_channel_name = 'train_{}'.format(idx)\n",
    "        train_channel_location = '{}/train/{}'.format(dataset_location, idx)\n",
    "        remote_inputs[train_channel_name] = train_channel_location\n",
    "        \n",
    "        remote_inputs['validation_{}'.format(idx)] = '{}/validation'.format(dataset_location)\n",
    "\n",
    "    remote_inputs['validation'] = '{}/validation'.format(dataset_location)\n",
    "    remote_inputs['eval'] = '{}/eval'.format(dataset_location)\n",
    "    \n",
    "    return remote_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ./data/train/0/train_0.tfrecords\n",
      "Generating ./data/train/1/train_1.tfrecords\n",
      "Generating ./data/train/2/train_2.tfrecords\n",
      "Generating ./data/train/3/train_3.tfrecords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "remote_inputs = shard_data_and_upload('./data', num_of_shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_0': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/train/0',\n",
       " 'validation_0': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/validation',\n",
       " 'train_1': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/train/1',\n",
       " 'validation_1': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/validation',\n",
       " 'train_2': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/train/2',\n",
       " 'validation_2': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/validation',\n",
       " 'train_3': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/train/3',\n",
       " 'validation_3': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/validation',\n",
       " 'validation': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/validation',\n",
       " 'eval': 's3://sagemaker-us-east-1-637338777613/data/DEMO-cifar10-tf/eval'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = os.path.join(os.getcwd(), 'source_dir')\n",
    "estimator_dist = TensorFlow(base_job_name='horovod-pipe-cifar10-tf',\n",
    "                       entry_point='cifar10_keras_main-tf2.py',\n",
    "                       source_dir=source_dir,\n",
    "                       role=role,\n",
    "                       framework_version='2.1.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       train_instance_count=train_instance_count,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'horovod-pipe'}],\n",
    "                       metric_definitions=keras_metric_definition,\n",
    "                       distributions=distributions,\n",
    "                       input_mode=input_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-29 02:40:16 Starting - Starting the training job...\n",
      "2020-04-29 02:40:19 Starting - Launching requested ML instances.........\n",
      "2020-04-29 02:41:54 Starting - Preparing the instances for training.........\n",
      "2020-04-29 02:43:30 Downloading - Downloading input data\n",
      "2020-04-29 02:43:30 Training - Downloading the training image.........\n",
      "2020-04-29 02:44:59 Training - Training image download completed. Training in progress.\u001b[34m2020-04-29 02:45:03,657 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-04-29 02:45:04,048 sagemaker-containers INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2020-04-29 02:45:04,048 sagemaker-containers INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2020-04-29 02:45:04,053 sagemaker-containers INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2020-04-29 02:45:04,053 sagemaker-containers INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:4'] process_per_hosts: 4 num_processes: 4\u001b[0m\n",
      "\u001b[34m2020-04-29 02:45:04,055 sagemaker-containers INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2020-04-29 02:45:04,101 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 4,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"eval\": \"/opt/ml/input/data/eval\",\n",
      "        \"train_2\": \"/opt/ml/input/data/train_2\",\n",
      "        \"train_1\": \"/opt/ml/input/data/train_1\",\n",
      "        \"validation_3\": \"/opt/ml/input/data/validation_3\",\n",
      "        \"train_0\": \"/opt/ml/input/data/train_0\",\n",
      "        \"validation_2\": \"/opt/ml/input/data/validation_2\",\n",
      "        \"validation_1\": \"/opt/ml/input/data/validation_1\",\n",
      "        \"validation_0\": \"/opt/ml/input/data/validation_0\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train_3\": \"/opt/ml/input/data/train_3\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 256,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"epochs\": 20\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"eval\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train_2\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train_1\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation_3\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train_0\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation_2\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation_1\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation_0\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train_3\": {\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"horovod-pipe-cifar10-tf-2020-04-29-02-40-16-420\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-637338777613/horovod-pipe-cifar10-tf-2020-04-29-02-40-16-420/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"cifar10_keras_main-tf2\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"cifar10_keras_main-tf2.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":256,\"epochs\":20,\"model_dir\":\"/opt/ml/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=cifar10_keras_main-tf2.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"train_0\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"train_1\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"train_2\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"train_3\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation_0\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation_1\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation_2\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation_3\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"eval\",\"train_0\",\"train_1\",\"train_2\",\"train_3\",\"validation\",\"validation_0\",\"validation_1\",\"validation_2\",\"validation_3\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=cifar10_keras_main-tf2\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-637338777613/horovod-pipe-cifar10-tf-2020-04-29-02-40-16-420/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train_0\":\"/opt/ml/input/data/train_0\",\"train_1\":\"/opt/ml/input/data/train_1\",\"train_2\":\"/opt/ml/input/data/train_2\",\"train_3\":\"/opt/ml/input/data/train_3\",\"validation\":\"/opt/ml/input/data/validation\",\"validation_0\":\"/opt/ml/input/data/validation_0\",\"validation_1\":\"/opt/ml/input/data/validation_1\",\"validation_2\":\"/opt/ml/input/data/validation_2\",\"validation_3\":\"/opt/ml/input/data/validation_3\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":256,\"epochs\":20,\"model_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"train_0\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"train_1\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"train_2\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"train_3\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation_0\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation_1\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation_2\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation_3\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"horovod-pipe-cifar10-tf-2020-04-29-02-40-16-420\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-637338777613/horovod-pipe-cifar10-tf-2020-04-29-02-40-16-420/source/sourcedir.tar.gz\",\"module_name\":\"cifar10_keras_main-tf2\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"cifar10_keras_main-tf2.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"256\",\"--epochs\",\"20\",\"--model_dir\",\"/opt/ml/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_EVAL=/opt/ml/input/data/eval\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN_2=/opt/ml/input/data/train_2\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN_1=/opt/ml/input/data/train_1\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION_3=/opt/ml/input/data/validation_3\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN_0=/opt/ml/input/data/train_0\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION_2=/opt/ml/input/data/validation_2\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION_1=/opt/ml/input/data/validation_1\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION_0=/opt/ml/input/data/validation_0\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN_3=/opt/ml/input/data/train_3\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:4 -np 4 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to socket -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.6/dist-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_EVAL -x SM_CHANNEL_TRAIN_2 -x SM_CHANNEL_TRAIN_1 -x SM_CHANNEL_VALIDATION_3 -x SM_CHANNEL_TRAIN_0 -x SM_CHANNEL_VALIDATION_2 -x SM_CHANNEL_VALIDATION_1 -x SM_CHANNEL_VALIDATION_0 -x SM_CHANNEL_VALIDATION -x SM_CHANNEL_TRAIN_3 -x SM_HP_BATCH-SIZE -x SM_HP_MODEL_DIR -x SM_HP_EPOCHS -x PYTHONPATH /usr/bin/python3 -m mpi4py cifar10_keras_main-tf2.py --batch-size 256 --epochs 20 --model_dir /opt/ml/model\n",
      "\n",
      "\n",
      " Data for JOB [32979,1] offset 0 Total slots allocated 4\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-252-187#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [32979,1] App: 0 Process rank: 0 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [32979,1] App: 0 Process rank: 1 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [32979,1] App: 0 Process rank: 2 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [32979,1] App: 0 Process rank: 3 Bound: UNBOUND\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mWARNING: Open MPI tried to bind a process but failed.  This is a\u001b[0m\n",
      "\u001b[34mwarning only; your job will continue, though performance may\u001b[0m\n",
      "\u001b[34mbe degraded.\n",
      "\n",
      "  Local host:        ip-10-0-252-187\n",
      "  Application name:  /usr/bin/python3\n",
      "  Error message:     failed to bind memory\n",
      "  Location:          rtc_hwloc.c:447\n",
      "\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Writing TensorBoard logs to s3://sagemaker-us-east-1-637338777613/horovod-pipe-cifar10-tf-2020-04-29-02-40-16-420/output\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:Writing TensorBoard logs to s3://sagemaker-us-east-1-637338777613/horovod-pipe-cifar10-tf-2020-04-29-02-40-16-420/output\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Using TensorFlow backend.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Using TensorFlow backend.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:Writing TensorBoard logs to s3://sagemaker-us-east-1-637338777613/horovod-pipe-cifar10-tf-2020-04-29-02-40-16-420/output\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:Writing TensorBoard logs to s3://sagemaker-us-east-1-637338777613/horovod-pipe-cifar10-tf-2020-04-29-02-40-16-420/output\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Using TensorFlow backend.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Using TensorFlow backend.\u001b[0m\n",
      "\u001b[34m[ip-10-0-252-187.ec2.internal:00053] 3 more processes have sent help message help-orte-odls-default.txt / memory not bound\u001b[0m\n",
      "\u001b[34m[ip-10-0-252-187.ec2.internal:00053] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Starting training\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:Starting training\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:Starting training\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:Starting training\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:45:10.079 algo-1:58 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:45:10.079 algo-1:59 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:45:10.079 algo-1:59 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:45:10.079 algo-1:58 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:45:10.079 algo-1:59 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:45:10.079 algo-1:58 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:10.081429: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:10.081427: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:45:10.092 algo-1:60 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:45:10.092 algo-1:60 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:45:10.092 algo-1:60 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:10.093624: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:45:10.094 algo-1:61 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:45:10.095 algo-1:61 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:45:10.095 algo-1:61 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:10.096323: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:45:10.122 algo-1:59 INFO keras.py:68] Executing in TF2.x eager mode.TF 2.x eager doesn't provide gradient and optimizer variable values.SageMaker Debugger will not be saving gradients and optimizer variables in this case\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:45:10.122 algo-1:60 INFO keras.py:68] Executing in TF2.x eager mode.TF 2.x eager doesn't provide gradient and optimizer variable values.SageMaker Debugger will not be saving gradients and optimizer variables in this case\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:45:10.125 algo-1:61 INFO keras.py:68] Executing in TF2.x eager mode.TF 2.x eager doesn't provide gradient and optimizer variable values.SageMaker Debugger will not be saving gradients and optimizer variables in this case\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:45:10.832 algo-1:58 INFO keras.py:68] Executing in TF2.x eager mode.TF 2.x eager doesn't provide gradient and optimizer variable values.SageMaker Debugger will not be saving gradients and optimizer variables in this case\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:45:13.820 algo-1:59 INFO hook.py:364] Monitoring the collections: metrics, sm_metrics, losses\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:45:13.830 algo-1:61 INFO hook.py:364] Monitoring the collections: losses, sm_metrics, metrics\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:45:13.987 algo-1:60 INFO hook.py:364] Monitoring the collections: metrics, sm_metrics, losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:45:14.380 algo-1:58 INFO hook.py:364] Monitoring the collections: sm_metrics, metrics, losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.252.187<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.4.7+cuda10.1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.252.187<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.252.187<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.252.187<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 00 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 01 :    0   2   1   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 02 :    0   3   1   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 03 :    0   3   2   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 04 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 05 :    0   2   1   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 06 :    0   3   1   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Channel 07 :    0   3   2   1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 00 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO Ring 01 : 1[1] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 01 : 0[0] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 01 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO Ring 01 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 02 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO Ring 02 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 02 : 3[3] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO Ring 02 : 2[2] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO Ring 03 : 1[1] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 03 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO Ring 03 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 03 : 3[3] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO Ring 04 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 04 : 0[0] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 04 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO Ring 04 : 2[2] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO Ring 05 : 1[1] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 05 : 0[0] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 05 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO Ring 05 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 06 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO Ring 06 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 06 : 3[3] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO Ring 06 : 2[2] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO Ring 07 : 1[1] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Ring 07 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO Ring 07 : 3[3] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO Ring 07 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO comm 0x7f981830dfd0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:59:210 [1] NCCL INFO comm 0x7f733430dba0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:61:212 [3] NCCL INFO comm 0x7f3c7430d5e0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:60:213 [2] NCCL INFO comm 0x7fd2a830da70 rank 2 nranks 4 cudaDev 2 nvmlDev 2 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:58:211 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00058] Read -1, expected 4089, errno = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00058] Read -1, expected 4393, errno = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00058] Read -1, expected 4393, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00058] Read -1, expected 5161, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 524288, errno = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[algo-1:00058] Read -1, expected 4857, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 36864, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 18432, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 147456, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 73728, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 294912, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00059] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00061] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00060] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:19.680315: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:19.681762: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:ERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.348244). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.348244). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.348632). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.348632). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.348422). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.348422). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:19.720123: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.767805). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.767805). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.709793). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.709793). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.355487). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.355487). Check your callbacks.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:21.676328: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:21.676470: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:21.677479: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:21.678311: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00001: saving model to /opt/ml/output/checkpoint-1.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:ERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:ERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:ERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 12s - loss: 2.1774 - accuracy: 0.2332 - val_loss: 2.1926 - val_accuracy: 0.1444\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 12s - loss: 2.1737 - accuracy: 0.2332 - val_loss: 2.1926 - val_accuracy: 0.1444\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 12s - loss: 2.1819 - accuracy: 0.2332 - val_loss: 2.1926 - val_accuracy: 0.1444\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 2/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 2/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 2/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 12s - loss: 2.1713 - accuracy: 0.2332 - val_loss: 2.1926 - val_accuracy: 0.1444\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 2/20\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:24.856507: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:24.856876: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:24.857066: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:24.859463: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00002: saving model to /opt/ml/output/checkpoint-2.ckpt\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 3s - loss: 1.6748 - accuracy: 0.3642 - val_loss: 1.9652 - val_accuracy: 0.2635\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 3s - loss: 1.6889 - accuracy: 0.3642 - val_loss: 1.9652 - val_accuracy: 0.2635\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 3s - loss: 1.6675 - accuracy: 0.3642 - val_loss: 1.9652 - val_accuracy: 0.2635\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 3/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 3/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 3/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.6654 - accuracy: 0.3642 - val_loss: 1.9652 - val_accuracy: 0.2635\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 3/20\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:26.933491: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:26.934238: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:26.934334: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:26.935234: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00003: saving model to /opt/ml/output/checkpoint-3.ckpt\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 1.4772 - accuracy: 0.4520 - val_loss: 2.3476 - val_accuracy: 0.2419\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 1.4735 - accuracy: 0.4520 - val_loss: 2.3476 - val_accuracy: 0.2419\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.4828 - accuracy: 0.4520 - val_loss: 2.3476 - val_accuracy: 0.2419\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 4/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 4/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 4/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.4671 - accuracy: 0.4520 - val_loss: 2.3476 - val_accuracy: 0.2419\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 4/20\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:29.087163: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:29.087462: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:29.088174: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:29.088504: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00004: saving model to /opt/ml/output/checkpoint-4.ckpt\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 1.3341 - accuracy: 0.5131 - val_loss: 2.4267 - val_accuracy: 0.2921\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 1.3460 - accuracy: 0.5131 - val_loss: 2.4267 - val_accuracy: 0.2921\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.3403 - accuracy: 0.5131 - val_loss: 2.4267 - val_accuracy: 0.2921\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 5/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 5/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 5/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.3412 - accuracy: 0.5131 - val_loss: 2.4267 - val_accuracy: 0.2921\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 5/20\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:31.393600: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:31.397856: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:31.397993: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:31.404612: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 5: finished gradual learning rate warmup to 0.004.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 5: finished gradual learning rate warmup to 0.004.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 5: finished gradual learning rate warmup to 0.004.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 5: finished gradual learning rate warmup to 0.004.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00005: saving model to /opt/ml/output/checkpoint-5.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 1.2472 - accuracy: 0.5564 - val_loss: 1.7643 - val_accuracy: 0.3851\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 6/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 1.2216 - accuracy: 0.5564 - val_loss: 1.7643 - val_accuracy: 0.3851\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.2376 - accuracy: 0.5564 - val_loss: 1.7643 - val_accuracy: 0.3851\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 6/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 6/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.2436 - accuracy: 0.5564 - val_loss: 1.7643 - val_accuracy: 0.3851\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 6/20\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:33.771234: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:33.771284: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:33.771849: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:33.774713: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00006: saving model to /opt/ml/output/checkpoint-6.ckpt\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.1697 - accuracy: 0.5919 - val_loss: 1.6098 - val_accuracy: 0.4564\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 1.1581 - accuracy: 0.5919 - val_loss: 1.6098 - val_accuracy: 0.4564\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 1.1342 - accuracy: 0.5919 - val_loss: 1.6098 - val_accuracy: 0.4564\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 7/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 7/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 7/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.1340 - accuracy: 0.5919 - val_loss: 1.6098 - val_accuracy: 0.4564\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 7/20\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:35.809260: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:35.810524: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:35.810547: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:35.810641: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00007: saving model to /opt/ml/output/checkpoint-7.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 1.0520 - accuracy: 0.6256 - val_loss: 1.3501 - val_accuracy: 0.5455\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 1.0420 - accuracy: 0.6256 - val_loss: 1.3501 - val_accuracy: 0.5455\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.0617 - accuracy: 0.6256 - val_loss: 1.3501 - val_accuracy: 0.5455\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 8/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 8/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 8/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 1.0631 - accuracy: 0.6256 - val_loss: 1.3501 - val_accuracy: 0.5455\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 8/20\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:38.281882: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:38.283646: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:38.293309: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:38.297113: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00008: saving model to /opt/ml/output/checkpoint-8.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.9922 - accuracy: 0.6441 - val_loss: 1.2556 - val_accuracy: 0.5800\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 1.0146 - accuracy: 0.6441 - val_loss: 1.2556 - val_accuracy: 0.5800\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 9/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 1.0073 - accuracy: 0.6441 - val_loss: 1.2556 - val_accuracy: 0.5800\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 9/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 9/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 3s - loss: 0.9928 - accuracy: 0.6441 - val_loss: 1.2556 - val_accuracy: 0.5800\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 9/20\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:40.434909: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:40.436445: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:40.436743: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:40.437855: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00009: saving model to /opt/ml/output/checkpoint-9.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.9488 - accuracy: 0.6672 - val_loss: 1.2548 - val_accuracy: 0.5590\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.9649 - accuracy: 0.6672 - val_loss: 1.2548 - val_accuracy: 0.5590\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 10/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 10/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.9408 - accuracy: 0.6672 - val_loss: 1.2548 - val_accuracy: 0.5590\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 10/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.9484 - accuracy: 0.6672 - val_loss: 1.2548 - val_accuracy: 0.5590\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 10/20\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:42.559654: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:42.560324: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:42.560948: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:42.561677: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00010: saving model to /opt/ml/output/checkpoint-10.ckpt\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.9090 - accuracy: 0.6851 - val_loss: 1.4020 - val_accuracy: 0.5417\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.8971 - accuracy: 0.6851 - val_loss: 1.4020 - val_accuracy: 0.5417\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 11/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 11/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.8923 - accuracy: 0.6851 - val_loss: 1.4020 - val_accuracy: 0.5417\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 11/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.8931 - accuracy: 0.6851 - val_loss: 1.4020 - val_accuracy: 0.5417\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 11/20\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:44.759524: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:44.759927: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:44.760260: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:44.766216: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00011: saving model to /opt/ml/output/checkpoint-11.ckpt[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.8605 - accuracy: 0.6978 - val_loss: 1.0729 - val_accuracy: 0.6399\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.8423 - accuracy: 0.6978 - val_loss: 1.0729 - val_accuracy: 0.6399\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.8803 - accuracy: 0.6978 - val_loss: 1.0729 - val_accuracy: 0.6399\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 12/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 12/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 12/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.8698 - accuracy: 0.6978 - val_loss: 1.0729 - val_accuracy: 0.6399\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 12/20\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:46.966010: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:46.967959: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:46.973522: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:46.974106: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00012: saving model to /opt/ml/output/checkpoint-12.ckpt\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.7971 - accuracy: 0.7095 - val_loss: 0.8830 - val_accuracy: 0.6972\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.8466 - accuracy: 0.7095 - val_loss: 0.8830 - val_accuracy: 0.6972\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.8373 - accuracy: 0.7095 - val_loss: 0.8830 - val_accuracy: 0.6972\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 13/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 13/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 13/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.8399 - accuracy: 0.7095 - val_loss: 0.8830 - val_accuracy: 0.6972\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 13/20\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:49.355671: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:49.357334: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:49.357908: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:49.361442: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00013: saving model to /opt/ml/output/checkpoint-13.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.8032 - accuracy: 0.7186 - val_loss: 0.9360 - val_accuracy: 0.6788\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.7968 - accuracy: 0.7186 - val_loss: 0.9360 - val_accuracy: 0.6788\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.8273 - accuracy: 0.7186 - val_loss: 0.9360 - val_accuracy: 0.6788\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 14/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 14/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 14/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.7930 - accuracy: 0.7186 - val_loss: 0.9360 - val_accuracy: 0.6788\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 14/20\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:51.571626: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:51.573256: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:51.573804: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:51.577104: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00014: saving model to /opt/ml/output/checkpoint-14.ckpt[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.7825 - accuracy: 0.7303 - val_loss: 0.7464 - val_accuracy: 0.7370\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.7749 - accuracy: 0.7303 - val_loss: 0.7464 - val_accuracy: 0.7370\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.7840 - accuracy: 0.7303 - val_loss: 0.7464 - val_accuracy: 0.7370\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 15/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 15/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 15/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 3s - loss: 0.7835 - accuracy: 0.7303 - val_loss: 0.7464 - val_accuracy: 0.7370\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 15/20\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:54.310097: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:54.310230: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:54.310360: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:54.324878: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00015: saving model to /opt/ml/output/checkpoint-15.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 3s - loss: 0.7385 - accuracy: 0.7402 - val_loss: 0.7987 - val_accuracy: 0.7207\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 3s - loss: 0.7647 - accuracy: 0.7402 - val_loss: 0.7987 - val_accuracy: 0.7207\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 3s - loss: 0.7676 - accuracy: 0.7402 - val_loss: 0.7987 - val_accuracy: 0.7207\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 16/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 16/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 16/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.7516 - accuracy: 0.7402 - val_loss: 0.7987 - val_accuracy: 0.7207\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 16/20\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:56.546977: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:56.547802: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:56.548185: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:56.554114: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00016: saving model to /opt/ml/output/checkpoint-16.ckpt\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.7308 - accuracy: 0.7517 - val_loss: 0.7040 - val_accuracy: 0.7611\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.7284 - accuracy: 0.7517 - val_loss: 0.7040 - val_accuracy: 0.7611\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.7123 - accuracy: 0.7517 - val_loss: 0.7040 - val_accuracy: 0.7611\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 17/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 17/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 17/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.7193 - accuracy: 0.7517 - val_loss: 0.7040 - val_accuracy: 0.7611\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 17/20\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:45:58.565272: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:45:58.565445: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:45:58.565675: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:45:58.568451: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00017: saving model to /opt/ml/output/checkpoint-17.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.7020 - accuracy: 0.7594 - val_loss: 0.7496 - val_accuracy: 0.7433\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.6862 - accuracy: 0.7594 - val_loss: 0.7496 - val_accuracy: 0.7433\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.7191 - accuracy: 0.7594 - val_loss: 0.7496 - val_accuracy: 0.7433\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 18/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 18/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 18/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.6988 - accuracy: 0.7594 - val_loss: 0.7496 - val_accuracy: 0.7433\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 18/20\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:46:00.888280: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:46:00.890506: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:46:00.891682: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:46:00.892602: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00018: saving model to /opt/ml/output/checkpoint-18.ckpt\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.6910 - accuracy: 0.7666 - val_loss: 0.8955 - val_accuracy: 0.6940\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.6712 - accuracy: 0.7666 - val_loss: 0.8955 - val_accuracy: 0.6940\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 19/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.7002 - accuracy: 0.7666 - val_loss: 0.8955 - val_accuracy: 0.6940\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 19/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 19/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.6812 - accuracy: 0.7666 - val_loss: 0.8955 - val_accuracy: 0.6940\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 19/20\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:46:02.986436: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:46:02.988640: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:46:02.990809: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:46:02.996255: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00019: saving model to /opt/ml/output/checkpoint-19.ckpt\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.6766 - accuracy: 0.7654 - val_loss: 0.6164 - val_accuracy: 0.7857\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.6859 - accuracy: 0.7654 - val_loss: 0.6164 - val_accuracy: 0.7857\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.6878 - accuracy: 0.7654 - val_loss: 0.6164 - val_accuracy: 0.7857\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 20/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 20/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 20/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.6837 - accuracy: 0.7654 - val_loss: 0.6164 - val_accuracy: 0.7857\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 20/20\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2020-04-29 02:46:05.127014: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2020-04-29 02:46:05.126985: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:46:05.128593: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2020-04-29 02:46:05.134368: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Epoch 1/20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch 00020: saving model to /opt/ml/output/checkpoint-20.ckpt\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:39/39 - 2s - loss: 0.6196 - accuracy: 0.7751 - val_loss: 0.7130 - val_accuracy: 0.7560\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:39/39 - 2s - loss: 0.6610 - accuracy: 0.7751 - val_loss: 0.7130 - val_accuracy: 0.7560\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:39/39 - 2s - loss: 0.6540 - accuracy: 0.7751 - val_loss: 0.7130 - val_accuracy: 0.7560\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-04-29 02:46:05.714 algo-1:59 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-04-29 02:46:05.714 algo-1:61 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-04-29 02:46:05.717 algo-1:60 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:39/39 - 2s - loss: 0.6599 - accuracy: 0.7751 - val_loss: 0.7130 - val_accuracy: 0.7560\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:46:06.003798: W tensorflow/core/framework/dataset.cc:392] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\u001b[0m\n",
      "\n",
      "2020-04-29 02:46:15 Uploading - Uploading generated training model\u001b[34m[1,0]<stdout>:39/39 - 1s - loss: 0.7420 - accuracy: 0.7517\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Test loss:0.7419657508532206\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Test accuracy:0.7517027258872986\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2020-04-29 02:46:08.544127: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:If using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:If using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/assets\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/assets\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-04-29 02:46:10.306 algo-1:58 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-04-29 02:46:11,318 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving containers. The SavedModel bundle is under directory \"model\", not a numeric name.\u001b[0m\n",
      "\u001b[34m2020-04-29 02:46:11,318 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-04-29 02:46:23 Completed - Training job completed\n",
      "Training seconds: 181\n",
      "Billable seconds: 181\n"
     ]
    }
   ],
   "source": [
    "estimator_dist.fit(remote_inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local TensorBoard command\n",
    "Using TensorBoard we can compare the jobs we ran. The following command prints the TensorBoard command.\n",
    "Run it in any environment where you have TensorBoard installed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_REGION=us-east-1 tensorboard --logdir horovod-pipe:\"s3://sagemaker-us-east-1-637338777613/horovod-pipe-cifar10-tf-2020-04-29-02-40-16-420/model\",horovod:\"s3://sagemaker-us-east-1-637338777613/horovod-cifar10-tf-2020-04-29-02-34-12-249/model\",pipe:\"s3://sagemaker-us-east-1-637338777613/pipe-cifar10-tf-2020-04-29-02-33-23-861/model\",file:\"s3://sagemaker-us-east-1-637338777613/cifar10-tf-2020-04-29-02-27-40-012/model\"\n"
     ]
    }
   ],
   "source": [
    "!python generate_tensorboard_command.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install [TensorBoard](https://github.com/tensorflow/tensorboard) locally using `pip install tensorboard`.  \n",
    "To access an S3 log directory, configure the TensorBoard default region. You can do this by configuring an environment variable named AWS_REGION, and setting the value of the environment variable to the AWS region your training jobs run in.  \n",
    "For example, `export AWS_REGION = 'us-east-1'`\n",
    "\n",
    "You can access TensorBoard locally at http://localhost:6006\n",
    "\n",
    "Based on the TensorBoard metrics, we can see that:\n",
    "1. All jobs run for 10 epochs (0 - 9).\n",
    "2. File mode and Pipe mode runs for ~1 minute - Pipemode doesn't effect training performance.\n",
    "3. Distributed mode runs for 45 seconds.\n",
    "4. All of the training jobs resulted in similar validation accuracy.\n",
    "\n",
    "This example uses a small dataset (179 MB). For larger datasets, pipemode can significantly reduce training time because it does not copy the entire dataset into local memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model\n",
    "\n",
    "The deploy() method creates an endpoint that serves prediction requests in real-time.\n",
    "The model saves keras artifacts, to use TensorFlow serving for deployment, you'll need to save the artifacts in SavedModel format.\n",
    "\n",
    "We are using the solutions from the [deploy trained keras or tensorflow models using amazon sagemaker](https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/) blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predictions\n",
    "To verify the that the endpoint functions properly, we generate random data in the correct shape and get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is 8\n"
     ]
    }
   ],
   "source": [
    "# Creating fake prediction data\n",
    "import numpy as np\n",
    "data = np.random.randn(1, 32, 32, 3)\n",
    "print(\"Predicted class is {}\".format(np.argmax(predictor.predict(data)['predictions'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating accuracy and create a confusion matrix based on the test dataset\n",
    "\n",
    "Our endpoint works as expected, we'll now use the test dataset for predictions and calculate our model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "def predict(data):\n",
    "    predictions = predictor.predict(data)['predictions']\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "predicted = []\n",
    "actual = []\n",
    "batches = 0\n",
    "for data in datagen.flow(x_test,y_test,batch_size=batch_size):\n",
    "    for i,prediction in enumerate(predict(data[0])):\n",
    "        predicted.append(np.argmax(prediction))\n",
    "        actual.append(data[1][i][0])\n",
    "    batches += 1\n",
    "    if batches >= len(x_test) / batch_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Average accuracy: 72.18%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_pred=predicted,y_true=actual)\n",
    "display('Average accuracy: {}%'.format(round(accuracy*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc3701ef5c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_pred=predicted,y_true=actual)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sn.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(cm, annot=True,annot_kws={\"size\": 10})# font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this heatmap we can calculate the accuracy of each one of the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up\n",
    "To avoid incurring charges to your AWS account for the resources used in this tutorial you need to delete the SageMaker Endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
