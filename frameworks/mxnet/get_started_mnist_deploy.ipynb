{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a Trained Model\n",
    "\n",
    "In this notebook, we walk through the process of deploying a trained model to a SageMaker endpoint. If you recently ran [the notebook for training](get_started_mnist_deploy.ipynb) with %store% magic, the `model_data` can be restored. Otherwise, we retrieve the \n",
    "model artifact from a public S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hel\n"
     ]
    }
   ],
   "source": [
    "# setups\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNetModel\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "# Get global config\n",
    "with open('code/config.json', 'r') as f:\n",
    "    CONFIG=json.load(f)\n",
    "\n",
    "sess = Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "#%store -r mx_mnist_model_data\n",
    "\n",
    "try: \n",
    "    mx_mnist_model_data\n",
    "except NameError:\n",
    "    import json\n",
    "    # copy a pretrained model from a public public to your default bucket\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket = CONFIG['public_bucket']\n",
    "    key = 'datasets/image/MNIST/model/mxnet-training-2020-11-21-01-38-01-009/model.tar.gz'\n",
    "    s3.download_file(bucket, key, 'model.tar.gz')\n",
    "    \n",
    "    # upload to default bucket\n",
    "    mx_mnist_model_data = sess.upload_data(\n",
    "        path='model.tar.gz',bucket=sess.default_bucket(),key_prefix='model/mxnet')\n",
    "    \n",
    "    # delete local copy\n",
    "    os.remove('model.tar.gz')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always download from remote\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNetModel\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Get global config\n",
    "with open('code/config.json', 'r') as f:\n",
    "    CONFIG=json.load(f)\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = Session()\n",
    "\n",
    "# copy a pretrained model from a public public to your default bucket\n",
    "s3 = boto3.client('s3')\n",
    "bucket = CONFIG['public_bucket']\n",
    "key = 'datasets/image/MNIST/model/mxnet-training-2020-11-21-01-38-01-009/model.tar.gz'\n",
    "s3.download_file(bucket, key, 'model.tar.gz')\n",
    "\n",
    "# upload to default bucket\n",
    "mx_mnist_model_data = sess.upload_data(\n",
    "    path='model.tar.gz',bucket=sess.default_bucket(),key_prefix='model/mxnet')\n",
    "\n",
    "# delete local copy\n",
    "os.remove('model.tar.gz')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-688520471316/model/mxnet/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(mx_mnist_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXNet Model Object\n",
    "\n",
    "The `MXNetModel` class allows you to define an environment for making inference using your\n",
    "model artifact. Like `MXNet` class we discussed \n",
    "[in this notebook for training an MXNet model](\n",
    "get_started_mnist_train.ipynb), it is high level API used to set up a docker image for your model hosting service.\n",
    "\n",
    "Once it is properly configured, it can be used to create a SageMaker\n",
    "Endpoint on an EC2 instance. The SageMaker endpoint is a containerized environment that uses your trained model \n",
    "to make inference on incoming data via RESTful API calls. \n",
    "\n",
    "Some common parameters used to initiate the `MXNetModel` class are:\n",
    "- entry_point: A user defined python file to be used by the inference image as handlers of incoming requests\n",
    "- source_dir: The directory of the `entry_point`\n",
    "- role: An IAM role to make AWS service requests\n",
    "- model_data: the S3 bucket URI of the compressed model artifact. It can be a path to a local file if the endpoint \n",
    "is to be deployed on the SageMaker instance you are using to run this notebook (local mode)\n",
    "- framework_version: version of the MXNet package to be used\n",
    "- py_version: python version to be used\n",
    "\n",
    "We elaborate on the `entry_point` below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MXNetModel(\n",
    "    entry_point='inference.py',\n",
    "    source_dir='code',\n",
    "    role=role,\n",
    "    model_data=mx_mnist_model_data,\n",
    "    framework_version='1.7.0',\n",
    "    py_version='py3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mtmp92sp1gd1_algo-1-k7y72_1 exited with code 137\n",
      "\u001b[0mAborting on container exit...\n",
      "-------------![8.0]\n"
     ]
    }
   ],
   "source": [
    "os.system(\"docker container ls | grep 8080 | awk '{print $1}' | xargs docker container rm -f\")\n",
    "\n",
    "model = MXNetModel(\n",
    "    entry_point='inference.py',\n",
    "    source_dir='code',\n",
    "    role=role,\n",
    "    model_data=mx_mnist_model_data,\n",
    "    framework_version='1.7.0',\n",
    "    py_version='py3'\n",
    ")\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# set local_mode to False if you want to deploy on a remote\n",
    "# SageMaker instance\n",
    "\n",
    "local_mode=False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type='local'\n",
    "else:\n",
    "    instance_type='ml.c4.xlarge'\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    "    )\n",
    "\n",
    "\n",
    "import random\n",
    "dummy_data = {\n",
    "    'inputs': [random.random() for _ in range(784)]\n",
    "}\n",
    "\n",
    "res = predictor.predict(dummy_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.0]\n"
     ]
    }
   ],
   "source": [
    "res = predictor.predict(dummy_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry Point for the Inference Image\n",
    "\n",
    "Your model artifacts pointed by `model_data` is pulled by the `MXNetModel` and it is decompressed and saved in\n",
    "in the docker image it defines. They become regular model checkpoint files that you would produce outside SageMaker. This means in order to use your trained model for serving, \n",
    "you need to tell `MXNetModel` class how to a recover a MXNet model from the static checkpoint.\n",
    "\n",
    "Also, the deployed endpoint interacts with RESTful API calls, you need to tell it how to parse an incoming \n",
    "request to your model. \n",
    "\n",
    "These two instructions needs to be defined as two functions in the python file pointed by `entry_point`.\n",
    "\n",
    "By convention, we name this entry point file `inference.py` and we put it in the `code` directory.\n",
    "\n",
    "To tell the inference image how to load the model checkpoint, you need to implement a function called \n",
    "`model_fn`. This function takes one positional argument \n",
    "\n",
    "- `model_dir`: the directory of the static model checkpoints in the inference image.\n",
    "\n",
    "The return of `model_fn` is an MXNet model. In this example, the `model_fn`\n",
    "looks like:\n",
    "\n",
    "```python\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the gluon model. Called once when hosting service starts.\n",
    "\n",
    "    :param: model_dir The directory where model files are stored.\n",
    "    :return: a model (in this case a Gluon network)\n",
    "    \"\"\"\n",
    "    net = gluon.SymbolBlock.imports(\n",
    "            symbol_file=os.path.join(model_dir, 'compiled-symbol.json'),\n",
    "            input_names=['data'],\n",
    "            param_file=os.path.join(model_dir, 'compiled-0000.params'))\n",
    "    return net\n",
    "```\n",
    "\n",
    "Next, you need to tell the hosting service how to handle the incoming data. This includes:\n",
    "\n",
    "* How to parse the incoming request\n",
    "* How to use the trained model to make inference\n",
    "* How to return the prediction to the caller of the service\n",
    "\n",
    "\n",
    "You do it by implementing a function\n",
    "called `transform_fn`. This function takes 4 positional arguments:\n",
    "\n",
    "- `net`: the return from `model_fn`\n",
    "- `data`: the payload of the incoming request\n",
    "- `content_type`: the content type of the incoming request\n",
    "- `accept_type`: the conetent type of the response\n",
    "\n",
    "In this example, the `transform_fn` looks like:\n",
    "```python\n",
    "\n",
    "def transform_fn(net, data, input_content_type, output_content_type):\n",
    "    assert input_content_type=='application/json'\n",
    "    assert output_content_type=='application/json' \n",
    "\n",
    "    # parsed should be a 1d array of length 728\n",
    "    parsed = json.loads(data)\n",
    "    parsed = parsed['inputs'] \n",
    "    \n",
    "    # convert to numpy array\n",
    "    arr = np.array(parsed).reshape(-1, 1, 28, 28)\n",
    "    \n",
    "    # convert to mxnet ndarray\n",
    "    nda = mx.nd.array(arr)\n",
    "\n",
    "    output = net(nda)\n",
    "    \n",
    "    prediction = mx.nd.argmax(output, axis=1)\n",
    "    response_body = json.dumps(prediction.asnumpy().tolist())\n",
    "\n",
    "    return response_body, output_content_type\n",
    "```\n",
    "\n",
    "The `content_type` is used by the function to parse the `data`. \n",
    "In the following example, the functions requires the\n",
    "content type of the payload to be a json string and it\n",
    "parses the json string into a python dictionary by `json.loads`.\n",
    "Moreover, it assumes the parsed dictionary contains a key `inputs`\n",
    "that maps to the input data to be consumed by the model. \n",
    "It also assumes the input data is a flattened 1D array representation\n",
    "that can be reshaped into a numpy array of shape (-1, 1, 28, 28).\n",
    "The input images of a MXNet model follows NCHW convention. \n",
    "It also assumes the input data is already normalized and can be readily\n",
    "consumed by the neural network.\n",
    "\n",
    "After the inference, the function uses `accept_type` to encode the \n",
    "prediction into the content type of the response. In this example,\n",
    "the function requires the caller of the service to accept json string.\n",
    "\n",
    "The return of `transform_fn` is always a tuple of encoded response body\n",
    "and the content type to be accepted by the caller. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the inference container\n",
    "Once the `MXNetModel` class is initiated, we can call its `deploy` method to run the container for the hosting\n",
    "service. Some common parameters needed to call `deploy` methods are:\n",
    "\n",
    "- initial_instance_count: the number of SageMaker instances to be used to run the hosting service.\n",
    "- instance_type: the type of SageMaker instance to run the hosting service. Set it to `local` if you want run the hosting service on the local SageMaker instance. Local mode are typically used for debugging. \n",
    "- serializer: A python callable used to serialize (encode) the request data.\n",
    "- deserializer: A python callable used to deserialize (decode) the response data.\n",
    "\n",
    "Commonly used serializers and deserialzers are implemented in `sagemaker.serializers` and `sagemaker.deserializer`\n",
    "submodules of the SageMaker Python SDK. \n",
    "\n",
    "Since in the `transform_fn` we declared that the incoming requests are json-encoded, we need use a json serializer,\n",
    "to encode the incoming data into a json string. Also, we declared the return content type to be json string, we\n",
    "need to use a json deserializer to parse the response into a an (in this case, an \n",
    "integer represeting the predicted hand-written digit). \n",
    "\n",
    "<span style=\"color:red\"> Note: local mode is not supported in SageMaker Studio </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the short-lived AWS credentials found in session. They might expire while running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmptnp5es6i_algo-1-a3xvt_1\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Warning: Calling MMS with mxnet-model-server. Please move to multi-model-server.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,169 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m MMS Home: /usr/local/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Current directory: /\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Number of CPUs: 8\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Max heap size: 7035 M\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Python executable: /usr/local/bin/python3.6\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Config file: /etc/sagemaker-mms.properties\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Model Store: /.sagemaker/mms/models\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Initial Models: ALL\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Log dir: /logs\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Netty threads: 0\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Netty client threads: 0\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Default workers per model: 8\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Preload model: false\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,241 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,369 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_mxnet_serving_container.handler_service --model-path /.sagemaker/mms/models/model --model-name model --preload-model false --tmp-dir /home/model-server/tmp\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,369 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,370 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 71\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,370 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,370 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.10\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,370 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,376 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,383 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,383 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,383 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,383 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,385 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,385 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,383 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,385 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,437 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m Model server started.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,444 [WARN ] pool-2-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,444 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,444 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,444 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,445 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,447 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,448 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,450 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:12,451 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,295 [INFO ] pool-1-thread-10 ACCESS_LOG - /172.18.0.1:53442 \"GET /ping HTTP/1.1\" 200 33\n",
      "!\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,677 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000031-00000004-d88b16ff0455bea6-d58c7809\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,679 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2160\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,680 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-6\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,691 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000031-00000000-2bbbe6ff0455bea6-5c0e879d\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,691 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2186\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,691 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-3\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,729 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000031-00000007-3e4196ff0455bea6-8e82ce4c\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,730 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2225\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,731 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-7\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,733 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000031-00000005-f18896ff0455bea6-29b1345e\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,733 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2229\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,734 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-4\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,736 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000031-00000008-1f7596ff0455bea6-57f4a16d\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,738 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000031-00000001-626fe6ff0455bea6-b4eb9b50\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,739 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000031-00000002-2c2c16ff0455bea6-aba17824\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,802 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2217\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,802 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,802 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2221\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,802 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2215\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,803 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,803 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-5\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,819 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000031-00000006-5c6a96ff0455bea6-715be38a\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,819 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2295\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:22:14,819 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-8\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# set local_mode to False if you want to deploy on a remote\n",
    "# SageMaker instance\n",
    "\n",
    "local_mode=True\n",
    "\n",
    "if local_mode:\n",
    "    instance_type='local'\n",
    "else:\n",
    "    instance_type='ml.c4.xlarge'\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predictor` we get above can be used to make prediction requests agaist a SageMaker endpoint. For more\n",
    "information, check [the api reference for SageMaker Predictor](\n",
    "https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor)\n",
    "\n",
    "Now, let's test the endpoint with some dummy data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "dummy_data = {\n",
    "    'inputs': [random.random() for _ in range(784)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `transform_fn`, we declared that the parsed data is a python dictionary with a key `inputs` and its value should \n",
    "be a 1D array of length 784. Hence, the definition of `dummy_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:23:16,417 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 35\r\n",
      "\u001b[36malgo-1-a3xvt_1  |\u001b[0m 2020-11-24 20:23:16,417 [INFO ] W-9000-model ACCESS_LOG - /172.18.0.1:53494 \"POST /invocations HTTP/1.1\" 200 37\r\n"
     ]
    }
   ],
   "source": [
    "res = predictor.predict(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted digit:\", *map(int, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input data does not look exactly like `dummy_data`, the endpoint will raise an exception. This is because \n",
    "of the stringent way we defined the `transform_fn`. Let's test the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = [random.random() for _ in range(784)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `dummy_data` is parsed in `transform_fn`, it does not have an `inputs` field, so `transform_fn` will crush. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to make inference on incorrectly formated input data\n",
    "# res = predictor.predict(dummy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use real MNIST test to test the endpoint. We use helper functions defined in `code.utils` to \n",
    "download MNIST data set and normalize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import mnist_to_numpy, normalize\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_dir = '/tmp/data'\n",
    "X, _ = mnist_to_numpy(data_dir, train=False)\n",
    "\n",
    "# randomly sample 16 images to inspect\n",
    "mask = random.sample(range(X.shape[0]), 16)\n",
    "samples = X[mask]\n",
    "\n",
    "# plot the images \n",
    "fig, axs = plt.subplots(nrows=1, ncols=16, figsize=(16, 1))\n",
    "\n",
    "for i, splt in enumerate(axs):\n",
    "    splt.imshow(samples[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us use the model to infer the samples one-by-one. This is the typical use case\n",
    "for an online application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the input \n",
    "samples = normalize(samples, axis=(1, 2)) # mean 0; std 1\n",
    "\n",
    "res = []\n",
    "for img in samples:\n",
    "    data = {\n",
    "        \"inputs\": img.flatten().tolist()\n",
    "    }\n",
    "    res.append(predictor.predict(data)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions: \", *map(int, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in `transform_fn`, the parsed numpy array could have take on any value for its batch\n",
    "dimension, we can send the entire `samples` at once and let the model do a batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\": samples.tolist()\n",
    "}\n",
    "res = predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions: \", *map(int, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and debug the entry point before deployment\n",
    "\n",
    "When deploying a model to a SageMaker endpoint, it is a good practice to test the entry \n",
    "point. The following snippet shows you how you can test and debug the `model_fn` and \n",
    "`transform_fn` you implemented in the entry point for the inference image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize code/test_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `test` function simulates how the inference container works. It pulls the model\n",
    "artifact loads the model into \n",
    "memory by calling `model_fn` and parse it with `model_dir`. When it receives a request, \n",
    "it calls `transform_fn` and parse it with the loaded model, \n",
    "the payload of the request, request content type and response content type. \n",
    "\n",
    "Implementing such a test function helps you debugging the entry point before put it into\n",
    "the production. If `test` runs correctly, then you can be certain that if the incoming\n",
    "data and its content type are what they suppose to be, then the endpoint point is going\n",
    "to work as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Clean up \n",
    "\n",
    "If you do not plan to use the endpoint, you should delete it to free up some computation \n",
    "resource. If you use local, you will need to manually delete the docker container bounded\n",
    "at port 8080 (the port that listens to the incoming request).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not local_mode:\n",
    "    predictor.delete_endpoint()\n",
    "else:\n",
    "    os.system(\"docker container ls | grep 8080 | awk '{print $1}' | xargs docker container rm -f\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
