{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Deploy GPT-J-6B model using Tensor Parallelism approach within SageMaker Model Parallel Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please run this notebook with Data Science-> Python 3 Kernel on SageMaker Studio Notebook or a conda_pytorch_p38 Kernel on SageMaker Notebook instances.*\n",
    "\n",
    "This notebook walks you through how to use the tensor parallelism feature provided by the SageMaker model parallelism library. You'll learn how to train the GPT-J model with tensor parallelism on the GLUE sst2 dataset.\n",
    "\n",
    "This notebook walks you through how to train the [EleutherAI's](https://www.eleuther.ai/) [GPT-J](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/) model with SageMaker's model parallelism.\n",
    "EleutherAI released GPT-J 6B, an open-source alternative to [OpenAIs GPT-3](https://openai.com/blog/gpt-3-apps/). [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6B) is the 6 billion parameter successor to EleutherAIs GPT-NEO family, a family of transformer-based language models based on the GPT architecture for text generation.\n",
    "\n",
    "EleutherAI's primary goal is to train a model that is equivalent in size to GPT⁠-⁠3 and make it available to the public under an open license.\n",
    "Over the last few months, GPT-J gained a lot of interest from Researchers, Data Scientists, and even Software Developers, but it remained very challenging to fine tune GPT-J.\n",
    "\n",
    "The weights of the 6 billion parameter model represent a ~24GB memory footprint. To load it in float32, one would need at least 2x model size CPU RAM: 1x for initial weights and another 1x to load the checkpoint. Apart from the model parameters, there are the gradients, optimizer states, and activations taking memory, so the actual memory usage might be significantly higher than 48GB. Just as an example, with Adam optimizer and FP32 training, the use from parameters, gradients and optimizer states might be 96GB+, and activation memory footprint would be even more than this, so the total memory usage might be easily larger than 200 GB.\n",
    "\n",
    "![GPT-J Memory requirements](img/GPT-J-Memory.png)\n",
    "\n",
    "In this notebook, you will learn how to easily fine tune GPT-J using Amazon SageMaker and Hugging Face on NVIDIA GPU instances. The notebook demonstrates the use of Tensor Parallel approach of SageMaker Model Parallel library.\n",
    "\n",
    "This notebook depends on the following files and folders:\n",
    "\n",
    "1. `train_gptj_smp_tensor_parallel_script.py`: This is an entry-point script that is passed to the PyTorch estimator in the notebook instructions. This script is responsible for end to end training of the GPT-J model with SMP. The script has additional comments at places where the SMP API is used.\n",
    "<!-- 2. `fp16`: This folder is used for 16-bit float training, which contains a fp16 optimizer and various fp16 utilities. -->\n",
    "3. `learning_rates.py`: This contains the functions for learning rate schedule.\n",
    "4. `requirements.txt`: This will install the dependencies, like the right version of huggingface transformers.\n",
    "5. `memory_tracker.py`: This contains a function to print the memory status.\n",
    "\n",
    "\n",
    "## SageMaker Distributed Training \n",
    "\n",
    "SageMaker provides distributed training libraries for data parallelism and model parallelism. The libraries are optimized for the SageMaker training environment, help adapt your distributed training jobs to SageMaker, and improve training speed and throughput.\n",
    "\n",
    "### Approaches\n",
    "\n",
    "![SageMaker Distributed Training Approaches](img/TypesOfDistributedTraining.png)\n",
    "\n",
    "\n",
    "### SageMaker Model Parallel\n",
    "\n",
    "Model parallelism is the process of splitting a model up between multiple devices or nodes (such as GPU-equipped instances) and creating an efficient pipeline to train the model across these devices to maximize GPU utilization.\n",
    "\n",
    "Increasing deep learning model size (layers and parameters) can result in better accuracy. However, there is a limit to the maximum model size you can fit in a single GPU. When training deep learning models, GPU memory limitations can be a bottleneck in the following ways:\n",
    "\n",
    "1. They can limit the size of the model you train. Given that larger models tend to achieve higher accuracy, this directly translates to trained model accuracy.\n",
    "\n",
    "2. They can limit the batch size you train with, leading to lower GPU utilization and slower training.\n",
    "\n",
    "To overcome the limitations associated with training a model on a single GPU, you can use model parallelism to distribute and train your model on multiple computing devices.\n",
    "\n",
    "### Core features of SageMaker Model Parallel \n",
    "\n",
    "1. [Automated Model Splitting](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html): When you use SageMaker's model parallel library, you can take advantage of automated model splitting, also referred to as automated model partitioning. The library uses a partitioning algorithm that balances memory, minimizes communication between devices, and optimizes performance. You can configure the automated partitioning algorithm to optimize for speed or memory.\n",
    "\n",
    "2. [Pipeline Execution Schedule](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html): A core feature of SageMaker's distributed model parallel library is pipelined execution, which determines the order in which computations are made and data is processed across devices during model training. Pipelining is a technique to achieve true parallelization in model parallelism, by having the GPUs compute simultaneously on different data samples, and to overcome the performance loss due to sequential computation.\n",
    "\n",
    "Pipelining is based on splitting a mini-batch into microbatches, which are fed into the training pipeline one-by-one and follow an execution schedule defined by the library runtime. A microbatch is a smaller subset of a given training mini-batch. The pipeline schedule determines which microbatch is executed by which device for every time slot.\n",
    "\n",
    "In addition to its [core features](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html), the SageMaker distributed model parallel library offers [memory-saving features](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch.html) for training deep learning models with PyTorch: [tensor parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-tensor-parallelism.html), [optimizer state sharding](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-optimizer-state-sharding.html), [activation checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html), and [activation offloading](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-offloading.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Model Parallel configuration\n",
    "\n",
    "Please refer to all the [configuration parameters](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html) related to SageMaker Distributed Training.\n",
    "\n",
    "As we are going to use PyTorch and Hugging Face for training GPT-J, it is important to understand all the SageMaker Distributed configuration parameters specific to PyTorch [here](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#pytorch-specific-parameters).\n",
    "\n",
    "#### Important\n",
    "\n",
    "`process_per_host` must not be greater than the number of GPUs per instance and typically will be equal to the number of GPUs per instance.\n",
    "\n",
    "#### SageMaker Tensor Parallel\n",
    "\n",
    "Tensor parallelism splits individual layers, or nn.Modules, across devices, to be run in parallel. The following figure shows the simplest example of how the library splits a model with four layers to achieve two-way tensor parallelism (\"tensor_parallel_degree\": 2). The layers of each model replica are bisected and distributed into two GPUs. In this example case, the model parallel configuration also includes \"pipeline_parallel_degree\": 1 and \"ddp\": True (uses PyTorch DistributedDataParallel package in the background), so the degree of data parallelism becomes eight. The library manages communication across the tensor-distributed model replicas.\n",
    "\n",
    "![SageMaker Distributed Training Approaches](img/smdmp-tensor-parallel-only.png)\n",
    "\n",
    "The usefulness of this feature is in the fact that you can select specific layers or a subset of layers to apply tensor parallelism. To dive deep into tensor parallelism and other memory-saving features for PyTorch, and to learn how to set a combination of pipeline and tensor parallelism, see Extended Features of the SageMaker Model Parallel Library for PyTorch.\n",
    "\n",
    "\n",
    "\n",
    "#### Additional Resources\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to learn more about SMP and using SageMaker with PyTorch.\n",
    "\n",
    "1. To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "2. To learn more about using the SageMaker Python SDK with PyTorch, see Using [PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "3. To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through how to use the tensor parallelism feature provided by the SageMaker model parallelism library. You'll learn how to run FP16 training of the GPT-J model with tensor parallelism on the GLUE sst2 dataset.\n",
    "\n",
    "## Install and Upgrade Libraries\n",
    "\n",
    "The SageMaker model parallelism library's tensor parallelism feature requires the SageMaker Python SDK and the SageMaker Experiments library. Run the following cell to install or upgrade the libraries.\n",
    "\n",
    "**Note:** To finish applying the changes, you must restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.25.63 requires botocore==1.27.62, but you have botocore 1.29.70 which is incompatible.\n",
      "awscli 1.25.63 requires rsa<4.8,>=3.1.2, but you have rsa 4.9 which is incompatible.\n",
      "aiobotocore 2.3.4 requires botocore<1.24.22,>=1.24.21, but you have botocore 1.29.70 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run once, restart kernel, then comment out this cell\n",
    "! pip install -qU pip\n",
    "! pip install -qU \"sagemaker>=2,<3\"\n",
    "! pip install -qU sagemaker-experiments\n",
    "! pip install -qU transformers datasets\n",
    "! pip install -qU 'sagemaker[local]' --upgrade\n",
    "\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Important</b> After you run the above cell, comment it out for future runs.\n",
    "\n",
    "Import and check if the SageMaker Python SDK version is successfully set to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.132.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Throughout this example, you'll use a training script of GPT-J model and a text dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment: your AWS account ID, the AWS Region you are using to run the notebook, and the ARN of your Amazon SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role:arn:aws:iam::232838030412:role/service-role/AmazonSageMaker-ExecutionRole-20211204T182243\n",
      "AWS account:232838030412\n",
      "AWS region:us-west-2\n",
      "\n",
      "Default bucket for this session:  sagemaker-us-west-2-232838030412\n",
      "CPU times: user 520 ms, sys: 57.2 ms, total: 577 ms\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import boto3\n",
    "\n",
    "\n",
    "def get_notebook_name():\n",
    "    import json\n",
    "\n",
    "    log_path = \"/opt/ml/metadata/resource-metadata.json\"\n",
    "    with open(log_path, \"r\") as logs:\n",
    "        _logs = json.load(logs)\n",
    "    return _logs[\"ResourceName\"]\n",
    "\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This completes the SageMaker setup._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare glue-sst2 data\n",
    "Here you will download, prepare the glue-sst2 dataset and then copy the files to S3. This is done because the `train_gptj_smp_tensor_parallel_script.py` requires either S3 input or paths in an FSx file system of an already tokenized dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and specify parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import transformers\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from transformers.testing_utils import CaptureLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"dataset_name\": \"glue\",\n",
    "    \"dataset_config_name\": \"sst2\",\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"cache_dir\": \"tmp\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data\n",
    "This section loads the dataset and splits it to training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4664ec004c24bfb807683e92ab6d377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/27.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c74c748f58a42b28ea71b83ef8cd28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\n",
    "    hyperparameters[\"dataset_name\"],\n",
    "    hyperparameters[\"dataset_config_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"validation\" not in raw_datasets.keys():\n",
    "    raw_datasets[\"validation\"] = load_dataset(\n",
    "        hyperparameters[\"dataset_name\"],\n",
    "        hyperparameters[\"dataset_config_name\"],\n",
    "        split=\"train[:5%]\",\n",
    "        cache_dir=hyperparameters[\"cache_dir\"],\n",
    "    )\n",
    "\n",
    "    raw_datasets[\"train\"] = load_dataset(\n",
    "        hyperparameters[\"dataset_name\"],\n",
    "        hyperparameters[\"dataset_config_name\"],\n",
    "        split=\"train[5%:]\",\n",
    "        cache_dir=hyperparameters[\"cache_dir\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load tokenizer\n",
    "Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model.  \n",
    "The following cell loads a tokenizer with [AutoTokenizer.from_pretrained()](https://huggingface.co/docs/transformers/v4.19.4/en/autoclass_tutorial#autotokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": hyperparameters[\"cache_dir\"],\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\", **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tok_logger = transformers.utils.logging.get_logger(\n",
    "        \"transformers.tokenization_utils_base\"\n",
    "    )\n",
    "\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        output = tokenizer(examples[text_column_name])\n",
    "        # clm input could be much much longer than block_size\n",
    "        if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "            tok_logger.warning(\n",
    "                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\"\n",
    "            )\n",
    "    return output\n",
    "\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1483fb63de4be6a6faf2256c49c099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4bf096b5f3400882dcd93069093313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c478f0e8db6441a297795411c1cecc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:The tokenizer picked seems to have a very large `model_max_length` (2048). Picking 1024 instead. You can change that default value by passing --block_size xxx.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e674f2038a3e45baa06eaebd98fd2330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef76f9e4aac4d9990974f58c5d16cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead7e3e54ab748b59edbce3493aaf841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "# since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
    "tok_logger = transformers.utils.logging.get_logger(\n",
    "    \"transformers.tokenization_utils_base\"\n",
    ")\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "\n",
    "block_size = tokenizer.model_max_length\n",
    "if block_size > 1024:\n",
    "    logger.warning(\n",
    "        f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "        \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\n",
    "    )\n",
    "    block_size = 1024\n",
    "else:\n",
    "    if args.block_size > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The block_size passed ({block_size}) is larger than the maximum length for the model\"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(block_size, tokenizer.model_max_length)\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    #     num_proc=args.preprocessing_num_workers,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparameters[\"do_train\"]:\n",
    "    if \"train\" not in tokenized_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = lm_datasets[\"train\"]\n",
    "\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    if \"validation\" not in tokenized_datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    eval_dataset = lm_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38459ad719e34487ac8c86315763bb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b370af922884429bbdd6d0a7c609d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_dataset_location = None\n",
    "validation_dataset_location = None\n",
    "\n",
    "\n",
    "if hyperparameters[\"do_train\"]:\n",
    "    train_dataset.to_json(\"./training.json\")\n",
    "    training_dataset_location = \"s3://{}/dataset/train/\".format(default_bucket)\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    eval_dataset.to_json(\"./validation.json\")\n",
    "    validation_dataset_location = \"s3://{}/dataset/validation/\".format(default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_dataset_location is not None:\n",
    "    command = \"aws s3 cp ./training.json {}\".format(training_dataset_location)\n",
    "    os.system(command)\n",
    "\n",
    "if validation_dataset_location is not None:\n",
    "    command = \"aws s3 cp ./validation.json {}\".format(validation_dataset_location)\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparameters[\"do_train\"]:\n",
    "    command = \"rm ./training.json\"\n",
    "    os.system(command)\n",
    "\n",
    "if hyperparameters[\"do_eval\"]:\n",
    "    command = \"rm ./validation.json\"\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'training_dataset_location' (str)\n",
      "Stored 'validation_dataset_location' (str)\n"
     ]
    }
   ],
   "source": [
    "%store training_dataset_location\n",
    "%store validation_dataset_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "training_dataset_location               -> 's3://sagemaker-us-west-2-232838030412/dataset/tra\n",
      "validation_dataset_location             -> 's3://sagemaker-us-west-2-232838030412/dataset/val\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Amazon S3 Bucket Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need to specify the paths for training data to be used by your job. The bucket used must be in the same region as where training will run. In the cells above you downloaded the glue-sst2 training and validation split datasets and uploaded the json files in an S3 bucket in your account. This example will train on those json files.\n",
    "\n",
    "After you successfully run this example tensor parallel training job, you can modify the S3 bucket to where your own dataset is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_dataset_location\n",
    "%store -r validation_dataset_location\n",
    "\n",
    "# if you're bringing your own data, uncomment the following lines and specify the locations there\n",
    "# training_dataset_location = YOUR_S3_BUCKET/training\n",
    "# validation_dataset_location = YOUR_S3_BUCKET/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_bucket = training_dataset_location\n",
    "s3_test_bucket = validation_dataset_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below bucket will store output artifacts of the training job. You can modify this as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = f\"s3://sagemaker-{region}-{account}/smp-tensorparallel-outputdir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Channels for SageMaker Training\n",
    "\n",
    "In this step, you define SageMaker training data channels using the above buckets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set use_fsx to False by default\n",
    "# Set below var to True if you want to use FSx (see next cell)\n",
    "use_fsx = False\n",
    "if not use_fsx:\n",
    "    train = sagemaker.inputs.TrainingInput(\n",
    "        s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "    )\n",
    "    test = sagemaker.inputs.TrainingInput(\n",
    "        s3_test_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "    )\n",
    "    data_channels = {\"train\": train, \"test\": test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up FSx and use FSx for data channels and checkpoints\n",
    "\n",
    "While the above option is easier to setup, using an FSx can be beneficial for performance when dealing with large input sizes and large model sizes. If you are using models with more than 13B parameters, checkpointing should be done using FSx. \n",
    "\n",
    "Amazon FSx for Lustre is a high performance file system optimized for workloads, such as machine learning, analytics and high performance computing. With Amazon FSx for Lustre, you can accelerate your File mode training jobs by avoiding the initial Amazon S3 download time.\n",
    "\n",
    "\n",
    "Please see the instructions at [Distributed Training of Mask-RCNN in Amazon SageMaker using FSx](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-scriptmode-fsx.ipynb), to create an Amazon FSx Lustre file-system and import data from the S3 bucket to your FSx file system. Note that the FSx must be created in a private subnet with internet gateway to ensure that training job has access to the internet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions obtained from:\n",
    "# https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-scriptmode-fsx.ipynb\n",
    "\n",
    "if use_fsx:\n",
    "    from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "    # Specify FSx Lustre file system id.\n",
    "    file_system_id = \"<fs-id>\"\n",
    "\n",
    "    # Specify the SG and subnet used by the FSx, these are passed to SM Estimator so jobs use this as well\n",
    "    fsx_security_group_id = \"<sg-id>\"\n",
    "    fsx_subnet = \"<subnet-id>\"\n",
    "\n",
    "    # Specify directory path for input data on the file system.\n",
    "    # You need to provide normalized and absolute path below.\n",
    "    # Your mount name can be provided by you when creating FSx, or generated automatically.\n",
    "    # You can find this mount_name on the FSx page in console.\n",
    "    # Example of FSx generated mount_name: \"3x8abcde\"\n",
    "    base_path = \"</3x8abcde>\"\n",
    "\n",
    "    # Specify your file system type.\n",
    "    file_system_type = \"FSxLustre\"\n",
    "\n",
    "    train = FileSystemInput(\n",
    "        file_system_id=file_system_id,\n",
    "        file_system_type=file_system_type,\n",
    "        directory_path=base_path,\n",
    "        file_system_access_mode=\"rw\",\n",
    "    )\n",
    "\n",
    "    data_channels = {\"train\": train, \"test\": train}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Hyperparameters, Metric Definitions, and MPI Options\n",
    "The following `hyperparameters` dictionary is to pass arguments to the training script (`train_gptj_smp_tesnor_parallel_script.py`) and set the model parallel configuration when creating the training job.\n",
    "\n",
    "You can also add custom mpi flags. By default, we have `--mca btl_vader_single_copy_mechanism none` to remove unnecessary logs.\n",
    "\n",
    "Next we add a base metric definitions to enable the metric upload in SageMaker. You can add any further metric definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_steps\": 100,\n",
    "    \"seed\": 12345,\n",
    "    \"fp16\": 1,\n",
    "    \"lr\": 2.0e-4,\n",
    "    \"lr_decay_iters\": 125000,\n",
    "    \"min_lr\": 0.00001,\n",
    "    \"lr-decay-style\": \"linear\",\n",
    "    \"warmup\": 0.01,\n",
    "    \"num_kept_checkpoints\": 5,\n",
    "    \"checkpoint_freq\": 200,\n",
    "    \"validation_freq\": 1000,\n",
    "    \"logging_freq\": 10,\n",
    "    \"save_final_full_model\": 1,\n",
    "    \"manual_partition\": 0,\n",
    "    \"skip_full_optimizer\": 1,\n",
    "    \"shard_optimizer_state\": 1,\n",
    "    \"activation_checkpointing\": 1,\n",
    "    \"activation_strategy\": \"each\",\n",
    "    \"optimize\": \"speed\",\n",
    "    # below flag loads model and optimizer state from checkpoint_s3_uri\n",
    "    # 'load_partial': 1,\n",
    "}\n",
    "\n",
    "if use_fsx:\n",
    "    # make sure to update paths for training-dir and test-dir based on the paths of datasets in FSx\n",
    "    # If you want to resume training, set checkpoint-dir to the same path as a previous job.\n",
    "    SM_TRAIN_DIR = \"/opt/ml/input/data/train\"\n",
    "    hyperparameters[\"checkpoint-dir\"] = f\"{SM_TRAIN_DIR}/checkpointdir-job2\"\n",
    "    hyperparameters[\"model-dir\"] = f\"{SM_TRAIN_DIR}/modeldir-job2\"\n",
    "    hyperparameters[\n",
    "        \"training-dir\"\n",
    "    ] = f\"{SM_TRAIN_DIR}/datasets/pytorch_gpt2/train_synthetic\"\n",
    "    hyperparameters[\"test-dir\"] = f\"{SM_TRAIN_DIR}/datasets/pytorch_gpt2/val_synthetic\"\n",
    "\n",
    "# The checkpoint path (hyperparameters['checkpoint-dir'] or checkpoint_s3_uri) is not unique per job.\n",
    "# You need to modify as needed for different runs.\n",
    "# If same path is used for unrelated runs, this may increase time when downloading unnecessary checkpoints,\n",
    "# and cause conflicts when loading checkpoints.\n",
    "\n",
    "\n",
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "mpioptions += \"-x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    "mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\"\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
    "]  # Add your custom metric definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model configuration below.   \n",
    "<font color='red'>\n",
    "Note that gpt-j-6B needs at least a single g5.48xlarge, p3dn.24xlarge, or p4d.24xlarge, or multiple nodes of smaller GPU instances. If you do not want to start an instance of this type, please use the smaller gpt-j-xl config. That model is a smaller 1.5B parameter model, which can fit on fewer or smaller GPUs in g5.24xlarge or p3 instances.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \"gpt-j-6B\"\n",
    "\n",
    "if model_config == \"gpt-j-6B\":\n",
    "    model_params = {\n",
    "        \"tensor_parallel_degree\": 8,\n",
    "        \"pipeline_parallel_degree\": 1,\n",
    "        \"train_batch_size\": 8,\n",
    "        \"val_batch_size\": 8,\n",
    "        \"prescaled_batch\": 1,\n",
    "        \"max_context_width\": 2048,\n",
    "        \"finetune_6b\": 1,\n",
    "    }\n",
    "elif model_config == \"gpt-j-xl\":\n",
    "    model_params = {\n",
    "        \"tensor_parallel_degree\": 4,\n",
    "        \"pipeline_parallel_degree\": 1,\n",
    "        \"train_batch_size\": 8,\n",
    "        \"val_batch_size\": 8,\n",
    "        \"prescaled_batch\": 1,\n",
    "        \"hidden_width\": 1600,\n",
    "        \"num_heads\": 25,\n",
    "        \"num_layers\": 48,\n",
    "    }\n",
    "\n",
    "for k, v in model_params.items():\n",
    "    hyperparameters[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up SageMaker Studio Experiment\n",
    "Create or load [SageMaker Experiment](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) for the example training job. This will create an experiment trial object in SageMaker Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Specify your experiment name\n",
    "experiment_name = \"smp-gptj-tensor-parallel\"\n",
    "# Specify your trial name\n",
    "trial_name = f\"{experiment_name}-trial1\"\n",
    "\n",
    "all_experiment_names = [exp.experiment_name for exp in Experiment.list()]\n",
    "# Load the experiment if it exists, otherwise create\n",
    "if experiment_name not in all_experiment_names:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client\n",
    "    )\n",
    "else:\n",
    "    experiment = Experiment.load(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client\n",
    "    )\n",
    "\n",
    "# Create the trial\n",
    "trial = Trial.create(\n",
    "    trial_name=\"smp-{}-{}\".format(trial_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())),\n",
    "    experiment_name=experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm_boto_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Essential Parameters for a SageMaker Training Job\n",
    "\n",
    "Next, you will use the [`SageMaker Estimator API`](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to define a SageMaker Training Job, passing values through the following parameters for training job name, the number of EC2 instances, the instance type, and the size of the volume attached to the instances. \n",
    "\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "* `base_job_name`\n",
    "\n",
    "### Update the Type of EC2 Instance to Use\n",
    "\n",
    "The instance type and the number of instances you specify to the `instance_type` and `instance_count` parameters, respectively, will determine the total number of GPUs (world size).\n",
    "\n",
    "$$ \\text{(world size) = (the number of GPUs on a single instance)}\\times\\text{(the number of instance)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select one of the below instances to train the gpt-j-xl model remotely\n",
    "<li>ml.g5.12xlarge\n",
    "<li>ml.g5.24xlarge\n",
    "<li>ml.g4dn.12xlarge\n",
    "<li>ml.p3.8xlarge\n",
    "<li>ml.p3.16xlarge\n",
    "<li>ml.p2.16xlarge  </font><br>\n",
    "<font color='red'></font>\n",
    "\n",
    "#### Select one of the below instances to train the gpt-j-6B model remotely\n",
    "<li>g5.48xlarge\n",
    "<li>p3dn.24xlarge\n",
    "<li>p4d.24xlarge<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "\n",
    "instance_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ml.p4d.24xlarge'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes_per_host is set to: 8\n"
     ]
    }
   ],
   "source": [
    "if instance_type in [\n",
    "    \"ml.p3.16xlarge\",\n",
    "    \"ml.p3dn.24xlarge\",\n",
    "    \"ml.g5.48xlarge\",\n",
    "    \"ml.p4d.24xlarge\",\n",
    "]:\n",
    "    processes_per_host = 8\n",
    "elif instance_type == \"ml.p2.16xlarge\":\n",
    "    processes_per_host = 16\n",
    "else:\n",
    "    processes_per_host = 4\n",
    "\n",
    "print(\"processes_per_host is set to:\", processes_per_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look up the number of GPUs of different instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that, for example, a given instance type `p4d.24xlarge` has a corresponding instance type `ml.p4d.24xlarge` in SageMaker.\n",
    "For SageMaker supported `ml` instances and cost information, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach an EBS Volume to the Training Instance\n",
    "The volume size you specify in `volume_size` must be larger than your input data size. In this example, the volume size is set to 500GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a Base Job Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "pp_degree = hyperparameters[\"pipeline_parallel_degree\"]\n",
    "tp_degree = hyperparameters[\"tensor_parallel_degree\"]\n",
    "base_job_name = f'smp-{model_config}-{machine_str}-tp{tp_degree}-pp{pp_degree}-bs{hyperparameters[\"train_batch_size\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_fsx:\n",
    "    # If you want to resume training, set checkpoint_s3_uri to the same path as a previous job.\n",
    "    # Previous checkpoint to load must have same model config.\n",
    "    checkpoint_bucket = f\"s3://sagemaker-{region}-{account}/\"\n",
    "    checkpoint_s3_uri = f\"{checkpoint_bucket}/experiments/gptj_synthetic_simpletrainer_checkpoints/{base_job_name}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker HuggingFace 🤗 Estimator\n",
    "\n",
    "The following cell constructs a PyTorch estimator using the parameters defined above. To see how the SageMaker tensor parallelism modules and functions are applied to the script, see the `train_gptj_smp_tensor_parallel_script.py` file and the private preview documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "if use_fsx:\n",
    "    # Use the security group and subnet that was used to create the FSx filesystem\n",
    "    kwargs[\"security_group_ids\"] = [fsx_security_group_id]\n",
    "    kwargs[\"subnets\"] = [fsx_subnet]\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train_gptj_smp_tensor_parallel_script.py\",\n",
    "    source_dir=os.getcwd(),\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=volume_size,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": processes_per_host,\n",
    "            \"custom_mpi_options\": mpioptions,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"ddp\": True,\n",
    "                    \"tensor_parallel_degree\": hyperparameters[\"tensor_parallel_degree\"],\n",
    "                    # partitions is a required param in the current SM SDK so it needs to be passed,\n",
    "                    # these two map to the same config\n",
    "                    \"partitions\": hyperparameters[\"pipeline_parallel_degree\"],\n",
    "                    \"shard_optimizer_state\": hyperparameters[\"shard_optimizer_state\"]\n",
    "                    > 0,\n",
    "                    \"prescaled_batch\": hyperparameters[\"prescaled_batch\"] > 0,\n",
    "                    \"fp16\": hyperparameters[\"fp16\"] > 0,\n",
    "                    \"optimize\": hyperparameters[\"optimize\"],\n",
    "                    \"auto_partition\": False\n",
    "                    if hyperparameters[\"manual_partition\"]\n",
    "                    else True,\n",
    "                    \"default_partition\": 0,\n",
    "                    \"optimize\": hyperparameters[\"optimize\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    framework_version=\"1.12\",\n",
    "    py_version=\"py38\",\n",
    "    output_path=s3_output_bucket,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri if not use_fsx else None,\n",
    "    checkpoint_local_path=hyperparameters[\"checkpoint-dir\"] if use_fsx else None,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    base_job_name=base_job_name,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the estimator to launch the SageMaker training job of GPT-J model with tensor parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you receive a `ResourceLimitExceeded` error message when running the following cell, you can request an increase on the default quota by contacting [AWS support](https://console.aws.amazon.com/support). Open the [AWS Support Center](https://console.aws.amazon.com/support), and then choose Create case. Choose Service limit increase. For Limit Type choose SageMaker Training Jobs. Complete the rest of the form and submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: smp-gpt-j-6B-p4d24x-tp8-pp1-bs8-2023-02-14-08-33-39-465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-14 08:33:41 Starting - Starting the training job......\n",
      "2023-02-14 08:34:21 Starting - Preparing the instances for training.........\n",
      "2023-02-14 08:35:50 Downloading - Downloading input data.........\n",
      "2023-02-14 08:37:20 Training - Downloading the training image............\n",
      "2023-02-14 08:39:16 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:08,149 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:08,211 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:08,220 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:08,222 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:08,586 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting datasets\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.9.0-py3-none-any.whl (462 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 462.8/462.8 kB 13.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.127.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.1.42)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchnet in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (0.0.4)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.21.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 80.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug in /opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg (from -r requirements.txt (line 8)) (1.0.24b20230106)\u001b[0m\n",
      "\u001b[34mCollecting humanize\u001b[0m\n",
      "\u001b[34mDownloading humanize-4.6.0-py3-none-any.whl (109 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.0/110.0 kB 29.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting smart-open>=5.2.1\u001b[0m\n",
      "\u001b[34mDownloading smart_open-6.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 15.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3>=1.23 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.26.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (2.28.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[34mDownloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 772.3/772.3 kB 75.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 99.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.1.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.3/190.3 kB 46.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.21.0->-r requirements.txt (line 7)) (22.0)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (10.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213.0/213.0 kB 40.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (2022.11.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 94.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.26.45)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from torchnet->-r requirements.txt (line 6)) (1.12.1+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: visdom in /opt/conda/lib/python3.8/site-packages (from torchnet->-r requirements.txt (line 6)) (0.2.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from torchnet->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument==3.4.2 in /opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg (from smdebug->-r requirements.txt (line 8)) (3.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg (from pyinstrument==3.4.2->smdebug->-r requirements.txt (line 8)) (0.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker->-r requirements.txt (line 3)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.45 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker->-r requirements.txt (line 3)) (1.29.45)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 34.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 30.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 262.1/262.1 kB 50.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.0->-r requirements.txt (line 7)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker->-r requirements.txt (line 3)) (3.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.21.0->-r requirements.txt (line 7)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.21.0->-r requirements.txt (line 7)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2022.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker->-r requirements.txt (line 3)) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websocket-client in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpatch in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.32)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (2.8.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.8/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 6)) (2.3)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, xxhash, smart-open, regex, multidict, humanize, frozenlist, filelock, async-timeout, yarl, responses, huggingface-hub, aiosignal, transformers, aiohttp, datasets\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.9.0 filelock-3.9.0 frozenlist-1.3.3 huggingface-hub-0.12.0 humanize-4.6.0 multidict-6.0.4 regex-2022.10.31 responses-0.18.0 smart-open-6.3.0 tokenizers-0.12.1 transformers-4.21.0 xxhash-3.2.0 yarl-1.8.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.3.1 -> 23.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,111 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,111 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,177 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,250 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,259 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,259 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,261 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,261 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:8'] process_per_hosts: 8 num_processes: 8\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,262 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,324 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,396 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,405 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"activation_checkpointing\": 1,\n",
      "        \"activation_strategy\": \"each\",\n",
      "        \"checkpoint_freq\": 200,\n",
      "        \"finetune_6b\": 1,\n",
      "        \"fp16\": 1,\n",
      "        \"logging_freq\": 10,\n",
      "        \"lr\": 0.0002,\n",
      "        \"lr-decay-style\": \"linear\",\n",
      "        \"lr_decay_iters\": 125000,\n",
      "        \"manual_partition\": 0,\n",
      "        \"max_context_width\": 2048,\n",
      "        \"max_steps\": 100,\n",
      "        \"min_lr\": 1e-05,\n",
      "        \"mp_parameters\": {\n",
      "            \"ddp\": true,\n",
      "            \"tensor_parallel_degree\": 8,\n",
      "            \"partitions\": 1,\n",
      "            \"shard_optimizer_state\": true,\n",
      "            \"prescaled_batch\": true,\n",
      "            \"fp16\": true,\n",
      "            \"optimize\": \"speed\",\n",
      "            \"auto_partition\": true,\n",
      "            \"default_partition\": 0\n",
      "        },\n",
      "        \"num_kept_checkpoints\": 5,\n",
      "        \"optimize\": \"speed\",\n",
      "        \"pipeline_parallel_degree\": 1,\n",
      "        \"prescaled_batch\": 1,\n",
      "        \"save_final_full_model\": 1,\n",
      "        \"seed\": 12345,\n",
      "        \"shard_optimizer_state\": 1,\n",
      "        \"skip_full_optimizer\": 1,\n",
      "        \"tensor_parallel_degree\": 8,\n",
      "        \"train_batch_size\": 8,\n",
      "        \"val_batch_size\": 8,\n",
      "        \"validation_freq\": 1000,\n",
      "        \"warmup\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": true,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"smp-gpt-j-6B-p4d24x-tp8-pp1-bs8-2023-02-14-08-33-39-465\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-232838030412/smp-gpt-j-6B-p4d24x-tp8-pp1-bs8-2023-02-14-08-33-39-465/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_gptj_smp_tensor_parallel_script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_gptj_smp_tensor_parallel_script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"activation_checkpointing\":1,\"activation_strategy\":\"each\",\"checkpoint_freq\":200,\"finetune_6b\":1,\"fp16\":1,\"logging_freq\":10,\"lr\":0.0002,\"lr-decay-style\":\"linear\",\"lr_decay_iters\":125000,\"manual_partition\":0,\"max_context_width\":2048,\"max_steps\":100,\"min_lr\":1e-05,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"fp16\":true,\"optimize\":\"speed\",\"partitions\":1,\"prescaled_batch\":true,\"shard_optimizer_state\":true,\"tensor_parallel_degree\":8},\"num_kept_checkpoints\":5,\"optimize\":\"speed\",\"pipeline_parallel_degree\":1,\"prescaled_batch\":1,\"save_final_full_model\":1,\"seed\":12345,\"shard_optimizer_state\":1,\"skip_full_optimizer\":1,\"tensor_parallel_degree\":8,\"train_batch_size\":8,\"val_batch_size\":8,\"validation_freq\":1000,\"warmup\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_gptj_smp_tensor_parallel_script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_gptj_smp_tensor_parallel_script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-232838030412/smp-gpt-j-6B-p4d24x-tp8-pp1-bs8-2023-02-14-08-33-39-465/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"activation_checkpointing\":1,\"activation_strategy\":\"each\",\"checkpoint_freq\":200,\"finetune_6b\":1,\"fp16\":1,\"logging_freq\":10,\"lr\":0.0002,\"lr-decay-style\":\"linear\",\"lr_decay_iters\":125000,\"manual_partition\":0,\"max_context_width\":2048,\"max_steps\":100,\"min_lr\":1e-05,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"fp16\":true,\"optimize\":\"speed\",\"partitions\":1,\"prescaled_batch\":true,\"shard_optimizer_state\":true,\"tensor_parallel_degree\":8},\"num_kept_checkpoints\":5,\"optimize\":\"speed\",\"pipeline_parallel_degree\":1,\"prescaled_batch\":1,\"save_final_full_model\":1,\"seed\":12345,\"shard_optimizer_state\":1,\"skip_full_optimizer\":1,\"tensor_parallel_degree\":8,\"train_batch_size\":8,\"val_batch_size\":8,\"validation_freq\":1000,\"warmup\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"smp-gpt-j-6B-p4d24x-tp8-pp1-bs8-2023-02-14-08-33-39-465\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-232838030412/smp-gpt-j-6B-p4d24x-tp8-pp1-bs8-2023-02-14-08-33-39-465/source/sourcedir.tar.gz\",\"module_name\":\"train_gptj_smp_tensor_parallel_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_gptj_smp_tensor_parallel_script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--activation_checkpointing\",\"1\",\"--activation_strategy\",\"each\",\"--checkpoint_freq\",\"200\",\"--finetune_6b\",\"1\",\"--fp16\",\"1\",\"--logging_freq\",\"10\",\"--lr\",\"0.0002\",\"--lr-decay-style\",\"linear\",\"--lr_decay_iters\",\"125000\",\"--manual_partition\",\"0\",\"--max_context_width\",\"2048\",\"--max_steps\",\"100\",\"--min_lr\",\"1e-05\",\"--mp_parameters\",\"auto_partition=True,ddp=True,default_partition=0,fp16=True,optimize=speed,partitions=1,prescaled_batch=True,shard_optimizer_state=True,tensor_parallel_degree=8\",\"--num_kept_checkpoints\",\"5\",\"--optimize\",\"speed\",\"--pipeline_parallel_degree\",\"1\",\"--prescaled_batch\",\"1\",\"--save_final_full_model\",\"1\",\"--seed\",\"12345\",\"--shard_optimizer_state\",\"1\",\"--skip_full_optimizer\",\"1\",\"--tensor_parallel_degree\",\"8\",\"--train_batch_size\",\"8\",\"--val_batch_size\",\"8\",\"--validation_freq\",\"1000\",\"--warmup\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_ACTIVATION_CHECKPOINTING=1\u001b[0m\n",
      "\u001b[34mSM_HP_ACTIVATION_STRATEGY=each\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINT_FREQ=200\u001b[0m\n",
      "\u001b[34mSM_HP_FINETUNE_6B=1\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=1\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_FREQ=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LR-DECAY-STYLE=linear\u001b[0m\n",
      "\u001b[34mSM_HP_LR_DECAY_ITERS=125000\u001b[0m\n",
      "\u001b[34mSM_HP_MANUAL_PARTITION=0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_CONTEXT_WIDTH=2048\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=100\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_LR=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"fp16\":true,\"optimize\":\"speed\",\"partitions\":1,\"prescaled_batch\":true,\"shard_optimizer_state\":true,\"tensor_parallel_degree\":8}\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_KEPT_CHECKPOINTS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZE=speed\u001b[0m\n",
      "\u001b[34mSM_HP_PIPELINE_PARALLEL_DEGREE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PRESCALED_BATCH=1\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_FINAL_FULL_MODEL=1\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=12345\u001b[0m\n",
      "\u001b[34mSM_HP_SHARD_OPTIMIZER_STATE=1\u001b[0m\n",
      "\u001b[34mSM_HP_SKIP_FULL_OPTIMIZER=1\u001b[0m\n",
      "\u001b[34mSM_HP_TENSOR_PARALLEL_DEGREE=8\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_VAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FREQ=1000\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x FI_PROVIDER=efa -x NCCL_PROTO=simple -x FI_EFA_USE_DEVICE_RDMA=1 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_NUM_NEURONS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TEST -x SM_CHANNEL_TRAIN -x SM_HP_ACTIVATION_CHECKPOINTING -x SM_HP_ACTIVATION_STRATEGY -x SM_HP_CHECKPOINT_FREQ -x SM_HP_FINETUNE_6B -x SM_HP_FP16 -x SM_HP_LOGGING_FREQ -x SM_HP_LR -x SM_HP_LR-DECAY-STYLE -x SM_HP_LR_DECAY_ITERS -x SM_HP_MANUAL_PARTITION -x SM_HP_MAX_CONTEXT_WIDTH -x SM_HP_MAX_STEPS -x SM_HP_MIN_LR -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_KEPT_CHECKPOINTS -x SM_HP_OPTIMIZE -x SM_HP_PIPELINE_PARALLEL_DEGREE -x SM_HP_PRESCALED_BATCH -x SM_HP_SAVE_FINAL_FULL_MODEL -x SM_HP_SEED -x SM_HP_SHARD_OPTIMIZER_STATE -x SM_HP_SKIP_FULL_OPTIMIZER -x SM_HP_TENSOR_PARALLEL_DEGREE -x SM_HP_TRAIN_BATCH_SIZE -x SM_HP_VAL_BATCH_SIZE -x SM_HP_VALIDATION_FREQ -x SM_HP_WARMUP -x PYTHONPATH smddpmprun -i ml.p4d.24xlarge --allow-bypass /opt/conda/bin/python3.8 -m mpi4py train_gptj_smp_tensor_parallel_script.py --activation_checkpointing 1 --activation_strategy each --checkpoint_freq 200 --finetune_6b 1 --fp16 1 --logging_freq 10 --lr 0.0002 --lr-decay-style linear --lr_decay_iters 125000 --manual_partition 0 --max_context_width 2048 --max_steps 100 --min_lr 1e-05 --mp_parameters auto_partition=True,ddp=True,default_partition=0,fp16=True,optimize=speed,partitions=1,prescaled_batch=True,shard_optimizer_state=True,tensor_parallel_degree=8 --num_kept_checkpoints 5 --optimize speed --pipeline_parallel_degree 1 --prescaled_batch 1 --save_final_full_model 1 --seed 12345 --shard_optimizer_state 1 --skip_full_optimizer 1 --tensor_parallel_degree 8 --train_batch_size 8 --val_batch_size 8 --validation_freq 1000 --warmup 0.01\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:16,466 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-02-14 08:40:18,794 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m[algo-1:00165] Warning: could not find environment variable \"SM_HP_LR-DECAY-STYLE\"\u001b[0m\n",
      "\u001b[34mData for JOB [41027,1] offset 0 Total slots allocated 8\u001b[0m\n",
      "\u001b[34m========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41027,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41027,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41027,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41027,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41027,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41027,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41027,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41027,1] App: 0 Process rank: 7 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:40:22.893: W smdistributed/modelparallel/backend/core.py:305] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:40:22.893: W smdistributed/modelparallel/backend/core.py:305] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:40:22.893: W smdistributed/modelparallel/backend/core.py:305] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:40:22.893: W smdistributed/modelparallel/backend/core.py:305] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:22.893: W smdistributed/modelparallel/backend/core.py:305] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:40:22.893: W smdistributed/modelparallel/backend/core.py:305] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:40:22.893: W smdistributed/modelparallel/backend/core.py:305] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:40:22.893: W smdistributed/modelparallel/backend/core.py:305] smddp backend does not support training on single node. Falling back to nccl backend.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:22.894: I smdistributed/modelparallel/torch/state_mod.py:106] [0] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:40:22.894: I smdistributed/modelparallel/torch/state_mod.py:106] [1] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:40:22.894: I smdistributed/modelparallel/torch/state_mod.py:106] [6] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:40:22.894: I smdistributed/modelparallel/torch/state_mod.py:106] [4] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:40:22.894: I smdistributed/modelparallel/torch/state_mod.py:106] [5] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:40:22.894: I smdistributed/modelparallel/torch/state_mod.py:106] [7] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:40:22.895: I smdistributed/modelparallel/torch/state_mod.py:106] [2] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:40:22.895: I smdistributed/modelparallel/torch/state_mod.py:106] [3] Initializing torch distributed process groups with nccl backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:40:23.094: I smdistributed/modelparallel/torch/state_mod.py:169] [7] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 7, dp_rank: 7, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:40:23.102: I smdistributed/modelparallel/torch/state_mod.py:169] [1] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 1, dp_rank: 1, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:40:23.103: I smdistributed/modelparallel/torch/state_mod.py:169] [3] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 3, dp_rank: 3, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.104: I smdistributed/modelparallel/torch/state_mod.py:169] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.104: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:40:23.104: I smdistributed/modelparallel/torch/state_mod.py:169] [5] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 5, dp_rank: 5, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:40:23.104: I smdistributed/modelparallel/torch/state_mod.py:169] [2] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 2, dp_rank: 2, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:40:23.104: I smdistributed/modelparallel/torch/state_mod.py:169] [6] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 6, dp_rank: 6, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:40:23.104: I smdistributed/modelparallel/torch/state_mod.py:169] [4] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 4, dp_rank: 4, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.120: I smdistributed/modelparallel/backend/config.py:293] Configuration parameters:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.120: I smdistributed/modelparallel/backend/config.py:296]   pipeline_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.120: I smdistributed/modelparallel/backend/config.py:296]   microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.120: I smdistributed/modelparallel/backend/config.py:296]   pipeline: interleaved\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   horovod: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   ddp: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   tensor_parallel_degree: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   sdp_reduce_bucket_size: 500000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   sdp_param_persistence_threshold: 1000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   sdp_max_live_parameters: 1000000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   sdp_hierarchical_allgather: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   sdp_gradient_clipping: 1.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   ddp_port: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   ddp_dist_backend: auto\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   contiguous: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   placement_strategy: cluster\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   optimize: speed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   default_partition: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   auto_partition: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.121: I smdistributed/modelparallel/backend/config.py:296]   prescaled_batch: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   active_microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   fp16: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   fp16_params: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   bf16: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   offload_activations: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   shard_optimizer_state: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   sharded_data_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   delayed_parameter_initialization: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   skip_tracing: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: I smdistributed/modelparallel/backend/config.py:296]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.122: W smdistributed/modelparallel/backend/config.py:301] WARNING: \"fp16_params\" is a deprecated config key, please use \"fp16\" instead\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:40:23.452: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:40:23.470: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:40:23.470: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:40:23.473: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:40:23.475: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:40:23.477: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:40:23.484: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:40:23.488: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Arguments: [1,mpirank:0,algo-1]<stdout>:{'train_batch_size': 8, 'val_batch_size': 8, 'max_steps': 100, 'seed': 12345, 'same_seed': 0, 'n_gpus': '8', 'fp16': 1, 'bf16': 0, 'sharded_data_parallel_degree': 1, 'grad_clip': 1.0, 'weight_decay': 0.01, 'beta1': 0.9, 'beta2': 0.95, 'activation_checkpointing': 1, 'logging_freq': 10, 'use_bert_data': 0, 'zipped_data': 0, 'epochs': 1, 'output_data_dir': '/opt/ml/output/data', 'checkpoint_dir': '/opt/ml/checkpoints', 'model_dir': '/opt/ml/model', 'training_dir': '/opt/ml/input/data/train', 'test_dir': '/opt/ml/input/data/test', 'parallel_proc_data_processing': 0, 'save_final_full_model': 1, 'load_partial': 0, 'load_full': 0, 'logits_output': '', 'prescaled_batch': 1, 'max_context_width': 2048, 'vocab_size': 50400, 'hidden_width': 768, 'num_layers': 12, 'num_heads': 12, 'resid_pdrop': 0.1, 'embd_pdrop': 0.1, 'attn_pdrop': 0.1, 'summary_first_pdrop': 0.1, 'use_adamw': 0, 'finetune_6b': 1, 'use_distributed_transformer': 1, 'checkpoint_sublayers': 0, 'tensor_parallel_degree': 8, 'pipeline_parallel_degree': 1, 'microbatches': 1, 'active_microbatches': None, 'optimize': 'speed', 'activation_strategy': 'each', 'shard_optimizer_state': 1, 'offload_activations': 0, 'fast_mode': 0, 'static_mode': 0, 'delayed_param': 0, 'same_partition_load': 0, 'attention_in_fp32': 0, 'placement_strategy': 'cluster', 'activation_loading_horizon': 4, 'skip_tracing': 0, 'query_key_layer_scaling': 1, 'fused_softmax': 1, 'fused_dropout': 0, 'fused_bias_gelu': 1, 'gradient_accumulation': 1, 'num_kept_checkpoints': 5, 'checkpoint_freq': 200, 'validation_freq': 1000, 'validation_batches': 10, 'manual_partition': 0, 'partition_assignment': '', 'preserve_np_state': 0, 'fast_validation': 1, 'gather_if_shard': 1, 'clean_cache': 0, 'use_fsx': 0, 'enable_memory_profiling': 0, 'lr': 0.0002, 'lr_decay_style': 'linear', 'lr_decay_iters': 125000, 'min_lr': 1e-05, 'warmup': 0.01, 'plateau': 0.4, 'ci': False, 'time_to_train': None, 'throughput': None, 'loss': None}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Transformers version: 4.21.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:smdistributed.modelparallel version: 1.13.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:smdistributed config: {'ddp': True, 'tensor_parallel_degree': 8, 'pipeline_parallel_degree': 1, 'microbatches': 1, 'shard_optimizer_state': True, 'prescaled_batch': True, 'fp16': True, 'bf16': False, 'offload_activations': False, 'delayed_parameter_initialization': False, 'optimize': 'speed', 'placement_strategy': 'cluster', 'activation_loading_horizon': 4, 'skip_tracing': False, 'auto_partition': True, 'default_partition': 0, 'static_mode': False, 'fast_mode': False, 'sharded_data_parallel_degree': 1}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[Warning] Note that save_final_full_model only saves the final model at the end of all steps. It does not save optimizer state. Optimizer state is only saved with partial models which are saved at checkpointing_freq during training. If you want to restart training you need partial checkpoints.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading config.json:   0%|          | 0.00/836 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading config.json: 100%|██████████| 836/836 [00:00<00:00, 1.66MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   0%|          | 0.00/11.3G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   0%|          | 8.06M/11.3G [00:00<02:23, 84.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   0%|          | 19.8M/11.3G [00:00<01:52, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   0%|          | 31.6M/11.3G [00:00<01:45, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   0%|          | 43.4M/11.3G [00:00<01:41, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   0%|          | 55.2M/11.3G [00:00<01:40, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   1%|          | 67.1M/11.3G [00:00<01:38, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   1%|          | 79.0M/11.3G [00:00<01:38, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   1%|          | 90.7M/11.3G [00:00<01:37, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   1%|          | 103M/11.3G [00:00<01:37, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   1%|          | 114M/11.3G [00:01<01:36, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   1%|          | 126M/11.3G [00:01<01:36, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   1%|          | 138M/11.3G [00:01<01:36, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   1%|▏         | 150M/11.3G [00:01<01:36, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   1%|▏         | 162M/11.3G [00:01<01:36, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   2%|▏         | 174M/11.3G [00:01<01:35, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   2%|▏         | 186M/11.3G [00:01<01:35, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   2%|▏         | 198M/11.3G [00:01<01:35, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   2%|▏         | 209M/11.3G [00:01<01:35, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   2%|▏         | 221M/11.3G [00:01<01:35, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   2%|▏         | 233M/11.3G [00:02<01:35, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   2%|▏         | 245M/11.3G [00:02<01:35, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   2%|▏         | 257M/11.3G [00:02<01:35, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   2%|▏         | 269M/11.3G [00:02<01:35, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   2%|▏         | 281M/11.3G [00:02<01:34, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   3%|▎         | 292M/11.3G [00:02<01:34, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   3%|▎         | 304M/11.3G [00:02<01:34, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   3%|▎         | 316M/11.3G [00:02<01:34, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   3%|▎         | 328M/11.3G [00:02<01:34, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   3%|▎         | 340M/11.3G [00:02<01:34, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   3%|▎         | 352M/11.3G [00:03<01:34, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   3%|▎         | 364M/11.3G [00:03<01:34, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   3%|▎         | 376M/11.3G [00:03<01:34, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   3%|▎         | 387M/11.3G [00:03<01:33, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   3%|▎         | 399M/11.3G [00:03<01:39, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   4%|▎         | 411M/11.3G [00:03<01:37, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   4%|▎         | 423M/11.3G [00:03<01:36, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   4%|▍         | 435M/11.3G [00:03<01:35, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   4%|▍         | 447M/11.3G [00:03<01:34, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   4%|▍         | 459M/11.3G [00:03<01:34, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   4%|▍         | 470M/11.3G [00:04<01:34, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   4%|▍         | 482M/11.3G [00:04<01:33, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   4%|▍         | 494M/11.3G [00:04<01:33, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   4%|▍         | 506M/11.3G [00:04<01:33, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   4%|▍         | 518M/11.3G [00:04<01:33, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   5%|▍         | 530M/11.3G [00:04<01:32, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   5%|▍         | 542M/11.3G [00:04<01:32, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   5%|▍         | 553M/11.3G [00:04<01:32, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   5%|▍         | 565M/11.3G [00:04<01:32, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   5%|▌         | 577M/11.3G [00:04<01:32, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   5%|▌         | 589M/11.3G [00:05<01:32, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   5%|▌         | 601M/11.3G [00:05<01:32, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   5%|▌         | 613M/11.3G [00:05<01:32, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   5%|▌         | 625M/11.3G [00:05<01:32, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   6%|▌         | 637M/11.3G [00:05<01:31, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   6%|▌         | 649M/11.3G [00:05<01:31, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   6%|▌         | 660M/11.3G [00:05<01:31, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   6%|▌         | 672M/11.3G [00:05<01:31, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   6%|▌         | 684M/11.3G [00:05<01:31, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   6%|▌         | 696M/11.3G [00:05<01:31, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   6%|▌         | 708M/11.3G [00:06<01:31, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   6%|▌         | 720M/11.3G [00:06<01:31, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   6%|▋         | 732M/11.3G [00:06<01:31, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   6%|▋         | 744M/11.3G [00:06<01:31, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   7%|▋         | 756M/11.3G [00:06<01:31, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   7%|▋         | 767M/11.3G [00:06<01:30, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   7%|▋         | 779M/11.3G [00:06<01:30, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   7%|▋         | 791M/11.3G [00:06<01:30, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   7%|▋         | 803M/11.3G [00:06<01:30, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   7%|▋         | 815M/11.3G [00:06<01:30, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   7%|▋         | 827M/11.3G [00:07<01:30, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   7%|▋         | 839M/11.3G [00:07<01:30, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   7%|▋         | 851M/11.3G [00:07<01:30, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   7%|▋         | 862M/11.3G [00:07<01:30, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 874M/11.3G [00:07<01:30, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 886M/11.3G [00:07<01:29, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 898M/11.3G [00:07<01:29, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 910M/11.3G [00:07<01:29, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 922M/11.3G [00:07<01:29, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 934M/11.3G [00:07<01:29, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 946M/11.3G [00:08<01:29, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 958M/11.3G [00:08<01:29, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 969M/11.3G [00:08<01:28, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   8%|▊         | 981M/11.3G [00:08<01:28, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   9%|▊         | 993M/11.3G [00:08<01:28, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   9%|▊         | 0.98G/11.3G [00:08<01:28, 125MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   9%|▉         | 0.99G/11.3G [00:08<01:28, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   9%|▉         | 1.00G/11.3G [00:08<01:28, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   9%|▉         | 1.02G/11.3G [00:08<01:28, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   9%|▉         | 1.03G/11.3G [00:08<01:28, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   9%|▉         | 1.04G/11.3G [00:09<01:42, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   9%|▉         | 1.05G/11.3G [00:09<01:38, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:   9%|▉         | 1.06G/11.3G [00:09<01:35, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  10%|▉         | 1.07G/11.3G [00:09<01:33, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  10%|▉         | 1.09G/11.3G [00:09<01:31, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  10%|▉         | 1.10G/11.3G [00:09<01:30, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  10%|▉         | 1.11G/11.3G [00:09<01:30, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  10%|▉         | 1.12G/11.3G [00:09<01:29, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  10%|█         | 1.13G/11.3G [00:09<01:28, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  10%|█         | 1.14G/11.3G [00:09<01:28, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  10%|█         | 1.15G/11.3G [00:10<01:28, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  10%|█         | 1.17G/11.3G [00:10<01:27, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  10%|█         | 1.18G/11.3G [00:10<01:27, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  11%|█         | 1.19G/11.3G [00:10<01:27, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  11%|█         | 1.20G/11.3G [00:10<01:27, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  11%|█         | 1.21G/11.3G [00:10<01:27, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  11%|█         | 1.22G/11.3G [00:10<01:27, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  11%|█         | 1.24G/11.3G [00:10<01:27, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  11%|█         | 1.25G/11.3G [00:10<01:27, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  11%|█         | 1.26G/11.3G [00:10<01:27, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  11%|█▏        | 1.27G/11.3G [00:11<01:26, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  11%|█▏        | 1.28G/11.3G [00:11<01:26, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  11%|█▏        | 1.29G/11.3G [00:11<01:26, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  12%|█▏        | 1.30G/11.3G [00:11<01:26, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  12%|█▏        | 1.32G/11.3G [00:11<01:54, 93.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  12%|█▏        | 1.33G/11.3G [00:11<01:46, 100MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  12%|█▏        | 1.34G/11.3G [00:11<01:40, 106MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  12%|█▏        | 1.35G/11.3G [00:11<01:36, 111MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  12%|█▏        | 1.36G/11.3G [00:11<01:32, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  12%|█▏        | 1.37G/11.3G [00:12<01:30, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  12%|█▏        | 1.38G/11.3G [00:12<01:29, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  12%|█▏        | 1.40G/11.3G [00:12<01:28, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  12%|█▏        | 1.41G/11.3G [00:12<01:27, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  13%|█▎        | 1.42G/11.3G [00:12<01:26, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  13%|█▎        | 1.43G/11.3G [00:12<01:26, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  13%|█▎        | 1.44G/11.3G [00:12<01:26, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  13%|█▎        | 1.45G/11.3G [00:12<01:25, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  13%|█▎        | 1.47G/11.3G [00:12<01:25, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  13%|█▎        | 1.48G/11.3G [00:12<01:25, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  13%|█▎        | 1.49G/11.3G [00:13<01:25, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  13%|█▎        | 1.50G/11.3G [00:13<01:25, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  13%|█▎        | 1.51G/11.3G [00:13<01:24, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  14%|█▎        | 1.52G/11.3G [00:13<01:24, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  14%|█▎        | 1.53G/11.3G [00:13<01:24, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  14%|█▎        | 1.55G/11.3G [00:13<01:24, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  14%|█▍        | 1.56G/11.3G [00:13<01:24, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  14%|█▍        | 1.57G/11.3G [00:13<01:24, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  14%|█▍        | 1.58G/11.3G [00:13<01:24, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  14%|█▍        | 1.59G/11.3G [00:13<01:23, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  14%|█▍        | 1.60G/11.3G [00:14<01:23, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  14%|█▍        | 1.62G/11.3G [00:14<02:09, 80.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  14%|█▍        | 1.63G/11.3G [00:14<01:55, 89.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  15%|█▍        | 1.64G/11.3G [00:14<01:46, 97.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  15%|█▍        | 1.65G/11.3G [00:14<01:39, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  15%|█▍        | 1.66G/11.3G [00:14<01:34, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  15%|█▍        | 1.67G/11.3G [00:14<01:31, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  15%|█▍        | 1.68G/11.3G [00:14<01:28, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  15%|█▌        | 1.70G/11.3G [00:15<01:26, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  15%|█▌        | 1.71G/11.3G [00:15<01:25, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  15%|█▌        | 1.72G/11.3G [00:15<01:24, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  15%|█▌        | 1.73G/11.3G [00:15<01:23, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  15%|█▌        | 1.74G/11.3G [00:15<01:23, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  16%|█▌        | 1.75G/11.3G [00:15<01:23, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  16%|█▌        | 1.77G/11.3G [00:15<01:23, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  16%|█▌        | 1.78G/11.3G [00:15<01:22, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  16%|█▌        | 1.79G/11.3G [00:15<01:22, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  16%|█▌        | 1.80G/11.3G [00:15<01:22, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  16%|█▌        | 1.81G/11.3G [00:16<01:22, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  16%|█▌        | 1.82G/11.3G [00:16<01:22, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  16%|█▋        | 1.83G/11.3G [00:16<01:22, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  16%|█▋        | 1.85G/11.3G [00:16<01:22, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  16%|█▋        | 1.86G/11.3G [00:16<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 1.87G/11.3G [00:16<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 1.88G/11.3G [00:16<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 1.89G/11.3G [00:16<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 1.90G/11.3G [00:16<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 1.91G/11.3G [00:16<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 1.93G/11.3G [00:17<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 1.94G/11.3G [00:17<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 1.95G/11.3G [00:17<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 1.96G/11.3G [00:17<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  17%|█▋        | 1.97G/11.3G [00:17<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  18%|█▊        | 1.98G/11.3G [00:17<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  18%|█▊        | 1.99G/11.3G [00:17<01:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  18%|█▊        | 2.01G/11.3G [00:17<01:20, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  18%|█▊        | 2.02G/11.3G [00:17<01:20, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  18%|█▊        | 2.03G/11.3G [00:17<01:20, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  18%|█▊        | 2.04G/11.3G [00:18<01:20, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  18%|█▊        | 2.05G/11.3G [00:18<01:20, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  18%|█▊        | 2.06G/11.3G [00:18<01:20, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  18%|█▊        | 2.08G/11.3G [00:18<01:20, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  19%|█▊        | 2.09G/11.3G [00:18<01:20, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  19%|█▊        | 2.10G/11.3G [00:18<01:20, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  19%|█▊        | 2.11G/11.3G [00:18<01:19, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  19%|█▉        | 2.12G/11.3G [00:18<01:19, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  19%|█▉        | 2.13G/11.3G [00:18<01:19, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  19%|█▉        | 2.14G/11.3G [00:18<01:19, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  19%|█▉        | 2.16G/11.3G [00:19<01:19, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  19%|█▉        | 2.17G/11.3G [00:19<01:19, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  19%|█▉        | 2.18G/11.3G [00:19<01:19, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  19%|█▉        | 2.19G/11.3G [00:19<01:19, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  20%|█▉        | 2.20G/11.3G [00:19<01:19, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  20%|█▉        | 2.21G/11.3G [00:19<01:19, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  20%|█▉        | 2.22G/11.3G [00:19<01:18, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  20%|█▉        | 2.24G/11.3G [00:19<01:18, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  20%|█▉        | 2.25G/11.3G [00:19<01:18, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  20%|██        | 2.26G/11.3G [00:19<01:18, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  20%|██        | 2.27G/11.3G [00:20<01:18, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  20%|██        | 2.28G/11.3G [00:20<01:18, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  20%|██        | 2.29G/11.3G [00:20<01:17, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  20%|██        | 2.31G/11.3G [00:20<01:17, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  21%|██        | 2.32G/11.3G [00:20<01:17, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  21%|██        | 2.33G/11.3G [00:20<01:17, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  21%|██        | 2.34G/11.3G [00:20<01:17, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  21%|██        | 2.35G/11.3G [00:20<01:17, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  21%|██        | 2.36G/11.3G [00:20<01:17, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  21%|██        | 2.37G/11.3G [00:20<01:17, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  21%|██        | 2.39G/11.3G [00:21<01:17, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  21%|██▏       | 2.40G/11.3G [00:21<01:17, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  21%|██▏       | 2.41G/11.3G [00:21<01:17, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  21%|██▏       | 2.42G/11.3G [00:21<01:16, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  22%|██▏       | 2.43G/11.3G [00:21<01:16, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  22%|██▏       | 2.44G/11.3G [00:21<01:16, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  22%|██▏       | 2.45G/11.3G [00:21<01:16, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  22%|██▏       | 2.47G/11.3G [00:21<01:16, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  22%|██▏       | 2.48G/11.3G [00:21<01:16, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  22%|██▏       | 2.49G/11.3G [00:21<01:16, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  22%|██▏       | 2.50G/11.3G [00:22<01:16, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  22%|██▏       | 2.51G/11.3G [00:22<01:16, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  22%|██▏       | 2.52G/11.3G [00:22<01:16, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  22%|██▏       | 2.54G/11.3G [00:22<01:16, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  23%|██▎       | 2.55G/11.3G [00:22<01:17, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  23%|██▎       | 2.56G/11.3G [00:22<01:17, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  23%|██▎       | 2.57G/11.3G [00:22<01:16, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  23%|██▎       | 2.58G/11.3G [00:22<01:41, 91.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  23%|██▎       | 2.59G/11.3G [00:23<01:33, 99.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  23%|██▎       | 2.60G/11.3G [00:23<01:28, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  23%|██▎       | 2.62G/11.3G [00:23<01:24, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  23%|██▎       | 2.63G/11.3G [00:23<01:21, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  23%|██▎       | 2.64G/11.3G [00:23<01:19, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  24%|██▎       | 2.65G/11.3G [00:23<01:18, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  24%|██▎       | 2.66G/11.3G [00:23<01:17, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  24%|██▎       | 2.67G/11.3G [00:23<01:16, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  24%|██▍       | 2.68G/11.3G [00:23<01:15, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  24%|██▍       | 2.70G/11.3G [00:23<01:15, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  24%|██▍       | 2.71G/11.3G [00:24<01:15, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  24%|██▍       | 2.72G/11.3G [00:24<01:24, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  24%|██▍       | 2.73G/11.3G [00:24<01:21, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  24%|██▍       | 2.74G/11.3G [00:24<01:57, 78.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  24%|██▍       | 2.75G/11.3G [00:24<02:01, 75.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  24%|██▍       | 2.76G/11.3G [00:24<01:46, 85.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  25%|██▍       | 2.77G/11.3G [00:24<01:36, 94.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  25%|██▍       | 2.78G/11.3G [00:24<01:29, 102MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  25%|██▍       | 2.79G/11.3G [00:25<01:24, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  25%|██▍       | 2.81G/11.3G [00:25<01:21, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  25%|██▍       | 2.82G/11.3G [00:25<01:19, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  25%|██▌       | 2.83G/11.3G [00:25<01:17, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  25%|██▌       | 2.84G/11.3G [00:25<01:16, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  25%|██▌       | 2.85G/11.3G [00:25<01:15, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  25%|██▌       | 2.86G/11.3G [00:25<01:14, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  26%|██▌       | 2.88G/11.3G [00:25<01:14, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  26%|██▌       | 2.89G/11.3G [00:25<01:13, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  26%|██▌       | 2.90G/11.3G [00:25<01:13, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  26%|██▌       | 2.91G/11.3G [00:26<01:13, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  26%|██▌       | 2.92G/11.3G [00:26<01:12, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  26%|██▌       | 2.93G/11.3G [00:26<01:12, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  26%|██▌       | 2.94G/11.3G [00:26<01:12, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  26%|██▌       | 2.96G/11.3G [00:26<01:12, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  26%|██▋       | 2.97G/11.3G [00:26<01:12, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  26%|██▋       | 2.98G/11.3G [00:26<01:12, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  27%|██▋       | 2.99G/11.3G [00:26<01:12, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  27%|██▋       | 3.00G/11.3G [00:26<01:12, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  27%|██▋       | 3.01G/11.3G [00:26<01:11, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  27%|██▋       | 3.02G/11.3G [00:27<01:11, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  27%|██▋       | 3.04G/11.3G [00:27<01:11, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  27%|██▋       | 3.05G/11.3G [00:27<01:11, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  27%|██▋       | 3.06G/11.3G [00:27<01:11, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  27%|██▋       | 3.07G/11.3G [00:27<01:11, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  27%|██▋       | 3.08G/11.3G [00:27<01:11, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  27%|██▋       | 3.09G/11.3G [00:27<01:11, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  28%|██▊       | 3.11G/11.3G [00:27<01:11, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  28%|██▊       | 3.12G/11.3G [00:27<01:11, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  28%|██▊       | 3.13G/11.3G [00:27<01:11, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  28%|██▊       | 3.14G/11.3G [00:28<01:10, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  28%|██▊       | 3.15G/11.3G [00:28<01:10, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  28%|██▊       | 3.16G/11.3G [00:28<01:10, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  28%|██▊       | 3.17G/11.3G [00:28<01:10, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  28%|██▊       | 3.19G/11.3G [00:28<01:10, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  28%|██▊       | 3.20G/11.3G [00:28<01:10, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  28%|██▊       | 3.21G/11.3G [00:28<01:10, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  29%|██▊       | 3.22G/11.3G [00:28<01:10, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  29%|██▊       | 3.23G/11.3G [00:28<01:10, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  29%|██▉       | 3.24G/11.3G [00:28<01:10, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  29%|██▉       | 3.25G/11.3G [00:29<01:09, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  29%|██▉       | 3.27G/11.3G [00:29<01:09, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  29%|██▉       | 3.28G/11.3G [00:29<01:09, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  29%|██▉       | 3.29G/11.3G [00:29<01:09, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  29%|██▉       | 3.30G/11.3G [00:29<01:09, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  29%|██▉       | 3.31G/11.3G [00:29<01:09, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  29%|██▉       | 3.32G/11.3G [00:29<01:09, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  30%|██▉       | 3.34G/11.3G [00:29<01:09, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  30%|██▉       | 3.35G/11.3G [00:29<01:09, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  30%|██▉       | 3.36G/11.3G [00:29<01:08, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  30%|██▉       | 3.37G/11.3G [00:30<01:08, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  30%|██▉       | 3.38G/11.3G [00:30<01:08, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  30%|███       | 3.39G/11.3G [00:30<01:08, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  30%|███       | 3.40G/11.3G [00:30<01:08, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  30%|███       | 3.42G/11.3G [00:30<01:08, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  30%|███       | 3.43G/11.3G [00:30<01:08, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  30%|███       | 3.44G/11.3G [00:30<01:08, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  31%|███       | 3.45G/11.3G [00:30<01:08, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  31%|███       | 3.46G/11.3G [00:30<01:08, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  31%|███       | 3.47G/11.3G [00:30<01:08, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  31%|███       | 3.48G/11.3G [00:31<01:07, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  31%|███       | 3.50G/11.3G [00:31<01:07, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  31%|███       | 3.51G/11.3G [00:31<01:07, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  31%|███       | 3.52G/11.3G [00:31<01:07, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  31%|███▏      | 3.53G/11.3G [00:31<01:07, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  31%|███▏      | 3.54G/11.3G [00:31<01:07, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  32%|███▏      | 3.55G/11.3G [00:31<01:07, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  32%|███▏      | 3.56G/11.3G [00:31<01:07, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  32%|███▏      | 3.58G/11.3G [00:31<01:07, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  32%|███▏      | 3.59G/11.3G [00:31<01:07, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  32%|███▏      | 3.60G/11.3G [00:32<01:07, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  32%|███▏      | 3.61G/11.3G [00:32<01:06, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  32%|███▏      | 3.62G/11.3G [00:32<01:06, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  32%|███▏      | 3.63G/11.3G [00:32<01:06, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  32%|███▏      | 3.65G/11.3G [00:32<01:06, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  32%|███▏      | 3.66G/11.3G [00:32<01:06, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  33%|███▎      | 3.67G/11.3G [00:32<01:07, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  33%|███▎      | 3.68G/11.3G [00:32<01:07, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  33%|███▎      | 3.69G/11.3G [00:32<01:06, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  33%|███▎      | 3.70G/11.3G [00:32<01:06, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  33%|███▎      | 3.71G/11.3G [00:33<01:06, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  33%|███▎      | 3.73G/11.3G [00:33<01:06, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  33%|███▎      | 3.74G/11.3G [00:33<01:05, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  33%|███▎      | 3.75G/11.3G [00:33<01:05, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  33%|███▎      | 3.76G/11.3G [00:33<01:05, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  33%|███▎      | 3.77G/11.3G [00:33<01:05, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  34%|███▎      | 3.78G/11.3G [00:33<01:05, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  34%|███▎      | 3.79G/11.3G [00:33<01:05, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  34%|███▍      | 3.81G/11.3G [00:33<01:05, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  34%|███▍      | 3.82G/11.3G [00:33<01:05, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  34%|███▍      | 3.83G/11.3G [00:34<01:05, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  34%|███▍      | 3.84G/11.3G [00:34<01:04, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  34%|███▍      | 3.85G/11.3G [00:34<01:04, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  34%|███▍      | 3.86G/11.3G [00:34<01:08, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  34%|███▍      | 3.87G/11.3G [00:34<01:07, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  34%|███▍      | 3.89G/11.3G [00:34<01:06, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  35%|███▍      | 3.90G/11.3G [00:34<01:43, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  35%|███▍      | 3.91G/11.3G [00:34<01:31, 86.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  35%|███▍      | 3.92G/11.3G [00:35<01:23, 94.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  35%|███▍      | 3.93G/11.3G [00:35<01:17, 102MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  35%|███▍      | 3.94G/11.3G [00:35<01:13, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  35%|███▌      | 3.95G/11.3G [00:35<01:10, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  35%|███▌      | 3.97G/11.3G [00:35<01:08, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  35%|███▌      | 3.98G/11.3G [00:35<01:07, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  35%|███▌      | 3.99G/11.3G [00:35<01:05, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  35%|███▌      | 4.00G/11.3G [00:35<01:05, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  36%|███▌      | 4.01G/11.3G [00:35<01:04, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  36%|███▌      | 4.02G/11.3G [00:35<01:03, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  36%|███▌      | 4.03G/11.3G [00:36<01:03, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  36%|███▌      | 4.05G/11.3G [00:36<01:03, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  36%|███▌      | 4.06G/11.3G [00:36<01:03, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  36%|███▌      | 4.07G/11.3G [00:36<01:02, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  36%|███▌      | 4.08G/11.3G [00:36<01:02, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  36%|███▋      | 4.09G/11.3G [00:36<01:02, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  36%|███▋      | 4.10G/11.3G [00:36<01:02, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  36%|███▋      | 4.11G/11.3G [00:36<01:02, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  37%|███▋      | 4.13G/11.3G [00:36<01:02, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  37%|███▋      | 4.14G/11.3G [00:36<01:02, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  37%|███▋      | 4.15G/11.3G [00:37<01:02, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  37%|███▋      | 4.16G/11.3G [00:37<01:02, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  37%|███▋      | 4.17G/11.3G [00:37<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  37%|███▋      | 4.18G/11.3G [00:37<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  37%|███▋      | 4.19G/11.3G [00:37<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  37%|███▋      | 4.21G/11.3G [00:37<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  37%|███▋      | 4.22G/11.3G [00:37<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 4.23G/11.3G [00:37<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 4.24G/11.3G [00:37<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 4.25G/11.3G [00:37<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 4.26G/11.3G [00:38<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 4.28G/11.3G [00:38<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 4.29G/11.3G [00:38<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 4.30G/11.3G [00:38<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 4.31G/11.3G [00:38<01:01, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 4.32G/11.3G [00:38<01:00, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  38%|███▊      | 4.33G/11.3G [00:38<01:00, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  39%|███▊      | 4.34G/11.3G [00:38<01:00, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  39%|███▊      | 4.36G/11.3G [00:38<01:00, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  39%|███▊      | 4.37G/11.3G [00:38<01:00, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  39%|███▉      | 4.38G/11.3G [00:39<01:00, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  39%|███▉      | 4.39G/11.3G [00:39<01:00, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  39%|███▉      | 4.40G/11.3G [00:39<01:00, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  39%|███▉      | 4.41G/11.3G [00:39<01:00, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  39%|███▉      | 4.42G/11.3G [00:39<01:00, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  39%|███▉      | 4.44G/11.3G [00:39<00:59, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  39%|███▉      | 4.45G/11.3G [00:39<00:59, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  40%|███▉      | 4.46G/11.3G [00:39<00:59, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  40%|███▉      | 4.47G/11.3G [00:39<00:59, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  40%|███▉      | 4.48G/11.3G [00:39<00:59, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  40%|███▉      | 4.49G/11.3G [00:40<00:59, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  40%|███▉      | 4.50G/11.3G [00:40<00:59, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  40%|████      | 4.52G/11.3G [00:40<00:58, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  40%|████      | 4.53G/11.3G [00:40<00:58, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  40%|████      | 4.54G/11.3G [00:40<00:58, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  40%|████      | 4.55G/11.3G [00:40<00:58, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  40%|████      | 4.56G/11.3G [00:40<00:58, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  41%|████      | 4.57G/11.3G [00:40<00:58, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  41%|████      | 4.58G/11.3G [00:40<00:58, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  41%|████      | 4.60G/11.3G [00:40<00:58, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  41%|████      | 4.61G/11.3G [00:41<00:58, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  41%|████      | 4.62G/11.3G [00:41<01:26, 82.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  41%|████      | 4.63G/11.3G [00:41<01:17, 91.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  41%|████      | 4.64G/11.3G [00:41<01:11, 99.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  41%|████▏     | 4.65G/11.3G [00:41<01:07, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  41%|████▏     | 4.66G/11.3G [00:41<01:04, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  41%|████▏     | 4.68G/11.3G [00:41<01:02, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  42%|████▏     | 4.69G/11.3G [00:41<01:00, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  42%|████▏     | 4.70G/11.3G [00:42<00:59, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  42%|████▏     | 4.71G/11.3G [00:42<00:58, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  42%|████▏     | 4.72G/11.3G [00:42<00:58, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  42%|████▏     | 4.73G/11.3G [00:42<00:57, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  42%|████▏     | 4.74G/11.3G [00:42<00:57, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  42%|████▏     | 4.76G/11.3G [00:42<00:57, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  42%|████▏     | 4.77G/11.3G [00:42<00:57, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  42%|████▏     | 4.78G/11.3G [00:42<00:56, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  42%|████▏     | 4.79G/11.3G [00:42<00:56, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  43%|████▎     | 4.80G/11.3G [00:42<00:57, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  43%|████▎     | 4.81G/11.3G [00:43<00:57, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  43%|████▎     | 4.82G/11.3G [00:43<00:56, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  43%|████▎     | 4.84G/11.3G [00:43<00:56, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  43%|████▎     | 4.85G/11.3G [00:43<00:56, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  43%|████▎     | 4.86G/11.3G [00:43<00:56, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  43%|████▎     | 4.87G/11.3G [00:43<00:56, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  43%|████▎     | 4.88G/11.3G [00:43<00:55, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  43%|████▎     | 4.89G/11.3G [00:43<00:55, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  44%|████▎     | 4.90G/11.3G [00:43<00:55, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  44%|████▎     | 4.92G/11.3G [00:43<00:55, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  44%|████▎     | 4.93G/11.3G [00:44<00:55, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  44%|████▍     | 4.94G/11.3G [00:44<00:55, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  44%|████▍     | 4.95G/11.3G [00:44<00:55, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  44%|████▍     | 4.96G/11.3G [00:44<00:55, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  44%|████▍     | 4.97G/11.3G [00:44<00:54, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  44%|████▍     | 4.99G/11.3G [00:44<00:54, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  44%|████▍     | 5.00G/11.3G [00:44<00:54, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  44%|████▍     | 5.01G/11.3G [00:44<01:04, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  45%|████▍     | 5.02G/11.3G [00:44<01:01, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  45%|████▍     | 5.03G/11.3G [00:45<00:59, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  45%|████▍     | 5.04G/11.3G [00:45<00:58, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  45%|████▍     | 5.05G/11.3G [00:45<00:56, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  45%|████▍     | 5.07G/11.3G [00:45<00:55, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  45%|████▌     | 5.08G/11.3G [00:45<00:55, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  45%|████▌     | 5.09G/11.3G [00:45<00:54, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  45%|████▌     | 5.10G/11.3G [00:45<00:54, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  45%|████▌     | 5.11G/11.3G [00:45<00:54, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  45%|████▌     | 5.12G/11.3G [00:45<00:53, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  46%|████▌     | 5.13G/11.3G [00:45<00:53, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  46%|████▌     | 5.15G/11.3G [00:46<00:53, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  46%|████▌     | 5.16G/11.3G [00:46<00:53, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  46%|████▌     | 5.17G/11.3G [00:46<00:53, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  46%|████▌     | 5.18G/11.3G [00:46<00:53, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  46%|████▌     | 5.19G/11.3G [00:46<00:53, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  46%|████▌     | 5.20G/11.3G [00:46<00:52, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  46%|████▋     | 5.21G/11.3G [00:46<00:52, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  46%|████▋     | 5.23G/11.3G [00:46<00:52, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  46%|████▋     | 5.24G/11.3G [00:46<00:52, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 5.25G/11.3G [00:46<00:52, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 5.26G/11.3G [00:47<00:52, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 5.27G/11.3G [00:47<00:52, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 5.28G/11.3G [00:47<00:52, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 5.30G/11.3G [00:47<00:52, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 5.31G/11.3G [00:47<00:51, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 5.32G/11.3G [00:47<00:51, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 5.33G/11.3G [00:47<00:51, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 5.34G/11.3G [00:47<00:51, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  47%|████▋     | 5.35G/11.3G [00:47<00:51, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  48%|████▊     | 5.36G/11.3G [00:47<00:51, 124MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  48%|████▊     | 5.38G/11.3G [00:48<00:51, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  48%|████▊     | 5.39G/11.3G [00:48<00:51, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  48%|████▊     | 5.40G/11.3G [00:48<01:13, 85.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  48%|████▊     | 5.41G/11.3G [00:48<01:06, 94.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  48%|████▊     | 5.42G/11.3G [00:48<01:03, 98.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  48%|████▊     | 5.43G/11.3G [00:48<01:01, 101MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  48%|████▊     | 5.44G/11.3G [00:48<01:00, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  48%|████▊     | 5.45G/11.3G [00:48<00:59, 106MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  48%|████▊     | 5.46G/11.3G [00:48<00:58, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▊     | 5.47G/11.3G [00:49<00:57, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▊     | 5.48G/11.3G [00:49<00:57, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▊     | 5.49G/11.3G [00:49<00:56, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▉     | 5.50G/11.3G [00:49<00:56, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▉     | 5.51G/11.3G [00:49<00:56, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▉     | 5.52G/11.3G [00:49<00:55, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▉     | 5.53G/11.3G [00:49<00:55, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▉     | 5.54G/11.3G [00:49<00:55, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▉     | 5.55G/11.3G [00:49<00:55, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▉     | 5.56G/11.3G [00:49<00:55, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  49%|████▉     | 5.57G/11.3G [00:50<00:55, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  50%|████▉     | 5.59G/11.3G [00:50<00:55, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  50%|████▉     | 5.60G/11.3G [00:50<00:53, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  50%|████▉     | 5.61G/11.3G [00:50<00:52, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  50%|████▉     | 5.62G/11.3G [00:50<00:51, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  50%|████▉     | 5.63G/11.3G [00:50<00:50, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  50%|█████     | 5.64G/11.3G [00:50<00:49, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  50%|█████     | 5.65G/11.3G [00:50<00:50, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  50%|█████     | 5.66G/11.3G [00:50<00:51, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  50%|█████     | 5.68G/11.3G [00:50<00:52, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  50%|█████     | 5.69G/11.3G [00:51<00:52, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  51%|█████     | 5.70G/11.3G [00:51<00:53, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  51%|█████     | 5.71G/11.3G [00:51<00:53, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  51%|█████     | 5.72G/11.3G [00:51<00:53, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  51%|█████     | 5.73G/11.3G [00:51<00:53, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  51%|█████     | 5.74G/11.3G [00:51<00:52, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  51%|█████     | 5.75G/11.3G [00:51<00:51, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  51%|█████     | 5.76G/11.3G [00:51<00:50, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  51%|█████     | 5.77G/11.3G [00:51<00:49, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  51%|█████▏    | 5.78G/11.3G [00:51<00:49, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  51%|█████▏    | 5.80G/11.3G [00:52<01:15, 77.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  52%|█████▏    | 5.81G/11.3G [00:52<01:07, 87.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  52%|█████▏    | 5.82G/11.3G [00:52<01:01, 95.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  52%|█████▏    | 5.83G/11.3G [00:52<00:57, 102MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  52%|█████▏    | 5.84G/11.3G [00:52<00:54, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  52%|█████▏    | 5.85G/11.3G [00:52<00:52, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  52%|█████▏    | 5.86G/11.3G [00:52<00:50, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  52%|█████▏    | 5.88G/11.3G [00:52<00:49, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  52%|█████▏    | 5.89G/11.3G [00:53<00:48, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  52%|█████▏    | 5.90G/11.3G [00:53<00:48, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  52%|█████▏    | 5.91G/11.3G [00:53<00:48, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  53%|█████▎    | 5.92G/11.3G [00:53<00:47, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  53%|█████▎    | 5.93G/11.3G [00:53<00:47, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  53%|█████▎    | 5.94G/11.3G [00:53<00:46, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  53%|█████▎    | 5.96G/11.3G [00:53<00:46, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  53%|█████▎    | 5.97G/11.3G [00:53<00:46, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  53%|█████▎    | 5.98G/11.3G [00:53<00:46, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  53%|█████▎    | 5.99G/11.3G [00:53<00:46, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  53%|█████▎    | 6.00G/11.3G [00:54<00:46, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  53%|█████▎    | 6.01G/11.3G [00:54<00:46, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  53%|█████▎    | 6.02G/11.3G [00:54<00:45, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▎    | 6.04G/11.3G [00:54<00:45, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▎    | 6.05G/11.3G [00:54<00:45, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▎    | 6.06G/11.3G [00:54<00:45, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▍    | 6.07G/11.3G [00:54<00:45, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▍    | 6.08G/11.3G [00:54<00:45, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▍    | 6.09G/11.3G [00:54<00:45, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▍    | 6.10G/11.3G [00:54<00:45, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▍    | 6.12G/11.3G [00:55<00:45, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▍    | 6.13G/11.3G [00:55<00:45, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  54%|█████▍    | 6.14G/11.3G [00:55<00:45, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  55%|█████▍    | 6.15G/11.3G [00:55<00:44, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  55%|█████▍    | 6.16G/11.3G [00:55<00:44, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  55%|█████▍    | 6.17G/11.3G [00:55<00:44, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  55%|█████▍    | 6.18G/11.3G [00:55<00:44, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  55%|█████▍    | 6.20G/11.3G [00:55<00:44, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  55%|█████▌    | 6.21G/11.3G [00:55<00:44, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  55%|█████▌    | 6.22G/11.3G [00:55<00:44, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  55%|█████▌    | 6.23G/11.3G [00:56<00:44, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  55%|█████▌    | 6.24G/11.3G [00:56<00:46, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  55%|█████▌    | 6.25G/11.3G [00:56<00:47, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▌    | 6.26G/11.3G [00:56<00:47, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▌    | 6.27G/11.3G [00:56<00:47, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▌    | 6.28G/11.3G [00:56<00:48, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▌    | 6.29G/11.3G [00:56<00:48, 111MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▌    | 6.31G/11.3G [00:56<00:48, 111MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▌    | 6.32G/11.3G [00:56<00:48, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▌    | 6.33G/11.3G [00:57<00:48, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▌    | 6.34G/11.3G [00:57<01:04, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▋    | 6.35G/11.3G [00:57<00:59, 88.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▋    | 6.36G/11.3G [00:57<00:56, 93.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  56%|█████▋    | 6.37G/11.3G [00:57<00:53, 98.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 6.38G/11.3G [00:57<00:52, 101MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 6.39G/11.3G [00:57<00:50, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 6.40G/11.3G [00:57<00:49, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 6.41G/11.3G [00:57<00:49, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 6.42G/11.3G [00:58<00:48, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 6.43G/11.3G [00:58<00:48, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 6.44G/11.3G [00:58<00:47, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 6.45G/11.3G [00:58<00:47, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 6.46G/11.3G [00:58<00:47, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 6.47G/11.3G [00:58<00:47, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  57%|█████▋    | 6.48G/11.3G [00:58<00:47, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  58%|█████▊    | 6.49G/11.3G [00:58<00:46, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  58%|█████▊    | 6.50G/11.3G [00:58<00:46, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  58%|█████▊    | 6.51G/11.3G [00:58<00:46, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  58%|█████▊    | 6.52G/11.3G [00:59<00:46, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  58%|█████▊    | 6.53G/11.3G [00:59<00:46, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  58%|█████▊    | 6.54G/11.3G [00:59<00:45, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  58%|█████▊    | 6.55G/11.3G [00:59<00:44, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  58%|█████▊    | 6.56G/11.3G [00:59<00:43, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  58%|█████▊    | 6.57G/11.3G [00:59<00:43, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  58%|█████▊    | 6.59G/11.3G [00:59<00:42, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▊    | 6.60G/11.3G [00:59<00:42, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▊    | 6.61G/11.3G [00:59<00:42, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▊    | 6.62G/11.3G [00:59<00:41, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▉    | 6.63G/11.3G [01:00<00:41, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▉    | 6.64G/11.3G [01:00<00:41, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▉    | 6.65G/11.3G [01:00<00:41, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▉    | 6.66G/11.3G [01:00<00:41, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▉    | 6.68G/11.3G [01:00<00:41, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▉    | 6.69G/11.3G [01:00<00:43, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▉    | 6.70G/11.3G [01:00<00:44, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  59%|█████▉    | 6.71G/11.3G [01:00<00:44, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|█████▉    | 6.72G/11.3G [01:00<00:44, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|█████▉    | 6.73G/11.3G [01:00<00:45, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|█████▉    | 6.74G/11.3G [01:01<00:44, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|█████▉    | 6.75G/11.3G [01:01<00:45, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|█████▉    | 6.76G/11.3G [01:01<00:46, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|██████    | 6.77G/11.3G [01:01<00:46, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|██████    | 6.78G/11.3G [01:01<00:47, 103MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|██████    | 6.79G/11.3G [01:01<00:47, 102MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|██████    | 6.80G/11.3G [01:01<00:47, 101MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|██████    | 6.81G/11.3G [01:01<00:47, 101MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  60%|██████    | 6.82G/11.3G [01:01<00:46, 102MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  61%|██████    | 6.83G/11.3G [01:01<00:44, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  61%|██████    | 6.84G/11.3G [01:02<00:42, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  61%|██████    | 6.85G/11.3G [01:02<00:41, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  61%|██████    | 6.86G/11.3G [01:02<01:02, 75.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  61%|██████    | 6.87G/11.3G [01:02<00:55, 85.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  61%|██████    | 6.88G/11.3G [01:02<00:49, 94.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  61%|██████    | 6.90G/11.3G [01:02<00:46, 102MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  61%|██████▏   | 6.91G/11.3G [01:02<00:43, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  61%|██████▏   | 6.92G/11.3G [01:02<00:42, 111MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  61%|██████▏   | 6.93G/11.3G [01:03<00:40, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 6.94G/11.3G [01:03<00:39, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 6.95G/11.3G [01:03<00:39, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 6.96G/11.3G [01:03<00:38, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 6.97G/11.3G [01:03<00:38, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 6.99G/11.3G [01:03<00:38, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 7.00G/11.3G [01:03<00:38, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 7.01G/11.3G [01:03<00:39, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 7.02G/11.3G [01:03<00:39, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 7.03G/11.3G [01:03<00:40, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  62%|██████▏   | 7.04G/11.3G [01:04<00:40, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  63%|██████▎   | 7.05G/11.3G [01:04<00:40, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  63%|██████▎   | 7.06G/11.3G [01:04<00:40, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  63%|██████▎   | 7.07G/11.3G [01:04<00:40, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  63%|██████▎   | 7.08G/11.3G [01:04<00:40, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  63%|██████▎   | 7.09G/11.3G [01:04<00:40, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  63%|██████▎   | 7.10G/11.3G [01:04<00:39, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  63%|██████▎   | 7.11G/11.3G [01:04<00:39, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  63%|██████▎   | 7.12G/11.3G [01:04<00:47, 93.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  63%|██████▎   | 7.13G/11.3G [01:05<01:02, 70.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  63%|██████▎   | 7.14G/11.3G [01:05<00:54, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  63%|██████▎   | 7.16G/11.3G [01:05<00:48, 91.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▎   | 7.17G/11.3G [01:05<00:44, 99.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▎   | 7.18G/11.3G [01:05<00:41, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▍   | 7.19G/11.3G [01:05<00:39, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▍   | 7.20G/11.3G [01:05<00:38, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▍   | 7.21G/11.3G [01:05<00:37, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▍   | 7.22G/11.3G [01:05<00:36, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▍   | 7.24G/11.3G [01:06<00:36, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▍   | 7.25G/11.3G [01:06<00:35, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▍   | 7.26G/11.3G [01:06<00:35, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  64%|██████▍   | 7.27G/11.3G [01:06<00:57, 74.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▍   | 7.28G/11.3G [01:06<00:50, 84.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▍   | 7.29G/11.3G [01:06<00:55, 77.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▍   | 7.30G/11.3G [01:07<00:56, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▍   | 7.31G/11.3G [01:07<00:49, 86.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▍   | 7.32G/11.3G [01:07<00:44, 94.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▌   | 7.34G/11.3G [01:07<00:41, 102MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▌   | 7.35G/11.3G [01:07<00:39, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▌   | 7.36G/11.3G [01:07<00:37, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▌   | 7.37G/11.3G [01:07<00:36, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  65%|██████▌   | 7.38G/11.3G [01:07<00:35, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  66%|██████▌   | 7.39G/11.3G [01:07<00:35, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  66%|██████▌   | 7.40G/11.3G [01:07<00:34, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  66%|██████▌   | 7.42G/11.3G [01:08<00:34, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  66%|██████▌   | 7.43G/11.3G [01:08<00:33, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  66%|██████▌   | 7.44G/11.3G [01:08<01:05, 62.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  66%|██████▌   | 7.45G/11.3G [01:08<01:08, 60.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  66%|██████▌   | 7.46G/11.3G [01:08<00:57, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  66%|██████▋   | 7.47G/11.3G [01:08<00:49, 81.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  66%|██████▋   | 7.48G/11.3G [01:09<00:44, 91.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  66%|██████▋   | 7.49G/11.3G [01:09<00:41, 99.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 7.50G/11.3G [01:09<00:38, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 7.52G/11.3G [01:09<00:36, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 7.53G/11.3G [01:09<00:35, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 7.54G/11.3G [01:09<00:34, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 7.55G/11.3G [01:09<00:33, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 7.56G/11.3G [01:09<00:33, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 7.57G/11.3G [01:09<00:32, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 7.58G/11.3G [01:10<00:48, 81.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 7.59G/11.3G [01:10<01:04, 61.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  67%|██████▋   | 7.60G/11.3G [01:10<01:02, 63.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  68%|██████▊   | 7.61G/11.3G [01:10<00:52, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  68%|██████▊   | 7.62G/11.3G [01:10<00:45, 86.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  68%|██████▊   | 7.64G/11.3G [01:10<00:41, 95.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  68%|██████▊   | 7.65G/11.3G [01:10<00:38, 102MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  68%|██████▊   | 7.66G/11.3G [01:10<00:36, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  68%|██████▊   | 7.67G/11.3G [01:11<00:34, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  68%|██████▊   | 7.68G/11.3G [01:11<00:33, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  68%|██████▊   | 7.69G/11.3G [01:11<00:32, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  68%|██████▊   | 7.70G/11.3G [01:11<00:32, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  68%|██████▊   | 7.72G/11.3G [01:11<00:31, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▊   | 7.73G/11.3G [01:11<00:31, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▊   | 7.74G/11.3G [01:11<00:31, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▊   | 7.75G/11.3G [01:11<00:43, 87.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▉   | 7.76G/11.3G [01:12<01:09, 54.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▉   | 7.77G/11.3G [01:12<00:57, 65.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▉   | 7.78G/11.3G [01:12<00:49, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▉   | 7.79G/11.3G [01:12<01:02, 60.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▉   | 7.80G/11.3G [01:12<00:51, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▉   | 7.81G/11.3G [01:12<00:45, 82.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  69%|██████▉   | 7.83G/11.3G [01:13<00:40, 91.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|██████▉   | 7.84G/11.3G [01:13<00:37, 99.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|██████▉   | 7.85G/11.3G [01:13<00:34, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|██████▉   | 7.86G/11.3G [01:13<00:33, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|██████▉   | 7.87G/11.3G [01:13<00:43, 84.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|██████▉   | 7.88G/11.3G [01:13<00:53, 68.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|██████▉   | 7.89G/11.3G [01:13<00:45, 79.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|███████   | 7.90G/11.3G [01:13<00:40, 88.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|███████   | 7.91G/11.3G [01:14<00:37, 97.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|███████   | 7.93G/11.3G [01:14<00:34, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|███████   | 7.94G/11.3G [01:14<00:32, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  70%|███████   | 7.95G/11.3G [01:14<00:31, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  71%|███████   | 7.96G/11.3G [01:14<00:30, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  71%|███████   | 7.97G/11.3G [01:14<00:30, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  71%|███████   | 7.98G/11.3G [01:14<00:29, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  71%|███████   | 7.99G/11.3G [01:14<00:29, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  71%|███████   | 8.01G/11.3G [01:14<00:29, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  71%|███████   | 8.02G/11.3G [01:15<00:50, 68.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  71%|███████   | 8.03G/11.3G [01:15<00:49, 70.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  71%|███████▏  | 8.03G/11.3G [01:15<00:51, 67.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  71%|███████▏  | 8.05G/11.3G [01:15<00:43, 79.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  71%|███████▏  | 8.06G/11.3G [01:15<00:38, 89.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 8.07G/11.3G [01:15<00:35, 97.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 8.08G/11.3G [01:15<00:32, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 8.09G/11.3G [01:16<00:31, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 8.10G/11.3G [01:16<00:30, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 8.11G/11.3G [01:16<00:29, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 8.13G/11.3G [01:16<00:28, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 8.14G/11.3G [01:16<00:28, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 8.15G/11.3G [01:16<00:27, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 8.16G/11.3G [01:16<00:27, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  72%|███████▏  | 8.17G/11.3G [01:16<00:36, 91.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  73%|███████▎  | 8.18G/11.3G [01:17<00:58, 57.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  73%|███████▎  | 8.19G/11.3G [01:17<00:57, 58.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  73%|███████▎  | 8.20G/11.3G [01:17<00:47, 70.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  73%|███████▎  | 8.21G/11.3G [01:17<00:40, 81.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  73%|███████▎  | 8.22G/11.3G [01:17<00:36, 90.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  73%|███████▎  | 8.23G/11.3G [01:17<00:32, 99.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  73%|███████▎  | 8.25G/11.3G [01:17<00:30, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  73%|███████▎  | 8.26G/11.3G [01:17<00:29, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  73%|███████▎  | 8.27G/11.3G [01:18<00:28, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  73%|███████▎  | 8.28G/11.3G [01:18<00:27, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  74%|███████▎  | 8.29G/11.3G [01:18<00:27, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  74%|███████▎  | 8.30G/11.3G [01:18<00:26, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  74%|███████▎  | 8.31G/11.3G [01:18<00:26, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  74%|███████▍  | 8.33G/11.3G [01:18<00:54, 57.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  74%|███████▍  | 8.33G/11.3G [01:19<00:51, 61.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  74%|███████▍  | 8.35G/11.3G [01:19<00:43, 72.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  74%|███████▍  | 8.36G/11.3G [01:19<00:37, 83.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  74%|███████▍  | 8.37G/11.3G [01:19<00:33, 92.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  74%|███████▍  | 8.38G/11.3G [01:19<00:31, 99.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  74%|███████▍  | 8.39G/11.3G [01:19<00:29, 106MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▍  | 8.40G/11.3G [01:19<00:27, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▍  | 8.41G/11.3G [01:19<00:26, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▍  | 8.43G/11.3G [01:19<00:26, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▍  | 8.44G/11.3G [01:19<00:25, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▍  | 8.45G/11.3G [01:20<00:25, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▌  | 8.46G/11.3G [01:20<00:25, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▌  | 8.47G/11.3G [01:20<00:49, 60.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▌  | 8.48G/11.3G [01:20<00:41, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▌  | 8.49G/11.3G [01:20<00:36, 81.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  75%|███████▌  | 8.51G/11.3G [01:20<00:32, 90.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  76%|███████▌  | 8.52G/11.3G [01:20<00:30, 98.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  76%|███████▌  | 8.53G/11.3G [01:21<00:28, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  76%|███████▌  | 8.54G/11.3G [01:21<00:26, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  76%|███████▌  | 8.55G/11.3G [01:21<00:25, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  76%|███████▌  | 8.56G/11.3G [01:21<00:25, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  76%|███████▌  | 8.57G/11.3G [01:21<00:24, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  76%|███████▌  | 8.59G/11.3G [01:21<00:24, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  76%|███████▋  | 8.60G/11.3G [01:21<00:23, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  76%|███████▋  | 8.61G/11.3G [01:21<00:23, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  76%|███████▋  | 8.62G/11.3G [01:21<00:23, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  77%|███████▋  | 8.63G/11.3G [01:21<00:23, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  77%|███████▋  | 8.64G/11.3G [01:22<00:34, 81.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  77%|███████▋  | 8.65G/11.3G [01:22<00:31, 90.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  77%|███████▋  | 8.67G/11.3G [01:22<00:28, 98.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  77%|███████▋  | 8.68G/11.3G [01:22<00:26, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  77%|███████▋  | 8.69G/11.3G [01:22<00:25, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  77%|███████▋  | 8.70G/11.3G [01:22<00:24, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  77%|███████▋  | 8.71G/11.3G [01:22<00:23, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  77%|███████▋  | 8.72G/11.3G [01:22<00:23, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  77%|███████▋  | 8.73G/11.3G [01:23<00:22, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 8.75G/11.3G [01:23<00:22, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 8.76G/11.3G [01:23<00:22, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 8.77G/11.3G [01:23<00:22, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 8.78G/11.3G [01:23<00:21, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 8.79G/11.3G [01:23<00:21, 123MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 8.80G/11.3G [01:23<00:44, 59.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 8.81G/11.3G [01:24<00:37, 69.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 8.83G/11.3G [01:24<00:32, 80.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 8.84G/11.3G [01:24<00:30, 86.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  78%|███████▊  | 8.85G/11.3G [01:24<00:28, 90.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▊  | 8.86G/11.3G [01:24<00:27, 94.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▊  | 8.87G/11.3G [01:24<00:26, 97.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▊  | 8.88G/11.3G [01:24<00:25, 100MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▉  | 8.89G/11.3G [01:24<00:25, 102MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▉  | 8.90G/11.3G [01:24<00:24, 103MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▉  | 8.91G/11.3G [01:24<00:24, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▉  | 8.92G/11.3G [01:25<00:24, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▉  | 8.93G/11.3G [01:25<00:42, 59.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▉  | 8.94G/11.3G [01:25<00:37, 67.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▉  | 8.95G/11.3G [01:25<00:33, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  79%|███████▉  | 8.96G/11.3G [01:25<00:30, 82.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|███████▉  | 8.97G/11.3G [01:25<00:28, 88.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|███████▉  | 8.98G/11.3G [01:25<00:26, 93.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|███████▉  | 8.99G/11.3G [01:26<00:25, 96.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|███████▉  | 8.99G/11.3G [01:26<00:24, 99.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|███████▉  | 9.00G/11.3G [01:26<00:24, 101MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|███████▉  | 9.01G/11.3G [01:26<00:23, 103MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|████████  | 9.02G/11.3G [01:26<00:23, 103MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|████████  | 9.03G/11.3G [01:26<00:23, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|████████  | 9.04G/11.3G [01:26<00:22, 105MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|████████  | 9.05G/11.3G [01:26<00:22, 106MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|████████  | 9.06G/11.3G [01:26<00:22, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  80%|████████  | 9.07G/11.3G [01:26<00:21, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  81%|████████  | 9.09G/11.3G [01:27<00:21, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  81%|████████  | 9.10G/11.3G [01:27<00:21, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  81%|████████  | 9.11G/11.3G [01:27<00:21, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  81%|████████  | 9.12G/11.3G [01:27<00:21, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  81%|████████  | 9.13G/11.3G [01:27<00:21, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  81%|████████  | 9.14G/11.3G [01:27<00:21, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  81%|████████  | 9.15G/11.3G [01:27<00:20, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  81%|████████  | 9.16G/11.3G [01:27<00:20, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  81%|████████▏ | 9.17G/11.3G [01:27<00:20, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  81%|████████▏ | 9.18G/11.3G [01:27<00:20, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  81%|████████▏ | 9.19G/11.3G [01:28<00:20, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 9.20G/11.3G [01:28<00:20, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 9.21G/11.3G [01:28<00:20, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 9.22G/11.3G [01:28<00:20, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 9.23G/11.3G [01:28<00:20, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 9.24G/11.3G [01:28<00:20, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 9.25G/11.3G [01:28<00:19, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 9.26G/11.3G [01:28<00:19, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 9.27G/11.3G [01:28<00:19, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 9.28G/11.3G [01:29<00:24, 86.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 9.29G/11.3G [01:29<00:33, 63.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  82%|████████▏ | 9.30G/11.3G [01:29<00:28, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 9.31G/11.3G [01:29<00:24, 84.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 9.32G/11.3G [01:29<00:22, 92.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 9.33G/11.3G [01:29<00:21, 99.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 9.34G/11.3G [01:29<00:19, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 9.35G/11.3G [01:29<00:19, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 9.36G/11.3G [01:30<00:18, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 9.38G/11.3G [01:30<00:18, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 9.39G/11.3G [01:30<00:17, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 9.40G/11.3G [01:30<00:17, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  83%|████████▎ | 9.41G/11.3G [01:30<00:17, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  84%|████████▎ | 9.42G/11.3G [01:30<00:17, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  84%|████████▎ | 9.43G/11.3G [01:30<00:17, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  84%|████████▎ | 9.44G/11.3G [01:30<00:17, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  84%|████████▍ | 9.45G/11.3G [01:30<00:17, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  84%|████████▍ | 9.46G/11.3G [01:30<00:16, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  84%|████████▍ | 9.47G/11.3G [01:31<00:16, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  84%|████████▍ | 9.48G/11.3G [01:31<00:16, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  84%|████████▍ | 9.49G/11.3G [01:31<00:16, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  84%|████████▍ | 9.51G/11.3G [01:31<00:25, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  84%|████████▍ | 9.52G/11.3G [01:31<00:22, 84.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▍ | 9.53G/11.3G [01:31<00:20, 91.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▍ | 9.54G/11.3G [01:31<00:18, 98.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▍ | 9.55G/11.3G [01:31<00:17, 103MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▍ | 9.56G/11.3G [01:32<00:17, 107MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▍ | 9.57G/11.3G [01:32<00:16, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▍ | 9.58G/11.3G [01:32<00:16, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▌ | 9.59G/11.3G [01:32<00:15, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▌ | 9.60G/11.3G [01:32<00:15, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▌ | 9.62G/11.3G [01:32<00:15, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▌ | 9.63G/11.3G [01:32<00:15, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  85%|████████▌ | 9.64G/11.3G [01:32<00:14, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▌ | 9.65G/11.3G [01:32<00:14, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▌ | 9.66G/11.3G [01:32<00:14, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▌ | 9.67G/11.3G [01:33<00:27, 63.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▌ | 9.68G/11.3G [01:33<00:23, 73.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▌ | 9.69G/11.3G [01:33<00:20, 82.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▌ | 9.70G/11.3G [01:33<00:18, 91.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▌ | 9.72G/11.3G [01:33<00:16, 98.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▋ | 9.73G/11.3G [01:33<00:15, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▋ | 9.74G/11.3G [01:33<00:15, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  86%|████████▋ | 9.75G/11.3G [01:34<00:14, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  87%|████████▋ | 9.76G/11.3G [01:34<00:14, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  87%|████████▋ | 9.77G/11.3G [01:34<00:13, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  87%|████████▋ | 9.78G/11.3G [01:34<00:13, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  87%|████████▋ | 9.79G/11.3G [01:34<00:13, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  87%|████████▋ | 9.81G/11.3G [01:34<00:13, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  87%|████████▋ | 9.82G/11.3G [01:34<00:13, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  87%|████████▋ | 9.83G/11.3G [01:34<00:12, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  87%|████████▋ | 9.84G/11.3G [01:34<00:12, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  87%|████████▋ | 9.85G/11.3G [01:34<00:12, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  87%|████████▋ | 9.86G/11.3G [01:35<00:12, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  88%|████████▊ | 9.87G/11.3G [01:35<00:12, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  88%|████████▊ | 9.88G/11.3G [01:35<00:12, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  88%|████████▊ | 9.90G/11.3G [01:35<00:12, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  88%|████████▊ | 9.91G/11.3G [01:35<00:12, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  88%|████████▊ | 9.92G/11.3G [01:35<00:12, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  88%|████████▊ | 9.93G/11.3G [01:35<00:11, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  88%|████████▊ | 9.94G/11.3G [01:35<00:13, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  88%|████████▊ | 9.95G/11.3G [01:36<00:23, 61.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  88%|████████▊ | 9.96G/11.3G [01:36<00:19, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  88%|████████▊ | 9.97G/11.3G [01:36<00:17, 81.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  89%|████████▊ | 9.98G/11.3G [01:36<00:15, 90.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  89%|████████▊ | 10.0G/11.3G [01:36<00:13, 98.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  89%|████████▉ | 10.0G/11.3G [01:36<00:13, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  89%|████████▉ | 10.0G/11.3G [01:36<00:12, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  89%|████████▉ | 10.0G/11.3G [01:36<00:11, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  89%|████████▉ | 10.0G/11.3G [01:36<00:11, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  89%|████████▉ | 10.1G/11.3G [01:37<00:11, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  89%|████████▉ | 10.1G/11.3G [01:37<00:11, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  89%|████████▉ | 10.1G/11.3G [01:37<00:10, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  89%|████████▉ | 10.1G/11.3G [01:37<00:10, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  90%|████████▉ | 10.1G/11.3G [01:37<00:10, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  90%|████████▉ | 10.1G/11.3G [01:37<00:10, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  90%|████████▉ | 10.1G/11.3G [01:37<00:10, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  90%|████████▉ | 10.1G/11.3G [01:37<00:10, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  90%|████████▉ | 10.1G/11.3G [01:37<00:10, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  90%|█████████ | 10.2G/11.3G [01:37<00:09, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  90%|█████████ | 10.2G/11.3G [01:38<00:09, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  90%|█████████ | 10.2G/11.3G [01:38<00:09, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  90%|█████████ | 10.2G/11.3G [01:38<00:09, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  90%|█████████ | 10.2G/11.3G [01:38<00:09, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  91%|█████████ | 10.2G/11.3G [01:38<00:11, 95.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  91%|█████████ | 10.2G/11.3G [01:38<00:14, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  91%|█████████ | 10.2G/11.3G [01:38<00:13, 86.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  91%|█████████ | 10.2G/11.3G [01:38<00:14, 78.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  91%|█████████ | 10.3G/11.3G [01:39<00:12, 87.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  91%|█████████ | 10.3G/11.3G [01:39<00:11, 95.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  91%|█████████ | 10.3G/11.3G [01:39<00:10, 102MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  91%|█████████ | 10.3G/11.3G [01:39<00:09, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  91%|█████████▏| 10.3G/11.3G [01:39<00:09, 111MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  91%|█████████▏| 10.3G/11.3G [01:39<00:09, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  92%|█████████▏| 10.3G/11.3G [01:39<00:08, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  92%|█████████▏| 10.3G/11.3G [01:39<00:08, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  92%|█████████▏| 10.3G/11.3G [01:39<00:08, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  92%|█████████▏| 10.4G/11.3G [01:39<00:08, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  92%|█████████▏| 10.4G/11.3G [01:40<00:08, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  92%|█████████▏| 10.4G/11.3G [01:40<00:09, 101MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  92%|█████████▏| 10.4G/11.3G [01:40<00:09, 106MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  92%|█████████▏| 10.4G/11.3G [01:40<00:11, 82.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  92%|█████████▏| 10.4G/11.3G [01:40<00:10, 91.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  92%|█████████▏| 10.4G/11.3G [01:40<00:09, 98.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 10.4G/11.3G [01:40<00:08, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 10.4G/11.3G [01:40<00:08, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 10.5G/11.3G [01:41<00:07, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 10.5G/11.3G [01:41<00:07, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 10.5G/11.3G [01:41<00:07, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 10.5G/11.3G [01:41<00:07, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 10.5G/11.3G [01:41<00:07, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 10.5G/11.3G [01:41<00:06, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 10.5G/11.3G [01:41<00:06, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  93%|█████████▎| 10.5G/11.3G [01:41<00:06, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  94%|█████████▎| 10.5G/11.3G [01:41<00:06, 122MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  94%|█████████▎| 10.6G/11.3G [01:42<00:10, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  94%|█████████▎| 10.6G/11.3G [01:42<00:08, 86.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  94%|█████████▍| 10.6G/11.3G [01:42<00:07, 94.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  94%|█████████▍| 10.6G/11.3G [01:42<00:07, 101MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  94%|█████████▍| 10.6G/11.3G [01:42<00:06, 106MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  94%|█████████▍| 10.6G/11.3G [01:42<00:06, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  94%|█████████▍| 10.6G/11.3G [01:42<00:06, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  94%|█████████▍| 10.6G/11.3G [01:42<00:05, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  94%|█████████▍| 10.6G/11.3G [01:42<00:05, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  95%|█████████▍| 10.7G/11.3G [01:43<00:05, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  95%|█████████▍| 10.7G/11.3G [01:43<00:05, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  95%|█████████▍| 10.7G/11.3G [01:43<00:05, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  95%|█████████▍| 10.7G/11.3G [01:43<00:05, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  95%|█████████▍| 10.7G/11.3G [01:43<00:05, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  95%|█████████▌| 10.7G/11.3G [01:43<00:05, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  95%|█████████▌| 10.7G/11.3G [01:43<00:04, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  95%|█████████▌| 10.7G/11.3G [01:43<00:04, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  95%|█████████▌| 10.7G/11.3G [01:43<00:04, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  95%|█████████▌| 10.8G/11.3G [01:43<00:04, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▌| 10.8G/11.3G [01:44<00:04, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▌| 10.8G/11.3G [01:44<00:04, 121MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▌| 10.8G/11.3G [01:44<00:04, 120MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▌| 10.8G/11.3G [01:44<00:04, 119MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▌| 10.8G/11.3G [01:44<00:04, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▌| 10.8G/11.3G [01:44<00:04, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▌| 10.8G/11.3G [01:44<00:03, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▌| 10.8G/11.3G [01:44<00:03, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▋| 10.9G/11.3G [01:44<00:03, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▋| 10.9G/11.3G [01:44<00:03, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  96%|█████████▋| 10.9G/11.3G [01:45<00:03, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  97%|█████████▋| 10.9G/11.3G [01:45<00:03, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  97%|█████████▋| 10.9G/11.3G [01:45<00:03, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  97%|█████████▋| 10.9G/11.3G [01:45<00:03, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  97%|█████████▋| 10.9G/11.3G [01:45<00:03, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  97%|█████████▋| 10.9G/11.3G [01:45<00:03, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  97%|█████████▋| 10.9G/11.3G [01:45<00:03, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  97%|█████████▋| 11.0G/11.3G [01:45<00:02, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  97%|█████████▋| 11.0G/11.3G [01:45<00:02, 118MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  97%|█████████▋| 11.0G/11.3G [01:46<00:05, 58.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  97%|█████████▋| 11.0G/11.3G [01:46<00:05, 61.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  98%|█████████▊| 11.0G/11.3G [01:46<00:04, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  98%|█████████▊| 11.0G/11.3G [01:46<00:03, 81.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  98%|█████████▊| 11.0G/11.3G [01:46<00:03, 89.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  98%|█████████▊| 11.0G/11.3G [01:46<00:02, 96.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  98%|█████████▊| 11.0G/11.3G [01:46<00:02, 102MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  98%|█████████▊| 11.1G/11.3G [01:47<00:02, 106MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  98%|█████████▊| 11.1G/11.3G [01:47<00:02, 109MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  98%|█████████▊| 11.1G/11.3G [01:47<00:01, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  98%|█████████▊| 11.1G/11.3G [01:47<00:01, 113MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  98%|█████████▊| 11.1G/11.3G [01:47<00:01, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▊| 11.1G/11.3G [01:47<00:01, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▊| 11.1G/11.3G [01:47<00:01, 89.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▊| 11.1G/11.3G [01:47<00:02, 77.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▉| 11.1G/11.3G [01:48<00:01, 86.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▉| 11.1G/11.3G [01:48<00:01, 93.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▉| 11.2G/11.3G [01:48<00:01, 99.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▉| 11.2G/11.3G [01:48<00:01, 104MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▉| 11.2G/11.3G [01:48<00:00, 108MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▉| 11.2G/11.3G [01:48<00:00, 110MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▉| 11.2G/11.3G [01:48<00:00, 112MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin:  99%|█████████▉| 11.2G/11.3G [01:48<00:00, 114MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin: 100%|█████████▉| 11.2G/11.3G [01:48<00:00, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin: 100%|█████████▉| 11.2G/11.3G [01:48<00:00, 115MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin: 100%|█████████▉| 11.2G/11.3G [01:49<00:00, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin: 100%|█████████▉| 11.3G/11.3G [01:49<00:00, 116MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin: 100%|█████████▉| 11.3G/11.3G [01:49<00:00, 117MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading pytorch_model.bin: 100%|██████████| 11.3G/11.3G [01:49<00:00, 111MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:44:30.311: I smdistributed/modelparallel/torch/model.py:146] [2] Bit16_Module initialized, using dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:44:31.182: I smdistributed/modelparallel/torch/model.py:146] [4] Bit16_Module initialized, using dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:44:31.325: I smdistributed/modelparallel/torch/model.py:146] [5] Bit16_Module initialized, using dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:# total parameters: 6050882784\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:44:31.329: I smdistributed/modelparallel/torch/model.py:146] [0] Bit16_Module initialized, using dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:44:31.505: I smdistributed/modelparallel/torch/model.py:146] [6] Bit16_Module initialized, using dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:44:31.546: I smdistributed/modelparallel/torch/model.py:146] [7] Bit16_Module initialized, using dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:44:32.983: I smdistributed/modelparallel/torch/model.py:146] [1] Bit16_Module initialized, using dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:44:33.073: I smdistributed/modelparallel/torch/model.py:146] [3] Bit16_Module initialized, using dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:44:56.163: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:44:56.164: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:44:58.100: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:44:58.100: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:44:59.474: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:44:59.474: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:00.846: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:00.847: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:02.219: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:02.219: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:03.591: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:03.591: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:04.963: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:04.963: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:06.333: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:06.333: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:07.703: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:07.703: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:09.074: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:09.074: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:10.445: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:10.445: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:11.817: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:11.818: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:13.188: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:13.188: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:14.558: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:14.558: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:15.928: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:15.928: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:17.298: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:17.298: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:18.669: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:18.669: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:20.039: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:20.039: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:21.409: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:21.409: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:22.779: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:22.779: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:24.150: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:24.151: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:25.521: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:25.521: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:26.892: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:26.892: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:28.264: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:28.264: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:29.634: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:29.635: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:31.005: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:31.006: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:32.376: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:32.376: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:33.746: W smdistributed/modelparallel/torch/nn/transformer.py:158] Disabling flash attention as it only supports head sizes of 16, 32, 64, 128.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:45:33.746: W smdistributed/modelparallel/torch/nn/transformer.py:169] query_key_layer_scaling is only supported in SMP's regular attention implementation, so disabling flash_attention. You may consider training with flash_attention with bfloat16 training which does not require query_key_layer_scaling for numerical stability and might result in better performance.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:46:13.097: I smdistributed/modelparallel/torch/optimizers/optimizer.py:502] [1] Bit16_Optimizer initialized with dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:46:13.126: I smdistributed/modelparallel/torch/optimizers/optimizer.py:502] [3] Bit16_Optimizer initialized with dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:46:13.488: I smdistributed/modelparallel/torch/optimizers/optimizer.py:502] [2] Bit16_Optimizer initialized with dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:13.488: I smdistributed/modelparallel/torch/optimizers/optimizer.py:502] [0] Bit16_Optimizer initialized with dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Learning rate decay style: linear\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Creating val dataloader\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Created val dataloader\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:46:13.513: I smdistributed/modelparallel/torch/optimizers/optimizer.py:502] [7] Bit16_Optimizer initialized with dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:46:13.529: I smdistributed/modelparallel/torch/optimizers/optimizer.py:502] [4] Bit16_Optimizer initialized with dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:46:13.534: I smdistributed/modelparallel/torch/optimizers/optimizer.py:502] [5] Bit16_Optimizer initialized with dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:46:13.563: I smdistributed/modelparallel/torch/optimizers/optimizer.py:502] [6] Bit16_Optimizer initialized with dtype torch.float16\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:46:13.633: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'smdistributed.modelparallel.torch.fp16.fp16.Bit16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:46:13.653: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'smdistributed.modelparallel.torch.fp16.fp16.Bit16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:46:14.063: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'smdistributed.modelparallel.torch.fp16.fp16.Bit16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:14.063: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'smdistributed.modelparallel.torch.fp16.fp16.Bit16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:14.064: I smdistributed/modelparallel/torch/worker.py:300] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:46:14.092: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'smdistributed.modelparallel.torch.fp16.fp16.Bit16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:46:14.093: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'smdistributed.modelparallel.torch.fp16.fp16.Bit16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:46:14.102: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'smdistributed.modelparallel.torch.fp16.fp16.Bit16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:46:14.139: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'smdistributed.modelparallel.torch.fp16.fp16.Bit16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:15.602: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:15.611: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:15.997: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.001: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.005: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.008: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.012: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.016: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.019: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.023: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.027: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.030: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.034: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.038: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.041: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.045: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.048: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.052: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.055: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.059: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.063: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.067: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.070: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.074: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.077: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.081: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.084: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.088: W smdistributed/modelparallel/torch/nn/transformer.py:1908] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.478: I smdistributed/modelparallel/torch/model.py:682] Partition assignments:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.478: I smdistributed/modelparallel/torch/model.py:691] main: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.478: I smdistributed/modelparallel/torch/model.py:696] Tensor-parallel distributed modules:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.478: I smdistributed/modelparallel/torch/model.py:705] main/module/module/module\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:16.486: I smdistributed/modelparallel/torch/model.py:616] Number of parameters on partition 0 are 285. 285 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:17.084: I smdistributed/modelparallel/torch/model.py:742] Finished partitioning the model\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:19.652: I smdistributed/modelparallel/torch/model.py:751] Broadcasted parameters and buffers for partition 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:46:24.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [6] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:46:24.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [7] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:46:24.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [2] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:46:24.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [1] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:46:24.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [5] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:46:24.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [4] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:46:24.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [3] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:24.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [0] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:46:24.130: I smdistributed/modelparallel/torch/ddp_model.py:632] [2] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:46:24.130: I smdistributed/modelparallel/torch/ddp_model.py:632] [0] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:46:24.130: I smdistributed/modelparallel/torch/ddp_model.py:632] [4] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:46:24.130: I smdistributed/modelparallel/torch/ddp_model.py:632] [3] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:46:24.130: I smdistributed/modelparallel/torch/ddp_model.py:632] [1] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:46:24.130: I smdistributed/modelparallel/torch/ddp_model.py:632] [7] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:46:24.130: I smdistributed/modelparallel/torch/ddp_model.py:632] [5] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:46:24.130: I smdistributed/modelparallel/torch/ddp_model.py:632] [6] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(17s), Batch 9 Loss: 10.953125, Speed: 17.220903980608384 samples/sec, TFLOPS/GPU: 853.6200922823094\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(22s), Batch 19 Loss: 10.953125, Speed: 13.630731920338794 samples/sec, TFLOPS/GPU: 675.6594574139167\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(27s), Batch 29 Loss: 10.828125, Speed: 14.601821962243065 samples/sec, TFLOPS/GPU: 723.7952563312202\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(33s), Batch 39 Loss: 10.4453125, Speed: 14.653819010152816 samples/sec, TFLOPS/GPU: 726.3726892514148\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(38s), Batch 49 Loss: 9.84375, Speed: 14.642941398004547 samples/sec, TFLOPS/GPU: 725.8334987248156\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(44s), Batch 59 Loss: 9.25, Speed: 14.63527730732509 samples/sec, TFLOPS/GPU: 725.4535987032823\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(49s), Batch 69 Loss: 8.8984375, Speed: 14.603626791917753 samples/sec, TFLOPS/GPU: 723.8847196297321\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(55s), Batch 79 Loss: 8.4765625, Speed: 14.640366645839805 samples/sec, TFLOPS/GPU: 725.7058712679211\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(60s), Batch 89 Loss: 8.28125, Speed: 14.60563550715995 samples/sec, TFLOPS/GPU: 723.9842892976395\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:47:18.224: I smdistributed/modelparallel/torch/checkpoint.py:235] [4] Saving full checkpoint with tag fullmodel.pt to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:47:18.224: I smdistributed/modelparallel/torch/checkpoint.py:235] [5] Saving full checkpoint with tag fullmodel.pt to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:47:18.224: I smdistributed/modelparallel/torch/checkpoint.py:235] [3] Saving full checkpoint with tag fullmodel.pt to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:47:18.225: I smdistributed/modelparallel/torch/checkpoint.py:235] [1] Saving full checkpoint with tag fullmodel.pt to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:47:18.225: I smdistributed/modelparallel/torch/checkpoint.py:235] [7] Saving full checkpoint with tag fullmodel.pt to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:47:18.225: I smdistributed/modelparallel/torch/checkpoint.py:235] [2] Saving full checkpoint with tag fullmodel.pt to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:47:18.225: I smdistributed/modelparallel/torch/checkpoint.py:235] [6] Saving full checkpoint with tag fullmodel.pt to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:47:18.231: I smdistributed/modelparallel/torch/checkpoint.py:235] [0] Saving full checkpoint with tag fullmodel.pt to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:47:18.967: I smdistributed/modelparallel/torch/model.py:919] [6] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:47:18.967: W smdistributed/modelparallel/torch/model.py:929] [6] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:47:18.978: I smdistributed/modelparallel/torch/model.py:919] [7] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:47:18.978: W smdistributed/modelparallel/torch/model.py:929] [7] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:47:18.978: I smdistributed/modelparallel/torch/model.py:919] [3] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:47:18.979: W smdistributed/modelparallel/torch/model.py:929] [3] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:47:19.002: I smdistributed/modelparallel/torch/model.py:919] [2] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:47:19.003: W smdistributed/modelparallel/torch/model.py:929] [2] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:47:19.013: I smdistributed/modelparallel/torch/model.py:919] [1] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:47:19.013: W smdistributed/modelparallel/torch/model.py:929] [1] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:47:19.041: I smdistributed/modelparallel/torch/model.py:919] [5] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:47:19.041: W smdistributed/modelparallel/torch/model.py:929] [5] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:47:19.045: I smdistributed/modelparallel/torch/model.py:919] [4] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:47:19.045: W smdistributed/modelparallel/torch/model.py:929] [4] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:47:19.564: I smdistributed/modelparallel/torch/model.py:919] [0] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:47:19.564: W smdistributed/modelparallel/torch/model.py:929] [0] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-02-14 08:47:21.540: I smdistributed/modelparallel/torch/checkpoint.py:352] [1] model checkpoint saved.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-02-14 08:47:21.636: I smdistributed/modelparallel/torch/checkpoint.py:352] [2] model checkpoint saved.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-02-14 08:47:24.192: I smdistributed/modelparallel/torch/checkpoint.py:352] [3] model checkpoint saved.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-02-14 08:47:24.286: I smdistributed/modelparallel/torch/checkpoint.py:352] [4] model checkpoint saved.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-02-14 08:47:25.660: I smdistributed/modelparallel/torch/checkpoint.py:352] [5] model checkpoint saved.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-02-14 08:47:26.119: I smdistributed/modelparallel/torch/checkpoint.py:352] [6] model checkpoint saved.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-02-14 08:47:26.437: I smdistributed/modelparallel/torch/checkpoint.py:352] [7] model checkpoint saved.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-02-14 08:47:52.262: I smdistributed/modelparallel/torch/checkpoint.py:352] [0] model checkpoint saved.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:SMP training finished successfully\u001b[0m\n",
      "\u001b[34m2023-02-14 08:47:54,848 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-02-14 08:47:54,848 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-02-14 08:47:54,848 sagemaker-training-toolkit INFO     Begin writing status file from leader node to worker nodes (if any)\u001b[0m\n",
      "\u001b[34m2023-02-14 08:48:24,878 sagemaker-training-toolkit INFO     Finished writing status file from leader node to worker nodes (if any)\u001b[0m\n",
      "\u001b[34m2023-02-14 08:48:24,879 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-02-14 08:48:29 Uploading - Uploading generated training model\n",
      "2023-02-14 09:06:26 Completed - Training job completed\n",
      "Training seconds: 1836\n",
      "Billable seconds: 1836\n"
     ]
    }
   ],
   "source": [
    "smp_estimator.fit(\n",
    "    inputs=data_channels,\n",
    "    experiment_config={\n",
    "        \"ExperimentName\": experiment.experiment_name,\n",
    "        \"TrialName\": trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Training\",\n",
    "    },\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = smp_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-232838030412/smp-tensorparallel-outputdir/smp-gpt-j-6B-p4d24x-tp8-pp1-bs8-2023-02-14-08-33-39-465/output/model.tar.gz'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Training Logs\n",
    "\n",
    "You can access the training logs from [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html). Make sure to look at the logs of algo-1 as that is the master node whose output stream will have the training job logs.\n",
    "\n",
    "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see *Processing Job, Training Job, Batch Transform Job, and Endpoint Instance Metrics* in [Monitor Amazon SageMaker with Amazon CloudWatch](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html).\n",
    "\n",
    "If you are a new user of CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html). \n",
    "\n",
    "For additional information on monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html).\n",
    "\n",
    "# Deploying Trained Model for Inference\n",
    "\n",
    "In most cases the trained model can be deployed on a single device for inference, since inference has smaller memory requirements. You can use the SMP API to create a single, unified model after training. For TensorFlow, a SavedModel can be created using `smp.DistributedModel.save_model` API, and for PyTorch, `smp.save()` can be used.\n",
    "\n",
    "After you build and train your models, you can deploy them to get predictions in one of two ways:\n",
    "\n",
    "* To set up a persistent endpoint to get predictions from your models, use SageMaker hosting services. For an overview on deploying a single model or multiple models with SageMaker hosting services, see [Deploy a Model on SageMaker Hosting Services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html#how-it-works-hosting).\n",
    "* To get predictions for an entire dataset, use SageMaker batch transform. For an overview on deploying a model with SageMaker batch transform, see [Get Inferences for an Entire Dataset with Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html).\n",
    "\n",
    "To learn more about deploying models for inference using SageMaker, see [Deploy Models for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> If you are deploying the gpt-j-xl configuration of the model you can deploy the model by running the below cells. If you are deploying the gpt-j-6B configuration of the model, please refer to this open-sourced guide to the deployment workflow of GPT-J with DeepSpeed which is available on [GitHub](https://github.com/mantiumai/aws-sagemaker-gptj-deepspeed-blog)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy gpt-j-xl model using SageMaker Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "model_data = model_location\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=model_data,  # path to your trained sagemaker model\n",
    "    role=role,  # iam role with permissions to create an Endpoint\n",
    "    transformers_version=\"4.17\",  # transformers version used\n",
    "    pytorch_version=\"1.10\",  # pytorch version used\n",
    "    py_version=\"py38\",  # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy gpt-j-xl model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.g4dn.2xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "    \"inputs\": \"The new Hugging Face SageMaker DLC makes it super easy to deploy models in production. It is great!\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict({\"inputs\": \"Can you please let us know more details about your \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor.predict(\n",
    "    {\n",
    "        \"inputs\": \"Can you please let us know more \",\n",
    "        \"parameters\": {\n",
    "            \"min_length\": 220,\n",
    "            \"temperature\": 0.6,\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "end_sequence = \".\"\n",
    "temparature = 40\n",
    "max_generated_token_length = 100\n",
    "input = \"Can you please let us know more details about your \"\n",
    "\n",
    "predictor.predict(\n",
    "    {\n",
    "        \"inputs\": input,\n",
    "        \"parameters\": {\n",
    "            \"min_length\": int(len(input) + max_generated_token_length),\n",
    "            \"temperature\": temparature,\n",
    "            \"eos_token_id\": tokenizer.convert_tokens_to_ids(end_sequence),\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "hide_input": false,
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
