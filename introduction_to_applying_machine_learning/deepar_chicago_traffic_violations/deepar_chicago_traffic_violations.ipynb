{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Amazon SageMaker DeepAR algorithm to forecast traffic violations\n",
    "_**Using the Amazon SageMaker DeepAR algorithm to predict streets where motorists are most likely to drive above speed limits at different times of the year**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "  1. [Evaluate](#Evaluate)\n",
    "1. [Extensions](#Extensions)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "This notebook demonstrates time series forecasting using the Amazon SageMaker DeepAR algorithm by analyzing city of Chicago’s red light traffic camera violations. The dataset is hosted by [Data.gov](https://data.gov), and is managed by the [U.S. General Services Administration, Technology Transformation Service](http://www.gsa.gov/portal/category/25729).\n",
    "\n",
    "These violations are captured by camera systems and available to improve the lives of public through the [city of Chicago data portal](https://data.cityofchicago.org/). The [Speed Camera Violation  dataset](https://data.cityofchicago.org/Transportation/Red-Light-Camera-Violations/spqx-js37) can be used to discern patterns in the data and gain meaningful insights.\n",
    "\n",
    "The dataset contains multiple camera locations and daily violation counts. Each daily violations for a camera can be considered a separate time series. Amazon SageMaker’s DeepAR algorithm can be used to train a model for multiple streets simultaneously, and predict violation for multiple street cameras using the Amazon SageMaker’s [DeepAR algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html).\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "This notebook was created and tested on an ml.m4.xlarge notebook instance.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the notebook instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace `sagemaker.get_execution_role()` with the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-deepar-chicago' # change to your desired S3 prefix\n",
    "region = sess.boto_region_name\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import Python libraries like s3fs, matplotlib, pandas and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "!conda install -y s3fs\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "\n",
    "\n",
    "The speed violations are captured by camera systems and available to improve the lives of public from the city of Chicago data portal. The Speed Camera Violation dataset can be used to discern patterns in the data and gain meaningful insights.\n",
    "\n",
    "The dataset contains multiple camera locations and daily violation counts. If we imagine that each daily violations for a camera as one time series, we can use Amazon SageMaker’s DeepAR algorithm to train a model for multiple streets simultaneously, and predict violation for multiple street cameras using the Amazon SageMaker’s DeepAR algorithm.\n",
    "\n",
    "The dataset contains several columns, we use the address, violation date, violations for the forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'Chicago_Speed_Camera_Violations.csv'\n",
    "speeding_violation_data_path = '{}/{}/{}.csv'.format(bucket, prefix, datafile)\n",
    "s3_output_path = '{}/{}/output'.format(bucket, prefix)\n",
    "train_data_path = '{}/{}/train/train.json'.format(bucket, prefix)\n",
    "test_data_path = '{}/{}/test/test.json'.format(bucket, prefix)\n",
    "\n",
    "url = 'https://data.cityofchicago.org/api/views/hhkd-xvj4/rows.csv?accessType=DOWNLOAD'\n",
    "\n",
    "# get the data from City of Chicago site\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open(datafile, 'wb').write(r.content)\n",
    "  \n",
    "# read the input file, and display sample rows/columns\n",
    "pd.set_option('display.max_columns', 500)     \n",
    "pd.set_option('display.max_rows', 50)    \n",
    "df = pd.read_csv(open(datafile, 'rb'), encoding='utf-8')\n",
    "\n",
    "# print first 10 lines to look at part of the dataset\n",
    "df[['ADDRESS', 'VIOLATION DATE', 'VIOLATIONS']][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the violation date from string format to date format, determine the range of violation dates, and look at how many unique street addresses/cameras we have in our dataset.\n",
    "\n",
    "The dataset contains multiple camera locations and daily violation counts. If we imagine that each camera's daily violations as one time series, we can use Amazon SageMaker’s DeepAR algorithm to train a model for multiple streets simultaneously, and predict the violation count for multiple street cameras using the Amazon SageMaker’s DeepAR algorithm.\n",
    "\n",
    "As described in [Amazon SageMaker DeepAR input/output interface](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html#deepar-inputoutput) section, we will convert the data into array, and use 0 for the violation count when data for a given camera on a given date is not available. Using the Matplotlib library we display each camera location as a timeseries to visualize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VIOLATION_DT'] = pd.to_datetime(df['VIOLATION DATE'])\n",
    "df[['ADDRESS', 'VIOLATION_DT', 'VIOLATIONS']]\n",
    "unique_addresses = df.ADDRESS.unique()\n",
    "idx = pd.date_range(df.VIOLATION_DT.min(), df.VIOLATION_DT.max())\n",
    "number_of_addresses = len(unique_addresses)\n",
    "print('Unique Addresses {}'.format(number_of_addresses))\n",
    "print('Minimum violation date is {}, maximum violation date is {}'.format(df.VIOLATION_DT.min(), df.VIOLATION_DT.max()))\n",
    "\n",
    "\n",
    "violation_list = []\n",
    "for key in unique_addresses:\n",
    "    temp_df = df[['VIOLATION_DT', 'VIOLATIONS']][df.ADDRESS == key]\n",
    "    temp_df.set_index(['VIOLATION_DT'], inplace=True)\n",
    "    temp_df.index = pd.DatetimeIndex(temp_df.index)\n",
    "    temp_df = temp_df.reindex(idx, fill_value=0)\n",
    "    violation_list.append(temp_df['VIOLATIONS'])\n",
    "\n",
    "plt.figure(figsize=(12,6), dpi=100, facecolor='w')\n",
    "for key, address in enumerate(unique_addresses):\n",
    "    plt.plot(violation_list[key], label=address)\n",
    "\n",
    "plt.ylabel('Violations')\n",
    "plt.xlabel('Date')\n",
    "plt.title('Chicago Speed Camera Violations')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), shadow=False, ncol=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define prediction length as 30 days, and split the data with last 30 days of data as test data. We use rest of the data for training of the model. We can use the last 30 days of data to evaluate the accuracy of our trained model. We write the training and test data files in JSON format in the S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = 30\n",
    "\n",
    "# Split the data for training and validation/hold out\n",
    "violation_list_training = []\n",
    "for i in violation_list:\n",
    "    violation_list_training.append((i[:-prediction_length]))\n",
    "\n",
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {'start': str(ts.index[0]), 'target': list(ts)}\n",
    "    if cat:\n",
    "        obj['cat'] = cat\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))\n",
    "\n",
    "encoding = 'utf-8'\n",
    "s3filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "with s3filesystem.open(train_data_path, 'wb') as fp:\n",
    "    for ts in violation_list_training:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))\n",
    "\n",
    "with s3filesystem.open(test_data_path, 'wb') as fp:\n",
    "    for ts in violation_list:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train\n",
    "\n",
    "We use [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) to create an [estimator](https://sagemaker.readthedocs.io/en/stable/estimators.html) object to kick off training job. The train_use_spot parameter indicates the use of [managed spot training](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html). The training will run at most 1 hour (3600 seconds). \n",
    "\n",
    "We use the [Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) or Hyperparameter optimization for identifying the best values for the [DeepAR hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html). The Automatic Model Tuning job will kick of 10 parallel jobs (set by by max_parallel_jobs) to search the best hyperparameters for this dataset. The jobs will try to minimize the root mean square error on the test dataset using predicted and actual values.\n",
    "\n",
    "You can consider increasing the max_parallel_jobs and train_max_run and train_max_wait parameters to allow for finding better hyperparameters, and allow additional tuning of the hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "\n",
    "container = get_image_uri(region_name=region,\n",
    "                          repo_name='forecasting-deepar')\n",
    "\n",
    "deepar = sagemaker.estimator.Estimator(container,\n",
    "                                       role,\n",
    "                                       train_instance_count=1,\n",
    "                                       train_instance_type='ml.m4.xlarge',\n",
    "                                       train_use_spot_instances=True, # use spot instances\n",
    "                                       train_max_run=3600, # max training time in seconds\n",
    "                                       train_max_wait=3600, # seconds to wait for spot instance\n",
    "                                       output_path='s3://{}/{}'.format(bucket, s3_output_path),\n",
    "                                       sagemaker_session=sess)\n",
    "freq = 'D'\n",
    "context_length = 30\n",
    "\n",
    "deepar.set_hyperparameters(time_freq=freq,\n",
    "                           context_length=str(context_length),\n",
    "                           prediction_length=str(prediction_length))\n",
    "\n",
    "hyperparameter_ranges = {'mini_batch_size': IntegerParameter(100, 400),\n",
    "                         'epochs': IntegerParameter(200, 400),\n",
    "                         'num_cells': IntegerParameter(30,100),\n",
    "                         'likelihood': CategoricalParameter(['negative-binomial', 'student-T']),\n",
    "                         'learning_rate': ContinuousParameter(0.0001, 0.1)}\n",
    "\n",
    "objective_metric_name = 'test:RMSE'\n",
    "\n",
    "tuner = HyperparameterTuner(deepar,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=10,\n",
    "                            strategy='Bayesian',\n",
    "                            objective_type='Minimize',\n",
    "                            max_parallel_jobs=10,\n",
    "                            early_stopping_type='Auto')\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train/'.format(bucket, prefix),\n",
    "                                    content_type='json')\n",
    "s3_input_test = sagemaker.s3_input(s3_data='s3://{}/{}/test/'.format(bucket, prefix),\n",
    "                                   content_type='json')\n",
    "\n",
    "tuner.fit({'train': s3_input_train, 'test': s3_input_test}, \n",
    "          include_cls_metadata=False)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Host\n",
    "\n",
    "We use the [HyperParameterTuner](https://sagemaker.readthedocs.io/en/stable/tuner.html) to host the best model using a single ml.m4.xlarge instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tuning_job_name = tuner.best_training_job()\n",
    "endpoint_name = tuner.deploy(initial_instance_count=1,\n",
    "                             endpoint_name=best_tuning_job_name,\n",
    "                             instance_type='ml.m4.xlarge',\n",
    "                             wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "To evaluate the model, we define a DeepARPredictor class. This class extends the [RealTimePredictor](https://sagemaker.readthedocs.io/en/stable/predictors.html) class. Implementing encode and decode functions helps us make requests using `pandas.Series` objects rather than raw JSON strings.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        \"\"\"Set the time frequency and prediction length parameters. This method **must** be \n",
    "        called before being able to use `predict`.\n",
    "        \n",
    "        Parameters:\n",
    "        freq -- string indicating the time frequency\n",
    "        prediction_length -- integer, number of predicted time points\n",
    "       \n",
    "        Return value: none.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def predict(self, ts, cat=None, encoding='utf-8', num_samples=100, quantiles=['0.1', '0.5', '0.9']):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the \n",
    "        (optional) corresponding category listed in `cat`.\n",
    "        \n",
    "        Parameters:\n",
    "        ts -- list of `pandas.Series` objects, the time series to predict\n",
    "        cat -- list of integers (default: None)\n",
    "        encoding -- string, encoding to use for the request (default: 'utf-8')\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: ['0.1', '0.5', '0.9'])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_times = [x.index[-1]+1 for x in ts]\n",
    "        req = self.__encode_request(ts, cat, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, encoding, num_samples, quantiles):\n",
    "        instances = [series_to_obj(ts[k], cat[k] if cat else None) for k in range(len(ts))]\n",
    "        configuration = {'num_samples': num_samples, 'output_types': ['quantiles'], 'quantiles': quantiles}\n",
    "        http_request_data = {'instances': instances, 'configuration': configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "    \n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "            list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "        return list_of_df\n",
    "\n",
    "\n",
    "predictor = DeepARPredictor(endpoint=best_tuning_job_name,\n",
    "                            sagemaker_session=sess,\n",
    "                            content_type='application/json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the previously created `predictor` object. We will predict only the first few time series, and compare the results with the actual data we kept in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_prediction_parameters(freq, prediction_length)\n",
    "list_of_df = predictor.predict(violation_list_training[:5])\n",
    "actual_data = violation_list[:5]\n",
    "for k in range(len(list_of_df)):\n",
    "    plt.figure(figsize=(12,6), dpi=75, facecolor='w')\n",
    "    plt.ylabel('Violations')\n",
    "    plt.xlabel('Date')\n",
    "    plt.title('Chicago Speed Camera Violations:' + unique_addresses[k])\n",
    "    actual_data[k][-prediction_length-context_length:].plot(label='target')\n",
    "    p10 = list_of_df[k]['0.1']\n",
    "    p90 = list_of_df[k]['0.9']\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5,label='80% confidence interval')\n",
    "    list_of_df[k]['0.5'].plot(label='prediction median')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Clean-up\n",
    "\n",
    "At the end of this exercise, delete the endpoint to avoid accumulating charges in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
