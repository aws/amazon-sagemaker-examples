{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf0e3e5",
   "metadata": {},
   "source": [
    "# Triton on Sagemaker - NLP Bert\n",
    "\n",
    "### TODO: Add Sagemaker, Triton and NGC introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e7818",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Basic: PyTorch NLP-Bert](#PyTorch-NLP-Bert)\n",
    "2. [Advanced: TensorRT NLP-Bert](#TensorRT-NLP-Bert)\n",
    "3. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker transformers==4.9.1\n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184fd216",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_image_uri = \"195202947636.dkr.ecr.us-west-2.amazonaws.com/tritonserver:21.07-py3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87756432",
   "metadata": {},
   "source": [
    "The `tritonclient` package provides utility methods to generate the payload without having to know the details of the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3eb942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    enc = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    encoded_text = enc(text, padding=\"max_length\", max_length=128)\n",
    "    return encoded_text[\"input_ids\"], encoded_text[\"attention_mask\"]\n",
    "\n",
    "\n",
    "def _get_sample_tokenized_text_binary(text, input_names, output_names):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    inputs.append(httpclient.InferInput(input_names[0], [1, 128], \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(input_names[1], [1, 128], \"INT32\"))\n",
    "    indexed_tokens, attention_mask = tokenize_text(text)\n",
    "\n",
    "    indexed_tokens = np.array(indexed_tokens, dtype=np.int32)\n",
    "    indexed_tokens = np.expand_dims(indexed_tokens, axis=0)\n",
    "    inputs[0].set_data_from_numpy(indexed_tokens, binary_data=True)\n",
    "\n",
    "    attention_mask = np.array(attention_mask, dtype=np.int32)\n",
    "    attention_mask = np.expand_dims(attention_mask, axis=0)\n",
    "    inputs[1].set_data_from_numpy(attention_mask, binary_data=True)\n",
    "\n",
    "    outputs.append(httpclient.InferRequestedOutput(output_names[0], binary_data=True))\n",
    "    outputs.append(httpclient.InferRequestedOutput(output_names[1], binary_data=True))\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length\n",
    "\n",
    "\n",
    "def get_sample_tokenized_text_binary_pt(text):\n",
    "    return _get_sample_tokenized_text_binary(\n",
    "        text, [\"INPUT__0\", \"INPUT__1\"], [\"OUTPUT__0\", \"1634__1\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_sample_tokenized_text_binary_trt(text):\n",
    "    return _get_sample_tokenized_text_binary(text, [\"token_ids\", \"attn_mask\"], [\"output\", \"1634\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7466e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it \\\n",
    "            -v `pwd`/workspace:/workspace nvcr.io/nvidia/pytorch:21.07-py3 \\\n",
    "            /bin/bash generate_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aef1a0",
   "metadata": {},
   "source": [
    "## PyTorch NLP-Bert\n",
    "\n",
    "For a simple use case we will take the pre-trained NLP Bert model from [Hugging Face](https://huggingface.co/transformers/model_doc/bert.html) and deploy it on Sagemaker with Triton as the model server. The script for exporting this model can be found [here](./workspace/pt_exporter.py). This is run as part of the `generate_models.sh` script. After the model is serialized we package it into the format that Triton and Sagemaker expect it to be. We used the pre-configured `config.pbtxt` file provided with this repo [here](./triton-serve-pt/bert/config.pbtxt) to specify model [configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) which Triton uses to load the model. We tar the model directory and upload it to s3 to later create a [Sagemaker Model](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d5f1f6",
   "metadata": {},
   "source": [
    "### Packaging model files and uploading to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c969c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-pt/bert/1/\n",
    "!mv -f workspace/model.pt triton-serve-pt/bert/1/\n",
    "!tar -C triton-serve-pt/ -czf model.tar.gz bert\n",
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"triton-serve-pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a61ad",
   "metadata": {},
   "source": [
    "### Create Sagemaker Enpoint\n",
    "\n",
    "We start off by creating a [sagemaker model](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html) from the model files we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also provide an additional Environment Variable i.e. `SAGEMAKER_TRITON_DEFAULT_MODEL_NAME` which specifies the name of the model to be loaded by Triton. **The value of this key should match the folder name in the model package uploaded to s3**. This variable is optional in case of a single model. In case of ensemble models, this key **has to be** specified for Triton to startup in Sagemaker.\n",
    "\n",
    "*Note*: The current release of Triton (21.06-py3) on Sagemaker doesn't support running instances of different models on the same server, except in case of [ensembles](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md#ensemble-models). Only multiple model instances of the same model are supported, which can be specified under the [instance-groups](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#instance-groups) section of the config.pbtxt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96467520",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"triton-nlp-bert-pt-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"bert\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cb07d",
   "metadata": {},
   "source": [
    "Using the model above, we create an [endpoint configuration](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html) where we can specify the type and number of instances we want in the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"triton-nlp-bert-pt-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20ce7a",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dbde4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"triton-nlp-bert-pt-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de810f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb83fec",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "\n",
    "Once we have the endpoint running we can use the [sample image](./kitten.jpg) provided to do an inference using json as the payload format. For inference request format, Triton uses the KFServing community standard [inference protocols](https://github.com/triton-inference-server/server/blob/main/docs/protocol/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ddc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\"\n",
    "input_ids, attention_mask = tokenize_text(text)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT__0\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"INPUT__1\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69725daa",
   "metadata": {},
   "source": [
    "We can also use binary+json as the payload format to get better performance for the inference call. The specification of this format is provided [here](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md).\n",
    "\n",
    "**Note:** With the `binary+json` format, we have to specify the length of the request metadata in the header to allow Triton to correctly parse the binary payload. This is done using a custom Content-Type header `application/vnd.sagemaker-triton.binary+json;json-header-size={}`.\n",
    "\n",
    "Please not, this is different than using `Inference-Header-Content-Length` header on a stand-alone Triton server since custom headers are not allowed in Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Amazon SageMaker helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML.\"\n",
    "request_body, header_length = get_sample_tokenized_text_binary_pt(text)\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/vnd.sagemaker-triton.binary+json;json-header-size={}\".format(\n",
    "        header_length\n",
    "    ),\n",
    "    Body=request_body,\n",
    ")\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response[\"Body\"].read(), header_length=int(header_length_str)\n",
    ")\n",
    "output0_data = result.as_numpy(\"OUTPUT__0\")\n",
    "output1_data = result.as_numpy(\"1442__1\")\n",
    "print(output0_data)\n",
    "print(output1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d078370",
   "metadata": {},
   "source": [
    "### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d30434",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6741f3a",
   "metadata": {},
   "source": [
    "## TensorRT NLP-Bert\n",
    "\n",
    "Another way to improve performance is to convert the PyTorch Resnet50 model to a TensorRT plan and use it natively to run inferneces on Triton. By using the [onnx_exporter.py](./workspace/onnx_exporter.py) script and `trtexec` we create a TensorRT plan from the pre-trained PyTorch ResNet50 model. This is already done as part of the `generate_models.sh` script that we ran earlier in this notebook. We'll package the model and the provided `config.pbtxt` according the Triton model specification and upload to s3 for creating a SageMaker model and endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34013d",
   "metadata": {},
   "source": [
    "### Packaging model files and uploading to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81edf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-trt/bert/1/\n",
    "!mv -f workspace/model_bs16.plan triton-serve-trt/bert/1/model.plan\n",
    "!tar -C triton-serve-trt/ -czf model.tar.gz bert\n",
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"triton-serve-trt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6892944",
   "metadata": {},
   "source": [
    "### Create Sagemaker Enpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2183f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"triton-nlp-bert-trt-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"bert\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"triton-nlp-bert-trt-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e73b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"triton-nlp-bert-trt-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924fd2dd",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "\n",
    "Once we have the endpoint running we can run the inference both using a json payload as well as binary+json payload as described in the standart PyTorch deployment section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\"\n",
    "input_ids, attention_mask = tokenize_text(text)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"token_ids\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"attn_mask\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Amazon SageMaker helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML.\"\n",
    "request_body, header_length = get_sample_tokenized_text_binary_trt(text)\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/vnd.sagemaker-triton.binary+json;json-header-size={}\".format(\n",
    "        header_length\n",
    "    ),\n",
    "    Body=request_body,\n",
    ")\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response[\"Body\"].read(), header_length=int(header_length_str)\n",
    ")\n",
    "output0_data = result.as_numpy(\"output\")\n",
    "output1_data = result.as_numpy(\"1442\")\n",
    "print(output0_data)\n",
    "print(output1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7e8b0",
   "metadata": {},
   "source": [
    "### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b11f3",
   "metadata": {},
   "source": [
    "### TODO: Add comparision charts for base PT case vs TRT compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75690749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
