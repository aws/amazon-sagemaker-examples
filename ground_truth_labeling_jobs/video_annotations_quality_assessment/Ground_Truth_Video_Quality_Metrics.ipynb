{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit and Improve Video Annotation Quality Using Amazon SageMaker Ground Truth\n",
    "\n",
    "This notebook walks through how to evaluate the quality of video annotations received from SageMaker Ground Truth annotators using several metrics.\n",
    "\n",
    "The standard functionality of this notebook works with the standard Conda Python3/Data Science kernel; however, there is an optional section that uses a PyTorch model to generate image embeddings.\n",
    "\n",
    "Start by importing the required libraries and initializing the session and other variables used in this notebook. By default, the notebook uses the default Amazon S3 bucket in the same AWS Region you use to run this notebook. If you want to use a different S3 bucket, make sure it is in the same AWS Region you use to complete this tutorial, and specify the bucket name for `bucket`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import sagemaker as sm\n",
    "import subprocess\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import numpy as np\n",
    "from matplotlib import patches\n",
    "from plotting_funcs import *\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Create some of the resources you need to launch a Ground Truth audit labeling job in this notebook. To execute this notebook, you must create the following resources:\n",
    "\n",
    "* A work team: A work team is a group of workers that complete labeling tasks. If you want to preview the worker UI and execute the labeling task, you must create a private work team, add yourself as a worker to this team, and provide the following work team ARN. This [GIF](images/create-workteam-loop.gif) demonstrates how to quickly create a private work team on the Amazon SageMaker console. To learn more about private, vendor, and Amazon Mechanical Turk workforces, see [Create and Manage Workforces](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "WORKTEAM_ARN = \"<<ADD WORK TEAM ARN HERE>>\"\n",
    "\n",
    "print(f\"This notebook will use the work team ARN: {WORKTEAM_ARN}\")\n",
    "\n",
    "# Make sure workteam arn is populated\n",
    "assert WORKTEAM_ARN != \"<<ADD WORK TEAM ARN HERE>>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The IAM execution role you used to create this notebook instance must have the following permissions:\n",
    "    * `AmazonSageMakerFullAccess`: If you do not require granular permissions for your use case, you can attach the [AmazonSageMakerFullAccess](https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/AmazonSageMakerFullAccess) policy to your IAM user or role. If you are running this example in a SageMaker notebook instance, this is the IAM execution role used to create your notebook instance. If you need granular permissions, see [Assign IAM Permissions to Use Ground Truth](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-security-permission.html#sms-security-permissions-get-started) for granular policy to use Ground Truth.\n",
    "    * The AWS managed policy [AmazonSageMakerGroundTruthExecution](https://console.aws.amazon.com/iam/home#policies/arn:aws:iam::aws:policy/AmazonSageMakerGroundTruthExecution). Run the following code snippet to see your IAM execution role name. This [GIF](images/add-policy-loop.gif) demonstrates how to attach this policy to an IAM role in the IAM console. For further instructions see the: [Adding and removing IAM identity permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html#add-policies-console) section in the *AWS Identity and Access Management User Guide*.\n",
    "    * Amazon S3 permissions: When you create your role, you specify Amazon S3 permissions. Make sure that your IAM role has access to the S3 bucket that you plan to use in this example. If you do not specify a S3 bucket in this notebook, the default bucket in the AWS region in which you are running this notebook instance is used. If you do not require granular permissions, you can attach [AmazonS3FullAccess](https://console.aws.amazon.com/iam/home#policies/arn:aws:iam::aws:policy/AmazonS3FullAccess) to your role.\n",
    "\n",
    "* The S3 bucket that you use for this demo must have a CORS policy attached. To learn more about this requirement, and how to attach a CORS policy to an S3 bucket, see [Video Frame Job Permission Requirements](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-video-overview.html#sms-security-permission-video-frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "role = sm.get_execution_role()\n",
    "role_name = role.split(\"/\")[-1]\n",
    "print(\n",
    "    \"IMPORTANT: Make sure this execution role has the AWS Managed policy AmazonGroundTruthExecution attached.\"\n",
    ")\n",
    "print(\"********************************************************************************\")\n",
    "print(\"The IAM execution role name:\", role_name)\n",
    "print(\"The IAM execution role ARN:\", role)\n",
    "print(\"********************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sagemaker_cl = boto3.client(\"sagemaker\")\n",
    "# Make sure the bucket is in the same region as this notebook.\n",
    "bucket = \"<< YOUR S3 BUCKET NAME >>\"\n",
    "\n",
    "sm_session = sm.Session()\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "if bucket == \"<< YOUR S3 BUCKET NAME >>\":\n",
    "    bucket = sm_session.default_bucket()\n",
    "region = boto3.session.Session().region_name\n",
    "bucket_region = s3.head_bucket(Bucket=bucket)[\"ResponseMetadata\"][\"HTTPHeaders\"][\n",
    "    \"x-amz-bucket-region\"\n",
    "]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), f\"Your S3 bucket {bucket} and this notebook need to be in the same region.\"\n",
    "print(f\"IMPORTANT: make sure the role {role_name} has the access to read and write to this bucket.\")\n",
    "print(\n",
    "    \"********************************************************************************************************\"\n",
    ")\n",
    "print(f\"This notebook will use the following S3 bucket: {bucket}\")\n",
    "print(\n",
    "    \"********************************************************************************************************\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "\n",
    "Download a dataset from the Multi-Object Tracking Challenge, a commonly used benchmark for multi-object tracking. Depending on your connection speed, this can take 5â€“10 minutes. Unzip it and upload it to a `bucket` in Amazon S3.\n",
    "\n",
    "Disclosure regarding the Multiple Object Tracking Benchmark:\n",
    "\n",
    "Multiple Object Tracking Benchmark is created by Anton Milan, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taixe. We have not modified the images or the accompanying annotations. You can obtain the images and the annotations [here](https://motchallenge.net/data/MOT17/). The images and annotations are licensed by the authors under [Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License](https://creativecommons.org/licenses/by-nc-sa/3.0/). The following paper describes Multiple Object Tracking Benchmark in depth: from the data collection and annotation to detailed statistics about the data and evaluation of models trained on it.\n",
    "\n",
    "MOT17: A Benchmark for Multi-Object Tracking.\n",
    "Anton Milan, Ian Reid, Stefan Roth, Konrad Schindler, Laura Leal-Taixe [arXiv:1603.00831](https://arxiv.org/abs/1603.00831)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grab our data this will take ~5 minutes\n",
    "!wget https://motchallenge.net/data/MOT17.zip -O /tmp/MOT17.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip our data\n",
    "!unzip -q /tmp/MOT17.zip -d MOT17\n",
    "!rm /tmp/MOT17.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# send our data to s3 this will take a couple minutes\n",
    "!aws s3 cp --recursive MOT17/MOT17/train s3://{bucket}/MOT17/train --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View images and labels\n",
    "The scene is a street setting with a large number of cars and pedestrians. Grab image paths and plot the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = glob(\"MOT17/MOT17/train/MOT17-13-SDP/img1/*.jpg\")\n",
    "img_paths.sort()\n",
    "\n",
    "imgs = []\n",
    "for imgp in img_paths:\n",
    "    img = Image.open(imgp)\n",
    "    imgs.append(img)\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labels\n",
    "The MOT17 dataset has labels for each scene in a single text file. Load the labels and organize them into a frame-level dictionary so you can easily plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab our labels\n",
    "labels = []\n",
    "with open(\"MOT17/MOT17/train/MOT17-13-SDP/gt/gt.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        labels.append(line.replace(\"\\n\", \"\").split(\",\"))\n",
    "\n",
    "lab_dict = {}\n",
    "for i in range(1, len(img_paths) + 1):\n",
    "    lab_dict[i] = []\n",
    "\n",
    "for lab in labels:\n",
    "    lab_dict[int(lab[0])].append(lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View MOT17 annotations\n",
    "\n",
    "In the existing MOT-17 annotations, the labels include both bounding box coordinates and unique IDs for each object being tracked. By plotting the following two frames, you can see how the objects of interest persist across frames. Since our video has a high number of frames per second, look at frame 1 and then frame 31 to see the same scene with approximately one second between frames. You can adjust the start index, end index, and step values to view different labeled frames in the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 1\n",
    "end_index = 32\n",
    "step = 30\n",
    "\n",
    "for j in range(start_index, end_index, step):\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1, figsize=(24, 12))\n",
    "    ax.set_title(f\"Frame {j}\", fontdict={\"fontsize\": 20})\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(imgs[j])\n",
    "\n",
    "    for i, annot in enumerate(lab_dict[j]):\n",
    "        annot = np.array(annot, dtype=np.float32)\n",
    "\n",
    "        # if class is non-pedestrian display box\n",
    "        if annot[6] == 0:\n",
    "            rect = patches.Rectangle(\n",
    "                (annot[2], annot[3]),\n",
    "                annot[4],\n",
    "                annot[5],\n",
    "                linewidth=1,\n",
    "                edgecolor=\"r\",\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            plt.text(\n",
    "                annot[2],\n",
    "                annot[3] - 10,\n",
    "                f\"Object {int(annot[1])}\",\n",
    "                bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate labels\n",
    "\n",
    "For demonstration purposes, we've labeled three vehicles in one of the videos and inserted a few labeling anomalies into the annotations. Identifying mistakes and then sending directed recommendations for frames and objects to fix makes the label auditing process more efficient. If a labeler only has to focus on a few frames instead of a deep review of the entire scene, it can drastically improve speed and reduce cost.\"\n",
    "\n",
    "We have provided a JSON file containing intentionally flawed labels. For a typical Ground Truth Video job, this file is in the Amazon S3 output location you specified when creating your labeling job. This label file is organized as a sequential list of labels. Each entry in the list consists of the labels for one frame.\n",
    "\n",
    "For more information about Ground Truth's output data format, see the [Output Data](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-data-output.html) section of the *Amazon SageMaker Developer Guide*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  labels\n",
    "lab_path = \"SeqLabel.json\"\n",
    "with open(lab_path, \"r\") as f:\n",
    "    flawed_labels = json.load(f)\n",
    "\n",
    "img_paths = glob(\"MOT17/MOT17/train/MOT17-13-SDP/img1/*.jpg\")\n",
    "img_paths.sort()\n",
    "\n",
    "# Let's grab our images\n",
    "imgs = []\n",
    "for imgp in img_paths:\n",
    "    img = Image.open(imgp)\n",
    "    imgs.append(img)\n",
    "\n",
    "flawed_labels[\"tracking-annotations\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View annotations\n",
    "\n",
    "We annotated 3 vehicles, one of which enters the scene at frame 9. View the scene starting at frame 9 to see all of our labeled vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's view our tracking labels\n",
    "start_index = 9\n",
    "end_index = 16\n",
    "step = 3\n",
    "\n",
    "for j in range(start_index, end_index, step):\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1, figsize=(24, 12))\n",
    "    ax.set_title(f\"Frame {j}\")\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(np.array(imgs[j]))\n",
    "\n",
    "    for i, annot in enumerate(flawed_labels[\"tracking-annotations\"][j][\"annotations\"]):\n",
    "        rect = patches.Rectangle(\n",
    "            (annot[\"left\"], annot[\"top\"]),\n",
    "            annot[\"width\"],\n",
    "            annot[\"height\"],\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(\n",
    "            annot[\"left\"] - 5,\n",
    "            annot[\"top\"] - 10,\n",
    "            f\"{annot['object-name']}\",\n",
    "            bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze tracking data\n",
    "\n",
    "Put the tracking data into a form that's easier to analyze.\n",
    "\n",
    "The following function turns our tracking output into a dataframe. You can use this dataframe to plot values and compute metrics to help you understand how the object labels move through the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataframes\n",
    "label_frame = create_annot_frame(flawed_labels[\"tracking-annotations\"])\n",
    "label_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View label progression plots\n",
    "\n",
    "The following plots illustrate how the coordinates of a given object progress through the frames of a video. Each bounding box has a left and top coordinate, representing the top-left point of the bounding box. It also has height and width values that represent the other 3 points of the box.\n",
    "\n",
    "In the following plots, the blue lines represent the progression of our 4 values (top coordinate, left coordinate, width, and height) through the video frames and the orange lines represent a rolling average of these values. If a video has 5 frames per second or more, the objects within the video (and therefore the bounding boxes drawn around them) should have some amount of overlap between frames. Our video has vehicles driving at a normal pace, so our plots should show a relatively smooth progression.\n",
    "\n",
    "You can also plot the deviation between the rolling average and the actual values of bounding box coordinates. You may want to look at frames in which the actual value deviates substantially from the rolling average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out progression of different metrics\n",
    "\n",
    "plot_timeseries(label_frame, obj=\"Vehicle:1\", roll_len=5)\n",
    "plot_deviations(label_frame, obj=\"Vehicle:1\", roll_len=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot box sizes\n",
    "\n",
    "Combine the width and height values to examine how the size of the bounding box for a given object progresses through the scene. For Vehicle 1, we intentionally reduced the size of the bounding box on frame 139 and restored it on frame 141. We also removed a bounding box on frame 217. We can see both of these flaws reflected in our size progression plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_size_prog(annot_frame, obj=\"Vehicle:1\", roll_len=5, figsize=(17, 10)):\n",
    "    \"\"\"\n",
    "    Plot size progression of a bounding box for a given object.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figsize)\n",
    "    lframe_len = max(annot_frame[\"frameid\"])\n",
    "    ann_subframe = annot_frame[annot_frame.obj == obj]\n",
    "    ann_subframe.index = list(np.arange(len(ann_subframe)))\n",
    "    size_vec = np.zeros(lframe_len + 1)\n",
    "    size_vec[ann_subframe[\"frameid\"].values] = ann_subframe[\"height\"] * ann_subframe[\"width\"]\n",
    "    ax.plot(size_vec)\n",
    "    ax.plot(pd.Series(size_vec).rolling(roll_len).mean())\n",
    "    ax.title.set_text(f\"{obj} Size progression\")\n",
    "    ax.set_xlabel(\"Frame Number\")\n",
    "    ax.set_ylabel(\"Box size\")\n",
    "\n",
    "\n",
    "plot_size_prog(label_frame, obj=\"Vehicle:1\")\n",
    "plot_size_prog(label_frame, obj=\"Vehicle:2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View box size differential\n",
    "\n",
    "Now, look at how the size of the box changes from frame to frame by plotting the actual size differential to get a better idea of the magnitude of these changes.\n",
    "\n",
    "You can also normalize the magnitude of the size changes by dividing the size differentials by the sizes of the boxes to express the differential as a percentage change from the original size of the box. This makes it easier to set thresholds beyond which you can classify this frame as potentially problematic for this object bounding box.\n",
    "\n",
    "The following plots visualize both the absolute size differential and the size differential as a percentage. You can also add lines representing where the bounding box changed by more than 20% in size from one frame to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at rolling size differential, try changing the object\n",
    "\n",
    "\n",
    "def plot_size_diff(lab_frame, obj=\"Vehicle:1\", hline=0.5, figsize=(24, 16)):\n",
    "    \"\"\"\n",
    "    Plot the sequential size differential between the bounding box for a given object between frames\n",
    "    \"\"\"\n",
    "    ann_subframe = lab_frame[lab_frame.obj == obj]\n",
    "    lframe_len = max(lab_frame[\"frameid\"])\n",
    "    ann_subframe.index = list(np.arange(len(ann_subframe)))\n",
    "    size_vec = np.zeros(lframe_len + 1)\n",
    "    size_vec[ann_subframe[\"frameid\"].values] = ann_subframe[\"height\"] * ann_subframe[\"width\"]\n",
    "    size_diff = np.array(size_vec[:-1]) - np.array(size_vec[1:])\n",
    "    norm_size_diff = size_diff / np.array(size_vec[:-1])\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=2, figsize=figsize)\n",
    "    ax[0].plot(size_diff)\n",
    "    ax[0].set_title(\"Absolute size differential\")\n",
    "    ax[1].plot(norm_size_diff)\n",
    "    ax[1].set_title(\"Normalized size differential\")\n",
    "    ax[1].hlines(-hline, 0, len(size_diff), colors=\"red\")\n",
    "    ax[1].hlines(hline, 0, len(size_diff), colors=\"red\")\n",
    "\n",
    "\n",
    "plot_size_diff(label_frame, obj=\"Vehicle:1\", hline=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you normalize the size differential, you can use a threshold to identify which frames to flag for review. The preceding plot sets a threshold of 20% change from the previous box size; there a few frames that exceed that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prob_frames(lab_frame, obj=\"Vehicle:2\", thresh=0.25):\n",
    "    \"\"\"\n",
    "    Find potentially problematic frames via size differential\n",
    "    \"\"\"\n",
    "    lframe_len = max(lab_frame[\"frameid\"])\n",
    "    ann_subframe = lab_frame[lab_frame.obj == obj]\n",
    "    size_vec = np.zeros(lframe_len + 1)\n",
    "    size_vec[ann_subframe[\"frameid\"].values] = ann_subframe[\"height\"] * ann_subframe[\"width\"]\n",
    "    size_diff = np.array(size_vec[:-1]) - np.array(size_vec[1:])\n",
    "    norm_size_diff = size_diff / np.array(size_vec[:-1])\n",
    "    norm_size_diff[np.where(np.isnan(norm_size_diff))[0]] = 0\n",
    "    norm_size_diff[np.where(np.isinf(norm_size_diff))[0]] = 0\n",
    "    problem_frames = (\n",
    "        np.where(np.abs(norm_size_diff) > thresh)[0] + 1\n",
    "    )  # adding 1 since we are are looking\n",
    "    worst_frame = np.argmax(np.abs(norm_size_diff)) + 1\n",
    "    return problem_frames, worst_frame\n",
    "\n",
    "\n",
    "obj = \"Vehicle:1\"\n",
    "problem_frames, worst_frame = find_prob_frames(label_frame, obj=obj, thresh=0.2)\n",
    "print(f\"Worst frame for {obj} is: {worst_frame}\")\n",
    "print(\"problem frames for\", obj, \":\", problem_frames.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the frames with the largest size differential\n",
    "\n",
    "With the indices for the frames with the largest size differential, you can view them in sequence. In the following frames, you can identify frames including Vehicle 1 where our labeler made a mistake. There was a large difference between frame 216 and frame 217, the subsequent frame, so frame 217 was flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = worst_frame - 1\n",
    "\n",
    "# let's view our tracking labels\n",
    "for j in range(start_index, start_index + 3):\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1, figsize=(24, 12))\n",
    "    ax.set_title(f\"Frame {j}\")\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(imgs[j])\n",
    "\n",
    "    for i, annot in enumerate(flawed_labels[\"tracking-annotations\"][j][\"annotations\"]):\n",
    "        rect = patches.Rectangle(\n",
    "            (annot[\"left\"], annot[\"top\"]),\n",
    "            annot[\"width\"],\n",
    "            annot[\"height\"],\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )  # 50,100),40,30\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(\n",
    "            annot[\"left\"] - 5,\n",
    "            annot[\"top\"] - 10,\n",
    "            f\"{annot['object-name']}\",\n",
    "            bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "        )  #\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling IoU\n",
    "\n",
    "IoU (Intersection over Union) is a commonly used evaluation metric for object detection. It's calculated by dividing the area of overlap between two bounding boxes by the area of union for two bounding boxes. While it's typically used to evaluate the accuracy of a predicted box against a ground truth box, you can use it to evaulate how much overlap a given bounding box has from one frame of a video to the next.\n",
    "\n",
    "Since there are differences from one frame to the next, we would not expect a given bounding box for a single object to have 100% overlap with the corresponding bounding box from the next frame. However, depending on the frames per second (FPS) for the video, there often is only a small change between one frame and the next since the time elapsed between frames is only a fraction of a second. For higher FPS video, we would expect a substantial amount of overlap between frames. The MOT17 videos are all shot at 25 FPS, so these videos qualify. Operating with this assumption, you can use IoU to identify outlier frames where you see substantial differences between a bounding box in one frame to the next. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rolling intersection over union\n",
    "\n",
    "\n",
    "def calc_frame_int_over_union(annot_frame, obj, i):\n",
    "    lframe_len = max(annot_frame[\"frameid\"])\n",
    "    annot_frame = annot_frame[annot_frame.obj == obj]\n",
    "    annot_frame.index = list(np.arange(len(annot_frame)))\n",
    "    coord_vec = np.zeros((lframe_len + 1, 4))\n",
    "    coord_vec[annot_frame[\"frameid\"].values, 0] = annot_frame[\"left\"]\n",
    "    coord_vec[annot_frame[\"frameid\"].values, 1] = annot_frame[\"top\"]\n",
    "    coord_vec[annot_frame[\"frameid\"].values, 2] = annot_frame[\"width\"]\n",
    "    coord_vec[annot_frame[\"frameid\"].values, 3] = annot_frame[\"height\"]\n",
    "\n",
    "    boxA = [\n",
    "        coord_vec[i, 0],\n",
    "        coord_vec[i, 1],\n",
    "        coord_vec[i, 0] + coord_vec[i, 2],\n",
    "        coord_vec[i, 1] + coord_vec[i, 3],\n",
    "    ]\n",
    "    boxB = [\n",
    "        coord_vec[i + 1, 0],\n",
    "        coord_vec[i + 1, 1],\n",
    "        coord_vec[i + 1, 0] + coord_vec[i + 1, 2],\n",
    "        coord_vec[i + 1, 1] + coord_vec[i + 1, 3],\n",
    "    ]\n",
    "    return bb_int_over_union(boxA, boxB)\n",
    "\n",
    "\n",
    "# create list of objects\n",
    "objs = list(np.unique(label_frame.obj))\n",
    "\n",
    "# iterate through our objects to get rolling IoU values for each\n",
    "iou_dict = {}\n",
    "for obj in objs:\n",
    "    iou_vec = np.ones(len(np.unique(label_frame.frameid)))\n",
    "    ious = []\n",
    "    for i in label_frame[label_frame.obj == obj].frameid[:-1]:\n",
    "        iou = calc_frame_int_over_union(label_frame, obj, i)\n",
    "        ious.append(iou)\n",
    "        iou_vec[i] = iou\n",
    "    iou_dict[obj] = iou_vec\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(24, 8), sharey=True)\n",
    "ax[0].set_title(f\"Rolling IoU {objs[0]}\")\n",
    "ax[0].set_xlabel(\"frames\")\n",
    "ax[0].set_ylabel(\"IoU\")\n",
    "ax[0].plot(iou_dict[objs[0]])\n",
    "ax[1].set_title(f\"Rolling IoU {objs[1]}\")\n",
    "ax[1].set_xlabel(\"frames\")\n",
    "ax[1].set_ylabel(\"IoU\")\n",
    "ax[1].plot(iou_dict[objs[1]])\n",
    "ax[2].set_title(f\"Rolling IoU {objs[2]}\")\n",
    "ax[2].set_xlabel(\"frames\")\n",
    "ax[2].set_ylabel(\"IoU\")\n",
    "ax[2].plot(iou_dict[objs[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify low overlap frames\n",
    "\n",
    "With the IoU for your objects, you can set an IoU threshold and identify objects below it. The following code snippet identifies frames in which the bounding box for a given object has less than 50% overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ID problem indices\n",
    "iou_thresh = 0.5\n",
    "vehicle = 1  # because index starts at 0, 0 -> vehicle:1, 1 -> vehicle:2, etc.\n",
    "\n",
    "# use np.where to identify frames below our threshold.\n",
    "inds = np.where(np.array(iou_dict[objs[vehicle]]) < iou_thresh)[0]\n",
    "worst_ind = np.argmin(np.array(iou_dict[objs[vehicle]]))\n",
    "\n",
    "print(objs[vehicle], \"worst frame:\", worst_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize low overlap frames\n",
    "\n",
    "With low overlap frames identified by the IoU metric, you can see that there is an issue with Vehicle 2 on frame 102. The bounding box for Vehicle 2 does not go low enough and clearly needs to be extended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = worst_ind - 1\n",
    "\n",
    "# let's view our tracking labels\n",
    "for j in range(start_index, start_index + 3):\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1, figsize=(24, 12))\n",
    "    ax.set_title(f\"Frame {j}\")\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(imgs[j])\n",
    "\n",
    "    for i, annot in enumerate(flawed_labels[\"tracking-annotations\"][j][\"annotations\"]):\n",
    "        rect = patches.Rectangle(\n",
    "            (annot[\"left\"], annot[\"top\"]),\n",
    "            annot[\"width\"],\n",
    "            annot[\"height\"],\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(\n",
    "            annot[\"left\"] - 5,\n",
    "            annot[\"top\"] - 10,\n",
    "            f\"{annot['object-name']}\",\n",
    "            bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding comparison (optional)\n",
    "\n",
    "The preceding two methods work because they are simple and are based on the reasonable assumption that objects in high-FPS video won't move too much from frame to frame. They can be considered more classical methods of comparison.\n",
    "\n",
    "Can we improve upon them? Try something more experimental to identify outliers: Generate embeddings for bounding box crops with an image classification model like ResNet and compare these across frames.\n",
    "\n",
    "Convolutional neural network image classification models have a final fully connected layer using a softmax function or another scaling activation function that outputs probabilities. If you remove the final layer of your network, your \"predictions\" are the image embedding that is essentially the neural network's representation of the image. If you isolate objects by cropping images, you can compare the representations of these objects across frames to identify any outliers.\n",
    "\n",
    "Start by importing a model from Torchhub and using a ResNet18 model trained on ImageNet. Since ImageNet is a very large and generic dataset, the network has learned information about images and is able to classify them into different categories. While a neural network more finely tuned on vehicles would likely perform better, a network trained on a large dataset like ImageNet should have learned enough information to indicate if images are similar.\n",
    "\n",
    "Note: As mentioned at the beginning of the notebook, if you wish to run this section, you'll need to use a PyTorch kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# download our model from torchhub\n",
    "model = torch.hub.load(\"pytorch/vision:v0.6.0\", \"resnet18\", pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# in order to get embeddings instead of a classification from a model we import, we need to remove the top layer of the network\n",
    "modules = list(model.children())[:-1]\n",
    "model = nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embeddings\n",
    "\n",
    "Use your headless model to generate image embeddings for your object crops. The following code iterates through images, generates crops of labeled objects, resizes them to 224x224x3 to work with your headless model, and then predicts the image crop embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_crops = {}\n",
    "img_embeds = {}\n",
    "\n",
    "for j, img in tqdm(enumerate(imgs[:300])):\n",
    "    img_arr = np.array(img)\n",
    "    img_embeds[j] = {}\n",
    "    img_crops[j] = {}\n",
    "    for i, annot in enumerate(flawed_labels[\"tracking-annotations\"][j][\"annotations\"]):\n",
    "\n",
    "        # crop our image using our annotation coordinates\n",
    "        crop = img_arr[\n",
    "            annot[\"top\"] : (annot[\"top\"] + annot[\"height\"]),\n",
    "            annot[\"left\"] : (annot[\"left\"] + annot[\"width\"]),\n",
    "            :,\n",
    "        ]\n",
    "\n",
    "        # resize image crops to work with our model which takes in 224x224x3 sized inputs\n",
    "        new_crop = np.array(Image.fromarray(crop).resize((224, 224)))\n",
    "        img_crops[j][annot[\"object-name\"]] = new_crop\n",
    "\n",
    "        # reshape array so that it follows (batch dimension, color channels, image dimension, image dimension)\n",
    "        new_crop = np.reshape(new_crop, (1, 224, 224, 3))\n",
    "        new_crop = np.reshape(new_crop, (1, 3, 224, 224))\n",
    "\n",
    "        torch_arr = torch.tensor(new_crop, dtype=torch.float)\n",
    "\n",
    "        # return image crop embedding from headless model\n",
    "        with torch.no_grad():\n",
    "            embed = model(torch_arr)\n",
    "\n",
    "        img_embeds[j][annot[\"object-name\"]] = embed.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View image crops\n",
    "\n",
    "To generate image crops, use the bounding box label dimensions and then resize the cropped images. Look at a few of them in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_crops(obj=\"Vehicle:1\", start=0, figsize=(20, 12)):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=5, figsize=figsize)\n",
    "    for i, a in enumerate(ax):\n",
    "        a.imshow(img_crops[i + start][obj])\n",
    "        a.set_title(f\"Frame {i+start}\")\n",
    "\n",
    "\n",
    "plot_crops(start=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distance\n",
    "\n",
    "Compare image embeddings by computing the distance between sequential embeddings for a given object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(img_embeds, dist_func=distance.euclidean, obj=\"Vehicle:1\"):\n",
    "    dists = []\n",
    "    inds = []\n",
    "    for i in img_embeds:\n",
    "        if (i > 0) & (obj in list(img_embeds[i].keys())):\n",
    "            if obj in list(img_embeds[i - 1].keys()):\n",
    "                dist = dist_func(\n",
    "                    img_embeds[i - 1][obj], img_embeds[i][obj]\n",
    "                )  # distance  between frame at t0 and t1\n",
    "                dists.append(dist)\n",
    "                inds.append(i)\n",
    "    return dists, inds\n",
    "\n",
    "\n",
    "obj = \"Vehicle:2\"\n",
    "dists, inds = compute_dist(img_embeds, obj=obj)\n",
    "\n",
    "# look for distances that are 2 standard deviation greater than the mean distance\n",
    "prob_frames = np.where(dists > (np.mean(dists) + np.std(dists) * 2))[0]\n",
    "prob_inds = np.array(inds)[prob_frames]\n",
    "print(prob_inds)\n",
    "print(\"The frame with the greatest distance is frame:\", inds[np.argmax(dists)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View outlier frames\n",
    "\n",
    "In outlier frame crops, you can see that we were able to catch the issue on frame 102, where the bounding box was off-center.\n",
    "\n",
    "While this method is fun to play with, it's substantially more computationally expensive than the more generic methods and is not guaranteed to improve accuracy. Using such a generic model will inevitably produce false positives. Feel free to try a model fine-tuned on vehicles, which would likely yield better results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_crops(obj=\"Vehicle:1\", start=0):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(20, 12))\n",
    "    for i, a in enumerate(ax):\n",
    "        a.imshow(img_crops[i + start][obj])\n",
    "        a.set_title(f\"Frame {i+start}\")\n",
    "\n",
    "\n",
    "plot_crops(obj=obj, start=np.argmax(dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the metrics\n",
    "\n",
    "Having explored several methods for identifying anomalous and potentially problematic frames, you can combine them and identify all of those outlier frames. While you might have a few false positives, they are likely to be in areas with a lot of action that you might want our annotators to review regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_problem_frames(\n",
    "    lab_frame,\n",
    "    flawed_labels,\n",
    "    size_thresh=0.25,\n",
    "    iou_thresh=0.4,\n",
    "    embed=False,\n",
    "    imgs=None,\n",
    "    verbose=False,\n",
    "    embed_std=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function for identifying potentially problematic frames using bounding box size, rolling IoU, and optionally embedding comparison.\n",
    "    \"\"\"\n",
    "    if embed:\n",
    "        model = torch.hub.load(\"pytorch/vision:v0.6.0\", \"resnet18\", pretrained=True)\n",
    "        model.eval()\n",
    "        modules = list(model.children())[:-1]\n",
    "        model = nn.Sequential(*modules)\n",
    "\n",
    "    frame_res = {}\n",
    "    for obj in list(np.unique(lab_frame.obj)):\n",
    "        frame_res[obj] = {}\n",
    "        lframe_len = max(lab_frame[\"frameid\"])\n",
    "        ann_subframe = lab_frame[lab_frame.obj == obj]\n",
    "        size_vec = np.zeros(lframe_len + 1)\n",
    "        size_vec[ann_subframe[\"frameid\"].values] = ann_subframe[\"height\"] * ann_subframe[\"width\"]\n",
    "        size_diff = np.array(size_vec[:-1]) - np.array(size_vec[1:])\n",
    "        norm_size_diff = size_diff / np.array(size_vec[:-1])\n",
    "        norm_size_diff[np.where(np.isnan(norm_size_diff))[0]] = 0\n",
    "        norm_size_diff[np.where(np.isinf(norm_size_diff))[0]] = 0\n",
    "        frame_res[obj][\"size_diff\"] = [int(x) for x in size_diff]\n",
    "        frame_res[obj][\"norm_size_diff\"] = [int(x) for x in norm_size_diff]\n",
    "        try:\n",
    "            problem_frames = [int(x) for x in np.where(np.abs(norm_size_diff) > size_thresh)[0]]\n",
    "            if verbose:\n",
    "                worst_frame = np.argmax(np.abs(norm_size_diff))\n",
    "                print(\"Worst frame for\", obj, \"in\", frame, \"is: \", worst_frame)\n",
    "        except:\n",
    "            problem_frames = []\n",
    "        frame_res[obj][\"size_problem_frames\"] = problem_frames\n",
    "\n",
    "        iou_vec = np.ones(len(np.unique(lab_frame.frameid)))\n",
    "        for i in lab_frame[lab_frame.obj == obj].frameid[:-1]:\n",
    "            iou = calc_frame_int_over_union(lab_frame, obj, i)\n",
    "            iou_vec[i] = iou\n",
    "\n",
    "        frame_res[obj][\"iou\"] = iou_vec.tolist()\n",
    "        inds = [int(x) for x in np.where(iou_vec < iou_thresh)[0]]\n",
    "        frame_res[obj][\"iou_problem_frames\"] = inds\n",
    "\n",
    "        if embed:\n",
    "            img_crops = {}\n",
    "            img_embeds = {}\n",
    "\n",
    "            for j, img in tqdm(enumerate(imgs)):\n",
    "                img_arr = np.array(img)\n",
    "                img_embeds[j] = {}\n",
    "                img_crops[j] = {}\n",
    "                for i, annot in enumerate(flawed_labels[\"tracking-annotations\"][j][\"annotations\"]):\n",
    "                    try:\n",
    "                        crop = img_arr[\n",
    "                            annot[\"top\"] : (annot[\"top\"] + annot[\"height\"]),\n",
    "                            annot[\"left\"] : (annot[\"left\"] + annot[\"width\"]),\n",
    "                            :,\n",
    "                        ]\n",
    "                        new_crop = np.array(Image.fromarray(crop).resize((224, 224)))\n",
    "                        img_crops[j][annot[\"object-name\"]] = new_crop\n",
    "                        new_crop = np.reshape(new_crop, (1, 224, 224, 3))\n",
    "                        new_crop = np.reshape(new_crop, (1, 3, 224, 224))\n",
    "                        torch_arr = torch.tensor(new_crop, dtype=torch.float)\n",
    "                        with torch.no_grad():\n",
    "                            emb = model(torch_arr)\n",
    "                        img_embeds[j][annot[\"object-name\"]] = emb.squeeze()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            dists = compute_dist(img_embeds, obj=obj)\n",
    "\n",
    "            # look for distances that are 2+ standard deviations greater than the mean distance\n",
    "            prob_frames = np.where(dists > (np.mean(dists) + np.std(dists) * embed_std))[0]\n",
    "            frame_res[obj][\"embed_prob_frames\"] = prob_frames.tolist()\n",
    "\n",
    "    return frame_res\n",
    "\n",
    "\n",
    "# if you want to add in embedding comparison, set embed=True\n",
    "num_images_to_validate = 300\n",
    "embed = False\n",
    "frame_res = get_problem_frames(\n",
    "    label_frame,\n",
    "    flawed_labels,\n",
    "    size_thresh=0.25,\n",
    "    iou_thresh=0.5,\n",
    "    embed=embed,\n",
    "    imgs=imgs[:num_images_to_validate],\n",
    ")\n",
    "\n",
    "prob_frame_dict = {}\n",
    "all_prob_frames = []\n",
    "for obj in frame_res:\n",
    "    prob_frames = list(frame_res[obj][\"size_problem_frames\"])\n",
    "    prob_frames.extend(list(frame_res[obj][\"iou_problem_frames\"]))\n",
    "    if embed:\n",
    "        prob_frames.extend(list(frame_res[obj][\"embed_prob_frames\"]))\n",
    "    all_prob_frames.extend(prob_frames)\n",
    "\n",
    "prob_frame_dict = [int(x) for x in np.unique(all_prob_frames)]\n",
    "prob_frame_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command line interface\n",
    "\n",
    "For use outside of a notebook, you can use the following command line interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage for the CLI is like this\n",
    "\n",
    "# !{sys.executable} quality_metrics_cli.py run-quality-check --bucket mybucket \\\n",
    "# --lab_path job_results/bag-track-mot20-test-tracking/annotations/consolidated-annotation/output/0/SeqLabel.json \\\n",
    "# --save_path example_quality_output/bag-track-mot20-test-tracking.json\n",
    "\n",
    "# To get the help text\n",
    "!{sys.executable} quality_metrics_cli.py run-quality-check --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch a directed audit job\n",
    "\n",
    "Take a look at how to create a Ground Truth [video frame tracking adjustment job](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-video-object-tracking.html). Ground Truth provides a worker UI and infrastructure to streamline the process of creating this type of labeling job. All you have to do is specify the worker instructions, labels, and input data.\n",
    "\n",
    "With problematic annotations identified, you can launch a new audit labeling job. You can do this in SageMaker using the console; however, when you want to launch jobs in a more automated fashion, using the Boto3 API is very helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new labeling job, first create your label categories so Ground Truth knows what labels to display for your workers. In this file, also specify the labeling instructions. You can use the outlier frames identified above to give directed instructions to your workers so they can spend less time reviewing the entire scene and focus more on potential problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label categories\n",
    "\n",
    "os.makedirs(\"tracking_manifests\", exist_ok=True)\n",
    "\n",
    "labelcats = {\n",
    "    \"document-version\": \"2020-08-15\",\n",
    "    \"auditLabelAttributeName\": \"Person\",\n",
    "    \"labels\": [\n",
    "        {\n",
    "            \"label\": \"Vehicle\",\n",
    "            \"attributes\": [\n",
    "                {\"name\": \"color\", \"type\": \"string\", \"enum\": [\"Silver\", \"Red\", \"Blue\", \"Black\"]}\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"Pedestrian\",\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"Other\",\n",
    "        },\n",
    "    ],\n",
    "    \"instructions\": {\n",
    "        \"shortInstruction\": f\"Please draw boxes around pedestrians, with a specific focus on the following frames {prob_frame_dict}\",\n",
    "        \"fullInstruction\": f\"Please draw boxes around pedestrians, with a specific focus on the following frames {prob_frame_dict}\",\n",
    "    },\n",
    "}\n",
    "\n",
    "filename = \"tracking_manifests/label_categories.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(labelcats, f)\n",
    "\n",
    "s3.upload_file(Filename=filename, Bucket=bucket, Key=\"tracking_manifests/label_categories.json\")\n",
    "\n",
    "LABEL_CATEGORIES_S3_URI = f\"s3://{bucket}/tracking_manifests/label_categories.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate manifests\n",
    "\n",
    "SageMaker Ground Truth operates using [manifests](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-input-data-input-manifest.html). When you use a modality like image classification, a single image corresponds to a single entry in a manifest and a given manifest contains paths for all of the images to be labeled. Because videos have multiple frames and you can have [multiple videos in a single manifest](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-video-manual-data-setup.html), a manifest is instead organized with a JSON sequence file for each video that contains the paths to frames in Amazon S3. This allows a single manifest to contain multiple videos for a single job.\n",
    "\n",
    "In this example, the image files are all split out, so you can just grab file paths. If your data is in the form of video files, you can use the Ground Truth console to split videos into video frames. To learn more, see [Automated Video Frame Input Data Setup](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-video-automated-data-setup.html). You can also use other tools like [ffmpeg](https://ffmpeg.org/) to split video files into individual image frames. The following block stores file paths in a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our target MP4 files,\n",
    "vids = glob(\"MOT17/MOT17/train/*\")\n",
    "vids.sort()\n",
    "\n",
    "# we assume we have folders with the same name as the mp4 file in the same root folder\n",
    "vid_dict = {}\n",
    "for vid in vids:\n",
    "    files = glob(f\"{vid}/img1/*jpg\")\n",
    "    files.sort()\n",
    "    files = files[:300]  # look at first 300 images\n",
    "    fileset = []\n",
    "    for fil in files:\n",
    "        fileset.append(\"/\".join(fil.split(\"/\")[5:]))\n",
    "    vid_dict[vid] = fileset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your image paths, you can iterate through frames and create a list of entries for each in your sequence file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sequences\n",
    "all_vids = {}\n",
    "for vid in vid_dict:\n",
    "    frames = []\n",
    "    for i, v in enumerate(vid_dict[vid]):\n",
    "        frame = {\n",
    "            \"frame-no\": i + 1,\n",
    "            \"frame\": f\"{v.split('/')[-1]}\",\n",
    "            \"unix-timestamp\": int(time.time()),\n",
    "        }\n",
    "        frames.append(frame)\n",
    "    all_vids[vid] = {\n",
    "        \"version\": \"2020-07-01\",\n",
    "        \"seq-no\": np.random.randint(1, 1000),\n",
    "        \"prefix\": f\"s3://{bucket}/{'/'.join(vid.split('/')[1:])}/img1/\",\n",
    "        \"number-of-frames\": len(vid_dict[vid]),\n",
    "        \"frames\": frames,\n",
    "    }\n",
    "\n",
    "# save sequences\n",
    "for vid in all_vids:\n",
    "    with open(f\"tracking_manifests/{vid.split('/')[-1]}_seq.json\", \"w\") as f:\n",
    "        json.dump(all_vids[vid], f)\n",
    "\n",
    "!cp SeqLabel.json tracking_manifests/SeqLabel.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your sequence file, you can create your manifest file. To create a new job with no existing labels, you can simply pass in a path to your sequence file. Since you already have labels and instead want to launch an adjustment job, point to the location of those labels in Amazon S3 and provide metadata for those labels in your manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create manifest\n",
    "manifest_dict = {}\n",
    "for vid in all_vids:\n",
    "    source_ref = f\"s3://{bucket}/tracking_manifests/{vid.split('/')[-1]}_seq.json\"\n",
    "    annot_labels = f\"s3://{bucket}/tracking_manifests/SeqLabel.json\"\n",
    "\n",
    "    manifest = {\n",
    "        \"source-ref\": source_ref,\n",
    "        \"Person\": annot_labels,\n",
    "        \"Person-metadata\": {\n",
    "            \"class-map\": {\"2\": \"Vehicle\"},\n",
    "            \"human-annotated\": \"yes\",\n",
    "            \"creation-date\": \"2020-05-25T12:53:54+0000\",\n",
    "            \"type\": \"groundtruth/video-object-tracking\",\n",
    "        },\n",
    "    }\n",
    "    manifest_dict[vid] = manifest\n",
    "\n",
    "# save videos as individual jobs\n",
    "for vid in all_vids:\n",
    "    with open(f\"tracking_manifests/{vid.split('/')[-1]}.manifest\", \"w\") as f:\n",
    "        json.dump(manifest_dict[vid], f)\n",
    "\n",
    "print(\"Example manifest: \", manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send data to s3\n",
    "!aws s3 cp --recursive tracking_manifests s3://{bucket}/tracking_manifests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch jobs (optional)\n",
    "\n",
    "Now that you've created your manifests, you're ready to launch your adjustment labeling job. Use this template for launching labeling jobs via [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html). In order to access the labeling job, make sure you followed the steps to create a private work team.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate jobs\n",
    "\n",
    "job_names = []\n",
    "outputs = []\n",
    "\n",
    "arn_region_map = {\n",
    "    \"us-west-2\": \"081040173940\",\n",
    "    \"us-east-1\": \"432418664414\",\n",
    "    \"us-east-2\": \"266458841044\",\n",
    "    \"eu-west-1\": \"568282634449\",\n",
    "    \"eu-west-2\": \"487402164563\",\n",
    "    \"ap-northeast-1\": \"477331159723\",\n",
    "    \"ap-northeast-2\": \"845288260483\",\n",
    "    \"ca-central-1\": \"918755190332\",\n",
    "    \"eu-central-1\": \"203001061592\",\n",
    "    \"ap-south-1\": \"565803892007\",\n",
    "    \"ap-southeast-1\": \"377565633583\",\n",
    "    \"ap-southeast-2\": \"454466003867\",\n",
    "}\n",
    "\n",
    "region_account = arn_region_map[region]\n",
    "\n",
    "LABELING_JOB_NAME = f\"mot17-tracking-adjust-{int(time.time())}\"\n",
    "task = \"AdjustmentVideoObjectTracking\"\n",
    "job_names.append(LABELING_JOB_NAME)\n",
    "INPUT_MANIFEST_S3_URI = f\"s3://{bucket}/tracking_manifests/MOT17-13-SDP.manifest\"\n",
    "\n",
    "human_task_config = {\n",
    "    \"PreHumanTaskLambdaArn\": f\"arn:aws:lambda:{region}:{region_account}:function:PRE-{task}\",\n",
    "    \"MaxConcurrentTaskCount\": 200,  # Maximum of 200 objects will be available to the workteam at any time\n",
    "    \"NumberOfHumanWorkersPerDataObject\": 1,  # We will obtain and consolidate 1 human annotationsfor each frame.\n",
    "    \"TaskAvailabilityLifetimeInSeconds\": 864000,  # Your workteam has 24 hours to complete all pending tasks.\n",
    "    \"TaskDescription\": f\"Please draw boxes around vehicles, with a specific focus around the following frames {prob_frame_dict}\",\n",
    "    # If using public workforce, specify \"PublicWorkforceTaskPrice\"\n",
    "    \"WorkteamArn\": WORKTEAM_ARN,\n",
    "    \"AnnotationConsolidationConfig\": {\n",
    "        \"AnnotationConsolidationLambdaArn\": f\"arn:aws:lambda:{region}:{region_account}:function:ACS-{task}\"\n",
    "    },\n",
    "    \"TaskKeywords\": [\"Image Classification\", \"Labeling\"],\n",
    "    \"TaskTimeLimitInSeconds\": 14400,\n",
    "    \"TaskTitle\": LABELING_JOB_NAME,\n",
    "    \"UiConfig\": {\n",
    "        \"HumanTaskUiArn\": f\"arn:aws:sagemaker:{region}:394669845002:human-task-ui/VideoObjectTracking\"\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "createLabelingJob_request = {\n",
    "    \"LabelingJobName\": LABELING_JOB_NAME,\n",
    "    \"HumanTaskConfig\": human_task_config,\n",
    "    \"InputConfig\": {\n",
    "        \"DataAttributes\": {\n",
    "            \"ContentClassifiers\": [\"FreeOfPersonallyIdentifiableInformation\", \"FreeOfAdultContent\"]\n",
    "        },\n",
    "        \"DataSource\": {\"S3DataSource\": {\"ManifestS3Uri\": INPUT_MANIFEST_S3_URI}},\n",
    "    },\n",
    "    \"LabelAttributeName\": \"Person-ref\",\n",
    "    \"LabelCategoryConfigS3Uri\": LABEL_CATEGORIES_S3_URI,\n",
    "    \"OutputConfig\": {\"S3OutputPath\": f\"s3://{bucket}/gt_job_results\"},\n",
    "    \"RoleArn\": role,\n",
    "    \"StoppingConditions\": {\"MaxPercentageOfInputDatasetLabeled\": 100},\n",
    "}\n",
    "print(createLabelingJob_request)\n",
    "out = sagemaker_cl.create_labeling_job(**createLabelingJob_request)\n",
    "outputs.append(out)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook introduced how to measure the quality of annotations using statistical analysis and various quality metrics like IoU, rolling IoU, and embedding comparisons. It also demonstrated how to flag frames which may not be labeled properly using these quality metrics and how to send those frames for verification and audit jobs using SageMaker Ground Truth.\n",
    "\n",
    "Using this approach, you can perform automated quality checks on the annotations at scale, which reduces the number of frames humans need to verify or audit. Please try the notebook with your own data and add your own quality metrics for different task types supported by SageMaker Ground Truth. With this process in place, you can generate high-quality datasets for a wide range of business use cases in a cost-effective manner without compromising the quality of annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Use the following command to stop your labeling job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "sagemaker_cl.stop_labeling_job(LABELING_JOB_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
