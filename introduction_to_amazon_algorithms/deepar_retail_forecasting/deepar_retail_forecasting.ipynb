{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Retail Sales Forecasting with DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting is a central problem in many businesses. In the retail industry probabilistic forecasts are important for inventory management to ensure that there is enough product on-hand to meet the seasonal spikes in sales for differnet categories of products. \n",
    "\n",
    "Most forecasting methods have been developed in the setting of forecasting individual time series where model parameters are independently estimated from past observations for each given time series. Today retailers are faced with forecasting demand on potentially millions of time series for different products across their catalog. Retailers also face cold start problems where they need to forecast for a new item that has no existing time series data. \n",
    "\n",
    "In this notebook we will see how the [DeepAR forecasting algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) can help retailers solve these business problems. Using a generated dataset of daily sales for a set of clothing products we will see how DeepAR can learn jointly across the related time series to capture complex group dependent behavior at a categorical level. Finally we will see how this learned categorical behavior can be used to forecast for products with existing time series as well as new \"cold\" products with no existing data.\n",
    "\n",
    "For a more rigorous explaination of the DeepAR algorithm check out the [DeepAR white paper](https://pdfs.semanticscholar.org/4eeb/e0d12aefeedf3ca85256bc8aa3b4292d47d9.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules and defining helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modules and helper functions below will be used throughout this lab to generate the dataset and convert data between formats. You do not need to read over and understand each function to proceed with the lab but comments have been added for the inquisitive to reference. Run the below cell before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import datetime\n",
    "import tempfile\n",
    "import random\n",
    "import boto3\n",
    "import copy\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def make_product(categories, num_years):\n",
    "    \"\"\" Creates a random product with a unique product id from the list of potential category and subcategory pairings\n",
    "    \n",
    "    Args:\n",
    "        categories (dict): Dictionary containing mappings of categories to lists of subcategories.\n",
    "        num_years (int): Range of years from today's date to pick start date for time series data from.\n",
    "    Returns:\n",
    "        (dict)\n",
    "        Dictionary containing product information for the generated product.\n",
    "    \"\"\"\n",
    "    product_id = str(uuid.uuid4().fields[-1])\n",
    "    cat, subcats = random.choice(list(categories.items()))\n",
    "    subcat = random.choice(subcats)\n",
    "    start_date = datetime.date.today() - datetime.timedelta(random.randint(num_years/2, num_years)*365)\n",
    "    return {'product_id': product_id, \n",
    "            'category': cat, \n",
    "            'subcategory': subcat, \n",
    "            'start_date': start_date}\n",
    "\n",
    "def make_weights(categories, num_years):\n",
    "    \"\"\" Creates normalized weights in the interval 0-1 for each category for each month of each year in the input range of years.\n",
    "    \n",
    "    Args:\n",
    "        categories (dict): Dictionary containing mappings of categories to lists of subcategories.\n",
    "        num_years (int): Range of years from today's date to pick start date for time series data from.\n",
    "    Returns:\n",
    "        (dict)\n",
    "        Dictionary containing weights for each category for each month of each year in the input range of years.\n",
    "    \"\"\"\n",
    "    end_year = datetime.date.today().year\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    for year in range(end_year-num_years, end_year+1):\n",
    "        for month in range(1, 13):\n",
    "            rands = np.random.random(size=len(categories))\n",
    "            weights = rands / rands.sum()\n",
    "            weight_index = 0\n",
    "            for category in categories:\n",
    "                results[category][year][month] = weights[weight_index]\n",
    "                weight_index += 1\n",
    "    return results\n",
    "\n",
    "def make_category_weights(categories, num_years):\n",
    "    \"\"\" Creates random weights for each category and subcategory to further augment seasonality within these groupings\n",
    "    Weights are randomly created for every category and subcategory for each year and month\n",
    "    At the category and subcategory levels the weights are normalized to sum to 1\n",
    "    \n",
    "    Args:\n",
    "        categories (dict): Dictionary containing mappings of categories to lists of subcategories.\n",
    "        num_years (int): Range of years from today's date to pick start date for time series data from.\n",
    "    Returns:\n",
    "        (dict)\n",
    "        Dictionary containing weights for each category for each month of each year in the input range of years.\n",
    "    \"\"\"\n",
    "    category_weights = make_weights(list(categories.keys()), num_years) # Create weights for the top level categories\n",
    "    for category, subcategories in categories.items(): # Add weights for the corresponding subcategories\n",
    "        subcat_weights = make_weights(subcategories, num_years)\n",
    "        for subcat, weights in subcat_weights.items():\n",
    "            category_weights[category][subcat] = weights\n",
    "    return category_weights\n",
    "\n",
    "def make_time_point_value(date, cat, subcat, seasonality, weights):\n",
    "    \"\"\" Creates daily time point sales value for a product based on baseline seasonality and product weights\n",
    "    \n",
    "    Args:\n",
    "        date (datetime): Date to generate time point value for.\n",
    "        cat (string): Category of product.\n",
    "        subcat (string): Subcategory of product.\n",
    "        seasonality (dict): Dictionary containing details about seasonality of sales for products.\n",
    "        weights (dict): Dictionary containing weights for each category and subcategory for each month of each year.\n",
    "    Returns:\n",
    "        (int)\n",
    "        Time point sales value for a given product on a given day.\n",
    "    \"\"\"\n",
    "    month = date.month\n",
    "    year = date.year\n",
    "    baseline = seasonality[month]\n",
    "    noise = np.random.normal(scale=0.1)\n",
    "    cat_weight = weights[cat][year][month]\n",
    "    subcat_weight = weights[cat][subcat][year][month]\n",
    "    value = (baseline + noise*baseline)*cat_weight*subcat_weight\n",
    "    return int(value)\n",
    "\n",
    "def make_data_for_product(product, seasonality, weights):\n",
    "    \"\"\" Creates DataFrame containing time point sales values for input product based on baseline seasonality and product weights.\n",
    "    \n",
    "    Args:\n",
    "        product (dict): Dictionary containing product information for a single product.\n",
    "        seasonality (dict): Dictionary containing details about seasonality of sales for products.\n",
    "        weights (dict): Dictionary containing weights for each category and subcategory for each month of each year.\n",
    "    Returns:\n",
    "        (DataFrame)\n",
    "        DataFrame containing time point sales values for a given product.\n",
    "    \"\"\"\n",
    "    cat = product['category']\n",
    "    subcat = product['subcategory']\n",
    "    today = datetime.date.today()\n",
    "    start_date = product['start_date']\n",
    "    delta = today - start_date\n",
    "    data = []\n",
    "    for i in range(delta.days + 1):\n",
    "        local_product = product.copy()\n",
    "        local_product.pop('start_date', None)\n",
    "        date = start_date + datetime.timedelta(days=i)\n",
    "        local_product['date'] = date\n",
    "        local_product['sales'] = make_time_point_value(date, cat, subcat, seasonality, weights)\n",
    "        data.append(local_product)\n",
    "    return pd.DataFrame(data)    \n",
    "\n",
    "def make_data_for_products(products, seasonality, weights):\n",
    "    \"\"\" Creates DataFrame containing time point sales values for all input products based on baseline return seasonality and product weights.\n",
    "    \n",
    "    Args:\n",
    "        products (list): List of dictionaries containing product information for all products.\n",
    "        seasonality (dict): Dictionary containing details about seasonality of sales for products.\n",
    "        weights (dict): Dictionary containing weights for each category and subcategory for each month of each year.\n",
    "    Returns:\n",
    "        (DataFrame)\n",
    "        DataFrame containing time series sales values for a given product.\n",
    "    \"\"\"\n",
    "    df = pd.concat([make_data_for_product(product, seasonality, weights) for product in products])\n",
    "    df = df[['product_id', 'date', 'category', 'subcategory', 'sales']]\n",
    "    df['product_id'] = df['product_id'].apply(str)\n",
    "    df['date'] = df['date'].apply(lambda t: pd.to_datetime(t, format='%Y-%m-%d'))\n",
    "    return df\n",
    "    \n",
    "def plot_dataset_ts(df, condition):\n",
    "    \"\"\" Plots summed time point sales values of data at monthly granularity grouped by condition.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing time point sales data for products.\n",
    "        condition (str): Condition to group DataFrame by ('category' or 'subcategory').\n",
    "    Returns:\n",
    "        (None)\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    data = data.groupby(['date', condition]).sum().unstack().fillna(0.0)\n",
    "    data = data.groupby([(data.index.year),(data.index.month)]).sum()\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    data.plot(ax=ax)\n",
    "\n",
    "def ts_to_json_obj(series, cat):\n",
    "    \"\"\" Converts input time point DataFrame into a JSON object in DeepAR format\n",
    "    \n",
    "    Args:\n",
    "        series (DataFrame): DataFrame containing time point sales data for a product.\n",
    "        cat (str): Label encoded category for product.\n",
    "    Returns:\n",
    "        (dict)\n",
    "        Dictionary containing time point sales data converted to DeepAR format.\n",
    "    \"\"\"\n",
    "    ts = series.copy() # Make a local copy of series so as not to modify original DataFrame\n",
    "    ts = ts.rename('sales') # Rename series\n",
    "    ts = ts.reset_index() # Input series is indexed by date, re-index to make df with date as column\n",
    "    start_date_index = ts['sales'].nonzero()[0][0] # Pull out index of first non-zero day of return values\n",
    "    ts = ts.iloc[start_date_index:].reset_index(drop=True) # Truncate leading 0 return rows from time series and reset index to 0\n",
    "    json_obj = {\"start\": str(ts['date'][0]), \"cat\": int(cat), \"target\": ts['sales'].astype(int).tolist()}\n",
    "    return json_obj\n",
    "\n",
    "def transform_to_json_objs(df, le):\n",
    "    \"\"\" Converts input DataFrame of all time point sales values for all products into list of JSON objects in DeepAR format for each product.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing time point sales data for all products.\n",
    "        le (LabelEncoder): SciKit LabelEncoder fit on product categories and subcategories to label encode these values with.\n",
    "    Returns:\n",
    "        (dict)\n",
    "        Dictionary containing time point sales data converted to DeepAR format for all products.\n",
    "    \"\"\"\n",
    "    cats_df = df[['product_id', 'category', 'subcategory']].drop_duplicates().set_index('product_id')\n",
    "    ts_df = df.groupby(['date', 'product_id']).sum().unstack().fillna(0.0)\n",
    "    ts_df.columns = ts_df.columns.droplevel() # Drop unneeded multi-index level\n",
    "    json_objs = []\n",
    "    for column in list(ts_df.columns.values):\n",
    "        cat = cats_df.loc[column, 'category']\n",
    "        subcat = cats_df.loc[column, 'subcategory']\n",
    "        num_cat = le.transform([cat])[0]\n",
    "        num_subcat = le.transform([subcat])[0]\n",
    "        json_objs.append(ts_to_json_obj(ts_df.loc[:, column], num_cat))\n",
    "        json_objs.append(ts_to_json_obj(ts_df.loc[:, column], num_subcat))\n",
    "    return json_objs\n",
    "\n",
    "def make_train_set(json_objs, prediction_length):\n",
    "    \"\"\" Creates a training set from an input dataset by removing prediction_length number of time points from each time series.\n",
    "    \n",
    "    Args:\n",
    "        json_objs (list): List of JSON objects in DeepAR format.\n",
    "        prediction_length (int): Number of time points to remove from each time series.\n",
    "    Returns:\n",
    "        (list)\n",
    "        List of JSON objects in DeepAR format where each object has had prediction_length number of time points removed.\n",
    "    \"\"\"\n",
    "    objs = copy.deepcopy(json_objs)\n",
    "    for obj in objs:\n",
    "        obj['target'] = obj['target'][:-prediction_length]\n",
    "    return objs\n",
    "\n",
    "def write_json_to_file(json_objs, channel):\n",
    "    \"\"\" Writes list of JSON objects to file in JSON Lines format encoded as utf-8. \n",
    "    Output file is named after channel.\n",
    "    \n",
    "    Args:\n",
    "        json_objs (list): List of JSON objects in DeepAR format.\n",
    "        channel (str): Channel that JSON Lines data will be used for ('train' or 'test').\n",
    "    Returns:\n",
    "        (str)\n",
    "        Path of file data was written to.\n",
    "    \"\"\"\n",
    "    file_name = '{}.json'.format(channel)\n",
    "    file_path = os.path.join(os.getcwd(), file_name)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        for obj in json_objs:\n",
    "            line = json.dumps(obj) + '\\n'\n",
    "            line = line.encode('utf-8')\n",
    "            f.write(line)\n",
    "        f.seek(0)\n",
    "        f.flush()\n",
    "    return file_path\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we will be working with a dataset composed of category + subcategory pairings. The cell below generates a catalog of products using possible category + subcategory pairings from the `categories` dictionary of categories and subcategories. Each product is given a unique product ID to be identified by.\n",
    "\n",
    "Next each product category and subcategory is given a random normalized weight for each month of each year in the time series range. These weights are used to simulate seasonality in different product categories and subcategories, such as boots being more popular in winter than in summer.\n",
    "\n",
    "Lastly, time series sales data is generated for each product using the the product catalog, the normalized weights, and the `seasonality` dictionary, which is used to augment the sales values roughly around what a typical retailers yearly sales trends might look like. To simulate a store adding products over time each product is randomly assigned a \"start\" date in the time series interval in which it begins to have non-zero sales values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories and subcategories to generate products with\n",
    "# Each product id is generated with a random category and subcategory from that category's options\n",
    "categories = {\n",
    "    'shoe': ['sneaker', 'boot', 'slipper'],\n",
    "    'outerwear': ['coat', 'jacket', 'shell'],\n",
    "    'top': ['shirt', 't-shirt', 'sweater', 'knit'],\n",
    "    'bottom': ['skirt', 'pant', 'short', 'leggings'],\n",
    "    'accessories': ['belt', 'tie', 'scarf', 'hat', 'brooche']\n",
    "}\n",
    "\n",
    "# Seasonality for time series data to be generated around\n",
    "seasonality = {\n",
    "    1: 500,\n",
    "    2: 400,\n",
    "    3: 200,\n",
    "    4: 100,\n",
    "    5: 40,\n",
    "    6: 80,\n",
    "    7: 150,\n",
    "    8: 180,\n",
    "    9: 140,\n",
    "    10: 240,\n",
    "    11: 100,\n",
    "    12: 400\n",
    "}\n",
    "\n",
    "n_products = 100 # Number of products to generate. Increase this to generate more individual product data\n",
    "\n",
    "n_years = 4 # Number of years in past from current date to generate date for. Increase this to generate longer time series for each product\n",
    "\n",
    "products = [make_product(categories, n_years) for i in range(n_products)]\n",
    "\n",
    "category_weights = make_category_weights(categories, n_years)\n",
    "\n",
    "product_data = make_data_for_products(products, seasonality, category_weights)\n",
    "\n",
    "print(product_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated our dataset let's take a look at the time series sales values over time at the category and subcategory levels across products. This can be done by running the code in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize monthly sales of products at category level\n",
    "plot_dataset_ts(product_data, 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize monthly sales of products at subcategory level\n",
    "plot_dataset_ts(product_data, 'subcategory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset for DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated our raw dataset we need to wrangle it into the format and encoding expected by DeepAR. \n",
    "\n",
    "DeepAR expects data (for training or inference) in [JSON Lines](http://jsonlines.org/) or [Parquet](https://parquet.apache.org/) format. For this lab we'll be working with JSON Lines.\n",
    "\n",
    "In JSON Lines format each line is a seperate JSON object representing a time series for a single product. DeepAR expects each JSON object to have a `start` key, a string or datetime object representing the time the time series data starts at, and a `target` key, whose value is an array of floats (or integers) that represent the time series variable’s values. Additionally each JSON object can include a `cat` key, which is an integer that encodes the categorical grouping that record’s time series is a member of. This allows the model to learn typical behavior for that group and can increase accuracy.\n",
    "\n",
    "Currently our product sales data is in a pandas DataFrame and is not yet converted into JSON objects with time series grouped by product ID as the DeepAR algorithm expects for training. In the following cells we'll wrangle the data into the required format.\n",
    "\n",
    "To start, our categorical grouping values (product category and subcategory) are currently strings but DeepAR expects integers for these values. Let's encode our category and subcategory values to integers to use for our `cat` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_cats = product_data.category.unique().tolist()\n",
    "uniq_subcats = product_data.subcategory.unique().tolist()\n",
    "\n",
    "le = LabelEncoder().fit((uniq_cats + uniq_subcats))\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we've encoded class labels for both the product subcategories and the categories. Later we will see how this allows us to make forecasting predictions for existing product subcategories at a granular level while also allowing us to generalize our predictions to new subcategories that we might wish to introduce by predicting at the higher category level.\n",
    "\n",
    "Next we set the epochs, frequency, prediction length, context length, cardinality, and embedding dimension hyperparameters we wish to train our DeepAR model on.\n",
    "\n",
    "Epochs specifies the maximum number of times to pass over the data when training. For this lab we set the value to '50' to minimize training job runtime due to time constraints.\n",
    "\n",
    "Frequency specifies the granularity of the time series in the dataset. In this case out time series values correspond to daily sales results for each product so our frequency is 'D' for daily. Other possible values are 'min' (every minute), 'H' (hourly), 'W' (weekly), and 'M' (monthly).\n",
    "\n",
    "Prediction length controls the number of time steps (based off the unit of frequency) that the model is trained to predict, also called the forecast horizon. Our prediction length is set to '28' to predict roughly a month's worth of days into the future for forecast requests submitted to the trained DeepAR model.\n",
    "\n",
    "Context length controls the the number of time points that the model gets to see before making a prediction. The value for this parameter should be about the same as the prediction_length. The model also receives lagged inputs from the target, so context_length can be much smaller than typical seasonalities. For example, a daily time series can have yearly seasonality. The model automatically includes a lag of one year, so the context length can be shorter than a year. The lag values that the model picks depend on the frequency of the time series. For example, lag values for daily frequency are the previous week, 2 weeks, 3 weeks, 4 weeks, and year. Here we set it to '28' to predict based off roughly the last month's worth of time points.\n",
    "\n",
    "Cardinality is only required when specifying categorical level groupings in your data, and controls the number of unique categories found in your dataset. The label encoder we trained already has a set of the unique classes found in our dataset so we can use the length of this set as our cardinality.\n",
    "\n",
    "Embedding dimension is only required when specifying categorical level groupings in your data, and specifies the size of the embedding vector the algorithm can learn to capture the common properties of all the time series within a categorical level grouping. Because of the small size of our dataset we leave set this to '2'. For larger datasets with more unique categories this value commonly ranges from 10-100.\n",
    "\n",
    "Values for the epochs, frequency, prediction length, and context length hyperparameters are required when training a DeepAR model, but you can also configure other optional hyperparameters to further tune your model. For a more exhaustive list of all the different DeepAR hyperparameters you can tune [check out the DeepAR documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = '50'\n",
    "freq = 'D'\n",
    "prediction_length = '28'\n",
    "context_length = '28'\n",
    "cardinality = str(len(le.classes_))\n",
    "embedding_dimension = '2'\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": epochs,\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": context_length,\n",
    "    \"prediction_length\": prediction_length,\n",
    "    \"cardinality\": cardinality,\n",
    "    \"embedding_dimension\": embedding_dimension\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we transform our pandas DataFrame into JSON objects for each product ID as expected by the DeepAR algorithm. This transformation is implemented in the `transform_to_json_objs` helper function created at the start of this notebook for reference.\n",
    "\n",
    "The DeepAR algorithm has an optional test channel for training that can be used to calculate accuracy metrics for the model after training, such as RMSE and quantile loss. For our test set we'll use the full time series for each product. For our training set we'll use the full time series minus our prediction length worth of time points. The loss for our model will then be calculated by how well our model predicts these missing time points in comparison to the ground truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objs = transform_to_json_objs(product_data, le)\n",
    "train_set = make_train_set(json_objs, int(prediction_length))\n",
    "test_set = json_objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we upload our data to S3 so it can be accessed by the DeepAR algorithm during the training job. The SageMaker `Session` class has a convenient method, [`upload_data`](http://sagemaker.readthedocs.io/en/latest/session.html#sagemaker.session.Session.upload_data), to help us upload our training and testing data to S3 while also returning S3 URIs to pass to our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_username = <YOUR_USERNAME_HERE>\n",
    "\n",
    "resource_prefix = 'deepar-retail-{}'.format(your_username)\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "train_file = write_json_to_file(train_set, 'train')\n",
    "test_file = write_json_to_file(test_set, 'test')\n",
    "\n",
    "train_location = sagemaker_session.upload_data(train_file, key_prefix=resource_prefix)\n",
    "test_location = sagemaker_session.upload_data(test_file, key_prefix=resource_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and deploying a DeepAR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepAR is one of SageMaker's [built-in algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) so a container for training and hosting the algorithm is provided by the service. Here we select the container corresponding to the region we're running our notebook in and specify the output path in S3 for our SageMaker training job to output trained model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "containers = {\n",
    "    'us-east-1': '522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-east-2': '566113047672.dkr.ecr.us-east-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-west-2': '156387875391.dkr.ecr.us-west-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'eu-west-1': '224300973850.dkr.ecr.eu-west-1.amazonaws.com/forecasting-deepar:latest'\n",
    "}\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "image_name = containers[boto3.Session().region_name]\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, resource_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an [Estimator](http://sagemaker.readthedocs.io/en/latest/estimators.html) object for our DeepAR model, which is a high level interface for training and deploying SageMaker models programatically, and configure it with the hyperparameters we set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c5.2xlarge',\n",
    "    base_job_name=resource_prefix,\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify the input data channels using the S3 URIs returned earlier when we uploaded our train and test datasets to S3 and fit the DeepAR model. Note that this cell may take 5-15 minutes to run to run to completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    \"train\": train_location,\n",
    "    \"test\": test_location\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we deploy our trained model to a SageMaker model endpoint where we can leverage it for forecasting predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    endpoint_name=resource_prefix,\n",
    "    content_type=\"application/json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging deployed DeepAR model endpoint for forecasting predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a SageMaker model endpoint deployed let's use it to make some predictions. DeepAR model endpoints expect requests in JSON format with the following keys:\n",
    "\n",
    "`instances` - A list of the time series that should be forecast by the model. Each entry in the list should be a JSON object in the same format that DeepAR expects for training.\n",
    "\n",
    "`configuration` - Optional. A dictionary of configuration information for the type of response desired by the request.\n",
    "\n",
    "Within `configuration` the following keys can be configured:\n",
    "\n",
    "`num_samples` - An integer specifying the number of sample paths that the model generates when making a probabilistic prediction.\n",
    "\n",
    "`output_types` - A list specifying the type of response. `mean` returns a single value for each time point which is the average of `num_samples` samples generated by the model. `quantiles` looks at the list of `num_samples` generated by the model and attempts to generate quantile estimates for each time point based on these values. `samples` returns the list of `num_samples` for each time point in the prediction length. \n",
    "\n",
    "`quantiles` - If your specify `quantiles` as one of your desired output types then this list lets you control which quantiles estimates are generated and returned for in the response.\n",
    "\n",
    "Below is an example of what a JSON query to a DeepAR model endpoint might look like.\n",
    "\n",
    "```json\n",
    "{\n",
    " \"instances\": [\n",
    "  { \"start\": \"2009-11-01 00:00:00\", \"target\": [4.0, 10.0, 50.0, 100.0, 113.0], \"cat\": 0},\n",
    "  { \"start\": \"2012-01-30\", \"target\": [1.0], \"cat\": 2 },\n",
    "  { \"start\": \"1999-01-30\", \"target\": [2.0, 1.0], \"cat\": 1 }\n",
    " ],\n",
    " \"configuration\": {\n",
    "  \"num_samples\": 50,\n",
    "  \"output_types\": [\"mean\", \"quantiles\", \"samples\"],\n",
    "  \"quantiles\": [\"0.5\", \"0.9\"]\n",
    " }\n",
    "}\n",
    "```\n",
    "\n",
    "In the cells below you can try predictions for a sample item with existing time series taken from our testing dataset as well as a prediction for a new \"cold\" item with no existing time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction for item with existing time series sales history\n",
    "\n",
    "start_date = datetime.date.today() - datetime.timedelta(days=int(prediction_length))\n",
    "\n",
    "target = test_set[0]['target'][-int(prediction_length):]\n",
    "\n",
    "cat = test_set[0]['cat']\n",
    "\n",
    "request_json = {\n",
    " \"instances\": [\n",
    "  { \"start\": str(start_date), \"target\": target, \"cat\": cat}\n",
    " ],\n",
    " \"configuration\": {\n",
    "  \"num_samples\": 10,\n",
    "  \"output_types\": [\"quantiles\"],\n",
    "  \"quantiles\": [\"0.9\"]\n",
    " }\n",
    "}\n",
    "\n",
    "payload = json.dumps(request_json).encode('utf-8')\n",
    "\n",
    "response = predictor.predict(payload)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cold-start prediction for new item that has no time series sales history\n",
    "\n",
    "product_category = 'shoe'\n",
    "\n",
    "cat = int(le.transform([product_category])[0])\n",
    "\n",
    "request_json = {\n",
    " \"instances\": [\n",
    "  {\"start\": \"2019-06-12 00:00:00\", \"target\": [], \"cat\": cat} # No target values because this is a new product with no existing time series sales data\n",
    " ],\n",
    " \"configuration\": {\n",
    "  \"num_samples\": 10,\n",
    "  \"output_types\": [\"quantiles\"],\n",
    "  \"quantiles\": [\"0.9\"]\n",
    " }\n",
    "}\n",
    "\n",
    "payload = json.dumps(request_json).encode('utf-8')\n",
    "\n",
    "response = predictor.predict(payload)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
