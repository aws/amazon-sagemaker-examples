{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0d57a8",
   "metadata": {},
   "source": [
    "# Demo: Deploy Models Locally with SageMaker Model Builder in IN_PROCESS Mode\n",
    "\n",
    "This notebook was tested with the `Python 3` kernel on an Amazon SageMaker notebook instance of type `ml.g5.4xlarge`.\n",
    "\n",
    "In this notebook, we demonstrate how customers can deploy a model locally to a FastAPI server without needing to set up a container. This approach enables quicker validation and allows faster iteration before customers proceed with deployment using either local container mode or SageMaker endpoint mode. After successful in-process testing, customers can switch to another mode for further testing.\n",
    "\n",
    "You can either launch this notebook from an Amazon SageMaker notebook instance which handles all credentials automatically, or by running it locally and setting credentials manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a08e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54defff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these to run fast api servers\n",
    "!pip install --quiet torch transformers fastapi uvicorn nest-asyncio \"protobuf==4.23.0\"\n",
    "!pip install -U pyopenssl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b83ba",
   "metadata": {},
   "source": [
    "# [WalkThrough] Define the custom inference code\n",
    "Just tell us how to load your model and how to invoke it. We'll take care of the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e23e5d-c863-4528-a3b8-ec83cd6889e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serve.spec.inference_spec import InferenceSpec\n",
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "\n",
    "class MyInferenceSpec(InferenceSpec):\n",
    "    def load(self, model_dir: str):\n",
    "        return pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "    def invoke(self, input_data, model):\n",
    "        if isinstance(input_data, str):\n",
    "            input_data = json.loads(input_data)\n",
    "        response = model(question=input_data[\"question\"], context=input_data[\"context\"])\n",
    "        return response\n",
    "\n",
    "\n",
    "inf_spec = MyInferenceSpec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a0639-88ca-4aae-94d1-101d63e74501",
   "metadata": {},
   "source": [
    "# [WalkThrough] Start the IN_PROCESS mode server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58184f43-61b4-4299-b873-bc2cf6ff5fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serve import Mode\n",
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "\n",
    "# Expected output: the modelâ€™s answer based on the provided context\n",
    "schema = SchemaBuilder(\n",
    "    {\n",
    "        \"context\": \"The demo is focused on SageMaker and machine learning. It has gone well so far, with no major issues, and the participants are engaged.\",\n",
    "        \"question\": \"What is the demo about?\"\n",
    "    },\n",
    "    {\n",
    "        \"answer\": \"SageMaker and machine learning.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# deploying the model to a fast api server with minimum inputs from user\n",
    "predictor = ModelBuilder(\n",
    "    inference_spec=inf_spec,\n",
    "    schema_builder=schema,\n",
    "    mode=Mode.IN_PROCESS,  # you can change it to Mode.LOCAL_CONTAINER for local container testing\n",
    ").build().deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3bfe83-2e67-4ed7-a697-9ac99ca95360",
   "metadata": {},
   "source": [
    "# [WalkThrough] Now that the server is running, send a prompt and see the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input data for the question-answering model\n",
    "input_data = {\n",
    "    \"question\": \"What is the main topic?\",\n",
    "    \"context\": \"The demo is focused on SageMaker and machine learning. It has gone well so far, with no major issues, and the participants are engaged.\"\n",
    "}\n",
    "\n",
    "# Convert the input data to JSON format and pass it to `predict`\n",
    "response = predictor.predict(input_data)\n",
    "\n",
    "# Check the model's response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b21f62",
   "metadata": {},
   "source": [
    "## [WalkThrough] Cleanup the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2fd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_predictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c073467-4bb7-4719-bebf-718ed6c81bf1",
   "metadata": {},
   "source": [
    "---\n",
    "# Now try it out for yourself\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8665c-9623-4a54-a49d-6a813086636f",
   "metadata": {},
   "source": [
    "### Your custom load and invoke logic here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c93ee-2812-436b-8274-8e4119025ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInferenceSpec(InferenceSpec):\n",
    "    def load(self, model_dir: str):\n",
    "        # your load logic here <---\n",
    "        pass\n",
    "\n",
    "    def invoke(self, input_data, model):\n",
    "        # your invoke logic here <---\n",
    "        pass\n",
    "\n",
    "inf_spec = MyInferenceSpec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73b019-5913-4ae8-93e2-a9aeda9c7db7",
   "metadata": {},
   "source": [
    "### Now deploy it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb0c35-29b7-46b9-bf34-5d809cc87024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve import Mode\n",
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "\n",
    "schema = SchemaBuilder(\n",
    "    {},\n",
    "    {}\n",
    ")\n",
    "\n",
    "predictor = ModelBuilder(\n",
    "    inference_spec=inf_spec,\n",
    "    schema_builder=schema,\n",
    "    mode=Mode.IN_PROCESS,\n",
    ").build().deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103478a-8e22-4ebe-81b1-592933e7188b",
   "metadata": {},
   "source": [
    "### Now invoke it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74be1786-d1a9-4461-80ae-6d2122f39011",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {} # your input data here <---\n",
    "\n",
    "response = predictor.predict(input_data)\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
