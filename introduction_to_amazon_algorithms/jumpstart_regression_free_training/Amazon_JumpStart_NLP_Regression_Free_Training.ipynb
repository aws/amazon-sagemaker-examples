{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf65cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker notebook\n",
    "# Enabling the backward compatibility during the data updates using weight interpolation\n",
    "# This is meant to be a lightweight notebook implementing weight interpolation for reducing regressions in data updates \n",
    "# For Natural Language Processing Models\n",
    "\n",
    "# Consider citing us if you find this notebook useful\n",
    "#@article{Schumann2023BCWI,\n",
    "#  title={Backward Compatibility During Data Updates by Weight Interpolation\n",
    "#},\n",
    "#  author={Raphael Schumann and Elman Mansimov and Yi-An Lai and Nikolaos Pappas and Xibin Gao and Yi Zhang},\n",
    "#  journal={ArXiv},\n",
    "#  year={2023},\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc5bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (4.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (2.8.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (2021.11.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (1.21.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install huggingface transformers and datasets\n",
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cedeb6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import requests\n",
    "import copy\n",
    "\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa298a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared Variables\n",
    "DATA_URL = \"https://raw.githubusercontent.com/amazon-science/regression-constraint-model-upgrade/main/nlp/data/MASSIVE/\"\n",
    "PT_MODEL_NAME = 'roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "243c7ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PreTrainedTokenizer(name_or_path='roberta-base', vocab_size=50265, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = []\n",
    "\n",
    "def load_tokenizer():\n",
    "    if len(tokenizer) == 0:\n",
    "        tokenizer.append(RobertaTokenizer.from_pretrained(PT_MODEL_NAME))\n",
    "\n",
    "load_tokenizer()\n",
    "\n",
    "print (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b1f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Helper tokenize function\"\"\"\n",
    "    return tokenizer[0](examples['text'], padding=False, truncation=True, return_attention_mask=False)\n",
    "\n",
    "def load_dataset(splits, file_template):\n",
    "    \"\"\"Helper function to load MASSIVE dataset\"\"\"\n",
    "    data_files = dict()\n",
    "    for split in splits:\n",
    "        data_files[split] = file_template.format(split)\n",
    "\n",
    "    print (data_files)\n",
    "    dataset = datasets.load_dataset('json', data_files=data_files)\n",
    "    dataset_info = json.loads(requests.get(file_template.format('info')[:-1]).content)\n",
    "\n",
    "    new_label_ids = [dataset_info['labels'].index(c) for c in dataset_info['add_classes']]\n",
    "    old_label_ids = [i for i, c in enumerate(dataset_info['labels']) if c not in dataset_info['add_classes']]\n",
    "    dataset_info['new_label_ids'] = new_label_ids\n",
    "    dataset_info['old_label_ids'] = old_label_ids\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "    return tokenized_dataset, dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf5089b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b127397a3b6c8c0e\n",
      "Found cached dataset json (/home/ec2-user/.cache/huggingface/datasets/json/default-b127397a3b6c8c0e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 'https://raw.githubusercontent.com/amazon-science/regression-constraint-model-upgrade/main/nlp/data/MASSIVE/add_data/old/train.jsonl', 'dev': 'https://raw.githubusercontent.com/amazon-science/regression-constraint-model-upgrade/main/nlp/data/MASSIVE/add_data/old/dev.jsonl', 'test': 'https://raw.githubusercontent.com/amazon-science/regression-constraint-model-upgrade/main/nlp/data/MASSIVE/add_data/old/test.jsonl'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448eea9944e34c0bacbdc06cb6031969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/json/default-b127397a3b6c8c0e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-53a50176acadf523.arrow\n",
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/json/default-b127397a3b6c8c0e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7f8616a06d581f36.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbc1928963248dd9adc0a058f241742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old dataset before update\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "        num_rows: 333\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "})\n",
      "Old dataset before update info\n",
      "{'name': 'MASSIVE', 'splits': ['train', 'dev', 'test'], 'labels': ['audio_volume_other', 'play_music', 'iot_hue_lighton', 'general_greet', 'calendar_set', 'audio_volume_down', 'social_query', 'audio_volume_mute', 'iot_wemo_on', 'iot_hue_lightup', 'audio_volume_up', 'iot_coffee', 'takeaway_query', 'qa_maths', 'play_game', 'cooking_query', 'iot_hue_lightdim', 'iot_wemo_off', 'music_settings', 'weather_query', 'news_query', 'alarm_remove', 'social_post', 'recommendation_events', 'transport_taxi', 'takeaway_order', 'music_query', 'calendar_query', 'lists_query', 'qa_currency', 'recommendation_movies', 'general_joke', 'recommendation_locations', 'email_querycontact', 'lists_remove', 'play_audiobook', 'email_addcontact', 'lists_createoradd', 'play_radio', 'qa_stock', 'alarm_query', 'email_sendemail', 'general_quirky', 'music_likeness', 'cooking_recipe', 'email_query', 'datetime_query', 'transport_traffic', 'play_podcasts', 'iot_hue_lightchange', 'calendar_remove', 'transport_query', 'transport_ticket', 'qa_factoid', 'iot_cleaning', 'alarm_set', 'datetime_convert', 'iot_hue_lightoff', 'qa_definition', 'music_dislikeness'], 'add_classes': ['email_querycontact', 'takeaway_query', 'recommendation_events', 'iot_hue_lightchange', 'iot_hue_lightdim', 'play_game', 'play_podcasts', 'email_sendemail', 'email_query', 'recommendation_movies', 'email_addcontact', 'recommendation_locations', 'takeaway_order'], 'label_distribution': {'audio_volume_other': 0.0013287074629069167, 'play_music': 0.05624861592972614, 'iot_hue_lighton': 0.001993061194360375, 'general_greet': 0.001993061194360375, 'calendar_set': 0.0694618734775227, 'audio_volume_down': 0.004429024876356389, 'social_query': 0.009300952240348416, 'audio_volume_mute': 0.00922713515907581, 'iot_wemo_on': 0.004059939469993356, 'iot_hue_lightup': 0.006495903151989371, 'audio_volume_up': 0.009005683915257991, 'iot_coffee': 0.010186757215619695, 'takeaway_query': 0.010777293865800546, 'qa_maths': 0.00671735439580719, 'play_game': 0.009891488890529269, 'cooking_query': 0.0004429024876356389, 'iot_hue_lightdim': 0.006864988558352402, 'iot_wemo_off': 0.004207573632538569, 'music_settings': 0.0043552077950837825, 'weather_query': 0.051598139809551934, 'news_query': 0.04318299254447479, 'alarm_remove': 0.006791171477079796, 'social_post': 0.02458108806377796, 'recommendation_events': 0.015944489554883, 'transport_taxi': 0.009374769321621023, 'takeaway_order': 0.011441647597254004, 'music_query': 0.013582342954159592, 'calendar_query': 0.049309810290101126, 'lists_query': 0.018306636155606407, 'qa_currency': 0.012844172141433527, 'recommendation_movies': 0.006053000664353731, 'general_joke': 0.006422086070716764, 'recommendation_locations': 0.015058684579611722, 'email_querycontact': 0.010555842621982727, 'lists_remove': 0.014837233335793903, 'play_audiobook': 0.0136561600354322, 'email_addcontact': 0.0043552077950837825, 'lists_createoradd': 0.014911050417066508, 'play_radio': 0.02428581973868753, 'qa_stock': 0.012991806303978741, 'alarm_query': 0.010998745109618365, 'email_sendemail': 0.0307817228906769, 'general_quirky': 0.04871927363992028, 'music_likeness': 0.009522403484166235, 'cooking_recipe': 0.018306636155606407, 'email_query': 0.03624418690484978, 'datetime_query': 0.030560271646859084, 'transport_traffic': 0.0102605742968923, 'play_podcasts': 0.01675647744888167, 'iot_hue_lightchange': 0.010851110947073153, 'calendar_remove': 0.02650033217686573, 'transport_query': 0.019413892374695506, 'transport_ticket': 0.011220196353436185, 'qa_factoid': 0.04680002952683251, 'iot_cleaning': 0.008267513102531926, 'alarm_set': 0.01572303831106518, 'datetime_convert': 0.004502841957628996, 'iot_hue_lightoff': 0.012548903816343103, 'qa_definition': 0.023769100169779286, 'music_dislikeness': 0.0011810733003617038}, 'num_train_old': 1000, 'num_dev_old': 333, 'num_train_add': 500, 'num_dev_add': 167, 'random_seed': 1234, 'scenario': 'add_data', 'version': 'old', 'new_label_ids': [33, 12, 23, 49, 16, 14, 48, 41, 45, 30, 36, 32, 25], 'old_label_ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 29, 31, 34, 35, 37, 38, 39, 40, 42, 43, 44, 46, 47, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]}\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE: Pull the data before update\n",
    "old_dataset_files = os.path.join(DATA_URL, \"add_data\", \"old\", \"{}.jsonl\")\n",
    "old_dataset, old_dataset_info = load_dataset(['train', 'dev', 'test'], old_dataset_files)\n",
    "\n",
    "# should contain 1000 lines in train, 333 lines in dev, and 4000 lines in test\n",
    "print (\"Old dataset before update\")\n",
    "print (old_dataset)\n",
    "\n",
    "print (\"Old dataset before update info\")\n",
    "print (old_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7407800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker region: us-east-1\n",
      "sagemaker session: <sagemaker.session.Session object at 0x7f6e731e6e80>\n",
      "bucket name: sagemaker-us-east-1-339569644176\n",
      "role: arn:aws:iam::339569644176:role/saabm\n",
      "instance type: ml.p3.2xlarge\n"
     ]
    }
   ],
   "source": [
    "## Setup Sagemaker environment\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "import boto3\n",
    "\n",
    "# Use remote mode\n",
    "sagemaker_region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "print (f\"sagemaker region: {sagemaker_region}\")\n",
    "print (f\"sagemaker session: {sagemaker_session}\")\n",
    "print (f\"bucket name: {bucket_name}\")\n",
    "print (f\"role: {role}\")\n",
    "print (f\"instance type: {instance_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44225ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE: Train the model using the data before update\n",
    "# Something along these lines\n",
    "\n",
    "# git configuration to download regression-free training script\n",
    "git_config = {\n",
    "    \"repo\": \"https://github.com/amazon-science/regression-constraint-model-upgrade.git\",\n",
    "    \"branch\": \"main\",\n",
    "}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',\n",
    "        source_dir='nlp',\n",
    "        git_config=git_config,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.17.0',\n",
    "        pytorch_version='1.10.2',\n",
    "        py_version='py38',\n",
    "        hyperparameters = {\n",
    "            \"dataset\": \"MASSIVE\",\n",
    "            \"scenario\": \"add_data\",\n",
    "            \"data_type\": \"old\",\n",
    "            \"bucket_name\": bucket_name\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb97e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/tmp2myvuk5e'...\n",
      "Already on 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is up to date with 'origin/main'.\n",
      "2023-01-03 15:15:21 Starting - Starting the training job...\n",
      "2023-01-03 15:15:50 Starting - Preparing the instances for trainingProfilerReport-1672758921: InProgress\n",
      ".....................\n",
      "2023-01-03 15:19:10 Downloading - Downloading input data...\n",
      "2023-01-03 15:19:50 Training - Downloading the training image......\n",
      "2023-01-03 15:20:50 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-03 15:20:47,526 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-03 15:20:47,552 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-03 15:20:47,554 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-03 15:20:47,946 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bucket_name\": \"sagemaker-us-east-1-339569644176\",\n",
      "        \"data_type\": \"old\",\n",
      "        \"dataset\": \"MASSIVE\",\n",
      "        \"scenario\": \"add_data\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-01-03-15-15-20-630\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-339569644176/huggingface-pytorch-training-2023-01-03-15-15-20-630/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bucket_name\":\"sagemaker-us-east-1-339569644176\",\"data_type\":\"old\",\"dataset\":\"MASSIVE\",\"scenario\":\"add_data\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-339569644176/huggingface-pytorch-training-2023-01-03-15-15-20-630/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bucket_name\":\"sagemaker-us-east-1-339569644176\",\"data_type\":\"old\",\"dataset\":\"MASSIVE\",\"scenario\":\"add_data\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"huggingface-pytorch-training-2023-01-03-15-15-20-630\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-339569644176/huggingface-pytorch-training-2023-01-03-15-15-20-630/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bucket_name\",\"sagemaker-us-east-1-339569644176\",\"--data_type\",\"old\",\"--dataset\",\"MASSIVE\",\"--scenario\",\"add_data\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BUCKET_NAME=sagemaker-us-east-1-339569644176\u001b[0m\n",
      "\u001b[34mSM_HP_DATA_TYPE=old\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET=MASSIVE\u001b[0m\n",
      "\u001b[34mSM_HP_SCENARIO=add_data\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --bucket_name sagemaker-us-east-1-339569644176 --data_type old --dataset MASSIVE --scenario add_data\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/1.41k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 3.19kB [00:00, 2.57MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/878k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 878k/878k [00:00<00:00, 56.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/446k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 446k/446k [00:00<00:00, 56.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 481/481 [00:00<00:00, 613kB/s]\u001b[0m\n",
      "\u001b[34mseed 1111\u001b[0m\n",
      "\u001b[34mdataset_name MASSIVE\u001b[0m\n",
      "\u001b[34mdata_update_scenario add_data\u001b[0m\n",
      "\u001b[34mpretrained_model_name roberta-base\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-ad9d631d74cfc8dc\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-ad9d631d74cfc8dc/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 3/3 [00:00<00:00, 6894.75it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 3/3 [00:00<00:00, 1310.04it/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-ad9d631d74cfc8dc/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 3/3 [00:00<00:00, 823.97it/s]\u001b[0m\n",
      "\u001b[34mParameter 'function'=<function tokenize_function at 0x7fe7b7c2e820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m233ex [00:00, 2320.19ex/s]\u001b[0m\n",
      "\u001b[34m478ex [00:00, 2393.35ex/s]\u001b[0m\n",
      "\u001b[34m744ex [00:00, 2513.91ex/s]\u001b[0m\n",
      "\u001b[34m1000ex [00:00, 2421.79ex/s]\u001b[0m\n",
      "\u001b[34m1000ex [00:00, 2424.42ex/s]\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m259ex [00:00, 2581.81ex/s]\u001b[0m\n",
      "\u001b[34m333ex [00:00, 2524.50ex/s]\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m236ex [00:00, 2355.96ex/s]\u001b[0m\n",
      "\u001b[34m486ex [00:00, 2436.29ex/s]\u001b[0m\n",
      "\u001b[34m778ex [00:00, 2652.36ex/s]\u001b[0m\n",
      "\u001b[34m1044ex [00:00, 2478.85ex/s]\u001b[0m\n",
      "\u001b[34m1331ex [00:00, 2612.89ex/s]\u001b[0m\n",
      "\u001b[34m1610ex [00:00, 2671.19ex/s]\u001b[0m\n",
      "\u001b[34m1879ex [00:00, 2649.88ex/s]\u001b[0m\n",
      "\u001b[34m2154ex [00:00, 2678.95ex/s]\u001b[0m\n",
      "\u001b[34m2442ex [00:00, 2739.44ex/s]\u001b[0m\n",
      "\u001b[34m2736ex [00:01, 2799.65ex/s]\u001b[0m\n",
      "\u001b[34m3017ex [00:01, 2801.01ex/s]\u001b[0m\n",
      "\u001b[34m3313ex [00:01, 2847.18ex/s]\u001b[0m\n",
      "\u001b[34m3615ex [00:01, 2898.44ex/s]\u001b[0m\n",
      "\u001b[34m3912ex [00:01, 2918.28ex/s]\u001b[0m\n",
      "\u001b[34m4000ex [00:01, 2741.41ex/s]\u001b[0m\n",
      "\u001b[34mtrain_dataset Dataset({\n",
      "    features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "    num_rows: 1000\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34mdev_dataset\u001b[0m\n",
      "\u001b[34mDataset({\n",
      "    features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "    num_rows: 333\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34mtest_dataset Dataset({\n",
      "    features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "    num_rows: 4000\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34mInitializing new RoBERTa classifier\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/478M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|▏         | 6.62M/478M [00:00<00:07, 69.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 14.1M/478M [00:00<00:06, 74.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 21.3M/478M [00:00<00:06, 75.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 28.8M/478M [00:00<00:06, 76.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 36.3M/478M [00:00<00:05, 77.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 43.9M/478M [00:00<00:05, 78.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 51.5M/478M [00:00<00:05, 78.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 59.1M/478M [00:00<00:05, 78.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 66.7M/478M [00:00<00:05, 79.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 74.2M/478M [00:01<00:05, 79.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 81.8M/478M [00:01<00:05, 74.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▊        | 88.9M/478M [00:01<00:05, 71.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|██        | 95.8M/478M [00:01<00:05, 69.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██▏       | 102M/478M [00:01<00:05, 68.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 110M/478M [00:01<00:05, 70.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 117M/478M [00:01<00:05, 72.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 124M/478M [00:01<00:05, 72.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 131M/478M [00:01<00:04, 73.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 139M/478M [00:01<00:04, 75.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 146M/478M [00:02<00:04, 74.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 153M/478M [00:02<00:04, 74.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▎      | 161M/478M [00:02<00:04, 76.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▌      | 168M/478M [00:02<00:04, 76.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 176M/478M [00:02<00:04, 77.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 183M/478M [00:02<00:04, 75.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 191M/478M [00:02<00:03, 76.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████▏     | 198M/478M [00:02<00:03, 77.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 206M/478M [00:02<00:03, 77.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 213M/478M [00:02<00:03, 78.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 221M/478M [00:03<00:03, 78.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 228M/478M [00:03<00:03, 78.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 236M/478M [00:03<00:03, 74.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 243M/478M [00:03<00:03, 75.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 251M/478M [00:03<00:03, 76.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 258M/478M [00:03<00:02, 76.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 266M/478M [00:03<00:02, 77.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 274M/478M [00:03<00:02, 78.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 281M/478M [00:03<00:02, 78.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 289M/478M [00:03<00:02, 78.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 296M/478M [00:04<00:02, 78.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▎   | 304M/478M [00:04<00:02, 79.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▌   | 311M/478M [00:04<00:02, 78.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 319M/478M [00:04<00:02, 79.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 326M/478M [00:04<00:02, 79.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 334M/478M [00:04<00:01, 79.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████▏  | 342M/478M [00:04<00:01, 79.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 349M/478M [00:04<00:01, 79.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▍  | 357M/478M [00:04<00:01, 79.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 364M/478M [00:04<00:01, 79.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 372M/478M [00:05<00:01, 77.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 379M/478M [00:05<00:01, 76.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 387M/478M [00:05<00:01, 76.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 394M/478M [00:05<00:01, 76.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 401M/478M [00:05<00:01, 76.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 409M/478M [00:05<00:00, 77.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 416M/478M [00:05<00:00, 77.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▊ | 424M/478M [00:05<00:00, 77.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 431M/478M [00:05<00:00, 77.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 439M/478M [00:06<00:00, 77.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 446M/478M [00:06<00:00, 74.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 453M/478M [00:06<00:00, 74.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▋| 461M/478M [00:06<00:00, 75.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 468M/478M [00:06<00:00, 75.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 475M/478M [00:06<00:00, 75.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 478M/478M [00:06<00:00, 76.4MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mconfig: {'num_epochs': 16, 'batch_size': 16, 'eval_batch_size': 256, 'warmup_ratio': 0.1, 'learning_rate': 6e-05, 'weight_decay': 0.01, 'prior_wd': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'adam_epsilon': 1e-06, 'max_grad_norm': 5.0, 'seed': 1111}\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:21:08.034 algo-1:42 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:21:08.196 algo-1:42 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:21:08.199 algo-1:42 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:21:08.199 algo-1:42 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:21:08.200 algo-1:42 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:21:08.200 algo-1:42 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mdev ACC: 0.901\u001b[0m\n",
      "\u001b[34mdev loss: 4.074\u001b[0m\n",
      "\u001b[34mepoch 1\u001b[0m\n",
      "\u001b[34mtrain loss: 3.96041\u001b[0m\n",
      "\u001b[34mdev ACC: 15.616\u001b[0m\n",
      "\u001b[34mdev loss: 3.589\u001b[0m\n",
      "\u001b[34mepoch time: 5.9 sec\u001b[0m\n",
      "\u001b[34mepoch 2\u001b[0m\n",
      "\u001b[34mtrain loss: 2.90233\u001b[0m\n",
      "\u001b[34mdev ACC: 57.958\u001b[0m\n",
      "\u001b[34mdev loss: 2.054\u001b[0m\n",
      "\u001b[34mepoch time: 5.7 sec\u001b[0m\n",
      "\u001b[34mepoch 3\u001b[0m\n",
      "\u001b[34mtrain loss: 1.61592\u001b[0m\n",
      "\u001b[34mdev ACC: 69.970\u001b[0m\n",
      "\u001b[34mdev loss: 1.388\u001b[0m\n",
      "\u001b[34mepoch time: 6.0 sec\u001b[0m\n",
      "\u001b[34mepoch 4\u001b[0m\n",
      "\u001b[34mtrain loss: 0.93890\u001b[0m\n",
      "\u001b[34mdev ACC: 75.375\u001b[0m\n",
      "\u001b[34mdev loss: 1.129\u001b[0m\n",
      "\u001b[34mepoch time: 5.9 sec\u001b[0m\n",
      "\u001b[34mepoch 5\u001b[0m\n",
      "\u001b[34mtrain loss: 0.55037\u001b[0m\n",
      "\u001b[34mdev ACC: 75.676\u001b[0m\n",
      "\u001b[34mdev loss: 1.048\u001b[0m\n",
      "\u001b[34mepoch time: 5.8 sec\u001b[0m\n",
      "\u001b[34mepoch 6\u001b[0m\n",
      "\u001b[34mtrain loss: 0.31683\u001b[0m\n",
      "\u001b[34mdev ACC: 77.778\u001b[0m\n",
      "\u001b[34mdev loss: 1.052\u001b[0m\n",
      "\u001b[34mepoch time: 5.9 sec\u001b[0m\n",
      "\u001b[34mepoch 7\u001b[0m\n",
      "\u001b[34mtrain loss: 0.19679\u001b[0m\n",
      "\u001b[34mdev ACC: 79.880\u001b[0m\n",
      "\u001b[34mdev loss: 0.984\u001b[0m\n",
      "\u001b[34mepoch time: 5.8 sec\u001b[0m\n",
      "\u001b[34mepoch 8\u001b[0m\n",
      "\u001b[34mtrain loss: 0.12617\u001b[0m\n",
      "\u001b[34mdev ACC: 80.180\u001b[0m\n",
      "\u001b[34mdev loss: 1.018\u001b[0m\n",
      "\u001b[34mepoch time: 6.0 sec\u001b[0m\n",
      "\u001b[34mepoch 9\u001b[0m\n",
      "\u001b[34mtrain loss: 0.08579\u001b[0m\n",
      "\u001b[34mdev ACC: 78.378\u001b[0m\n",
      "\u001b[34mdev loss: 1.045\u001b[0m\n",
      "\u001b[34mepoch time: 5.9 sec\u001b[0m\n",
      "\u001b[34mepoch 10\u001b[0m\n",
      "\u001b[34mtrain loss: 0.04957\u001b[0m\n",
      "\u001b[34mdev ACC: 79.279\u001b[0m\n",
      "\u001b[34mdev loss: 1.078\u001b[0m\n",
      "\u001b[34mepoch time: 6.1 sec\u001b[0m\n",
      "\u001b[34mepoch 11\u001b[0m\n",
      "\u001b[34mtrain loss: 0.03312\u001b[0m\n",
      "\u001b[34mdev ACC: 79.880\u001b[0m\n",
      "\u001b[34mdev loss: 1.082\u001b[0m\n",
      "\u001b[34mepoch time: 5.8 sec\u001b[0m\n",
      "\u001b[34mepoch 12\u001b[0m\n",
      "\u001b[34mtrain loss: 0.02625\u001b[0m\n",
      "\u001b[34mdev ACC: 79.580\u001b[0m\n",
      "\u001b[34mdev loss: 1.123\u001b[0m\n",
      "\u001b[34mepoch time: 5.7 sec\u001b[0m\n",
      "\u001b[34mepoch 13\u001b[0m\n",
      "\u001b[34mtrain loss: 0.02037\u001b[0m\n",
      "\u001b[34mdev ACC: 79.880\u001b[0m\n",
      "\u001b[34mdev loss: 1.130\u001b[0m\n",
      "\u001b[34mepoch time: 5.7 sec\u001b[0m\n",
      "\u001b[34mepoch 14\u001b[0m\n",
      "\u001b[34mtrain loss: 0.01755\u001b[0m\n",
      "\u001b[34mdev ACC: 80.180\u001b[0m\n",
      "\u001b[34mdev loss: 1.139\u001b[0m\n",
      "\u001b[34mepoch time: 5.5 sec\u001b[0m\n",
      "\u001b[34mepoch 15\u001b[0m\n",
      "\u001b[34mtrain loss: 0.01368\u001b[0m\n",
      "\u001b[34mdev ACC: 79.880\u001b[0m\n",
      "\u001b[34mdev loss: 1.162\u001b[0m\n",
      "\u001b[34mepoch time: 5.7 sec\u001b[0m\n",
      "\u001b[34mepoch 16\u001b[0m\n",
      "\u001b[34mtrain loss: 0.01253\u001b[0m\n",
      "\u001b[34mdev ACC: 80.180\u001b[0m\n",
      "\u001b[34mdev loss: 1.166\u001b[0m\n",
      "\u001b[34mepoch time: 5.8 sec\u001b[0m\n",
      "\u001b[34mtrain time: 98 sec\u001b[0m\n",
      "\u001b[34msaved hparams to: ./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model/hparams.json\u001b[0m\n",
      "\u001b[34mmodel save to: ./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model\u001b[0m\n",
      "\u001b[34mWARNING:datasets.builder:Using custom data configuration default-709a397872e7b851\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-709a397872e7b851/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 3/3 [00:00<00:00, 8004.40it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 3/3 [00:00<00:00, 1569.72it/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-709a397872e7b851/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 3/3 [00:00<00:00, 864.80it/s]\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m310ex [00:00, 3092.37ex/s]\u001b[0m\n",
      "\u001b[34m623ex [00:00, 3110.56ex/s]\u001b[0m\n",
      "\u001b[34m938ex [00:00, 3127.64ex/s]\u001b[0m\n",
      "\u001b[34m1251ex [00:00, 3007.88ex/s]\u001b[0m\n",
      "\u001b[34m1500ex [00:00, 3037.62ex/s]\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m313ex [00:00, 3119.64ex/s]\u001b[0m\n",
      "\u001b[34m500ex [00:00, 3090.46ex/s]\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m266ex [00:00, 2654.41ex/s]\u001b[0m\n",
      "\u001b[34m582ex [00:00, 2945.82ex/s]\u001b[0m\n",
      "\u001b[34m899ex [00:00, 3045.18ex/s]\u001b[0m\n",
      "\u001b[34m1204ex [00:00, 2832.19ex/s]\u001b[0m\n",
      "\u001b[34m1492ex [00:00, 2846.76ex/s]\u001b[0m\n",
      "\u001b[34m1807ex [00:00, 2943.43ex/s]\u001b[0m\n",
      "\u001b[34m2107ex [00:00, 2960.48ex/s]\u001b[0m\n",
      "\u001b[34m2421ex [00:00, 3015.63ex/s]\u001b[0m\n",
      "\u001b[34m2738ex [00:00, 3063.01ex/s]\u001b[0m\n",
      "\u001b[34m3045ex [00:01, 3055.17ex/s]\u001b[0m\n",
      "\u001b[34m3354ex [00:01, 3063.88ex/s]\u001b[0m\n",
      "\u001b[34m3673ex [00:01, 3100.74ex/s]\u001b[0m\n",
      "\u001b[34m3990ex [00:01, 3119.14ex/s]\u001b[0m\n",
      "\u001b[34m4000ex [00:01, 3006.54ex/s]\u001b[0m\n",
      "\u001b[34mrun eval for train data\u001b[0m\n",
      "\u001b[34mrun eval for dev data\u001b[0m\n",
      "\u001b[34mrun eval for test data\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[34m{'train': {'ACC': 93.66666666666667, 'loss': 0.3388156917889913, 'ACC_OLD': 100.0, 'ACC_NEW': 81.0}, 'dev': {'ACC': 82.0, 'loss': 1.0802746863365174, 'ACC_OLD': 80.18018018018019, 'ACC_NEW': 85.62874251497006}, 'test': {'ACC': 81.975, 'loss': 1.021627191543579, 'ACC_OLD': 81.975, 'ACC_NEW': nan}}\u001b[0m\n",
      "\u001b[34msaved metrics to: ./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/metrics.json\u001b[0m\n",
      "\u001b[34msaved logits to: ./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/logits.pickle\u001b[0m\n",
      "\u001b[34mbucket name sagemaker-us-east-1-339569644176\u001b[0m\n",
      "\u001b[34mdir ./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model\u001b[0m\n",
      "\n",
      "2023-01-03 15:23:01 Uploading - Uploading generated training model\u001b[34muploaded ./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model to s3 bucket name sagemaker-us-east-1-339569644176\u001b[0m\n",
      "\u001b[34mtrain_metrics {'ACC': 93.66666666666667, 'loss': 0.3388156917889913, 'ACC_OLD': 100.0, 'ACC_NEW': 81.0}\u001b[0m\n",
      "\u001b[34mdev_metrics {'ACC': 82.0, 'loss': 1.0802746863365174, 'ACC_OLD': 80.18018018018019, 'ACC_NEW': 85.62874251497006}\u001b[0m\n",
      "\u001b[34mtest_metrics\u001b[0m\n",
      "\u001b[34m{'ACC': 81.975, 'loss': 1.021627191543579, 'ACC_OLD': 81.975, 'ACC_NEW': nan}\u001b[0m\n",
      "\u001b[34mfinished old models\u001b[0m\n",
      "\u001b[34m2023-01-03 15:22:54,876 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-03 15:22:54,876 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-03 15:22:54,877 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-03 15:23:31 Completed - Training job completed\n",
      "ProfilerReport-1672758921: NoIssuesFound\n",
      "Training seconds: 238\n",
      "Billable seconds: 238\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef60f780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from S3 ./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model\n",
      "Downloaded old_model_dir/config.json\n",
      "Downloaded old_model_dir/hparams.json\n",
      "Downloaded old_model_dir/pytorch_model.bin\n",
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=60, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE: Load old model from S3 \n",
    "# Used for inference and calculating negative flip rate\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "load_from_s3 = f\"./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model\"\n",
    "\n",
    "print (f\"Loading from S3 {load_from_s3}\")\n",
    "os.makedirs(\"old_model_dir\", exist_ok=True)\n",
    "# load stuff from s3\n",
    "with open(\"old_model_dir/config.json\", \"wb\") as f:\n",
    "    s3.download_fileobj(bucket_name, os.path.join(load_from_s3, \"config.json\"), f)\n",
    "    print(\"Downloaded old_model_dir/config.json\")\n",
    "with open(\"old_model_dir/hparams.json\", \"wb\") as f:\n",
    "    s3.download_fileobj(bucket_name, os.path.join(load_from_s3, \"hparams.json\"), f)\n",
    "    print(\"Downloaded old_model_dir/hparams.json\")\n",
    "with open(\"old_model_dir/pytorch_model.bin\", \"wb\") as f:\n",
    "    s3.download_fileobj(bucket_name, os.path.join(load_from_s3, \"pytorch_model.bin\"), f)\n",
    "    print(\"Downloaded old_model_dir/pytorch_model.bin\")\n",
    "\n",
    "from transformers import RobertaForSequenceClassification\n",
    "old_model = RobertaForSequenceClassification.from_pretrained(\"old_model_dir\")\n",
    "\n",
    "print (old_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0c0e5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 'https://raw.githubusercontent.com/amazon-science/regression-constraint-model-upgrade/main/nlp/data/MASSIVE/add_data/updated/train.jsonl', 'dev': 'https://raw.githubusercontent.com/amazon-science/regression-constraint-model-upgrade/main/nlp/data/MASSIVE/add_data/updated/dev.jsonl', 'test': 'https://raw.githubusercontent.com/amazon-science/regression-constraint-model-upgrade/main/nlp/data/MASSIVE/add_data/updated/test.jsonl'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-677ff51009bd10bc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/ec2-user/.cache/huggingface/datasets/json/default-677ff51009bd10bc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67860eaaffcd4f4791d055d674f050d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e006d55fb5b423689a7d07f1070c1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/35.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe208346d9854be793ac50318c919de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72096eb0f1e247a3bf654f7d49205826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/94.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b056dd932a465fb55f5d529808f37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad31a56cc1744ebf969f4c18a611587b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a8febba5ba45e2821efca53f375368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b2adf44ba840e9afdd593a122f5e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/json/default-677ff51009bd10bc/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801c445dec374ec89932a80e87ee34e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a83a9129174778a72bfac4291e28a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa39677d1f64b14965de69ddd8a7960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efcaa78b08b47b5bc57a246ce7fee68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset after update\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "})\n",
      "New dataset after update info\n",
      "{'name': 'MASSIVE', 'splits': ['train', 'dev', 'test'], 'labels': ['audio_volume_other', 'play_music', 'iot_hue_lighton', 'general_greet', 'calendar_set', 'audio_volume_down', 'social_query', 'audio_volume_mute', 'iot_wemo_on', 'iot_hue_lightup', 'audio_volume_up', 'iot_coffee', 'takeaway_query', 'qa_maths', 'play_game', 'cooking_query', 'iot_hue_lightdim', 'iot_wemo_off', 'music_settings', 'weather_query', 'news_query', 'alarm_remove', 'social_post', 'recommendation_events', 'transport_taxi', 'takeaway_order', 'music_query', 'calendar_query', 'lists_query', 'qa_currency', 'recommendation_movies', 'general_joke', 'recommendation_locations', 'email_querycontact', 'lists_remove', 'play_audiobook', 'email_addcontact', 'lists_createoradd', 'play_radio', 'qa_stock', 'alarm_query', 'email_sendemail', 'general_quirky', 'music_likeness', 'cooking_recipe', 'email_query', 'datetime_query', 'transport_traffic', 'play_podcasts', 'iot_hue_lightchange', 'calendar_remove', 'transport_query', 'transport_ticket', 'qa_factoid', 'iot_cleaning', 'alarm_set', 'datetime_convert', 'iot_hue_lightoff', 'qa_definition', 'music_dislikeness'], 'add_classes': ['email_querycontact', 'takeaway_query', 'recommendation_events', 'iot_hue_lightchange', 'iot_hue_lightdim', 'play_game', 'play_podcasts', 'email_sendemail', 'takeaway_order', 'email_query', 'recommendation_movies', 'email_addcontact', 'recommendation_locations'], 'label_distribution': {'audio_volume_other': 0.0013287074629069167, 'play_music': 0.05624861592972614, 'iot_hue_lighton': 0.001993061194360375, 'general_greet': 0.001993061194360375, 'calendar_set': 0.0694618734775227, 'audio_volume_down': 0.004429024876356389, 'social_query': 0.009300952240348416, 'audio_volume_mute': 0.00922713515907581, 'iot_wemo_on': 0.004059939469993356, 'iot_hue_lightup': 0.006495903151989371, 'audio_volume_up': 0.009005683915257991, 'iot_coffee': 0.010186757215619695, 'takeaway_query': 0.010777293865800546, 'qa_maths': 0.00671735439580719, 'play_game': 0.009891488890529269, 'cooking_query': 0.0004429024876356389, 'iot_hue_lightdim': 0.006864988558352402, 'iot_wemo_off': 0.004207573632538569, 'music_settings': 0.0043552077950837825, 'weather_query': 0.051598139809551934, 'news_query': 0.04318299254447479, 'alarm_remove': 0.006791171477079796, 'social_post': 0.02458108806377796, 'recommendation_events': 0.015944489554883, 'transport_taxi': 0.009374769321621023, 'takeaway_order': 0.011441647597254004, 'music_query': 0.013582342954159592, 'calendar_query': 0.049309810290101126, 'lists_query': 0.018306636155606407, 'qa_currency': 0.012844172141433527, 'recommendation_movies': 0.006053000664353731, 'general_joke': 0.006422086070716764, 'recommendation_locations': 0.015058684579611722, 'email_querycontact': 0.010555842621982727, 'lists_remove': 0.014837233335793903, 'play_audiobook': 0.0136561600354322, 'email_addcontact': 0.0043552077950837825, 'lists_createoradd': 0.014911050417066508, 'play_radio': 0.02428581973868753, 'qa_stock': 0.012991806303978741, 'alarm_query': 0.010998745109618365, 'email_sendemail': 0.0307817228906769, 'general_quirky': 0.04871927363992028, 'music_likeness': 0.009522403484166235, 'cooking_recipe': 0.018306636155606407, 'email_query': 0.03624418690484978, 'datetime_query': 0.030560271646859084, 'transport_traffic': 0.0102605742968923, 'play_podcasts': 0.01675647744888167, 'iot_hue_lightchange': 0.010851110947073153, 'calendar_remove': 0.02650033217686573, 'transport_query': 0.019413892374695506, 'transport_ticket': 0.011220196353436185, 'qa_factoid': 0.04680002952683251, 'iot_cleaning': 0.008267513102531926, 'alarm_set': 0.01572303831106518, 'datetime_convert': 0.004502841957628996, 'iot_hue_lightoff': 0.012548903816343103, 'qa_definition': 0.023769100169779286, 'music_dislikeness': 0.0011810733003617038}, 'num_train_old': 1000, 'num_dev_old': 333, 'num_train_add': 500, 'num_dev_add': 167, 'random_seed': 1234, 'train_add_start': 1000, 'dev_add_start': 333, 'scenario': 'add_data', 'version': 'updated', 'new_label_ids': [33, 12, 23, 49, 16, 14, 48, 41, 25, 45, 30, 36, 32], 'old_label_ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 29, 31, 34, 35, 37, 38, 39, 40, 42, 43, 44, 46, 47, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]}\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE: Pull the data after update\n",
    "new_dataset_files = os.path.join(DATA_URL, \"add_data\", \"updated\", \"{}.jsonl\")\n",
    "new_dataset, new_dataset_info = load_dataset(['train', 'dev', 'test'], new_dataset_files)\n",
    "\n",
    "# should contain 1500 lines in train, 500 lines in dev, and 4000 lines in test\n",
    "print (\"New dataset after update\")\n",
    "print (new_dataset)\n",
    "\n",
    "print (\"New dataset after update info\")\n",
    "print (new_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4842c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE: Get the old data checkpoint and continue training on the new data \n",
    "\n",
    "# git configuration to download regression-free training script\n",
    "git_config = {\n",
    "    \"repo\": \"https://github.com/amazon-science/regression-constraint-model-upgrade.git\",\n",
    "    \"branch\": \"main\",\n",
    "}\n",
    "\n",
    "huggingface_estimator_new = HuggingFace(\n",
    "        entry_point='train.py',\n",
    "        source_dir='nlp',\n",
    "        git_config=git_config,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.17.0',\n",
    "        pytorch_version='1.10.2',\n",
    "        py_version='py38',\n",
    "        hyperparameters = {\n",
    "            \"dataset\": \"MASSIVE\",\n",
    "            \"scenario\": \"add_data\",\n",
    "            \"data_type\": \"updated\",\n",
    "            \"load_from_s3\": f\"./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model\",\n",
    "            \"bucket_name\": bucket_name,\n",
    "            \"output_dir\": \"add_data\",\n",
    "            \"num_epochs\": 3\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3467d721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/tmpqclqbr5t'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is up to date with 'origin/main'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Already on 'main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 15:31:11 Starting - Starting the training job...\n",
      "2023-01-03 15:31:36 Starting - Preparing the instances for trainingProfilerReport-1672759870: InProgress\n",
      ".....................\n",
      "2023-01-03 15:34:55 Downloading - Downloading input data...\n",
      "2023-01-03 15:35:35 Training - Downloading the training image......\n",
      "2023-01-03 15:36:36 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-03 15:36:36,193 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-03 15:36:36,219 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-03 15:36:36,222 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-03 15:36:36,586 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bucket_name\": \"sagemaker-us-east-1-339569644176\",\n",
      "        \"data_type\": \"updated\",\n",
      "        \"dataset\": \"MASSIVE\",\n",
      "        \"load_from_s3\": \"./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model\",\n",
      "        \"num_epochs\": 3,\n",
      "        \"output_dir\": \"add_data\",\n",
      "        \"scenario\": \"add_data\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-01-03-15-31-09-820\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-339569644176/huggingface-pytorch-training-2023-01-03-15-31-09-820/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bucket_name\":\"sagemaker-us-east-1-339569644176\",\"data_type\":\"updated\",\"dataset\":\"MASSIVE\",\"load_from_s3\":\"./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model\",\"num_epochs\":3,\"output_dir\":\"add_data\",\"scenario\":\"add_data\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-339569644176/huggingface-pytorch-training-2023-01-03-15-31-09-820/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bucket_name\":\"sagemaker-us-east-1-339569644176\",\"data_type\":\"updated\",\"dataset\":\"MASSIVE\",\"load_from_s3\":\"./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model\",\"num_epochs\":3,\"output_dir\":\"add_data\",\"scenario\":\"add_data\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"huggingface-pytorch-training-2023-01-03-15-31-09-820\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-339569644176/huggingface-pytorch-training-2023-01-03-15-31-09-820/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bucket_name\",\"sagemaker-us-east-1-339569644176\",\"--data_type\",\"updated\",\"--dataset\",\"MASSIVE\",\"--load_from_s3\",\"./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model\",\"--num_epochs\",\"3\",\"--output_dir\",\"add_data\",\"--scenario\",\"add_data\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_BUCKET_NAME=sagemaker-us-east-1-339569644176\u001b[0m\n",
      "\u001b[34mSM_HP_DATA_TYPE=updated\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET=MASSIVE\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_FROM_S3=./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=add_data\u001b[0m\n",
      "\u001b[34mSM_HP_SCENARIO=add_data\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --bucket_name sagemaker-us-east-1-339569644176 --data_type updated --dataset MASSIVE --load_from_s3 ./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model --num_epochs 3 --output_dir add_data --scenario add_data\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/1.41k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 3.19kB [00:00, 2.04MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/878k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 878k/878k [00:00<00:00, 53.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/446k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 446k/446k [00:00<00:00, 48.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/481 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 481/481 [00:00<00:00, 559kB/s]\u001b[0m\n",
      "\u001b[34mseed 1111\u001b[0m\n",
      "\u001b[34mdataset_name MASSIVE\u001b[0m\n",
      "\u001b[34mdata_update_scenario add_data\u001b[0m\n",
      "\u001b[34mpretrained_model_name roberta-base\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-6084dd82fabb7d7b\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-6084dd82fabb7d7b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 3/3 [00:00<00:00, 8029.94it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 3/3 [00:00<00:00, 1611.95it/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-6084dd82fabb7d7b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 3/3 [00:00<00:00, 821.77it/s]\u001b[0m\n",
      "\u001b[34mParameter 'function'=<function tokenize_function at 0x7f9b959e01f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m235ex [00:00, 2345.46ex/s]\u001b[0m\n",
      "\u001b[34m486ex [00:00, 2438.19ex/s]\u001b[0m\n",
      "\u001b[34m756ex [00:00, 2553.16ex/s]\u001b[0m\n",
      "\u001b[34m1012ex [00:00, 2470.11ex/s]\u001b[0m\n",
      "\u001b[34m1294ex [00:00, 2593.24ex/s]\u001b[0m\n",
      "\u001b[34m1500ex [00:00, 2582.06ex/s]\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m276ex [00:00, 2757.50ex/s]\u001b[0m\n",
      "\u001b[34m500ex [00:00, 2817.12ex/s]\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m289ex [00:00, 2882.52ex/s]\u001b[0m\n",
      "\u001b[34m578ex [00:00, 2867.56ex/s]\u001b[0m\n",
      "\u001b[34m872ex [00:00, 2899.32ex/s]\u001b[0m\n",
      "\u001b[34m1162ex [00:00, 2846.48ex/s]\u001b[0m\n",
      "\u001b[34m1456ex [00:00, 2878.86ex/s]\u001b[0m\n",
      "\u001b[34m1747ex [00:00, 2886.44ex/s]\u001b[0m\n",
      "\u001b[34m2036ex [00:00, 2874.38ex/s]\u001b[0m\n",
      "\u001b[34m2327ex [00:00, 2884.04ex/s]\u001b[0m\n",
      "\u001b[34m2620ex [00:00, 2898.23ex/s]\u001b[0m\n",
      "\u001b[34m2918ex [00:01, 2921.57ex/s]\u001b[0m\n",
      "\u001b[34m3211ex [00:01, 2893.97ex/s]\u001b[0m\n",
      "\u001b[34m3509ex [00:01, 2918.93ex/s]\u001b[0m\n",
      "\u001b[34m3808ex [00:01, 2937.93ex/s]\u001b[0m\n",
      "\u001b[34m4000ex [00:01, 2888.53ex/s]\u001b[0m\n",
      "\u001b[34mtrain_dataset Dataset({\n",
      "    features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "    num_rows: 1500\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34mdev_dataset Dataset({\n",
      "    features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "    num_rows: 500\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34mtest_dataset\u001b[0m\n",
      "\u001b[34mDataset({\n",
      "    features: ['id', 'partition', 'label', 'text', 'label_name', 'input_ids'],\n",
      "    num_rows: 4000\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34mLoading from S3 ./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model\u001b[0m\n",
      "\u001b[34mDownloaded old_model_dir/config.json\u001b[0m\n",
      "\u001b[34mDownloaded old_model_dir/hparams.json\u001b[0m\n",
      "\u001b[34mDownloaded old_model_dir/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mInitialized RoBERTA from s3 ./bcwi_nlp_outputs/v1/MASSIVE/1111/add_data/old_model/model\u001b[0m\n",
      "\u001b[34mconfig: {'num_epochs': 3, 'batch_size': 16, 'eval_batch_size': 256, 'warmup_ratio': 0.1, 'learning_rate': 6e-05, 'weight_decay': 0.01, 'prior_wd': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.98, 'adam_epsilon': 1e-06, 'max_grad_norm': 5.0, 'seed': 1111}\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:36:53.125 algo-1:42 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:36:53.384 algo-1:42 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:36:53.386 algo-1:42 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:36:53.386 algo-1:42 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:36:53.387 algo-1:42 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-03 15:36:53.387 algo-1:42 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mdev ACC: 82.000\u001b[0m\n",
      "\u001b[34mdev loss: 1.080\u001b[0m\n",
      "\u001b[34mepoch 1\u001b[0m\n",
      "\u001b[34mtrain loss: 0.37180\u001b[0m\n",
      "\u001b[34mdev ACC: 82.000\u001b[0m\n",
      "\u001b[34mdev loss: 0.928\u001b[0m\n",
      "\u001b[34mepoch time: 9.3 sec\u001b[0m\n",
      "\u001b[34mepoch 2\u001b[0m\n",
      "\u001b[34mtrain loss: 0.17650\u001b[0m\n",
      "\u001b[34mdev ACC: 83.000\u001b[0m\n",
      "\u001b[34mdev loss: 0.904\u001b[0m\n",
      "\u001b[34mepoch time: 9.0 sec\u001b[0m\n",
      "\u001b[34mepoch 3\u001b[0m\n",
      "\u001b[34mtrain loss: 0.08963\u001b[0m\n",
      "\u001b[34mdev ACC: 83.200\u001b[0m\n",
      "\u001b[34mdev loss: 0.876\u001b[0m\n",
      "\u001b[34mepoch time: 9.4 sec\u001b[0m\n",
      "\u001b[34mtrain time: 33 sec\u001b[0m\n",
      "\u001b[34msaved hparams to: ./bcwi_nlp_outputs/add_data/MASSIVE/1111/add_data/old_model/model/hparams.json\u001b[0m\n",
      "\u001b[34mmodel save to: ./bcwi_nlp_outputs/add_data/MASSIVE/1111/add_data/old_model/model\u001b[0m\n",
      "\u001b[34mWARNING:datasets.builder:Using custom data configuration default-6084dd82fabb7d7b\u001b[0m\n",
      "\u001b[34mWARNING:datasets.builder:Reusing dataset json (/root/.cache/huggingface/datasets/json/default-6084dd82fabb7d7b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 3/3 [00:00<00:00, 788.01it/s]\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m266ex [00:00, 2650.23ex/s]\u001b[0m\n",
      "\u001b[34m532ex [00:00, 2654.30ex/s]\u001b[0m\n",
      "\u001b[34m843ex [00:00, 2859.60ex/s]\u001b[0m\n",
      "\u001b[34m1138ex [00:00, 2892.60ex/s]\u001b[0m\n",
      "\u001b[34m1442ex [00:00, 2941.94ex/s]\u001b[0m\n",
      "\u001b[34m1500ex [00:00, 2884.84ex/s]\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m312ex [00:00, 3114.05ex/s]\u001b[0m\n",
      "\u001b[34m500ex [00:00, 3145.99ex/s]\u001b[0m\n",
      "\u001b[34m0ex [00:00, ?ex/s]\u001b[0m\n",
      "\u001b[34m265ex [00:00, 2646.94ex/s]\u001b[0m\n",
      "\u001b[34m578ex [00:00, 2929.07ex/s]\u001b[0m\n",
      "\u001b[34m894ex [00:00, 3032.09ex/s]\u001b[0m\n",
      "\u001b[34m1198ex [00:00, 3021.12ex/s]\u001b[0m\n",
      "\u001b[34m1516ex [00:00, 3077.87ex/s]\u001b[0m\n",
      "\u001b[34m1824ex [00:00, 2986.79ex/s]\u001b[0m\n",
      "\u001b[34m2125ex [00:00, 2991.20ex/s]\u001b[0m\n",
      "\u001b[34m2427ex [00:00, 2999.67ex/s]\u001b[0m\n",
      "\u001b[34m2742ex [00:00, 3043.22ex/s]\u001b[0m\n",
      "\u001b[34m3047ex [00:01, 3031.82ex/s]\u001b[0m\n",
      "\u001b[34m3364ex [00:01, 3071.42ex/s]\u001b[0m\n",
      "\u001b[34m3672ex [00:01, 2936.09ex/s]\u001b[0m\n",
      "\u001b[34m3967ex [00:01, 2851.45ex/s]\u001b[0m\n",
      "\u001b[34m4000ex [00:01, 2946.40ex/s]\u001b[0m\n",
      "\u001b[34mrun eval for train data\u001b[0m\n",
      "\u001b[34mrun eval for dev data\u001b[0m\n",
      "\u001b[34mrun eval for test data\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/lib/function_base.py:495: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[34m{'train': {'ACC': 99.33333333333333, 'loss': 0.03685682902733485, 'ACC_OLD': 99.33333333333333, 'ACC_NEW': nan}, 'dev': {'ACC': 83.2, 'loss': 0.8758085250854493, 'ACC_OLD': 83.2, 'ACC_NEW': nan}, 'test': {'ACC': 82.75, 'loss': 0.8726195421218872, 'ACC_OLD': 82.75, 'ACC_NEW': nan}}\u001b[0m\n",
      "\u001b[34msaved metrics to: ./bcwi_nlp_outputs/add_data/MASSIVE/1111/add_data/old_model/metrics.json\u001b[0m\n",
      "\u001b[34msaved logits to: ./bcwi_nlp_outputs/add_data/MASSIVE/1111/add_data/old_model/logits.pickle\u001b[0m\n",
      "\u001b[34mbucket name sagemaker-us-east-1-339569644176\u001b[0m\n",
      "\u001b[34mdir ./bcwi_nlp_outputs/add_data/MASSIVE/1111/add_data/old_model\u001b[0m\n",
      "\n",
      "2023-01-03 15:37:46 Uploading - Uploading generated training model\u001b[34muploaded ./bcwi_nlp_outputs/add_data/MASSIVE/1111/add_data/old_model to s3 bucket name sagemaker-us-east-1-339569644176\u001b[0m\n",
      "\u001b[34mtrain_metrics {'ACC': 99.33333333333333, 'loss': 0.03685682902733485, 'ACC_OLD': 99.33333333333333, 'ACC_NEW': nan}\u001b[0m\n",
      "\u001b[34mdev_metrics {'ACC': 83.2, 'loss': 0.8758085250854493, 'ACC_OLD': 83.2, 'ACC_NEW': nan}\u001b[0m\n",
      "\u001b[34mtest_metrics {'ACC': 82.75, 'loss': 0.8726195421218872, 'ACC_OLD': 82.75, 'ACC_NEW': nan}\u001b[0m\n",
      "\u001b[34mfinished old models\u001b[0m\n",
      "\u001b[34m2023-01-03 15:37:38,180 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-03 15:37:38,180 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-03 15:37:38,181 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-03 15:38:16 Completed - Training job completed\n",
      "ProfilerReport-1672759870: NoIssuesFound\n",
      "Training seconds: 177\n",
      "Billable seconds: 177\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator_new.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d09b6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from S3 ./bcwi_nlp_outputs/add_data/MASSIVE/1111/add_data/old_model/model\n",
      "Downloaded new_model_dir/config.json\n",
      "Downloaded new_model_dir/hparams.json\n",
      "Downloaded new_model_dir/pytorch_model.bin\n",
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=60, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE: Load new model from S3\n",
    "# Used for inference and calculating negative flip rate\n",
    "\n",
    "load_from_s3 = f\"./bcwi_nlp_outputs/add_data/MASSIVE/1111/add_data/old_model/model\"\n",
    "\n",
    "print (f\"Loading from S3 {load_from_s3}\")\n",
    "os.makedirs(\"new_model_dir\", exist_ok=True)\n",
    "# load stuff from s3\n",
    "with open(\"new_model_dir/config.json\", \"wb\") as f:\n",
    "    s3.download_fileobj(bucket_name, os.path.join(load_from_s3, \"config.json\"), f)\n",
    "    print(\"Downloaded new_model_dir/config.json\")\n",
    "with open(\"new_model_dir/hparams.json\", \"wb\") as f:\n",
    "    s3.download_fileobj(bucket_name, os.path.join(load_from_s3, \"hparams.json\"), f)\n",
    "    print(\"Downloaded new_model_dir/hparams.json\")\n",
    "with open(\"new_model_dir/pytorch_model.bin\", \"wb\") as f:\n",
    "    s3.download_fileobj(bucket_name, os.path.join(load_from_s3, \"pytorch_model.bin\"), f)\n",
    "    print(\"Downloaded new_model_dir/pytorch_model.bin\")\n",
    "\n",
    "from transformers import RobertaForSequenceClassification\n",
    "new_model = RobertaForSequenceClassification.from_pretrained(\"new_model_dir\")\n",
    "\n",
    "print (new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d24909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for calculating negative flip rate and accuracy\n",
    "test_data = new_dataset[\"test\"] # test sets in old and new datasets are the same\n",
    "\n",
    "def calculate_accuracy(data, model, batch_size=40):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        if int(i/batch_size) % 10 == 0:\n",
    "            print (f\"working on {int(i/batch_size)} out of {int(len(data)/batch_size)}\")\n",
    "        torch.cuda.empty_cache() # clear memory\n",
    "        # process examples in the batch\n",
    "        examples = data[i:(i+batch_size)]\n",
    "        text = examples[\"text\"]\n",
    "        label = examples[\"label\"]\n",
    "        text_tokenizer = tokenizer[0](text, padding=\"max_length\", truncation=True, max_length=max([len(t.split()) for t in text]),\n",
    "                                   return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**text_tokenizer)\n",
    "        preds = outputs.logits.argmax(-1).tolist()\n",
    "\n",
    "        # merge them into the list that combines all labels and predictions\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(label)\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1366a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting old model predictions\n",
      "working on 0 out of 100\n",
      "working on 10 out of 100\n",
      "working on 20 out of 100\n",
      "working on 30 out of 100\n",
      "working on 40 out of 100\n",
      "working on 50 out of 100\n",
      "working on 60 out of 100\n",
      "working on 70 out of 100\n",
      "working on 80 out of 100\n",
      "working on 90 out of 100\n"
     ]
    }
   ],
   "source": [
    "# Get old model prediction\n",
    "print (\"Getting old model predictions\")\n",
    "old_model_preds, labels = calculate_accuracy(test_data, old_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8868a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting new model predictions\n",
      "working on 0 out of 100\n",
      "working on 10 out of 100\n",
      "working on 20 out of 100\n",
      "working on 30 out of 100\n",
      "working on 40 out of 100\n",
      "working on 50 out of 100\n",
      "working on 60 out of 100\n",
      "working on 70 out of 100\n",
      "working on 80 out of 100\n",
      "working on 90 out of 100\n"
     ]
    }
   ],
   "source": [
    "# Get new model prediction\n",
    "print (\"Getting new model predictions\")\n",
    "new_model_preds, _ = calculate_accuracy(test_data, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b517ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old model accuracy 81.7%\n",
      "New model accuracy 82.55%\n",
      "Negative Flip Rate 3.25%\n"
     ]
    }
   ],
   "source": [
    "# Get old model accuracy\n",
    "old_acc = 100 * sum([old_pred == l for old_pred, l in zip(old_model_preds, labels)]) / float(len(labels))\n",
    "print (f\"Old model accuracy {old_acc}%\")\n",
    "\n",
    "# Get new model accuracy\n",
    "new_acc = 100 * sum([new_pred == l for new_pred, l in zip(new_model_preds, labels)]) / float(len(labels))\n",
    "print (f\"New model accuracy {new_acc}%\")\n",
    "\n",
    "# Calculate negative flip rate\n",
    "nfr = 100 * sum([old_pred == l and new_pred != l for old_pred, new_pred, l in zip(old_model_preds, new_model_preds, labels)]) / float(len(labels))\n",
    "print (f\"Negative Flip Rate {nfr}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a39e7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate between old and new model\n",
    "# More details can be found in the Github repo\n",
    "\n",
    "def interpolate_weights(old_model, new_models, alpha, new_label_ids=None, weighted=None):\n",
    "\n",
    "    # Form soup ensemble of new models\n",
    "    new_state_dicts = [new_model.state_dict() for new_model in new_models]\n",
    "    new_model_state_dict = dict()\n",
    "    for key in new_models[0].state_dict():\n",
    "        if not (key.endswith('bias') or key.endswith('weight')):\n",
    "            continue\n",
    "\n",
    "        new_model_state_dict[key] = torch.mean(torch.stack([s[key] for s in new_state_dicts]), dim=0)\n",
    "\n",
    "    print('alpha', alpha)\n",
    "    metrics = dict()\n",
    "\n",
    "    # Use the old model as the basis of the interpolated model weights\n",
    "    model = copy.deepcopy(old_model)\n",
    "    # All weights of a model can be accessed by its state_dict\n",
    "    state_dict = model.state_dict()\n",
    "    for key in state_dict:\n",
    "        # Be sure to only interpolate weight matrices; includes e.g. layer norm matrices\n",
    "        if not (key.endswith('bias') or key.endswith('weight')):\n",
    "            continue\n",
    "\n",
    "        if weighted is not None:\n",
    "\n",
    "            # when alpha = 1.0, there can be NaN values due to numerical instabilities when values in the weight\n",
    "            # matrix are too small. In this case we replace the NaNs with the weights of the old model.\n",
    "            if alpha == 1.0:\n",
    "                c = state_dict[key].detach().clone()\n",
    "\n",
    "            # Inplace operations to modify the weights of the model.\n",
    "            # State_dict initially holds the weights of the old model.\n",
    "            state_dict[key] *= (alpha * weighted[key])\n",
    "            state_dict[key] += ((1-alpha) * new_model_state_dict[key])\n",
    "            state_dict[key] /= (alpha * weighted[key] + (1-alpha))\n",
    "\n",
    "            # Three lines above as one-liner\n",
    "            #state_dict[key].data.copy_(((alpha * weighted[key] * state_dict[key]) + ((1-alpha) * new_model_state_dict[key])) / (alpha * weighted[key] + (1-alpha)))\n",
    "\n",
    "\n",
    "            if alpha == 1.0:\n",
    "                nans = state_dict[key] != state_dict[key]\n",
    "                state_dict[key][nans] = c[nans]\n",
    "        else:\n",
    "            # Simple linear interpolation with parameter alpha.\n",
    "            # State_dict initially holds the weights of the old model.\n",
    "            state_dict[key] *= alpha\n",
    "            state_dict[key] += ((1 - alpha) * new_model_state_dict[key])\n",
    "\n",
    "        # Copy classifier weights of new classes from the new model. The old model was not trained on those classes.\n",
    "        if new_label_ids:\n",
    "            if key == 'classifier.out_proj.weight':\n",
    "                state_dict[key][new_label_ids, :] = new_model_state_dict[key][new_label_ids, :]\n",
    "            if key == 'classifier.out_proj.bias':\n",
    "                state_dict[key][new_label_ids] = new_model_state_dict[key][new_label_ids]\n",
    "    return state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7a7fb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interpolated model\n",
    "interpolated_model = RobertaForSequenceClassification.from_pretrained(\"old_model_dir\")\n",
    "\n",
    "# Initialize interpolated model with old model weights\n",
    "interpolated_state_dict = interpolate_weights(interpolated_model, [new_model], alpha=0.3)\n",
    "# load interpolated_state_dict into new model\n",
    "interpolated_model.load_state_dict(interpolated_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68945504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting interpolated model predictions\n",
      "working on 0 out of 100\n",
      "working on 10 out of 100\n",
      "working on 20 out of 100\n",
      "working on 30 out of 100\n",
      "working on 40 out of 100\n",
      "working on 50 out of 100\n",
      "working on 60 out of 100\n",
      "working on 70 out of 100\n",
      "working on 80 out of 100\n",
      "working on 90 out of 100\n"
     ]
    }
   ],
   "source": [
    "# Get interpolated model prediction\n",
    "print (\"Getting interpolated model predictions\")\n",
    "interpolated_model_preds, _ = calculate_accuracy(test_data, interpolated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b46f81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old model accuracy 81.7%\n",
      "New model accuracy 82.55%\n",
      "Interpolated model accuracy 82.65%\n",
      "Negative Flip Rate 3.25%\n",
      "Negative Flip Rate Old and Interpolated (Ours) 2.225%\n"
     ]
    }
   ],
   "source": [
    "# Get old model accuracy\n",
    "old_acc = 100 * sum([old_pred == l for old_pred, l in zip(old_model_preds, labels)]) / float(len(labels))\n",
    "print (f\"Old model accuracy {old_acc}%\")\n",
    "\n",
    "# Get new model accuracy\n",
    "new_acc = 100 * sum([new_pred == l for new_pred, l in zip(new_model_preds, labels)]) / float(len(labels))\n",
    "print (f\"New model accuracy {new_acc}%\")\n",
    "\n",
    "# Get interpolated model accuracy\n",
    "interpolated_acc = 100 * sum([interpolated_pred == l for interpolated_pred, l in zip(interpolated_model_preds, labels)]) / float(len(labels))\n",
    "print (f\"Interpolated model accuracy {interpolated_acc}%\")\n",
    "\n",
    "\n",
    "# Calculate negative flip rate\n",
    "nfr = 100 * sum([old_pred == l and new_pred != l for old_pred, new_pred, l in zip(old_model_preds, new_model_preds, labels)]) / float(len(labels))\n",
    "print (f\"Negative Flip Rate {nfr}%\")\n",
    "\n",
    "# Calculate negative flip rate of old model and interpolated models\n",
    "interpolate_nfr = 100 * sum([old_pred == l and interpolate_pred != l for old_pred, interpolate_pred, l in zip(old_model_preds, interpolated_model_preds, labels)]) / float(len(labels))\n",
    "print (f\"Negative Flip Rate Old and Interpolated (Ours) {interpolate_nfr}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a933296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b3471b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
