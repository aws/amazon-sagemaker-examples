{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4bd246",
   "metadata": {},
   "source": [
    "# Offline RL Training the Cart-pole Model with SageMaker RL and Ray Rllib\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will demonstrate how to use the Ray Rllib toolkit along with SageMaker RL to perform offline RL trainig. In offline RL (also known as batch RL), the agent is trained using previously generated experience datasets. This is highly valuable when simulating the actual environment is expensive. We consider the familiar cartpole problem. \n",
    "\n",
    "We have structured this notebook in three parts: <br />\n",
    "1) Generate the necessary experience dataset. <br />\n",
    "2) Train the offline RL model. <br />\n",
    "3) Evaluate the performance of the trained agent. <br />\n",
    "\n",
    "The objective of the cartpole problem is to balance the cartpole in the vertical position. For more details regarding the  observation and action spaces and reward structure see [Neuronlike adaptive elements that can solve difficult learning control problems](https://ieeexplore.ieee.org/document/6313077).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8e981",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "### Imports\n",
    "\n",
    "\n",
    "We will start by importing the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sys.path.append(\"common\")\n",
    "from misc import get_execution_role, wait_for_s3_object\n",
    "from docker_utils import build_and_push_docker_image\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b02511",
   "metadata": {},
   "source": [
    " ### Setup S3 bucket\n",
    "\n",
    " \n",
    " As a next step, we set up the S3 bucket for storing the experiences and other training artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = sage_session.default_bucket()\n",
    "s3_output_path = \"s3://{}/\".format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d659d",
   "metadata": {},
   "source": [
    "### Define Variables \n",
    "\n",
    "\n",
    "Next we will define a job name prefix. The job name prefix ends in \"-gen\" to indicate that the training job is used to generate the experience dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec79710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix = \"rl-cartpole-ray-gen\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8386e",
   "metadata": {},
   "source": [
    "### Configure where training happens\n",
    "\n",
    "\n",
    "\n",
    "The next step is to define the type of training instance. For this example, we will not be using local mode to generate the training experiences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9800ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_mode = False\n",
    "instance_type = \"ml.c5.2xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf142c",
   "metadata": {},
   "source": [
    "### Create an IAM role\n",
    "\n",
    "Obtain an execution role with the IAM permissions for SageMaker training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    role = get_execution_role()\n",
    "\n",
    "print(\"Using IAM role arn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7e10b",
   "metadata": {},
   "source": [
    "### Use docker image\n",
    "\n",
    "\n",
    "To train the model and generate the experiences, we need a docker image of the ray container. We get a public docker image for RLlib from the [Amazon SageMaker RL containers repository](https://github.com/aws/sagemaker-rl-container)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_or_gpu = \"gpu\" if instance_type.startswith(\"ml.p\") else \"cpu\"\n",
    "aws_region = boto3.Session().region_name\n",
    "custom_image_name = (\n",
    "    \"462105765813.dkr.ecr.%s.amazonaws.com/sagemaker-rl-ray-container:ray-0.8.5-tf-%s-py36\"\n",
    "    % (aws_region, cpu_or_gpu)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c246b",
   "metadata": {},
   "source": [
    "### Write the Training Code for Generating Offline Data\n",
    "The training code for generating the experinces is written in the file “train-rl-cartpole-ray-gen.py” located in the /src directory. We will use PPO algorithm to generate the training experinces. Note that we use the \"output\" key in the config dictionary to specify the location where the experience data will be stored.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/train-{job_name_prefix}.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece56309",
   "metadata": {},
   "source": [
    "## Generate the offline dataset using the Python SDK Script mode\n",
    "\n",
    "Here we specify the estimator object that will be used to generate the dataset. The location entrypoint script, source directory, docker image etc are included in the estimator definition. We start the training job by calling estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d76c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)\n",
    "\n",
    "estimator = RLEstimator(\n",
    "    entry_point=\"train-%s.py\" % job_name_prefix,\n",
    "    source_dir=\"src\",\n",
    "    dependencies=[\"common/sagemaker_rl\"],\n",
    "    image_uri=custom_image_name,\n",
    "    role=role,\n",
    "    train_instance_type=instance_type,\n",
    "    train_instance_count=1,\n",
    "    output_path=s3_output_path,\n",
    "    base_job_name=job_name_prefix,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters={\n",
    "        # Attention scientists!  You can override any Ray algorithm parameter here:\n",
    "        # \"rl.training.config.horizon\": 5000,\n",
    "        # \"rl.training.config.num_sgd_iter\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "estimator.fit(wait=local_mode)\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc2fb8",
   "metadata": {},
   "source": [
    "## Retrieving the Experience Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5b48a",
   "metadata": {},
   "source": [
    "Once the training is complete, the experience data that includes the sequence of observations, actions and rewards generated by the environment will be stored among the training artifacts in Amazon S3. We will retrieve this data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33270fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder = \"cartpole_data\"\n",
    "exp_loc = \"cartpole-out\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "os.mkdir(\"src/{}\".format(data_folder))\n",
    "s3.download_file(\n",
    "    s3_bucket,\n",
    "    \"{}/output/output.tar.gz\".format(job_name),\n",
    "    \"src/{}/output.tar.gz\".format(data_folder),\n",
    ")\n",
    "exp_tar = tarfile.open(\"src/{}/output.tar.gz\".format(data_folder))\n",
    "exp_tar.extractall(\"src/{}\".format(data_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc55a6a",
   "metadata": {},
   "source": [
    "The experience data generated from training can now be found stored in json format inside the data_folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "expfolder = Path.cwd() / \"src\" / data_folder / exp_loc\n",
    "offline_data = [filename for filename in expfolder.rglob(\"*.json\")]\n",
    "offline_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23eee96",
   "metadata": {},
   "source": [
    "# Training the Offline RL Agent Using IMPALA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11abf94",
   "metadata": {},
   "source": [
    "Next we will try to train the offline RL agent using the above data. We will use an off-policy RL algorithm called IMPALA to train the agent. The code for training the offline RL agent using IMPALA is written in train-rl-cartpole-ray-offline-IMPALA.py located in the /src directory. In the training config, we specify the location where the training experiences are stored using hte \"input\" key. Note that the \"explore\" key is set to be \"False\" in training config since we will only be sampling actions from the offline dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad505e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix = \"rl-cartpole-ray-offline-IMPALA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d19715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/train-{job_name_prefix}.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)\n",
    "\n",
    "estimator2 = RLEstimator(\n",
    "    entry_point=\"train-%s.py\" % job_name_prefix,\n",
    "    source_dir=\"src\",\n",
    "    dependencies=[\"common/sagemaker_rl\"],\n",
    "    image_uri=custom_image_name,\n",
    "    role=role,\n",
    "    train_instance_type=instance_type,\n",
    "    train_instance_count=1,\n",
    "    output_path=s3_output_path,\n",
    "    base_job_name=job_name_prefix,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters={\n",
    "        # Attention scientists!  You can override any Ray algorithm parameter here:\n",
    "        # \"rl.training.config.horizon\": 5000,\n",
    "        # \"rl.training.config.num_sgd_iter\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "estimator2.fit(wait=local_mode)\n",
    "job_name = estimator2.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3b747",
   "metadata": {},
   "source": [
    "## Evaluation of  RL model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c801d9",
   "metadata": {},
   "source": [
    "In this step, we run evaluation of the RL Agent using the final stored checkpoint. We will move the checkpoint data to either the local directory or Amazon S3 depending on whether evaluation is run in local mode or using SageMaker mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7bf894",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "\n",
    "\n",
    "if local_mode:\n",
    "    model_tar_key = \"{}/model.tar.gz\".format(job_name)\n",
    "else:\n",
    "    model_tar_key = \"{}/output/model.tar.gz\".format(job_name)\n",
    "\n",
    "local_checkpoint_dir = \"{}/model\".format(tmp_dir)\n",
    "\n",
    "wait_for_s3_object(s3_bucket, model_tar_key, tmp_dir, training_job_name=job_name)\n",
    "\n",
    "if not os.path.isfile(\"{}/model.tar.gz\".format(tmp_dir)):\n",
    "    raise FileNotFoundError(\"File model.tar.gz not found\")\n",
    "\n",
    "os.system(\"mkdir -p {}\".format(local_checkpoint_dir))\n",
    "os.system(\"tar -xvzf {}/model.tar.gz -C {}\".format(tmp_dir, local_checkpoint_dir))\n",
    "\n",
    "print(\"Checkpoint directory {}\".format(local_checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acee215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if local_mode:\n",
    "    checkpoint_path = \"file://{}\".format(local_checkpoint_dir)\n",
    "    print(\"Local checkpoint file path: {}\".format(local_checkpoint_dir))\n",
    "else:\n",
    "    checkpoint_path = \"s3://{}/{}/checkpoint/\".format(s3_bucket, job_name)\n",
    "    if not os.listdir(local_checkpoint_dir):\n",
    "        raise FileNotFoundError(\"Checkpoint files not found under the path\")\n",
    "    os.system(\"aws s3 cp --recursive {} {}\".format(local_checkpoint_dir, checkpoint_path))\n",
    "    print(\"S3 checkpoint file path: {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38779f72",
   "metadata": {},
   "source": [
    "Evaluation metrics such as mean_reward, max_reward etc can be calculated now by calling the estimator_eval.fit() script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0507c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator_eval = RLEstimator(\n",
    "    entry_point=\"evaluate-ray.py\",\n",
    "    source_dir=\"src\",\n",
    "    dependencies=[\"common/sagemaker_rl\"],\n",
    "    image_uri=custom_image_name,\n",
    "    role=role,\n",
    "    train_instance_type=instance_type,\n",
    "    train_instance_count=1,\n",
    "    base_job_name=job_name_prefix + \"-evaluation\",\n",
    "    hyperparameters={\"evaluate_episodes\": 10, \"algorithm\": \"IMPALA\", \"env\": \"CartPole-v1\"},\n",
    ")\n",
    "\n",
    "estimator_eval.fit({\"model\": checkpoint_path})\n",
    "job_name = estimator_eval.latest_training_job.job_name\n",
    "print(\"Evaluation job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6b978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
