{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using Linear Learner\n",
    "With [Amazon SageMaker multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html), customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of a large number of models, which can be served from a common inference container, needs to be invokable on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, a traditional endpoint is still the best choice.\n",
    "\n",
    "At a high level, Amazon SageMaker manages the loading and unloading of models for a multi-model endpoint, as they are needed. When an invocation request is made for a particular model, Amazon SageMaker routes the request to an instance assigned to that model, downloads the model artifacts from S3 onto that instance, and initiates loading of the model into the memory of the container. As soon as the loading is complete, Amazon SageMaker performs the requested invocation and returns the result. If the model is already loaded in memory on the selected instance, the downloading and loading steps are skipped and the invocation is performed immediately.\n",
    "\n",
    "Amazon SageMaker inference pipeline model consists of a sequence of containers that serve inference requests by combining preprocessing, predictions and post-processing data science tasks.  An inference pipeline allows you to apply the same preprocessing code used during model training, to process the inference request data used for predictions.\n",
    "\n",
    "To demonstrate how multi-model endpoints are created and used with inference pipeline, this notebook provides an example using a set of Linear Learner models that each predict housing prices for a single location. This domain is used as a simple example to easily experiment with multi-model endpoints.  \n",
    "\n",
    "This notebook showcases three MME capabilities: \n",
    "* Native MME support with Amazon SageMaker Linear Learner algorithm.  Because of the native support there is no need for you to create a custom container.  \n",
    "* Native MME support with Amazon SageMaker Inference Pipelines.\n",
    "* Granual InvokeModel access to multiple models hosted on the MME using IAM condition key.\n",
    "\n",
    "To demonstrate these capabilities, the notebook discusses the use case of predicting house prices in multiple cities using linear regression.  House prices are predicted based on features like number of bedrooms, number of garages, square footage etc.  Depending on the city, the features effect the house price differently.  For example, small changes in the square footage cause a drastic change in house prices in NewYork when compared to price changes in Houston.  For accurate house price predictions, we will train multiple linear regression models, a unique location specific model per city.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. [Generate synthetic data for housing models](#Generate-synthetic-data-for-housing-models)\n",
    "1. [Preprocess the raw housing data using Scikit Learn model](#Preprocess-synthetic-housing-data-using-scikit-learn)\n",
    "1. [Train multiple house value prediction models for multiple cities](#Train-multiple-house-value-prediction-models)\n",
    "1. [Create model entity with multi model support](#Create-sagemaker-multi-model-support)\n",
    "1. [Create an inference pipeline with sklearn model and MME linear learner model](#Create-inference-pipeline)\n",
    "1. [Exercise the inference pipeline - Get predictions from the different  linear learner models](#Exercise-inference-pipeline)\n",
    "1. [Update Multi Model Endpoint with new models](#update-models)\n",
    "1. [Fine grained control for model invocation using IAM conditional keys.](#Finegrain-control-invoke-models)\n",
    "1. [Latency analysis of Linear Learner MME](#Latency-analysis)\n",
    "1. [Clean up](#CleanUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 1 - Generate synthetic data for housing models <a id='Generate-synthetic-data-for-housing-models'></a>\n",
    "\n",
    "In this section, you will generate synthetic data that will be used to train the linear learner models.  The data generated consists of 6 numerical features - the year the house was built in, house size in square feet, number of bedrooms, number of bathroom, the lot size and number of garages and two categorial features - deck and front_porch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from random import choice\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "from sagemaker.multidatamodel import MULTI_MODEL_CONTAINER_MODE\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HOUSES_PER_LOCATION = 1000\n",
    "LOCATIONS  = ['NewYork_NY',    'LosAngeles_CA',   'Chicago_IL',    'Houston_TX',   'Dallas_TX',\n",
    "              'Phoenix_AZ',    'Philadelphia_PA', 'SanAntonio_TX', 'SanDiego_CA',  'SanFrancisco_CA']\n",
    "MAX_YEAR = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_price(house):\n",
    "    \"\"\"Generate price based on features of the house\"\"\"\n",
    "    \n",
    "    base_price = int(house['SQUARE_FEET'] * 150)\n",
    "    if house['FRONT_PORCH'] == 'y':\n",
    "        garage = 1\n",
    "    else:\n",
    "        garage = 0\n",
    "        \n",
    "    if house['FRONT_PORCH'] == 'y':\n",
    "        front_porch = 1\n",
    "    else:\n",
    "        front_porch = 0\n",
    "        \n",
    "    price = int(base_price + 10000 * house['NUM_BEDROOMS'] + \\\n",
    "                15000 * house['NUM_BATHROOMS'] + \\\n",
    "                15000 * house['LOT_ACRES'] + \\\n",
    "                10000 * garage + \\\n",
    "                10000 * front_porch + \\\n",
    "                15000 * house['GARAGE_SPACES'] - \\\n",
    "                5000 * (MAX_YEAR - house['YEAR_BUILT']))\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_yes_no():\n",
    "    \"\"\"Generate values (y/n) for categorical features\"\"\"\n",
    "    answer = choice(['y', 'n'])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_house():\n",
    "    \"\"\"Generate a row of data (single house information)\"\"\"\n",
    "    house = {'SQUARE_FEET':    np.random.normal(3000, 750),\n",
    "              'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "              'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "              'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "              'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "              'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10))),\n",
    "              'FRONT_PORCH':   gen_yes_no(),\n",
    "              'DECK':          gen_yes_no()\n",
    "             }\n",
    "    \n",
    "    price = gen_price(house)\n",
    "    \n",
    "    return [house['YEAR_BUILT'],   \n",
    "            house['SQUARE_FEET'], \n",
    "            house['NUM_BEDROOMS'], \n",
    "            house['NUM_BATHROOMS'], \n",
    "            house['LOT_ACRES'],    \n",
    "            house['GARAGE_SPACES'],\n",
    "            house['FRONT_PORCH'],    \n",
    "            house['DECK'], \n",
    "            price]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_houses(num_houses):\n",
    "    \"\"\"Generate housing dataset\"\"\"\n",
    "    house_list = []\n",
    "    \n",
    "    for i in range(num_houses):\n",
    "        house_list.append(gen_random_house())\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        house_list, \n",
    "        columns=[\n",
    "            'YEAR_BUILT',    \n",
    "            'SQUARE_FEET',  \n",
    "            'NUM_BEDROOMS',            \n",
    "            'NUM_BATHROOMS',\n",
    "            'LOT_ACRES',\n",
    "            'GARAGE_SPACES',\n",
    "            'FRONT_PORCH',\n",
    "            'DECK', \n",
    "            'PRICE']\n",
    "    )\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_locally(location, train, test): \n",
    "    \"\"\"Save the housing data locally\"\"\"\n",
    "    os.makedirs('data/{0}/train'.format(location),exist_ok=True)\n",
    "    train.to_csv('data/{0}/train/train.csv'.format(location), sep=',', header=False, index=False)\n",
    "       \n",
    "    os.makedirs('data/{0}/test'.format(location),exist_ok=True)\n",
    "    test.to_csv('data/{0}/test/test.csv'.format(location), sep=',', header=False, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate housing data for multiple locations.\n",
    "#Change \"PARALLEL_TRAINING_JOBS \" to a lower number to limit the number of training jobs and models. Or to a higher value to experiment with more models.\n",
    "\n",
    "#PARALLEL_TRAINING_JOBS = 4\n",
    "\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    houses = gen_houses(NUM_HOUSES_PER_LOCATION)\n",
    "    \n",
    "    #Spliting data into train and test in 90:10 ratio\n",
    "    #Not splitting the train data into train and val because its not preprocessed yet\n",
    "    train, test = train_test_split(houses, test_size=0.1)\n",
    "    save_data_locally(loc, train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>SQUARE_FEET</th>\n",
       "      <th>NUM_BEDROOMS</th>\n",
       "      <th>NUM_BATHROOMS</th>\n",
       "      <th>LOT_ACRES</th>\n",
       "      <th>GARAGE_SPACES</th>\n",
       "      <th>FRONT_PORCH</th>\n",
       "      <th>DECK</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999</td>\n",
       "      <td>3578.956826</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>544843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993</td>\n",
       "      <td>2011.989097</td>\n",
       "      <td>6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>300498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>3245.068849</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>515760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1992</td>\n",
       "      <td>4000.608981</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.48</td>\n",
       "      <td>2</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>589791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>5991.931901</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>3</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>996489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  NUM_BATHROOMS  LOT_ACRES  \\\n",
       "0        1999  3578.956826             6            2.0       1.20   \n",
       "1        1993  2011.989097             6            2.5       1.08   \n",
       "2        2002  3245.068849             3            2.5       1.10   \n",
       "3        1992  4000.608981             3            2.5       0.48   \n",
       "4        2009  5991.931901             4            3.0       1.18   \n",
       "\n",
       "   GARAGE_SPACES FRONT_PORCH DECK   PRICE  \n",
       "0              0           n    n  544843  \n",
       "1              1           n    y  300498  \n",
       "2              2           n    n  515760  \n",
       "3              2           y    n  589791  \n",
       "4              3           n    n  996489  "
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shows the first few lines of data.\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Preprocess the raw housing data using Scikit Learn <a id='Preprocess-synthetic-housing-data-using-scikit-learn'></a>\n",
    "\n",
    "In this section, the categorical features of the data (deck and porch) are pre-processed using sklearn to convert them to one hot encoding representation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET :  sagemaker-us-east-1-555360056434\n",
      "ROLE :  arn:aws:iam::555360056434:role/service-role/AmazonSageMaker-ExecutionRole-20200630T162532\n"
     ]
    }
   ],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "BUCKET  = sagemaker_session.default_bucket()\n",
    "print(\"BUCKET : \", BUCKET)\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"ROLE : \", role)\n",
    "\n",
    "ACCOUNT_ID = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION = boto3.Session().region_name\n",
    "\n",
    "DATA_PREFIX = 'DEMO_MME_LINEAR_LEARNER'\n",
    "HOUSING_MODEL_NAME = 'housing'\n",
    "MULTI_MODEL_ARTIFACTS = 'multi_model_artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the SKLearn estimator with the sklearn_preprocessor.py as the script\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'sklearn_preprocessor.py'\n",
    "\n",
    "sklearn_preprocessor = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    role=role,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw training data uploaded to :  s3://sagemaker-us-east-1-555360056434/housing-data/NewYork_NY/train/train.csv\n",
      "Raw training data uploaded to :  s3://sagemaker-us-east-1-555360056434/housing-data/LosAngeles_CA/train/train.csv\n",
      "Raw training data uploaded to :  s3://sagemaker-us-east-1-555360056434/housing-data/Chicago_IL/train/train.csv\n",
      "Raw training data uploaded to :  s3://sagemaker-us-east-1-555360056434/housing-data/Houston_TX/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "#Upload the raw training data to S3 bucket, to be accessed by SKLearn\n",
    "#prefix = 'housing-data'\n",
    "train_inputs = []\n",
    "\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    #WORK_DIRECTORY = \"data/\" + loc \n",
    "\n",
    "    train_input = sagemaker_session.upload_data(\n",
    "        #path='{}/{}'.format(WORK_DIRECTORY + \"/train/\", 'train.csv'), \n",
    "        path='data/{}/train/train.csv'.format(loc),\n",
    "        bucket=BUCKET,\n",
    "        #key_prefix='{}/{}/{}'.format(prefix, loc, 'train')\n",
    "        key_prefix='housing-data/{}/train'.format(loc)\n",
    "    )\n",
    "    \n",
    "    train_inputs.append(train_input)\n",
    "    print(\"Raw training data uploaded to : \", train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing fit input data at  0  for loc  NewYork_NY\n",
      "2020-07-06 00:08:58 Starting - Starting the training job...\n",
      "2020-07-06 00:09:01 Starting - Launching requested ML instances......\n",
      "2020-07-06 00:10:16 Starting - Preparing the instances for training......\n",
      "2020-07-06 00:11:08 Downloading - Downloading input data...\n",
      "2020-07-06 00:11:57 Training - Training image download completed. Training in progress..\u001b[34m2020-07-06 00:11:57,857 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-07-06 00:11:57,859 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-06 00:11:57,869 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-07-06 00:12:58,516 sagemaker-containers INFO     Module sklearn_preprocessor does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-07-06 00:12:58,517 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-07-06 00:12:58,517 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-07-06 00:12:58,517 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-preprocessor\n",
      "  Building wheel for sklearn-preprocessor (setup.py): started\n",
      "  Building wheel for sklearn-preprocessor (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor: filename=sklearn_preprocessor-1.0.0-py2.py3-none-any.whl size=9046 sha256=531fda7e3698b702cf9739968d0a919a2b637df899a04b6bd1f431babe5d2c2c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b7neq4j_/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-preprocessor-1.0.0\u001b[0m\n",
      "\u001b[34m2020-07-06 00:12:59,967 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-06 00:12:59,978 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"scikit-learn-preprocessor-2020-07-06-00-08-58\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-08-58/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sklearn_preprocessor\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sklearn_preprocessor.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sklearn_preprocessor.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sklearn_preprocessor\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-08-58/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"scikit-learn-preprocessor-2020-07-06-00-08-58\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-08-58/source/sourcedir.tar.gz\",\"module_name\":\"sklearn_preprocessor\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sklearn_preprocessor.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m sklearn_preprocessor\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mfile : /opt/ml/input/data/train/train.csv\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[34m0          1985  1645.553522             5  ...            y     y  176833\u001b[0m\n",
      "\u001b[34m1          1986  3853.289477             4  ...            y     n  515443\u001b[0m\n",
      "\u001b[34m2          1983  3138.319644             4  ...            n     y  383097\u001b[0m\n",
      "\u001b[34m3          1987  3969.190548             5  ...            n     y  529478\u001b[0m\n",
      "\u001b[34m4          1999  1174.612374             5  ...            n     n  154991\u001b[0m\n",
      "\u001b[34m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[34m895        2008  3384.390444             3  ...            y     y  545708\u001b[0m\n",
      "\u001b[34m896        1987  2566.390620             2  ...            y     n  319858\u001b[0m\n",
      "\u001b[34m897        2004  4071.512507             4  ...            n     y  630926\u001b[0m\n",
      "\u001b[34m898        1999  2561.865639             2  ...            y     n  403779\u001b[0m\n",
      "\u001b[34m899        1995  3347.656323             6  ...            y     y  545098\n",
      "\u001b[0m\n",
      "\u001b[34m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\u001b[0m\n",
      "\u001b[34msaved model!\u001b[0m\n",
      "\u001b[34m2020-07-06 00:13:02,216 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-07-06 00:13:11 Uploading - Uploading generated training model\n",
      "2020-07-06 00:13:11 Completed - Training job completed\n",
      "Training seconds: 123\n",
      "Billable seconds: 123\n",
      "preprocessing fit input data at  1  for loc  LosAngeles_CA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-06 00:13:41 Starting - Starting the training job...\n",
      "2020-07-06 00:13:44 Starting - Launching requested ML instances.........\n",
      "2020-07-06 00:15:27 Starting - Preparing the instances for training......\n",
      "2020-07-06 00:16:26 Downloading - Downloading input data...\n",
      "2020-07-06 00:16:52 Training - Downloading the training image.\u001b[34m2020-07-06 00:17:14,906 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-07-06 00:17:14,909 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-06 00:17:14,919 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-07-06 00:17:15,484 sagemaker-containers INFO     Module sklearn_preprocessor does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-07-06 00:17:15,485 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-07-06 00:17:15,485 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-07-06 00:17:15,485 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-preprocessor\n",
      "  Building wheel for sklearn-preprocessor (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sklearn-preprocessor (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor: filename=sklearn_preprocessor-1.0.0-py2.py3-none-any.whl size=9047 sha256=a0dc163da9c6ef315b40b978f3725bbb033d7b7beffef1ffbde9eae22179dc5e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5edwj2si/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-preprocessor-1.0.0\u001b[0m\n",
      "\u001b[34m2020-07-06 00:17:17,108 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-06 00:17:17,119 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"scikit-learn-preprocessor-2020-07-06-00-13-41\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-13-41/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sklearn_preprocessor\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sklearn_preprocessor.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sklearn_preprocessor.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sklearn_preprocessor\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-13-41/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"scikit-learn-preprocessor-2020-07-06-00-13-41\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-13-41/source/sourcedir.tar.gz\",\"module_name\":\"sklearn_preprocessor\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sklearn_preprocessor.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m sklearn_preprocessor\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mfile : /opt/ml/input/data/train/train.csv\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[34m0          1992  2996.762483             2  ...            y     n  438214\u001b[0m\n",
      "\u001b[34m1          1998  3601.887667             2  ...            n     n  528033\u001b[0m\n",
      "\u001b[34m2          1964  2613.351715             2  ...            y     y  250302\u001b[0m\n",
      "\u001b[34m3          1977  2940.105900             6  ...            n     n  344565\u001b[0m\n",
      "\u001b[34m4          1987  3691.608430             5  ...            y     n  494191\u001b[0m\n",
      "\u001b[34m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[34m895        1991  3838.256727             6  ...            n     n  568638\u001b[0m\n",
      "\u001b[34m896        2006  3780.199355             2  ...            y     y  633979\u001b[0m\n",
      "\u001b[34m897        1998  2312.237970             6  ...            n     n  350885\u001b[0m\n",
      "\u001b[34m898        1988  3139.906782             2  ...            y     n  398286\u001b[0m\n",
      "\u001b[34m899        1988  2874.935121             5  ...            n     y  417590\n",
      "\u001b[0m\n",
      "\u001b[34m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\u001b[0m\n",
      "\u001b[34msaved model!\u001b[0m\n",
      "\u001b[34m2020-07-06 00:17:19,716 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-07-06 00:17:31 Uploading - Uploading generated training model\n",
      "2020-07-06 00:17:31 Completed - Training job completed\n",
      "Training seconds: 65\n",
      "Billable seconds: 65\n",
      "preprocessing fit input data at  2  for loc  Chicago_IL\n",
      "2020-07-06 00:17:54 Starting - Starting the training job...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-06 00:17:56 Starting - Launching requested ML instances......\n",
      "2020-07-06 00:19:13 Starting - Preparing the instances for training......\n",
      "2020-07-06 00:20:14 Downloading - Downloading input data...\n",
      "2020-07-06 00:20:45 Training - Downloading the training image..\u001b[34m2020-07-06 00:21:07,308 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-07-06 00:21:07,310 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-06 00:21:07,320 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-07-06 00:21:07,828 sagemaker-containers INFO     Module sklearn_preprocessor does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-07-06 00:21:07,828 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-07-06 00:21:07,828 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-07-06 00:21:07,828 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-preprocessor\n",
      "  Building wheel for sklearn-preprocessor (setup.py): started\n",
      "  Building wheel for sklearn-preprocessor (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor: filename=sklearn_preprocessor-1.0.0-py2.py3-none-any.whl size=9045 sha256=42c09a04b4cbe4dad7e582474aae1eba1ccafa6421a9a4040563d4754c7b08b9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pz6bq2jp/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-preprocessor-1.0.0\u001b[0m\n",
      "\u001b[34m2020-07-06 00:21:09,395 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-06 00:21:09,406 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"scikit-learn-preprocessor-2020-07-06-00-17-54\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-17-54/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sklearn_preprocessor\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sklearn_preprocessor.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sklearn_preprocessor.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sklearn_preprocessor\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-17-54/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"scikit-learn-preprocessor-2020-07-06-00-17-54\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-17-54/source/sourcedir.tar.gz\",\"module_name\":\"sklearn_preprocessor\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sklearn_preprocessor.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m sklearn_preprocessor\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\n",
      "2020-07-06 00:21:24 Uploading - Uploading generated training model\n",
      "2020-07-06 00:21:24 Completed - Training job completed\n",
      "\u001b[34mfile : /opt/ml/input/data/train/train.csv\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[34m0          1999  2719.698057             3  ...            y     y  416754\u001b[0m\n",
      "\u001b[34m1          1997  3265.892734             2  ...            n     y  447883\u001b[0m\n",
      "\u001b[34m2          1999  3784.114581             3  ...            n     n  577267\u001b[0m\n",
      "\u001b[34m3          1987  2402.077363             3  ...            n     y  286561\u001b[0m\n",
      "\u001b[34m4          1995  3048.868314             4  ...            y     y  465280\u001b[0m\n",
      "\u001b[34m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[34m895        1990  2920.133106             4  ...            y     y  436719\u001b[0m\n",
      "\u001b[34m896        1990  2413.017484             6  ...            y     n  354852\u001b[0m\n",
      "\u001b[34m897        1993  1597.616587             2  ...            n     y  183942\u001b[0m\n",
      "\u001b[34m898        1990  2030.806230             2  ...            n     y  216670\u001b[0m\n",
      "\u001b[34m899        1987  1444.973487             3  ...            y     y  163296\n",
      "\u001b[0m\n",
      "\u001b[34m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\u001b[0m\n",
      "\u001b[34msaved model!\u001b[0m\n",
      "\u001b[34m2020-07-06 00:21:11,801 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 70\n",
      "Billable seconds: 70\n",
      "preprocessing fit input data at  3  for loc  Houston_TX\n",
      "2020-07-06 00:21:37 Starting - Starting the training job...\n",
      "2020-07-06 00:21:40 Starting - Launching requested ML instances.........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-06 00:23:22 Starting - Preparing the instances for training......\n",
      "2020-07-06 00:24:20 Downloading - Downloading input data...\n",
      "2020-07-06 00:25:07 Training - Training image download completed. Training in progress...\u001b[34m2020-07-06 00:25:09,025 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-07-06 00:25:09,027 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-06 00:25:09,037 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-07-06 00:25:09,476 sagemaker-containers INFO     Module sklearn_preprocessor does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-07-06 00:25:09,477 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-07-06 00:25:09,477 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-07-06 00:25:09,477 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-preprocessor\n",
      "  Building wheel for sklearn-preprocessor (setup.py): started\n",
      "  Building wheel for sklearn-preprocessor (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-preprocessor: filename=sklearn_preprocessor-1.0.0-py2.py3-none-any.whl size=9045 sha256=0a7837d492bb4de2129bea7ab247bed94eaded877207117b79904c5d463bda9d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-s1xm8e3o/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-preprocessor\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-preprocessor-1.0.0\u001b[0m\n",
      "\u001b[34m2020-07-06 00:25:10,962 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-06 00:25:10,972 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"scikit-learn-preprocessor-2020-07-06-00-21-36\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-21-36/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sklearn_preprocessor\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sklearn_preprocessor.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sklearn_preprocessor.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sklearn_preprocessor\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-21-36/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"scikit-learn-preprocessor-2020-07-06-00-21-36\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-555360056434/scikit-learn-preprocessor-2020-07-06-00-21-36/source/sourcedir.tar.gz\",\"module_name\":\"sklearn_preprocessor\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sklearn_preprocessor.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m sklearn_preprocessor\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mfile : /opt/ml/input/data/train/train.csv\n",
      "     YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  ...  FRONT_PORCH  DECK   PRICE\u001b[0m\n",
      "\u001b[34m0          1992  2681.797598             3  ...            y     n  427369\u001b[0m\n",
      "\u001b[34m1          1993  4619.275787             3  ...            y     n  654891\u001b[0m\n",
      "\u001b[34m2          2000  2422.373461             6  ...            y     n  441056\u001b[0m\n",
      "\u001b[34m3          1992  2915.636216             4  ...            y     y  411245\u001b[0m\n",
      "\u001b[34m4          1993  3537.796512             4  ...            y     n  533869\u001b[0m\n",
      "\u001b[34m..          ...          ...           ...  ...          ...   ...     ...\u001b[0m\n",
      "\u001b[34m895        1983  2131.631532             4  ...            n     y  208844\u001b[0m\n",
      "\u001b[34m896        2003  1748.978088             5  ...            y     n  303946\u001b[0m\n",
      "\u001b[34m897        2000  2612.667247             4  ...            y     y  455900\u001b[0m\n",
      "\u001b[34m898        1994  2199.998965             6  ...            y     y  373349\u001b[0m\n",
      "\u001b[34m899        2006  2166.012912             5  ...            y     y  409701\n",
      "\u001b[0m\n",
      "\u001b[34m[900 rows x 9 columns]\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\u001b[0m\n",
      "\u001b[34msaved model!\u001b[0m\n",
      "\u001b[34m2020-07-06 00:25:13,381 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-07-06 00:25:25 Uploading - Uploading generated training model\n",
      "2020-07-06 00:25:25 Completed - Training job completed\n",
      "Training seconds: 65\n",
      "Billable seconds: 65\n"
     ]
    }
   ],
   "source": [
    "##Launch multiple scikit learn training and batch transform jobs to process the raw synthetic data generated for multiple locations.\n",
    "##Before executing this, take the training instance limits in your account and cost into consideration.\n",
    "\n",
    "#sklearn_preprocessors = []\n",
    "#preprocessing_jobs = []\n",
    "\n",
    "preprocessor_transformers = []\n",
    "\n",
    "for index, loc in enumerate(LOCATIONS[:PARALLEL_TRAINING_JOBS]):\n",
    "    #for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    #index = LOCATIONS.index(loc)\n",
    "    print(\"preprocessing fit input data at \", index , \" for loc \", loc)\n",
    "    job_name='scikit-learn-preprocessor-'+strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "    \n",
    "    #sklearn_preprocessor = SKLearn(\n",
    "     #   entry_point=script_path,\n",
    "      #  role=role,\n",
    "       # train_instance_type=\"ml.c4.xlarge\",\n",
    "        #sagemaker_session=sagemaker_session)\n",
    "    \n",
    "    #sklearn_preprocessor.fit({'train': train_inputs[index]}, job_name=job_name, wait=False)\n",
    "    sklearn_preprocessor.fit({'train': train_inputs[index]}, job_name=job_name, wait=True)\n",
    "    \n",
    "    ##Once the preprocessor is fit, use tranformer to preprocess the raw training data and store the transformed data\n",
    "    ##right back into s3.\n",
    "    \n",
    "    transformer = sklearn_preprocessor.transformer(\n",
    "        instance_count=1, \n",
    "        instance_type='ml.m4.xlarge',\n",
    "        assemble_with = 'Line',\n",
    "        accept = 'text/csv'\n",
    "    )\n",
    "    \n",
    "    preprocessor_transformers.append(transformer)\n",
    "    \n",
    "    #sklearn_preprocessors.append(sklearn_preprocessor)\n",
    "    #preprocessing_jobs.append(job_name)\n",
    "    ##Wait a second to avoid throttling errors\n",
    "    #time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_training_job_to_complete(job_name):\n",
    "    \"\"\" Wait for the training job to complete \"\"\"\n",
    "    print('Waiting for job {} to complete...'.format(job_name))\n",
    "    \n",
    "    waiter = sm_client.get_waiter('training_job_completed_or_stopped')\n",
    "    waiter.wait(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessor_transformers = []\n",
    "\n",
    "#for index, loc in enumerate(LOCATIONS[:PARALLEL_TRAINING_JOBS]):\n",
    " #   print(\"Transforming input data at \", index , \" for loc \", loc)\n",
    "    \n",
    "    #ALREADY sklearn_preprocessor = sklearn_preprocessors[index]\n",
    "    \n",
    "    #ALREADY print (\"Using the preprocessor \", sklearn_preprocessor)\n",
    "    \n",
    "    #transformer = sklearn_preprocessor.transformer(\n",
    "     #   instance_count=1, \n",
    "      #  instance_type='ml.m4.xlarge',\n",
    "       # assemble_with = 'Line',\n",
    "        #accept = 'text/csv'\n",
    "    #)\n",
    "    \n",
    "    #preprocessor_transformers.append(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_batch_transform_job_to_complete(job_name):\n",
    "    \"\"\"Wait for the batch transform job to complete\"\"\"\n",
    "    print('Waiting for job {} to complete...'.format(job_name))\n",
    "    \n",
    "    waiter = sm_client.get_waiter('transform_job_completed_or_stopped')\n",
    "    waiter.wait(TransformJobName=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching batch transform job: sagemaker-scikit-learn-2020-07-06-00-25-49-730\n",
      "Launching batch transform job: sagemaker-scikit-learn-2020-07-06-00-25-50-048\n",
      "Launching batch transform job: sagemaker-scikit-learn-2020-07-06-00-25-52-568\n",
      "Launching batch transform job: sagemaker-scikit-learn-2020-07-06-00-25-56-165\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training input\n",
    "preprocessed_train_data_path = []\n",
    "\n",
    "for transformer in preprocessor_transformers: \n",
    "    index = preprocessor_transformers.index(transformer)\n",
    "    transformer.transform(train_inputs[index], content_type='text/csv')\n",
    "    print('Launching batch transform job: ' + transformer.latest_transform_job.job_name)\n",
    "    preprocessed_train_data_path.append(transformer.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job sagemaker-scikit-learn-2020-07-06-00-25-49-730 to complete...\n",
      "Waiting for job sagemaker-scikit-learn-2020-07-06-00-25-50-048 to complete...\n",
      "Waiting for job sagemaker-scikit-learn-2020-07-06-00-25-52-568 to complete...\n",
      "Waiting for job sagemaker-scikit-learn-2020-07-06-00-25-56-165 to complete...\n"
     ]
    }
   ],
   "source": [
    "#Wait for all the batch transform jobs to finish\n",
    "for transformer in preprocessor_transformers: \n",
    "    job_name=transformer.latest_transform_job.job_name\n",
    "    wait_for_batch_transform_job_to_complete(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_output_key: sagemaker-scikit-learn-2020-07-06-00-25-49-730/train.csv.out\n",
      "Download directory: preprocessed-data/NewYork_NY/\n",
      "transformer_output_key: sagemaker-scikit-learn-2020-07-06-00-25-50-048/train.csv.out\n",
      "Download directory: preprocessed-data/LosAngeles_CA/\n",
      "transformer_output_key: sagemaker-scikit-learn-2020-07-06-00-25-52-568/train.csv.out\n",
      "Download directory: preprocessed-data/Chicago_IL/\n",
      "transformer_output_key: sagemaker-scikit-learn-2020-07-06-00-25-56-165/train.csv.out\n",
      "Download directory: preprocessed-data/Houston_TX/\n"
     ]
    }
   ],
   "source": [
    "##Download the preprocessed data, split into train and val, upload back to S3 in the same directory as tranformer output path\n",
    "for transformer in preprocessor_transformers: \n",
    "    index = preprocessor_transformers.index(transformer)\n",
    "    transformer_output_key='{}/{}'.format(transformer.latest_transform_job.job_name, 'train.csv.out') \n",
    "    \n",
    "    preprocessed_data_download_dir = '{}/'.format(\"preprocessed-data/\"+LOCATIONS[index])\n",
    "    \n",
    "    sagemaker_session.download_data(\n",
    "        path=preprocessed_data_download_dir, \n",
    "        bucket=BUCKET,\n",
    "        key_prefix=transformer_output_key\n",
    "    )\n",
    "    \n",
    "    print(\"transformer_output_key:\", transformer_output_key )\n",
    "    print(\"Download directory:\", preprocessed_data_download_dir )\n",
    "    \n",
    "    train_df = pd.read_csv(preprocessed_data_download_dir+\"/train.csv.out\")\n",
    "    \n",
    "    #Spliting data into train and test in 70:30 ratio\n",
    "    _train, _val = train_test_split(train_df, test_size=0.3)\n",
    "    \n",
    "    _train.to_csv(preprocessed_data_download_dir+'train.csv', sep=',', header=False, index=False)\n",
    "    _val.to_csv(preprocessed_data_download_dir+'val.csv', sep=',', header=False, index=False)\n",
    "    \n",
    "    \n",
    "    train_input = sagemaker_session.upload_data(\n",
    "        path='{}/{}'.format(preprocessed_data_download_dir, 'train.csv'), \n",
    "        bucket=BUCKET,\n",
    "        key_prefix='{}'.format(transformer.latest_transform_job.job_name, 'train.csv'))\n",
    "    \n",
    "    val_input = sagemaker_session.upload_data(\n",
    "        path='{}/{}'.format(preprocessed_data_download_dir, 'val.csv'), \n",
    "        bucket=BUCKET,\n",
    "        key_prefix='{}'.format(transformer.latest_transform_job.job_name, 'val.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-49-730\n",
      "s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-50-048\n",
      "s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-52-568\n",
      "s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-56-165\n"
     ]
    }
   ],
   "source": [
    "##S3 location of the preprocessed data\n",
    "for preprocessed_train_data in preprocessed_train_data_path : \n",
    "    print(preprocessed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, loc in enumerate(LOCATIONS[:PARALLEL_TRAINING_JOBS]):\n",
    "    preprocessed_data_download_dir = '{}/'.format(\"preprocessed-data/\"+LOCATIONS[index])\n",
    "    path='{}/{}'.format(preprocessed_data_download_dir, 'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 : Train house value prediction models for multiple cities <a id='Train-multiple-house-value-prediction-models'></a>\n",
    "\n",
    "In this section, you will use the preprocessed housing data to train multiple linear learner models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch a single training job for a given housing location\n",
    "There is nothing specific to multi-model endpoints in terms of the models it will host. They are trained in the same way as all other SageMaker models. Here we are using the Linear Learner estimator and not waiting for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_training_job(location, transformer):\n",
    "    \"\"\"Launch a linear learner traing job\"\"\"\n",
    "    # clear out old versions of the data\n",
    "    #s3_bucket = s3.Bucket(BUCKET)\n",
    "    #full_input_prefix = '{}/model_prep/{}'.format(DATA_PREFIX, location)\n",
    "    #s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()\n",
    "    \n",
    "    train_inputs = transformer.output_path+\"/train.csv\"\n",
    "    val_inputs = transformer.output_path+\"/val.csv\"\n",
    "    \n",
    "    print(\"train_inputs:\", train_inputs)\n",
    "    print(\"val_inputs:\", val_inputs)\n",
    "     \n",
    "    #_job = 'll-{}'.format(location.replace('_', '-'))  ##CHECK\n",
    "    full_output_prefix = '{}/model_artifacts/{}'.format(DATA_PREFIX, location)\n",
    "    s3_output_path = 's3://{}/{}'.format(BUCKET, full_output_prefix)\n",
    "    \n",
    "    linear_estimator = sagemaker.estimator.Estimator(\n",
    "                            container,\n",
    "                            role, \n",
    "                            train_instance_count=1, \n",
    "                            train_instance_type='ml.c4.xlarge',\n",
    "                            output_path=s3_output_path,\n",
    "                            sagemaker_session=sagemaker_session)\n",
    "    \n",
    "    linear_estimator.set_hyperparameters(feature_dim=10,\n",
    "                           mini_batch_size=100,\n",
    "                           predictor_type='regressor',\n",
    "                           epochs=10,\n",
    "                           num_models=32,\n",
    "                           loss='absolute_loss')\n",
    "    \n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    train_input = sagemaker.s3_input(s3_data=train_inputs, \n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='text/csv;label_size=1')\n",
    "    val_input   = sagemaker.s3_input(s3_data=val_inputs,\n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='text/csv;label_size=1')\n",
    "    \n",
    "    remote_inputs = {'train': train_input, 'validation': val_input}\n",
    "     \n",
    "    linear_estimator.fit(remote_inputs, wait=False)\n",
    "   \n",
    "    #return linear_estimator, linear_estimator.latest_training_job.name\n",
    "    return linear_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kick off a model training job for each housing location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inputs: s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-49-730/train.csv\n",
      "val_inputs: s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-49-730/val.csv\n",
      "train_inputs: s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-50-048/train.csv\n",
      "val_inputs: s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-50-048/val.csv\n",
      "train_inputs: s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-52-568/train.csv\n",
      "val_inputs: s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-52-568/val.csv\n",
      "train_inputs: s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-56-165/train.csv\n",
      "val_inputs: s3://sagemaker-us-east-1-555360056434/sagemaker-scikit-learn-2020-07-06-00-25-56-165/val.csv\n",
      "4 training jobs launched: ['linear-learner-2020-07-06-00-32-26-786', 'linear-learner-2020-07-06-00-32-26-939', 'linear-learner-2020-07-06-00-32-27-451', 'linear-learner-2020-07-06-00-32-31-352']\n"
     ]
    }
   ],
   "source": [
    "training_jobs = []\n",
    "#linear_estimators = []\n",
    "\n",
    "#PARALLEL_TRAINING_JOBS = 4 \n",
    "    \n",
    "for transformer,loc in zip(preprocessor_transformers, LOCATIONS[:PARALLEL_TRAINING_JOBS]): \n",
    "    job = launch_training_job(loc, transformer)\n",
    "    training_jobs.append(job)\n",
    "    #linear_estimators.append(estimator)\n",
    "    \n",
    "print('{} training jobs launched: {}'.format(len(training_jobs), training_jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for all  training jobs to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job linear-learner-2020-07-06-00-32-26-786 to complete...\n",
      "Waiting for job linear-learner-2020-07-06-00-32-26-939 to complete...\n",
      "Waiting for job linear-learner-2020-07-06-00-32-27-451 to complete...\n",
      "Waiting for job linear-learner-2020-07-06-00-32-31-352 to complete...\n"
     ]
    }
   ],
   "source": [
    "#Wait for the jobs to finish\n",
    "for job_name in training_jobs:\n",
    "    wait_for_training_job_to_complete(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - Create Sagemaker model with multi model support <a id='Create-sagemaker-multi-model-support'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_model_artifacts(model_data_url):\n",
    "    # extract the s3 key from the full url to the model artifacts\n",
    "    _s3_key = model_data_url.split('s3://{}/'.format(BUCKET))[1]\n",
    "    # get the part of the key that identifies the model within the model artifacts folder\n",
    "    _model_name_plus = _s3_key[_s3_key.find('model_artifacts') + len('model_artifacts') + 1:]\n",
    "    # finally, get the unique model name (e.g., \"NewYork_NY\")\n",
    "    _model_name = re.findall('^(.*?)/', _model_name_plus)[0]\n",
    "    return _s3_key, _model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the model artifacts from the original output of the training job to the place in\n",
    "# s3 where the multi model endpoint will dynamically load individual models\n",
    "#MULTI_MODEL_ARTIFACTS='model_artifacts'\n",
    "def deploy_artifacts_to_mme(job_name):\n",
    "    print(\"job_name :\", job_name)\n",
    "    _resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "    _source_s3_key, _model_name = parse_model_artifacts(_resp['ModelArtifacts']['S3ModelArtifacts'])\n",
    "    _copy_source = {'Bucket': BUCKET, 'Key': _source_s3_key}\n",
    "    _key = '{}/{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, _model_name,_model_name)\n",
    "    \n",
    "    print('Copying {} model\\n   from: {}\\n     to: {}...'.format(_model_name, _source_s3_key, _key))\n",
    "    s3_client.copy_object(Bucket=BUCKET, CopySource=_copy_source, Key=_key)\n",
    "    #return _key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old model artifacts from DEMO_MME_LINEAR_LEARNER/multi_model_artifacts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': '3D690B7EDC723B9E',\n",
       "   'HostId': 'a+yb61YYYCfMML9AFWpjNz6gX71SQ6sdwXiEQNDfprQYGUkTumJNL8pDr/71Y8PIM45pgoFbsNc=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': 'a+yb61YYYCfMML9AFWpjNz6gX71SQ6sdwXiEQNDfprQYGUkTumJNL8pDr/71Y8PIM45pgoFbsNc=',\n",
       "    'x-amz-request-id': '3D690B7EDC723B9E',\n",
       "    'date': 'Mon, 06 Jul 2020 00:41:41 GMT',\n",
       "    'connection': 'close',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Chicago_IL/Chicago_IL.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/LosAngeles_CA/LosAngeles_CA.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/NewYork_NY/NewYork_NY.tar.gz'}]}]"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, clear out old versions of the model artifacts from previous runs of this notebook\n",
    "#s3 = boto3.resource('s3')\n",
    "#s3_bucket = s3.Bucket(BUCKET)\n",
    "#print(BUCKET)\n",
    "full_input_prefix = '{}/multi_model_artifacts'.format(DATA_PREFIX)\n",
    "print('Removing old model artifacts from {}'.format(full_input_prefix))\n",
    "s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_name : linear-learner-2020-07-06-00-32-26-786\n",
      "Copying NewYork_NY model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/model_artifacts/NewYork_NY/linear-learner-2020-07-06-00-32-26-786/output/model.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/NewYork_NY/NewYork_NY.tar.gz...\n",
      "job_name : linear-learner-2020-07-06-00-32-26-939\n",
      "Copying LosAngeles_CA model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/model_artifacts/LosAngeles_CA/linear-learner-2020-07-06-00-32-26-939/output/model.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/LosAngeles_CA/LosAngeles_CA.tar.gz...\n",
      "job_name : linear-learner-2020-07-06-00-32-27-451\n",
      "Copying Chicago_IL model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/model_artifacts/Chicago_IL/linear-learner-2020-07-06-00-32-27-451/output/model.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Chicago_IL/Chicago_IL.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "## Deploy all but the last model trained to MME\n",
    "for job_name in training_jobs[:-1]:\n",
    "    deploy_artifacts_to_mme(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = '{}-{}'.format(HOUSING_MODEL_NAME, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "\n",
    "_model_url  = 's3://{}/{}/{}/'.format(BUCKET, DATA_PREFIX, MULTI_MODEL_ARTIFACTS)\n",
    "\n",
    "ll_multi_model = MultiDataModel(\n",
    "        name=MODEL_NAME,\n",
    "        model_data_prefix=_model_url,\n",
    "        image=container,\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 : Create an inference pipeline with sklearn model and MME linear learner model <a id='Create-inference-pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the inference pipeline using the Pipeline Model API.  This sets up a list of models in a single endpoint; In this example, we configure our pipeline model with the fitted Scikit-learn inference model and the fitted Linear Learner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "scikit_learn_inference_model = sklearn_preprocessor.create_model()\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\n",
    "sm_model = PipelineModel(\n",
    "    name=model_name, \n",
    "    role=role, \n",
    "    sagemaker_session=sagemaker_session,\n",
    "    models=[\n",
    "        scikit_learn_inference_model, \n",
    "        ll_multi_model])\n",
    "\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DELETE AFTER VALIDATING THE FINE GRAINED ACCESS\n",
    "##endpoint_name = 'inference-pipeline-ep-2020-06-30-20-29-20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 :  Exercise the inference pipeline - Get predictions from  different  linear learner models. <a id='Exercise-inference-pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create RealTimePredictor\n",
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "\n",
    "predictor = RealTimePredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=csv_serializer,\n",
    "    content_type=CONTENT_TYPE_CSV,\n",
    "    accept=CONTENT_TYPE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_house_value(features, model_name, predictor_to_use):\n",
    "    print('Using model {} to predict price of this house: {}'.format(model_name,\n",
    "                                                                     features))\n",
    "    body = ','.join(map(str, features)) + '\\n'\n",
    "    start_time = time.time()\n",
    "     \n",
    "    response = predictor_to_use.predict(features, target_model=model_name)\n",
    "    \n",
    "    response_json = json.loads(response)\n",
    "        \n",
    "    predicted_value = response_json['predictions'][0]['score']    \n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print('${:,.2f}, took {:,d} ms\\n'.format(predicted_value, int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [2001, 2965.889936279411, 6, 2.0, 0.94, 1, 'y', 'n']\n",
      "$491,033.69, took 1,605 ms\n",
      "\n",
      "Using model Chicago_IL/Chicago_IL.tar.gz to predict price of this house: [1993, 3918.5819638165285, 5, 3.0, 0.83, 3, 'y', 'y']\n",
      "$630,143.50, took 1,097 ms\n",
      "\n",
      "Using model Chicago_IL/Chicago_IL.tar.gz to predict price of this house: [2002, 3390.27810269848, 6, 1.5, 0.7, 0, 'n', 'y']\n",
      "$522,261.44, took 50 ms\n",
      "\n",
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [2009, 4729.440394975367, 4, 2.5, 0.61, 0, 'n', 'n']\n",
      "$761,033.50, took 43 ms\n",
      "\n",
      "Using model Chicago_IL/Chicago_IL.tar.gz to predict price of this house: [2008, 3617.635669845823, 5, 1.5, 0.6, 2, 'y', 'n']\n",
      "$618,076.25, took 59 ms\n",
      "\n",
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [1987, 3465.345261682198, 5, 2.0, 1.31, 2, 'n', 'n']\n",
      "$490,928.34, took 46 ms\n",
      "\n",
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [1984, 3619.282058687926, 6, 1.5, 1.18, 3, 'y', 'n']\n",
      "$529,520.62, took 46 ms\n",
      "\n",
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [1989, 2644.1881946961466, 5, 1.0, 1.02, 0, 'y', 'y']\n",
      "$332,182.19, took 42 ms\n",
      "\n",
      "Using model Chicago_IL/Chicago_IL.tar.gz to predict price of this house: [1980, 3791.628613711861, 4, 3.0, 1.4, 1, 'y', 'n']\n",
      "$507,590.34, took 40 ms\n",
      "\n",
      "Using model LosAngeles_CA/LosAngeles_CA.tar.gz to predict price of this house: [2003, 2952.1517565973854, 3, 2.0, 0.7, 2, 'n', 'n']\n",
      "$463,028.69, took 40 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    #model_name = LOCATIONS[np.random.randint(1, len(LOCATIONS[:PARALLEL_TRAINING_JOBS - 1]))]\n",
    "    model_name = LOCATIONS[np.random.randint(1, PARALLEL_TRAINING_JOBS - 1)]\n",
    "    full_model_name = '{}/{}.tar.gz'.format(model_name,model_name)\n",
    "    predict_one_house_value(gen_random_house()[:-1], full_model_name,predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7 - Add new model to the endpoint, simply by copying the model artifact to the S3 location\n",
    "<a id='update-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_name : linear-learner-2020-07-06-00-32-31-352\n",
      "Copying Houston_TX model\n",
      "   from: DEMO_MME_LINEAR_LEARNER/model_artifacts/Houston_TX/linear-learner-2020-07-06-00-32-31-352/output/model.tar.gz\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "## Copy the last model\n",
    "last_training_job=training_jobs[PARALLEL_TRAINING_JOBS-1]\n",
    "deploy_artifacts_to_mme(last_training_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/Houston_TX.tar.gz to predict price of this house: [1985, 3457.8311086653835, 4, 2.5, 1.1, 2, 'y', 'n']\n",
      "$499,028.28, took 1,185 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = LOCATIONS[PARALLEL_TRAINING_JOBS-1]\n",
    "full_model_name = '{}/{}.tar.gz'.format(model_name,model_name)\n",
    "predict_one_house_value(gen_random_house()[:-1], full_model_name,predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8 - Latency Analysis <a id='Latency-Analysis'></a>\n",
    "\n",
    "With MME, the models are dynamically loaded into the containers memory of the instance hosting the endpoint when invoked.  Therefore, the model invocation may longer when it is invoked for the first time. And after the model is already in the instance containers memory, the subsequent invocations will be faster. If an instance memory utilization is high and a new model needs to be loaded then unused models are unloaded.  The unloaded models will remain in the instances storage volume and can be loaded into containers memory later without being downloaded from the S3 bucket again.  If the instances storage volume if full, unused models are deleted from storage volume.    \n",
    "Managing the loading/unloading of the models is completely handled by Amazon SageMaker behind the scenes without you having to take any specific actions.  However, it is important to understand this behavior because it has implications on the model invocation latency.\n",
    "\n",
    "Amazon SageMaker provides CloudWatch metrics for multi-model endpoints so you can determine the endpoint usage and the cache hit rate and optimize your endpoint.  To analyze the endpoint and the container behavior, you will invoke multiple models in this sequence :\n",
    "\n",
    "    a. Create 200 copies of the original model and save with different names.\n",
    "    b. Starting with no models loaded into the container, Invoke the first 100 models\n",
    "    c. Invoke the same 100 models again\n",
    "    d. Invoke all 200 models\n",
    "\n",
    "We use this sequence to observe the behavior of the CloudWatch metrics - LoadedModelCount,MemoryUtilization and ModelCacheHit.  You are encouraged to experiment with loading varying number of models to use the CloudWatch charts to help make ongoing decisions on the optimal choice of instance type, instance count, and number of models that a given endpoint should host.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the model artifacts in S3 bucket with new names so we have multiple models to understand the latency behavior.\n",
    "def copy_additional_artifacts_to_mme(num_copies):\n",
    "    \n",
    "    #_model_prefix = \"Houston_TX\"\n",
    "    #_model_name = \"Houston_TX\"\n",
    "    #source_s3_model_key = '{}/{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, _model_prefix,_model_name)\n",
    "    source_s3_model_key = '{}/{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, model_name,model_name)\n",
    "    _copy_source = {'Bucket': BUCKET, 'Key': source_s3_model_key}\n",
    "    for i in range(num_copies):\n",
    "        copy_num = str(i)\n",
    "        new_model_name=\"{}_{}\".format(i, model_name)\n",
    "        #_new_model_name=copy_num + \"_\" + _model_name\n",
    "        dest_s3_model_key = '{}/{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, _model_prefix,new_model_name)\n",
    "        #print('Copying {} model\\n   from: {}\\n     to: {}...'.format(_model_name, source_s3_model_key, dest_s3_model_key))\n",
    "        s3_client.copy_object(Bucket=BUCKET, CopySource=_copy_source, Key=dest_s3_model_key)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create 200 copies of the original model and save with different names.\n",
    "copy_additional_artifacts_to_mme(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Invoke multiple models in a loop\n",
    "#_model_prefix = \"Houston_TX\"\n",
    "#_model_name = \"Houston_TX.tar.gz\"\n",
    "def invoke_multiple_models_mme(model_range_low,model_range_high):\n",
    "    for i in range(model_range_low,model_range_high):\n",
    "        copy_num = str(i)\n",
    "        _new_model_name=copy_num + \"_\" + _model_name\n",
    "        full_model_name = _model_prefix + \"/\" + _new_model_name\n",
    "        predict_one_house_value(gen_random_house()[:-1], full_model_name,predictor)\n",
    "        time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/0_Houston_TX.tar.gz to predict price of this house: [1997, 2565.0242683195997, 5, 2.0, 0.89, 2, 'y', 'y']\n",
      "$419,119.69, took 1,185 ms\n",
      "\n",
      "Using model Houston_TX/1_Houston_TX.tar.gz to predict price of this house: [2001, 2124.638834058075, 2, 1.5, 0.89, 1, 'n', 'n']\n",
      "$299,461.91, took 1,177 ms\n",
      "\n",
      "Using model Houston_TX/2_Houston_TX.tar.gz to predict price of this house: [1999, 3846.298691252199, 4, 2.5, 0.89, 3, 'y', 'n']\n",
      "$636,314.62, took 1,085 ms\n",
      "\n",
      "Using model Houston_TX/3_Houston_TX.tar.gz to predict price of this house: [2001, 1534.997026116037, 2, 2.5, 0.82, 1, 'n', 'y']\n",
      "$222,500.02, took 1,123 ms\n",
      "\n",
      "Using model Houston_TX/4_Houston_TX.tar.gz to predict price of this house: [1979, 3820.2379746770575, 6, 1.0, 0.83, 1, 'n', 'n']\n",
      "$482,779.06, took 1,059 ms\n",
      "\n",
      "Using model Houston_TX/5_Houston_TX.tar.gz to predict price of this house: [1997, 3477.045397952993, 2, 1.0, 1.19, 1, 'n', 'n']\n",
      "$483,016.19, took 1,064 ms\n",
      "\n",
      "Using model Houston_TX/6_Houston_TX.tar.gz to predict price of this house: [1988, 2892.493549033626, 3, 3.0, 0.95, 3, 'y', 'n']\n",
      "$436,914.62, took 1,466 ms\n",
      "\n",
      "Using model Houston_TX/7_Houston_TX.tar.gz to predict price of this house: [1974, 3556.112376076215, 4, 2.5, 1.15, 0, 'n', 'n']\n",
      "$412,034.81, took 1,081 ms\n",
      "\n",
      "Using model Houston_TX/8_Houston_TX.tar.gz to predict price of this house: [1982, 2725.0509734748653, 2, 1.0, 1.09, 1, 'n', 'n']\n",
      "$294,534.69, took 1,078 ms\n",
      "\n",
      "Using model Houston_TX/9_Houston_TX.tar.gz to predict price of this house: [1998, 2857.295737321423, 5, 2.5, 0.82, 2, 'y', 'y']\n",
      "$474,521.22, took 1,085 ms\n",
      "\n",
      "Using model Houston_TX/10_Houston_TX.tar.gz to predict price of this house: [1999, 2599.0901737512777, 6, 2.5, 0.74, 1, 'y', 'y']\n",
      "$434,516.75, took 1,106 ms\n",
      "\n",
      "Using model Houston_TX/11_Houston_TX.tar.gz to predict price of this house: [1987, 3954.6021602070296, 4, 1.5, 0.95, 0, 'y', 'n']\n",
      "$536,326.88, took 1,067 ms\n",
      "\n",
      "Using model Houston_TX/12_Houston_TX.tar.gz to predict price of this house: [2007, 1811.5016275131245, 6, 1.5, 1.25, 0, 'n', 'n']\n",
      "$315,371.53, took 1,071 ms\n",
      "\n",
      "Using model Houston_TX/13_Houston_TX.tar.gz to predict price of this house: [1994, 1245.464938118672, 4, 3.0, 1.06, 1, 'y', 'n']\n",
      "$199,821.00, took 1,505 ms\n",
      "\n",
      "Using model Houston_TX/14_Houston_TX.tar.gz to predict price of this house: [1988, 2328.2492992742727, 4, 3.0, 1.32, 1, 'n', 'y']\n",
      "$317,955.50, took 1,070 ms\n",
      "\n",
      "Using model Houston_TX/15_Houston_TX.tar.gz to predict price of this house: [2005, 3636.22601536233, 4, 1.0, 1.32, 2, 'n', 'n']\n",
      "$584,086.94, took 1,083 ms\n",
      "\n",
      "Using model Houston_TX/16_Houston_TX.tar.gz to predict price of this house: [1995, 2859.4425433530855, 3, 2.5, 0.97, 2, 'n', 'y']\n",
      "$422,074.91, took 1,053 ms\n",
      "\n",
      "Using model Houston_TX/17_Houston_TX.tar.gz to predict price of this house: [2006, 3382.1599313703773, 2, 3.0, 0.84, 1, 'y', 'y']\n",
      "$554,617.12, took 1,026 ms\n",
      "\n",
      "Using model Houston_TX/18_Houston_TX.tar.gz to predict price of this house: [1994, 3195.7064459749263, 4, 1.5, 0.81, 1, 'n', 'n']\n",
      "$447,168.38, took 1,054 ms\n",
      "\n",
      "Using model Houston_TX/19_Houston_TX.tar.gz to predict price of this house: [2010, 1406.3030227677202, 2, 2.5, 1.06, 1, 'y', 'n']\n",
      "$273,051.75, took 1,063 ms\n",
      "\n",
      "Using model Houston_TX/20_Houston_TX.tar.gz to predict price of this house: [1991, 1031.162280388117, 2, 1.5, 1.46, 0, 'y', 'n']\n",
      "$101,924.53, took 1,063 ms\n",
      "\n",
      "Using model Houston_TX/21_Houston_TX.tar.gz to predict price of this house: [2014, 2248.609016985203, 2, 1.0, 1.19, 2, 'n', 'n']\n",
      "$394,072.44, took 1,081 ms\n",
      "\n",
      "Using model Houston_TX/22_Houston_TX.tar.gz to predict price of this house: [1996, 2746.0331686569734, 2, 1.5, 0.77, 1, 'y', 'n']\n",
      "$387,153.91, took 1,057 ms\n",
      "\n",
      "Using model Houston_TX/23_Houston_TX.tar.gz to predict price of this house: [1980, 1987.6292559942644, 2, 1.0, 1.1, 0, 'y', 'n']\n",
      "$178,828.97, took 1,096 ms\n",
      "\n",
      "Using model Houston_TX/24_Houston_TX.tar.gz to predict price of this house: [2009, 2680.9371963645963, 5, 1.0, 0.99, 0, 'n', 'n']\n",
      "$433,526.28, took 1,080 ms\n",
      "\n",
      "Using model Houston_TX/25_Houston_TX.tar.gz to predict price of this house: [1997, 2454.3319451107786, 3, 3.0, 0.33, 0, 'y', 'n']\n",
      "$358,074.50, took 1,170 ms\n",
      "\n",
      "Using model Houston_TX/26_Houston_TX.tar.gz to predict price of this house: [2007, 3021.326518006429, 6, 1.5, 0.53, 3, 'y', 'n']\n",
      "$548,866.12, took 1,090 ms\n",
      "\n",
      "Using model Houston_TX/27_Houston_TX.tar.gz to predict price of this house: [1988, 2727.2593920880536, 5, 3.0, 1.32, 3, 'y', 'n']\n",
      "$440,049.97, took 1,091 ms\n",
      "\n",
      "Using model Houston_TX/28_Houston_TX.tar.gz to predict price of this house: [1990, 3436.786277262381, 3, 2.5, 1.38, 1, 'n', 'n']\n",
      "$480,146.12, took 1,125 ms\n",
      "\n",
      "Using model Houston_TX/29_Houston_TX.tar.gz to predict price of this house: [1986, 4031.0259050578115, 2, 3.0, 1.22, 3, 'y', 'y']\n",
      "$592,719.50, took 1,043 ms\n",
      "\n",
      "Using model Houston_TX/30_Houston_TX.tar.gz to predict price of this house: [1995, 3635.5245847598726, 3, 1.0, 1.05, 2, 'y', 'y']\n",
      "$538,320.56, took 1,054 ms\n",
      "\n",
      "Using model Houston_TX/31_Houston_TX.tar.gz to predict price of this house: [1991, 2801.479539931079, 5, 2.0, 1.17, 0, 'n', 'y']\n",
      "$381,728.22, took 1,084 ms\n",
      "\n",
      "Using model Houston_TX/32_Houston_TX.tar.gz to predict price of this house: [2010, 2508.748869029725, 5, 2.0, 1.07, 0, 'n', 'n']\n",
      "$429,159.75, took 1,090 ms\n",
      "\n",
      "Using model Houston_TX/33_Houston_TX.tar.gz to predict price of this house: [1976, 4281.373040130034, 3, 3.0, 1.1, 2, 'y', 'y']\n",
      "$575,648.62, took 1,054 ms\n",
      "\n",
      "Using model Houston_TX/34_Houston_TX.tar.gz to predict price of this house: [1981, 2599.1986576087766, 4, 2.5, 0.78, 0, 'y', 'n']\n",
      "$314,040.19, took 1,074 ms\n",
      "\n",
      "Using model Houston_TX/35_Houston_TX.tar.gz to predict price of this house: [2010, 3647.2988959810145, 5, 3.0, 0.8, 0, 'y', 'y']\n",
      "$630,158.31, took 1,180 ms\n",
      "\n",
      "Using model Houston_TX/36_Houston_TX.tar.gz to predict price of this house: [2017, 3821.1626355852145, 5, 1.5, 1.15, 3, 'n', 'n']\n",
      "$699,790.00, took 1,064 ms\n",
      "\n",
      "Using model Houston_TX/37_Houston_TX.tar.gz to predict price of this house: [2013, 2955.5801750167443, 5, 1.5, 0.99, 1, 'n', 'y']\n",
      "$515,213.12, took 1,087 ms\n",
      "\n",
      "Using model Houston_TX/38_Houston_TX.tar.gz to predict price of this house: [1986, 2638.989066847411, 6, 1.0, 1.22, 3, 'y', 'n']\n",
      "$394,928.56, took 1,075 ms\n",
      "\n",
      "Using model Houston_TX/39_Houston_TX.tar.gz to predict price of this house: [1973, 4162.314741479463, 2, 3.0, 1.05, 1, 'y', 'y']\n",
      "$516,923.50, took 1,092 ms\n",
      "\n",
      "Using model Houston_TX/40_Houston_TX.tar.gz to predict price of this house: [2002, 3271.565356440003, 3, 1.0, 1.01, 1, 'n', 'y']\n",
      "$481,503.41, took 1,065 ms\n",
      "\n",
      "Using model Houston_TX/41_Houston_TX.tar.gz to predict price of this house: [1998, 2825.3819691978683, 5, 2.5, 1.27, 1, 'y', 'y']\n",
      "$463,704.38, took 1,044 ms\n",
      "\n",
      "Using model Houston_TX/42_Houston_TX.tar.gz to predict price of this house: [1980, 1721.8058631132026, 3, 2.0, 1.11, 1, 'n', 'n']\n",
      "$159,127.09, took 1,064 ms\n",
      "\n",
      "Using model Houston_TX/43_Houston_TX.tar.gz to predict price of this house: [1992, 3710.586452607651, 2, 1.5, 0.8, 0, 'n', 'y']\n",
      "$477,924.28, took 1,045 ms\n",
      "\n",
      "Using model Houston_TX/44_Houston_TX.tar.gz to predict price of this house: [1992, 3950.911777578553, 4, 2.0, 0.69, 0, 'n', 'y']\n",
      "$540,830.88, took 1,095 ms\n",
      "\n",
      "Using model Houston_TX/45_Houston_TX.tar.gz to predict price of this house: [2009, 2683.1147905101025, 4, 2.0, 0.77, 2, 'y', 'y']\n",
      "$482,324.34, took 1,064 ms\n",
      "\n",
      "Using model Houston_TX/46_Houston_TX.tar.gz to predict price of this house: [1996, 1164.7171806646516, 2, 2.5, 0.54, 2, 'n', 'y']\n",
      "$151,482.50, took 1,085 ms\n",
      "\n",
      "Using model Houston_TX/47_Houston_TX.tar.gz to predict price of this house: [1970, 1678.3986401272573, 2, 1.0, 1.04, 1, 'y', 'n']\n",
      "$97,089.66, took 1,073 ms\n",
      "\n",
      "Using model Houston_TX/48_Houston_TX.tar.gz to predict price of this house: [2017, 3921.5239661119135, 4, 2.0, 0.98, 0, 'n', 'y']\n",
      "$663,126.31, took 1,073 ms\n",
      "\n",
      "Using model Houston_TX/49_Houston_TX.tar.gz to predict price of this house: [1993, 4276.197735812006, 4, 2.0, 1.35, 0, 'y', 'y']\n",
      "$627,861.06, took 1,098 ms\n",
      "\n",
      "Using model Houston_TX/50_Houston_TX.tar.gz to predict price of this house: [1989, 2915.419336366495, 2, 3.0, 0.97, 2, 'n', 'n']\n",
      "$400,275.16, took 1,079 ms\n",
      "\n",
      "Using model Houston_TX/51_Houston_TX.tar.gz to predict price of this house: [1991, 2244.8328017565755, 3, 1.0, 0.96, 0, 'n', 'n']\n",
      "$258,706.34, took 1,055 ms\n",
      "\n",
      "Using model Houston_TX/52_Houston_TX.tar.gz to predict price of this house: [1968, 3410.8489083055215, 6, 2.0, 0.94, 0, 'n', 'n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$370,235.91, took 1,054 ms\n",
      "\n",
      "Using model Houston_TX/53_Houston_TX.tar.gz to predict price of this house: [1976, 3435.711022762174, 4, 2.0, 0.81, 3, 'n', 'n']\n",
      "$433,348.91, took 1,073 ms\n",
      "\n",
      "Using model Houston_TX/54_Houston_TX.tar.gz to predict price of this house: [1994, 3057.4187892410355, 3, 2.0, 1.27, 0, 'n', 'y']\n",
      "$415,964.09, took 1,094 ms\n",
      "\n",
      "Using model Houston_TX/55_Houston_TX.tar.gz to predict price of this house: [1997, 3619.9813391122443, 6, 3.0, 1.15, 0, 'y', 'y']\n",
      "$580,307.00, took 1,078 ms\n",
      "\n",
      "Using model Houston_TX/56_Houston_TX.tar.gz to predict price of this house: [1998, 1373.4094471228366, 6, 1.0, 1.38, 1, 'y', 'y']\n",
      "$233,503.53, took 1,066 ms\n",
      "\n",
      "Using model Houston_TX/57_Houston_TX.tar.gz to predict price of this house: [2012, 2374.4588553443946, 4, 1.0, 0.88, 0, 'n', 'n']\n",
      "$389,029.50, took 1,033 ms\n",
      "\n",
      "Using model Houston_TX/58_Houston_TX.tar.gz to predict price of this house: [1974, 2638.7830126166587, 2, 2.0, 0.73, 0, 'n', 'y']\n",
      "$234,718.50, took 1,062 ms\n",
      "\n",
      "Using model Houston_TX/59_Houston_TX.tar.gz to predict price of this house: [1991, 3453.972176832428, 6, 2.5, 1.0, 3, 'n', 'n']\n",
      "$541,117.56, took 1,123 ms\n",
      "\n",
      "Using model Houston_TX/60_Houston_TX.tar.gz to predict price of this house: [1989, 2868.953994271495, 6, 1.5, 0.6, 1, 'y', 'y']\n",
      "$408,904.78, took 1,054 ms\n",
      "\n",
      "Using model Houston_TX/61_Houston_TX.tar.gz to predict price of this house: [1988, 2762.406496335902, 4, 2.0, 0.89, 1, 'y', 'y']\n",
      "$380,207.16, took 1,055 ms\n",
      "\n",
      "Using model Houston_TX/62_Houston_TX.tar.gz to predict price of this house: [1993, 3424.1036681398095, 4, 1.0, 0.97, 1, 'y', 'n']\n",
      "$492,499.28, took 1,094 ms\n",
      "\n",
      "Using model Houston_TX/63_Houston_TX.tar.gz to predict price of this house: [1993, 2819.0320746740886, 5, 1.0, 0.96, 0, 'n', 'n']\n",
      "$376,332.66, took 1,103 ms\n",
      "\n",
      "Using model Houston_TX/64_Houston_TX.tar.gz to predict price of this house: [1999, 3223.7090250118763, 3, 2.0, 0.99, 1, 'n', 'n']\n",
      "$476,280.75, took 1,146 ms\n",
      "\n",
      "Using model Houston_TX/65_Houston_TX.tar.gz to predict price of this house: [1984, 2786.35629721894, 6, 2.5, 1.01, 1, 'y', 'n']\n",
      "$397,057.69, took 1,075 ms\n",
      "\n",
      "Using model Houston_TX/66_Houston_TX.tar.gz to predict price of this house: [1980, 3239.086758529763, 3, 2.5, 0.48, 1, 'n', 'n']\n",
      "$384,330.97, took 1,157 ms\n",
      "\n",
      "Using model Houston_TX/67_Houston_TX.tar.gz to predict price of this house: [1991, 2870.5263825135817, 3, 1.0, 1.64, 1, 'n', 'y']\n",
      "$379,680.09, took 1,070 ms\n",
      "\n",
      "Using model Houston_TX/68_Houston_TX.tar.gz to predict price of this house: [2012, 2918.2406660632264, 3, 1.0, 1.22, 0, 'n', 'n']\n",
      "$467,457.62, took 1,096 ms\n",
      "\n",
      "Using model Houston_TX/69_Houston_TX.tar.gz to predict price of this house: [2005, 3019.5293916482365, 5, 1.5, 1.21, 3, 'n', 'n']\n",
      "$521,425.97, took 1,079 ms\n",
      "\n",
      "Using model Houston_TX/70_Houston_TX.tar.gz to predict price of this house: [2015, 2733.9536559350463, 3, 2.5, 0.41, 1, 'n', 'n']\n",
      "$476,069.81, took 1,060 ms\n",
      "\n",
      "Using model Houston_TX/71_Houston_TX.tar.gz to predict price of this house: [1995, 1447.607113359474, 6, 2.5, 0.92, 3, 'y', 'y']\n",
      "$273,660.00, took 1,073 ms\n",
      "\n",
      "Using model Houston_TX/72_Houston_TX.tar.gz to predict price of this house: [2001, 3034.63530247122, 3, 1.5, 1.06, 0, 'n', 'n']\n",
      "$436,342.78, took 1,925 ms\n",
      "\n",
      "Using model Houston_TX/73_Houston_TX.tar.gz to predict price of this house: [1989, 2168.195823190484, 4, 2.0, 1.2, 2, 'n', 'n']\n",
      "$297,262.56, took 1,195 ms\n",
      "\n",
      "Using model Houston_TX/74_Houston_TX.tar.gz to predict price of this house: [1982, 2518.452683175176, 3, 3.0, 0.85, 2, 'n', 'n']\n",
      "$314,421.56, took 1,118 ms\n",
      "\n",
      "Using model Houston_TX/75_Houston_TX.tar.gz to predict price of this house: [1981, 2947.374447256919, 5, 1.5, 1.42, 3, 'n', 'y']\n",
      "$396,647.41, took 1,102 ms\n",
      "\n",
      "Using model Houston_TX/76_Houston_TX.tar.gz to predict price of this house: [1978, 1941.3966697621474, 5, 1.5, 0.65, 1, 'y', 'y']\n",
      "$205,629.58, took 1,096 ms\n",
      "\n",
      "Using model Houston_TX/77_Houston_TX.tar.gz to predict price of this house: [2005, 3215.186719536059, 4, 2.0, 0.61, 1, 'n', 'n']\n",
      "$507,198.41, took 1,301 ms\n",
      "\n",
      "Using model Houston_TX/78_Houston_TX.tar.gz to predict price of this house: [1982, 2487.613066315117, 6, 2.5, 0.81, 1, 'n', 'n']\n",
      "$318,108.81, took 1,131 ms\n",
      "\n",
      "Using model Houston_TX/79_Houston_TX.tar.gz to predict price of this house: [1996, 3572.541983118301, 4, 1.5, 1.2, 3, 'y', 'y']\n",
      "$569,382.94, took 1,063 ms\n",
      "\n",
      "Using model Houston_TX/80_Houston_TX.tar.gz to predict price of this house: [1988, 3303.096214399298, 2, 1.5, 1.01, 2, 'n', 'y']\n",
      "$430,304.06, took 1,116 ms\n",
      "\n",
      "Using model Houston_TX/81_Houston_TX.tar.gz to predict price of this house: [2013, 2666.4405448286407, 2, 3.0, 0.92, 2, 'n', 'y']\n",
      "$476,256.47, took 1,090 ms\n",
      "\n",
      "Using model Houston_TX/82_Houston_TX.tar.gz to predict price of this house: [1999, 3891.15571139648, 5, 2.5, 1.18, 3, 'n', 'n']\n",
      "$639,067.81, took 1,058 ms\n",
      "\n",
      "Using model Houston_TX/83_Houston_TX.tar.gz to predict price of this house: [1981, 3213.8021426465693, 5, 1.5, 0.53, 0, 'n', 'y']\n",
      "$375,661.44, took 1,084 ms\n",
      "\n",
      "Using model Houston_TX/84_Houston_TX.tar.gz to predict price of this house: [1997, 816.4204505134485, 2, 1.5, 1.37, 2, 'y', 'y']\n",
      "$124,498.47, took 1,057 ms\n",
      "\n",
      "Using model Houston_TX/85_Houston_TX.tar.gz to predict price of this house: [1987, 3530.0201448128673, 5, 3.0, 1.29, 3, 'n', 'n']\n",
      "$536,028.25, took 1,084 ms\n",
      "\n",
      "Using model Houston_TX/86_Houston_TX.tar.gz to predict price of this house: [1983, 2826.770598708515, 4, 1.0, 0.9, 3, 'n', 'y']\n",
      "$359,855.44, took 1,099 ms\n",
      "\n",
      "Using model Houston_TX/87_Houston_TX.tar.gz to predict price of this house: [1993, 3936.371792089046, 4, 1.0, 0.35, 2, 'n', 'n']\n",
      "$552,609.75, took 1,076 ms\n",
      "\n",
      "Using model Houston_TX/88_Houston_TX.tar.gz to predict price of this house: [1991, 3706.331276568145, 4, 1.0, 1.13, 3, 'n', 'y']\n",
      "$536,254.00, took 1,063 ms\n",
      "\n",
      "Using model Houston_TX/89_Houston_TX.tar.gz to predict price of this house: [2002, 2607.6038196138898, 3, 2.0, 0.99, 2, 'y', 'n']\n",
      "$432,373.56, took 1,062 ms\n",
      "\n",
      "Using model Houston_TX/90_Houston_TX.tar.gz to predict price of this house: [1985, 4464.099085016312, 6, 1.5, 0.92, 3, 'n', 'n']\n",
      "$648,139.00, took 1,074 ms\n",
      "\n",
      "Using model Houston_TX/91_Houston_TX.tar.gz to predict price of this house: [2013, 3171.494692487796, 6, 1.0, 1.05, 3, 'y', 'n']\n",
      "$603,089.06, took 1,106 ms\n",
      "\n",
      "Using model Houston_TX/92_Houston_TX.tar.gz to predict price of this house: [1984, 3400.4793018129794, 3, 3.0, 1.04, 1, 'y', 'y']\n",
      "$465,164.88, took 1,103 ms\n",
      "\n",
      "Using model Houston_TX/93_Houston_TX.tar.gz to predict price of this house: [1992, 2379.875086117996, 6, 1.5, 1.39, 0, 'y', 'y']\n",
      "$349,983.12, took 1,083 ms\n",
      "\n",
      "Using model Houston_TX/94_Houston_TX.tar.gz to predict price of this house: [2005, 2552.9057393953844, 3, 1.5, 1.32, 1, 'y', 'n']\n",
      "$422,662.62, took 1,053 ms\n",
      "\n",
      "Using model Houston_TX/95_Houston_TX.tar.gz to predict price of this house: [1985, 3394.3294720138333, 5, 2.5, 1.43, 2, 'y', 'y']\n",
      "$504,663.53, took 1,064 ms\n",
      "\n",
      "Using model Houston_TX/96_Houston_TX.tar.gz to predict price of this house: [1993, 3677.181753861657, 3, 2.0, 1.11, 3, 'n', 'n']\n",
      "$547,594.69, took 1,085 ms\n",
      "\n",
      "Using model Houston_TX/97_Houston_TX.tar.gz to predict price of this house: [1992, 3084.6736805274354, 6, 3.0, 0.84, 0, 'y', 'n']\n",
      "$470,656.44, took 1,084 ms\n",
      "\n",
      "Using model Houston_TX/98_Houston_TX.tar.gz to predict price of this house: [1996, 3200.1586827779734, 6, 2.5, 1.34, 1, 'y', 'n']\n",
      "$524,248.19, took 1,084 ms\n",
      "\n",
      "Using model Houston_TX/99_Houston_TX.tar.gz to predict price of this house: [1982, 3174.323513844508, 6, 1.5, 1.83, 1, 'y', 'n']\n",
      "$446,679.88, took 1,064 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Starting with no models loaded into the container\n",
    "##Invoke the first 100 models\n",
    "invoke_multiple_models_mme(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/0_Houston_TX.tar.gz to predict price of this house: [2004, 2794.484558800993, 6, 1.0, 0.57, 0, 'y', 'n']\n",
      "$449,006.00, took 44 ms\n",
      "\n",
      "Using model Houston_TX/1_Houston_TX.tar.gz to predict price of this house: [1976, 2192.398435096421, 5, 2.0, 0.89, 3, 'n', 'y']\n",
      "$255,495.86, took 36 ms\n",
      "\n",
      "Using model Houston_TX/2_Houston_TX.tar.gz to predict price of this house: [1993, 3384.7252863609297, 5, 3.0, 0.52, 1, 'n', 'y']\n",
      "$497,157.50, took 36 ms\n",
      "\n",
      "Using model Houston_TX/3_Houston_TX.tar.gz to predict price of this house: [1979, 3577.556959770925, 2, 1.5, 1.23, 1, 'n', 'y']\n",
      "$417,818.12, took 36 ms\n",
      "\n",
      "Using model Houston_TX/4_Houston_TX.tar.gz to predict price of this house: [1972, 2129.4818467259574, 4, 3.0, 1.2, 0, 'y', 'n']\n",
      "$215,097.62, took 35 ms\n",
      "\n",
      "Using model Houston_TX/5_Houston_TX.tar.gz to predict price of this house: [1988, 4667.519348033415, 3, 1.0, 1.13, 3, 'n', 'y']\n",
      "$656,766.62, took 38 ms\n",
      "\n",
      "Using model Houston_TX/6_Houston_TX.tar.gz to predict price of this house: [2003, 3011.047376300804, 3, 2.5, 0.58, 3, 'n', 'y']\n",
      "$490,938.78, took 37 ms\n",
      "\n",
      "Using model Houston_TX/7_Houston_TX.tar.gz to predict price of this house: [1995, 3026.261227361758, 6, 1.5, 1.03, 2, 'n', 'n']\n",
      "$466,304.91, took 35 ms\n",
      "\n",
      "Using model Houston_TX/8_Houston_TX.tar.gz to predict price of this house: [2003, 2996.0833240778056, 3, 1.0, 0.38, 1, 'y', 'y']\n",
      "$452,585.62, took 36 ms\n",
      "\n",
      "Using model Houston_TX/9_Houston_TX.tar.gz to predict price of this house: [1994, 3825.620926258728, 4, 1.0, 0.94, 3, 'n', 'n']\n",
      "$566,808.00, took 36 ms\n",
      "\n",
      "Using model Houston_TX/10_Houston_TX.tar.gz to predict price of this house: [1985, 2362.504403456277, 3, 1.0, 0.89, 3, 'n', 'n']\n",
      "$290,192.25, took 36 ms\n",
      "\n",
      "Using model Houston_TX/11_Houston_TX.tar.gz to predict price of this house: [2004, 4099.928769762388, 2, 2.0, 1.04, 2, 'y', 'n']\n",
      "$658,505.25, took 37 ms\n",
      "\n",
      "Using model Houston_TX/12_Houston_TX.tar.gz to predict price of this house: [1989, 2918.598342209301, 4, 1.0, 0.66, 1, 'y', 'n']\n",
      "$390,571.94, took 36 ms\n",
      "\n",
      "Using model Houston_TX/13_Houston_TX.tar.gz to predict price of this house: [2002, 2847.615663192725, 3, 2.0, 1.15, 1, 'y', 'n']\n",
      "$457,116.25, took 38 ms\n",
      "\n",
      "Using model Houston_TX/14_Houston_TX.tar.gz to predict price of this house: [1994, 2143.166612508089, 5, 1.5, 0.49, 0, 'n', 'n']\n",
      "$277,393.03, took 38 ms\n",
      "\n",
      "Using model Houston_TX/15_Houston_TX.tar.gz to predict price of this house: [1979, 2868.3765082389464, 5, 2.0, 1.29, 0, 'n', 'y']\n",
      "$336,039.16, took 35 ms\n",
      "\n",
      "Using model Houston_TX/16_Houston_TX.tar.gz to predict price of this house: [1986, 2123.018957940101, 4, 1.0, 1.14, 1, 'n', 'y']\n",
      "$243,079.61, took 38 ms\n",
      "\n",
      "Using model Houston_TX/17_Houston_TX.tar.gz to predict price of this house: [1996, 3020.0966016929206, 6, 3.0, 0.76, 1, 'y', 'y']\n",
      "$491,773.38, took 38 ms\n",
      "\n",
      "Using model Houston_TX/18_Houston_TX.tar.gz to predict price of this house: [1987, 4020.1484809540184, 5, 1.0, 0.56, 1, 'y', 'y']\n",
      "$554,602.88, took 46 ms\n",
      "\n",
      "Using model Houston_TX/19_Houston_TX.tar.gz to predict price of this house: [1986, 4182.933061995478, 2, 1.5, 1.17, 3, 'n', 'n']\n",
      "$573,269.94, took 40 ms\n",
      "\n",
      "Using model Houston_TX/20_Houston_TX.tar.gz to predict price of this house: [1997, 2734.2074285455055, 5, 3.0, 0.91, 2, 'n', 'n']\n",
      "$441,921.44, took 41 ms\n",
      "\n",
      "Using model Houston_TX/21_Houston_TX.tar.gz to predict price of this house: [2000, 2727.3708091752433, 6, 3.0, 1.23, 3, 'y', 'n']\n",
      "$506,961.62, took 105 ms\n",
      "\n",
      "Using model Houston_TX/22_Houston_TX.tar.gz to predict price of this house: [1993, 2612.8039953809216, 3, 3.0, 1.32, 3, 'n', 'y']\n",
      "$404,170.00, took 44 ms\n",
      "\n",
      "Using model Houston_TX/23_Houston_TX.tar.gz to predict price of this house: [2011, 3649.121067281075, 3, 2.0, 0.9, 3, 'n', 'y']\n",
      "$624,858.12, took 58 ms\n",
      "\n",
      "Using model Houston_TX/24_Houston_TX.tar.gz to predict price of this house: [2001, 2558.47830442668, 6, 1.5, 0.93, 0, 'y', 'y']\n",
      "$411,726.62, took 47 ms\n",
      "\n",
      "Using model Houston_TX/25_Houston_TX.tar.gz to predict price of this house: [1990, 3442.3785179498095, 2, 2.0, 0.66, 3, 'n', 'y']\n",
      "$476,661.44, took 41 ms\n",
      "\n",
      "Using model Houston_TX/26_Houston_TX.tar.gz to predict price of this house: [2002, 2680.3054620052794, 2, 2.5, 1.17, 1, 'n', 'n']\n",
      "$409,180.19, took 38 ms\n",
      "\n",
      "Using model Houston_TX/27_Houston_TX.tar.gz to predict price of this house: [1991, 3014.3408397027542, 4, 1.0, 1.09, 3, 'n', 'y']\n",
      "$430,698.06, took 36 ms\n",
      "\n",
      "Using model Houston_TX/28_Houston_TX.tar.gz to predict price of this house: [2006, 2286.7911573102906, 5, 1.0, 0.83, 2, 'n', 'n']\n",
      "$385,607.19, took 36 ms\n",
      "\n",
      "Using model Houston_TX/29_Houston_TX.tar.gz to predict price of this house: [2011, 3883.670548760635, 5, 2.0, 0.63, 3, 'n', 'y']\n",
      "$676,135.88, took 36 ms\n",
      "\n",
      "Using model Houston_TX/30_Houston_TX.tar.gz to predict price of this house: [1984, 3332.793275421688, 3, 3.0, 0.8, 3, 'n', 'y']\n",
      "$459,515.06, took 42 ms\n",
      "\n",
      "Using model Houston_TX/31_Houston_TX.tar.gz to predict price of this house: [2004, 3136.155340074693, 2, 1.0, 1.37, 1, 'y', 'y']\n",
      "$487,315.59, took 36 ms\n",
      "\n",
      "Using model Houston_TX/32_Houston_TX.tar.gz to predict price of this house: [1985, 3253.53922399287, 2, 3.0, 0.86, 2, 'y', 'n']\n",
      "$450,112.47, took 37 ms\n",
      "\n",
      "Using model Houston_TX/33_Houston_TX.tar.gz to predict price of this house: [1977, 2700.285770227734, 4, 1.5, 0.98, 3, 'y', 'y']\n",
      "$340,986.91, took 37 ms\n",
      "\n",
      "Using model Houston_TX/34_Houston_TX.tar.gz to predict price of this house: [1995, 2790.775214988216, 6, 3.0, 1.0, 3, 'n', 'n']\n",
      "$467,732.81, took 42 ms\n",
      "\n",
      "Using model Houston_TX/35_Houston_TX.tar.gz to predict price of this house: [1994, 2365.8912948278657, 6, 2.5, 1.02, 3, 'y', 'n']\n",
      "$411,432.72, took 36 ms\n",
      "\n",
      "Using model Houston_TX/36_Houston_TX.tar.gz to predict price of this house: [1990, 4195.208962382762, 6, 2.5, 0.88, 1, 'n', 'y']\n",
      "$615,155.94, took 35 ms\n",
      "\n",
      "Using model Houston_TX/37_Houston_TX.tar.gz to predict price of this house: [1981, 3150.6956223810535, 2, 2.0, 1.12, 2, 'y', 'y']\n",
      "$403,247.09, took 37 ms\n",
      "\n",
      "Using model Houston_TX/38_Houston_TX.tar.gz to predict price of this house: [2009, 4054.511432692979, 3, 1.5, 0.9, 1, 'y', 'y']\n",
      "$659,649.50, took 36 ms\n",
      "\n",
      "Using model Houston_TX/39_Houston_TX.tar.gz to predict price of this house: [2004, 3614.0374121955947, 5, 1.0, 1.62, 1, 'y', 'y']\n",
      "$596,003.38, took 38 ms\n",
      "\n",
      "Using model Houston_TX/40_Houston_TX.tar.gz to predict price of this house: [1987, 3583.836053730448, 5, 2.0, 0.85, 3, 'y', 'y']\n",
      "$538,860.25, took 52 ms\n",
      "\n",
      "Using model Houston_TX/41_Houston_TX.tar.gz to predict price of this house: [1985, 3552.817227475677, 2, 1.0, 0.82, 0, 'n', 'y']\n",
      "$412,849.75, took 36 ms\n",
      "\n",
      "Using model Houston_TX/42_Houston_TX.tar.gz to predict price of this house: [1998, 3841.4835304662847, 5, 2.5, 0.84, 3, 'n', 'n']\n",
      "$620,117.50, took 37 ms\n",
      "\n",
      "Using model Houston_TX/43_Houston_TX.tar.gz to predict price of this house: [2006, 2110.6485091102463, 6, 2.0, 0.5, 1, 'y', 'y']\n",
      "$382,169.78, took 36 ms\n",
      "\n",
      "Using model Houston_TX/44_Houston_TX.tar.gz to predict price of this house: [1995, 3680.588481686932, 5, 2.0, 1.05, 3, 'n', 'n']\n",
      "$577,629.56, took 36 ms\n",
      "\n",
      "Using model Houston_TX/45_Houston_TX.tar.gz to predict price of this house: [1992, 2216.2338351641547, 5, 1.5, 0.85, 3, 'y', 'n']\n",
      "$349,982.06, took 35 ms\n",
      "\n",
      "Using model Houston_TX/46_Houston_TX.tar.gz to predict price of this house: [1992, 3249.9061223335007, 3, 1.0, 1.01, 0, 'y', 'y']\n",
      "$435,223.50, took 35 ms\n",
      "\n",
      "Using model Houston_TX/47_Houston_TX.tar.gz to predict price of this house: [1994, 2523.6789942255477, 6, 3.0, 1.15, 0, 'y', 'y']\n",
      "$399,769.00, took 36 ms\n",
      "\n",
      "Using model Houston_TX/48_Houston_TX.tar.gz to predict price of this house: [1982, 2316.8886980708317, 5, 1.5, 1.08, 3, 'y', 'y']\n",
      "$319,590.41, took 43 ms\n",
      "\n",
      "Using model Houston_TX/49_Houston_TX.tar.gz to predict price of this house: [1989, 3127.037202005932, 4, 3.0, 0.78, 3, 'y', 'n']\n",
      "$484,477.88, took 37 ms\n",
      "\n",
      "Using model Houston_TX/50_Houston_TX.tar.gz to predict price of this house: [1987, 3541.0712246432204, 3, 1.0, 1.3, 3, 'n', 'n']\n",
      "$486,280.81, took 41 ms\n",
      "\n",
      "Using model Houston_TX/51_Houston_TX.tar.gz to predict price of this house: [1983, 3039.406347687365, 2, 3.0, 0.98, 0, 'y', 'n']\n",
      "$380,925.25, took 37 ms\n",
      "\n",
      "Using model Houston_TX/52_Houston_TX.tar.gz to predict price of this house: [1991, 3527.181688450042, 2, 1.5, 1.26, 1, 'n', 'n']\n",
      "$470,548.16, took 914 ms\n",
      "\n",
      "Using model Houston_TX/53_Houston_TX.tar.gz to predict price of this house: [1986, 1928.79259064938, 3, 1.5, 0.98, 0, 'n', 'n']\n",
      "$194,668.72, took 35 ms\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/54_Houston_TX.tar.gz to predict price of this house: [1999, 4006.755219480705, 4, 1.5, 1.36, 3, 'y', 'y']\n",
      "$652,765.44, took 35 ms\n",
      "\n",
      "Using model Houston_TX/55_Houston_TX.tar.gz to predict price of this house: [2006, 3302.0395404577816, 4, 1.5, 0.95, 2, 'y', 'n']\n",
      "$558,970.75, took 36 ms\n",
      "\n",
      "Using model Houston_TX/56_Houston_TX.tar.gz to predict price of this house: [2006, 3702.8977963297243, 4, 2.5, 0.73, 2, 'n', 'n']\n",
      "$610,570.81, took 36 ms\n",
      "\n",
      "Using model Houston_TX/57_Houston_TX.tar.gz to predict price of this house: [2002, 1715.2535238946057, 5, 1.0, 0.6, 0, 'y', 'y']\n",
      "$264,349.12, took 35 ms\n",
      "\n",
      "Using model Houston_TX/58_Houston_TX.tar.gz to predict price of this house: [1999, 2185.608272580093, 5, 1.5, 1.37, 3, 'n', 'y']\n",
      "$367,545.78, took 35 ms\n",
      "\n",
      "Using model Houston_TX/59_Houston_TX.tar.gz to predict price of this house: [1988, 3962.879224134585, 4, 2.0, 0.9, 2, 'y', 'y']\n",
      "$576,875.00, took 37 ms\n",
      "\n",
      "Using model Houston_TX/60_Houston_TX.tar.gz to predict price of this house: [1997, 3079.302333237355, 2, 1.0, 0.73, 0, 'y', 'y']\n",
      "$417,699.41, took 35 ms\n",
      "\n",
      "Using model Houston_TX/61_Houston_TX.tar.gz to predict price of this house: [2005, 2164.1287998786897, 2, 2.0, 0.69, 1, 'n', 'n']\n",
      "$328,605.47, took 35 ms\n",
      "\n",
      "Using model Houston_TX/62_Houston_TX.tar.gz to predict price of this house: [2003, 4188.197362241875, 3, 3.0, 1.22, 2, 'y', 'y']\n",
      "$694,683.75, took 35 ms\n",
      "\n",
      "Using model Houston_TX/63_Houston_TX.tar.gz to predict price of this house: [2004, 3895.286514612186, 6, 1.5, 1.49, 2, 'n', 'y']\n",
      "$648,772.50, took 35 ms\n",
      "\n",
      "Using model Houston_TX/64_Houston_TX.tar.gz to predict price of this house: [2013, 2699.887177304117, 2, 2.0, 1.06, 2, 'n', 'y']\n",
      "$468,719.94, took 35 ms\n",
      "\n",
      "Using model Houston_TX/65_Houston_TX.tar.gz to predict price of this house: [2007, 3757.2687564425582, 5, 2.0, 1.06, 1, 'n', 'y']\n",
      "$616,545.38, took 37 ms\n",
      "\n",
      "Using model Houston_TX/66_Houston_TX.tar.gz to predict price of this house: [1991, 3611.8250921952667, 5, 1.0, 1.02, 0, 'y', 'n']\n",
      "$507,998.97, took 36 ms\n",
      "\n",
      "Using model Houston_TX/67_Houston_TX.tar.gz to predict price of this house: [2000, 3214.4026157015046, 3, 2.5, 0.73, 2, 'y', 'y']\n",
      "$515,551.56, took 39 ms\n",
      "\n",
      "Using model Houston_TX/68_Houston_TX.tar.gz to predict price of this house: [1996, 2279.1532453665072, 3, 3.0, 0.82, 0, 'n', 'y']\n",
      "$314,407.44, took 92 ms\n",
      "\n",
      "Using model Houston_TX/69_Houston_TX.tar.gz to predict price of this house: [2017, 1708.957461339608, 2, 2.5, 1.1, 3, 'n', 'y']\n",
      "$361,182.56, took 35 ms\n",
      "\n",
      "Using model Houston_TX/70_Houston_TX.tar.gz to predict price of this house: [2001, 2787.066622842872, 5, 1.0, 1.12, 1, 'n', 'n']\n",
      "$428,050.44, took 34 ms\n",
      "\n",
      "Using model Houston_TX/71_Houston_TX.tar.gz to predict price of this house: [1989, 3101.2496219297986, 5, 1.5, 1.0, 2, 'n', 'n']\n",
      "$437,512.34, took 35 ms\n",
      "\n",
      "Using model Houston_TX/72_Houston_TX.tar.gz to predict price of this house: [1986, 3266.6781217817847, 6, 2.5, 1.41, 0, 'y', 'n']\n",
      "$472,525.91, took 43 ms\n",
      "\n",
      "Using model Houston_TX/73_Houston_TX.tar.gz to predict price of this house: [1999, 3680.2877583372374, 2, 1.5, 0.84, 1, 'n', 'y']\n",
      "$522,725.56, took 34 ms\n",
      "\n",
      "Using model Houston_TX/74_Houston_TX.tar.gz to predict price of this house: [1988, 3852.633626315698, 6, 3.0, 1.17, 0, 'y', 'n']\n",
      "$573,950.50, took 35 ms\n",
      "\n",
      "Using model Houston_TX/75_Houston_TX.tar.gz to predict price of this house: [1992, 4617.634002686654, 5, 1.5, 0.84, 0, 'n', 'y']\n",
      "$647,530.00, took 44 ms\n",
      "\n",
      "Using model Houston_TX/76_Houston_TX.tar.gz to predict price of this house: [2011, 3023.7081389166638, 4, 2.5, 1.31, 0, 'n', 'n']\n",
      "$513,788.31, took 34 ms\n",
      "\n",
      "Using model Houston_TX/77_Houston_TX.tar.gz to predict price of this house: [2001, 2251.646467328676, 6, 2.0, 1.43, 1, 'y', 'y']\n",
      "$397,302.22, took 41 ms\n",
      "\n",
      "Using model Houston_TX/78_Houston_TX.tar.gz to predict price of this house: [1981, 3186.419443256166, 4, 1.5, 0.91, 0, 'n', 'n']\n",
      "$370,012.81, took 44 ms\n",
      "\n",
      "Using model Houston_TX/79_Houston_TX.tar.gz to predict price of this house: [2005, 2882.742160965098, 5, 1.0, 1.11, 2, 'n', 'y']\n",
      "$474,788.34, took 36 ms\n",
      "\n",
      "Using model Houston_TX/80_Houston_TX.tar.gz to predict price of this house: [2011, 1686.7245035915196, 2, 2.0, 0.93, 2, 'n', 'y']\n",
      "$303,098.81, took 35 ms\n",
      "\n",
      "Using model Houston_TX/81_Houston_TX.tar.gz to predict price of this house: [1987, 3294.061821044371, 4, 2.0, 0.84, 0, 'n', 'y']\n",
      "$420,049.06, took 37 ms\n",
      "\n",
      "Using model Houston_TX/82_Houston_TX.tar.gz to predict price of this house: [1988, 2853.9520379273977, 6, 3.0, 1.0, 0, 'n', 'n']\n",
      "$399,288.06, took 35 ms\n",
      "\n",
      "Using model Houston_TX/83_Houston_TX.tar.gz to predict price of this house: [1998, 2449.127269451202, 3, 2.5, 1.17, 3, 'n', 'n']\n",
      "$394,686.03, took 36 ms\n",
      "\n",
      "Using model Houston_TX/84_Houston_TX.tar.gz to predict price of this house: [2001, 3328.619113773379, 3, 2.5, 1.21, 1, 'n', 'y']\n",
      "$512,138.03, took 35 ms\n",
      "\n",
      "Using model Houston_TX/85_Houston_TX.tar.gz to predict price of this house: [1993, 3197.938042345623, 5, 1.5, 0.54, 1, 'n', 'n']\n",
      "$447,928.09, took 35 ms\n",
      "\n",
      "Using model Houston_TX/86_Houston_TX.tar.gz to predict price of this house: [1999, 2967.3422386081415, 2, 2.5, 1.02, 3, 'n', 'y']\n",
      "$462,965.03, took 34 ms\n",
      "\n",
      "Using model Houston_TX/87_Houston_TX.tar.gz to predict price of this house: [1989, 2496.1880513288543, 6, 3.0, 0.78, 2, 'y', 'n']\n",
      "$395,246.75, took 35 ms\n",
      "\n",
      "Using model Houston_TX/88_Houston_TX.tar.gz to predict price of this house: [2002, 2728.3552124833564, 2, 1.5, 1.18, 0, 'n', 'n']\n",
      "$386,639.38, took 36 ms\n",
      "\n",
      "Using model Houston_TX/89_Houston_TX.tar.gz to predict price of this house: [1987, 2831.6419772102604, 2, 1.0, 1.14, 2, 'y', 'n']\n",
      "$370,716.97, took 34 ms\n",
      "\n",
      "Using model Houston_TX/90_Houston_TX.tar.gz to predict price of this house: [2009, 2256.2365150825676, 3, 3.0, 0.91, 3, 'y', 'n']\n",
      "$441,546.84, took 36 ms\n",
      "\n",
      "Using model Houston_TX/91_Houston_TX.tar.gz to predict price of this house: [1994, 3234.836552711468, 5, 3.0, 1.07, 0, 'n', 'y']\n",
      "$475,258.44, took 36 ms\n",
      "\n",
      "Using model Houston_TX/92_Houston_TX.tar.gz to predict price of this house: [1996, 2012.9867279378693, 4, 3.0, 1.46, 3, 'y', 'y']\n",
      "$361,237.16, took 35 ms\n",
      "\n",
      "Using model Houston_TX/93_Houston_TX.tar.gz to predict price of this house: [1989, 3067.640657803442, 4, 1.5, 1.17, 1, 'n', 'y']\n",
      "$408,887.00, took 39 ms\n",
      "\n",
      "Using model Houston_TX/94_Houston_TX.tar.gz to predict price of this house: [1992, 1661.5943674361263, 5, 2.5, 1.01, 1, 'y', 'n']\n",
      "$255,012.14, took 35 ms\n",
      "\n",
      "Using model Houston_TX/95_Houston_TX.tar.gz to predict price of this house: [1994, 3394.481306734555, 2, 2.0, 0.33, 2, 'n', 'n']\n",
      "$469,335.94, took 34 ms\n",
      "\n",
      "Using model Houston_TX/96_Houston_TX.tar.gz to predict price of this house: [1995, 2753.3423801433973, 6, 1.5, 0.84, 1, 'y', 'y']\n",
      "$425,117.31, took 41 ms\n",
      "\n",
      "Using model Houston_TX/97_Houston_TX.tar.gz to predict price of this house: [1991, 1856.9054586823245, 4, 2.5, 0.87, 3, 'n', 'y']\n",
      "$274,146.69, took 34 ms\n",
      "\n",
      "Using model Houston_TX/98_Houston_TX.tar.gz to predict price of this house: [1993, 2937.075273163852, 4, 1.0, 0.9, 1, 'y', 'n']\n",
      "$417,397.81, took 33 ms\n",
      "\n",
      "Using model Houston_TX/99_Houston_TX.tar.gz to predict price of this house: [1989, 2905.256627718407, 6, 1.0, 0.9, 2, 'y', 'n']\n",
      "$428,887.78, took 34 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Invoke the same 100 models again\n",
    "invoke_multiple_models_mme(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/0_Houston_TX.tar.gz to predict price of this house: [2000, 3568.0627454483993, 5, 3.0, 0.96, 2, 'y', 'y']\n",
      "$602,203.12, took 34 ms\n",
      "\n",
      "Using model Houston_TX/1_Houston_TX.tar.gz to predict price of this house: [1998, 2827.705072295027, 5, 3.0, 0.9, 0, 'y', 'y']\n",
      "$449,849.25, took 34 ms\n",
      "\n",
      "Using model Houston_TX/2_Houston_TX.tar.gz to predict price of this house: [1994, 3995.3203388096554, 2, 2.5, 1.0, 0, 'n', 'n']\n",
      "$551,548.56, took 35 ms\n",
      "\n",
      "Using model Houston_TX/3_Houston_TX.tar.gz to predict price of this house: [2015, 2380.6663201950896, 2, 1.0, 1.16, 2, 'y', 'y']\n",
      "$436,848.09, took 35 ms\n",
      "\n",
      "Using model Houston_TX/4_Houston_TX.tar.gz to predict price of this house: [1985, 2406.790788985759, 6, 1.0, 0.86, 1, 'n', 'n']\n",
      "$298,406.50, took 35 ms\n",
      "\n",
      "Using model Houston_TX/5_Houston_TX.tar.gz to predict price of this house: [1996, 4779.7331536301945, 2, 1.0, 0.86, 2, 'y', 'n']\n",
      "$703,882.44, took 37 ms\n",
      "\n",
      "Using model Houston_TX/6_Houston_TX.tar.gz to predict price of this house: [2009, 3005.6573268536195, 6, 1.0, 1.19, 0, 'n', 'y']\n",
      "$495,428.91, took 34 ms\n",
      "\n",
      "Using model Houston_TX/7_Houston_TX.tar.gz to predict price of this house: [1999, 1728.467981452003, 6, 1.0, 1.11, 3, 'n', 'n']\n",
      "$297,764.75, took 34 ms\n",
      "\n",
      "Using model Houston_TX/8_Houston_TX.tar.gz to predict price of this house: [1984, 3274.64554984109, 3, 1.0, 1.06, 2, 'n', 'n']\n",
      "$412,057.41, took 34 ms\n",
      "\n",
      "Using model Houston_TX/9_Houston_TX.tar.gz to predict price of this house: [1986, 2812.4180845365345, 3, 2.0, 0.78, 3, 'y', 'n']\n",
      "$396,495.31, took 35 ms\n",
      "\n",
      "Using model Houston_TX/10_Houston_TX.tar.gz to predict price of this house: [1998, 3358.03277192268, 4, 1.0, 0.98, 0, 'y', 'y']\n",
      "$490,581.56, took 34 ms\n",
      "\n",
      "Using model Houston_TX/11_Houston_TX.tar.gz to predict price of this house: [1978, 2835.8461179015685, 5, 2.0, 0.65, 2, 'y', 'y']\n",
      "$363,421.62, took 35 ms\n",
      "\n",
      "Using model Houston_TX/12_Houston_TX.tar.gz to predict price of this house: [1998, 4263.321530679366, 3, 2.5, 0.58, 0, 'n', 'y']\n",
      "$612,237.88, took 34 ms\n",
      "\n",
      "Using model Houston_TX/13_Houston_TX.tar.gz to predict price of this house: [2006, 2474.267499925919, 6, 2.0, 1.33, 2, 'n', 'n']\n",
      "$449,484.12, took 36 ms\n",
      "\n",
      "Using model Houston_TX/14_Houston_TX.tar.gz to predict price of this house: [2002, 3436.491231591471, 4, 3.0, 1.01, 0, 'y', 'n']\n",
      "$554,682.00, took 34 ms\n",
      "\n",
      "Using model Houston_TX/15_Houston_TX.tar.gz to predict price of this house: [1992, 3333.354106443411, 5, 1.5, 1.05, 0, 'y', 'y']\n",
      "$477,280.25, took 94 ms\n",
      "\n",
      "Using model Houston_TX/16_Houston_TX.tar.gz to predict price of this house: [2008, 3498.5358932569006, 2, 2.0, 0.89, 0, 'n', 'y']\n",
      "$532,737.31, took 34 ms\n",
      "\n",
      "Using model Houston_TX/17_Houston_TX.tar.gz to predict price of this house: [1997, 1917.0667897016656, 5, 1.5, 1.02, 2, 'y', 'n']\n",
      "$317,501.69, took 35 ms\n",
      "\n",
      "Using model Houston_TX/18_Houston_TX.tar.gz to predict price of this house: [1995, 2203.11399249999, 5, 3.0, 1.47, 3, 'y', 'y']\n",
      "$395,870.00, took 34 ms\n",
      "\n",
      "Using model Houston_TX/19_Houston_TX.tar.gz to predict price of this house: [1989, 4080.8046458855597, 5, 1.5, 1.3, 2, 'y', 'y']\n",
      "$610,161.31, took 34 ms\n",
      "\n",
      "Using model Houston_TX/20_Houston_TX.tar.gz to predict price of this house: [1991, 3306.2359039384783, 2, 2.5, 0.75, 1, 'n', 'n']\n",
      "$442,528.84, took 41 ms\n",
      "\n",
      "Using model Houston_TX/21_Houston_TX.tar.gz to predict price of this house: [1987, 1399.136567488078, 6, 2.5, 1.23, 0, 'y', 'y']\n",
      "$189,467.19, took 34 ms\n",
      "\n",
      "Using model Houston_TX/22_Houston_TX.tar.gz to predict price of this house: [1994, 2262.456125357065, 3, 2.0, 0.61, 1, 'y', 'y']\n",
      "$317,658.50, took 35 ms\n",
      "\n",
      "Using model Houston_TX/23_Houston_TX.tar.gz to predict price of this house: [2006, 2959.0312915256936, 5, 3.0, 0.55, 3, 'n', 'y']\n",
      "$525,663.62, took 36 ms\n",
      "\n",
      "Using model Houston_TX/24_Houston_TX.tar.gz to predict price of this house: [1998, 2553.822419345464, 5, 1.5, 1.27, 0, 'n', 'y']\n",
      "$372,424.66, took 35 ms\n",
      "\n",
      "Using model Houston_TX/25_Houston_TX.tar.gz to predict price of this house: [1993, 3090.482898748569, 5, 2.5, 0.84, 0, 'n', 'n']\n",
      "$438,080.12, took 35 ms\n",
      "\n",
      "Using model Houston_TX/26_Houston_TX.tar.gz to predict price of this house: [1981, 3114.1691051016564, 5, 2.5, 0.69, 1, 'n', 'y']\n",
      "$393,683.38, took 36 ms\n",
      "\n",
      "Using model Houston_TX/27_Houston_TX.tar.gz to predict price of this house: [1987, 2432.5827619212255, 2, 1.0, 0.79, 2, 'y', 'n']\n",
      "$303,513.94, took 35 ms\n",
      "\n",
      "Using model Houston_TX/28_Houston_TX.tar.gz to predict price of this house: [1995, 2327.373771269282, 6, 2.0, 0.66, 0, 'y', 'n']\n",
      "$351,723.94, took 35 ms\n",
      "\n",
      "Using model Houston_TX/29_Houston_TX.tar.gz to predict price of this house: [2003, 810.2859522424569, 3, 1.5, 0.99, 0, 'y', 'n']\n",
      "$128,015.88, took 36 ms\n",
      "\n",
      "Using model Houston_TX/30_Houston_TX.tar.gz to predict price of this house: [1995, 3020.203503207443, 5, 3.0, 0.68, 3, 'n', 'n']\n",
      "$485,781.19, took 38 ms\n",
      "\n",
      "Using model Houston_TX/31_Houston_TX.tar.gz to predict price of this house: [1992, 2538.794810188545, 6, 2.0, 1.02, 0, 'y', 'n']\n",
      "$376,172.81, took 37 ms\n",
      "\n",
      "Using model Houston_TX/32_Houston_TX.tar.gz to predict price of this house: [2004, 2714.0216618553886, 5, 2.0, 0.93, 2, 'n', 'y']\n",
      "$456,221.91, took 36 ms\n",
      "\n",
      "Using model Houston_TX/33_Houston_TX.tar.gz to predict price of this house: [2003, 2892.106804294575, 3, 2.5, 0.7, 3, 'y', 'n']\n",
      "$497,036.97, took 36 ms\n",
      "\n",
      "Using model Houston_TX/34_Houston_TX.tar.gz to predict price of this house: [1999, 4212.677808062194, 2, 2.0, 1.08, 0, 'y', 'y']\n",
      "$621,095.25, took 36 ms\n",
      "\n",
      "Using model Houston_TX/35_Houston_TX.tar.gz to predict price of this house: [1976, 3208.462454385207, 2, 3.0, 1.06, 2, 'n', 'y']\n",
      "$381,768.44, took 43 ms\n",
      "\n",
      "Using model Houston_TX/36_Houston_TX.tar.gz to predict price of this house: [1983, 3328.096727874689, 6, 1.5, 0.62, 2, 'n', 'n']\n",
      "$445,927.41, took 41 ms\n",
      "\n",
      "Using model Houston_TX/37_Houston_TX.tar.gz to predict price of this house: [1982, 2614.1162931797635, 6, 1.0, 0.74, 1, 'n', 'y']\n",
      "$311,305.91, took 37 ms\n",
      "\n",
      "Using model Houston_TX/38_Houston_TX.tar.gz to predict price of this house: [1999, 3123.2697869839235, 4, 2.0, 1.1, 3, 'y', 'y']\n",
      "$521,609.16, took 36 ms\n",
      "\n",
      "Using model Houston_TX/39_Houston_TX.tar.gz to predict price of this house: [2003, 3883.863585574625, 3, 1.5, 1.18, 3, 'n', 'n']\n",
      "$621,041.88, took 35 ms\n",
      "\n",
      "Using model Houston_TX/40_Houston_TX.tar.gz to predict price of this house: [1994, 4410.7688924757895, 4, 1.0, 1.03, 0, 'y', 'n']\n",
      "$633,209.25, took 36 ms\n",
      "\n",
      "Using model Houston_TX/41_Houston_TX.tar.gz to predict price of this house: [1991, 3771.359611638598, 5, 3.0, 1.24, 1, 'n', 'n']\n",
      "$561,588.06, took 42 ms\n",
      "\n",
      "Using model Houston_TX/42_Houston_TX.tar.gz to predict price of this house: [1977, 3228.4904354575015, 4, 3.0, 0.88, 2, 'y', 'y']\n",
      "$427,301.16, took 35 ms\n",
      "\n",
      "Using model Houston_TX/43_Houston_TX.tar.gz to predict price of this house: [1989, 3646.1795686467603, 4, 1.0, 0.59, 0, 'n', 'n']\n",
      "$464,538.94, took 36 ms\n",
      "\n",
      "Using model Houston_TX/44_Houston_TX.tar.gz to predict price of this house: [2004, 4074.861209062644, 6, 2.5, 1.1, 0, 'y', 'n']\n",
      "$676,111.88, took 44 ms\n",
      "\n",
      "Using model Houston_TX/45_Houston_TX.tar.gz to predict price of this house: [1998, 2096.7426956981826, 5, 1.5, 0.88, 1, 'y', 'n']\n",
      "$332,144.31, took 39 ms\n",
      "\n",
      "Using model Houston_TX/46_Houston_TX.tar.gz to predict price of this house: [1996, 2130.906162587142, 6, 2.5, 1.03, 3, 'n', 'n']\n",
      "$365,585.31, took 36 ms\n",
      "\n",
      "Using model Houston_TX/47_Houston_TX.tar.gz to predict price of this house: [1998, 3830.702212798141, 4, 1.0, 1.3, 2, 'y', 'y']\n",
      "$597,746.19, took 36 ms\n",
      "\n",
      "Using model Houston_TX/48_Houston_TX.tar.gz to predict price of this house: [1980, 2834.428894795272, 4, 3.0, 1.0, 2, 'n', 'y']\n",
      "$364,341.59, took 36 ms\n",
      "\n",
      "Using model Houston_TX/49_Houston_TX.tar.gz to predict price of this house: [1987, 2695.486888570243, 4, 1.5, 0.65, 3, 'n', 'n']\n",
      "$363,806.81, took 36 ms\n",
      "\n",
      "Using model Houston_TX/50_Houston_TX.tar.gz to predict price of this house: [1990, 4300.3740686112305, 3, 3.0, 0.29, 2, 'n', 'y']\n",
      "$610,523.75, took 36 ms\n",
      "\n",
      "Using model Houston_TX/51_Houston_TX.tar.gz to predict price of this house: [1996, 3415.445476651624, 6, 2.5, 0.57, 3, 'n', 'y']\n",
      "$549,548.62, took 36 ms\n",
      "\n",
      "Using model Houston_TX/52_Houston_TX.tar.gz to predict price of this house: [1986, 3443.91135096448, 2, 3.0, 1.1, 3, 'y', 'n']\n",
      "$503,129.41, took 35 ms\n",
      "\n",
      "Using model Houston_TX/53_Houston_TX.tar.gz to predict price of this house: [1990, 3015.159262600017, 3, 1.0, 0.97, 0, 'y', 'n']\n",
      "$390,847.75, took 36 ms\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/54_Houston_TX.tar.gz to predict price of this house: [2009, 3119.340980111573, 6, 1.5, 1.03, 3, 'n', 'y']\n",
      "$561,295.38, took 39 ms\n",
      "\n",
      "Using model Houston_TX/55_Houston_TX.tar.gz to predict price of this house: [2006, 4098.558783815178, 4, 2.0, 0.95, 0, 'n', 'y']\n",
      "$636,054.94, took 36 ms\n",
      "\n",
      "Using model Houston_TX/56_Houston_TX.tar.gz to predict price of this house: [1981, 3588.4696347421045, 4, 2.0, 1.06, 1, 'n', 'y']\n",
      "$454,515.53, took 36 ms\n",
      "\n",
      "Using model Houston_TX/57_Houston_TX.tar.gz to predict price of this house: [1987, 2416.3885544447107, 4, 2.5, 1.22, 3, 'n', 'n']\n",
      "$347,895.06, took 35 ms\n",
      "\n",
      "Using model Houston_TX/58_Houston_TX.tar.gz to predict price of this house: [1983, 2755.0087500935538, 6, 1.0, 0.81, 2, 'y', 'n']\n",
      "$375,323.31, took 36 ms\n",
      "\n",
      "Using model Houston_TX/59_Houston_TX.tar.gz to predict price of this house: [1994, 3097.5411074680205, 4, 2.0, 1.14, 2, 'y', 'y']\n",
      "$479,563.69, took 35 ms\n",
      "\n",
      "Using model Houston_TX/60_Houston_TX.tar.gz to predict price of this house: [1990, 2693.975144969254, 4, 2.5, 1.1, 0, 'n', 'y']\n",
      "$356,409.94, took 36 ms\n",
      "\n",
      "Using model Houston_TX/61_Houston_TX.tar.gz to predict price of this house: [2019, 3106.577086852423, 6, 3.0, 0.63, 1, 'y', 'n']\n",
      "$615,425.00, took 35 ms\n",
      "\n",
      "Using model Houston_TX/62_Houston_TX.tar.gz to predict price of this house: [2005, 1894.0006465541662, 4, 3.0, 1.56, 3, 'y', 'y']\n",
      "$388,763.34, took 97 ms\n",
      "\n",
      "Using model Houston_TX/63_Houston_TX.tar.gz to predict price of this house: [1984, 2693.5563761469125, 5, 2.5, 1.11, 3, 'n', 'n']\n",
      "$383,693.56, took 38 ms\n",
      "\n",
      "Using model Houston_TX/64_Houston_TX.tar.gz to predict price of this house: [1984, 2593.9326194106507, 6, 2.5, 0.89, 1, 'y', 'y']\n",
      "$363,962.50, took 35 ms\n",
      "\n",
      "Using model Houston_TX/65_Houston_TX.tar.gz to predict price of this house: [1990, 1700.4292209307023, 4, 2.5, 0.87, 2, 'n', 'n']\n",
      "$232,544.27, took 35 ms\n",
      "\n",
      "Using model Houston_TX/66_Houston_TX.tar.gz to predict price of this house: [1984, 3457.990724138399, 4, 2.0, 1.21, 1, 'n', 'y']\n",
      "$452,198.53, took 35 ms\n",
      "\n",
      "Using model Houston_TX/67_Houston_TX.tar.gz to predict price of this house: [2000, 2519.265914374393, 5, 2.0, 0.73, 0, 'y', 'y']\n",
      "$394,231.81, took 36 ms\n",
      "\n",
      "Using model Houston_TX/68_Houston_TX.tar.gz to predict price of this house: [2004, 3333.971404315891, 5, 1.5, 1.06, 1, 'y', 'n']\n",
      "$552,042.56, took 47 ms\n",
      "\n",
      "Using model Houston_TX/69_Houston_TX.tar.gz to predict price of this house: [2012, 2843.3991482869965, 5, 2.5, 0.58, 2, 'n', 'n']\n",
      "$517,087.09, took 36 ms\n",
      "\n",
      "Using model Houston_TX/70_Houston_TX.tar.gz to predict price of this house: [2003, 2253.1976807499555, 6, 2.0, 0.8, 3, 'y', 'y']\n",
      "$424,424.66, took 35 ms\n",
      "\n",
      "Using model Houston_TX/71_Houston_TX.tar.gz to predict price of this house: [1984, 2386.5157608771488, 5, 1.0, 1.02, 2, 'y', 'n']\n",
      "$317,942.34, took 34 ms\n",
      "\n",
      "Using model Houston_TX/72_Houston_TX.tar.gz to predict price of this house: [2004, 3119.433602566397, 6, 3.0, 1.23, 3, 'y', 'n']\n",
      "$585,708.69, took 34 ms\n",
      "\n",
      "Using model Houston_TX/73_Houston_TX.tar.gz to predict price of this house: [1997, 2624.2609796135343, 2, 2.5, 0.78, 0, 'y', 'n']\n",
      "$374,367.72, took 35 ms\n",
      "\n",
      "Using model Houston_TX/74_Houston_TX.tar.gz to predict price of this house: [1998, 3347.7543206960936, 6, 3.0, 0.96, 3, 'y', 'y']\n",
      "$584,347.00, took 34 ms\n",
      "\n",
      "Using model Houston_TX/75_Houston_TX.tar.gz to predict price of this house: [2014, 3268.4828942004233, 2, 3.0, 0.97, 2, 'y', 'n']\n",
      "$595,015.88, took 34 ms\n",
      "\n",
      "Using model Houston_TX/76_Houston_TX.tar.gz to predict price of this house: [2003, 3357.908085600878, 5, 1.5, 1.12, 0, 'n', 'y']\n",
      "$515,500.94, took 33 ms\n",
      "\n",
      "Using model Houston_TX/77_Houston_TX.tar.gz to predict price of this house: [1991, 2342.3084333942143, 3, 2.5, 1.33, 2, 'n', 'n']\n",
      "$332,995.19, took 39 ms\n",
      "\n",
      "Using model Houston_TX/78_Houston_TX.tar.gz to predict price of this house: [1992, 3216.401493802559, 5, 2.5, 0.97, 1, 'n', 'n']\n",
      "$469,517.09, took 34 ms\n",
      "\n",
      "Using model Houston_TX/79_Houston_TX.tar.gz to predict price of this house: [2000, 3088.492846599028, 3, 1.0, 1.11, 1, 'y', 'n']\n",
      "$467,813.03, took 33 ms\n",
      "\n",
      "Using model Houston_TX/80_Houston_TX.tar.gz to predict price of this house: [1994, 3581.146294689652, 6, 2.5, 0.7, 0, 'y', 'n']\n",
      "$545,155.94, took 34 ms\n",
      "\n",
      "Using model Houston_TX/81_Houston_TX.tar.gz to predict price of this house: [1995, 2898.712746202652, 6, 1.5, 1.11, 1, 'n', 'y']\n",
      "$432,207.72, took 50 ms\n",
      "\n",
      "Using model Houston_TX/82_Houston_TX.tar.gz to predict price of this house: [1998, 2430.39794159975, 3, 2.5, 0.64, 0, 'y', 'y']\n",
      "$356,007.75, took 34 ms\n",
      "\n",
      "Using model Houston_TX/83_Houston_TX.tar.gz to predict price of this house: [1993, 2417.2115536840847, 3, 1.5, 0.91, 2, 'n', 'y']\n",
      "$328,947.94, took 33 ms\n",
      "\n",
      "Using model Houston_TX/84_Houston_TX.tar.gz to predict price of this house: [1969, 3265.432717871636, 6, 3.0, 1.23, 3, 'n', 'y']\n",
      "$416,449.66, took 49 ms\n",
      "\n",
      "Using model Houston_TX/85_Houston_TX.tar.gz to predict price of this house: [1990, 3545.295134089282, 4, 1.5, 1.11, 0, 'y', 'n']\n",
      "$491,983.28, took 35 ms\n",
      "\n",
      "Using model Houston_TX/86_Houston_TX.tar.gz to predict price of this house: [1993, 4375.9129220669965, 2, 2.5, 1.07, 3, 'n', 'n']\n",
      "$649,782.94, took 37 ms\n",
      "\n",
      "Using model Houston_TX/87_Houston_TX.tar.gz to predict price of this house: [1992, 3419.478109656062, 2, 2.5, 0.63, 1, 'y', 'y']\n",
      "$480,712.78, took 34 ms\n",
      "\n",
      "Using model Houston_TX/88_Houston_TX.tar.gz to predict price of this house: [1987, 3456.982675547979, 3, 1.5, 0.7, 2, 'n', 'n']\n",
      "$454,887.50, took 35 ms\n",
      "\n",
      "Using model Houston_TX/89_Houston_TX.tar.gz to predict price of this house: [1983, 4564.004637042948, 5, 1.5, 1.15, 1, 'y', 'y']\n",
      "$636,653.62, took 34 ms\n",
      "\n",
      "Using model Houston_TX/90_Houston_TX.tar.gz to predict price of this house: [1990, 3054.870360520185, 6, 2.0, 1.46, 1, 'n', 'n']\n",
      "$447,695.19, took 37 ms\n",
      "\n",
      "Using model Houston_TX/91_Houston_TX.tar.gz to predict price of this house: [1986, 3825.7442756482333, 5, 2.5, 0.87, 1, 'n', 'n']\n",
      "$530,776.44, took 37 ms\n",
      "\n",
      "Using model Houston_TX/92_Houston_TX.tar.gz to predict price of this house: [2002, 2236.530206804259, 2, 3.0, 0.88, 1, 'y', 'y']\n",
      "$362,539.31, took 43 ms\n",
      "\n",
      "Using model Houston_TX/93_Houston_TX.tar.gz to predict price of this house: [2003, 3065.2944894824836, 4, 2.0, 1.03, 2, 'y', 'y']\n",
      "$516,157.59, took 37 ms\n",
      "\n",
      "Using model Houston_TX/94_Houston_TX.tar.gz to predict price of this house: [2001, 3604.931583123531, 4, 3.0, 0.85, 1, 'n', 'y']\n",
      "$565,157.56, took 34 ms\n",
      "\n",
      "Using model Houston_TX/95_Houston_TX.tar.gz to predict price of this house: [1985, 2269.404641466347, 6, 1.0, 0.26, 1, 'y', 'n']\n",
      "$286,134.97, took 34 ms\n",
      "\n",
      "Using model Houston_TX/96_Houston_TX.tar.gz to predict price of this house: [1990, 3292.8418370730597, 6, 1.0, 1.01, 2, 'n', 'y']\n",
      "$472,766.75, took 38 ms\n",
      "\n",
      "Using model Houston_TX/97_Houston_TX.tar.gz to predict price of this house: [1988, 2374.465705956174, 4, 1.5, 1.11, 0, 'n', 'n']\n",
      "$284,855.66, took 43 ms\n",
      "\n",
      "Using model Houston_TX/98_Houston_TX.tar.gz to predict price of this house: [1990, 1917.813501996417, 4, 2.5, 1.17, 1, 'y', 'y']\n",
      "$275,086.88, took 35 ms\n",
      "\n",
      "Using model Houston_TX/99_Houston_TX.tar.gz to predict price of this house: [2006, 2713.631969798921, 4, 1.0, 0.88, 0, 'y', 'y']\n",
      "$429,831.31, took 34 ms\n",
      "\n",
      "Using model Houston_TX/100_Houston_TX.tar.gz to predict price of this house: [1981, 4287.026963251746, 4, 2.0, 0.83, 3, 'y', 'y']\n",
      "$605,383.94, took 1,086 ms\n",
      "\n",
      "Using model Houston_TX/101_Houston_TX.tar.gz to predict price of this house: [2006, 2977.8962949744105, 4, 1.0, 1.05, 0, 'n', 'n']\n",
      "$454,622.94, took 1,067 ms\n",
      "\n",
      "Using model Houston_TX/102_Houston_TX.tar.gz to predict price of this house: [1980, 401.01552264579414, 2, 1.0, 0.76, 0, 'y', 'y']\n",
      "$-69,635.00, took 1,052 ms\n",
      "\n",
      "Using model Houston_TX/103_Houston_TX.tar.gz to predict price of this house: [1979, 3768.4763213876618, 4, 1.0, 0.74, 2, 'y', 'n']\n",
      "$487,056.12, took 1,086 ms\n",
      "\n",
      "Using model Houston_TX/104_Houston_TX.tar.gz to predict price of this house: [2005, 3142.953545084231, 6, 2.5, 1.29, 2, 'n', 'y']\n",
      "$551,138.38, took 1,064 ms\n",
      "\n",
      "Using model Houston_TX/105_Houston_TX.tar.gz to predict price of this house: [2006, 3402.23899676697, 3, 1.5, 1.03, 3, 'n', 'y']\n",
      "$558,110.50, took 1,074 ms\n",
      "\n",
      "Using model Houston_TX/106_Houston_TX.tar.gz to predict price of this house: [1994, 3085.184705780783, 5, 1.5, 1.49, 1, 'y', 'n']\n",
      "$474,245.75, took 1,076 ms\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/107_Houston_TX.tar.gz to predict price of this house: [1994, 3428.296601149494, 5, 3.0, 0.92, 3, 'n', 'n']\n",
      "$547,376.94, took 1,098 ms\n",
      "\n",
      "Using model Houston_TX/108_Houston_TX.tar.gz to predict price of this house: [2006, 2278.9185222287347, 5, 2.5, 1.18, 0, 'n', 'y']\n",
      "$383,129.38, took 1,129 ms\n",
      "\n",
      "Using model Houston_TX/109_Houston_TX.tar.gz to predict price of this house: [2001, 2370.5332753950756, 3, 3.0, 0.77, 3, 'n', 'y']\n",
      "$395,596.66, took 1,603 ms\n",
      "\n",
      "Using model Houston_TX/110_Houston_TX.tar.gz to predict price of this house: [1991, 2740.176776668868, 5, 2.5, 0.77, 0, 'y', 'n']\n",
      "$394,140.69, took 1,066 ms\n",
      "\n",
      "Using model Houston_TX/111_Houston_TX.tar.gz to predict price of this house: [2004, 2321.1918414062934, 3, 1.0, 1.37, 1, 'y', 'n']\n",
      "$376,043.47, took 1,164 ms\n",
      "\n",
      "Using model Houston_TX/112_Houston_TX.tar.gz to predict price of this house: [1994, 1543.6141118970802, 3, 1.5, 0.93, 0, 'n', 'y']\n",
      "$172,503.56, took 1,105 ms\n",
      "\n",
      "Using model Houston_TX/113_Houston_TX.tar.gz to predict price of this house: [2010, 3169.636742550663, 4, 3.0, 1.02, 1, 'y', 'n']\n",
      "$567,928.25, took 1,104 ms\n",
      "\n",
      "Using model Houston_TX/114_Houston_TX.tar.gz to predict price of this house: [1978, 2779.4477745551776, 5, 2.5, 0.81, 1, 'y', 'y']\n",
      "$350,937.75, took 1,084 ms\n",
      "\n",
      "Using model Houston_TX/115_Houston_TX.tar.gz to predict price of this house: [1984, 3363.393289373913, 3, 1.5, 1.46, 0, 'y', 'n']\n",
      "$431,650.62, took 1,105 ms\n",
      "\n",
      "Using model Houston_TX/116_Houston_TX.tar.gz to predict price of this house: [2003, 2643.7645807766467, 5, 3.0, 1.0, 0, 'y', 'y']\n",
      "$448,159.25, took 1,092 ms\n",
      "\n",
      "Using model Houston_TX/117_Houston_TX.tar.gz to predict price of this house: [1974, 3096.797045664983, 3, 3.0, 1.16, 0, 'n', 'y']\n",
      "$338,204.88, took 1,057 ms\n",
      "\n",
      "Using model Houston_TX/118_Houston_TX.tar.gz to predict price of this house: [2001, 2717.2177709252232, 2, 3.0, 1.78, 0, 'y', 'y']\n",
      "$433,209.28, took 1,105 ms\n",
      "\n",
      "Using model Houston_TX/119_Houston_TX.tar.gz to predict price of this house: [1980, 3360.035099273644, 5, 2.5, 1.37, 3, 'y', 'n']\n",
      "$490,415.75, took 1,067 ms\n",
      "\n",
      "Using model Houston_TX/120_Houston_TX.tar.gz to predict price of this house: [1999, 5249.48441953538, 6, 2.0, 1.07, 1, 'y', 'n']\n",
      "$836,209.06, took 1,114 ms\n",
      "\n",
      "Using model Houston_TX/121_Houston_TX.tar.gz to predict price of this house: [2009, 3083.7510188058272, 5, 2.5, 1.04, 0, 'n', 'y']\n",
      "$516,822.00, took 1,090 ms\n",
      "\n",
      "Using model Houston_TX/122_Houston_TX.tar.gz to predict price of this house: [1991, 2976.60216156279, 6, 2.0, 1.07, 0, 'n', 'n']\n",
      "$418,438.84, took 1,069 ms\n",
      "\n",
      "Using model Houston_TX/123_Houston_TX.tar.gz to predict price of this house: [2002, 3490.7275914512056, 3, 3.0, 0.94, 0, 'y', 'n']\n",
      "$551,043.81, took 1,136 ms\n",
      "\n",
      "Using model Houston_TX/124_Houston_TX.tar.gz to predict price of this house: [1991, 2951.4962332949535, 5, 2.0, 1.4, 3, 'y', 'y']\n",
      "$473,142.03, took 1,126 ms\n",
      "\n",
      "Using model Houston_TX/125_Houston_TX.tar.gz to predict price of this house: [1989, 2677.888976687499, 5, 1.5, 1.15, 0, 'n', 'y']\n",
      "$345,280.44, took 1,161 ms\n",
      "\n",
      "Using model Houston_TX/126_Houston_TX.tar.gz to predict price of this house: [1988, 3715.2715397069437, 4, 2.0, 0.76, 1, 'n', 'n']\n",
      "$503,457.75, took 1,094 ms\n",
      "\n",
      "Using model Houston_TX/127_Houston_TX.tar.gz to predict price of this house: [1997, 2926.6189974509953, 6, 2.0, 1.28, 3, 'n', 'n']\n",
      "$488,102.62, took 1,114 ms\n",
      "\n",
      "Using model Houston_TX/128_Houston_TX.tar.gz to predict price of this house: [2009, 3632.1688414819578, 4, 2.5, 0.7, 0, 'y', 'n']\n",
      "$604,569.12, took 1,108 ms\n",
      "\n",
      "Using model Houston_TX/129_Houston_TX.tar.gz to predict price of this house: [2002, 2756.2332340303865, 2, 1.5, 1.29, 2, 'n', 'y']\n",
      "$420,752.50, took 1,083 ms\n",
      "\n",
      "Using model Houston_TX/130_Houston_TX.tar.gz to predict price of this house: [2016, 3416.4972150489593, 6, 2.5, 0.8, 1, 'y', 'y']\n",
      "$641,818.88, took 1,083 ms\n",
      "\n",
      "Using model Houston_TX/131_Houston_TX.tar.gz to predict price of this house: [1983, 2634.149209274168, 2, 1.0, 0.56, 1, 'y', 'n']\n",
      "$295,502.50, took 1,104 ms\n",
      "\n",
      "Using model Houston_TX/132_Houston_TX.tar.gz to predict price of this house: [2007, 1316.2113004037308, 6, 3.0, 0.91, 3, 'n', 'n']\n",
      "$300,853.41, took 1,084 ms\n",
      "\n",
      "Using model Houston_TX/133_Houston_TX.tar.gz to predict price of this house: [1985, 2365.9630944844394, 6, 2.0, 0.89, 0, 'n', 'n']\n",
      "$293,419.03, took 1,077 ms\n",
      "\n",
      "Using model Houston_TX/134_Houston_TX.tar.gz to predict price of this house: [1973, 2679.4358197103165, 4, 2.0, 0.92, 2, 'y', 'n']\n",
      "$311,879.59, took 1,363 ms\n",
      "\n",
      "Using model Houston_TX/135_Houston_TX.tar.gz to predict price of this house: [1983, 2869.7563707185463, 3, 1.5, 1.08, 0, 'y', 'y']\n",
      "$343,065.81, took 1,113 ms\n",
      "\n",
      "Using model Houston_TX/136_Houston_TX.tar.gz to predict price of this house: [2011, 3348.7494195114864, 5, 1.0, 1.1, 3, 'n', 'n']\n",
      "$590,561.69, took 1,637 ms\n",
      "\n",
      "Using model Houston_TX/137_Houston_TX.tar.gz to predict price of this house: [1988, 3155.3862014980505, 4, 1.0, 0.82, 1, 'y', 'y']\n",
      "$423,044.81, took 1,083 ms\n",
      "\n",
      "Using model Houston_TX/138_Houston_TX.tar.gz to predict price of this house: [1982, 1702.3000839163055, 2, 3.0, 1.24, 3, 'n', 'y']\n",
      "$200,959.73, took 1,083 ms\n",
      "\n",
      "Using model Houston_TX/139_Houston_TX.tar.gz to predict price of this house: [1998, 2510.6679014265314, 4, 1.5, 0.8, 1, 'n', 'n']\n",
      "$362,626.75, took 1,074 ms\n",
      "\n",
      "Using model Houston_TX/140_Houston_TX.tar.gz to predict price of this house: [2011, 3730.6928624562206, 5, 2.5, 0.68, 3, 'y', 'n']\n",
      "$683,380.69, took 1,114 ms\n",
      "\n",
      "Using model Houston_TX/141_Houston_TX.tar.gz to predict price of this house: [1986, 3948.2027972858214, 2, 1.5, 0.66, 1, 'y', 'n']\n",
      "$518,604.88, took 1,104 ms\n",
      "\n",
      "Using model Houston_TX/142_Houston_TX.tar.gz to predict price of this house: [1991, 2017.5635902880576, 5, 2.0, 0.88, 3, 'n', 'n']\n",
      "$303,141.19, took 1,087 ms\n",
      "\n",
      "Using model Houston_TX/143_Houston_TX.tar.gz to predict price of this house: [2003, 4746.985000888366, 4, 1.5, 1.28, 0, 'n', 'y']\n",
      "$718,438.38, took 1,120 ms\n",
      "\n",
      "Using model Houston_TX/144_Houston_TX.tar.gz to predict price of this house: [1995, 3666.8192154643257, 5, 3.0, 0.82, 2, 'n', 'y']\n",
      "$570,070.06, took 1,085 ms\n",
      "\n",
      "Using model Houston_TX/145_Houston_TX.tar.gz to predict price of this house: [1992, 3036.562273531542, 5, 3.0, 1.23, 3, 'y', 'n']\n",
      "$504,522.72, took 1,197 ms\n",
      "\n",
      "Using model Houston_TX/146_Houston_TX.tar.gz to predict price of this house: [2012, 2600.422697425555, 4, 2.0, 0.72, 3, 'n', 'n']\n",
      "$479,553.44, took 1,105 ms\n",
      "\n",
      "Using model Houston_TX/147_Houston_TX.tar.gz to predict price of this house: [1991, 2769.8523566037734, 6, 1.5, 1.33, 3, 'y', 'y']\n",
      "$447,121.25, took 1,135 ms\n",
      "\n",
      "Using model Houston_TX/148_Houston_TX.tar.gz to predict price of this house: [1988, 2086.5250588109393, 4, 1.5, 1.01, 1, 'y', 'n']\n",
      "$274,167.31, took 1,097 ms\n",
      "\n",
      "Using model Houston_TX/149_Houston_TX.tar.gz to predict price of this house: [2015, 3287.0189803083167, 5, 3.0, 0.69, 0, 'y', 'y']\n",
      "$597,700.56, took 1,101 ms\n",
      "\n",
      "Using model Houston_TX/150_Houston_TX.tar.gz to predict price of this house: [1980, 1796.5467646454242, 2, 2.0, 0.56, 2, 'y', 'y']\n",
      "$182,512.44, took 1,126 ms\n",
      "\n",
      "Using model Houston_TX/151_Houston_TX.tar.gz to predict price of this house: [2004, 1457.3603371447532, 6, 3.0, 0.47, 2, 'n', 'n']\n",
      "$284,472.31, took 1,082 ms\n",
      "\n",
      "Using model Houston_TX/152_Houston_TX.tar.gz to predict price of this house: [1993, 3382.6847831382074, 3, 3.0, 1.2, 1, 'y', 'y']\n",
      "$509,175.38, took 1,081 ms\n",
      "\n",
      "Using model Houston_TX/153_Houston_TX.tar.gz to predict price of this house: [1999, 3685.7987875109525, 5, 3.0, 1.04, 2, 'n', 'n']\n",
      "$598,219.38, took 1,143 ms\n",
      "\n",
      "Using model Houston_TX/154_Houston_TX.tar.gz to predict price of this house: [1992, 1634.7595954051737, 3, 2.0, 0.69, 2, 'n', 'y']\n",
      "$209,019.55, took 1,255 ms\n",
      "\n",
      "Using model Houston_TX/155_Houston_TX.tar.gz to predict price of this house: [2013, 3468.590599128545, 5, 3.0, 0.85, 3, 'y', 'y']\n",
      "$662,696.69, took 1,133 ms\n",
      "\n",
      "Using model Houston_TX/156_Houston_TX.tar.gz to predict price of this house: [1999, 1628.910718280751, 4, 1.0, 1.41, 2, 'n', 'y']\n",
      "$251,175.56, took 1,136 ms\n",
      "\n",
      "Using model Houston_TX/157_Houston_TX.tar.gz to predict price of this house: [1993, 1735.1573084301992, 6, 3.0, 1.18, 3, 'n', 'y']\n",
      "$300,051.25, took 1,185 ms\n",
      "\n",
      "Using model Houston_TX/158_Houston_TX.tar.gz to predict price of this house: [2002, 2595.6616051422616, 6, 2.5, 1.26, 1, 'n', 'n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$440,087.19, took 1,077 ms\n",
      "\n",
      "Using model Houston_TX/159_Houston_TX.tar.gz to predict price of this house: [2003, 2092.5380512405945, 3, 1.0, 0.88, 2, 'n', 'y']\n",
      "$320,000.81, took 1,084 ms\n",
      "\n",
      "Using model Houston_TX/160_Houston_TX.tar.gz to predict price of this house: [2004, 2976.420693765037, 5, 2.0, 1.11, 2, 'y', 'n']\n",
      "$521,224.50, took 1,126 ms\n",
      "\n",
      "Using model Houston_TX/161_Houston_TX.tar.gz to predict price of this house: [2005, 2253.2190916737027, 5, 2.0, 0.59, 2, 'n', 'y']\n",
      "$384,708.50, took 1,075 ms\n",
      "\n",
      "Using model Houston_TX/162_Houston_TX.tar.gz to predict price of this house: [1987, 2743.220406909226, 6, 2.5, 1.16, 2, 'n', 'y']\n",
      "$400,879.88, took 1,075 ms\n",
      "\n",
      "Using model Houston_TX/163_Houston_TX.tar.gz to predict price of this house: [1983, 4044.6155346332425, 6, 2.0, 1.29, 1, 'n', 'n']\n",
      "$560,354.50, took 1,099 ms\n",
      "\n",
      "Using model Houston_TX/164_Houston_TX.tar.gz to predict price of this house: [2007, 3056.150301809477, 6, 1.5, 1.01, 0, 'y', 'y']\n",
      "$517,704.56, took 1,073 ms\n",
      "\n",
      "Using model Houston_TX/165_Houston_TX.tar.gz to predict price of this house: [1994, 3504.7288337341124, 6, 1.5, 1.07, 3, 'y', 'n']\n",
      "$569,532.12, took 1,103 ms\n",
      "\n",
      "Using model Houston_TX/166_Houston_TX.tar.gz to predict price of this house: [2007, 3532.1682666308034, 4, 1.5, 0.98, 2, 'y', 'y']\n",
      "$597,608.38, took 1,084 ms\n",
      "\n",
      "Using model Houston_TX/167_Houston_TX.tar.gz to predict price of this house: [1996, 2801.6260354074366, 5, 1.5, 1.36, 2, 'y', 'y']\n",
      "$451,545.84, took 1,088 ms\n",
      "\n",
      "Using model Houston_TX/168_Houston_TX.tar.gz to predict price of this house: [2000, 1990.6659799618806, 2, 2.0, 1.32, 3, 'n', 'n']\n",
      "$319,709.44, took 1,094 ms\n",
      "\n",
      "Using model Houston_TX/169_Houston_TX.tar.gz to predict price of this house: [2005, 2745.404035698204, 2, 2.5, 0.74, 2, 'n', 'n']\n",
      "$439,944.75, took 1,063 ms\n",
      "\n",
      "Using model Houston_TX/170_Houston_TX.tar.gz to predict price of this house: [1973, 3100.002444195139, 4, 2.5, 1.1, 3, 'y', 'y']\n",
      "$399,767.00, took 1,114 ms\n",
      "\n",
      "Using model Houston_TX/171_Houston_TX.tar.gz to predict price of this house: [1995, 3939.2789440962397, 5, 1.5, 0.97, 2, 'y', 'y']\n",
      "$611,412.50, took 1,113 ms\n",
      "\n",
      "Using model Houston_TX/172_Houston_TX.tar.gz to predict price of this house: [1991, 3086.2378880604538, 6, 2.5, 0.83, 1, 'y', 'y']\n",
      "$471,262.03, took 1,117 ms\n",
      "\n",
      "Using model Houston_TX/173_Houston_TX.tar.gz to predict price of this house: [1992, 2054.1269999063807, 5, 3.0, 0.96, 3, 'y', 'y']\n",
      "$348,899.12, took 1,135 ms\n",
      "\n",
      "Using model Houston_TX/174_Houston_TX.tar.gz to predict price of this house: [1987, 3420.0883699589244, 2, 1.0, 0.73, 2, 'y', 'n']\n",
      "$451,880.50, took 1,096 ms\n",
      "\n",
      "Using model Houston_TX/175_Houston_TX.tar.gz to predict price of this house: [1975, 3073.537840255648, 2, 2.5, 0.83, 2, 'n', 'n']\n",
      "$346,017.88, took 1,124 ms\n",
      "\n",
      "Using model Houston_TX/176_Houston_TX.tar.gz to predict price of this house: [1996, 4280.150150406805, 6, 2.0, 0.9, 0, 'y', 'n']\n",
      "$656,906.50, took 1,198 ms\n",
      "\n",
      "Using model Houston_TX/177_Houston_TX.tar.gz to predict price of this house: [2006, 2288.8745553080553, 6, 1.0, 0.55, 3, 'y', 'n']\n",
      "$425,844.12, took 1,111 ms\n",
      "\n",
      "Using model Houston_TX/178_Houston_TX.tar.gz to predict price of this house: [1980, 4308.862833059132, 3, 2.5, 0.99, 3, 'y', 'y']\n",
      "$604,104.12, took 1,116 ms\n",
      "\n",
      "Using model Houston_TX/179_Houston_TX.tar.gz to predict price of this house: [1994, 3700.4194523711203, 2, 3.0, 0.63, 1, 'y', 'n']\n",
      "$542,234.00, took 1,132 ms\n",
      "\n",
      "Using model Houston_TX/180_Houston_TX.tar.gz to predict price of this house: [2004, 1488.2544454123026, 3, 3.0, 1.3, 3, 'y', 'n']\n",
      "$308,584.69, took 1,131 ms\n",
      "\n",
      "Using model Houston_TX/181_Houston_TX.tar.gz to predict price of this house: [1991, 2451.715243834091, 5, 3.0, 0.85, 3, 'y', 'n']\n",
      "$403,761.50, took 1,077 ms\n",
      "\n",
      "Using model Houston_TX/182_Houston_TX.tar.gz to predict price of this house: [1977, 2288.1670732807056, 2, 1.5, 0.98, 0, 'n', 'y']\n",
      "$193,348.69, took 1,094 ms\n",
      "\n",
      "Using model Houston_TX/183_Houston_TX.tar.gz to predict price of this house: [1996, 2957.9429540329606, 4, 2.5, 0.96, 1, 'y', 'n']\n",
      "$459,223.00, took 1,073 ms\n",
      "\n",
      "Using model Houston_TX/184_Houston_TX.tar.gz to predict price of this house: [1993, 2620.363592432953, 2, 2.5, 0.91, 1, 'n', 'n']\n",
      "$351,462.44, took 1,124 ms\n",
      "\n",
      "Using model Houston_TX/185_Houston_TX.tar.gz to predict price of this house: [1984, 3028.727835357305, 6, 2.0, 0.86, 0, 'y', 'y']\n",
      "$406,864.22, took 1,076 ms\n",
      "\n",
      "Using model Houston_TX/186_Houston_TX.tar.gz to predict price of this house: [1986, 2181.263808164514, 3, 2.5, 1.34, 1, 'y', 'n']\n",
      "$290,029.09, took 1,165 ms\n",
      "\n",
      "Using model Houston_TX/187_Houston_TX.tar.gz to predict price of this house: [1998, 2222.3532554282456, 3, 1.5, 1.18, 3, 'n', 'n']\n",
      "$345,229.50, took 1,095 ms\n",
      "\n",
      "Using model Houston_TX/188_Houston_TX.tar.gz to predict price of this house: [1991, 2516.6090412307763, 5, 2.0, 1.58, 1, 'n', 'y']\n",
      "$361,230.47, took 1,096 ms\n",
      "\n",
      "Using model Houston_TX/189_Houston_TX.tar.gz to predict price of this house: [2000, 2686.5719034030417, 5, 1.0, 1.42, 0, 'n', 'y']\n",
      "$397,464.59, took 1,164 ms\n",
      "\n",
      "Using model Houston_TX/190_Houston_TX.tar.gz to predict price of this house: [1990, 2023.2269361678873, 6, 1.5, 0.94, 3, 'n', 'y']\n",
      "$301,520.00, took 1,098 ms\n",
      "\n",
      "Using model Houston_TX/191_Houston_TX.tar.gz to predict price of this house: [1987, 3273.38277174538, 4, 1.0, 1.02, 2, 'n', 'n']\n",
      "$436,122.47, took 1,095 ms\n",
      "\n",
      "Using model Houston_TX/192_Houston_TX.tar.gz to predict price of this house: [1994, 3444.2965873591543, 6, 1.5, 1.11, 2, 'y', 'y']\n",
      "$544,823.12, took 1,055 ms\n",
      "\n",
      "Using model Houston_TX/193_Houston_TX.tar.gz to predict price of this house: [2003, 2275.2087788721774, 4, 1.5, 0.65, 1, 'n', 'n']\n",
      "$348,294.81, took 1,126 ms\n",
      "\n",
      "Using model Houston_TX/194_Houston_TX.tar.gz to predict price of this house: [1992, 2256.8782945223725, 6, 2.5, 0.47, 2, 'n', 'y']\n",
      "$338,102.69, took 1,196 ms\n",
      "\n",
      "Using model Houston_TX/195_Houston_TX.tar.gz to predict price of this house: [2003, 2542.6216286508525, 5, 1.5, 0.86, 2, 'y', 'n']\n",
      "$438,196.34, took 1,132 ms\n",
      "\n",
      "Using model Houston_TX/196_Houston_TX.tar.gz to predict price of this house: [1992, 3698.811926559986, 4, 2.0, 0.9, 3, 'y', 'n']\n",
      "$572,603.44, took 1,065 ms\n",
      "\n",
      "Using model Houston_TX/197_Houston_TX.tar.gz to predict price of this house: [2010, 3019.266410980379, 5, 2.0, 0.91, 2, 'y', 'y']\n",
      "$551,275.94, took 1,126 ms\n",
      "\n",
      "Using model Houston_TX/198_Houston_TX.tar.gz to predict price of this house: [1981, 2500.2729143998313, 6, 1.0, 1.24, 0, 'y', 'y']\n",
      "$304,356.16, took 1,086 ms\n",
      "\n",
      "Using model Houston_TX/199_Houston_TX.tar.gz to predict price of this house: [1995, 2416.291906621088, 2, 2.5, 1.12, 2, 'n', 'n']\n",
      "$349,016.88, took 1,114 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##This itme invoke all 200 models to observe behavior\n",
    "invoke_multiple_models_mme(0,200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CloudWatch charts for LoadedModelCount,MemoryUtilization and ModelCacheHit metrics will be similar to charts below.\n",
    "\n",
    "![](cw_charts/ModelCountMemUtilization.png)\n",
    "\n",
    "LoadedModelCount continuously increases, as more models are invocated, till it levels off at 121.  MemoryUtilization of the container also increased correspondingly to around 79%.  This shows that the instance chosen to host the endpoint, could only maintain 121 models in memory, when 200 model invocations are made.  \n",
    "\n",
    "![](cw_charts/ModelCountMemUtilizationCacheHit.png)\n",
    "\n",
    "As the number of models loaded to the container memory increase, the ModelCacheHit improves.  When the same 100 models are invoked the second time, the ModelCacheHit reaches 1.  When new models, not yet loaded are invoked the ModelCacheHit decreases again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9 - Explore granular access to the target models of MME <a id='Finegrain-control-invoke-models'></a>\n",
    "\n",
    "If the role attached to this notebook instance allows invoking SageMaker endpoints, it is able to invoke all models hosted on the MME.  Using IAM conditional keys, you can restrict this model invocation access to specific models.  To explore this, you will create a new IAM role and IAM policy with conditional key to restrict access to a single model.  Assume this new role and verify that only a single target model can be invoked.\n",
    "\n",
    "Note that to execute this section, the role attached to the notebook instance should allow the following actions :\n",
    "    \"iam:CreateRole\",\n",
    "    \"iam:CreatePolicy\",\n",
    "    \"iam:AttachRolePolicy\",\n",
    "    \"iam:UpdateAssumeRolePolicy\"\n",
    "    \n",
    "If this is not the case, please work the Administrator of this AWS account to ensure this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_client = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the role assumed by this notebook instance.\n",
    "#role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new role that can be assumed by this notebook.  The roles should allow access to only a single model.\n",
    "\n",
    "#job_name='scikit-learn-preprocessor-'+strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "path='/'\n",
    "role_name='allow_invoke_ny_model_role'+strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "description='Role that allows invoking a single model'\n",
    "\n",
    "action_string = \"sts:AssumeRole\"\n",
    "    \n",
    "trust_policy={\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"statement1\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"AWS\": role\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = iam_client.create_role(\n",
    "    Path=path,\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "    Description=description,\n",
    "    MaxSessionDuration=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role arn is : arn:aws:iam::555360056434:role/allow_invoke_ny_model_role2020-07-06-01-47-13\n"
     ]
    }
   ],
   "source": [
    "role_arn=response['Role']['Arn']\n",
    "print(\"Role arn is :\", role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint arn is : arn:aws:sagemaker:us-east-1:555360056434:endpoint/inference-pipeline-ep-2020-07-06-00-42-02\n"
     ]
    }
   ],
   "source": [
    "endpoint_resource_arn=\"arn:aws:sagemaker:\" + REGION + \":\" + ACCOUNT_ID + \":endpoint/\" + endpoint_name\n",
    "print(\"Endpoint arn is :\", endpoint_resource_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create the IAM policy with the IAM condition key\n",
    "policy_name= 'allow_invoke_ny_model_policy'+strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "managed_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"SageMakerAccess\",\n",
    "            \"Action\": \"sagemaker:InvokeEndpoint\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Resource\":endpoint_resource_arn,\n",
    "            \"Condition\": {\n",
    "                \"StringLike\": {\n",
    "                    \"sagemaker:TargetModel\": [\"NewYork_NY/*\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = iam_client.create_policy(\n",
    "  PolicyName=policy_name,\n",
    "  PolicyDocument=json.dumps(managed_policy)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_arn=response['Policy']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '030e9369-7bc4-49b1-8272-eb237154109f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '030e9369-7bc4-49b1-8272-eb237154109f',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212',\n",
       "   'date': 'Mon, 06 Jul 2020 01:47:28 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Attach policy to role\n",
    "iam_client.attach_role_policy(\n",
    "    PolicyArn=policy_arn,\n",
    "    RoleName=role_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sts::555360056434:assumed-role/allow_invoke_ny_model_role2020-07-06-01-47-13/MME_Invoke_NY_Model'"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Invoke with the role that has access to only NY model\n",
    "sts_connection = boto3.client('sts')\n",
    "assumed_role_limited_access = sts_connection.assume_role(\n",
    "    RoleArn=role_arn,\n",
    "    RoleSessionName=\"MME_Invoke_NY_Model\"\n",
    ")\n",
    "assumed_role_limited_access['AssumedRoleUser']['Arn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_policy={\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"statement1\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"AWS\": role\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    },\n",
    "    {\n",
    "      \"Sid\": \"statement2\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "          \"AWS\": assumed_role_limited_access['AssumedRoleUser']['Arn']\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }  \n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '194a73d1-c091-4d9e-9304-c372aa339514',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '194a73d1-c091-4d9e-9304-c372aa339514',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '224',\n",
       "   'date': 'Mon, 06 Jul 2020 01:47:32 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iam_client.update_assume_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyDocument=json.dumps(trust_policy)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY = assumed_role_limited_access['Credentials']['AccessKeyId']\n",
    "SECRET_KEY = assumed_role_limited_access['Credentials']['SecretAccessKey']\n",
    "SESSION_TOKEN = assumed_role_limited_access['Credentials']['SessionToken']\n",
    "\n",
    "runtime_sm_client_with_assumed_role = boto3.client(\n",
    "    service_name='sagemaker-runtime', \n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    aws_session_token=SESSION_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    " sagemakerSessionAssumedRole = sagemaker.Session(sagemaker_runtime_client=runtime_sm_client_with_assumed_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorAssumedRole = RealTimePredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemakerSessionAssumedRole,\n",
    "    serializer=csv_serializer,\n",
    "    content_type=CONTENT_TYPE_CSV,\n",
    "    accept=CONTENT_TYPE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model NewYork_NY/NewYork_NY.tar.gz to predict price of this house: [1983, 4157.329826676647, 2, 2.5, 0.56, 0, 'y', 'y']\n",
      "$533,722.12, took 1,123 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_model_name = 'NewYork_NY/NewYork_NY.tar.gz'\n",
    "predict_one_house_value(gen_random_house()[:-1], full_model_name,predictorAssumedRole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Chicago_IL/Chicago_IL.tar.gz to predict price of this house: [2002, 4111.711154533768, 2, 2.0, 0.72, 2, 'y', 'y']\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:sts::555360056434:assumed-role/allow_invoke_ny_model_role2020-07-06-01-47-13/MME_Invoke_NY_Model is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:us-east-1:555360056434:endpoint/inference-pipeline-ep-2020-07-06-00-42-02",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-600-688a00218634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##This should fail with \"AccessDeniedException\" since the assumed role does not have access to Chicago model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfull_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Chicago_IL/Chicago_IL.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredict_one_house_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_random_house\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictorAssumedRole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-564-fa3777c3a986>\u001b[0m in \u001b[0;36mpredict_one_house_value\u001b[0;34m(features, model_name, predictor_to_use)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor_to_use\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDeniedException) when calling the InvokeEndpoint operation: User: arn:aws:sts::555360056434:assumed-role/allow_invoke_ny_model_role2020-07-06-01-47-13/MME_Invoke_NY_Model is not authorized to perform: sagemaker:InvokeEndpoint on resource: arn:aws:sagemaker:us-east-1:555360056434:endpoint/inference-pipeline-ep-2020-07-06-00-42-02"
     ]
    }
   ],
   "source": [
    "##This should fail with \"AccessDeniedException\" since the assumed role does not have access to Chicago model\n",
    "full_model_name = 'Chicago_IL/Chicago_IL.tar.gz'\n",
    "predict_one_house_value(gen_random_house()[:-1], full_model_name,predictorAssumedRole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up<a id='CleanUp'></a>\n",
    "Clean up the endpoint to avoid unneccessary costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shut down the endpoint\n",
    "#sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
