{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Tabular Data\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to preprocess tabular data for training a machine learning model via Amazon SageMaker. In this notebook we focus on preprocessing our tabular data. In a sequel notebook, [02_feature_selection_tabular_data.ipynb](02_feature_selection_tabular_data.ipynb) we use our preprocessed tabular data to select important features and prune unimportant ones out. In our final sequel notebook, [03_training_model_on_tabular_data.ipynb](03_training_model_on_tabular_data.ipynb) we use our selected features to train a machine learning model. We showcase how to preprocess 2 different tabular data sets. \n",
    "\n",
    "\n",
    "#### Notes\n",
    "In this notebook, we use the sklearn framework for data partitionining and `storemagic` to share dataframes in [02_feature_selection_tabular_data.ipynb](02_feature_selection_tabular_data.ipynb) and [03_training_model_on_tabular_data.ipynb](03_training_model_on_tabular_data.ipynb). While we load data into memory here we do note that is it possible to skip this and load your partitioned data directly to an S3 bucket.\n",
    "\n",
    "#### Tabular Data Sets\n",
    "* [california house data](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)\n",
    "* [diabetes data ](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html)\n",
    "\n",
    "\n",
    "#### Library Dependencies:\n",
    "* sagemaker>=2.15.0\n",
    "* numpy \n",
    "* pandas\n",
    "* plotly\n",
    "* sklearn \n",
    "* matplotlib \n",
    "* seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "def get_sagemaker_version():\n",
    "    \"Return the version of 'sagemaker' in your kernel or -1 if 'sagemaker' is not installed\"\n",
    "    for i in pkg_resources.working_set:\n",
    "        if i.key == \"sagemaker\":\n",
    "            return \"%s==%s\" % (i.key, i.version)\n",
    "    return -1\n",
    "\n",
    "# Store original 'sagemaker' version\n",
    "sagemaker_version = get_sagemaker_version()\n",
    "\n",
    "# Install any missing dependencies\n",
    "!{sys.executable} -m pip install -qU 'plotly' 'sagemaker>=2.15.0'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import *\n",
    "import sklearn.model_selection\n",
    "\n",
    "# SageMaker dependencies\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "# This instantiates a SageMaker session that we will be operating in. \n",
    "session = sagemaker.Session()\n",
    "\n",
    "# This object represents the IAM role that we are assigned.\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Select and Download Data\n",
    "\n",
    "Here you can select the tabular data set of your choice to preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = {'diabetes': 'load_diabetes()', 'california': 'fetch_california_housing()'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do select a particular dataset, assign **choosen_data_set** below to be one of 'diabetes', or 'california' where each name corresponds to the it's respective dataset.\n",
    "\n",
    "* 'california' : [california house data](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)\n",
    "* 'diabetes' : [diabetes data ](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change choosen_data_set variable to one of the data sets above. \n",
    "choosen_data_set = 'california'\n",
    "assert choosen_data_set in data_sets.keys()\n",
    "print(\"I selected the '{}' dataset!\".format(choosen_data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Describe Feature Information \n",
    "\n",
    "Here you can select the tabular data set of your choice to preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = eval(data_sets[choosen_data_set])\n",
    "\n",
    "X = pd.DataFrame(data_set.data, columns=data_set.feature_names)\n",
    "Y = pd.DataFrame(data_set.target)\n",
    "\n",
    "print(\"Features:\", list(X.columns))\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"Dataset Type:\", type(X))\n",
    "print(\"Label set shape:\", Y.shape)\n",
    "print(\"Label set Type:\", type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We describe both our training data inputs X and outputs Y by computing the count, mean, std, min, percentiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Plot on Feature Correlation\n",
    "Here we show a heatmap and clustergrid across all our features. These visualizations help us analyze correlated features and are particularly important if we want to remove redundant features. The heatmap computes a similarity score across each feature and colors like features using this score. The clustergrid is similar, however it presents feature correlations hierarchically.\n",
    "\n",
    "**Note**: For the purposes of this notebook we do not remove any features but by gathering the findings from these plots one may choose to and can do so at this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "cor = X.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=sns.diverging_palette(20, 220, n=200))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map = sns.clustermap(cor, cmap =sns.diverging_palette(20, 220, n=200), linewidths = 0.1); \n",
    "plt.setp(cluster_map.ax_heatmap.yaxis.get_majorticklabels(), rotation = 0) \n",
    "cluster_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Partition Dataset into Train, Test, Validation Splits\n",
    "Here using the sklearn framework we partition our selected dataset into Train, Test and Validation splits. We choose a partition size of 1/3 and then further split the training set into 2/3 training and 1/3 validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We partition the dataset into 2/3 training and 1/3 test set.\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.33)\n",
    "\n",
    "# We further split the training set into a validation set i.e., 2/3 training set, and 1/3 validation set\n",
    "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(X_train, Y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Store Variables using `storemagic`\n",
    "We use storemagic to persist all relevant variables so they can be reused in our sequel notebooks, [02_feature_selection_tabular_data.ipynb ](02_feature_selection_tabular_data.ipynb ) and [03_training_model_on_tabular_data.ipynb](03_training_model_on_tabular_data.ipynb). \n",
    "\n",
    "Alternatively, it is possible to upload your partitioned data to an S3 bucket and point to it during the model training phase. We note that this is beyond the scope of this notebook hence why we omit it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using storemagic we persist the variables below so we can access them in the 02_feature_selection_tabular_data.ipynb and training_model_on_tabular_data.ipynb\n",
    "%store X_train\n",
    "%store X_test\n",
    "%store X_val\n",
    "%store Y_train\n",
    "%store Y_test\n",
    "%store Y_val\n",
    "%store choosen_data_set\n",
    "%store sagemaker_version"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
