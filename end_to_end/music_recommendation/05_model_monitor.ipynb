{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='05-nb'></a>\n",
    "\n",
    "# Music Recommender Part 5: Model Monitor\n",
    "\n",
    "----\n",
    "In this notebook, we'll set up [SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html) to detect when our model or data significantly deviates from its \"normal\" behavior. SageMaker Model Monitor provides the ability to monitor machine learning models in production and detect deviations in data quality in comparison to a baseline dataset (e.g. training data set). This notebook walks you through enabling data capture and setting up continous monitoring for an existing Endpoint.\n",
    "\n",
    "This Notebook helps with the following:\n",
    "* Update your existing SageMaker Endpoint to enable Model Monitoring\n",
    "* Analyze the training dataset to generate a baseline constraint\n",
    "* Setup a MonitoringSchedule for monitoring deviations from the specified baseline\n",
    "\n",
    "----\n",
    "### Contents\n",
    "- [Overview](00_overview_arch_data.ipynb)\n",
    "- [Part 1: Data Prep using Data Wrangler](01_music_dataprep.flow)\n",
    "- [Part 2a: Feature Store Creation - Tracks](02a_export_fg_tracks.ipynb)\n",
    "- [Part 2b: Feature Store Creation - User Preferences](02b_export_fg_5star_features.ipynb)\n",
    "- [Part 2c: Feature Store Creation - Ratings](02c_export_fg_ratings.ipynb)\n",
    "- [Part 3: Train Model with Debugger Hooks. Set Artifacts and Register Model.](03_train_model_lineage_registry_debugger.ipynb)\n",
    "- [Part 4: Deploy Model & Inference using Online Feature Store](04_deploy_inference_explainability.ipynb)\n",
    "- [Part 5: Model Monitor](05_model_monitor.ipynb)\n",
    "    - [Enable data capture](#05-capture)\n",
    "    - [Baselining](#05-baseline)\n",
    "    - [Enable continous monitoring](#05-continuous)\n",
    "- [Part 6: SageMaker Pipelines](06_pipeline.ipynb)\n",
    "- [Part 7: Resource Cleanup](07_clean_up.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id='05-capture'></a>\n",
    "\n",
    "## Step 1: Enable real-time inference data capture\n",
    "\n",
    "##### [back to top](#05-nb)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable data capture for monitoring the model data quality, you specify the new capture option called `DataCaptureConfig`. You can capture the request payload, the response payload or both with this configuration. The capture config applies to all variants. Please provide the Endpoint name in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import session\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "sys.path.insert(1, './code')\n",
    "from parameter_store import ParameterStore\n",
    "ps = ParameterStore(verbose=False)\n",
    "\n",
    "parameters = ps.read('music-rec')\n",
    "\n",
    "bucket = parameters['bucket']\n",
    "dw_ecrlist = parameters['dw_ecrlist']\n",
    "fg_name_ratings = parameters['fg_name_ratings']\n",
    "fg_name_tracks = parameters['fg_name_tracks']\n",
    "fg_name_user_preferences = parameters['fg_name_user_preferences']\n",
    "\n",
    "flow_export_id = parameters['flow_export_id']\n",
    "flow_s3_uri = parameters['flow_s3_uri']\n",
    "pretrained_model_path = parameters['pretrained_model_path']\n",
    "prefix = parameters['prefix']\n",
    "ratings_data_source = parameters['ratings_data_source']\n",
    "tracks_data_source = parameters['tracks_data_source']\n",
    "endpoint_name = parameters['endpoint_name']\n",
    "val_data_uri = parameters['val_data_uri']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_session = session.Session(boto3.Session())\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in the following for enabling data capture\n",
    "s3_capture_upload_path = f's3://{bucket}/{prefix}/endpoint-data-capture/' #example: s3://bucket-name/path/to/endpoint-data-capture/\n",
    "\n",
    "##### \n",
    "## IMPORTANT\n",
    "##\n",
    "## Please make sure to add the \"s3:PutObject\" permission to the \"role' you provided in the SageMaker Model \n",
    "## behind this Endpoint. Otherwise, Endpoint data capture will not work.\n",
    "## \n",
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Change parameters as you would like - adjust sampling percentage, \n",
    "#  chose to capture request or response or both\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture = True,\n",
    "    sampling_percentage=25,\n",
    "    destination_s3_uri=s3_capture_upload_path,\n",
    "    kms_key_id=None,\n",
    "    capture_options=[\"REQUEST\", \"RESPONSE\"],\n",
    "    csv_content_types=[\"text/csv\"],\n",
    "    json_content_types=[\"application/json\"]\n",
    ")\n",
    "\n",
    "# Now it is time to apply the new configuration and wait for it to be applied\n",
    "predictor = Predictor(endpoint_name=endpoint_name)\n",
    "predictor.update_data_capture_config(data_capture_config=data_capture_config)\n",
    "sm_session.wait_for_endpoint(endpoint=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you proceed:\n",
    "Currently SageMaker supports monitoring Endpoints out of the box only for **tabular (csv, flat-json)** datasets. If your Endpoint uses some other datasets, these following steps will NOT work for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='05-baseline'></a>\n",
    "\n",
    "## Step 2: Model Monitor - Baselining\n",
    "\n",
    "##### [back to top](#05-nb)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to collecting the data, SageMaker allows you to monitor and evaluate the data observed by the Endpoints. For this :\n",
    "1. We need to create a baseline with which we compare the realtime traffic against. \n",
    "1. Once a baseline is ready, we can setup a schedule to continously evaluate/compare against the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint suggestion with baseline/training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset with which you trained the model is usually a good baseline dataset. Note that the training dataset's data schema and the inference dataset schema should exactly match (i.e. number and order of the features).\n",
    "\n",
    "Using our training dataset, we'll ask SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##'s3://bucketname/path/to/baseline/data' - Where your validation data is\n",
    "baseline_data_uri = val_data_uri \n",
    "##'s3://bucketname/path/to/baseline/data' - Where the results are to be stored in\n",
    "baseline_results_uri = f's3://{bucket}/{prefix}/baseline/results' \n",
    "\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a baselining job with the validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the training data ready in S3, let's kick off a job to `suggest` constraints. `DefaultModelMonitor.suggest_baseline(..)` kicks off a `ProcessingJob` using a SageMaker provided Model Monitor container to generate the constraints. Please edit the configurations to fit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker import get_execution_role\n",
    "import datetime\n",
    "\n",
    "role = get_execution_role(sagemaker_session=sm_session)\n",
    "\n",
    "datetime_stamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    base_job_name=f\"{prefix}-monitor-{datetime_stamp}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "monitor_baseline = my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    job_name=f\"{prefix}-monitor-baseline-{datetime_stamp}\",\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis of the Processing Jobs underlying SageMaker Monitor\n",
    "In this short section [next few cells] we will be showing you how to further view the underlying jobs for the monitoring job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "def get_last_processing_job():\n",
    "    \n",
    "    response = client.list_processing_jobs(\n",
    "        NameContains=f\"{prefix}-monitor-baseline-{datetime_stamp}\",\n",
    "        StatusEquals='Completed',\n",
    "        SortBy='CreationTime',\n",
    "        SortOrder='Descending',\n",
    "        MaxResults=20\n",
    "    )\n",
    "    pprint.pprint(response['ProcessingJobSummaries'][0])\n",
    "    return response['ProcessingJobSummaries'][0]['ProcessingJobName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing  import ProcessingJob \n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.model_monitor.model_monitoring import ModelMonitor\n",
    "\n",
    "my_default_monitor_name = get_last_processing_job()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor_reload = ProcessingJob.from_processing_name(sm_session, my_default_monitor_name)\n",
    "\n",
    "response = client.describe_processing_job(\n",
    "    ProcessingJobName=my_default_monitor_name\n",
    ")\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to enable monitoring, you could chose to edit the constraint file as required to fine tune the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='05-continuous'></a>\n",
    "\n",
    "## Step 3: Enable continous monitoring\n",
    "\n",
    "##### [back to top](#05-nb)\n",
    "\n",
    "----\n",
    "\n",
    "We have collected the data above, here we proceed to analyze and monitor the data with MonitoringSchedules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create a model monitoring schedule for the Endpoint created earlier with the baseline resources (constraints and statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "import datetime as datetime\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "mon_schedule_name = 'music-rec-monitor-schedule-{}'.format(datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\"))\n",
    "s3_report_path = f's3://{bucket}/{prefix}/monitor/report'\n",
    "\n",
    "try:\n",
    "    my_default_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=mon_schedule_name,\n",
    "        endpoint_input=endpoint_name,\n",
    "        output_s3_uri=s3_report_path,\n",
    "        statistics=my_default_monitor.baseline_statistics(),\n",
    "        constraints=my_default_monitor.suggested_constraints(),\n",
    "        schedule_cron_expression=CronExpressionGenerator.daily(),\n",
    "        enable_cloudwatch_metrics=True,\n",
    "    )\n",
    "    print(f\"Created monitoring schedule {mon_schedule_name}\")\n",
    "except:\n",
    "    my_default_monitor.update_monitoring_schedule(\n",
    "        endpoint_input=endpoint_name,\n",
    "        schedule_cron_expression=CronExpressionGenerator.daily(),\n",
    "        enable_cloudwatch_metrics=True,\n",
    "    )\n",
    "    print(f\"Updated monitoring schedule {my_default_monitor.monitoring_schedule_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Pending\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "while desc_schedule_result['MonitoringScheduleStatus'] != 'Scheduled':\n",
    "    print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))\n",
    "    desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "    time.sleep(30)\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All set\n",
    "Now that your monitoring schedule has been created. Please return to the Amazon SageMaker Studio to list the executions for this Schedule and observe the results going forward."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
