{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify a CPU bottleneck caused by a callback process with Amazon SageMaker Debugger \n",
    "\n",
    "In this notebook we demonstrate how to identify a training bottleneck that is caused by a TensorFlow Keras callback.\n",
    "To simulate this type of bottleneck, we will program the callback associated with the tensor monitoring feature of Amazon SageMaker Debugger, to collect an excessive number of tensors, and at a high frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install sagemaker\n",
    "To use the new Debugger profiling features, ensure that you have the latest version of SageMaker SDK installed. The following cell updates the library and restarts the Jupyter kernel to apply the updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "install_needed = False  # should only be True once\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install -U sagemaker\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare training dataset\n",
    "\n",
    "### Tensorflow Datasets package\n",
    "\n",
    "First of all, set the notebook kernel to Tensorflow 2.x.\n",
    "\n",
    "We will use CIFAR-10 dataset for this experiment. To download CIFAR-10 datasets and convert it into TFRecord format, install `tensorflow-datasets` package, run `demo/generate_cifar10_tfrecords`, and upload tfrecord files to your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python demo/generate_cifar10_tfrecords.py --data-dir=./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "s3_bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "dataset_prefix = \"data/cifar10-tfrecords\"\n",
    "desired_s3_uri = f\"s3://{s3_bucket}/{dataset_prefix}\"\n",
    "\n",
    "dataset_location = sagemaker.s3.S3Uploader.upload(local_path=\"data\", desired_s3_uri=desired_s3_uri)\n",
    "print(f\"Dataset uploaded to {dataset_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Training Job with Profiling Enabled<a class=\"anchor\" id=\"option-1\"></a>\n",
    "\n",
    "We will use the standard [SageMaker Estimator API for Tensorflow](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-estimator) to create a training job. To enable profiling, we create a `ProfilerConfig` object and pass it to the `profiler_config` parameter of the `TensorFlow` estimator. For this demo, we set the the profiler to probe the system once every 500 miliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a profiler configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    "    framework_profile_params=FrameworkProfile(\n",
    "        local_path=\"/opt/ml/output/profiler/\", start_step=5, num_steps=2\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Debugger hook\n",
    "We configure the debugger hook to collect an excessive number of tensors, every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.debugger import DebuggerHookConfig, CollectionConfig\n",
    "\n",
    "debugger_hook_config = DebuggerHookConfig(\n",
    "    hook_parameters={\"save_interval\": \"50\"},\n",
    "    collection_configs=[\n",
    "        CollectionConfig(name=\"outputs\"),\n",
    "        CollectionConfig(name=\"gradients\"),\n",
    "        CollectionConfig(name=\"weights\"),\n",
    "        CollectionConfig(name=\"layers\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters\n",
    "\n",
    "The start-up script is set to [train_tf_bottleneck.py](./demo/train_tf_bottleneck.py). Define hyperparameters such as number of epochs, and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"epoch\": 2, \"batch_size\": 128}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the image URI\n",
    "The image that we will is dependent on the region that you are running this notebook in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SageMaker Tensorflow Estimator\n",
    "To enable profiling, you need to pass the Debugger profiling configuration (`profiler_config`), a list of Debugger rules (`rules`), and the image URI (`image_uri`) to the estimator. Debugger enables monitoring and profiling while the SageMaker estimator requests a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "job_name = \"network-bottleneck\"\n",
    "instance_count = 1\n",
    "instance_type = \"ml.p2.xlarge\"\n",
    "entry_script = \"train_tf_bottleneck.py\"\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    image_uri=image_uri,\n",
    "    base_job_name=job_name,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    entry_point=entry_script,\n",
    "    source_dir=\"demo\",\n",
    "    profiler_config=profiler_config,\n",
    "    debugger_hook_config=debugger_hook_config,\n",
    "    script_mode=True,\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_mode=\"Pipe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you see an error, `TypeError: __init__() got an unexpected keyword argument 'instance_type'`, that means SageMaker Python SDK is out-dated. Please update your SageMaker Python SDK to 2.x by executing the below command and restart this notebook.\n",
    "\n",
    "```bash\n",
    "pip install --upgrade sagemaker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training job\n",
    "\n",
    "The following `estimator.fit()` with `wait=False` argument initiates the training job in the background. You can proceed to run the dashboard or analysis notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_inputs = {\"train\": dataset_location + \"/train\"}\n",
    "\n",
    "estimator.fit(remote_inputs, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monitor the system resource utilization using SageMaker Studio\n",
    "\n",
    "SageMaker Studio provides the visualization tool for Sagemaker Debugger where you can find the analysis report and the system and framework resource utilization history.\n",
    "\n",
    "To access this information in SageMaker Studio, click on the last icon on the left to open `SageMaker Components and registries` and choose `Experiments and trials`. You will see the list of training jobs. Right click on the job you want to investigate shows a pop-up menu, then click on `Open Debugger for insights` which opens a new tab for SageMaker Debugger.\n",
    "\n",
    "There are two tabs, `Overview` and `Nodes`. `Overview` gives profiling summaries for quick review, and `Nodes` gives a detailed utilization information on all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SageMaker Debugger profiling analysis utilities\n",
    "We can use the profiling analysis utilities to gain deeper insights into what the source of the issue is.\n",
    "For this step, we will rely on the bokeh and smdebug packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bokeh==2.1.1\n",
    "! pip install smdebug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use smdebug to extract gpu and framework metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "from smdebug.profiler.analysis.utils.profiler_data_to_pandas import PandasFrame\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "region = \"us-east-1\"\n",
    "\n",
    "tj = TrainingJob(training_job_name, region)\n",
    "\n",
    "pf = PandasFrame(tj.profiler_s3_output_path)\n",
    "\n",
    "# extract gpu metrics\n",
    "system_metrics_df = pf.get_all_system_metrics()\n",
    "gpus = system_metrics_df[system_metrics_df[\"dimension\"] == \"GPUUtilization\"]\n",
    "timestamps = gpus[\"timestamp_us\"].to_numpy()\n",
    "values = gpus[\"value\"].to_numpy()\n",
    "\n",
    "# exctract framework metrics\n",
    "framework_metrics_df = pf.get_all_framework_metrics(\n",
    "    selected_framework_metrics=[\"Step:ModeKeys.TRAIN\", \"Step:ModeKeys.GLOBAL\"]\n",
    ")\n",
    "train_steps = framework_metrics_df[\n",
    "    framework_metrics_df[\"framework_metric\"].isin([\"Step:ModeKeys.TRAIN\", \"Step:ModeKeys.GLOBAL\"])\n",
    "]\n",
    "start_step = train_steps[\"start_time_us\"].to_numpy()\n",
    "end_step = train_steps[\"end_time_us\"].to_numpy()\n",
    "step_num = train_steps[\"step\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use bokeh to plot the gpu metrics and the training progression on the same graph. This enables us to correlate between the two. We can see that the drops in gpu utilization coincide with every 50th step, which are marked in yellow. These are precisely the steps in which we have chosen to capture all of the graph tensors.\n",
    "![bokeh-graph](./images/bokeh_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bokeh.models import ColumnDataSource, CustomJS, Div, HoverTool, HBar\n",
    "from bokeh.models.glyphs import Circle, Line\n",
    "from bokeh.plotting import figure, show\n",
    "\n",
    "plot = figure(\n",
    "    plot_height=400,\n",
    "    plot_width=1400,\n",
    "    x_range=(timestamps[0], timestamps[-1]),\n",
    "    y_range=(-1, 110),\n",
    "    tools=\"crosshair,xbox_select,pan,reset,save,xwheel_zoom\",\n",
    ")\n",
    "x_range = plot.x_range\n",
    "\n",
    "plot.xgrid.visible = False\n",
    "plot.ygrid.visible = False\n",
    "\n",
    "colors = np.where(step_num % 50 == 0, \"yellow\", \"purple\")\n",
    "\n",
    "# pad framework metrics to match length of system metrics\n",
    "pad = values.size - step_num.size\n",
    "source = ColumnDataSource(\n",
    "    data=dict(\n",
    "        x=timestamps,\n",
    "        y=values,\n",
    "        left=np.pad(start_step, (0, pad)),\n",
    "        right=np.pad(end_step, (0, pad)),\n",
    "        color=np.pad(colors, (0, pad)),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "callback = CustomJS(\n",
    "    args=dict(s1=source, div=Div(width=250, height=100, height_policy=\"fixed\")),\n",
    "    code=\"\"\"\n",
    "        console.log('Running CustomJS callback now.');\n",
    "        var inds = s1.selected.indices;\n",
    "        console.log(inds);\n",
    "        var line = \"<span style=float:left;clear:left;font_size=13px><b> Selected index range: [\" + Math.min.apply(Math,inds) + \",\" + Math.max.apply(Math,inds) + \"]</b></span>\\\\n\";\n",
    "        console.log(line)\n",
    "        var text = div.text.concat(line);\n",
    "        var lines = text.split(\"\\\\n\")\n",
    "        if (lines.length > 35)\n",
    "            lines.shift();\n",
    "        div.text = lines.join(\"\\\\n\");\"\"\",\n",
    ")\n",
    "\n",
    "plot.js_on_event(\"selectiongeometry\", callback)\n",
    "\n",
    "line = Line(x=\"x\", y=\"y\", line_color=\"white\")\n",
    "circle = Circle(x=\"x\", y=\"y\", fill_alpha=0, line_width=0)\n",
    "hbar = HBar(\n",
    "    y=105, height=5, right=\"right\", left=\"left\", fill_color=\"color\", line_cap=\"round\", line_width=0\n",
    ")\n",
    "\n",
    "\n",
    "p = plot.add_glyph(source, line)\n",
    "p = plot.add_glyph(source, circle)\n",
    "p = plot.add_glyph(source, hbar)\n",
    "\n",
    "# create tooltip for hover tool\n",
    "hover = HoverTool(renderers=[p], tooltips=[(\"index\", \"$index\"), (\"(x,y)\", \"($x, $y)\")])\n",
    "\n",
    "plot.xaxis.axis_label = \"Time in ms\"\n",
    "plot.yaxis.axis_label = \"GPU Utilization\"\n",
    "plot.add_tools(hover)\n",
    "show(plot, notebook_handle=True)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
