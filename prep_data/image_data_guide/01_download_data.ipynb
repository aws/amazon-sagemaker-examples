{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and Combining Datasets (Part 1/4)\n",
    "**Download** | Structure | Preprocessing | Train Model\n",
    "\n",
    "**Note**: This notebook work best running on an ml.t3.xlarge instance. If you're experiencing any network errors while downloading the dataset or out of memory errors, you may need to increase the instance size you're using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will use a dataset manifest to download animal images from the COCO dataset for all ten animal classes. You will then download frog images from the CIFAR dataset and add them to your COCO animal images. In order to simulate coming to SageMaker with your own dataset, we will keep the data in an unstructured form until the next notebook where you will learn the best practices for structuring an image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* #### [The COCO and CIFAR datasets](#ipg1.1)\n",
    "* #### [Download the annotations](#ipg1.2)\n",
    "* #### [Extract animal annotations](#ipg1.3)\n",
    "* #### [Sample the dataset](#ipg1.4)\n",
    "* #### [Combine with CIFAR-10 frog data](#ipg1.5)\n",
    "* #### [Store annotations for next guides](#ipg1.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import shutil\n",
    "import urllib\n",
    "import pathlib\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread, imwrite\n",
    "from joblib import Parallel, delayed, parallel_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg1.1'></a>\n",
    "## The COCO and CIFAR Datasets\n",
    "___\n",
    "For this series of notebooks we will be sampling images from the [COCO dataset](https://cocodataset.org) and [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) (before beginning the notebooks in this series, it's a good idea to browse each dataset website to familiaraize youreself with the data). Both are datasets of images, but come formatted very differently. The COCO dataset contains images from Flickr that represent a real-world dataset which isn't formatted or resized specifically for deep learning. This makes it a good dataset for this guide because we want it to be as comprehensive as possible. The CIFAR-10 images, on the other hand, are preprocessed specifically for deep learning as they come cropped, resized and vectorized (i.e. not in a readable image format). This notebooks will show you how to work with both types of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg1.2'></a>\n",
    "## Download the annotations\n",
    "____\n",
    "The dataset annotation file contains info on each image in the dataset such as the class, superclass, file name and url to download the file. Just the annotations for the COCO dataset are about 242MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "urllib.request.urlretrieve(anno_url, \"coco-annotations.zip\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.unpack_archive(\"coco-annotations.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the annotations into Python\n",
    "The training and validation annotations come in separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"annotations/instances_train2017.json\", \"r\") as f:\n",
    "    train_metadata = json.load(f)\n",
    "\n",
    "with open(\"annotations/instances_val2017.json\", \"r\") as f:\n",
    "    val_metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg1.3'></a>\n",
    "## Extract only the animal annotations\n",
    "___\n",
    "To limit the scope of the dataset for this guide we're only using the images of animals in the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = {\n",
    "    c[\"id\"]: c[\"name\"] for c in train_metadata[\"categories\"] if c[\"supercategory\"] == \"animal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract metadata and image filepaths\n",
    "For the train and validation sets, the data we need for the image labels and the filepaths are under different headings in the annotations. We have to extract each out and combine them into a single annotation in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annos = {}\n",
    "for a in train_metadata[\"annotations\"]:\n",
    "    if a[\"category_id\"] in category_labels:\n",
    "        train_annos[a[\"image_id\"]] = {\"category_id\": a[\"category_id\"]}\n",
    "\n",
    "train_images = {}\n",
    "for i in train_metadata[\"images\"]:\n",
    "    train_images[i[\"id\"]] = {\"coco_url\": i[\"coco_url\"], \"file_name\": i[\"file_name\"]}\n",
    "\n",
    "val_annos = {}\n",
    "for a in val_metadata[\"annotations\"]:\n",
    "    if a[\"category_id\"] in category_labels:\n",
    "        val_annos[a[\"image_id\"]] = {\"category_id\": a[\"category_id\"]}\n",
    "\n",
    "val_images = {}\n",
    "for i in val_metadata[\"images\"]:\n",
    "    val_images[i[\"id\"]] = {\"coco_url\": i[\"coco_url\"], \"file_name\": i[\"file_name\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine label and filepath info\n",
    "Later in this series of guides we'll make our own train, validation and test splits. For this reason we'll combine the training and validation datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, anno in train_annos.items():\n",
    "    anno.update(train_images[id])\n",
    "\n",
    "for id, anno in val_annos.items():\n",
    "    anno.update(val_images[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_annos = {}\n",
    "for k, v in train_annos.items():\n",
    "    all_annos.update({k: v})\n",
    "for k, v in val_annos.items():\n",
    "    all_annos.update({k: v})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg1.4'></a>\n",
    "## Sample the dataset\n",
    "___\n",
    "In order to make working with the data easier, we'll select 250 images from each class at random. To make sure you get the same set of cell images for each run of this we'll also set Numpy's random seed to 0. This is a small fraction of the dataset, but it demonstrates how using transfer learning can give you good results without needing very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_annos = {}\n",
    "\n",
    "for category_id in category_labels:\n",
    "    subset = [k for k, v in all_annos.items() if v[\"category_id\"] == category_id]\n",
    "    sample = np.random.choice(subset, size=250, replace=False)\n",
    "    for k in sample:\n",
    "        sample_annos[k] = all_annos[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a download function\n",
    "In order to parallelize downloading the images we must wrap the download and save process with a function for multi-threading with joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, path):\n",
    "    data = imread(url)\n",
    "    imwrite(path / url.split(\"/\")[-1], data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the sample of the dataset (2,500 images, ~5min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = pathlib.Path(\"data_sample_2500\")\n",
    "sample_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_backend(\"threading\", n_jobs=5):\n",
    "    Parallel(verbose=3)(\n",
    "        delayed(download_image)(a[\"coco_url\"], sample_dir) for a in sample_annos.values()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg1.5'></a>\n",
    "## Combine with CIFAR-10 frog data\n",
    "___\n",
    "The COCO dataset doesn't include any images of frogs, but let's say our model must also be able to label images of frogs. To fix this we can download another dataset of images which includes frogs, sample 250 frog images and add them to our existing image data. These images are much smaller (32x32) so they will appear pixelated and blurry when we increase the size of them to (244x244). We'll use the CIFAR-10 dataset to achieve this. As you'll see the CIFAR-10 dataset comes formatted in a very different manner from COCO dataset. We must process the CIFAR-10 data into individual image files so that it's congruent to our COCO images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cs.toronto.edu/%7Ekriz/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tarfile.open(\"cifar-10-python.tar.gz\")\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open first batch of CIFAR-10 dataset\n",
    "The CIFAR-10 dataset comes in five training batches and one test batch. Each training batch has 10,000 randomly ordered images. Since we only need 250 frog images for our dataset, just pulling from the first batch will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./cifar-10-batches-py/data_batch_1\", \"rb\") as f:\n",
    "    batch_1 = pickle.load(f, encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = batch_1[b\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull 250 sample frog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frog_indices = np.array(batch_1[b\"labels\"]) == 6\n",
    "sample_frog_indices = np.random.choice(frog_indices.nonzero()[0], size=250, replace=False)\n",
    "sample_data = image_data[sample_frog_indices, :]\n",
    "frog_images = sample_data.reshape(len(sample_data), 3, 32, 32).transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View frog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4, figsize=(10, 7))\n",
    "indices = np.random.randint(low=0, high=249, size=12)\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(frog_images[indices[i]])\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write sample frog images to `data_sample_2500` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frog_filenames = np.array(batch_1[b\"filenames\"])[sample_frog_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, filename in enumerate(frog_filenames):\n",
    "    filename = filename.decode()\n",
    "    data = frog_images[idx]\n",
    "    if filename.endswith(\".png\"):\n",
    "        filename = filename.replace(\".png\", \".jpg\")\n",
    "    imwrite(sample_dir / filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir.rename(\"data_sample_2750\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add frog annotations to `sample_annos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels[26] = \"frog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_anno_idx = np.array(list(sample_annos.keys())).max() + 1\n",
    "\n",
    "frog_anno_ids = range(next_anno_idx, next_anno_idx + len(frog_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, frog_id in enumerate(frog_anno_ids):\n",
    "    sample_annos[frog_id] = {\n",
    "        \"category_id\": 26,\n",
    "        \"file_name\": frog_filenames[idx].decode().replace(\".png\", \".jpg\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg1.6'></a>\n",
    "## Store annotations for next guides\n",
    "___\n",
    "This is just the first in a series of guides for training a deep learning model with image data. In order to make sure your work carries over to subsequent notebooks, you will create a folder called `pickled_data` which will store your dataset annotations and asset names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_dir = pathlib.Path(\"./pickled_data\")\n",
    "pickled_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(\"pickled_data/sample_annos.pickle\", \"wb\") as f:\n",
    "    pickle.dump(sample_annos, f)\n",
    "\n",
    "with open(\"./pickled_data/category_labels.pickle\", \"wb\") as f:\n",
    "    pickle.dump(category_labels, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
