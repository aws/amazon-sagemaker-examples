{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef06ca1-2057-4f38-afd8-90fbb479fa02",
   "metadata": {},
   "source": [
    "# Deploy open-source Large Language Models on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a8104-9618-4e6c-b9b6-f83a07573e87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d3f23-3262-42ea-bd7a-76f109266d00",
   "metadata": {},
   "source": [
    "In this notebook, we will show you how to deploy the open-source LLMs from HuggingFace on Amazon SageMaker. The notebook contains three sections:\n",
    "- Section 1: Deploy Falcon model and embedding model to Amazon SageMaker\n",
    "- Section 2: Use RAG based approach with [LangChain](https://python.langchain.com/en/latest/index.html) and SageMaker endpoints to build a simplified question and answering application.\n",
    "\n",
    "***\n",
    "This notebook is designed to run on `Python 3 Data Science 3.0` kernel in Amazon SageMaker Studio\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b730c-dc87-45e2-bc41-482b05b8c2c0",
   "metadata": {},
   "source": [
    "#### 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy BLOOM to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. You can safely ignore the errors from the pip install if there is any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf395449-857b-4305-94b0-a1379646a743",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker==2.180.0 boto3==1.28.35 --quiet\n",
    "!pip install ipywidgets==7.0.0 langchain==0.0.148 faiss-cpu==1.7.4 unstructured==0.10.6 --quiet --use-deprecated=legacy-resolver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5dd15-9d1b-4020-8b06-159530972ca6",
   "metadata": {},
   "source": [
    "## Secton 1: Deploy Falcon model and embedding model to Amazon SageMaker\n",
    "In this section, we will deploy the open-source [Falcon 7b instruct model](https://huggingface.co/tiiuae/falcon-7b-instruct) on SageMaker for real-time inference. \n",
    "To deploy [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) to Amazon SageMaker we create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc. We will use a `g5.2xlarge` instance type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11498376-bf21-457e-b5de-aaad86309ee1",
   "metadata": {},
   "source": [
    "This is an example on how to deploy the open-source LLMs to Amazon SageMaker for inference using the [Large Model Inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html) container from DLC to SageMaker and run inference with it. We will deploy the 7B-Instruct [Falcon](https://huggingface.co/tiiuae/falcon-7b-instruct) an open-source Chat LLM trained by TII.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20547cd8-c7c1-4e23-a2f0-da24c5c0ac33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {region}\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbbb2f-d974-464b-8251-e940cab1958b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Start preparing model artifacts\n",
    "In LMI container, we expect some artifacts to help setting up the model\n",
    "- serving.properties (required): Defines the model server settings\n",
    "- model.py (optional): A python file to define the core inference logic\n",
    "- requirements.txt (optional): Any additional pip wheel need to install\n",
    "\n",
    "In the **serving.properties** file defines the engine to use and model to host. Note the `tensor_parallel_degree` parameter which is also required in this scenario. We will use tensor parallelism to divide the model into multiple parts because no single GPU has enough memory for the entire model. In this case we will use a 'ml.g5.2xlarge' instance which provides 1 GPU. Be careful not to specify a value larger than the instance provides or your deployment will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d0083-685f-4088-8e68-9cadbe385fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=MPI\n",
    "option.model_id=tiiuae/falcon-7b-instruct\n",
    "option.trust_remote_code=true\n",
    "option.tensor_parallel_degree=1\n",
    "option.paged_attention=true\n",
    "option.max_rolling_batch_size=64\n",
    "option.rolling_batch=lmi-dist\n",
    "option.max_rolling_batch_prefill_tokens=1560\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6458412-cddd-4856-92f0-dc9bba969214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel-7b\n",
    "mv serving.properties mymodel-7b/\n",
    "tar czvf mymodel-7b.tar.gz mymodel-7b/\n",
    "rm -rf mymodel-7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70cadef-d65a-40e1-8a14-ebdfc9b00ece",
   "metadata": {},
   "source": [
    "### Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch\n",
    "\n",
    "#### Getting the container image URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4581d4-d256-4b90-80ad-1c1da3048b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(framework=\"djl-deepspeed\", region=region, version=\"0.23.0\")\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed661a-2f38-4c7c-a12e-001e5196309f",
   "metadata": {},
   "source": [
    "Then we upload the artifacts on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b36cd-e86d-42fe-ae8c-cb5a7e655bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "code_artifact = sess.upload_data(\"mymodel-7b.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "falcon_model_name = sagemaker.utils.name_from_base(\"lmi-model-falcon-7b\")\n",
    "model = Model(\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_uri,\n",
    "    model_data=code_artifact,\n",
    "    role=role,\n",
    "    name=falcon_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ef70a-cf7d-404e-8b42-35ef25ebc7e4",
   "metadata": {},
   "source": [
    "#### Create SageMaker endpoint\n",
    "\n",
    "We now can call the deploy function to create the LLM endpoint. You need to specify the instance to use and endpoint names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfdad7-0384-43d0-bf25-7767598c0cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = falcon_model_name\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=900,\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4ac097-7cbc-4722-a934-3f4c661a3d85",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can take 10-15 minutes. During this time, please continue the following section to deploy the embedding model for the RAG solution. We will invoke the deployed endpoint when all the models are deployed successfully.\n",
    "\n",
    "To see more model deployment examples, you can find an example notebook [here at the SageMaker examples gitrepo.](https://github.com/aws/amazon-sagemaker-examples/tree/51cbf4a77c98dc9b74fde6a8c47db0dad40fb910/inference/generativeai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afbb7b-ff6f-4cb2-b5b5-7936f4317557",
   "metadata": {},
   "source": [
    "#### Deploy the GPT-J 6B embedding on SageMaker using SageMaker Jumpstart\n",
    "\n",
    "In this section, we host the pre-trained [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6b) Hugging Face sentence transformer model, into SageMaker and generate an embedding vector with 4096 dimensions of the input text string. In this lab, we will use the `GPT-J 6B Embedding FP16` provided by SageMaker Jumpstart which loads a 16-bit quantized version of the original model by specifying the half-precision dtype, torch.float16. By using half precision, this model consumes less GPU memory and performs faster inference than the full precision version. For more information, please view the Hugging Face documentation for [FP16 optimization](https://huggingface.co/docs/diffusers/optimization/fp16)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602bb61-4eb5-4510-9f5f-62106b2d88a9",
   "metadata": {},
   "source": [
    "There are different ways you can choose to deploy the GPT-J-6B model. Here we show you two options:\n",
    "- deploy the GPT-J-6B embedding model from the Jumpstart UI\n",
    "- deploy the GPT-J-6B embedding model using SageMaker python SDK\n",
    "\n",
    "Please choose only one of the below two options to deploy the embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2646f-af6f-412f-9c77-534b9021e370",
   "metadata": {},
   "source": [
    "#### Option 1: Deploy the GPT-J-6B embedding model from the Jumpstart UI\n",
    "On the left-hand-side navigation pane, got to **Home**, under **SageMaker JumpStart**, choose **Model, notebooks, solutions**. You’re presented with a range of solutions, foundation models, and other artifacts that can help you get started with a specific model or a specific business problem or use case. If you want to experiment in a particular area, you can use the search function. Or you can simply browse the artifacts to find the relevant model or business solution for your needs. To start exploring the Stable Diffusion models, complete the following steps:\n",
    "\n",
    "1. Go to the **Foundation Models** section. In the search bar, search for the **embedding** model and select the **GPT-J 6B Embedding FP16**.\n",
    "\n",
    "![Image jumpstart](./img/embedding_model.png)\n",
    "\n",
    "2. A new tab is opened with the options to train, deploy and view model details as shown below. In the Deploy Model section, expand Deployment Configuration. For SageMaker hosting instance, choose the hosting instance (for this lab, we use ml.g5.4xlarge). You can also change the Endpoint name as needed. Then click the Deploy button.\n",
    "\n",
    "![Image deploy](./img/embedding_deploy.png)\n",
    "\n",
    "3. The deploy action will start a new tab showing the model creation status and the model deployment status. \n",
    "\n",
    "While the endpoint is deploying, update the embedding endpoint name in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d8156-b2fa-4f27-ae6e-1d9a52ff4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_embed = \"<your embedding model endpoint name>\"  # change the endpoint name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d1018-ab6c-4a92-bc5f-79932ed7d33d",
   "metadata": {},
   "source": [
    "Now you can directly go to section **[Wait until all the endpoints are up and running](#Wait-until-all-the-endpoints-are-up-and-running)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935c4461-77fb-4a3a-b35c-ea228d3307c3",
   "metadata": {},
   "source": [
    "#### Option 2: deploy the GPT-J-6B embedding model using SageMaker python SDK\n",
    "Now we will show you how to use code to deploy the pretrained models from SageMaker Jumpstart using the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1399b4-4897-4426-941c-e58d095ada04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "import os\n",
    "\n",
    "instance_type = \"ml.g5.4xlarge\"  # instance type to use for deployment\n",
    "model_version = \"*\"\n",
    "env = {\"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\", \"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b66e2e3-1e20-4a8d-88f7-d5bb0503cfce",
   "metadata": {},
   "source": [
    "We can directly load the pretrained model artifacts from SageMaker JumpStart. The SageMaker Python SDK uses model IDs and model versions to access the necessary utilities for pre-trained models. The [table provided by the readme doc](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html) serves to provide the core material plus some extra information that can be useful in selecting the correct model ID and corresponding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67dac5b-4e5d-4f7d-ac67-5a3ac1d1958d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"huggingface-textembedding-gpt-j-6b-fp16\"\n",
    "\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "embed_endpoint_name = sagemaker.utils.name_from_base(model_id)\n",
    "\n",
    "# Retrieve the inference container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "model_inference = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=model_id,\n",
    "    env=env,\n",
    ")\n",
    "model_predictor_inference = model_inference.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=embed_endpoint_name,\n",
    "    wait=False,\n",
    ")\n",
    "print(f\"Model {model_id} has been created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b351236-bf12-4949-9c3c-46b6ff09e883",
   "metadata": {},
   "source": [
    "### Wait until all the endpoints are up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde02055-1a3a-443f-b3c6-ced34078a367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait for the endpoint to be deployed successfully\n",
    "def wait_for_endpoint(endpoint_name=None):\n",
    "    describe_endpoint_response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "    while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "        describe_endpoint_response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "        time.sleep(15)\n",
    "\n",
    "    print(f\"endpoint {endpoint_name} is in service now.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7af04-7a00-4d46-9e0c-39049f46ce92",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ep_name in [endpoint_name, embed_endpoint_name]:\n",
    "    wait_for_endpoint(ep_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41314b01-96b5-4c05-924a-b48c62daaf49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test endpoint outputs\n",
    "Now we can invoke each endpoint to test the endpoint outputs. First, let's check the text-to-text endpoint using Falcon model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda84f44-7304-4db0-a113-5603be78a373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "body = {\"inputs\": \"what is life?\", \"parameters\": {\"max_new_tokens\": 400, \"return_full_text\": False}}\n",
    "output = smr.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\"\n",
    ")\n",
    "resp = json.loads(output[\"Body\"].read().decode(\"utf8\"))\n",
    "print(resp[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826495d-015b-46f5-bcd5-bc8b3bf94c7a",
   "metadata": {},
   "source": [
    "Then run the follow code to generate embeddings of the input using the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed94501-85d5-43c0-959b-ca93133119b0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_str = {\"text_inputs\": resp[\"generated_text\"]}\n",
    "output = smr.invoke_endpoint(\n",
    "    EndpointName=embed_endpoint_name, Body=json.dumps(input_str), ContentType=\"application/json\"\n",
    ")\n",
    "embeddings = output[\"Body\"].read().decode(\"utf-8\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25196b-8520-467c-82d4-ff56f7c626e4",
   "metadata": {},
   "source": [
    "## Section2: Use RAG based approach with [LangChain](https://python.langchain.com/en/latest/index.html) and SageMaker endpoints to build a simplified question and answering application.\n",
    "\n",
    "\n",
    "We plan to use document embeddings to fetch the most relevant documents in our document knowledge library and combine them with the prompt that we provide to LLM.\n",
    "\n",
    "To achieve that, we will do following.\n",
    "\n",
    "1. **Generate embedings for each of document in the knowledge library with Huggingface all-MiniLM-L6-v2 embedding model.**\n",
    "2. **Identify top K most relevant documents based on user query.**\n",
    "    - 2.1 **For a query of your interest, generate the embedding of the query using the same embedding model.**\n",
    "    - 2.2 **Search the indexes of top K most relevant documents in the embedding space using in-memory Faiss search.**\n",
    "    - 2.3 **Use the indexes to retrieve the corresponded documents.**\n",
    "3. **Combine the retrieved documents with prompt and question and send them into SageMaker LLM.**\n",
    "\n",
    "\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt -- maximum sequence length. \n",
    "\n",
    "---\n",
    "To build a simplied QA application with LangChain, we need: \n",
    "1. Wrap up our SageMaker endpoints for embedding model and LLM into `langchain.embeddings.SagemakerEndpointEmbeddings` and `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. That requires a small overwritten of `SagemakerEndpointEmbeddings` class to make it compatible with SageMaker embedding mdoel.\n",
    "2. Prepare the dataset to build the knowledge data base. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c101e-1fbe-4c31-a1ef-8b63212fa790",
   "metadata": {},
   "source": [
    "Wrap up our SageMaker endpoints for embedding model into `langchain.embeddings.SagemakerEndpointEmbeddings`. That requires a small overwritten of `SagemakerEndpointEmbeddings` class to make it compatible with SageMaker embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f8cae-807a-408d-a9cf-98b43bbb2542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from typing import Any, Dict, List, Optional\n",
    "import json\n",
    "\n",
    "\n",
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(self, texts: List[str], chunk_size: int = 5) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i : i + _chunk_size])\n",
    "            print\n",
    "            results.extend(response)\n",
    "        return results\n",
    "\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddingsJumpStart(\n",
    "    endpoint_name=embed_endpoint_name,\n",
    "    region_name=region,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7a5f2-058b-4620-a37b-95cfca6a1e80",
   "metadata": {},
   "source": [
    "Next, we wrap up our SageMaker endpoints for LLM into `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8461b-6eff-475a-85e3-f0973880a555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "parameters = {\"max_new_tokens\": 500, \"return_full_text\": False, \"temperature\": 0.1}\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": {**model_kwargs}})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = output.read()\n",
    "        res = json.loads(response_json)\n",
    "        ans = res[\"generated_text\"]\n",
    "        return ans\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c1370-3271-4561-b449-0c90782e91b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b7547-a52d-4f74-9982-0e7501386642",
   "metadata": {},
   "source": [
    "Use langchain to read the `txt` data. There are multiple built-in functions in LangChain to read different format of files such as `csv`, `html`, and `pdf`. For details, see [LangChain document loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27d89d-8d56-413f-b753-1e48e79446e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"./data/\", glob=\"**/*.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4a131-9b09-4141-96c5-6a13751e99ff",
   "metadata": {},
   "source": [
    "We generate embedings for each of document in the knowledge library with Huggingface all-MiniLM-L6-v2 embedding model.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51eb647-9b91-4581-bdfa-12f55ab0f4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4604c98-a75d-4d5c-8672-5774ef7060a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"what is the recommended way to first customize a foundation model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a972160-933c-4353-b0e6-1c4aaa878ae8",
   "metadata": {},
   "source": [
    "Based on the question above, we then **identify top K most relevant documents based on user query, where K = 3 in this setup**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c6fb8-ec23-42d4-9853-d0f3c292f103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = docsearch.similarity_search_with_score(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aec596-f6be-4780-9599-6793a2f26b22",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6375e8-321b-40a2-b631-0ff456063849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source = []\n",
    "context = []\n",
    "for doc, score in docs:\n",
    "    context.append(doc)\n",
    "    source.append(doc.metadata[\"source\"].split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22f198-7474-4c57-b4d7-0be16d90248c",
   "metadata": {},
   "source": [
    "Finally, we **combine the retrieved documents with prompt and question and send them into SageMaker LLM.** \n",
    "\n",
    "We define a customized prompt as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53822d7-e44c-4076-8735-59a6ce7f7ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.:\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b71b4-a04b-4a66-b5ff-b5e2d132703f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm=sm_llm, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb6db1-3897-410c-9599-5d6f63333a63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = chain({\"input_documents\": context, \"question\": question}, return_only_outputs=True)[\n",
    "    \"output_text\"\n",
    "]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927245c3-a8e9-498f-8b94-1edcfc8727e2",
   "metadata": {},
   "source": [
    "### Run the Question and Answering chatbot application\n",
    "\n",
    "Once all the endpoints are deployed successfully, you can open a terminal in SageMaker Studio and use the below command to run the chatbot [Streamlit](https://streamlit.io/) application. Note that you need to install the required python packages that are specified in the “requirements.txt” file. You also need to update the environment variables with the endpoint names deployed in your account accordingly. When you execute the `chatbot-steamlit.py` file, it will automatically update the endpoint names based on the environment variables.\n",
    "\n",
    "```\n",
    "$ pip install -r requirements.txt\n",
    "$ export nlp_ep_name=<the falcon endpoint name deployed in your account>\n",
    "$ export embed_ep_name=<the embedding endpoint name deployed in your account>\n",
    "$ streamlit run chatbot-streamlit.py --server.port 6006 --server.maxUploadSize 6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e7780-c954-4003-b56c-b9dc6d0ea230",
   "metadata": {},
   "source": [
    "To access the Streamlit UI, copy your SageMaker Studio url and replace `lab?` with `proxy/[PORT NUMBER]/`. Because we specified the server port to 6006, so the url should look like:\n",
    "\n",
    "```\n",
    "https://<domain ID>.studio.<region>.sagemaker.aws/jupyter/default/proxy/6006/\n",
    "```\n",
    "\n",
    "Replace the domain ID and region with the correct value in your account to access the UI as below:\n",
    "![streamlitUI](./img/Streamlit_UI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb216f5-1582-4708-869c-d7de017bd7d3",
   "metadata": {},
   "source": [
    "You can find some suggested `prompt` on the left-hand-side sidebar. When you upload the sample files (you can find the sample files in the test folder), the chatbot will automatically provide prompt suggestions based on the input data type.\n",
    "\n",
    "**Congratulations on finishing lab 1 !!!**\n",
    "\n",
    "_____________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f9f81c-f762-43a7-9c46-cf26f2455b48",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceced14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The below lines will delete the falcon model endpoint, change the endpoint and model name to delete other resources created.\n",
    "# sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# sm_client.delete_model(ModelName=falcon_model_name)\n",
    "# sm_client.delete_endpoint(ModelName=embed_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edefd0a-f79f-49d6-8026-d6db830aba3e",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|generativeai|llm-workshop|chatbot-apps|build_cahtbot_applications_using_rag_on_sagemaker.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb520172-b099-4ac9-b27e-17f57fe35fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
