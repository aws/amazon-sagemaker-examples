{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0524d4b4",
   "metadata": {},
   "source": [
    "#  Using TorchServe on SageMaker Inf2.24xlarge with LLAMAv2-13B\n",
    "\n",
    "\n",
    "## Step 1: Let's bump up SageMaker and import stuff\n",
    "\n",
    "The wheel installed here is a private preview wheel, you need to add into allowlist to run this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6901ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b98a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest aws cli v2 if it is not installed\n",
    "!curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "!unzip awscliv2.zip\n",
    "#Follow the instruction to install aws v2 on the terminal\n",
    "!cat aws/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conform it is aws-cli/2.xx.xx\n",
    "!aws --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d68be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install sagemaker pip --upgrade  --quiet\n",
    "!pip install numpy\n",
    "!pip install pillow\n",
    "!pip install -U sagemaker\n",
    "!pip install -U boto \n",
    "!pip install -U botocore\n",
    "!pip install -U boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4874a3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "account=084495728311, region=us-west-2, role=arn:aws:iam::084495728311:role/service-role/AmazonSageMaker-ExecutionRole-20230505T104760, output_path=s3://sagemaker-us-west-2-084495728311/torchserve\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "boto3_session=boto3.session.Session(region_name=\"us-west-2\")\n",
    "smr = boto3.client('sagemaker-runtime-demo')\n",
    "sm = boto3.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess= sagemaker.session.Session(boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr)  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "# Configuration:\n",
    "bucket_name = sess.default_bucket()\n",
    "prefix = \"torchserve\"\n",
    "output_path = f\"s3://{bucket_name}/{prefix}\"\n",
    "print(f'account={account_id}, region={region}, role={role}, output_path={output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c2c9a",
   "metadata": {},
   "source": [
    "## Step 2: Build a BYOD TorchServe Docker container and push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a00114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install our own dependencies\n",
    "!cat workspace/docker/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c82d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture build_output\n",
    "\n",
    "baseimage = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.12.0-ubuntu20.04\"\n",
    "reponame = \"neuronx212\"\n",
    "versiontag = \"latest\"\n",
    "\n",
    "# Build our own docker image\n",
    "!cd workspace/docker && ./build_and_push.sh {reponame} {versiontag} {baseimage} {region} {account}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Error response from daemon' in str(build_output):\n",
    "    print(build_output)\n",
    "    raise SystemExit('\\n\\n!!There was an error with the container build!!')\n",
    "else:\n",
    "    container = str(build_output).strip().split('\\n')[-1]\n",
    "\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eef005",
   "metadata": {},
   "source": [
    "## Step 3: AOT Pre-Compile Model on EC2 (Optional in this example)\n",
    "\n",
    "This step is to precompile model with batchSize and save it in cache dir to use it in later model loading. It can significantly reduce model loading latency to a few seconds.\n",
    "\n",
    "#### Precompile the model at local EC2\n",
    "\n",
    "Follow [Instruction](https://github.com/pytorch/serve/pull/2458/files#diff-bc416c811f749ead11ab8f100d5a4198fa453adc995b0760272563971638307d) at local EC2 to precompile the model. The precompiled model will be stored in `model_store/llama-2-13b/neuron_cache`\n",
    "\n",
    "#### Main steps on EC2\n",
    "                     \n",
    "* download llama-2-13b from HF \n",
    "* Save the model split checkpoints compatible with `transformers-neuronx`\n",
    "* Create EC2 model artifacts at local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d62e7",
   "metadata": {},
   "source": [
    "#### Keynotes\n",
    "\n",
    "1. **Turn on neuron_cache** in function initialize of inf2_handler.py\n",
    "```\n",
    "os.environ[\"NEURONX_CACHE\"] = \"on\"\n",
    "os.environ[\"NEURONX_DUMP_TO\"] = f\"{model_dir}/neuron_cache\"\n",
    "```\n",
    "\n",
    "2. Set **micro_batch_size: 4** in model-config.yaml to precompile the model with batch size 4 in this example\n",
    "```\n",
    "minWorkers: 1\n",
    "maxWorkers: 1\n",
    "maxBatchDelay: 100\n",
    "responseTimeout: 10800\n",
    "batchSize: 16\n",
    "\n",
    "handler:\n",
    "    amp: \"bf16\"\n",
    "    tp_degree: 6\n",
    "    max_length: 100\n",
    "\n",
    "micro_batching:\n",
    "    micro_batch_size: 4\n",
    "    parallelism:\n",
    "        preprocess: 2\n",
    "        inference: 1\n",
    "        postprocess: 2\n",
    "```\n",
    "\n",
    "3. The generated cache files are **tightly coupled with neuronx SDK version**\n",
    "```\n",
    "This example is based on torch-neuronx==1.13.1.1.9.0\n",
    "```\n",
    "\n",
    "4. Create **SageMaker model artifacts format** once the model artifacts is generated on EC2\n",
    "```\n",
    "cd model_store/llama-2-13b/\n",
    "mkdir code\n",
    "mv requirements.txt code/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48ab5d",
   "metadata": {},
   "source": [
    "## Step 4: Upload model artifacts to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115df40",
   "metadata": {},
   "source": [
    "You can choose either option 1 or option 2.\n",
    "\n",
    "* option 1: Sample command to copy llama-2-13b from EC2 to S3\n",
    "```\n",
    "print(output_path)\n",
    "!aws s3 cp llama-2-13b {output_path}/llama-2-13b --recursive\n",
    "s3_uri = f\"{output_path}/llama-2-13b-neuronx-b4/\"\n",
    "```\n",
    "\n",
    "* option 2: The model artifacts of this example is available in s3://torchserve/mar_files/sm-neuronx/llama-2-13b-neuronx-b4/ which supports llama2-13b on neuronx batchSize = 4 based on torch-neuronx==1.13.1.1.9.0. You can copy it to your SM S3 model repo. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d5d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://torchserve/mar_files/sm-neuronx/llama-2-13b-neuronx-b4/ llama-2-13b-neuronx-b4 --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp llama-2-13b-neuronx-b4 {output_path}/llama-2-13b-neuronx-b4 --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2735a786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-084495728311/torchserve/llama-2-13b-neuronx-b4/\n"
     ]
    }
   ],
   "source": [
    "s3_uri = f\"{output_path}/llama-2-13b-neuronx-b4/\"\n",
    "print(s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c259c",
   "metadata": {},
   "source": [
    "## Step 5: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771053b",
   "metadata": {},
   "source": [
    "### Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bcfdfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.model.Model object at 0x7fd86599f400>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "instance_type = \"ml.inf2.24xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"ts-inf2-llama2-13b-b1\")\n",
    "\n",
    "model = Model(\n",
    "    name=\"torchserve-inf2-llama2-13b\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n",
    "    model_data={\n",
    "        \"S3DataSource\": {\n",
    "                \"S3Uri\": s3_uri,\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"CompressionType\": \"None\",\n",
    "        }\n",
    "    },\n",
    "    image_uri=f\"084495728311.dkr.ecr.us-west-2.amazonaws.com/neuronx212:latest\",\n",
    "    #image_uri=container,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53eaa416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your model is not compiled. Please compile your model before using Inferentia.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    volume_size=512, # increase the size to store large model\n",
    "    model_data_download_timeout=3600, # increase the timeout to download large model\n",
    "    container_startup_health_check_timeout=3600, # ncrease the timeout to load large model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39513f65",
   "metadata": {},
   "source": [
    "## Run the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "360839a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "class Parser:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "        data = self.buff.getvalue()\n",
    "        \n",
    "    def scan_lines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b'\\n':\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "                \n",
    "    def reset(self):\n",
    "        self.read_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f760056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on going to the beach. I am going to take my camera and take some pictures. I am going to take pictures of the beach and the ocean. I am going to take pictures of the people and the animals. I am going to take pictures of the sun and the sky. I am going to take pictures of the sand and the water. I am going to take pictures of the trees and the flowers. I am going to take pictures of the birds and the butterflies. I a "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "body = \"Today the weather is really nice and I am planning on\".encode('utf-8')\n",
    "resp = smr.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=body, ContentType=\"application/json\")\n",
    "event_stream = resp['Body']\n",
    "parser = Parser()\n",
    "for event in event_stream:\n",
    "    parser.write(event['PayloadPart']['Bytes'])\n",
    "    for line in parser.scan_lines():\n",
    "        print(line.decode(\"utf-8\"), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2e7b4",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bcaf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
