{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Training using MXNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and MXNet. This notebook example shows how to use Horovod with MXNet in SageMaker using MNIST dataset.\n",
    "\n",
    "For more information about the MXNet in SageMaker, please visit following github repositories:\n",
    "1. [sagemaker-mxnet-training-toolkit](https://github.com/aws/sagemaker-mxnet-training-toolkit/)\n",
    "2. [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) \n",
    "3. [sagemaker-training-toolkit](https://github.com/aws/sagemaker-training-toolkit)\n",
    "4. [deep-learning-containers](https://github.com/aws/deep-learning-containers)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p2.xlarge notebook instance._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the [Amazon SageMaker Roles](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "sess= sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-mxnet-mnist-horovod'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "output_path='s3://' + sess.default_bucket() + '/' + prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "\n",
    "We will download MNIST data from a public bucket and upload it to the default bucket associated with your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import json\n",
    "# Download training and testing data from a public S3 bucket\n",
    "\n",
    "public_bucket = 'sagemaker-sample-files'\n",
    "local_data_dir = '/tmp/data'\n",
    "\n",
    "def download_from_s3(data_dir, train=True):\n",
    "    \"\"\"Download MNIST dataset and convert it to numpy array\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): directory to save the data\n",
    "        train (bool): download training set\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    if train:\n",
    "        images_file = \"train-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "    else:\n",
    "        images_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "  \n",
    "    # download objects\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket = public_bucket\n",
    "    for obj in [images_file, labels_file]:\n",
    "        key = os.path.join(\"datasets/image/MNIST\", obj)\n",
    "        dest = os.path.join(data_dir, obj)\n",
    "        if not os.path.exists(dest):\n",
    "            s3.download_file(bucket, key, dest)\n",
    "    return\n",
    "\n",
    "\n",
    "download_from_s3(local_data_dir, True)\n",
    "download_from_s3(local_data_dir, False)\n",
    "\n",
    "\n",
    "# upload to the default bucket\n",
    "\n",
    "prefix = 'mnist'\n",
    "bucket = sess.default_bucket()\n",
    "loc = sess.upload_data(path=local_data_dir, bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "channels = {\n",
    "    \"training\": loc,\n",
    "    \"testing\": loc\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Define hyperparameters of training job. Note, that `entry_point` param defines training script which will be executed on Horovod distributed cluster. Additionally, you can also define any parameters of your training script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Scipt\n",
    "\n",
    "The mnist.py script provides the code we need for training a SageMaker model. The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "- `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to. These artifacts are uploaded to S3 for model hosting.\n",
    "- `SM_NUM_GPUS`: The number of gpus available in the current container.\n",
    "- `SM_CURRENT_HOST`: The name of the current container on the container network.\n",
    "- `SM_HOSTS`: JSON encoded list containing all the hosts .\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's fit() method, the following will be set, following the format SM_CHANNEL_[channel_name]:\n",
    "\n",
    "- `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit SageMaker Containers.\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "This script uses Horovod framework for distributed training. \n",
    "\n",
    "You can run the following command to view the script run by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.:wq\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;49;00m\n",
      "\u001b[37m# may not use this file except in compliance with the License. A copy of\u001b[39;49;00m\n",
      "\u001b[37m# the License is located at\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m#     http://aws.amazon.com/apache2.0/\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m# or in the \"license\" file accompanying this file. This file is\u001b[39;49;00m\n",
      "\u001b[37m# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\u001b[39;49;00m\n",
      "\u001b[37m# ANY KIND, either express or implied. See the License for the specific\u001b[39;49;00m\n",
      "\u001b[37m# language governing permissions and limitations under the License.\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgzip\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmx\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mgluon\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dataset, DataLoader\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mhorovod\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mhvd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m autograd, gluon, nd\n",
      "\n",
      "logging.basicConfig(level=logging.DEBUG)\n",
      "logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mSo should it be\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mnormalize\u001b[39;49;00m(x, axis):\n",
      "    eps = np.finfo(\u001b[36mfloat\u001b[39;49;00m).eps\n",
      "    mean = np.mean(x, axis=axis, keepdims=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \u001b[37m# avoid division by zero\u001b[39;49;00m\n",
      "    std = np.std(x, axis=axis, keepdims=\u001b[34mTrue\u001b[39;49;00m) + eps\n",
      "    \u001b[34mreturn\u001b[39;49;00m (x - mean) / std\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_transformer\u001b[39;49;00m(data, label):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mData shape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, data.shape)\n",
      "    \u001b[37m# normalize the pixels\u001b[39;49;00m\n",
      "    data = normalize(data.astype(np.float32), axis=(\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m))\n",
      "    \n",
      "    \u001b[37m# add channel dim\u001b[39;49;00m\n",
      "    data = np.expand_dims(data, axis=\u001b[34m1\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m data, label\n",
      "\n",
      "\u001b[37m# MNIST data set\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMNIST\u001b[39;49;00m(Dataset):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, data_dir, train=\u001b[34mTrue\u001b[39;49;00m, transform=\u001b[34mNone\u001b[39;49;00m):\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m train:\n",
      "            images_file=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "            labels_file=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            images_file=\u001b[33m\"\u001b[39;49;00m\u001b[33mt10k-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "            labels_file=\u001b[33m\"\u001b[39;49;00m\u001b[33mt10k-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        \n",
      "        \u001b[36mself\u001b[39;49;00m.images, \u001b[36mself\u001b[39;49;00m.labels = \u001b[36mself\u001b[39;49;00m._convert_to_numpy(\n",
      "                data_dir, images_file, labels_file)\n",
      "\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32m_id\u001b[39;49;00m(x, y):\n",
      "            \u001b[34mreturn\u001b[39;49;00m x, y\n",
      "        \u001b[36mself\u001b[39;49;00m.transform = transform \u001b[35mor\u001b[39;49;00m _id \n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.labels)\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, idx):\n",
      "        img, label = \u001b[36mself\u001b[39;49;00m.images[idx], \u001b[36mself\u001b[39;49;00m.labels[idx]\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.transform(img, label)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_convert_to_numpy\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, data_dir, images_file, labels_file):\n",
      "        \u001b[33m\"\"\"Byte string to numpy arrays \u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[34mwith\u001b[39;49;00m gzip.open(os.path.join(data_dir, images_file), \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "            images = np.frombuffer(f.read(), \n",
      "                    np.uint8, offset=\u001b[34m16\u001b[39;49;00m).reshape(-\u001b[34m1\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m)\n",
      "\n",
      "            \u001b[37m# normalize \u001b[39;49;00m\n",
      "            images = normalize(images.astype(np.float32), axis=(\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m))\n",
      "            \n",
      "            \u001b[37m# add channel dim\u001b[39;49;00m\n",
      "            images = np.expand_dims(images, axis=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[34mwith\u001b[39;49;00m gzip.open(os.path.join(data_dir, labels_file), \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "            labels = np.frombuffer(f.read(), np.uint8, offset=\u001b[34m8\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m (images, labels)\n",
      "\n",
      "    \n",
      "\u001b[37m# model\u001b[39;49;00m\n",
      "    \u001b[37m# Function to define neural network\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconv_nets\u001b[39;49;00m():\n",
      "    kernel_size = \u001b[34m5\u001b[39;49;00m\n",
      "    strides = \u001b[34m2\u001b[39;49;00m\n",
      "    pool_size = \u001b[34m2\u001b[39;49;00m\n",
      "    hidden_dim = \u001b[34m512\u001b[39;49;00m\n",
      "    output_dim = \u001b[34m10\u001b[39;49;00m\n",
      "    activation = \u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    \n",
      "    net = gluon.nn.HybridSequential()\n",
      "    \u001b[34mwith\u001b[39;49;00m net.name_scope():\n",
      "        net.add(gluon.nn.Conv2D(channels=\u001b[34m20\u001b[39;49;00m, kernel_size=kernel_size, activation=activation))\n",
      "        net.add(gluon.nn.MaxPool2D(pool_size=pool_size, strides=strides))\n",
      "        net.add(gluon.nn.Conv2D(channels=\u001b[34m50\u001b[39;49;00m, kernel_size=kernel_size, activation=activation))\n",
      "        net.add(gluon.nn.MaxPool2D(pool_size=pool_size, strides=strides))\n",
      "        net.add(gluon.nn.Flatten())\n",
      "        net.add(gluon.nn.Dense(hidden_dim, activation=activation))\n",
      "        net.add(gluon.nn.Dense(output_dim))\n",
      "    \u001b[34mreturn\u001b[39;49;00m net\n",
      "\n",
      "  \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\n",
      "    model_dir = args.model_dir\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(model_dir):\n",
      "        os.makedirs(model_dir)\n",
      "\n",
      "    \u001b[37m# Initialize Horovod\u001b[39;49;00m\n",
      "    hvd.init()\n",
      "\n",
      "    \u001b[37m# Horovod: pin context to local rank\u001b[39;49;00m\n",
      "    context = mx.cpu(hvd.local_rank()) \u001b[34mif\u001b[39;49;00m args.no_cuda \u001b[34melse\u001b[39;49;00m mx.gpu(hvd.local_rank())\n",
      "    num_workers = hvd.size()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining Context: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, context)\n",
      "    \n",
      "    train_set = MNIST(args.data_dir, train=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    test_set = MNIST(args.data_dir, train=\u001b[34mFalse\u001b[39;49;00m)\n",
      "\n",
      "    train_iter = DataLoader(train_set, batch_size=args.batch_size, \n",
      "        shuffle=\u001b[34mTrue\u001b[39;49;00m, last_batch=\u001b[33m'\u001b[39;49;00m\u001b[33mrollover\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    test_iter = DataLoader(test_set, batch_size=args.batch_size,\n",
      "        shuffle=\u001b[34mFalse\u001b[39;49;00m, last_batch=\u001b[33m'\u001b[39;49;00m\u001b[33mrollover\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "    \u001b[37m# Build model\u001b[39;49;00m\n",
      "    model = conv_nets()\n",
      "    model.cast(args.dtype)\n",
      "    model.hybridize()\n",
      "\n",
      "    \u001b[37m# Create optimizer\u001b[39;49;00m\n",
      "    optimizer_params = {\u001b[33m'\u001b[39;49;00m\u001b[33mmomentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.momentum,\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.lr * hvd.size()}\n",
      "\n",
      "    opt = mx.optimizer.create(\u001b[33m'\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, **optimizer_params)\n",
      "\n",
      "    \u001b[37m# Initialize parameters\u001b[39;49;00m\n",
      "    initializer = mx.init.Xavier(rnd_type=\u001b[33m'\u001b[39;49;00m\u001b[33mgaussian\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, factor_type=\u001b[33m\"\u001b[39;49;00m\u001b[33min\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                                 magnitude=\u001b[34m2\u001b[39;49;00m)\n",
      "    model.initialize(initializer, ctx=context)\n",
      "\n",
      "    \u001b[37m# Horovod: fetch and broadcast parameters\u001b[39;49;00m\n",
      "    params = model.collect_params()\n",
      "    \u001b[34mif\u001b[39;49;00m params \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        hvd.broadcast_parameters(params, root_rank=\u001b[34m0\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Horovod: create DistributedTrainer, a subclass of gluon.Trainer\u001b[39;49;00m\n",
      "    trainer = hvd.DistributedTrainer(params, opt)\n",
      "\n",
      "    \u001b[37m# Create loss function and train metric\u001b[39;49;00m\n",
      "    loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
      "    metric = mx.metric.Accuracy()\n",
      "\n",
      "    \u001b[37m# Global training timing\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m hvd.rank() == \u001b[34m0\u001b[39;49;00m:\n",
      "        global_tic = time.time()\n",
      "    \n",
      "    \u001b[37m# Train model\u001b[39;49;00m\n",
      "    best_val_acc = \u001b[34m0.0\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStart training ...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(args.epochs):\n",
      "        tic = time.time()\n",
      "        metric.reset()\n",
      "        \u001b[34mfor\u001b[39;49;00m nbatch, (data, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_iter, start=\u001b[34m1\u001b[39;49;00m):\n",
      "            data = data.as_in_context(context)\n",
      "            label = label.as_in_context(context)\n",
      "            \u001b[34mwith\u001b[39;49;00m autograd.record():\n",
      "                output = model(data.astype(args.dtype, copy=\u001b[34mFalse\u001b[39;49;00m))\n",
      "                loss = loss_fn(output, label)\n",
      "                loss.backward()\n",
      "\n",
      "            trainer.step(args.batch_size)\n",
      "            metric.update([label], [output])\n",
      "\n",
      "            \u001b[34mif\u001b[39;49;00m nbatch % \u001b[34m100\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\n",
      "                name, acc = metric.get()\n",
      "                logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33m[Epoch \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m Batch \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m] Training: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\n",
      "                             (epoch, nbatch, name, acc))\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m hvd.rank() == \u001b[34m0\u001b[39;49;00m:\n",
      "            elapsed = time.time() - tic\n",
      "            speed = nbatch * args.batch_size * hvd.size() / elapsed\n",
      "            logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mEpoch[\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mSpeed=\u001b[39;49;00m\u001b[33m%.2f\u001b[39;49;00m\u001b[33m samples/s\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mTime cost=\u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                         epoch, speed, elapsed)\n",
      "\n",
      "        \u001b[37m# Evaluate model accuracy\u001b[39;49;00m\n",
      "        _, train_acc = metric.get()\n",
      "        name, val_acc = evaluate(model, test_iter, context)\n",
      "        \u001b[34mif\u001b[39;49;00m val_acc > best_val_acc:\n",
      "            best_val_acc = val_acc\n",
      "            \u001b[34mif\u001b[39;49;00m hvd.rank() == \u001b[34m0\u001b[39;49;00m:\n",
      "                model.export(model_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m hvd.rank() == \u001b[34m0\u001b[39;49;00m:\n",
      "            logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mEpoch[\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mTrain: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mValidation: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "            epoch, name,train_acc, name, val_acc)\n",
      "\n",
      "\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m hvd.rank()==\u001b[34m0\u001b[39;49;00m: \n",
      "        global_training_time =time.time() - global_tic \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mGlobal elpased time on training:\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(global_training_time))\n",
      "        device = context.device_type + \u001b[36mstr\u001b[39;49;00m(num_workers)\n",
      "        logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mDevice info: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, device)\n",
      "    \u001b[34mreturn\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# Function to evaluate accuracy for a model\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mevaluate\u001b[39;49;00m(model, data_iter, context):\n",
      "    metric = mx.metric.Accuracy()\n",
      "    \u001b[34mfor\u001b[39;49;00m _, (data, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(data_iter):\n",
      "        data = data.as_in_context(context)\n",
      "        label = label.as_in_context(context)\n",
      "        output = model(data)\n",
      "        metric.update([label], [output])\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m metric.get()\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    \u001b[37m# Handling script arguments\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mMXNet MNIST Distributed Example\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mtraining batch size (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dtype\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mtraining data type (default: float32)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of training epochs (default: 5)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.9)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--no-cuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mdisable training on GPU (default: False)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Container Environment\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.no_cuda:\n",
      "        \u001b[37m# Disable CUDA if there are no GPUs.\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m mx.context.num_gpus() == \u001b[34m0\u001b[39;49;00m:\n",
      "            args.no_cuda = \u001b[34mTrue\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m args \n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    \n",
      "    args = parse_args()\n",
      "    \u001b[37m# logging.info(args)\u001b[39;49;00m\n",
      "\n",
      "    train(args)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training in SageMaker\n",
    "\n",
    "The `MXNet` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters. In this case we are going to run our training job on 2 `ml.p2.8xlarge` instances. But this example can be ran on one or multiple, cpu or gpu instances ([full list of available instances](https://aws.amazon.com/sagemaker/pricing/instance-types/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker MXNet Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator API in Sagemaker Python SDK supports distributed training functionality via the distributions parameter.\n",
    "To leverage Horovod, we specify `mpi` dictionary in the distributions parameter. The dictionary can contain following keys\n",
    "- `enabled`: True/False\n",
    "- `custom_mpi_options`: string\n",
    "- `processes_per_host`: integer\n",
    "\n",
    "Note: `train_instance_type` and `processes_per_host` are interlinked. Make sure that `processes_per_host` doesn't exceed the number of available GPUs in the instance.\n",
    "\n",
    "\n",
    "For further details on various AWS EC2 instances & available GPUs refer:\n",
    "- P3 (https://aws.amazon.com/ec2/instance-types/p3/)\n",
    "- G4 (https://aws.amazon.com/ec2/instance-types/g4/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpi_options = '-verbose -x orte_base_help_aggregate=0'\n",
    "distributions = {\n",
    "    'mpi':{\n",
    "        'enabled': True,\n",
    "        'custom_mpi_options': mpi_options,\n",
    "        'processes_per_host': 4\n",
    "    }\n",
    "}\n",
    "hyperparameters = {\n",
    "    'batch-size': 64,\n",
    "    'dtype': 'float32',\n",
    "    'epochs': 10,\n",
    "    'lr': 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributions has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "estimator = MXNet(\n",
    "    entry_point='train.py',\n",
    "    source_dir='code',\n",
    "    role=role,\n",
    "    instance_type='ml.p3.8xlarge',\n",
    "    instance_count=1,\n",
    "    framework_version='1.7.0',\n",
    "    output_path=output_path,\n",
    "    py_version='py3',\n",
    "    distributions=distributions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `MXNet` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-02 00:38:57 Starting - Starting the training job...\n",
      "2020-12-02 00:38:59 Starting - Launching requested ML instances.........\n",
      "2020-12-02 00:40:42 Starting - Preparing the instances for training......\n",
      "2020-12-02 00:41:32 Downloading - Downloading input data...\n",
      "2020-12-02 00:41:59 Training - Downloading the training image...\n",
      "2020-12-02 00:42:57 Training - Training image download completed. Training in progress..\u001b[34m2020-12-02 00:42:57,862 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2020-12-02 00:42:57,905 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"batch-size\":64,\"dtype\":\"float32\",\"epochs\":10,\"lr\":0.01}', 'SM_USER_ENTRY_POINT': 'train.py', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"testing\",\"training\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'train', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '32', 'SM_NUM_GPUS': '4', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-688520471316/mxnet-training-2020-12-02-00-38-56-686/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":64,\"dtype\":\"float32\",\"epochs\":10,\"lr\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-12-02-00-38-56-686\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-688520471316/mxnet-training-2020-12-02-00-38-56-686/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}', 'SM_USER_ARGS': '[\"--batch-size\",\"64\",\"--dtype\",\"float32\",\"--epochs\",\"10\",\"--lr\",\"0.01\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TESTING': '/opt/ml/input/data/testing', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'SM_HP_BATCH-SIZE': '64', 'SM_HP_LR': '0.01', 'SM_HP_DTYPE': 'float32', 'SM_HP_EPOCHS': '10'}\u001b[0m\n",
      "\u001b[34m2020-12-02 00:43:01,371 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2020-12-02 00:43:01,371 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2020-12-02 00:43:01,375 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2020-12-02 00:43:01,375 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:4'] process_per_hosts: 4 num_processes: 4\u001b[0m\n",
      "\u001b[34m2020-12-02 00:43:01,377 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2020-12-02 00:43:01,420 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 4,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-verbose -x orte_base_help_aggregate=0\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 64,\n",
      "        \"lr\": 0.01,\n",
      "        \"dtype\": \"float32\",\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"mxnet-training-2020-12-02-00-38-56-686\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-688520471316/mxnet-training-2020-12-02-00-38-56-686/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":64,\"dtype\":\"float32\",\"epochs\":10,\"lr\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-688520471316/mxnet-training-2020-12-02-00-38-56-686/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":64,\"dtype\":\"float32\",\"epochs\":10,\"lr\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-12-02-00-38-56-686\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-688520471316/mxnet-training-2020-12-02-00-38-56-686/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"64\",\"--dtype\",\"float32\",\"--epochs\",\"10\",\"--lr\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_DTYPE=float32\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:4 -np 4 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose -x orte_base_help_aggregate=0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TESTING -x SM_CHANNEL_TRAINING -x SM_HP_BATCH-SIZE -x SM_HP_LR -x SM_HP_DTYPE -x SM_HP_EPOCHS -x PYTHONPATH /usr/local/bin/python3.6 -m mpi4py train.py --batch-size 64 --dtype float32 --epochs 10 --lr 0.01\n",
      "\n",
      "\n",
      " Data for JOB [41836,1] offset 0 Total slots allocated 4\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-108-60#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [41836,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41836,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41836,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41836,1] App: 0 Process rank: 3 Bound: N/A\n",
      "\n",
      " =============================================================\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,0]<stderr>:INFO:root:So should it be\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:So should it be\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:So should it be\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:So should it be\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Training Context:  gpu(0)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Training Context:  gpu(2)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Training Context:  gpu(1)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Training Context:  gpu(3)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Start training ...\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Start training ...\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Start training ...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Start training ...\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-02 00:43:12.262 algo-1:24 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-12-02 00:43:12.262 algo-1:26 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-12-02 00:43:12.262 algo-1:25 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-02 00:43:12.262 algo-1:23 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-12-02 00:43:12.262 algo-1:25 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-02 00:43:12.262 algo-1:24 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-12-02 00:43:12.262 algo-1:26 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-02 00:43:12.262 algo-1:23 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-12-02 00:43:12.262 algo-1:26 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-12-02 00:43:12.262 algo-1:25 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-02 00:43:12.262 algo-1:24 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-02 00:43:12.262 algo-1:24 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-02 00:43:12.262 algo-1:23 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-12-02 00:43:12.262 algo-1:26 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-12-02 00:43:12.262 algo-1:25 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-02 00:43:12.262 algo-1:24 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-02 00:43:12.262 algo-1:23 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-12-02 00:43:12.262 algo-1:26 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-12-02 00:43:12.262 algo-1:25 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-02 00:43:12.263 algo-1:23 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2020-12-02 00:43:12.275 algo-1:24 INFO hook.py:235] Registering hook for block softmaxcrossentropyloss0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2020-12-02 00:43:12.275 algo-1:23 INFO hook.py:235] Registering hook for block softmaxcrossentropyloss0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2020-12-02 00:43:12.276 algo-1:25 INFO hook.py:235] Registering hook for block softmaxcrossentropyloss0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2020-12-02 00:43:12.276 algo-1:26 INFO hook.py:235] Registering hook for block softmaxcrossentropyloss0\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 50000, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 50000, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 50000, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 10240, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00024] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 131072, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00025] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00026] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[00:43:12] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (set the environment variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:[00:43:12] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (set the environment variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[00:43:12] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (set the environment variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[00:43:12] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (set the environment variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=5.0949201583862305,Timestamp=1606869792.4055445,IterationNumber=0)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:metrics_file_path=/opt/ml/output/metrics/sagemaker/23.json\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Bootstrap : Using [0]eth0:10.0.108.60<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.108.60<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.4.8+cuda10.1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO Bootstrap : Using [0]eth0:10.0.108.60<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO Bootstrap : Using [0]eth0:10.0.108.60<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO Bootstrap : Using [0]eth0:10.0.108.60<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.108.60<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.108.60<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.108.60<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Channel 00 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Channel 01 :    0   2   1   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Channel 02 :    0   3   1   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Channel 03 :    0   3   2   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Channel 04 :    0   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Channel 05 :    0   2   1   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Channel 06 :    0   3   1   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Channel 07 :    0   3   2   1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO Ring 00 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO Ring 01 : 1[1] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Ring 01 : 0[0] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO Ring 01 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO Ring 01 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO Ring 02 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Ring 02 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO Ring 02 : 3[3] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO Ring 02 : 2[2] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO Ring 03 : 1[1] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Ring 03 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO Ring 03 : 3[3] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO Ring 03 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Ring 04 : 0[0] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO Ring 04 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO Ring 04 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO Ring 04 : 2[2] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Ring 05 : 0[0] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO Ring 05 : 1[1] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO Ring 05 : 3[3] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO Ring 05 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Ring 06 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO Ring 06 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO Ring 06 : 3[3] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO Ring 06 : 2[2] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO Ring 07 : 1[1] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Ring 07 : 0[0] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO Ring 07 : 3[3] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO Ring 07 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO comm 0x7fce7c21b9b0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:24:502 [1] NCCL INFO comm 0x7fad7820e6b0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:26:509 [3] NCCL INFO comm 0x7fed8820e6f0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:25:507 [2] NCCL INFO comm 0x7f24d020dec0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:23:503 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 0 Batch 100] Training: accuracy=0.156250\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 0 Batch 100] Training: accuracy=0.154688\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 0 Batch 100] Training: accuracy=0.158125\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 0 Batch 100] Training: accuracy=0.152812\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 0 Batch 200] Training: accuracy=0.326016\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 0 Batch 200] Training: accuracy=0.332578\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 0 Batch 200] Training: accuracy=0.321484\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 0 Batch 200] Training: accuracy=0.321562\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 0 Batch 300] Training: accuracy=0.479219\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 0 Batch 300] Training: accuracy=0.475677\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 0 Batch 300] Training: accuracy=0.475469\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 0 Batch 300] Training: accuracy=0.481250\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 0 Batch 400] Training: accuracy=0.577891\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 0 Batch 400] Training: accuracy=0.582109\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 0 Batch 400] Training: accuracy=0.578516\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 0 Batch 400] Training: accuracy=0.581484\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 0 Batch 500] Training: accuracy=0.645844\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 0 Batch 500] Training: accuracy=0.644375\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 0 Batch 500] Training: accuracy=0.649000\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 0 Batch 500] Training: accuracy=0.649438\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.3289102613925934,Timestamp=1606869795.2819705,IterationNumber=500)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 0 Batch 600] Training: accuracy=0.695911\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 0 Batch 600] Training: accuracy=0.692344\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 0 Batch 600] Training: accuracy=0.694974\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 0 Batch 600] Training: accuracy=0.693411\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 0 Batch 700] Training: accuracy=0.730469\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 0 Batch 700] Training: accuracy=0.729062\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 0 Batch 700] Training: accuracy=0.728326\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 0 Batch 700] Training: accuracy=0.727277\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 0 Batch 800] Training: accuracy=0.756113\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 0 Batch 800] Training: accuracy=0.753906\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 0 Batch 800] Training: accuracy=0.757012\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 0 Batch 800] Training: accuracy=0.754062\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 0 Batch 900] Training: accuracy=0.777986\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 0 Batch 900] Training: accuracy=0.775833\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 0 Batch 900] Training: accuracy=0.775660\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 0 Batch 900] Training: accuracy=0.778715\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[0]#011Speed=44506.84 samples/s#011Time cost=5.389554\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[0]#011Train: accuracy=0.782567#011Validation: accuracy=0.951723\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 1 Batch 100] Training: accuracy=0.956250\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 1 Batch 100] Training: accuracy=0.952031\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 1 Batch 100] Training: accuracy=0.956094\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 1 Batch 100] Training: accuracy=0.959375\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 1 Batch 200] Training: accuracy=0.955078\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 1 Batch 200] Training: accuracy=0.953359\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 1 Batch 200] Training: accuracy=0.954688\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 1 Batch 200] Training: accuracy=0.955547\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 1 Batch 300] Training: accuracy=0.954375\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 1 Batch 300] Training: accuracy=0.955677\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 1 Batch 300] Training: accuracy=0.954792\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 1 Batch 300] Training: accuracy=0.954531\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 1 Batch 400] Training: accuracy=0.955859\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 1 Batch 400] Training: accuracy=0.955859\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 1 Batch 400] Training: accuracy=0.955977\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 1 Batch 400] Training: accuracy=0.955391\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.07686702907085419,Timestamp=1606869799.9027927,IterationNumber=1500)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 1 Batch 500] Training: accuracy=0.956844\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 1 Batch 500] Training: accuracy=0.956562\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 1 Batch 500] Training: accuracy=0.956844\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 1 Batch 500] Training: accuracy=0.956125\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 1 Batch 600] Training: accuracy=0.957109\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 1 Batch 600] Training: accuracy=0.956536\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 1 Batch 600] Training: accuracy=0.956745\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 1 Batch 600] Training: accuracy=0.957891\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 1 Batch 700] Training: accuracy=0.957076\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 1 Batch 700] Training: accuracy=0.957790\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 1 Batch 700] Training: accuracy=0.958214\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 1 Batch 700] Training: accuracy=0.957656\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 1 Batch 800] Training: accuracy=0.958184\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 1 Batch 800] Training: accuracy=0.958418\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 1 Batch 800] Training: accuracy=0.957734\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 1 Batch 800] Training: accuracy=0.957871\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 1 Batch 900] Training: accuracy=0.957743\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 1 Batch 900] Training: accuracy=0.958559\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 1 Batch 900] Training: accuracy=0.958490\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 1 Batch 900] Training: accuracy=0.957604\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.05542081221938133,Timestamp=1606869802.4404478,IterationNumber=2000)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[1]#011Speed=50459.53 samples/s#011Time cost=4.758823\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[1]#011Train: accuracy=0.958655#011Validation: accuracy=0.964343\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 2 Batch 100] Training: accuracy=0.967344\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 2 Batch 100] Training: accuracy=0.963437\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 2 Batch 100] Training: accuracy=0.966562\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 2 Batch 100] Training: accuracy=0.965313\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 2 Batch 200] Training: accuracy=0.964063\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 2 Batch 200] Training: accuracy=0.967031\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 2 Batch 200] Training: accuracy=0.966406\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 2 Batch 200] Training: accuracy=0.965938\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 2 Batch 300] Training: accuracy=0.963802\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 2 Batch 300] Training: accuracy=0.965260\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 2 Batch 300] Training: accuracy=0.967396\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 2 Batch 300] Training: accuracy=0.965469\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.2548854947090149,Timestamp=1606869804.4922743,IterationNumber=2500)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 2 Batch 400] Training: accuracy=0.965313\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 2 Batch 400] Training: accuracy=0.966719\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 2 Batch 400] Training: accuracy=0.965703\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 2 Batch 400] Training: accuracy=0.964258\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 2 Batch 500] Training: accuracy=0.964656\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 2 Batch 500] Training: accuracy=0.966531\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 2 Batch 500] Training: accuracy=0.965875\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 2 Batch 500] Training: accuracy=0.965437\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 2 Batch 600] Training: accuracy=0.965547\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 2 Batch 600] Training: accuracy=0.966276\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 2 Batch 600] Training: accuracy=0.966510\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 2 Batch 600] Training: accuracy=0.966016\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 2 Batch 700] Training: accuracy=0.966205\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 2 Batch 700] Training: accuracy=0.966674\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 2 Batch 700] Training: accuracy=0.966607\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 2 Batch 700] Training: accuracy=0.966317\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 2 Batch 800] Training: accuracy=0.966465\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 2 Batch 800] Training: accuracy=0.966309\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 2 Batch 800] Training: accuracy=0.967266\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 2 Batch 800] Training: accuracy=0.966797\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.11313736438751221,Timestamp=1606869807.0307453,IterationNumber=3000)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 2 Batch 900] Training: accuracy=0.966372\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 2 Batch 900] Training: accuracy=0.967396\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 2 Batch 900] Training: accuracy=0.966823\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 2 Batch 900] Training: accuracy=0.967222\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[2]#011Speed=50427.11 samples/s#011Time cost=4.756806\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[2]#011Train: accuracy=0.967466#011Validation: accuracy=0.966046\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 3 Batch 100] Training: accuracy=0.971094\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 3 Batch 100] Training: accuracy=0.969375\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 3 Batch 100] Training: accuracy=0.970313\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 3 Batch 100] Training: accuracy=0.970156\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 3 Batch 200] Training: accuracy=0.971719\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 3 Batch 200] Training: accuracy=0.969375\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 3 Batch 200] Training: accuracy=0.970938\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 3 Batch 200] Training: accuracy=0.969609\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.06047781929373741,Timestamp=1606869809.0818691,IterationNumber=3500)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 3 Batch 300] Training: accuracy=0.969167\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 3 Batch 300] Training: accuracy=0.970521\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 3 Batch 300] Training: accuracy=0.970365\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 3 Batch 300] Training: accuracy=0.971927\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 3 Batch 400] Training: accuracy=0.970195\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 3 Batch 400] Training: accuracy=0.970508\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 3 Batch 400] Training: accuracy=0.972812\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 3 Batch 400] Training: accuracy=0.971445\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 3 Batch 500] Training: accuracy=0.971437\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 3 Batch 500] Training: accuracy=0.972750\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 3 Batch 500] Training: accuracy=0.972344\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 3 Batch 500] Training: accuracy=0.971531\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 3 Batch 600] Training: accuracy=0.972031\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 3 Batch 600] Training: accuracy=0.971745\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 3 Batch 600] Training: accuracy=0.972448\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 3 Batch 600] Training: accuracy=0.971979\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 3 Batch 700] Training: accuracy=0.972009\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 3 Batch 700] Training: accuracy=0.971496\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 3 Batch 700] Training: accuracy=0.972321\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 3 Batch 700] Training: accuracy=0.972098\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.06005708500742912,Timestamp=1606869811.620519,IterationNumber=4000)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 3 Batch 800] Training: accuracy=0.972598\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 3 Batch 800] Training: accuracy=0.971914\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 3 Batch 800] Training: accuracy=0.972363\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 3 Batch 800] Training: accuracy=0.972383\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 3 Batch 900] Training: accuracy=0.971944\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 3 Batch 900] Training: accuracy=0.972726\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 3 Batch 900] Training: accuracy=0.972500\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 3 Batch 900] Training: accuracy=0.972292\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[3]#011Speed=50459.65 samples/s#011Time cost=4.758812\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[3]#011Train: accuracy=0.972465#011Validation: accuracy=0.968650\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 4 Batch 100] Training: accuracy=0.973125\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 4 Batch 100] Training: accuracy=0.975156\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 4 Batch 100] Training: accuracy=0.977031\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 4 Batch 100] Training: accuracy=0.974531\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.02162085846066475,Timestamp=1606869813.6921406,IterationNumber=4500)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 4 Batch 200] Training: accuracy=0.975000\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 4 Batch 200] Training: accuracy=0.973281\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 4 Batch 200] Training: accuracy=0.976016\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 4 Batch 200] Training: accuracy=0.975781\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 4 Batch 300] Training: accuracy=0.975208\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 4 Batch 300] Training: accuracy=0.976146\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 4 Batch 300] Training: accuracy=0.975104\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 4 Batch 300] Training: accuracy=0.973229\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 4 Batch 400] Training: accuracy=0.975742\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 4 Batch 400] Training: accuracy=0.975781\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 4 Batch 400] Training: accuracy=0.972930\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 4 Batch 400] Training: accuracy=0.975742\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 4 Batch 500] Training: accuracy=0.974969\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 4 Batch 500] Training: accuracy=0.974781\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 4 Batch 500] Training: accuracy=0.973219\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 4 Batch 500] Training: accuracy=0.975406\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 4 Batch 600] Training: accuracy=0.975469\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 4 Batch 600] Training: accuracy=0.974635\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 4 Batch 600] Training: accuracy=0.974505\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 4 Batch 600] Training: accuracy=0.972865\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.07347422093153,Timestamp=1606869816.2307837,IterationNumber=5000)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 4 Batch 700] Training: accuracy=0.972656\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 4 Batch 700] Training: accuracy=0.974196\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 4 Batch 700] Training: accuracy=0.974688\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 4 Batch 700] Training: accuracy=0.974821\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 4 Batch 800] Training: accuracy=0.973516\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 4 Batch 800] Training: accuracy=0.974941\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 4 Batch 800] Training: accuracy=0.974668\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 4 Batch 800] Training: accuracy=0.974570\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 4 Batch 900] Training: accuracy=0.973507\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 4 Batch 900] Training: accuracy=0.974566\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 4 Batch 900] Training: accuracy=0.974618\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 4 Batch 900] Training: accuracy=0.974444\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[4]#011Speed=50421.75 samples/s#011Time cost=4.757312\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[4]#011Train: accuracy=0.974686#011Validation: accuracy=0.967949\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.07433318346738815,Timestamp=1606869818.2710752,IterationNumber=5500)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 5 Batch 100] Training: accuracy=0.976406\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 5 Batch 100] Training: accuracy=0.976406\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 5 Batch 100] Training: accuracy=0.976562\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 5 Batch 100] Training: accuracy=0.974844\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 5 Batch 200] Training: accuracy=0.975469\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 5 Batch 200] Training: accuracy=0.976719\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 5 Batch 200] Training: accuracy=0.976953\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 5 Batch 200] Training: accuracy=0.977031\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 5 Batch 300] Training: accuracy=0.977031\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 5 Batch 300] Training: accuracy=0.975313\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 5 Batch 300] Training: accuracy=0.977448\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 5 Batch 300] Training: accuracy=0.976979\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 5 Batch 400] Training: accuracy=0.975547\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 5 Batch 400] Training: accuracy=0.977500\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 5 Batch 400] Training: accuracy=0.977305\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 5 Batch 400] Training: accuracy=0.976758\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 5 Batch 500] Training: accuracy=0.977875\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 5 Batch 500] Training: accuracy=0.976406\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 5 Batch 500] Training: accuracy=0.976812\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 5 Batch 500] Training: accuracy=0.977531\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.04748110473155975,Timestamp=1606869820.8097496,IterationNumber=6000)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 5 Batch 600] Training: accuracy=0.976094\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 5 Batch 600] Training: accuracy=0.976094\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 5 Batch 600] Training: accuracy=0.977318\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 5 Batch 600] Training: accuracy=0.977708\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 5 Batch 700] Training: accuracy=0.977121\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 5 Batch 700] Training: accuracy=0.977232\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 5 Batch 700] Training: accuracy=0.976362\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 5 Batch 700] Training: accuracy=0.976250\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 5 Batch 800] Training: accuracy=0.977656\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 5 Batch 800] Training: accuracy=0.977441\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 5 Batch 800] Training: accuracy=0.976738\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 5 Batch 800] Training: accuracy=0.976367\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 5 Batch 900] Training: accuracy=0.976875\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 5 Batch 900] Training: accuracy=0.977292\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 5 Batch 900] Training: accuracy=0.976337\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 5 Batch 900] Training: accuracy=0.977535\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[5]#011Speed=50410.21 samples/s#011Time cost=4.763479\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[5]#011Train: accuracy=0.977545#011Validation: accuracy=0.970853\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 6 Batch 100] Training: accuracy=0.977812\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 6 Batch 100] Training: accuracy=0.978750\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 6 Batch 100] Training: accuracy=0.979219\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 6 Batch 100] Training: accuracy=0.978594\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 6 Batch 200] Training: accuracy=0.979062\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 6 Batch 200] Training: accuracy=0.977969\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 6 Batch 200] Training: accuracy=0.978984\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 6 Batch 200] Training: accuracy=0.979844\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 6 Batch 300] Training: accuracy=0.979167\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 6 Batch 300] Training: accuracy=0.978229\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 6 Batch 300] Training: accuracy=0.978906\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 6 Batch 300] Training: accuracy=0.979635\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 6 Batch 400] Training: accuracy=0.978711\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 6 Batch 400] Training: accuracy=0.979258\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 6 Batch 400] Training: accuracy=0.979219\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 6 Batch 400] Training: accuracy=0.978281\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.06830169260501862,Timestamp=1606869825.403969,IterationNumber=7000)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 6 Batch 500] Training: accuracy=0.979406\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 6 Batch 500] Training: accuracy=0.979531\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 6 Batch 500] Training: accuracy=0.978375\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 6 Batch 500] Training: accuracy=0.978750\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 6 Batch 600] Training: accuracy=0.979062\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 6 Batch 600] Training: accuracy=0.978464\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 6 Batch 600] Training: accuracy=0.977995\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 6 Batch 600] Training: accuracy=0.978672\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 6 Batch 700] Training: accuracy=0.978170\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 6 Batch 700] Training: accuracy=0.979062\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 6 Batch 700] Training: accuracy=0.979062\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 6 Batch 700] Training: accuracy=0.978393\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 6 Batch 800] Training: accuracy=0.979102\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 6 Batch 800] Training: accuracy=0.979004\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 6 Batch 800] Training: accuracy=0.977969\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 6 Batch 800] Training: accuracy=0.978437\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 6 Batch 900] Training: accuracy=0.977986\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 6 Batch 900] Training: accuracy=0.978247\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 6 Batch 900] Training: accuracy=0.979115\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 6 Batch 900] Training: accuracy=0.978715\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[6]#011Speed=50322.37 samples/s#011Time cost=4.766707\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[6]#011Train: accuracy=0.978772#011Validation: accuracy=0.969651\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 7 Batch 100] Training: accuracy=0.979062\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 7 Batch 100] Training: accuracy=0.981250\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 7 Batch 100] Training: accuracy=0.979531\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 7 Batch 100] Training: accuracy=0.977344\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 7 Batch 200] Training: accuracy=0.978203\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 7 Batch 200] Training: accuracy=0.978203\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 7 Batch 200] Training: accuracy=0.980703\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 7 Batch 200] Training: accuracy=0.977422\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 7 Batch 300] Training: accuracy=0.979740\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 7 Batch 300] Training: accuracy=0.979167\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 7 Batch 300] Training: accuracy=0.979844\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 7 Batch 300] Training: accuracy=0.979740\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.10766345262527466,Timestamp=1606869830.0339496,IterationNumber=8000)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 7 Batch 400] Training: accuracy=0.979180\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 7 Batch 400] Training: accuracy=0.979648\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 7 Batch 400] Training: accuracy=0.979766\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 7 Batch 400] Training: accuracy=0.979844\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 7 Batch 500] Training: accuracy=0.979375\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 7 Batch 500] Training: accuracy=0.979469\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 7 Batch 500] Training: accuracy=0.979500\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 7 Batch 500] Training: accuracy=0.980000\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 7 Batch 600] Training: accuracy=0.979740\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 7 Batch 600] Training: accuracy=0.979714\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 7 Batch 600] Training: accuracy=0.979583\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 7 Batch 600] Training: accuracy=0.979583\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 7 Batch 700] Training: accuracy=0.979844\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 7 Batch 700] Training: accuracy=0.979196\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 7 Batch 700] Training: accuracy=0.979241\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 7 Batch 700] Training: accuracy=0.979330\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 7 Batch 800] Training: accuracy=0.980117\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 7 Batch 800] Training: accuracy=0.979141\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 7 Batch 800] Training: accuracy=0.979531\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 7 Batch 800] Training: accuracy=0.979336\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.0761505663394928,Timestamp=1606869832.5716667,IterationNumber=8500)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 7 Batch 900] Training: accuracy=0.979549\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 7 Batch 900] Training: accuracy=0.979948\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 7 Batch 900] Training: accuracy=0.979983\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 7 Batch 900] Training: accuracy=0.979635\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[7]#011Speed=50093.36 samples/s#011Time cost=4.793609\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[7]#011Train: accuracy=0.980061#011Validation: accuracy=0.973229\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 8 Batch 100] Training: accuracy=0.981875\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 8 Batch 100] Training: accuracy=0.979219\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 8 Batch 100] Training: accuracy=0.981563\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 8 Batch 100] Training: accuracy=0.978437\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 8 Batch 200] Training: accuracy=0.978203\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 8 Batch 200] Training: accuracy=0.981172\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 8 Batch 200] Training: accuracy=0.980156\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 8 Batch 200] Training: accuracy=0.980000\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.011159596964716911,Timestamp=1606869834.6131625,IterationNumber=9000)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 8 Batch 300] Training: accuracy=0.978906\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 8 Batch 300] Training: accuracy=0.980365\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 8 Batch 300] Training: accuracy=0.981771\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 8 Batch 300] Training: accuracy=0.980573\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 8 Batch 400] Training: accuracy=0.981484\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 8 Batch 400] Training: accuracy=0.980430\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 8 Batch 400] Training: accuracy=0.978477\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 8 Batch 400] Training: accuracy=0.980625\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 8 Batch 500] Training: accuracy=0.980406\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 8 Batch 500] Training: accuracy=0.980750\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 8 Batch 500] Training: accuracy=0.981031\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 8 Batch 500] Training: accuracy=0.978781\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 8 Batch 600] Training: accuracy=0.980755\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 8 Batch 600] Training: accuracy=0.978750\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 8 Batch 600] Training: accuracy=0.980573\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 8 Batch 600] Training: accuracy=0.980521\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 8 Batch 700] Training: accuracy=0.980290\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 8 Batch 700] Training: accuracy=0.978884\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 8 Batch 700] Training: accuracy=0.980402\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 8 Batch 700] Training: accuracy=0.980960\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.030002757906913757,Timestamp=1606869837.1507738,IterationNumber=9500)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 8 Batch 800] Training: accuracy=0.980723\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 8 Batch 800] Training: accuracy=0.979141\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 8 Batch 800] Training: accuracy=0.980820\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 8 Batch 800] Training: accuracy=0.981230\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 8 Batch 900] Training: accuracy=0.980799\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 8 Batch 900] Training: accuracy=0.981215\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 8 Batch 900] Training: accuracy=0.980677\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 8 Batch 900] Training: accuracy=0.979375\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[8]#011Speed=50464.86 samples/s#011Time cost=4.753248\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[8]#011Train: accuracy=0.981107#011Validation: accuracy=0.971655\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 9 Batch 100] Training: accuracy=0.982031\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 9 Batch 100] Training: accuracy=0.984688\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 9 Batch 100] Training: accuracy=0.982344\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 9 Batch 100] Training: accuracy=0.980156\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.011276405304670334,Timestamp=1606869839.1872048,IterationNumber=10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 9 Batch 200] Training: accuracy=0.982578\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 9 Batch 200] Training: accuracy=0.983359\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 9 Batch 200] Training: accuracy=0.982891\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 9 Batch 200] Training: accuracy=0.983750\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 9 Batch 300] Training: accuracy=0.982917\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 9 Batch 300] Training: accuracy=0.982135\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 9 Batch 300] Training: accuracy=0.982396\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 9 Batch 300] Training: accuracy=0.981979\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 9 Batch 400] Training: accuracy=0.982031\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 9 Batch 400] Training: accuracy=0.982812\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 9 Batch 400] Training: accuracy=0.982812\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 9 Batch 400] Training: accuracy=0.981914\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 9 Batch 500] Training: accuracy=0.983031\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 9 Batch 500] Training: accuracy=0.981563\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 9 Batch 500] Training: accuracy=0.982625\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 9 Batch 500] Training: accuracy=0.982594\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 9 Batch 600] Training: accuracy=0.981406\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 9 Batch 600] Training: accuracy=0.981953\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 9 Batch 600] Training: accuracy=0.982214\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 9 Batch 600] Training: accuracy=0.982344\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.026666490361094475,Timestamp=1606869841.7255416,IterationNumber=10500)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 9 Batch 700] Training: accuracy=0.981272\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 9 Batch 700] Training: accuracy=0.981629\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 9 Batch 700] Training: accuracy=0.981897\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 9 Batch 700] Training: accuracy=0.981741\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 9 Batch 800] Training: accuracy=0.981895\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 9 Batch 800] Training: accuracy=0.981465\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 9 Batch 800] Training: accuracy=0.981250\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 9 Batch 800] Training: accuracy=0.981641\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:[Epoch 9 Batch 900] Training: accuracy=0.981736\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:root:[Epoch 9 Batch 900] Training: accuracy=0.981476\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:root:[Epoch 9 Batch 900] Training: accuracy=0.981476\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:root:[Epoch 9 Batch 900] Training: accuracy=0.981198\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[9]#011Speed=50471.34 samples/s#011Time cost=4.757710\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Epoch[9]#011Train: accuracy=0.981676#011Validation: accuracy=0.970052\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Global elpased time on training:51.2979257106781\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:root:Device info: gpu4\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=5.9209747314453125,Timestamp=1606869792.4047594,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:metrics_file_path=/opt/ml/output/metrics/sagemaker/25.json\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.4172540307044983,Timestamp=1606869795.2814715,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.09914116561412811,Timestamp=1606869799.9023943,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.12904945015907288,Timestamp=1606869802.4401522,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.1659032255411148,Timestamp=1606869804.492082,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.25955843925476074,Timestamp=1606869807.030491,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.036897413432598114,Timestamp=1606869809.0813901,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.06463821232318878,Timestamp=1606869811.6199396,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.08202961087226868,Timestamp=1606869813.6917005,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.1749441772699356,Timestamp=1606869816.230068,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.0836627334356308,Timestamp=1606869818.2706923,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.21718421578407288,Timestamp=1606869820.8092234,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.008884118869900703,Timestamp=1606869825.4033036,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.11138996481895447,Timestamp=1606869830.033446,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.04270746558904648,Timestamp=1606869832.5715497,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.12070632725954056,Timestamp=1606869834.6125455,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.00606475630775094,Timestamp=1606869837.1503673,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.04750918596982956,Timestamp=1606869839.1865597,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.037565674632787704,Timestamp=1606869841.7249975,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=4.92369270324707,Timestamp=1606869792.404756,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:metrics_file_path=/opt/ml/output/metrics/sagemaker/26.json\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.26032009720802307,Timestamp=1606869795.2813337,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.1793992519378662,Timestamp=1606869799.902357,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.12945756316184998,Timestamp=1606869802.4402008,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.0308847576379776,Timestamp=1606869804.4920523,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.2232319563627243,Timestamp=1606869807.0304456,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.08320706337690353,Timestamp=1606869809.0812447,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.08563351631164551,Timestamp=1606869811.6200135,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.12491513788700104,Timestamp=1606869813.6916618,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.06647229194641113,Timestamp=1606869816.230118,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.05475398898124695,Timestamp=1606869818.2707028,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.06151001527905464,Timestamp=1606869820.8092608,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.07348994165658951,Timestamp=1606869825.4033191,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.03548623248934746,Timestamp=1606869830.0335119,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.06598573178052902,Timestamp=1606869832.5716524,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.026191899552941322,Timestamp=1606869834.6126268,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.013441956602036953,Timestamp=1606869837.1503007,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.03047267720103264,Timestamp=1606869839.1866896,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.02274448797106743,Timestamp=1606869841.725003,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=5.539459228515625,Timestamp=1606869792.4047604,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:metrics_file_path=/opt/ml/output/metrics/sagemaker/24.json\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.15785162150859833,Timestamp=1606869795.2813487,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.19140946865081787,Timestamp=1606869799.9024568,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.15644261240959167,Timestamp=1606869802.4400861,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.11943864822387695,Timestamp=1606869804.491838,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.08878356218338013,Timestamp=1606869807.030351,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.023443415760993958,Timestamp=1606869809.0813026,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.0986306369304657,Timestamp=1606869811.620024,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.08832108974456787,Timestamp=1606869813.6915984,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.006826408673077822,Timestamp=1606869816.230144,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.12552586197853088,Timestamp=1606869818.2706923,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.09142188727855682,Timestamp=1606869820.8092997,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.03055003099143505,Timestamp=1606869825.4033172,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.07983487099409103,Timestamp=1606869830.0334013,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.0199818704277277,Timestamp=1606869832.5714965,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.02433696575462818,Timestamp=1606869834.6126235,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.025723254308104515,Timestamp=1606869837.1503434,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.09960387647151947,Timestamp=1606869839.1865597,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:DEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.009593922644853592,Timestamp=1606869841.7250285,IterationNumber=10936)\u001b[0m\n",
      "\u001b[34m2020-12-02 00:44:04,372 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-12-02 00:44:16 Uploading - Uploading generated training model\n",
      "2020-12-02 00:44:16 Completed - Training job completed\n",
      "Training seconds: 164\n",
      "Billable seconds: 164\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs=channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "### Create an inference endpoint\n",
    "\n",
    "After training, we use the MXNetModel class to build and deploy an MXNetPredictor. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "This allows us to perform inference on json encoded multi-dimensional arrays.\n",
    "\n",
    "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances. Here we will deploy the model to a single `ml.m4.xlarge` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-688520471316/sagemaker/DEMO-mxnet-mnist-horovod/mxnet-training-2020-12-02-00-38-56-686/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(estimator.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ae3990be69a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mserializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJSONSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdeserializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJSONDeserializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         )\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   3191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2709\u001b[0m         )\n\u001b[1;32m   2710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2712\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   2968\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDescribeEndpoint\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2969\u001b[0m         \"\"\"\n\u001b[0;32m-> 2970\u001b[0;31m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_deploy_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2971\u001b[0m         \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EndpointStatus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_wait_until\u001b[0;34m(callable_fn, poll)\u001b[0m\n\u001b[1;32m   3866\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3867\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3868\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3869\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3870\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n",
    "from sagemaker.mxnet import MXNetModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "\n",
    "model = MXNetModel(\n",
    "    entry_point='inference.py',\n",
    "    source_dir='code',\n",
    "    role=role,\n",
    "    model_data=estimator.model_data,\n",
    "    framework_version='1.7.0',\n",
    "    py_version='py3'\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate \n",
    "We can now use this predictor to classify hand-written digits. We will download the MNIST test data from a public S3 bucket and use it to evaluate our trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Donwload MNIST test set from a public bucket\n",
    "with open('code/config.json', 'rb') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "fname = 't10k-images-idx3-ubyte.gz'\n",
    "bucket = CONFIG['public_bucket']\n",
    "key = 'datasets/image/MNIST/' + fname\n",
    "target = os.path.join('/tmp', fname)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "if not os.path.exists(target):\n",
    "    s3.download_file(bucket, key, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 16 test images\n",
    "\n",
    "with gzip.open(target, 'rb') as f:\n",
    "    images = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28, 28)\n",
    "\n",
    "\n",
    "# randomly sample 16 images to inspect\n",
    "mask = random.sample(range(images.shape[0]), 16)\n",
    "samples = images[mask]\n",
    "\n",
    "# plot the images \n",
    "fig, axs = plt.subplots(nrows=1, ncols=16, figsize=(16, 1))\n",
    "\n",
    "for i, splt in enumerate(axs):\n",
    "    splt.imshow(samples[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input arrays need to be normalized and json serialized, it also needs a channel dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, axis):\n",
    "    eps = np.finfo(float).eps\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    # avoid division by zero\n",
    "    std = np.std(x, axis=axis, keepdims=True) + eps\n",
    "    return (x - mean) / std\n",
    "\n",
    "samples = normalize(samples.astype(np.float32), axis=(1, 2)) # mean 0; std 1\n",
    "samples = np.expand_dims(samples, axis=1)\n",
    "\n",
    "data = {\n",
    "    'inputs': samples.tolist()\n",
    "}\n",
    "\n",
    "\n",
    "res = predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions: \", *map(int, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Clean up\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
