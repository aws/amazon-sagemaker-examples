{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "558bb476-6467-4fb4-a403-ab013f235d07",
   "metadata": {},
   "source": [
    "# Benchmarking AWS Graviton Performance on SageMaker with PyTorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon SageMaker real-time endpoints allow you to host ML applications at scale. In this notebook, we provide a design pattern for benchmarking AWS Graviton instance performance so you can choose the right deployment configuration for your application.\n",
    "\n",
    "## Environment Setup\n",
    "This notebook assumes you are running on AWS SageMaker today and have access to an S3 bucket from your SageMaker environment. If you are not and would like to get started, take a look at the getting started documentation [here.](https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html)\n",
    "In the next steps, you import standard methods and libraries as well as set variables that will be used in this notebook. The get_execution_role function retrieves the AWS Identity and Access Management (IAM) role you created at the time of creating your notebook instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0851f-40a3-4e8c-8153-2f9acf217e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install latest botocore\n",
    "!pip install --upgrade pip awscli botocore boto3 sagemaker torch transformers --quiet\n",
    "!mkdir -p ./models\n",
    "!mkdir -p ./example-payloads\n",
    "!mkdir -p ./tarballs\n",
    "\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.model import Model\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sagemaker_session = Session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9fe18-5f7c-45b9-abc1-d26357401c4b",
   "metadata": {},
   "source": [
    "## Preparing Model For Benchmarking\n",
    "Amazon SageMaker runs the Inference Recommender (IR) utility to automate performance benchmarking across different instances. This service can be used to get the real-time inference endpoint that delivers the best performance at the lowest cost for a given ML model. In order to benchmark a model using AWS SageMaker Inference Recommender we will need a model and an example payload to test the model. SageMaker expects all models and example payloads to be stored in S3. For this example I will be using the [twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) model downloaded from Hugging Face. We will be using this to perform sentiment analysis on an example payload. The following code block downloads the selected model and then creates a tarball of the model and then uploads that model to S3.\n",
    "\n",
    "### Download Model Using HuggingFace Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2da0b-fe85-4b66-91c2-215193a6bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "saved_model_path = \"./models/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "pipe = pipeline(model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "pipe.save_pretrained(saved_model_path)\n",
    "\n",
    "!mkdir {saved_model_path}/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9997c1ee-21ea-4237-898d-33683025e5bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Inference Script For Sagemaker Inference Recommender Job\n",
    "We need a script that SageMaker will call to execute the inference. This code gets packaged along with the model and is a pre-requisite for running an inference recommender job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d253e58-746e-4a6b-8858-0160f8ab3598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {saved_model_path}/code/inference.py\n",
    "\n",
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "REQUEST_CONTENT_TYPE = \"application/x-text\"\n",
    "STR_DECODE_CODE = \"utf-8\"\n",
    "RESULT_CLASS = \"sentiment\"\n",
    "RESULT_SCORE = \"score\"\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    sentiment_analysis = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=model_dir,\n",
    "        tokenizer=model_dir,\n",
    "        return_all_scores=True\n",
    "    )\n",
    "    return sentiment_analysis\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == REQUEST_CONTENT_TYPE:\n",
    "        input_data = request_body.decode(STR_DECODE_CODE)\n",
    "        return input_data\n",
    "    raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(request_content_type or \"unknown\"))\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    return model(input_data)\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    class_label = None\n",
    "    score = -1\n",
    "    for _pred in prediction[0]:\n",
    "        if _pred[\"score\"] > score:\n",
    "            score = _pred[\"score\"]\n",
    "            class_label = _pred[\"label\"]\n",
    "    return json.dumps({RESULT_CLASS: class_label, RESULT_SCORE: score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f709ce-1b52-42a8-a35b-0b89cb3a0b8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {saved_model_path}/code/requirements.txt\n",
    "transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781aeac1-28ba-4f04-bd33-40fc151fb091",
   "metadata": {},
   "source": [
    "### Create Example Payload\n",
    "We need an example payload to test our model with. We'll do this by creating a json file with the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf70c7-258c-4727-8a8c-342446c03324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text to run the sentiment analysis against\n",
    "# model_payload_path is the local path where you want to store the payload txt file\n",
    "model_payload = \"The sky is awfully cloudy today and I am quite tired of this winter weather.\"\n",
    "model_payload_path = \"./example-payloads/sentiment-analysis-payload.txt\"\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(model_payload_path, \"w\") as outfile:\n",
    "    outfile.write(model_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ebf51-1c60-4210-81e1-0697757a6279",
   "metadata": {},
   "source": [
    "### Create Model and Payload Tarballs\n",
    "AWS SageMaker requires models to be in a .tar.gz format containing the model file, and inference code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8732b13-8a7d-46ac-b4ad-82f761d3148b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tarball_path = \"./tarballs/twitter-roberta-base-sentiment-latest.tar.gz\"\n",
    "model_payload_tarball_path = \"./tarballs/twitter-roberta-base-sentiment-payload.tar.gz\"\n",
    "\n",
    "!tar -cvpzf {model_tarball_path} -C {saved_model_path} .\n",
    "!tar -cvpzf {model_payload_tarball_path} -C ./example-payloads ./sentiment-analysis-payload.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad2b3a-0189-4c65-b2d6-089c3fc75a05",
   "metadata": {},
   "source": [
    "### Upload Model and Payload to S3\n",
    "Now that our tarballs are ready for upload, lets go ahead and upload them to S3 using the AWS SageMaker Python SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41cf59-f165-4580-b220-aba930cf4961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model package tarball (model artifact + inference code)\n",
    "model_url = sagemaker_session.upload_data(path=model_tarball_path, key_prefix=\"model\")\n",
    "sample_payload_url = sagemaker_session.upload_data(path=model_payload_tarball_path, key_prefix=\"payload\")\n",
    "print(\"model uploaded to: {} and the sample payload to {}\".format(model_url, sample_payload_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccf766-142d-4c0d-8182-efc66955ee04",
   "metadata": {},
   "source": [
    "### Setup Job Details\n",
    "We're almost ready to create our Inference Recommender job. We just need to specify a few more details to let Inference Recommender know what framework and DLC we're running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d88af7-c817-4df5-bec3-2a30fc0066eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"pytorch-bert-sa\"\n",
    "ml_domain = \"NATURAL_LANGUAGE_PROCESSING\"\n",
    "ml_task = \"FILL_MASK\"\n",
    "framework = \"PYTORCH\"\n",
    "supported_content_types = [\"application/x-text\"]\n",
    "supported_response_types = [\"application/json\"]\n",
    "framework_version = \"2.0.0\"\n",
    "container_url = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-graviton:2.0.0-cpu-py310-ubuntu20.04-sagemaker\"\n",
    "benchmark_instance_types = [\"ml.c7g.4xlarge\"]\n",
    "\n",
    "# For the best throughput, we recommend setting \n",
    "# SAGEMAKER_MODEL_SERVER_WORKERS to the number of vCPUs the instance being evaluated has\n",
    "# and to not to over subscribe the threads, we recommend setting \n",
    "# OMP_NUM_THREADS to 1 so that each model server workers gets 1 thread.\n",
    "model_container_environment_variables = {\n",
    "    'OMP_NUM_THREADS': '1',\n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '16',\n",
    "    'SAGEMAKER_NGINX_PROXY_READ_TIMEOUT_SECONDS': '600',\n",
    "    'DNNL_DEFAULT_FPMATH_MODE': 'BF16',\n",
    "    'DNNL_VERBOSE': '1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d0ff5-8336-4176-b3bd-a9490c0ee120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_and_benchmark_model(model_name, container_url, model_url, execution_role, sample_payload_url, model_container_environment_variables, supported_content_types, supported_response_types, benchmark_instance_types, framework, framework_version, ml_domain, ml_task, sagemaker_session):\n",
    "    model_package_name = model_name + str(round(time.time()))\n",
    "    job_name = model_name + \"-ir-job-\" + str(round(time.time()))\n",
    "\n",
    "    benchmark_model = Model(\n",
    "        image_uri=container_url,\n",
    "        model_data=model_url,\n",
    "        role=execution_role,\n",
    "        env=model_container_environment_variables,\n",
    "        name=model_package_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    benchmark_model.right_size(sample_payload_url, supported_content_types,\n",
    "                               benchmark_instance_types, job_name, framework)\n",
    "    return job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bf6ff-21fe-49d5-aaec-0c3fcf45e57d",
   "metadata": {},
   "source": [
    "### Launch Inference Recommender Job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67e0e7-13f9-46fa-809c-cce9188425d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_name = create_and_benchmark_model(model_name, container_url, model_url, role, sample_payload_url, model_container_environment_variables, supported_content_types,\n",
    "                           supported_response_types, benchmark_instance_types, framework, framework_version, ml_domain, ml_task, sagemaker_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c2a47-675c-4dee-b719-f9642cc8085c",
   "metadata": {},
   "source": [
    "### Get Inference Recommender Results\n",
    "The next bit of code will allow you to pull the relevant cost metrics from the results of the SageMaker Inference Recommender job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed7e4a-7841-4b37-9213-cb1c2cbe32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_ir_job_results(job_name, instance_type):\n",
    "    response=sm_client.describe_inference_recommendations_job(JobName=job_name)\n",
    "    inference_recommendations =response['InferenceRecommendations'][0]['Metrics']\n",
    "    initial_instance_count = response['InferenceRecommendations'][0]['EndpointConfiguration']['InitialInstanceCount']\n",
    "    cost_per_hour = inference_recommendations['CostPerHour']\n",
    "    cost_per_inference = inference_recommendations['CostPerInference']\n",
    "    cost_per_million_inferences = cost_per_inference * 1000000\n",
    "    \n",
    "    data_frame_data = {\n",
    "        'InstanceType' : [instance_type],\n",
    "        'CostPerInference' : [cost_per_inference],\n",
    "        'CostPerHour' : [cost_per_hour],\n",
    "        'CostPerMillionInferences' : [cost_per_million_inferences]\n",
    "    }\n",
    "    \n",
    "    pd.set_option(\"max_colwidth\", 400)\n",
    "    \n",
    "    data_frame = pd.DataFrame(data_frame_data)\n",
    "    data_frame = data_frame.reindex(columns=['InstanceType', 'CostPerInference', 'CostPerHour', 'CostPerMillionInferences'])\n",
    "\n",
    "    \n",
    "    print(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f8808-ff45-464a-a0ab-edcbe9c317a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_ir_job_results(job_name, benchmark_instance_types[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19729b5-f838-4249-9269-20d672cd0c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18737d6e-fa86-4214-86a3-3e7bc5263127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ca4dc-937e-4f81-afa6-75c3ba780fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8 (main, Nov 10 2021, 06:03:50) \n[Clang 12.0.0 (clang-1200.0.32.29)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
