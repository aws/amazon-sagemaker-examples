{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef06ca1-2057-4f38-afd8-90fbb479fa02",
   "metadata": {},
   "source": [
    "# Deploy open-source Large Language Models on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d3f23-3262-42ea-bd7a-76f109266d00",
   "metadata": {},
   "source": [
    "In this notebook, we will show you how to deploy the open-source LLMs from HuggingFace on Amazon SageMaker. The notebook contains five sections:\n",
    "- Section 1: Deploy Falcon model and embedding model to Amazon SageMaker\n",
    "- Section 2: Use RAG based approach with [LangChain](https://python.langchain.com/en/latest/index.html) and SageMaker endpoints to build a simplified question and answering application.\n",
    "- Section 3: (Optional) Run SageMaker Inference Recommender job to determine the cost and performance of the LLM\n",
    "\n",
    "***\n",
    "This notebooks is designed to run on `Python 3 Data Science 3.0` kernel in Amazon SageMaker Studio\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b730c-dc87-45e2-bc41-482b05b8c2c0",
   "metadata": {},
   "source": [
    "#### 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy BLOOM to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf395449-857b-4305-94b0-a1379646a743",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 --upgrade --quiet\n",
    "!pip install ipywidgets==7.0.0 langchain==0.0.148 faiss-cpu==1.7.4 unstructured==0.9.3 --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5dd15-9d1b-4020-8b06-159530972ca6",
   "metadata": {},
   "source": [
    "## Secton 1: Deploy Falcon model and embedding model to Amazon SageMaker\n",
    "In this section, we will deploy the open-source [Falcon 7b instruct model](https://huggingface.co/tiiuae/falcon-7b-instruct) on SageMaker for real-time inference. \n",
    "To deploy [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) to Amazon SageMaker we create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc. We will use a `g5.2xlarge` instance type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11498376-bf21-457e-b5de-aaad86309ee1",
   "metadata": {},
   "source": [
    "This is an example on how to deploy the open-source LLMs to Amazon SageMaker for inference using the [Large Model Inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html) container from DLC to SageMaker and run inference with it. We will deploy the 7B-Instruct [Falcon](https://huggingface.co/tiiuae/falcon-7b-instruct) an open-source Chat LLM trained by TII.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20547cd8-c7c1-4e23-a2f0-da24c5c0ac33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "bucket = sess.default_bucket()  \n",
    "region = boto3.Session().region_name\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {region}\")\n",
    "sm_client = boto3.client('sagemaker')\n",
    "smr = boto3.client('sagemaker-runtime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbbb2f-d974-464b-8251-e940cab1958b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Start preparing model artifacts\n",
    "In LMI contianer, we expect some artifacts to help setting up the model\n",
    "- serving.properties (required): Defines the model server settings\n",
    "- model.py (optional): A python file to define the core inference logic\n",
    "- requirements.txt (optional): Any additional pip wheel need to install\n",
    "\n",
    "In the **serving.properties** files define the the engine to use and model to host. Note the `tensor_parallel_degree` parameter which is also required in this scenario. We will use tensor parallelism to divide the model into multiple parts because no single GPU has enough memory for the entire model. In this case we will use a 'ml.g5.12xlarge' instance which provides 4 GPUs. Be careful not to specify a value larger than the instance provides or your deployment will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d0083-685f-4088-8e68-9cadbe385fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=MPI\n",
    "option.model_id=tiiuae/falcon-7b-instruct\n",
    "option.trust_remote_code=true\n",
    "option.tensor_parallel_degree=1\n",
    "option.paged_attention=false\n",
    "option.rolling_batch=auto\n",
    "#option.s3url = {{s3url}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6458412-cddd-4856-92f0-dc9bba969214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel-7b\n",
    "mv serving.properties mymodel-7b/\n",
    "tar czvf mymodel-7b.tar.gz mymodel-7b/\n",
    "rm -rf mymodel-7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70cadef-d65a-40e1-8a14-ebdfc9b00ece",
   "metadata": {},
   "source": [
    "### Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch\n",
    "\n",
    "#### Getting the container image URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4581d4-d256-4b90-80ad-1c1da3048b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "framework=\"djl-deepspeed\", region=region, version=\"0.23.0\")\n",
    "image_uri "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed661a-2f38-4c7c-a12e-001e5196309f",
   "metadata": {},
   "source": [
    "Then we upload the artifacts on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b36cd-e86d-42fe-ae8c-cb5a7e655bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "code_artifact = sess.upload_data(\"mymodel-7b.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "model = Model(sagemaker_session=sess, image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ef70a-cf7d-404e-8b42-35ef25ebc7e4",
   "metadata": {},
   "source": [
    "#### Create SageMaker endpoint\n",
    "\n",
    "We now can call the deploy function to create the LLM endpoint. You need to specify the instance to use and endpoint names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfdad7-0384-43d0-bf25-7767598c0cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-model-falcon-7b\")\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=900,\n",
    "    wait=False\n",
    ")\n",
    "falcon_model_name = model.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4ac097-7cbc-4722-a934-3f4c661a3d85",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes. During this time, please continue the following section to deploy the embedding model for the RAG solution. We will invoke the deployed endpoint when all the models are deployed successfully.\n",
    "\n",
    "To see more model deployment examples, you can find an example notebook [here at the SageMaker examples gitrepo.](https://github.com/aws/amazon-sagemaker-examples/tree/51cbf4a77c98dc9b74fde6a8c47db0dad40fb910/inference/generativeai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afbb7b-ff6f-4cb2-b5b5-7936f4317557",
   "metadata": {},
   "source": [
    "#### Deploy the all-MiniLM-L6-v2 embedding model on SageMaker\n",
    "\n",
    "In this section, we host the pre-trained [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) Hugging Face sentence transformer model, into SageMaker and generate 384 dimensional vector embeddings for our product catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1399b4-4897-4426-941c-e58d095ada04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "import os\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\" # instance type to use for deployment\n",
    "model_version = \"*\"\n",
    "env= {\n",
    "            \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\", \n",
    "            \"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\",\n",
    "            # This model requires HF_TASK param \n",
    "            # https://huggingface.co/docs/transformers/main/main_classes/pipelines#transformers.pipeline.task\n",
    "            \"HF_TASK\": \"feature-extraction\" \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69439663-dcc4-47c1-bc0a-63d979506645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repository = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_name=repository.split(\"/\")[-1]\n",
    "embed_s3_location=f\"s3://{sess.default_bucket()}/sagemaker/{model_name}/model.tar.gz\"\n",
    "pwd = os.system(\"pwd\") # current path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c8071-7f27-4b9d-a2fc-ef562f89ae64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the model from hf.co/models with git clone.\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/$repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71312b4a-9bd8-416d-9294-4e5f7acd2882",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a model.tar.gz archive in S3 and delete downloaded folder\n",
    "%cd $model_name\n",
    "!tar zcvf model.tar.gz *\n",
    "!aws s3 cp model.tar.gz $embed_s3_location\n",
    "%cd ..\n",
    "%rm -r $model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67dac5b-4e5d-4f7d-ac67-5a3ac1d1958d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id= \"huggingface-textembedding-all-MiniLM-L6-v2\"\n",
    "\n",
    "embed_endpoint_name = model_id + '-' + instance_type.split('.')[-1]\n",
    "\n",
    "# Retrieve the inference container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "model_inference = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=embed_s3_location,\n",
    "    role=role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=model_id,\n",
    "    env=env,\n",
    ")\n",
    "model_predictor_inference = model_inference.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=embed_endpoint_name,\n",
    "    wait=False\n",
    ")\n",
    "print(f\"Model {model_id} has been deployed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b351236-bf12-4949-9c3c-46b6ff09e883",
   "metadata": {},
   "source": [
    "#### Wait until all the endpoints are up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde02055-1a3a-443f-b3c6-ced34078a367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait for the endpoint to be deployed successfully\n",
    "def wait_for_endpoint(endpoint_name=None):\n",
    "    describe_endpoint_response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "    while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "        describe_endpoint_response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "        time.sleep(15)\n",
    "\n",
    "    print(f'endpoint {endpoint_name} is in service now.')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7af04-7a00-4d46-9e0c-39049f46ce92",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ep_name in [endpoint_name, embed_endpoint_name]:\n",
    "    wait_for_endpoint(ep_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41314b01-96b5-4c05-924a-b48c62daaf49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test endpoint outputs\n",
    "Now we can invoke each endpoint to test the endpoint outputs. First, let's check the text-to-text endpoint using Falcon model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda84f44-7304-4db0-a113-5603be78a373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "body = {\"inputs\": \"what is life?\", \"parameters\": {\"max_new_tokens\":400, \"return_full_text\": False}}\n",
    "output = smr.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\")\n",
    "resp = json.loads(output['Body'].read().decode(\"utf8\"))\n",
    "print(resp[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826495d-015b-46f5-bcd5-bc8b3bf94c7a",
   "metadata": {},
   "source": [
    "Then run the follow code to generate embeddings of the input using the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed94501-85d5-43c0-959b-ca93133119b0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_str = {\"text_inputs\": resp[\"generated_text\"]}\n",
    "output = smr.invoke_endpoint(EndpointName=embed_endpoint_name, Body=json.dumps(input_str), ContentType=\"application/json\")\n",
    "embeddings = output['Body'].read().decode(\"utf-8\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25196b-8520-467c-82d4-ff56f7c626e4",
   "metadata": {},
   "source": [
    "## Section2: Use RAG based approach with [LangChain](https://python.langchain.com/en/latest/index.html) and SageMaker endpoints to build a simplified question and answering application.\n",
    "\n",
    "\n",
    "We plan to use document embeddings to fetch the most relevant documents in our document knowledge library and combine them with the prompt that we provide to LLM.\n",
    "\n",
    "To achieve that, we will do following.\n",
    "\n",
    "1. **Generate embedings for each of document in the knowledge library with Huggingface all-MiniLM-L6-v2 embedding model.**\n",
    "2. **Identify top K most relevant documents based on user query.**\n",
    "    - 2.1 **For a query of your interest, generate the embedding of the query using the same embedding model.**\n",
    "    - 2.2 **Search the indexes of top K most relevant documents in the embedding space using in-memory Faiss search.**\n",
    "    - 2.3 **Use the indexes to retrieve the corresponded documents.**\n",
    "3. **Combine the retrieved documents with prompt and question and send them into SageMaker LLM.**\n",
    "\n",
    "\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt -- maximum sequence length of 1024 tokens. \n",
    "\n",
    "---\n",
    "To build a simiplied QA application with LangChain, we need: \n",
    "1. Wrap up our SageMaker endpoints for embedding model and LLM into `langchain.embeddings.SagemakerEndpointEmbeddings` and `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. That requires a small overwritten of `SagemakerEndpointEmbeddings` class to make it compatible with SageMaker embedding mdoel.\n",
    "2. Prepare the dataset to build the knowledge data base. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c101e-1fbe-4c31-a1ef-8b63212fa790",
   "metadata": {},
   "source": [
    "Wrap up our SageMaker endpoints for embedding model into `langchain.embeddings.SagemakerEndpointEmbeddings`. That requires a small overwritten of `SagemakerEndpointEmbeddings` class to make it compatible with SageMaker embedding mdoel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f8cae-807a-408d-a9cf-98b43bbb2542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from typing import Any, Dict, List, Optional\n",
    "import json\n",
    "\n",
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(self, texts: List[str], chunk_size: int = 5) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i : i + _chunk_size])\n",
    "            print\n",
    "            results.extend(response)\n",
    "        return results\n",
    "\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddingsJumpStart(\n",
    "    endpoint_name=embed_endpoint_name,\n",
    "    region_name=region,\n",
    "    content_handler=content_handler,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7a5f2-058b-4620-a37b-95cfca6a1e80",
   "metadata": {},
   "source": [
    "Next, we wrap up our SageMaker endpoints for LLM into `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8461b-6eff-475a-85e3-f0973880a555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.1\n",
    "}\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": {**model_kwargs}})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = output.read()\n",
    "        res = json.loads(response_json)\n",
    "        ans = res['generated_text']\n",
    "        return ans \n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    region_name=region,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c1370-3271-4561-b449-0c90782e91b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b7547-a52d-4f74-9982-0e7501386642",
   "metadata": {},
   "source": [
    "Use langchain to read the `txt` data. There are multiple built-in functions in LangChain to read different format of files such as `csv`, `html`, and `pdf`. For details, see [LangChain document loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27d89d-8d56-413f-b753-1e48e79446e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"./data/\", glob=\"**/*.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4a131-9b09-4141-96c5-6a13751e99ff",
   "metadata": {},
   "source": [
    "We generate embedings for each of document in the knowledge library with Huggingface all-MiniLM-L6-v2 embedding model.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51eb647-9b91-4581-bdfa-12f55ab0f4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4604c98-a75d-4d5c-8672-5774ef7060a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"what is the recommended way to first customize a foundation model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a972160-933c-4353-b0e6-1c4aaa878ae8",
   "metadata": {},
   "source": [
    "Based on the question above, we then **identify top K most relevant documents based on user query, where K = 3 in this setup**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c6fb8-ec23-42d4-9853-d0f3c292f103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = docsearch.similarity_search_with_score(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aec596-f6be-4780-9599-6793a2f26b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6375e8-321b-40a2-b631-0ff456063849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source = []\n",
    "context = []\n",
    "for doc, score in docs:\n",
    "    context.append(doc)\n",
    "    source.append(doc.metadata['source'].split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22f198-7474-4c57-b4d7-0be16d90248c",
   "metadata": {},
   "source": [
    "Finally, we **combine the retrieved documents with prompt and question and send them into SageMaker LLM.** \n",
    "\n",
    "We define a customized prompt as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53822d7-e44c-4076-8735-59a6ce7f7ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.:\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b71b4-a04b-4a66-b5ff-b5e2d132703f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm=sm_llm, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb6db1-3897-410c-9599-5d6f63333a63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = chain({\"input_documents\": context, \"question\": question}, return_only_outputs=True)[\"output_text\"]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927245c3-a8e9-498f-8b94-1edcfc8727e2",
   "metadata": {},
   "source": [
    "### Run the Question and Answering chatbot application\n",
    "\n",
    "Once all the endpoints are deployed successfully, you can open a terminal in SageMaker Studio and use the below command to run the chatbot [Streamlit](https://streamlit.io/) application. Note that you need to install the required python packages that are specified in the “requirements.txt” file. You also need to update the environment variables with the endpoint names deployed in your account accordingly. When you execute the `chatbot-steamlit.py` file, it will automatically update the endpoint names based on the environment variables.\n",
    "\n",
    "```\n",
    "$ pip install -r requirements.txt\n",
    "$ export nlp_ep_name=<the falcon endpoint name deployed in your account>\n",
    "$ export embed_ep_name=<the embedding endpoint name deployed in your account>\n",
    "$ streamlit run chatbot-streamlit.py --server.port 6006 --server.maxUploadSize 6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e7780-c954-4003-b56c-b9dc6d0ea230",
   "metadata": {},
   "source": [
    "To access the Streamlit UI, copy your SageMaker Studio url and replace `lab?` with `proxy/[PORT NUMBER]/`. Because we specified the server port to 6006, so the url should look like:\n",
    "\n",
    "```\n",
    "https://<domain ID>.studio.<region>.sagemaker.aws/jupyter/default/proxy/6006/\n",
    "```\n",
    "\n",
    "Replace the domain ID and region with the correct value in your account to access the UI as below:\n",
    "![streamlitUI](./img/Streamlit_UI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb216f5-1582-4708-869c-d7de017bd7d3",
   "metadata": {},
   "source": [
    "You can find some suggested `prompt` on the left-hand-side sidebar. When you upload the sample files (you can find the sample files in the test folder), the chatbot will automatically provide prompt suggestions based on the input data type.\n",
    "\n",
    "**Congratulations on finishing lab 1 !!!**\n",
    "\n",
    "_____________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717f908-ae2f-4ec4-b637-a5cc309f1bea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Secton 3: (Optional) Understand the model hosting performance using SageMaker Inference Recommender "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5460ca6-ab3c-4da1-bd8e-338c1c89c6a2",
   "metadata": {},
   "source": [
    "[Amazon SageMaker Inference Recommender](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html) is a capability of Amazon SageMaker that reduces the time required to get machine learning (ML) models in production by automating load testing and model tuning across SageMaker ML instances. You can use Inference Recommender to deploy your model to a real-time or serverless inference endpoint that delivers the best performance at the lowest cost. Inference Recommender helps you select the best instance type and configuration (such as instance count, container parameters, and model optimizations) or serverless configuration (such as max concurrency and memory size) for your ML models and workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16877ea-94f0-4bc5-baa7-045315487574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -czvf payload.tar.gz test_file/payload.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03715d81-4d5a-4062-a1c3-aa3f375d231c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_location = f\"s3://{bucket}/sagemaker/InferenceRecommender/{model_name}\"\n",
    "payload_tar_url = sagemaker.s3.S3Uploader.upload(\"payload.tar.gz\", s3_location)\n",
    "print(payload_tar_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ef825-fab6-4239-8fa1-4376c0bdc8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_name = f\"{falcon_model_name}-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "response = sm_client.create_inference_recommendations_job(\n",
    "    JobName=job_name,\n",
    "    JobType='Default',\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'ContainerConfig': {\n",
    "            'Domain': 'NATURAL_LANGUAGE_PROCESSING',\n",
    "            'Task': 'TEXT_GENERATION',\n",
    "            'PayloadConfig': {\n",
    "                'SamplePayloadUrl': payload_tar_url,\n",
    "                'SupportedContentTypes': [\"application/json\"],\n",
    "            },\n",
    "            #specify the instance types you would like to test out\n",
    "            'SupportedInstanceTypes': ['ml.g5.2xlarge'], \n",
    "            'SupportedEndpointType': 'RealTime'\n",
    "        },\n",
    "        'ModelName': falcon_model_name\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd346679-cc89-4f13-bfb8-45d2aa73bf0f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # uncomment this section to wait for the inference job to finish\n",
    "# describe_IR_job_response = sm_client.describe_inference_recommendations_job(JobName=job_name)\n",
    "\n",
    "# while describe_IR_job_response[\"Status\"] in [\"IN_PROGRESS\", \"PENDING\"]:\n",
    "#     describe_IR_job_response = sm_client.describe_inference_recommendations_job(JobName=job_name)\n",
    "#     print(describe_IR_job_response[\"Status\"])\n",
    "#     time.sleep(15)\n",
    "    \n",
    "# print(f'Inference Recommender job {job_name} has finished with status {describe_IR_job_response[\"Status\"]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1984596-7f6c-4257-84a8-edceca547f61",
   "metadata": {},
   "source": [
    "Now, let's use the inference recommender job results to calculate the approximate invocation cost for the LLM endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746058b-3eea-42d0-a632-76fa124abbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_IR_job_response = sm_client.describe_inference_recommendations_job(JobName=job_name)\n",
    "describe_IR_job_response['InferenceRecommendations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0bcdbe-ed9b-4fc2-a94a-c26c2c3fbd0f",
   "metadata": {},
   "source": [
    "The inference recomender job reports the below metrics: \n",
    "- 'ModelLatency'\n",
    "- 'CostPerInference'\n",
    "- 'CostPerHour'\n",
    "- 'MaxInvocations' per minute\n",
    "\n",
    "and more.\n",
    "\n",
    "Note that the sample json input file consists of 6,200 characters, which is around 1550 tokens per invocation (1 token is approximately 4 characters). To calculate the approximate cost per 1K tokens, you can do the inference many times (with average payload size) and get the best token/s you get through the experiment (different instance types can result in different throughput, model latency, and cost). Then we will calculate the per token per second invocation price and multiply by 1,000. You can also use per invocation cost divide by the tokens per invocation and multiply by 1,000. The calculated price should be similar. SageMaker also supports auto-scaling to scale your endpoint out/in to save cost based on the invocation traffic pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c07c2-5375-4ffc-9ba8-685e4eac9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = describe_IR_job_response['InferenceRecommendations'][0]['Metrics']\n",
    "token_per_sec = round(metrics['MaxInvocations']*1550/60, 2)\n",
    "cost_per_sec = round(metrics['CostPerHour']/3600, 5)\n",
    "cost_per_1k_token = round(cost_per_sec/token_per_sec * 1000, 5)\n",
    "print(\"According to the Inference recommender job, the corresponding metrices are as below: /n\")\n",
    "print(f\"Max tokens per second is about {token_per_sec}\")\n",
    "print(f\"Cost per second is about ${cost_per_sec}\")\n",
    "print(f\"Cost per 1k tokens is about ${cost_per_1k_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f9f81c-f762-43a7-9c46-cf26f2455b48",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceced14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "# client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447cf5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
