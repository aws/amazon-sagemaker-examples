{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection for Tabular Data\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to select important features and prune unimportant ones prior to training our machine learning model. This is an important step that yields better prediction performance. \n",
    "\n",
    "#### Prerequisite\n",
    "This notebook is a sequel to the [01_preprocessing_tabular_data.ipynb](01_preprocessing_tabular_data.ipynb) notebook. Before running this notebook, run [01_preprocessing_tabular_data.ipynb](01_preprocessing_tabular_data.ipynb) to preprocess the data used in this notebook. \n",
    "\n",
    "#### Notes\n",
    "In this notebook, we use the sklearn framework for data partitionining and `storemagic` to share dataframes in [03_training_model_on_tabular_data.ipynb](03_training_model_on_tabular_data.ipynb). While we load data into memory here we do note that is it possible to skip this and load your partitioned data directly to an S3 bucket.\n",
    "\n",
    "#### Tabular Data Sets\n",
    "* [california house data](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)\n",
    "* [diabetes data ](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html)\n",
    "\n",
    "\n",
    "#### Library Dependencies:\n",
    "* sagemaker >= 2.0.0\n",
    "* numpy \n",
    "* pandas\n",
    "* plotly\n",
    "* sklearn \n",
    "* matplotlib \n",
    "* seaborn\n",
    "* xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import ast\n",
    "from matplotlib import pyplot\n",
    "\n",
    "## sklearn dependencies\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "import sklearn.model_selection\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "!{sys.executable} -m pip install -qU 'xgboost'\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "## SageMaker dependencies\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "## This instantiates a SageMaker session that we will be operating in. \n",
    "session = sagemaker.Session()\n",
    "\n",
    "## This object represents the IAM role that we are assigned.\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Relevant Variables from preprocessing_tabular_data.ipynb (Required for this notebook)\n",
    "Here we load in our training, test, and validation data sets. We preprocessed this data in the [01_preprocessing_tabular_data.ipynb](01_preprocessing_tabular_data.ipynb) and persisted it using storemagic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant dataframes and variables from preprocessing_tabular_data.ipynb required for this notebook\n",
    "%store -r X_train\n",
    "%store -r X_test\n",
    "%store -r X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Computing Feature Importance Scores to Select Features\n",
    "We show two approaches for computing feature importance scores for each feature. We can rank each feature by their corresponding feature importance score in an effort to prune unimportant features which will yield a better performing model. \n",
    "\n",
    "The first approach, uses XGBoost and the second uses permutation feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a: Ranking features by Feature Importance using XGBoost\n",
    "Here we use gradient boosting to extract importance scores for each feature. The importance scores calculated for each feature inform us how useful the feature was for constructing the boosted decision tree and can be ranked and compared to one another for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_label = make_regression(n_samples=X_train.shape[0], n_features=X_train.shape[1], n_informative=10, random_state=1)\n",
    "xgboost_model = XGBRegressor()\n",
    "xgboost_model.fit(X_data, y_label)\n",
    "\n",
    "feature_importances_xgboost = xgboost_model.feature_importances_\n",
    "for index, importance_score in enumerate(feature_importances_xgboost):\n",
    "    print('Feature: {}, Score: {}'.format(X_train.columns[index], importance_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_plot(feature_importances, X_train):\n",
    "    '''\n",
    "    Create a bar plot of features against their corresponding feature importance score. \n",
    "    '''\n",
    "    x_indices = [_ for _ in range(len(feature_importances))]\n",
    "    plt.figure(figsize = (15, 5))\n",
    "    plt.bar(x_indices, feature_importances, color='blue')\n",
    "    plt.xticks(x_indices, X_train.columns)\n",
    "    plt.xlabel('Feature', fontsize=18)\n",
    "    plt.ylabel('Importance Score', fontsize=18)\n",
    "    plt.title('Feature Importance Scores', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(feature_importances_xgboost, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we rank each feature based on corresponding importance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ranked_feature_importance_list(scores, data):\n",
    "    '''\n",
    "    Prints the features ranked by their corresponding importance score. \n",
    "    '''\n",
    "    lst = list(zip(data.columns, scores))\n",
    "    ranked_lst = sorted(lst, key= lambda t: t[1], reverse=True)\n",
    "    print(pd.DataFrame(ranked_lst, columns=['Feature', 'Importance Score']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ranked_feature_importance_list(feature_importances_xgboost, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b: Ranking features by Permutation Feature Importance using the Scikit-learn k-NN Algorithm\n",
    "This approach is commonly used for selecting features in tabular data. We first randomly shuffle a single feature value and train a model. In this example we use the k-nearest-neighbours algorithm to train our model. The permutation feature importance score is the decrease in models score when this single feature value is shuffled. The decrease in the model score is representative of how dependant the model is on the feature. This technique can be computed many times with altering permutations per feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_label = make_regression(n_samples=X_train.shape[0], n_features=X_train.shape[1], n_informative=10, random_state=1)\n",
    "k_nn_model = KNeighborsRegressor()\n",
    "k_nn_model.fit(X_data, y_label)\n",
    "feature_importances_permutations = permutation_importance(k_nn_model, X_data, y_label, scoring='neg_mean_squared_error').importances_mean\n",
    "\n",
    "for index, importance_score in enumerate(feature_importances_permutations):\n",
    "    print('Feature: {}, Score: {}'.format(X_train.columns[index], importance_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(feature_importances_permutations, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ranked_feature_importance_list(feature_importances_permutations, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prune Unimportant Features\n",
    "Thus far, we have discussed two common approaches for obtaining a ranked list of feature importance scores for each feature. From these lists we can infer unimportant features based on their importance scores and can eliminate them from our training, validation and test sets. For example, if feature A has a higher importance score then feature B's importance score, then this implies that feature A is more important then feature B and vice versa. We mention that both approaches constrain the removal of features to the dataset itself which is independent of the problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting your desired approach, move onto the next cell to prune features that have the importance score less than or equal to a threshold value. Depending on the approach of your choice and the distribution of scores, the `threshold` value may vary.\n",
    "\n",
    "In this example, we select the first approach with XGBoost and set the threshold value to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(lst, data, threshold):\n",
    "    '''\n",
    "    Remove features found in lst from data iff its importance score is below threshold.\n",
    "    '''\n",
    "    features_to_remove = []\n",
    "    for index, pair in enumerate(list(zip(data.columns, lst))):\n",
    "        if pair[1] <= threshold:\n",
    "            features_to_remove.append(pair[0])\n",
    "\n",
    "    if features_to_remove:\n",
    "        data.drop(features_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign `lst` to be `feature_importances_permutations` or `feature_importances_xgboost` if want to use the ranked list from that uses XGBoost or permutation feature importance respectively.\n",
    "\n",
    "We remove all features that are below `threshold` from our training data, `X_train`, validation data, `X_val` and testing data `X_test` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features(lst=feature_importances_xgboost, data=X_train, threshold=threshold)\n",
    "remove_features(lst=feature_importances_xgboost, data=X_val, threshold=threshold)\n",
    "remove_features(lst=feature_importances_xgboost, data=X_test, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Store Variables using `storemagic`\n",
    "After pruning the unimportant features, use `storemagic` to persist all relevant variables so that they can be reused in our next sequel notebook, [03_training_model_on_tabular_data.ipynb](03_training_model_on_tabular_data.ipynb), where we focus on model training. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using storemagic we persist the variables below so we can access them in 03_training_model_on_tabular_data.ipynb\n",
    "%store X_train\n",
    "%store X_test\n",
    "%store X_val"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
