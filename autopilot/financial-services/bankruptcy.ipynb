{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Bankruptcy using SageMaker AutoPilot\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon SageMaker Autopilot is an automated machine learning (commonly referred to as AutoML) solution for tabular datasets. You can use SageMaker Autopilot in different ways: on autopilot (hence the name) or with human guidance, without code through SageMaker Studio, or using the AWS SDKs. This notebook, as a first glimpse, will use the AWS SDKs to simply create and deploy a machine learning model.\n",
    "\n",
    "Predicting corporate bankruptcy is very important for any wholesale or capital market credit business. Predicting bankruptcy is also important for credit risk management.\n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "\n",
    "- - The IAM role ARN used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `boto` regular expression with a the appropriate full IAM role ARN string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install arff package, this package is used to read the bankruptcy data which is in ARFF format\n",
    "!pip install --upgrade pip \n",
    "!pip install --upgrade arff\n",
    "#install s3fs - this package is used by pandas to read file from s3\n",
    "!pip install --upgrade s3fs\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python packages\n",
    "First let's import the packages we need.\n",
    "You also need the `arff` package to load the bankruptcy data as it is in `arff` format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from time import gmtime, sleep, strftime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.metrics import (auc, f1_score, plot_precision_recall_curve,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score, roc_curve)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sagemaker\n",
    "import wget\n",
    "from sagemaker import AutoML, get_execution_role\n",
    "from sagemaker.automl.automl import AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset.\n",
    "\n",
    "You will use data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) the [Polish companies bankruptcy dataset](https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data). It has 64 features and one target attribute. More details are found here: \n",
    "Zieba, M., Tomczak, S. K., & Tomczak, J. M. (2016). Ensemble Boosted Trees with Synthetic Features Generation in Application to Bankruptcy Prediction. Expert Systems with Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "timestamp_suffix = strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "role = get_execution_role()\n",
    "url = \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/tabular/uci_polish_bankruptcy/data.zip\"\n",
    "bankruptcy_file = wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Let's take a quick look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankruptcydata1 = arff.loadarff(\"1year.arff\")\n",
    "bankruptcy_df = pd.DataFrame(bankruptcydata1[0])\n",
    "bankruptcy_df[\"class\"] = bankruptcy_df[\"class\"].map(int)\n",
    "bankruptcy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class 0 = Not Bankrupt (i.e did not file bankruptcy) and class 1 = Bankrupt (filed bankruptcy). As you can see, other than `amount`, other columns are anonymized in the dataset. All column descriptions are available in the html page. The column names were saved in the `bankruptcyfeatures.csv` file. Your goal is to predict which companies will file for bankruptcy next month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data, we want to predict bankruptcy filing for next month\n",
    "\n",
    "### Mapping Features Name to the Data Frame ##\n",
    "You create an attribute to feature name mapping in `bankruptcyfeatures.csv` from the column descriptions in the html page. You will use this file to rename the column names of `bankruptcy_df`. Please note, column names are just for display. **Autopilot does not need column names.**  \n",
    "\n",
    "Note, target attribute **class** is mapped to **bankrupt** to make it more clear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = pd.read_csv(\"bankruptcyfeatures.csv\", header=0)\n",
    "bankruptcy_df.columns = np.array(feature_names[\"economic_factor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check if the dataset is balanced. See if the number of bankruptcies represents roughly half of the dataset.\n",
    "Also check if the dataset has any NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our target is to predict bankrupt column\n",
    "target_variable = \"bankrupt\"\n",
    "print(bankruptcy_df[target_variable].value_counts())\n",
    "# check for null values\n",
    "print(bankruptcy_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of **bankruptcy** records are only around 4% of **not bankrupt**\n",
    "Also, you can see the dataset features have many NaN values. You will let Autopilot handle these NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, you need to split the data into train and test data sets.  The test data will be used to measure the ability of the Autopilot generated model to generalize to previously unseen data. You will use an 80-20 ratio of training versus testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(bankruptcy_df, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training dataset size = {train.shape}\")\n",
    "print(f\"Test dataset size = {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Autopilot\n",
    "\n",
    "Give a job name **automl-bankruptcy**, then create a session with the SageMaker client. You need to have an **s3** bucket to store train/test data and all other artifacts Autopilot will produce. In this notebook, you are using the default **s3** bucket, but you can create your own bucket if you wish. Training and test data is used from the\n",
    "previous steps and uploaded to **s3** bucket under \"train\" and \"test\" respectively. Training_data[target_variable] has the target (bankruptcy 1, Not bankrupt 0). **S3Uri** field in input_data_config tells Autopilot training data location. **TargetAttributeName** indicates target variable for the training job. \n",
    "\n",
    "After uploading the dataset to Amazon S3, you can invoke Autopilot to find the best ML pipeline to train a model on this dataset.\n",
    "\n",
    "The required inputs for invoking an Autopilot job are:\n",
    "\n",
    "    Amazon S3 location for input dataset and for all output artifacts\n",
    "    Name of the column of the dataset you want to predict (y in this case)\n",
    "    An IAM role\n",
    "\n",
    "Currently Autopilot supports only tabular datasets in CSV format. Either all files should have a header row, or the first file of the dataset, when sorted in alphabetical/lexical order, is expected to have a header row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ml_job_name = \"automl-bankruptcy\"\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "session = sagemaker.Session()\n",
    "\n",
    "prefix = f\"sagemaker/{auto_ml_job_name}\"\n",
    "bucket = session.default_bucket()\n",
    "training_data = train\n",
    "X_test = test.drop(columns=[target_variable])\n",
    "y_test = test[target_variable]\n",
    "\n",
    "test_data = X_test\n",
    "\n",
    "train_file = \"train_data.csv\"\n",
    "training_data.to_csv(train_file, index=False, header=True)\n",
    "train_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"/train\")\n",
    "print(f\"Train data uploaded to: {train_data_s3_path}\")\n",
    "\n",
    "test_file = \"test_data.csv\"\n",
    "test_data.to_csv(test_file, index=False, header=False)\n",
    "test_data_s3_path = session.upload_data(path=test_file, key_prefix=prefix + \"/test\")\n",
    "print(f\"Test data uploaded to: {test_data_s3_path}\")\n",
    "input_data_config = [\n",
    "    {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": f\"s3://{bucket}/{prefix}/train\",\n",
    "            }\n",
    "        },\n",
    "        \"TargetAttributeName\": target_variable,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_suffix = strftime(\"%d-%H-%M-%S\", gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Now, you need to create the Autopilot job. See the following for an example on creating an Autopilot job. You set the maximum candidate models (attribute `max_candidates`) with different parameters to 250. You also set `ProblemType='BinaryClassification'`. Please note you do not need to set `ProblemType` and `MetricName`. If you do not set these two fields, Autopilot will automatically determine the type of supervised learning problem by analyzing the data (for a binary classification problem the default metric is F1).  We set `MetricName` (parameter `job_objective`) to AUC or F1 (value of `eval_obj` when the function is called). More info: [options for the job configuration](https://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-auto-ml-job.html).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: define the functions here and use them later from a calling function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_automl_object(eval_obj, base_job_name):\n",
    "\n",
    "    target_attribute_name = target_variable\n",
    "    role = get_execution_role()\n",
    "    automl = AutoML(\n",
    "        role=role,\n",
    "        target_attribute_name=target_attribute_name,\n",
    "        base_job_name=base_job_name,\n",
    "        sagemaker_session=session,\n",
    "        problem_type=\"BinaryClassification\",\n",
    "        job_objective={\"MetricName\": eval_obj},\n",
    "        max_candidates=250,\n",
    "    )\n",
    "    return automl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the AutoML object is created, call the fit() function to train the AutoML object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_fit(automl, base_job_name):\n",
    "    automl.fit(train_data_s3_path, job_name=base_job_name, wait=False, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you create the Autopilot job, monitor the response of the Autopilot job that was created above. Check the job status every 30 seconds, and once the job status returns ‘Completed’, exit the loop.\n",
    "Before completing the job, loop will print **InProgress**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status(automl):\n",
    "    describe_response = automl.describe_auto_ml_job()\n",
    "    print(describe_response)\n",
    "    job_run_status = describe_response[\"AutoMLJobStatus\"]\n",
    "\n",
    "    while job_run_status not in (\"Failed\", \"Completed\", \"Stopped\"):\n",
    "        describe_response = automl.describe_auto_ml_job()\n",
    "        job_run_status = describe_response[\"AutoMLJobStatus\"]\n",
    "        print(job_run_status)\n",
    "        sleep(30)\n",
    "    print(\"completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best candidate and check the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_candidate(automl):\n",
    "    best_candidate = automl.describe_auto_ml_job()[\"BestCandidate\"]\n",
    "    best_candidate_name = best_candidate[\"CandidateName\"]\n",
    "    print(best_candidate)\n",
    "    print(\"\\n\")\n",
    "    print(f\"CandidateName: {best_candidate_name}\")\n",
    "    print( f\"FinalAutoMLJobObjectiveMetricName: {best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName']}\")\n",
    "    print(f\"FinalAutoMLJobObjectiveMetricValue: {best_candidate['FinalAutoMLJobObjectiveMetric']['Value']}\")\n",
    "    return best_candidate_name, best_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model from the best candidate. In addition to predicted label, you want probability of the prediction. This probability will be used later to plot AUC and Precision/Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(automl, best_candidate_name, best_candidate, timestamp_suffix):\n",
    "    model_name = f\"automl-bankruptcy-model-{timestamp_suffix}\"\n",
    "    inference_response_keys = [\"predicted_label\", \"probability\"]\n",
    "    model = automl.create_model(\n",
    "        name=best_candidate_name,\n",
    "        candidate=best_candidate,\n",
    "        inference_response_keys=inference_response_keys,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also select multiple candidates (example by Objective, in this case AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is created, run a Transform job to get inference (i.e Prediction about the default) from the test dataset and save in S3. It is worth noting that when you deploy the model as an endpoint or create a Transformer, SageMaker handles the deployment of the feature engineering pipeline and the ML algorithm, so end users can send the data in its raw format for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer(model, best_candidate, eval_obj):\n",
    "    s3_transform_output_path = f\"s3://{bucket}/{prefix}/inference-results/\"\n",
    "    output_path = f\"{s3_transform_output_path}{best_candidate['CandidateName']}/\"\n",
    "    transformer = model.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        assemble_with=\"Line\",\n",
    "        output_path=output_path,\n",
    "    )\n",
    "    transformer.transform(\n",
    "        data=test_data_s3_path, split_type=\"Line\", content_type=\"text/csv\", wait=False\n",
    "    )\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we read the inference/predicted data into Pandas dataframe\n",
    "\n",
    "This function will read the file from s3 (generated from create_transformer), create a Data Frame for label(predicted 0/1) and probability(probability of the prediction 0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:].strip(\"/\")\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3_resource=boto3.resource(\"s3\")\n",
    "    obj = None \n",
    "    loop = True\n",
    "    while (loop):\n",
    "        try:\n",
    "            obj = s3_resource.Object(bucket_name, f\"{prefix}/{file_name}\")\n",
    "            pred_body  = obj.get()[\"Body\"].read().decode(\"utf-8\")    \n",
    "            print (\"predict file is avilable s3\")    \n",
    "            loop = False\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            print(\"prediction file still not avilable in s3 sleeping for 2 minutes\")\n",
    "            time.sleep(120)\n",
    "    return pred_body\n",
    "\n",
    "def return_pred_df(transformer):\n",
    "    print(\"***predict output path ***\")\n",
    "    print(transformer.output_path, \"{}.out\".format(test_file))\n",
    "    pred_csv = get_csv_from_s3(transformer.output_path, \"{}.out\".format(test_file))\n",
    "    data = pd.read_csv(io.StringIO(pred_csv), header=None)\n",
    "    data.columns = [\"label\", \"proba\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download Candidate Definition notebook from the following s3 location.\n",
    "We can download data exploration notebook to see details of Autopilot data analysis. This report provides insights about the dataset you provided as input to the AutoML job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_notebooks(automl, eval_obj):\n",
    "    print(f\"download CandidateDefinitionNotebookLocation for {eval_obj}\")\n",
    "    print(\n",
    "        automl.describe_auto_ml_job()[\"AutoMLJobArtifacts\"][\n",
    "            \"CandidateDefinitionNotebookLocation\"\n",
    "        ]\n",
    "    )\n",
    "    print(f\"download DataExplorationNotebookLocation for {eval_obj}\" )\n",
    "    print(\n",
    "        automl.describe_auto_ml_job()[\"AutoMLJobArtifacts\"][\n",
    "            \"DataExplorationNotebookLocation\"\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper function run_automl_process is called with objective AUC and F1. This wrapper function calls multiple functions to creare AutoML object, run training process, create model from best trained job and finally return predicted data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_automl_process(eval_obj):\n",
    "    timestamp_suffix = strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "    base_job_name = f\"{auto_ml_job_name}-{eval_obj}{timestamp_suffix}\"\n",
    "    print(base_job_name)\n",
    "    automl = create_automl_object(eval_obj, base_job_name)\n",
    "    automl_fit(automl, base_job_name)\n",
    "    check_status(automl)\n",
    "    best_candidate_name, best_candidate = get_best_candidate(automl)\n",
    "    model = create_model(automl, best_candidate_name, best_candidate, timestamp_suffix)\n",
    "    transformer = create_transformer(model, best_candidate, eval_obj)\n",
    "    pred_df = return_pred_df(transformer)\n",
    "    pred_df.to_csv(f'data_{eval_obj}_bankruptcy.csv', index=False)\n",
    "    download_notebooks(automl, eval_obj)\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run for the auto pilot job. We call the wrapper function run_automl_process with objective AUC and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*********running with eval objective AUC***********\")\n",
    "data_auc = run_automl_process(\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*********running with eval objective F1***********\")\n",
    "data_f1 = run_automl_process(\"F1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is created, we run a Transform job to get inference (i.e Prediction about the default) from the test data set and save to S3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot ROC - the Area under the Curve (AUC) for true positive (in this data set Bankrupt) vs false positive (predicted as Bankrupt but not Bankrupt in the ground truth). The higher the prediction quality of the classification model, the more the AUC curve is skewed to the top left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "colors = [\"blue\", \"green\"]\n",
    "model_names = [\"Objective : AUC\", \"Objective : F1\"]\n",
    "models = [data_auc, data_f1]\n",
    "for i in range(len(models)):\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, models[i][\"proba\"])\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, models[i][\"proba\"])\n",
    "    auc_score = metrics.auc(fpr, tpr)\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        label=str(f\"Auto Pilot {auc_score:.3f} {model_names[i]}\"),\n",
    "        color=colors[i],\n",
    "    )\n",
    "\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"ROC Cuve\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The precision-recall curve compares the trade-off between precision and recall, with the best models having a precision-recall curve that is flat initially, dropping steeply as the recall approaches 1. The higher precision + recall, more the curve will be skewed towards upper right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"blue\", \"green\"]\n",
    "model_names = [\"Objective : AUC\", \"Objective : F1\"]\n",
    "models = [data_auc, data_f1]\n",
    "\n",
    "print(\"model \", \"F1 \", \"precision \", \"recall \")\n",
    "for i in range(0, len(models)):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, models[i][\"proba\"])\n",
    "    print(\n",
    "        model_names[i],\n",
    "        f1_score(y_test, np.array(models[i][\"label\"])),\n",
    "        precision_score(y_test, models[i][\"label\"]),\n",
    "        recall_score(y_test, models[i][\"label\"]),\n",
    "    )\n",
    "    plt.plot(recall, precision, color=colors[i], label=model_names[i])\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion <a name=\"Conclusion\"></a>\n",
    "We can see that with very little data science knowledge, we are able to create a moderately accurate prediction for a complex Financial event like Bankruptcy. From the AUC and Precision+Recall plots, we can also see that Autopilot handled highly imbalanced data resonably well. We think the reason for the 62% Recall (rather than achieving higher score) is as follows: the bankruptcy dataset is missing some important features of Bankruptcy filing - short term liquidity, short term funding source etc.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup <a name=\"Cleanup\"></a>\n",
    "The Autopilot job creates many underlying artifacts such as dataset splits, preprocessing scripts, or preprocessed data, etc. This code, when un-commented, deletes them. This operation deletes all the generated models and the auto-generated notebooks as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.resource('s3')\n",
    "# s3_bucket = s3.Bucket(bucket)\n",
    "\n",
    "# s3_bucket.objects.filter(Prefix=prefix).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can delete the models by calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
