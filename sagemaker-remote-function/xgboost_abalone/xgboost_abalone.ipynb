{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Regression with XGBoost\n",
    "\n",
    "The notebook shows how to use the @remote and RemoteExecutor introduced to SageMaker SDK\n",
    "to delegate data processing and model training workload to SageMaker job platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.remote_function import remote, RemoteExecutor\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "s3_root_folder = f\"s3://{sm_session.default_bucket()}/remote_function_demo/xgb_abalone\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Set\n",
    "The dataset you use is the [UCI Machine Learning Abalone Dataset](http://archive.ics.uci.edu/ml) [1]. The aim for this task is to determine the age of an abalone snail from its physical measurements. At the core, this is a regression problem.\n",
    "\n",
    "The dataset contains several features: length (the longest shell measurement), diameter (the diameter perpendicular to length), height (the height with meat in the shell), whole_weight (the weight of whole abalone), shucked_weight (the weight of meat), viscera_weight (the gut weight after bleeding), shell_weight (the weight after being dried), sex ('M', 'F', 'I' where 'I' is Infant), and rings (integer).\n",
    "\n",
    "The number of rings turns out to be a good approximation for age (age is rings + 1.5). However, to obtain this number requires cutting the shell through the cone, staining the section, and counting the number of rings through a microscope, which is a time-consuming task. However, the other physical measurements are easier to determine. You use the dataset to build a predictive model of the variable rings through these other physical measurements.\n",
    "\n",
    "Before you upload the data to an S3 bucket, install the SageMaker Python SDK and gather some constants you can use later in this notebook.\n",
    "\n",
    "[1] Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Configuration file path\n",
    "We need set the directory in which the config.yaml file resides so that remote decorator can make use of the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set path to config file\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Process the data set\n",
    "\n",
    "The preprocessing function uses scikit-learn to do the following:\n",
    "\n",
    "* Fill in missing sex category data and encode it so that it is suitable for training.\n",
    "* Scale and normalize all numerical fields, aside from sex and rings numerical data.\n",
    "* Split the data into training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Since we get a headerless CSV file, we specify the column names here.\n",
    "feature_columns_names = [\n",
    "    \"sex\",\n",
    "    \"length\",\n",
    "    \"diameter\",\n",
    "    \"height\",\n",
    "    \"whole_weight\",\n",
    "    \"shucked_weight\",\n",
    "    \"viscera_weight\",\n",
    "    \"shell_weight\",\n",
    "]\n",
    "label_column = \"rings\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"sex\": str,\n",
    "    \"length\": np.float64,\n",
    "    \"diameter\": np.float64,\n",
    "    \"height\": np.float64,\n",
    "    \"whole_weight\": np.float64,\n",
    "    \"shucked_weight\": np.float64,\n",
    "    \"viscera_weight\": np.float64,\n",
    "    \"shell_weight\": np.float64,\n",
    "}\n",
    "label_column_dtype = {\"rings\": np.float64}\n",
    "\n",
    "input_path = \"s3://sagemaker-sample-files/datasets/tabular/uci_abalone/abalone.csv\"\n",
    "\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "\n",
    "@remote(keep_alive_period_in_seconds=600)\n",
    "def process():\n",
    "    df = pd.read_csv(\n",
    "        input_path,\n",
    "        header=None,\n",
    "        names=feature_columns_names + [label_column],\n",
    "        dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype),\n",
    "    )\n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"sex\")\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    categorical_features = [\"sex\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y = df.pop(\"rings\")\n",
    "    x_pre = preprocess.fit_transform(df)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "    x = np.concatenate((y_pre, x_pre), axis=1)\n",
    "\n",
    "    np.random.shuffle(x)\n",
    "    train, validation, test = np.split(x, [int(0.7 * len(x)), int(0.85 * len(x))])\n",
    "\n",
    "    return pd.DataFrame(train), pd.DataFrame(validation), pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df, validation_df, test_df = process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train XGBoost model and do hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_df,\n",
    "    validation_df,\n",
    "    *,\n",
    "    num_round=50,\n",
    "    objective=\"reg:linear\",\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "    use_gpu=False,\n",
    "):\n",
    "    y_train = train_df.iloc[:, 0].to_numpy()\n",
    "    train_df.drop(train_df.columns[0], axis=1, inplace=True)\n",
    "    x_train = train_df.to_numpy()\n",
    "    train_dmatrix = xgboost.DMatrix(x_train, label=y_train)\n",
    "\n",
    "    y_validation = validation_df.iloc[:, 0].to_numpy()\n",
    "    validation_df.drop(validation_df.columns[0], axis=1, inplace=True)\n",
    "    x_validation = validation_df.to_numpy()\n",
    "    validation_dmatrix = xgboost.DMatrix(x_validation, label=y_validation)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": objective,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"eta\": eta,\n",
    "        \"gamma\": gamma,\n",
    "        \"min_child_weight\": min_child_weight,\n",
    "        \"subsample\": subsample,\n",
    "        \"tree_method\": \"gpu_hist\" if use_gpu else \"hist\",  # Use GPU accelerated algorithm\n",
    "    }\n",
    "\n",
    "    evaluation__results = {}  # Store accuracy result\n",
    "    booster = xgboost.train(\n",
    "        param,\n",
    "        train_dmatrix,\n",
    "        num_round,\n",
    "        evals=[(train_dmatrix, \"train\"), (validation_dmatrix, \"validation\")],\n",
    "        early_stopping_rounds=5,\n",
    "        evals_result=evaluation__results,\n",
    "    )\n",
    "\n",
    "    return booster, evaluation__results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with RemoteExecutor(max_parallel_jobs=2, keep_alive_period_in_seconds=60) as e:\n",
    "    futures = []\n",
    "    for max_depth in [3, 5, 7, 9]:\n",
    "        futures.append(e.submit(train, train_df, validation_df, max_depth=max_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Performs the model evaluation using test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "@remote(keep_alive_period_in_seconds=600)\n",
    "def evaluate(booster, test_df):\n",
    "    y_test = test_df.iloc[:, 0].to_numpy()\n",
    "    test_df.drop(test_df.columns[0], axis=1, inplace=True)\n",
    "    x_test = test_df.to_numpy()\n",
    "\n",
    "    predictions = booster.predict(xgboost.DMatrix(x_test))\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "\n",
    "    return mse, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the training result. Here we just pick the last trained model for demonstration.\n",
    "trained_booster, evaluation_results = futures[3].result()\n",
    "\n",
    "# Evaluate the trained model against the test data\n",
    "evaluate(trained_booster, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
