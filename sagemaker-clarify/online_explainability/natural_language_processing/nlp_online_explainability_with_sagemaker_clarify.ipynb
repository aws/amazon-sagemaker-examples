{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9947f913",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NLP Online Explainability with SageMaker Clarify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80d5f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff305d4b",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "1. [General Setup](#General-Setup)\n",
    "    1. [Install dependencies](#Install-dependencies)\n",
    "    1. [Import libraries](#Import-libraries)\n",
    "    1. [Set configurations](#Set-configurations)\n",
    "    1. [Create serializer and deserializer](#Create-serializer-and-deserializer)\n",
    "    1. [For visualization](#For-visualization)\n",
    "1. [Prepare data](#Prepare-data)\n",
    "    1. [Download data](#Download-data)\n",
    "    1. [Loading the data](#Loading-the-data)\n",
    "    1. [Data preparation for model training](#Data-preparation-for-model-training)\n",
    "    1. [Upload the dataset](#Upload-the-dataset)\n",
    "1. [Train and Deploy Hugging Face Model](#Train-and-Deploy-Hugging-Face-Model)\n",
    "    1. [Train model with Hugging Face estimator](#Train-model-with-Hugging-Face-estimator)\n",
    "    1. [Download the trained model files](#Download-the-trained-model-files)\n",
    "    1. [Prepare model container definition](#Prepare-model-container-definition)\n",
    "1. [Create endpoint](#Create-endpoint)\n",
    "    1. [Create model](#Create-model)\n",
    "    1. [Create endpoint config](#Create-endpoint-config)\n",
    "    1. [Create endpoint](#Create-endpoint)\n",
    "1. [Invoke endpoint](#Invoke-endpoint)\n",
    "    1. [Single record request](#Single-record-request)\n",
    "    1. [Single record request, no explanation](#Single-record-request,-no-explanation)\n",
    "    1. [Batch request, explain both](#Batch-request,-explain-both)\n",
    "    1. [Batch request with more records, explain some of the records](#Batch-request-with-more-records,-explain-some-of-the-records)\n",
    "1. [Cleanup](#Cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e481e8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Amazon SageMaker Clarify helps improve your machine learning models by detecting potential bias and helping explain how these models make predictions. The fairness and explainability functionality provided by SageMaker Clarify takes a step towards enabling AWS customers to build trustworthy and understandable machine learning models. \n",
    "\n",
    "SageMaker Clarify currently supports explainability for SageMaker models as an offline processing job. This example notebook showcases a new feature for explainability on a [SageMaker real-time inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html) endpoint, a.k.a. [Online Explainability](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-online-explainability.html).\n",
    "\n",
    "This example notebook walks you through:  \n",
    "1. Key terms and concepts needed to understand SageMaker Clarify\n",
    "1. Trained the model on the Women's ecommerce clothing reviews dataset.\n",
    "1. Create a model from trained model artifacts, create an endpoint configuration with the new SageMaker Clarify explainer configuration, and create an endpoint using the same explainer configuration.\n",
    "1. Invoke the endpoint with single and batch request with different `EnableExplanations` query.\n",
    "1. Explaining the importance of the various input features on the model's decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a9f9b",
   "metadata": {},
   "source": [
    "## General Setup\n",
    "\n",
    "We recommend you use `Python 3 (Data Science)` kernel on SageMaker Studio or `conda_python3` kernel on SageMaker Notebook Instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af597f",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "Install required dependencies. `datasets[s3]` and `transformers` are used for data preparation and training, `captum` is used to visualize the feature attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7375b3",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f759dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import tarfile\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from datasets import Dataset\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from captum.attr import visualization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.utils import unique_name_from_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a7b53d",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a747240",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_session = boto3.session.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Initialize sagemaker session\n",
    "sagemaker_session = Session(\n",
    "    boto_session=boto3_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_runtime_client=sagemaker_runtime_client,\n",
    ")\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "role = get_execution_role()\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "prefix = unique_name_from_base(\"DEMO-NLP-Women-Clothing\")\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = f\"sagemaker/{prefix}\"\n",
    "s3_key = f\"s3://{s3_bucket}/{s3_prefix}\"\n",
    "print(f\"Demo S3 key: {s3_key}\")\n",
    "\n",
    "model_name = f\"{prefix}-model\"\n",
    "print(f\"Demo model name: {model_name}\")\n",
    "endpoint_config_name = f\"{prefix}-endpoint-config\"\n",
    "print(f\"Demo endpoint config name: {endpoint_config_name}\")\n",
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "print(f\"Demo endpoint name: {endpoint_name}\")\n",
    "\n",
    "# SageMaker Clarify model directory name\n",
    "model_path = \"model/\"\n",
    "\n",
    "# Instance type for training and hosting\n",
    "instance_type = \"ml.m5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a3bf2b",
   "metadata": {},
   "source": [
    "### Create serializer and deserializer\n",
    "\n",
    "CSV serializer to serialize test data to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4394a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_serializer = CSVSerializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38990b",
   "metadata": {},
   "source": [
    "JSON deserializer to deserialize invoke endpoint response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70baa91",
   "metadata": {},
   "source": [
    "### For visualization\n",
    "We have some methods implemented for visualization in `visualization_utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34126440",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run visualization_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6468da5",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef1b76",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Data Source: `https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews/`\n",
    "\n",
    "The Women’s E-Commerce Clothing Reviews dataset has been made available under a Creative Commons Public Domain license. A copy of the dataset has been saved in a sample data Amazon S3 bucket. In the first section of the notebook, we’ll walk through how to download the data and get started with building the ML workflow as a SageMaker pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1911a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    f\"sagemaker-example-files-prod-{region}\",\n",
    "    \"datasets/tabular/womens_clothing_ecommerce/Womens_Clothing_E-Commerce_Reviews.csv\",\n",
    "    \"womens_clothing_reviews_dataset.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928470ca",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b9977",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"womens_clothing_reviews_dataset.csv\", index_col=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf7bbd",
   "metadata": {},
   "source": [
    "**Context**\n",
    "\n",
    "The Women’s Clothing E-Commerce dataset contains reviews written by customers. Because the dataset contains real commercial data, it has been anonymized, and any references to the company in the review text and body have been replaced with “retailer”.\n",
    "\n",
    "\n",
    "\n",
    "**Content**\n",
    "\n",
    "The dataset contains 23486 rows and 10 columns. Each row corresponds to a customer review.\n",
    "\n",
    "The columns include:\n",
    "\n",
    "* Clothing ID: Integer Categorical variable that refers to the specific piece being reviewed.\n",
    "* Age: Positive Integer variable of the reviewer's age.\n",
    "* Title: String variable for the title of the review.\n",
    "* Review Text: String variable for the review body.\n",
    "* Rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n",
    "* Recommended IND: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n",
    "* Positive Feedback Count: Positive Integer documenting the number of other customers who found this review positive.\n",
    "* Division Name: Categorical name of the product high level division.\n",
    "* Department Name: Categorical name of the product department name.\n",
    "* Class Name: Categorical name of the product class name.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "To predict the sentiment of a review based on the text, and then explain the predictions using SageMaker Clarify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5abd81",
   "metadata": {},
   "source": [
    "### Data preparation for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57ff2d",
   "metadata": {},
   "source": [
    "#### Target Variable Creation\n",
    "Since the dataset does not contain a column that indicates the sentiment of the customer reviews, lets create one. To do this, let's assume that reviews with a `Rating` of 4 or higher indicate positive sentiment and reviews with a `Rating` of 2 or lower indicate negative sentiment. Let's also assume that a `Rating` of 3 indicates neutral sentiment and exclude these rows from the dataset. Additionally, to predict the sentiment of a review, we are going to use the `Review Text` column; therefore let's remove rows that are empty in the `Review Text` column of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_column(df, min_positive_score, max_negative_score):\n",
    "    neutral_values = [i for i in range(max_negative_score + 1, min_positive_score)]\n",
    "    for neutral_value in neutral_values:\n",
    "        df = df[df[\"Rating\"] != neutral_value]\n",
    "    df[\"Sentiment\"] = df[\"Rating\"] >= min_positive_score\n",
    "    return df.replace({\"Sentiment\": {True: 1, False: 0}})\n",
    "\n",
    "\n",
    "df = create_target_column(df, 4, 2)\n",
    "df = df[~df[\"Review Text\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349c218",
   "metadata": {},
   "source": [
    "#### Train-Validation-Test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2fbf13",
   "metadata": {},
   "source": [
    "The most common approach for model evaluation is using the train/validation/test split. Although this approach can be very effective in general, it can result in misleading results and potentially fail when used on classification problems with a severe class imbalance. Instead, the technique must be modified to stratify the sampling by the class label as below. Stratification ensures that all classes are well represented across the train, validation and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296aa13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Sentiment\"\n",
    "cols = \"Review Text\"\n",
    "\n",
    "X = df[cols]\n",
    "y = df[target]\n",
    "\n",
    "# Data split: 11%(val) of the 90% (train and test) of the dataset ~ 10%; resulting in 80:10:10split\n",
    "test_dataset_size = 0.10\n",
    "val_dataset_size = 0.11\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Stratified train-val-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_dataset_size, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=val_dataset_size, stratify=y_train, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Dataset: train \",\n",
    "    X_train.shape,\n",
    "    y_train.shape,\n",
    "    y_train.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "print(\n",
    "    \"Dataset: validation \",\n",
    "    X_val.shape,\n",
    "    y_val.shape,\n",
    "    y_val.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "print(\n",
    "    \"Dataset: test \",\n",
    "    X_test.shape,\n",
    "    y_test.shape,\n",
    "    y_test.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "\n",
    "# Combine the independent columns with the label\n",
    "df_train = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "df_test = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "df_val = pd.concat([X_val, y_val], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e0f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = df_test.columns.to_list()\n",
    "feature_headers = headers[0]\n",
    "label_header = headers[1]\n",
    "print(f\"Feature names: {feature_headers}\")\n",
    "print(f\"Label name: {label_header}\")\n",
    "print(f\"Test data (without label column):\")\n",
    "test_data = df_test.iloc[:, :1]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609ec0ce",
   "metadata": {},
   "source": [
    "We have split the dataset into train, test, and validation datasets. We use the train and validation datasets during training process, and run Clarify on the test dataset.\n",
    "\n",
    "In the cell below, we convert the Pandas DataFrames into Hugging Face Datasets for downstream modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c70a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "val_dataset = Dataset.from_pandas(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc56b95",
   "metadata": {},
   "source": [
    "### Upload the dataset\n",
    "Here, we upload the prepared datasets to S3 buckets so that we can train the model with the Hugging Face Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd912e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 key prefix for the datasets\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f\"{s3_key}/train\"\n",
    "print(f\"training input path: {training_input_path}\")\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save val_dataset to s3\n",
    "val_input_path = f\"{s3_key}/test\"\n",
    "print(f\"validation input path: {val_input_path}\")\n",
    "val_dataset.save_to_disk(val_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950f117",
   "metadata": {},
   "source": [
    "## Train and Deploy Hugging Face Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19ba35",
   "metadata": {},
   "source": [
    "In this step of the workflow, we use the [Hugging Face Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) to load the pre-trained `distilbert-base-uncased` model and fine-tune the model on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad509be",
   "metadata": {},
   "source": [
    "### Train model with Hugging Face estimator\n",
    "The hyperparameters defined below are parameters that are passed to the custom PyTorch code in [`scripts/train.py`](./scripts/train.py). The only required parameter is `model_name`. The other parameters like `epoch`, `train_batch_size` all have default values which can be overridden by setting their values here.\n",
    "\n",
    "The training job requires GPU instance type. Here, we use `ml.g4dn.xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters passed into the training job\n",
    "hyperparameters = {\"epochs\": 1, \"model_name\": \"distilbert-base-uncased\"}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"scripts\",\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    instance_count=1,\n",
    "    transformers_version=\"4.6.1\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    ")\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": val_input_path}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf80375",
   "metadata": {},
   "source": [
    "### Download the trained model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {huggingface_estimator.model_data} model.tar.gz\n",
    "! mkdir -p {model_path}\n",
    "! tar -xvf model.tar.gz -C  {model_path}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df9a4e",
   "metadata": {},
   "source": [
    "### Prepare model container definition\n",
    "\n",
    "We are going to use the trained model files along with the HuggingFace Inference container to deploy the model to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68292cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(\"hf_model.tar.gz\", mode=\"w:gz\") as archive:\n",
    "    archive.add(model_path, recursive=True)\n",
    "    archive.add(\"code/\")\n",
    "directory_name = s3_prefix.split(\"/\")[-1]\n",
    "zipped_model_path = sagemaker_session.upload_data(\n",
    "    path=\"hf_model.tar.gz\", key_prefix=directory_name + \"/hf-model-sm\"\n",
    ")\n",
    "zipped_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb673fa",
   "metadata": {},
   "source": [
    "Create a new model object and then update its model artifact and inference script. The model object will be used to create the SageMaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c957b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = huggingface_estimator.create_model(name=model_name)\n",
    "container_def = model.prepare_container_def(instance_type=instance_type)\n",
    "container_def[\"ModelDataUrl\"] = zipped_model_path\n",
    "container_def[\"Environment\"][\"SAGEMAKER_PROGRAM\"] = \"inference.py\"\n",
    "pprint.pprint(container_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69846253",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088fe56",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "The following parameters are required to create a SageMaker model:\n",
    "\n",
    "* `ExecutionRoleArn`: The ARN of the IAM role that Amazon SageMaker can assume to access the model artifacts/ docker images for deployment\n",
    "\n",
    "* `ModelName`: name of the SageMaker model.\n",
    "\n",
    "* `PrimaryContainer`: The location of the primary docker image containing inference code, associated artifacts, and custom environment map that the inference code uses when the model is deployed for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.create_model(\n",
    "    ExecutionRoleArn=role,\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer=container_def,\n",
    ")\n",
    "print(f\"Model created: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98431adc",
   "metadata": {},
   "source": [
    "### Create endpoint config\n",
    "\n",
    "Create an endpoint configuration by calling the `create_endpoint_config` API. Here, supply the same `model_name` used in the `create_model` API call. The `create_endpoint_config` now supports the additional parameter `ClarifyExplainerConfig` to enable the Clarify explainer. The SHAP baseline is mandatory, it can be provided either as inline baseline data (the `ShapBaseline` parameter) or by a S3 baseline file (the `ShapBaselineUri` parameter). Baseline dataset type shall be the same as input dataset type, and baseline samples shall only include features. For more details on baseline selection please [refer this documentation](https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/clarify-feature-attribute-shap-baselines.html).\n",
    "\n",
    "Please see [the API documentation](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html) for details on other config parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca7fe6",
   "metadata": {},
   "source": [
    "Here we use a special token as the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e75947",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [[\"<UNK>\"]]\n",
    "print(f\"SHAP baseline: {baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808453f8",
   "metadata": {},
   "source": [
    "The `TextConfig` configured with `sentence` level granularity (When granularity is `sentence`, each sentence is a feature, and we need a few sentences per review for good visualization) and the language as English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d988fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"TestVariant\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InstanceType\": instance_type,\n",
    "        }\n",
    "    ],\n",
    "    ExplainerConfig={\n",
    "        \"ClarifyExplainerConfig\": {\n",
    "            \"InferenceConfig\": {\"FeatureTypes\": [\"text\"]},\n",
    "            \"ShapConfig\": {\n",
    "                \"ShapBaselineConfig\": {\"ShapBaseline\": csv_serializer.serialize(baseline)},\n",
    "                \"TextConfig\": {\"Granularity\": \"sentence\", \"Language\": \"en\"},\n",
    "            },\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8eee82",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "Once you have your model and endpoint configuration ready, use the `create_endpoint` API to create your endpoint. The `endpoint_name` must be unique within an AWS Region in your AWS account. The `create_endpoint` API is synchronous in nature and returns an immediate response with the endpoint status being `Creating` state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02382a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7444cb",
   "metadata": {},
   "source": [
    "Wait for the endpoint to be in \"InService\" state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d668226",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke endpoint\n",
    "\n",
    "There are expanding business needs and legislative regulations that require explanations of _why_ a model made the decision it did. SageMaker Clarify uses SHAP to explain the contribution that each input feature makes to the final decision.\n",
    "\n",
    "Kernel SHAP algorithm requires a baseline (also known as background dataset). By definition, `baseline` should either be a S3 URI to the baseline dataset file, or an in-place list of records. Baseline dataset type shall be the same as the original request data type, and baseline records shall only include features. \n",
    "\n",
    "Below are the several different combination of endpoint invocation, call them one by one and visualize the explanations by running the subsequent cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89102478",
   "metadata": {},
   "source": [
    "### Single record request\n",
    "\n",
    "Put only one record in the request body, and then send the request to the endpoint to get its predictions and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f73d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",\n",
    "    Accept=\"text/csv\",\n",
    "    Body=csv_serializer.serialize(test_data.iloc[:num_records, :].to_numpy()),\n",
    ")\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec37533",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = json_deserializer.deserialize(response[\"Body\"], content_type=response[\"ContentType\"])\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result(result, df_test[label_header][:num_records])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4b2f5",
   "metadata": {},
   "source": [
    "### Single record request, no explanation\n",
    "\n",
    "Use the `EnableExplanations` parameter to disable the explanations for this request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c78fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",\n",
    "    Accept=\"text/csv\",\n",
    "    Body=csv_serializer.serialize(test_data.iloc[:num_records, :].to_numpy()),\n",
    "    EnableExplanations=\"`false`\",  # Do not provide explanations\n",
    ")\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = json_deserializer.deserialize(response[\"Body\"], content_type=response[\"ContentType\"])\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e4b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result(result, df_test[label_header][:num_records])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d80c4f6",
   "metadata": {},
   "source": [
    "### Batch request, explain both\n",
    "\n",
    "Put two records in the request body, and then send the request to the endpoint to get their predictions and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f306d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e3a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",\n",
    "    Accept=\"text/csv\",\n",
    "    Body=csv_serializer.serialize(test_data.iloc[:num_records, :].to_numpy()),\n",
    ")\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fa15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = json_deserializer.deserialize(response[\"Body\"], content_type=response[\"ContentType\"])\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f9005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result(result, df_test[label_header][:num_records])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb60531",
   "metadata": {},
   "source": [
    "### Batch request with more records, explain some of the records\n",
    "\n",
    "Put a few more records to the request body, and then use the `EnableExplanations` expression to filter the records to be explained according to their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",\n",
    "    Accept=\"text/csv\",\n",
    "    Body=csv_serializer.serialize(test_data.iloc[:num_records, :].to_numpy()),\n",
    "    EnableExplanations=\"[0]>`0.99`\",  # Explain a record only when its prediction meets the condition\n",
    ")\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = json_deserializer.deserialize(response[\"Body\"], content_type=response[\"ContentType\"])\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4488886",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result(result, df_test[label_header][:num_records])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59483bc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Finally, don’t forget to clean up the resources we set up and used for this demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd832e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910062b8",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
