{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying pre-trained PyTorch vision models with Amazon SageMaker Neo On Inf1 Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neo is a capability of Amazon SageMaker that enables machine learning models to train once and run anywhere in the cloud and at the edge. Inf1 instances are built from the ground up to support machine learning inference applications and feature up to 16 AWS Inferentia chips, high-performance machine learning inference chips designed and built by AWS. This notebook will show you how to deploy a pretrained PyTorch model to an Inf1 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use sagemaker version at least 2.11.0 in order to support compile PyTorch model on Inf1 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install -qU \"sagemaker>=2.11.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import ResNet18 from TorchVision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import [ResNet18](https://arxiv.org/abs/1512.03385) model from TorchVision and create a model artifact `model.tar.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import tarfile\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "input_shape = [1, 3, 224, 224]\n",
    "trace = torch.jit.trace(resnet18.float().eval(), torch.zeros(input_shape).float())\n",
    "trace.save(\"model.pth\")\n",
    "\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as f:\n",
    "    f.add(\"model.pth\")\n",
    "    f.add(\"resnet18.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model with Default Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will forward the model artifact to Neo Compilation API. In this section, we will compile model with one core which is the default setting for compilation.\n",
    "\n",
    "We will go through how to compile model for multiple cores using compiler options in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "compilation_job_name = name_from_base(\"TorchVision-ResNet18-Neo-Inf1\")\n",
    "\n",
    "model_key = \"{}/model/model.tar.gz\".format(compilation_job_name)\n",
    "model_path = \"s3://{}/{}\".format(bucket, model_key)\n",
    "boto3.resource(\"s3\").Bucket(bucket).upload_file(\"model.tar.gz\", model_key)\n",
    "print(\"Uploaded model to s3:\")\n",
    "print(model_path)\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "compiled_model_path = \"s3://{}/{}/output\".format(bucket, compilation_job_name)\n",
    "print(\"Output path for compiled model:\")\n",
    "print(compiled_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a PyTorchModel object, with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_path,\n",
    "    role=role,\n",
    "    entry_point=\"resnet18.py\",\n",
    "    framework_version=\"1.5.1\",\n",
    "    py_version=\"py3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model on Inf1 instance for real-time inferences\n",
    "\n",
    "After creating the PyTorch model, we compile the model using Amazon SageMaker Neo to optize performance for our desired deployment target. To compile our model for deploying on Inf1 instances, we are using the  ``compile()`` method and select ``'ml_inf1'`` as our deployment target. The compiled model will then be deployed on an endpoint using Inf1 instances in Amazon SageMaker. \n",
    "\n",
    "## Compile the model \n",
    "\n",
    "The ``input_shape`` is the definition for the model's input tensor and ``output_path`` is where the compiled model will be stored in S3. **Important. If the following command result in a permission error, scroll up and locate the value of execution role returned by `get_execution_role()`. The role must have access to the S3 bucket specified in ``output_path``.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_model = pytorch_model.compile(\n",
    "    target_instance_family=\"ml_inf1\",\n",
    "    input_shape={\"input0\": [1, 3, 224, 224]},\n",
    "    output_path=compiled_model_path,\n",
    "    framework=\"pytorch\",\n",
    "    framework_version=\"1.5.1\",\n",
    "    role=role,\n",
    "    job_name=compilation_job_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the compiled model on a SageMaker endpoint\n",
    "\n",
    "Now that we have the compiled model, we will deploy it on an Amazon SageMaker endpoint. Inf1 instances in Amazon SageMaker are available in four sizes: ml.inf1.xlarge, ml.inf1.2xlarge, ml.inf1.6xlarge, and ml.inf1.24xlarge. In this example, we are using ``'ml.inf1.xlarge'`` for deploying our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = neo_model.deploy(instance_type=\"ml.inf1.xlarge\", initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the endpoint\n",
    "\n",
    "Once the endpoint is ready, you can send requests to it and receive inference results in real-time with low latency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to send a cat picture.\n",
    "\n",
    "![title](cat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "sm_runtime = boto3.Session().client(\"sagemaker-runtime\")\n",
    "\n",
    "with open(\"cat.jpg\", \"rb\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name, ContentType=\"application/x-image\", Body=payload\n",
    ")\n",
    "print(response)\n",
    "result = json.loads(response[\"Body\"].read().decode())\n",
    "print(\"Most likely class: {}\".format(np.argmax(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load names for ImageNet classes\n",
    "object_categories = {}\n",
    "with open(\"imagenet1000_clsidx_to_labels.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        key, val = line.strip().split(\":\")\n",
    "        object_categories[key] = val\n",
    "print(\n",
    "    \"Result: label - \"\n",
    "    + object_categories[str(np.argmax(result))]\n",
    "    + \" probability - \"\n",
    "    + str(np.amax(result))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model for Multiple Cores using Compiler Options\n",
    "In this section, we will compile the model for two cores and host on sagemaker using 2 sagemaker model server workers to utilize all 4 cores on inf1.xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "compilation_job_name = name_from_base(\"TorchVision-ResNet18-Neo-Inf1\")\n",
    "\n",
    "model_key = \"{}/model/model.tar.gz\".format(compilation_job_name)\n",
    "model_path = \"s3://{}/{}\".format(bucket, model_key)\n",
    "boto3.resource(\"s3\").Bucket(bucket).upload_file(\"model.tar.gz\", model_key)\n",
    "print(\"Uploaded model to s3:\")\n",
    "print(model_path)\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "compiled_model_path = \"s3://{}/{}/output\".format(bucket, compilation_job_name)\n",
    "print(\"Output path for compiled model:\")\n",
    "print(compiled_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to host model compiled for 2 cores, we set environment variables NEURONCORE_GROUP_SIZES and SAGEMAKER_MODEL_SERVER_WORKERS.\n",
    "### More Information on Environment Variables for Hosting\n",
    "NEURONCORE_GROUP_SIZES - If the model is compiled for n inferentia cores , set NEURONCORE_GROUP_SIZES=n . For more information on NEURONCORE_GROUP_SIZES, refer to https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/tensorflow-neuron/tutorials/tutorial-tensorflow-NeuronCore-Group.html\n",
    "\n",
    "SAGEMAKER_MODEL_SERVER_WORKERS - Number of workers required to utilize all inferentia cores. For example, on inf1.2xlarge or inf1.xlarge, if the model is compiled for one core,\n",
    "we need 4 workers to utilize all inferentia cores which will load the compiled model in different processes. If the model is compiled for 2 cores, we only need 2 workers to utilize \n",
    "all inferentia cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_path,\n",
    "    role=role,\n",
    "    entry_point=\"resnet18.py\",\n",
    "    framework_version=\"1.5.1\",\n",
    "    py_version=\"py3\",\n",
    "    env={\"NEURONCORE_GROUP_SIZES\": \"2\", \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"2\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model \n",
    "Then compile the model with compiler options to pass number of cores as 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_model = pytorch_model.compile(\n",
    "    target_instance_family=\"ml_inf1\",\n",
    "    input_shape={\"input0\": [1, 3, 224, 224]},\n",
    "    output_path=compiled_model_path,\n",
    "    framework=\"pytorch\",\n",
    "    framework_version=\"1.5.1\",\n",
    "    role=role,\n",
    "    job_name=compilation_job_name,\n",
    "    compiler_options={\"num-neuroncores\": 2},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the compiled model on a SageMaker endpoint\n",
    "Then we need to deploy the compiled model on an Amazon SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = neo_model.deploy(instance_type=\"ml.inf1.xlarge\", initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the endpoint\n",
    "\n",
    "Once the endpoint is ready, you can send requests to it and receive inference results in real-time with low latency. We will use the same cat picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "sm_runtime = boto3.Session().client(\"sagemaker-runtime\")\n",
    "\n",
    "with open(\"cat.jpg\", \"rb\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name, ContentType=\"application/x-image\", Body=payload\n",
    ")\n",
    "print(response)\n",
    "result = json.loads(response[\"Body\"].read().decode())\n",
    "print(\"Most likely class: {}\".format(np.argmax(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load names for ImageNet classes\n",
    "object_categories = {}\n",
    "with open(\"imagenet1000_clsidx_to_labels.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        key, val = line.strip().split(\":\")\n",
    "        object_categories[key] = val\n",
    "print(\n",
    "    \"Result: label - \"\n",
    "    + object_categories[str(np.argmax(result))]\n",
    "    + \" probability - \"\n",
    "    + str(np.amax(result))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
