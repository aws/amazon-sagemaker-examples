{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision for Medical Imaging\n",
    "This notebook showcases techniques and services offer by SageMaker to build a model which predicts if an image of cells contains cancer. This notebook shows how to build a model using hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset for this demo comes from the [Camelyon16 Challenge](https://camelyon16.grand-challenge.org/) made available under the CC0 licencse. The raw data provided by the challenge has been processed into 96x96 pixel tiles by [Bas Veeling](https://github.com/basveeling/pcam) and also made available under the CC0 license. For detailed information on each dataset please see the papers below:\n",
    "* Ehteshami Bejnordi et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer. JAMA: The Journal of the American Medical Association, 318(22), 2199â€“2210. [doi:jama.2017.14585](https://doi.org/10.1001/jama.2017.14585)\n",
    "* B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling. \"Rotation Equivariant CNNs for Digital Pathology\". [arXiv:1806.03962](http://arxiv.org/abs/1806.03962)\n",
    "\n",
    "The tiled dataset from Bas Veeling is over 6GB of data. In order to easily run this demo, the dataset has been pruned to the first 14,000 images of the tiled dataset and comes included in the repo with this notebook for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Sagemaker SDK and Boto3\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>NOTE</b> You may get an error from pip's dependency resolver; you can ignore this error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "\n",
    "\n",
    "def import_or_install(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        ! pip install $package\n",
    "\n",
    "\n",
    "required_packages = [\"sagemaker\", \"boto3\", \"h5py\", \"tqdm\", \"matplotlib\", \"opencv-python\"]\n",
    "\n",
    "for package in required_packages:\n",
    "    import_or_install(package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import h5py\n",
    "import zipfile\n",
    "import boto3\n",
    "import sagemaker\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "from inference_specification import InferenceSpecification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Boto3 Clients and Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-west-2\"  # Change region as needed\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Prepare Dataset\n",
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if directory exists\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# download zip file from public s3 bucket\n",
    "!wget -P data https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/pcam/medical_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"data/medical_images.zip\") as zf:\n",
    "    zf.extractall()\n",
    "with open(\"data/camelyon16_tiles.h5\", \"rb\") as hf:\n",
    "    f = h5py.File(hf, \"r\")\n",
    "\n",
    "    X = f[\"x\"][()]\n",
    "    y = f[\"y\"][()]\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to session s3 bucket\n",
    "s3_client.upload_file(\"data/medical_images.zip\", bucket, f\"data/medical_images.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete local copy\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"data/medical_images.zip\"):\n",
    "    os.remove(\"data/medical_images.zip\")\n",
    "else:\n",
    "    print(\"The file does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Sample Images from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_images(X, y, n, cols):\n",
    "    sample_images = X[:n]\n",
    "    sample_labels = y[:n]\n",
    "\n",
    "    rows = int(np.ceil(n / cols))\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(11.5, 7))\n",
    "\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        image = sample_images[i]\n",
    "        label = sample_labels[i]\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "preview_images(X, y, 15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_numpy = X[:]\n",
    "y_numpy = y[:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_numpy, y_numpy, test_size=1000, random_state=0\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=2000, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Splits to RecordIO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_recordio(X: np.ndarray, y: np.ndarray, prefix: str):\n",
    "    record = mx.recordio.MXIndexedRecordIO(idx_path=f\"{prefix}.idx\", uri=f\"{prefix}.rec\", flag=\"w\")\n",
    "    for idx, arr in enumerate(tqdm(X)):\n",
    "        header = mx.recordio.IRHeader(0, y[idx], idx, 0)\n",
    "        s = mx.recordio.pack_img(\n",
    "            header,\n",
    "            arr,\n",
    "            quality=95,\n",
    "            img_fmt=\".jpg\",\n",
    "        )\n",
    "        record.write_idx(idx, s)\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_recordio(X_train, y_train, prefix=\"data/train\")\n",
    "write_to_recordio(X_val, y_val, prefix=\"data/val\")\n",
    "write_to_recordio(X_test, y_test, prefix=\"data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data Splits to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"cv-metastasis-{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "try:\n",
    "    s3_client.create_bucket(\n",
    "        Bucket=bucket, ACL=\"private\", CreateBucketConfiguration={\"LocationConstraint\": region}\n",
    "    )\n",
    "    print(f\"Created S3 bucket: {bucket}\")\n",
    "\n",
    "except Exception as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"BucketAlreadyOwnedByYou\":\n",
    "        print(f\"Using existing bucket: {bucket}\")\n",
    "    else:\n",
    "        raise (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\"data/train.rec\", bucket, f\"{prefix}/data/train/train.rec\")\n",
    "s3_client.upload_file(\"data/val.rec\", bucket, f\"{prefix}/data/val/val.rec\")\n",
    "s3_client.upload_file(\"data/test.rec\", bucket, f\"{prefix}/data/test/test.rec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training the Model\n",
    "### Configure the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve(\"image-classification\", region)\n",
    "num_training_samples = X_train.shape[0]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "hyperparameters = {\n",
    "    \"num_layers\": 18,\n",
    "    \"use_pretrained_model\": 1,\n",
    "    \"augmentation_type\": \"crop_color_transform\",\n",
    "    \"image_shape\": \"3,96,96\",\n",
    "    \"num_classes\": num_classes,\n",
    "    \"num_training_samples\": num_training_samples,\n",
    "    \"mini_batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"precision_dtype\": \"float32\",\n",
    "}\n",
    "\n",
    "estimator_config = {\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"image_uri\": training_image,\n",
    "    \"role\": sagemaker.get_execution_role(),\n",
    "    \"instance_count\": 1,\n",
    "    \"instance_type\": \"ml.p3.2xlarge\",\n",
    "    \"volume_size\": 100,\n",
    "    \"max_run\": 360000,\n",
    "    \"output_path\": f\"s3://{bucket}/{prefix}/training_jobs\",\n",
    "}\n",
    "\n",
    "image_classifier = sagemaker.estimator.Estimator(**estimator_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Hyperparameter Tuner\n",
    "\n",
    "Although we would prefer to tune for recall, the current HyperparameterTuner implementation for Image Classification only supports validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"mini_batch_size\": sagemaker.parameter.CategoricalParameter([16, 32, 64]),\n",
    "    \"learning_rate\": sagemaker.parameter.CategoricalParameter([0.001, 0.01]),\n",
    "}\n",
    "\n",
    "hyperparameter_tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator=image_classifier,\n",
    "    objective_metric_name=\"validation:accuracy\",\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=6,\n",
    "    max_parallel_jobs=2,\n",
    "    base_tuning_job_name=prefix,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Data Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/{prefix}/data/train\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    input_mode=\"Pipe\",\n",
    ")\n",
    "\n",
    "val_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/{prefix}/data/val\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    input_mode=\"Pipe\",\n",
    ")\n",
    "\n",
    "data_channels = {\"train\": train_input, \"validation\": val_input}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Hyperparameter Tuning Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"tuning_job_name\" not in locals():\n",
    "    hyperparameter_tuner.fit(inputs=data_channels)\n",
    "    tuning_job_name = hyperparameter_tuner.describe().get(\"HyperParameterTuningJobName\")\n",
    "else:\n",
    "    print(f\"Using previous tuning job: {tuning_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Results\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>NOTE:</b> If your kernel has restarted after running the hyperparameter tuning job, everyting you need has been persisted to SageMaker. You can continue on without having to run the tuning job again.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sagemaker.analytics.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "results_df = results.dataframe()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_training_job_summary = results.description()[\"BestTrainingJob\"]\n",
    "best_training_job_name = best_training_job_summary[\"TrainingJobName\"]\n",
    "\n",
    "%store best_training_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Retrieving and Saving the Model in SageMaker Lineage and SageMaker Model Registry\n",
    "### Examine Lineage\n",
    "Though you already know the training job details from running the cells above, if we were just given the model uri, we could use SageMaker Lineage to retrieve the training job details which produced the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Lineage and Metrics for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.lineage import context, artifact, association, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sagemaker.analytics.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "results_df = results.dataframe()\n",
    "best_training_job_summary = results.description()[\"BestTrainingJob\"]\n",
    "best_training_job_details = sagemaker_boto_client.describe_training_job(\n",
    "    TrainingJobName=best_training_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_artifact_list = []\n",
    "for data_input in best_training_job_details[\"InputDataConfig\"]:\n",
    "    channel = data_input[\"ChannelName\"]\n",
    "    data_s3_uri = data_input[\"DataSource\"][\"S3DataSource\"][\"S3Uri\"]\n",
    "\n",
    "    matching_artifacts = list(\n",
    "        artifact.Artifact.list(source_uri=data_s3_uri, sagemaker_session=sagemaker_session)\n",
    "    )\n",
    "\n",
    "    if matching_artifacts:\n",
    "        data_artifact = matching_artifacts[0]\n",
    "        print(f\"Using existing artifact: {data_artifact.artifact_arn}\")\n",
    "    else:\n",
    "        data_artifact = artifact.Artifact.create(\n",
    "            artifact_name=channel,\n",
    "            source_uri=data_s3_uri,\n",
    "            artifact_type=\"DataSet\",\n",
    "            sagemaker_session=sagemaker_session,\n",
    "        )\n",
    "        print(f\"Create artifact {data_artifact.artifact_arn}: SUCCESSFUL\")\n",
    "    data_artifact_list.append(data_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_s3_uri = best_training_job_details[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "matching_artifacts = list(\n",
    "    artifact.Artifact.list(source_uri=trained_model_s3_uri, sagemaker_session=sagemaker_session)\n",
    ")\n",
    "\n",
    "if matching_artifacts:\n",
    "    model_artifact = matching_artifacts[0]\n",
    "    print(f\"Using existing artifact: {model_artifact.artifact_arn}\")\n",
    "else:\n",
    "    model_artifact = artifact.Artifact.create(\n",
    "        artifact_name=\"TrainedModel\",\n",
    "        source_uri=trained_model_s3_uri,\n",
    "        artifact_type=\"Model\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    print(f\"Create artifact {model_artifact.artifact_arn}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set artifact associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component = sagemaker_boto_client.describe_trial_component(\n",
    "    TrialComponentName=best_training_job_summary[\"TrainingJobName\"] + \"-aws-training-job\"\n",
    ")\n",
    "trial_component_arn = trial_component[\"TrialComponentArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_list = data_artifact_list + [model_artifact]\n",
    "\n",
    "for artif in artifact_list:\n",
    "    if artif.artifact_type == \"DataSet\":\n",
    "        assoc = \"ContributedTo\"\n",
    "    else:\n",
    "        assoc = \"Produced\"\n",
    "    try:\n",
    "        association.Association.create(\n",
    "            source_arn=artif.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type=assoc,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "        )\n",
    "        print(f\"Association with {artif.artifact_type}: SUCCESSFUL\")\n",
    "    except:\n",
    "        print(f\"Association already exists with {artif.artifact_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Registry\n",
    "You can also save your model in the model registry, which you can use to check and retrieve your model in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_name = prefix\n",
    "\n",
    "model_packages = sagemaker_boto_client.list_model_packages(ModelPackageGroupName=mpg_name)[\n",
    "    \"ModelPackageSummaryList\"\n",
    "]\n",
    "\n",
    "if model_packages:\n",
    "    print(f\"Using existing Model Package Group: {mpg_name}\")\n",
    "else:\n",
    "    mpg_input_dict = {\n",
    "        \"ModelPackageGroupName\": mpg_name,\n",
    "        \"ModelPackageGroupDescription\": \"Cancer metastasis detection\",\n",
    "    }\n",
    "\n",
    "    mpg_response = sagemaker_boto_client.create_model_package_group(**mpg_input_dict)\n",
    "    print(f\"Create Model Package Group {mpg_name}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_jobs = results_df[\"TrainingJobName\"]\n",
    "\n",
    "for job_name in training_jobs:\n",
    "    job_data = sagemaker_boto_client.describe_training_job(TrainingJobName=job_name)\n",
    "    model_uri = job_data.get(\"ModelArtifacts\", {}).get(\"S3ModelArtifacts\")\n",
    "    training_image = job_data[\"AlgorithmSpecification\"][\"TrainingImage\"]\n",
    "\n",
    "    mp_inference_spec = InferenceSpecification().get_inference_specification_dict(\n",
    "        ecr_image=training_image,\n",
    "        supports_gpu=False,\n",
    "        supported_content_types=[\"text/csv\"],\n",
    "        supported_mime_types=[\"text/csv\"],\n",
    "    )\n",
    "\n",
    "    mp_inference_spec[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"] = model_uri\n",
    "    mp_input_dict = {\n",
    "        \"ModelPackageGroupName\": mpg_name,\n",
    "        \"ModelPackageDescription\": \"SageMaker Image Classifier\",\n",
    "        \"ModelApprovalStatus\": \"PendingManualApproval\",\n",
    "    }\n",
    "\n",
    "    mp_input_dict.update(mp_inference_spec)\n",
    "    mp_response = sagemaker_boto_client.create_model_package(**mp_input_dict)\n",
    "\n",
    "model_packages = sagemaker_boto_client.list_model_packages(\n",
    "    ModelPackageGroupName=mpg_name, MaxResults=6\n",
    ")[\"ModelPackageSummaryList\"]\n",
    "model_packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Deploying the Model\n",
    "### Create Model from Existing Training Job Name for Deployment\n",
    "\n",
    "We can use the name of the best training job from our hyperparameter tuning experiment and create its corresponding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"metastasis-detection-{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "model_matches = sagemaker_boto_client.list_models(NameContains=model_name)[\"Models\"]\n",
    "training_image = sagemaker.image_uris.retrieve(\"image-classification\", region)\n",
    "\n",
    "if not model_matches:\n",
    "    print(f\"Creating model {model_name}\")\n",
    "    sagemaker_session.create_model_from_job(\n",
    "        name=model_name,\n",
    "        training_job_name=best_training_job_summary[\"TrainingJobName\"],\n",
    "        role=sagemaker_role,\n",
    "        image_uri=training_image,\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model {model_name} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model using Data from Model Registry\n",
    "\n",
    "As we saved data about model in the Model Resgistry, we can look up details about the model and use them to deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_jobs = results_df[\"TrainingJobName\"]\n",
    "best_model_index = np.where(training_jobs.values == best_training_job_summary[\"TrainingJobName\"])[\n",
    "    0\n",
    "][0]\n",
    "best_model_info = sagemaker_boto_client.describe_model_package(\n",
    "    ModelPackageName=model_packages[best_model_index][\"ModelPackageArn\"]\n",
    ")\n",
    "best_model_container = best_model_info.get(\"InferenceSpecification\").get(\"Containers\")[0]\n",
    "deploy_instance_type = best_model_info.get(\"InferenceSpecification\").get(\n",
    "    \"SupportedRealtimeInferenceInstanceTypes\"\n",
    ")[0]\n",
    "\n",
    "best_model = sagemaker.Model(\n",
    "    image_uri=best_model_container.get(\"Image\"),\n",
    "    model_data=best_model_container.get(\"ModelDataUrl\"),\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    name=mpg_name,\n",
    ")\n",
    "\n",
    "best_model.deploy(\n",
    "    initial_instance_count=1, instance_type=deploy_instance_type, endpoint_name=mpg_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Finally, the we can now validate the model for use. You can obtain the endpoint from the client library using the result from previous operations, and generate classifications from the trained model using that endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with h5py.File(\"data/camelyon16_tiles.h5\", \"r\") as hf:\n",
    "    X = hf[\"x\"][()]\n",
    "    y = hf[\"y\"][()]\n",
    "\n",
    "X_numpy = X[:]\n",
    "y_numpy = y[:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_numpy, y_numpy, test_size=1000, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view test image\n",
    "image = X_test[0]\n",
    "label = y_test[0]\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Label: {label}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.fromarray(X_test[0])\n",
    "file_name = \"data/test_image.jpg\"\n",
    "img.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "runtime = boto3.Session().client(service_name=\"runtime.sagemaker\")\n",
    "with open(file_name, \"rb\") as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=mpg_name, ContentType=\"application/x-image\", Body=payload\n",
    ")\n",
    "\n",
    "result = response[\"Body\"].read()\n",
    "\n",
    "# result will be in json format and convert it to ndarray\n",
    "result = json.loads(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result will output the probabilities for all classes\n",
    "# find the class with maximum probability and print the class index\n",
    "index = np.argmax(result)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in range(len(X_test)):\n",
    "    img = Image.fromarray(X_test[i])\n",
    "    file_name = f\"/tmp/test_image.jpg\"\n",
    "    img.save(file_name)\n",
    "\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=mpg_name, ContentType=\"application/x-image\", Body=payload\n",
    "    )\n",
    "\n",
    "    result = response[\"Body\"].read()\n",
    "    result = json.loads(result)\n",
    "    index = np.argmax(result)\n",
    "    predictions.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, predictions)\n",
    "print(f\"Precision = {precision[1]}\")\n",
    "print(f\"Recall = {recall[1]}\")\n",
    "print(f\"F1-Score = {f1[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.sagemaker_session.delete_endpoint(mpg_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
