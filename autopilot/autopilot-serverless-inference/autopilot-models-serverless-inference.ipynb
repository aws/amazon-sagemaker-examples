{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Autopilot models to serverless inference endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Serverless Inference is a purpose-built inference option that makes it easy for customers to deploy and scale ML models. Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless endpoints also automatically launch compute resources and scale them in and out depending on traffic, eliminating the need to choose instance types or manage scaling policies.\n",
    "\n",
    "In this notebook we'll use models generated with Amazon SageMaker Autopilot and then deploy these models to serverless endpoints.\n",
    "\n",
    "We will be using the public [UCI Direct Marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing) dataset for this example.\n",
    "\n",
    "**Notebook Settings:**\n",
    "\n",
    "- **SageMaker Classic Notebook Instance:** `ml.t3.xlarge` Notebook Instance & `conda_python3` Kernel\n",
    "- **SageMaker Studio:** `Python 3 (Data Science 2.0) Kernel`\n",
    "- **Regions Available:** SageMaker Serverless Inference is currently available in the following regions: \n",
    "        US East (Northern Virginia), US East (Ohio), US West (Oregon), EU (Ireland), Asia Pacific (Tokyo) and Asia Pacific (Sydney)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Let's ensure we have the latest packages installed. For this notebook, we'll need the below versions of sagemaker and boto3\n",
    "1. sagemaker current version >= `2.110.0`\n",
    "1. boto3 version >= `boto3-1.24.84`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Version: 2.116.0\n",
      "Boto3 Version: 1.25.3\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sys\n",
    "\n",
    "print(f\"SageMaker Version: {sagemaker.__version__}\")\n",
    "print(f\"Boto3 Version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Skip the below cell if installed sagemaker current version >= `2.110.0` and boto3 version >= `boto3-1.24.84`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U awscli sagemaker boto3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SageMaker Version: {sagemaker.__version__}\")\n",
    "print(f\"Boto3 Version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import packages, establish session and unique ID for job name suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: s3://sagemaker-us-east-1-726793866085/autopilot/bankadditional\n",
      "Region: us-east-1\n",
      "Role: arn:aws:iam::726793866085:role/service-role/AmazonSageMaker-ExecutionRole-20220313T104021\n",
      "Job and model prefix string: bankmrkt\n",
      "suffix string: d6e699-01Dec2022\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from time import gmtime, strftime, sleep\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "# Define region, bucket\n",
    "session = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "bucket = session.default_bucket()\n",
    "# use the below for default SageMaker execution role else replace with your own IAM Role ARN\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "prefix = \"autopilot/bankadditional\"\n",
    "\n",
    "today = datetime.now().strftime(\"%d%b%Y\")\n",
    "timestamp_suffix = f\"{str(uuid4())[:6]}-{today}\"\n",
    "\n",
    "# Define sagemaker client object to invoke Sagemaker services\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "# Set prefix for AutoML jobnames. Let's keep the prefix short as we'll be adding suffixes to distinguish job names.\n",
    "automl_job_prefix = \"bankmrkt\"  # 6-8 chars max\n",
    "model_prefix = automl_job_prefix\n",
    "\n",
    "print(f\"Bucket: s3://{bucket}/{prefix}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"Job and model prefix string: {automl_job_prefix}\")\n",
    "print(f\"suffix string: {timestamp_suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "This example uses [direct marketing dataset](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip):\n",
    "\n",
    "[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n",
    "\n",
    "Download and unzip the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-01 11:42:39--  https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n",
      "Resolving sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com (sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com)... 52.92.145.146, 52.218.193.113, 52.218.236.25, ...\n",
      "Connecting to sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com (sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com)|52.92.145.146|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 432828 (423K) [application/zip]\n",
      "Saving to: ‘data/bank-additional.zip’\n",
      "\n",
      "bank-additional.zip 100%[===================>] 422.68K   645KB/s    in 0.7s    \n",
      "\n",
      "2022-12-01 11:42:40 (645 KB/s) - ‘data/bank-additional.zip’ saved [432828/432828]\n",
      "\n",
      "Unzipping dataset to data folder...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "if not os.path.exists('data/bank-additional/bank-additional-full.csv'):\n",
    "    !wget -P data/ -N https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n",
    "    with zipfile.ZipFile(\"data/bank-additional.zip\", \"r\") as z:\n",
    "        print(\"Unzipping dataset to data folder...\")\n",
    "        z.extractall(\"data\")\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Skipping download..dataset exists at ./data/bank-additional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize dataset\n",
    "The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).\n",
    "\n",
    "Problem Type: **Binary Classification**\n",
    "\n",
    "Ref: <https://archive.ics.uci.edu/ml/datasets/bank+marketing>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>...</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41183</th>\n",
       "      <td>73</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41184</th>\n",
       "      <td>46</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41185</th>\n",
       "      <td>56</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41186</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41187</th>\n",
       "      <td>74</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41188 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age          job  marital            education  default  ...  \\\n",
       "0       56    housemaid  married             basic.4y       no  ...   \n",
       "1       57     services  married          high.school  unknown  ...   \n",
       "2       37     services  married          high.school       no  ...   \n",
       "3       40       admin.  married             basic.6y       no  ...   \n",
       "4       56     services  married          high.school       no  ...   \n",
       "...    ...          ...      ...                  ...      ...  ...   \n",
       "41183   73      retired  married  professional.course       no  ...   \n",
       "41184   46  blue-collar  married  professional.course       no  ...   \n",
       "41185   56      retired  married    university.degree       no  ...   \n",
       "41186   44   technician  married  professional.course       no  ...   \n",
       "41187   74      retired  married  professional.course       no  ...   \n",
       "\n",
       "      cons.price.idx cons.conf.idx euribor3m nr.employed    y  \n",
       "0             93.994         -36.4     4.857      5191.0   no  \n",
       "1             93.994         -36.4     4.857      5191.0   no  \n",
       "2             93.994         -36.4     4.857      5191.0   no  \n",
       "3             93.994         -36.4     4.857      5191.0   no  \n",
       "4             93.994         -36.4     4.857      5191.0   no  \n",
       "...              ...           ...       ...         ...  ...  \n",
       "41183         94.767         -50.8     1.028      4963.6  yes  \n",
       "41184         94.767         -50.8     1.028      4963.6   no  \n",
       "41185         94.767         -50.8     1.028      4963.6   no  \n",
       "41186         94.767         -50.8     1.028      4963.6  yes  \n",
       "41187         94.767         -50.8     1.028      4963.6   no  \n",
       "\n",
       "[41188 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(\"./data/bank-additional/bank-additional-full.csv\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 10)  # View all of the columns\n",
    "df_data  # show first 5 and last 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset to S3\n",
    "We'll upload the `bank-additional-full.csv` from the extracted Zip file to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to False for subsequent runs of this notebook\n",
    "upload_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping upload .. dataset is under: s3://sagemaker-us-east-1-726793866085/autopilot/bankadditional/raw/bank-additional-full.csv\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = \"data/bank-additional/bank-additional-full.csv\"\n",
    "\n",
    "if upload_dataset:\n",
    "    print(f\"Uploading data to s3...\")\n",
    "    dataset_s3uri = session.upload_data(DATA_FILE, key_prefix=f\"{prefix}/raw\")\n",
    "    print(f\"Data uploaded to : \\n {dataset_s3uri}\")\n",
    "else:\n",
    "    dataset_s3uri = f\"s3://{bucket}/{prefix}/raw/bank-additional-full.csv\"\n",
    "    print(f\"Skipping upload .. dataset is under: {dataset_s3uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Autopilot jobs in `ENSEMBLING` and `HPO` modes\n",
    "\n",
    "\n",
    "First we'll specify the AutoML job config constants\n",
    "- `TargetAttributeName` (Target column `y` for your dataset)\n",
    "- `Training Mode` - `Valid values: AUTO | ENSEMBLING | HYPERPARAMETER_TUNING`\n",
    "- `ProblemType` (optional) `Valid values: BinaryClassification | MulticlassClassification | Regression`\n",
    "- `ObjectiveMetric` (Optional) Valid Values: `Accuracy | F1 | MSE` [`AutoMLJobObjective`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AutoMLJobObjective.html)\n",
    "- `Max_Candidates` (Optional) (set only for HPO Jobs)\n",
    "- `OutputDataConfig` (Optional, set if you need to specify output location for artifacts generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autopilot job params\n",
    "target_column = \"y\"\n",
    "training_mode = \"ENSEMBLING\"\n",
    "\n",
    "# Optional Parameters\n",
    "problem_type = \"BinaryClassification\"\n",
    "objective_metric = \"F1\"\n",
    "max_job_runtime_seconds = 3600\n",
    "max_runtime_per_job_seconds = 1200\n",
    "max_candidates = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the Autopilot job config values\n",
    "- [`AutoMLJobConfig`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AutoMLJobConfig.html) (`Mode` = `AUTO | ENSEMBLING | HYPERPARAMETER_TUNING`)\n",
    "- [`InputDataConfig`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateAutoMLJob.html#sagemaker-CreateAutoMLJob-request-InputDataConfig)\n",
    "- [`AutoMLJobObjective`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AutoMLJobObjective.html) (Optional. `Accuracy | MSE | F1 | F1macro | AUC`)\n",
    "- [`OutputDataConfig`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AutoMLOutputDataConfig.html) (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Autopilot job config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_job_config = {\n",
    "    \"CompletionCriteria\": {\n",
    "        \"MaxRuntimePerTrainingJobInSeconds\": max_runtime_per_job_seconds,\n",
    "        \"MaxAutoMLJobRuntimeInSeconds\": max_job_runtime_seconds,\n",
    "    },\n",
    "    \"Mode\": training_mode,\n",
    "}\n",
    "\n",
    "automl_job_objective = {\"MetricName\": objective_metric}\n",
    "\n",
    "input_data_config = [\n",
    "    {\n",
    "        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": dataset_s3uri}},\n",
    "        \"TargetAttributeName\": target_column,\n",
    "    }\n",
    "]\n",
    "\n",
    "output_data_config = {\"S3OutputPath\": f\"s3://{bucket}/{prefix}/output\"}\n",
    "\n",
    "# Optional: Define a Tag\n",
    "tags_config = [{\"Key\": \"Project\", \"Value\": \"Autopilot-serverless\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Autopilot job with training mode set to `ENSEMBLING`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ens_automl_job_name = f\"{model_prefix}-ENS-{timestamp_suffix}\"\n",
    "    print(f\"Launching AutoMLJob → {ens_automl_job_name} with mode set to {training_mode}\")\n",
    "    response = sm_client.create_auto_ml_job(\n",
    "        AutoMLJobName=ens_automl_job_name,\n",
    "        InputDataConfig=input_data_config,\n",
    "        OutputDataConfig=output_data_config,\n",
    "        AutoMLJobConfig=automl_job_config,\n",
    "        ProblemType=problem_type,\n",
    "        AutoMLJobObjective=automl_job_objective,\n",
    "        RoleArn=role,\n",
    "        Tags=tags_config,\n",
    "    )\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error launching Autopilot Job: {ens_automl_job_name}\")\n",
    "    print(f\"Exception:\\n{e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Autopilot job with training mode set to `HYPERPARAMETER_TUNING` mode\n",
    "\n",
    "We'll update the `automl_job_config` dict to update `training_mode` to `HYPERPARAMETER_TUNING` and set the `MaxCandidates` to 15.\n",
    "\n",
    ">NOTE: In `HPO` mode the best model is derived by tuning various hyperparameters, default setting for `max_candidates` is 250 but for demonstration purposes we'll set the `max_candidates` to 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching AutoMLJob → bankmrkt-HPO-d6e699-01Dec2022 with mode set to HYPERPARAMETER_TUNING\n",
      "{'AutoMLJobArn': 'arn:aws:sagemaker:us-east-1:726793866085:automl-job/bankmrkt-HPO-d6e699-01Dec2022', 'ResponseMetadata': {'RequestId': 'b13d8624-1862-4f6f-bb7c-18d5d313b8c1', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b13d8624-1862-4f6f-bb7c-18d5d313b8c1', 'content-type': 'application/x-amz-json-1.1', 'content-length': '100', 'date': 'Thu, 01 Dec 2022 19:43:19 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# We'll use the defined job prefix to construct model name(s) and later to construct endpoint config and endpoint names.\n",
    "try:\n",
    "    training_mode = \"HYPERPARAMETER_TUNING\"\n",
    "    automl_job_config[\"Mode\"] = training_mode\n",
    "    automl_job_config[\"CompletionCriteria\"][\"MaxCandidates\"] = 15\n",
    "    hpo_automl_job_name = f\"{model_prefix}-HPO-{timestamp_suffix}\"\n",
    "    print(f\"Launching AutoMLJob → {hpo_automl_job_name} with mode set to {training_mode}\")\n",
    "    response = sm_client.create_auto_ml_job(\n",
    "        AutoMLJobName=hpo_automl_job_name,\n",
    "        InputDataConfig=input_data_config,\n",
    "        OutputDataConfig=output_data_config,\n",
    "        AutoMLJobConfig=automl_job_config,\n",
    "        ProblemType=problem_type,\n",
    "        AutoMLJobObjective=automl_job_objective,\n",
    "        RoleArn=role,\n",
    "        Tags=tags_config,\n",
    "    )\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error launching Autopilot Job: {ens_automl_job_name}\")\n",
    "    print(f\"{e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor AutoML job completion status\n",
    "\n",
    ">**NOTE:** Jobs with `ENSEMBLING` mode finishes faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(sm_client, job_name):\n",
    "    resp = sm_client.describe_auto_ml_job(AutoMLJobName=job_name)\n",
    "    p_status = resp[\"AutoMLJobStatus\"]\n",
    "    s_status = resp[\"AutoMLJobSecondaryStatus\"]\n",
    "    desc = f\"{job_name}: {p_status} | {s_status} ...\"\n",
    "    return (p_status, desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor job status launched in ensembling mode\n",
    "(p_status, desc) = get_job_status(sm_client, ens_automl_job_name)\n",
    "\n",
    "while p_status not in (\"Completed\", \"Failed\"):\n",
    "    (p_status, desc) = get_job_status(sm_client, ens_automl_job_name)\n",
    "    if p_status not in (\"Completed\", \"Failed\"):\n",
    "        print(desc)\n",
    "        sleep(60)\n",
    "        continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model from the best candidate generated by Autopilot\n",
    "- In `Ensemble` training mode Autopilot generates a single Inference container.\n",
    "\n",
    "![](./images/ap-jobprofile-ens-04Oct2022.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions to create model(s), serverless endpoint config and endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autopilot_model(sm_client, model_name, role, model_container, index):\n",
    "    try:\n",
    "        transform_mode = model_container[\"Environment\"][\"AUTOML_TRANSFORM_MODE\"]\n",
    "        if transform_mode:\n",
    "            model_name = f\"{model_name}-datamodel-{index}\"\n",
    "    except:\n",
    "        model_name = f\"{model_name}-Inf-{index}\"\n",
    "\n",
    "    if len(model_name) <= 63:\n",
    "        print(f\"Creating Model {index}: {model_name} ...\")\n",
    "        model_response = sm_client.create_model(\n",
    "            ModelName=model_name, ExecutionRoleArn=role, Containers=[model_container]\n",
    "        )\n",
    "        status_code = model_response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "        model_arn = model_response[\"ModelArn\"]\n",
    "        return (status_code, model_arn)\n",
    "    else:\n",
    "        print(f\"Model Name: {model_name} length exceeds max. allowed chars : 63\")\n",
    "        raise ValueError(\"Model name cannot exceed 63 chars.\")\n",
    "\n",
    "\n",
    "def create_serverless_endpoint_config(\n",
    "    sm_client, endpoint_config_name, model_name, memory: int = 2048, max_concurrency: int = 20\n",
    "):\n",
    "    if len(endpoint_config_name) <= 63:\n",
    "        print(f\"Creating Endpoint Config: {endpoint_config_name} ...\")\n",
    "        try:\n",
    "            epc_response = sm_client.create_endpoint_config(\n",
    "                EndpointConfigName=endpoint_config_name,\n",
    "                ProductionVariants=[\n",
    "                    {\n",
    "                        \"ModelName\": model_name,\n",
    "                        \"VariantName\": \"AllTraffic\",\n",
    "                        \"ServerlessConfig\": {\n",
    "                            \"MemorySizeInMB\": memory,\n",
    "                            \"MaxConcurrency\": max_concurrency,\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            status_code = epc_response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "            epc_arn = epc_response[\"EndpointConfigArn\"]\n",
    "            return (status_code, epc_arn)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating EndpointConfig: {endpoint_config_name}\")\n",
    "            print(f\"{e}\")\n",
    "    else:\n",
    "        print(f\"EndpointConfig name exceeds allowed 63 char limit\")\n",
    "        raise ValueError(\"EndpointConfig name cannot exceed 63 chars.\")\n",
    "\n",
    "\n",
    "def create_serverless_endpoint(sm_client, endpoint_name, endpoint_config_name):\n",
    "    if len(endpoint_config_name) <= 63:\n",
    "        print(f\"Creating Serverless Endpoint: {endpoint_name} ...\")\n",
    "        try:\n",
    "            ep_response = sm_client.create_endpoint(\n",
    "                EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "            )\n",
    "            status_code = ep_response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "            return status_code\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Endpoint: {endpoint_name}\")\n",
    "            print(f\"{e}\")\n",
    "    else:\n",
    "        print(f\"Endpoint name exceeds allowed 63 char limit\")\n",
    "        raise ValueError(\"Endpoint name cannot exceed 63 chars.\")\n",
    "\n",
    "\n",
    "def get_s3_objsize_in_MB(bucket, key):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    resp = s3.head_object(Bucket=bucket, Key=key)[\"ContentLength\"]\n",
    "    size = round(resp / (1024 * 1024))\n",
    "    if size < 1:\n",
    "        print(f\"Model Size: ~ {round(resp / 1024)} KB\")\n",
    "    else:\n",
    "        print(f\"Model Size: ~ {size} MB\")\n",
    "\n",
    "    return size\n",
    "\n",
    "\n",
    "def set_serverless_endpoint_memory(model_size: int):\n",
    "    if model_size <= 1024:\n",
    "        return 1024\n",
    "    elif model_size > 1024 and model_size <= 2048:\n",
    "        return 2048\n",
    "    elif model_size > 2048 and model_size <= 3072:\n",
    "        return 3072\n",
    "    elif model_size > 3072 and model_size <= 4096:\n",
    "        return 4096\n",
    "    elif model_size > 4096 and model_size <= 5120:\n",
    "        return 5120\n",
    "    elif model_size > 5120 and model_size <= 6144:\n",
    "        return 6144\n",
    "    elif model_size > 6144:\n",
    "        raise ValueError(\"Model size is greater than 6GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify model size and create serverless endpoint configuration accordingly\n",
    "\n",
    ">Serverless Inference auto-assigns compute resources proportional to the memory you select. \n",
    "If you choose a larger memory size, your container has access to more `vCPUs`. Choose your endpoint’s memory size according to your model size. \n",
    "Generally, the memory size should be at least as large as your model size. \n",
    "\n",
    "Ref: <https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-create.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.describe_auto_ml_job(AutoMLJobName=ens_automl_job_name)\n",
    "inference_container = response[\"BestCandidate\"][\"InferenceContainers\"][0]\n",
    "print(f\"Inference Container for AutoML job: {ens_automl_job_name}\")\n",
    "print(inference_container)\n",
    "\n",
    "# Verify generated model size before creating endpoint config.\n",
    "# Extract s3 Key from ModelDataUrl\n",
    "model_dataurl_key = inference_container[\"ModelDataUrl\"].split(f\"{bucket}\")[1][1:]\n",
    "ens_model_size = get_s3_objsize_in_MB(bucket, model_dataurl_key)\n",
    "print(f\"Ensemble Model Size: ~ {ens_model_size}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set serverless endpoint config `MemorySize` and `MaxConcurrency`. Generally, the memory size should be **at least** as large as your model size. \n",
    "\n",
    "We'll choose `4096` (4 GB) for endpoint memory size and set `MaxConcurrency` to 10.\n",
    "\n",
    "Your serverless endpoint has a minimum RAM size of **1024 MB (1 GB)**, and the maximum RAM size you can choose is **6144 MB (6 GB)**\n",
    "\n",
    "If you don't specify any Memory `2048` (2 GB) is chosen as default. The memory sizes you can choose are 1024 MB, 2048 MB, 3072 MB, 4096 MB, 5120 MB, or 6144 MB.\n",
    "\n",
    "Ref: <https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html#serverless-endpoints-how-it-works-memory>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list()\n",
    "# create model\n",
    "(status, model_arn) = create_autopilot_model(\n",
    "    sm_client, ens_automl_job_name, role, inference_container, 0\n",
    ")\n",
    "model_name = model_arn.split(\"/\")[1]\n",
    "models.append(model_name)\n",
    "\n",
    "endpoint_configs = list()\n",
    "endpoint_config_name = f\"epc-{model_name}\"\n",
    "memory = 4096\n",
    "# create endpoint config\n",
    "(status, epc_arn) = create_serverless_endpoint_config(\n",
    "    sm_client, endpoint_config_name, model_name, memory=memory, max_concurrency=10\n",
    ")\n",
    "endpoint_configs.append(endpoint_config_name)\n",
    "\n",
    "endpoints = list()\n",
    "endpoint_name = endpoint_config_name.replace(\"epc-\", \"ep-\")\n",
    "# create serverless endpoint\n",
    "create_serverless_endpoint(sm_client, endpoint_name, endpoint_config_name)\n",
    "endpoints.append(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for endpoint status to be `InService`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_endpoint_status(sm_client, endpoint_name):\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    desc = f\"{endpoint_name} | {status} ...\"\n",
    "    return (status, desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor endpoint status\n",
    "(status, desc) = get_endpoint_status(sm_client, endpoint_name)\n",
    "print(desc)\n",
    "while status not in (\"InService\", \"Failed\"):\n",
    "    (status, desc) = get_endpoint_status(sm_client, endpoint_name)\n",
    "    if status not in (\"InService\", \"Failed\"):\n",
    "        print(desc)\n",
    "        sleep(60)\n",
    "        continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Inference request to serverless endpoint with ENSEMBLE model\n",
    "\n",
    ">**NOTE:** Serverless endpoints, being fully-managed, provision compute resources on demand, as a result your endpoint may experience cold starts. Typically, you'll experience a cold start during the first inference request and after a brief period of inactivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "\n",
    "payload = \"51,technician,married,professional.course,no,yes,no,cellular,apr,thu,687,1,0,1,success,-1.8,93.075,-47.1,1.365,5099.1\"\n",
    "# payload = \"42,services,married,professional.course,no,yes,no,telephone,may,thu,813,1,999,0,nonexistent,1.1,93.994,-36.4,4.855,5191.0\"\n",
    "# payload = \"37,services,married,high.school,no,yes,no,telephone,may,mon,226,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\"\n",
    "# payload = \"55,admin.,married,high.school,no,no,no,telephone,may,thu,94,1,999,0,nonexistent,1.1,93.994,-36.4,4.855,5191.0\"\n",
    "# payload = \"34,blue-collar,married,basic.4y,no,no,no,telephone,may,tue,800,4,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\"\n",
    "\n",
    "try:\n",
    "    print(f\"Invoking endpoint: {endpoint} with payload .. \\n\")\n",
    "    print(payload)\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint,\n",
    "        sagmaker_session=session,\n",
    "        serializer=CSVSerializer(),\n",
    "        deserializer=CSVDeserializer(),\n",
    "    )\n",
    "    prediction = predictor.predict(payload)\n",
    "except Exception as e:\n",
    "    print(f\"Error invoking Endpoint: {endpoint}\")\n",
    "    print(f\"{e}\")\n",
    "    pass\n",
    "\n",
    "print(f\"\\nPredicted Label: {prediction[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup (ensemble endpoint)\n",
    "Delete endpoint, endpoint config and model in that order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epc_name = endpoint.replace(\"ep-\", \"epc-\")\n",
    "model_name = endpoint.replace(\"ep-\", \"\")\n",
    "\n",
    "print(f\"Deleting endpoint : {endpoint}\")\n",
    "try:\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "    pass\n",
    "\n",
    "print(f\"Deleting EndpointConfig : {epc_name}\")\n",
    "try:\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=epc_name)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "    pass\n",
    "\n",
    "print(f\"Deleting Model : {model_name}\")\n",
    "try:\n",
    "    sm_client.delete_model(ModelName=model_name)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy HPO models to serverless endpoints\n",
    "\n",
    "Autopilot in HYPERPARAMETER_TUNING mode generates 3 inference containers for binary classification problem types.\n",
    "\n",
    "Ref: <https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development-container-output.html#autopilot-problem-type-container-output>\n",
    "\n",
    "![](./images/ap-jobprofile-hpo-04Oct2022.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor HPO AutoML job completion status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor job status launched in hpo mode\n",
    "(p_status, desc) = get_job_status(sm_client, hpo_automl_job_name)\n",
    "print(desc)\n",
    "while p_status not in (\"Completed\", \"Failed\"):\n",
    "    (p_status, desc) = get_job_status(sm_client, hpo_automl_job_name)\n",
    "    if p_status not in (\"Completed\", \"Failed\"):\n",
    "        print(desc)\n",
    "        sleep(60)\n",
    "        continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll enumerate through `InferenceContainers` list from `BestCandidate` HPO Model and create endpoints accordingly\n",
    "\n",
    "- Step 1 : Create Model\n",
    "- Step 2 : Create Endpoint Config with Model Name\n",
    "- Step 3 : Create Endpoint with Endpoint Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_response = sm_client.describe_auto_ml_job(AutoMLJobName=hpo_automl_job_name)\n",
    "inference_containers = job_response[\"BestCandidate\"][\"InferenceContainers\"]\n",
    "print(f\"Inference Containers for AutoML job: {hpo_automl_job_name}\")\n",
    "print(inference_containers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get model sizes of generated inference containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, container in enumerate(inference_containers):\n",
    "    print(f\"calculating generated model_{idx} size\")\n",
    "    # Extract s3 Key from ModelDataUrl\n",
    "    model_dataurl_key = container[\"ModelDataUrl\"].split(f\"{bucket}\")[1][1:]\n",
    "    # print(model_dataurl_key)\n",
    "    model_size = get_s3_objsize_in_MB(bucket, model_dataurl_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All generated models are less than 1 MB. \n",
    "Let's set `MemorySize` to **2048 MB** and `MaxConcurrency` to **10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list()\n",
    "endpoint_configs = list()\n",
    "endpoints = list()\n",
    "\n",
    "memory = 2048\n",
    "max_concurreny = 10\n",
    "\n",
    "# Create model, endpoint_config, endpoint and store them in lists for easier access\n",
    "for idx, container in enumerate(inference_containers):\n",
    "    (status, model_arn) = create_autopilot_model(\n",
    "        sm_client, hpo_automl_job_name, role, container, idx\n",
    "    )\n",
    "    model_name = model_arn.split(\"/\")[1]\n",
    "    print(f\"\\tcreated model: {model_name}...\")\n",
    "    models.append(model_name)\n",
    "\n",
    "    endpoint_config_name = f\"epc-{model_name}\"\n",
    "    endpoint_name = f\"ep-{model_name}\"\n",
    "\n",
    "    (status, epc_arn) = create_serverless_endpoint_config(\n",
    "        sm_client, endpoint_config_name, model_name, memory=memory, max_concurrency=max_concurreny\n",
    "    )\n",
    "    print(f\"\\tcreated epc: {endpoint_config_name}\")\n",
    "    endpoint_configs.append(endpoint_config_name)\n",
    "\n",
    "    res = create_serverless_endpoint(sm_client, endpoint_name, endpoint_config_name)\n",
    "    print(f\"\\tcreated ep: {endpoint_name}\")\n",
    "    endpoints.append(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor Endpoint creation status\n",
    "Wait till all Endpoints are in `InService` status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = [get_endpoint_status(sm_client, ep)[0] for ep in endpoints]\n",
    "print(statuses)\n",
    "\n",
    "while statuses != [\"InService\", \"InService\", \"InService\"]:\n",
    "    statuses = [get_endpoint_status(sm_client, ep)[0] for ep in endpoints]\n",
    "    print(statuses)\n",
    "    if statuses != [\"InService\", \"InService\", \"InService\"]:\n",
    "        sleep(60)\n",
    "        continue\n",
    "    else:\n",
    "        print(statuses)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send inference request to get predictions from each endpoint\n",
    "\n",
    "Inference request flow:\n",
    "\n",
    "![](./images/ap-hpo-serverless-payloadflow.png)\n",
    "\n",
    ">NOTE: Serverless endpoints, being fully-managed, provision compute resources on demand, as a result the endpoint may experience cold starts. Typically, you'll experience a cold start during the first inference request and after a brief period of inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "# payload = \"51,technician,married,professional.course,no,yes,no,cellular,apr,thu,687,1,0,1,success,-1.8,93.075,-47.1,1.365,5099.1\"\n",
    "# payload = \"42,services,married,professional.course,no,yes,no,telephone,may,thu,813,1,999,0,nonexistent,1.1,93.994,-36.4,4.855,5191.0\"\n",
    "# payload = \"37,services,married,high.school,no,yes,no,telephone,may,mon,226,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\"\n",
    "# payload = \"55,admin,married,high.school,no,no,no,telephone,may,thu,94,1,999,0,nonexistent,1.1,93.994,-36.4,4.855,5191.0\"\n",
    "payload = \"34,blue-collar,married,basic.4y,no,no,no,telephone,may,tue,800,4,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\"\n",
    "# payload = \"100,services,married,high.school,no,yes,no,cellular,apr,thu,483,2,999,0,nonexistent,-1.8,93.075,-47.1,1.41,5099.1\"\n",
    "\n",
    "for _, ep in enumerate(endpoints):\n",
    "    try:\n",
    "        print(f\"Invoking Endpoint {_}: {ep} ... \\n\")\n",
    "        predictor = Predictor(\n",
    "            endpoint_name=ep,\n",
    "            sagemaker_session=session,\n",
    "            serializer=CSVSerializer(),\n",
    "            deserializer=CSVDeserializer(),\n",
    "        )\n",
    "        payload = predictor.predict(payload)\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Endpoint; {ep} \\n {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"Final Prediction: {payload[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (HPO endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deleting endpoints...\")\n",
    "for _, ep in enumerate(endpoints):\n",
    "    try:\n",
    "        print(f\"\\tDeleting {ep}...\")\n",
    "        sm_client.delete_endpoint(EndpointName=ep)\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        continue\n",
    "print(\"--\" * 15)\n",
    "print(\"Deleting endpoint configs...\")\n",
    "for (_, epc) in enumerate(endpoint_configs):\n",
    "    try:\n",
    "        print(f\"\\tDeleting {epc} ...\")\n",
    "        sm_client.delete_endpoint_config(EndpointConfigName=epc)\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        continue\n",
    "print(\"--\" * 15)\n",
    "print(\"Deleting models...\")\n",
    "for (_, mdl) in enumerate(models):\n",
    "    try:\n",
    "        print(f\"\\tDeleting {mdl}...\")\n",
    "        sm_client.delete_model(ModelName=mdl)\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Done\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_blog",
   "language": "python",
   "name": "conda_blog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c4c57eb149902836539fe532ea353cbda55dc8653105f24c3221170071603b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
