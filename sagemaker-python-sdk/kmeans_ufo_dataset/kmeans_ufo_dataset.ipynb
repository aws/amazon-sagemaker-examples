{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Ali Parandeh - Beginners Machine Learning - London\n",
    "\n",
    "# Introduction to Unsupervised Machine Learning with AWS Sagemaker\n",
    "In this interesting 3hr workshop, you will take the massive dataset of UFO sightings (80,000 reports over the past century) from [National UFO Reporting Center (NUFORC)](http://www.nuforc.org/) and use Amazon's machine learning services ([AWS Sagemaker](https://aws.amazon.com/sagemaker/)) to identify the top 10 locations that are most likely to have UFO sightings. To do so, you will need to use an unsupervised machine learning algorithm.\n",
    "\n",
    "You will then take your trained model, deserialise it, convert its output to a csv format and visualise it on a map using AWS [Quicksight](https://aws.amazon.com/quicksight/) to see where these locations are. Then you can try correlating these locations with landmarks.\n",
    "\n",
    "The general machine learning workflow with AWS Sagemaker is shown below. For this assignment we will not evaluate or deploy the model but only use its output to visualise the results on a world map.\n",
    "\n",
    "<img src=\"https://docs.aws.amazon.com/sagemaker/latest/dg/images/ml-concepts-10.png\">\n",
    "\n",
    "### What is Unsupervised Machine Learning? \n",
    "\n",
    "With unsupervised learning, data features are fed into the learning algorithm, which determines how to label them (usually with numbers 0,1,2..) and based on what. This “based on what” part dictates which unsupervised learning algorithm to follow.\n",
    "\n",
    "Most unsupervised learning-based applications utilize the sub-field called **Clustering**. \n",
    "\n",
    "One of the most famous topics under the realm of Unsupervised Learning in Machine Learning is k-Means Clustering. Even though this clustering algorithm is fairly simple, it can look challenging to newcomers into the field. \n",
    "\n",
    "### What is the difference between supervised and unsupervised machine learning?\n",
    "\n",
    "The main difference between Supervised and Unsupervised learning algorithms is the absence of data labels in the latter.\n",
    "\n",
    "### What does clustering mean?\n",
    "\n",
    "**Clustering** is the process of grouping data samples together into clusters based on a certain feature that they share — exactly the purpose of unsupervised learning in the first place.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*tWaaZX75oumVwBMcKN-eHA.png\">\n",
    "\n",
    "Source: [Clustering using K-means algorithm](https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6)\n",
    "\n",
    "### How does the K-Means Algorithm work?\n",
    "\n",
    "Being a clustering algorithm, **k-Means** takes data points as input and groups them into `k` clusters. This process of grouping is the training phase of the learning algorithm. The result would be a model that takes a data sample as input and returns the cluster that the new data point belongs to, according the training that the model went through.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*6EOTS1IE2ULWC9SKgf7mYw.png\">\n",
    "\n",
    "Source - [How Does k-Means Clustering in Machine Learning Work?](https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*4LOxZL6bFl3rXlr2uCiKlQ.gif\">\n",
    "\n",
    "Source: [How Does k-Means Clustering in Machine Learning Work?](https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0)\n",
    "\n",
    "Check out the the two articles below to learn more about how the K-Means Algorithm work:\n",
    "\n",
    "- [Clustering using K-means algorithm](https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6)\n",
    "- [How Does k-Means Clustering in Machine Learning Work?](https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0)\n",
    "\n",
    "\n",
    "### Where can you use k-means?\n",
    "\n",
    "The **k-means algorithm** can be a good fit for finding patterns or groups in large datasets that have not been explicitly labeled. Here are some example use cases in different domains:\n",
    "\n",
    "**E-commerce**\n",
    "\n",
    "- Classifying customers by purchase history or clickstream activity.\n",
    "\n",
    "**Healthcare**\n",
    "\n",
    "- Detecting patterns for diseases or success treatment scenarios.\n",
    "- Grouping similar images for image detection.\n",
    "\n",
    "**Finance**\n",
    "\n",
    "- Detecting fraud by detecting anomalies in the dataset. For example, detecting credit card frauds by abnormal purchase patterns.\n",
    "\n",
    "**Technology**\n",
    "\n",
    "- Building a network intrusion detection system that aims to identify attacks or malicious activity.\n",
    "\n",
    "**Meteorology**\n",
    "\n",
    "- Detecting anomalies in sensor data collection such as storm forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the assignment, we need to import the following packages: \n",
    "\n",
    "- **Amazon SageMaker Python SDK**: Amazon SageMaker Python SDK is an open source library for training and deploying machine-learned models on Amazon SageMaker. See [Documentation](https://sagemaker.readthedocs.io/en/stable/index.html)\n",
    "- **Python Built-in Library** [datetime](https://docs.python.org/2/library/datetime.html)\n",
    "- **Numpy** and **Pandas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the above packages below\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Construct a url to the the dataset location in your S3 bucket using the following expression and save it to `data_location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://YOUR_OWN_UNIQUE_BUCKET_NAME/ufo_dataset/ufo_complete.csv\n"
     ]
    }
   ],
   "source": [
    "# TODO: Construct the url path to your dataset file that you have just uploaded to your newly created S3 bucket\n",
    "bucket = \"YOUR_OWN_UNIQUE_BUCKET_NAME\"\n",
    "prefix = \"ufo_dataset\"\n",
    "data_key = \"ufo_complete.csv\"\n",
    "\n",
    "# Construct a url string and save it to data_location variable\n",
    "data_location = \"s3://{}/{}/{}\".format(bucket, prefix, data_key)\n",
    "\n",
    "# print data_location\n",
    "print(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internally do not process the file in chunks when loading the csv onto a dataframe \n",
    "# to ensure avoid mixed type inferences when importing the large UFO dataset. \n",
    "df = pd.read_csv(data_location, low_memory= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88870</th>\n",
       "      <td>09/09/2013 22:00</td>\n",
       "      <td>napa</td>\n",
       "      <td>ca</td>\n",
       "      <td>us</td>\n",
       "      <td>other</td>\n",
       "      <td>1200</td>\n",
       "      <td>hour</td>\n",
       "      <td>Napa UFO&amp;#44</td>\n",
       "      <td>9/30/2013</td>\n",
       "      <td>38.2972222</td>\n",
       "      <td>-122.284444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88871</th>\n",
       "      <td>09/09/2013 22:20</td>\n",
       "      <td>vienna</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>5</td>\n",
       "      <td>5 seconds</td>\n",
       "      <td>Saw a five gold lit cicular craft moving fastl...</td>\n",
       "      <td>9/30/2013</td>\n",
       "      <td>38.9011111</td>\n",
       "      <td>-77.265556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88872</th>\n",
       "      <td>09/09/2013 23:00</td>\n",
       "      <td>edmond</td>\n",
       "      <td>ok</td>\n",
       "      <td>us</td>\n",
       "      <td>cigar</td>\n",
       "      <td>1020</td>\n",
       "      <td>17 minutes</td>\n",
       "      <td>2 witnesses 2  miles apart&amp;#44 Red &amp;amp; White...</td>\n",
       "      <td>9/30/2013</td>\n",
       "      <td>35.6527778</td>\n",
       "      <td>-97.477778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88873</th>\n",
       "      <td>09/09/2013 23:00</td>\n",
       "      <td>starr</td>\n",
       "      <td>sc</td>\n",
       "      <td>us</td>\n",
       "      <td>diamond</td>\n",
       "      <td>0</td>\n",
       "      <td>2 nights</td>\n",
       "      <td>On September ninth my wife and i noticed stran...</td>\n",
       "      <td>9/30/2013</td>\n",
       "      <td>34.3769444</td>\n",
       "      <td>-82.695833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88874</th>\n",
       "      <td>09/09/2013 23:30</td>\n",
       "      <td>ft. lauderdale</td>\n",
       "      <td>fl</td>\n",
       "      <td>us</td>\n",
       "      <td>oval</td>\n",
       "      <td>0</td>\n",
       "      <td>still occuring</td>\n",
       "      <td>Hovering object lit with red and white lights&amp;...</td>\n",
       "      <td>9/30/2013</td>\n",
       "      <td>26.1219444</td>\n",
       "      <td>-80.143611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               datetime            city state country    shape  \\\n",
       "88870  09/09/2013 22:00            napa    ca      us    other   \n",
       "88871  09/09/2013 22:20          vienna    va      us   circle   \n",
       "88872  09/09/2013 23:00          edmond    ok      us    cigar   \n",
       "88873  09/09/2013 23:00           starr    sc      us  diamond   \n",
       "88874  09/09/2013 23:30  ft. lauderdale    fl      us     oval   \n",
       "\n",
       "      duration (seconds) duration (hours/min)  \\\n",
       "88870               1200                 hour   \n",
       "88871                  5            5 seconds   \n",
       "88872               1020           17 minutes   \n",
       "88873                  0             2 nights   \n",
       "88874                  0       still occuring   \n",
       "\n",
       "                                                comments date posted  \\\n",
       "88870                                       Napa UFO&#44   9/30/2013   \n",
       "88871  Saw a five gold lit cicular craft moving fastl...   9/30/2013   \n",
       "88872  2 witnesses 2  miles apart&#44 Red &amp; White...   9/30/2013   \n",
       "88873  On September ninth my wife and i noticed stran...   9/30/2013   \n",
       "88874  Hovering object lit with red and white lights&...   9/30/2013   \n",
       "\n",
       "         latitude   longitude  \n",
       "88870  38.2972222 -122.284444  \n",
       "88871  38.9011111  -77.265556  \n",
       "88872  35.6527778  -97.477778  \n",
       "88873  34.3769444  -82.695833  \n",
       "88874  26.1219444  -80.143611  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the tail of the dataframe\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88875, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clearning, transforming and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select the 'latitude' and 'longitude' columns and save it as a new dataframe `df_geo` with .copy().\n",
    "df_geo = df[[\"latitude\", \"longitude\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88870</th>\n",
       "      <td>38.2972222</td>\n",
       "      <td>-122.284444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88871</th>\n",
       "      <td>38.9011111</td>\n",
       "      <td>-77.265556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88872</th>\n",
       "      <td>35.6527778</td>\n",
       "      <td>-97.477778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88873</th>\n",
       "      <td>34.3769444</td>\n",
       "      <td>-82.695833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88874</th>\n",
       "      <td>26.1219444</td>\n",
       "      <td>-80.143611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         latitude   longitude\n",
       "88870  38.2972222 -122.284444\n",
       "88871  38.9011111  -77.265556\n",
       "88872  35.6527778  -97.477778\n",
       "88873  34.3769444  -82.695833\n",
       "88874  26.1219444  -80.143611"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the tail of df_geo\n",
    "df_geo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 88875 entries, 0 to 88874\n",
      "Data columns (total 2 columns):\n",
      "latitude     88875 non-null object\n",
      "longitude    88875 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Fully inspect the df_geo dataframe\n",
    "df_geo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successfull inspection of the above dataframe, you should note the following with this dataframe:\n",
    "\n",
    "- There are no `null` or missing values in both columns. However, we still need to check for other incorrect entries that are not **coordinates**. Examples include: `0`, `string`s, etc.\n",
    "- The `latitude` column has a `dtype` of `object`. This means the column may have missing or string values where the rest of the values are numbers. If the entries in the column are non-homogenous, pandas will store the column as a `string` or `object` data type. To clean the data in this column we can use pandas' `pd.to_numeric()` method to convert the data in this column to `float` for processing. The machine learning algorithm expects the data passed in to it to be numerical digits `float`s or `int`s not `string`s. - See [Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) on how to use this method.\n",
    "\n",
    "> **Exercise:** Convert the `latitude` column's datatype to `float`. You can pass in the `errors = \"coerce\"` option to `pd.to_numeric()` method to enforce the conversion. When conversion is not possible - i.e. values are `string`s - these strings will be replaced with `NaNs`. Therefore, you need to use a `.dropna()` method to drop rows where `NaNs` exist. Then check whether the column formats have been converted to numerical data types `float` and if any missing values are still present. **Note**: You can pass in `inplace = True` argument to `.dropna()` methods so that operations are performed in place and to avoid re-assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Convert the column values to numeric and whenever this is not possible replace the value with NaNs\n",
    "df_geo[\"latitude\"] = pd.to_numeric(df_geo.latitude, errors = \"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in the dataframe before dropping rows is 1\n",
      "Number of null values in the dataframe before dropping rows is 0\n"
     ]
    }
   ],
   "source": [
    "# Count the number of null values in the dataframe - Expecting this to be non-zero\n",
    "print(\"Number of null values in the dataframe before dropping rows is {}\".format(df_geo.isnull().any().sum()))\n",
    "\n",
    "# Drop all rows that NaN Values\n",
    "df_geo.dropna(inplace=True)\n",
    "\n",
    "# Count the number of null values in the dataframe - Expecting this to be zero\n",
    "print(\"Number of null values in the dataframe before dropping rows is {}\". format(df_geo.isnull().any().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latitude     1494\n",
      "longitude    1494\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count how many rows in the df have 0 values\n",
    "print(df_geo[(df_geo.longitude == 0) | (df_geo.latitude == 0) ].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows that have non-zero coordinate values and re-assign the selection to df_geo\n",
    "df_geo = df_geo[(df_geo.longitude != 0) &(df_geo.latitude != 0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [latitude, longitude]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check that the there are no coordinate values in the df_geo dataframe with 0\n",
    "print(df_geo[(df_geo.longitude == 0) &(df_geo.latitude == 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 87184 entries, 0 to 88874\n",
      "Data columns (total 2 columns):\n",
      "latitude     87184 non-null float64\n",
      "longitude    87184 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Re-checking the dataframe to ensure both columns have numerical datatype such as `float` or `int`.\n",
    "df_geo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any missing values? False\n"
     ]
    }
   ],
   "source": [
    "# Check if we have any missing values (NaNs) in our dataframe\n",
    "missing_values = df_geo.isnull().values.any()\n",
    "print(\"Are there any missing values? {}\".format(missing_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are any missing values in the dataframe, show them\n",
    "if (missing_values):\n",
    "    df_geo[df_geo.isnull().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 29.883055, -97.94111 ],\n",
       "       [ 29.38421 , -98.581085],\n",
       "       [ 53.2     ,  -2.916667],\n",
       "       ...,\n",
       "       [ 35.65278 , -97.477776],\n",
       "       [ 34.376945, -82.69583 ],\n",
       "       [ 26.121944, -80.14361 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the cleaned up dataframe column values as a 2D numpy array (matrix) with datatype of float32\n",
    "data_train = df_geo.values.astype(\"float32\")\n",
    "\n",
    "# Print the 2D numpy array\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualising the last 5000 reports of the data on the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the useful packages for visualising the data on a map is called **plotly**. \n",
    "\n",
    "We can import the following module from plotly package as `px`:\n",
    "\n",
    "- **plotly**'s [express](https://plot.ly/python/plotly-express/) - Plotly Express is a terse, consistent, high-level wrapper around `plotly.graph_objects` for rapid data exploration and figure generation.\n",
    "\n",
    "For data available as a tidy pandas DataFrame, we can use the Plotly Express function `px.scatter_geo` for a geographical scatter plot. The `color` argument is used to set the color of the markers from a given column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Showing only the last 5000 rows only on a map\n",
    "fig = px.scatter_geo(df_geo.iloc[-5000: -1, :], lat=\"latitude\", lon = \"longitude\", \n",
    "                     title=\"UFO Reports by Latitude/Longitude in the world - Last 5000 Reports\", color = \"longitude\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/LeJzFHj.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that most of the recent 5000 UFO reports have been located in the United States. Let's take a closer look at United States by using `plotly`'s `geo` layout feature to show sightings on the US map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import iplot\n",
    "\n",
    "data = [dict(\n",
    "        type = 'scattergeo',\n",
    "        locationmode = 'USA-states',\n",
    "        lat = df_geo.iloc[-5000:-1, 0],\n",
    "        lon = df_geo.iloc[-5000:-1, 1],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 5.5,\n",
    "            opacity = 0.75,\n",
    "            color = 'rgb(0, 163, 81)',\n",
    "            line = dict(color = 'rgb(255, 255, 255)', width = 1))\n",
    "        )]\n",
    "\n",
    "layout = dict(\n",
    "         title = 'UFO Reports by Latitude/Longitude in United States - Last 5000 Reports',\n",
    "         geo = dict(\n",
    "             scope = 'usa',\n",
    "             projection = dict(type = 'albers usa'),\n",
    "             showland = True,\n",
    "             landcolor = 'rgb(250, 250, 250)',\n",
    "             subunitwidth = 1,\n",
    "             subunitcolor = 'rgb(217, 217, 217)',\n",
    "             countrywidth = 1,\n",
    "             countrycolor = 'rgb(217, 217, 217)',\n",
    "             showlakes = True,\n",
    "             lakecolor = 'rgb(255, 255, 255)')\n",
    "        )\n",
    "\n",
    "figure = dict(data = data, layout = layout)\n",
    "iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/oIQQVIQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of clusters and output location URL to save the trained model\n",
    "num_clusters = 10\n",
    "output_location = \"s3://\" + bucket + \"/model-artifacts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass a training command to Amazon Sagemaker, we need to grab the details of the current execution role **ARN ID** whose credentials we are using to call the Sagemaker API. \n",
    "\n",
    "> **Exercise:** Grab the ARN ID of your current Execution role using the `sagemaker` SDK - See [Documentation](https://sagemaker.readthedocs.io/en/stable/session.html?highlight=get%20execution#sagemaker.session.get_execution_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the execution role ARN ID to pass to the sagemaker API later on\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Check that you have this step correctly performed\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can use Amazon's built-in K-means ML algorithm to find `k` clusters of data in our unlabeled UFO dataset.\n",
    "\n",
    "Amazon SageMaker uses a modified version of the web-scale k-means clustering algorithm. Compared with the original version of the algorithm, the version used by Amazon SageMaker is more accurate. Like the original algorithm, it scales to massive datasets and delivers improvements in training time. To do this, the version used by Amazon SageMaker streams mini-batches (small, random subsets) of the training data. The k-means algorithm expects tabular data, where rows represent the observations that you want to cluster, and the columns represent attributes of the observations. See [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html)\n",
    "\n",
    "To ask AWS sagemaker for training a model using this algorithm we need to define a **K-means Estimator**. KMeans estimators can be configured by setting **hyperparameters**. These hyperparameters are arguments passed into the estimator's Constructor Function. \n",
    "\n",
    "This estimator requires the following hyperparameters to be passed in `sagemaker.KMeans()`:\n",
    "\n",
    "- `role` (str) – An AWS IAM role (either name or full ARN)\n",
    "- `train_instance_count` (int) – Number of Amazon EC2 instances to use for training. We only need 1 for this exercise.\n",
    "- `train_instance_type` (str) – Type of EC2 instance to use for training, for example, ‘ml.c4.xlarge’. This is the **compute resources** that you want Amazon SageMaker to use for model training. Compute resources are ML compute instances that are managed by Amazon SageMaker.\n",
    "- `k` (int) – The number of clusters to produce. We need to 10 for this exercise.\n",
    "- `output_path` (str) - The URL of the S3 bucket where you want to store the output of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the training API request to AWS Sagemaker\n",
    "kmeans = sagemaker.KMeans(role = role,\n",
    "               train_instance_count = 1,\n",
    "               train_instance_type = \"ml.c4.xlarge\",\n",
    "               output_path = output_location,\n",
    "               k = num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram shows how you train and deploy a model with Amazon SageMakern - See [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) For this assignment, Amazon SageMaker provides training algorithms that are great out-of-the-box solution for quick model training.  We have used some helper code above to clean and prepare the dataset and configure AWS Sagemaker API calls but do not need to specify training code or even a training code image from EC2 Container Registry. We only need to pass in the dataset for training with AWS's KMeans default algorithm. If we wanted to specify our own algorithms or use one of the popular deep learning frameworks - tensorflow/etc. - then we provide additional training code.\n",
    "\n",
    "<img src=\"https://docs.aws.amazon.com/sagemaker/latest/dg/images/sagemaker-architecture-training-2.png\">\n",
    "\n",
    "To train a model in Amazon SageMaker, you create a **training job** using the `kmeans.fit()` method. - See [Documentation](https://sagemaker.readthedocs.io/en/stable/kmeans.html?highlight=kmeans.fit#sagemaker.KMeans.fit)\n",
    "\n",
    "The training job requires the following information passed in to `.fit()` method:\n",
    "\n",
    "- `record_set(data_train)` (str) - The training records to train the KMeans Estimator on. Here `data_train` must be passed in to the `kmeans.record_set()` method to convert our 2D numpy array data to a `RecordSet` object that is required by the algorithm. - See [Documentation](https://sagemaker.readthedocs.io/en/stable/sagemaker.amazon.amazon_estimator.html?highlight=record_set()#sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.record_set)\n",
    "- `job_name` (str) - Training job name. If not specified, the estimator generates a default job name, based on the training image name and current timestamp.\n",
    "\n",
    "Amazon SageMaker then launches the ML compute instances and uses the training dataset to train the model. It saves the resulting model artifacts and other output in the S3 bucket you specified for that purpose.\n",
    "\n",
    "Here we are going to construct a job name using the following expression and Python's built-in `datetime` module. This ensures our `job_name` is unique. Each training job requires a **unique** `job_name`. Otherwise, AWS will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the job name: kmeans-geo-job-20190825212305\n"
     ]
    }
   ],
   "source": [
    "# Construct a unique job_name using datetime module\n",
    "job_name = \"kmeans-geo-job-{}\".format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "\n",
    "# Print job_name\n",
    "print(\"Here is the job name: {}\".format(job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Create a training job using `kmeans.fit()`. Use the AWS documentation links above to figure out how to pass in the arguments to `kmeans.fit()` for the training job to commence. \n",
    "\n",
    "If you do this step right, you should see outputs like this appear underneath the code cell:\n",
    "\n",
    "```\n",
    "2019-07-29 00:54:46 Starting - Starting the training job...\n",
    "2019-07-29 00:54:47 Starting - Launching requested ML instances...\n",
    "2019-07-29 00:55:44 Starting - Preparing the instances for training......\n",
    "2019-07-29 00:56:24 Downloading - Downloading input data...\n",
    "2019-07-29 00:57:05 Training - Downloading the training image..\n",
    ".\n",
    ".\n",
    ".\n",
    "2019-07-29 00:57:31 Uploading - Uploading generated training model\n",
    "2019-07-29 00:57:31 Completed - Training job completed\n",
    "Billable seconds: 68\n",
    "CPU times: user 1.78 s, sys: 18.7 ms, total: 1.8 s\n",
    "Wall time: 3min 13s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-29 00:54:46 Starting - Starting the training job...\n",
      "2019-07-29 00:54:47 Starting - Launching requested ML instances...\n",
      "2019-07-29 00:55:44 Starting - Preparing the instances for training......\n",
      "2019-07-29 00:56:24 Downloading - Downloading input data...\n",
      "2019-07-29 00:57:05 Training - Downloading the training image..\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'_enable_profiler': u'false', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'_kvstore': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'true', u'epochs': u'1', u'init_method': u'random', u'local_lloyd_tol': u'0.0001', u'local_lloyd_max_iter': u'300', u'_disable_wait_to_read': u'false', u'extra_center_factor': u'auto', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'half_life_time_size': u'0', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'feature_dim': u'2', u'k': u'10', u'force_dense': u'True'}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Final configuration: {u'_tuning_objective_metric': u'', u'extra_center_factor': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'True', u'epochs': u'1', u'feature_dim': u'2', u'local_lloyd_tol': u'0.0001', u'_disable_wait_to_read': u'false', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'_enable_profiler': u'false', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'init_method': u'random', u'half_life_time_size': u'0', u'local_lloyd_max_iter': u'300', u'_kvstore': u'auto', u'k': u'10', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 WARNING 140106530510656] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Using default worker.\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Loaded iterator creator application/x-recordio-protobuf for content type ('application/x-recordio-protobuf', '1.0')\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Create Store: local\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] nvidia-smi took: 0.0251741409302 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Setting up with params: {u'_tuning_objective_metric': u'', u'extra_center_factor': u'auto', u'local_lloyd_init_method': u'kmeans++', u'force_dense': u'True', u'epochs': u'1', u'feature_dim': u'2', u'local_lloyd_tol': u'0.0001', u'_disable_wait_to_read': u'false', u'eval_metrics': u'[\"msd\"]', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'_enable_profiler': u'false', u'_num_gpus': u'auto', u'local_lloyd_num_trials': u'auto', u'_log_level': u'info', u'init_method': u'random', u'half_life_time_size': u'0', u'local_lloyd_max_iter': u'300', u'_kvstore': u'auto', u'k': u'10', u'_num_slices': u'1'}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] 'extra_center_factor' was set to 'auto', evaluated to 10.\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:21 INFO 140106530510656] number of center slices 1\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 5000, \"sum\": 5000.0, \"min\": 5000}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 5000, \"sum\": 5000.0, \"min\": 5000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 5000, \"sum\": 5000.0, \"min\": 5000}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1564361841.993677, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1564361841.993646}\n",
      "\u001b[0m\n",
      "\u001b[31m[2019-07-29 00:57:21.998] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 48, \"num_examples\": 1, \"num_bytes\": 160000}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] Iter 10: Short term msd 46.405341. Long term msd 50.694177\u001b[0m\n",
      "\u001b[31m[2019-07-29 00:57:22.253] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 254, \"num_examples\": 18, \"num_bytes\": 2789888}\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] processed a total of 87184 examples\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 87184, \"sum\": 87184.0, \"min\": 87184}, \"Total Batches Seen\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}, \"Total Records Seen\": {\"count\": 1, \"max\": 92184, \"sum\": 92184.0, \"min\": 92184}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 87184, \"sum\": 87184.0, \"min\": 87184}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1564361842.253606, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\", \"epoch\": 0}, \"StartTime\": 1564361841.998304}\n",
      "\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] #throughput_metric: host=algo-1, train throughput=341305.295323 records/second\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 WARNING 140106530510656] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] shrinking 100 centers into 10\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #0. Current mean square distance 42.058079\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #1. Current mean square distance 47.231346\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #2. Current mean square distance 42.428646\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #3. Current mean square distance 49.768463\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #4. Current mean square distance 38.967083\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #5. Current mean square distance 39.077560\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #6. Current mean square distance 38.824406\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #7. Current mean square distance 40.465004\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #8. Current mean square distance 40.437111\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] local kmeans attempt #9. Current mean square distance 42.829659\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] finished shrinking process. Mean Square Distance = 39\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] #quality_metric: host=algo-1, train msd <loss>=38.8244056702\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] predict compute msd took: 30.2583%, (0.078261 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] compute all data-center distances: inner product took: 22.9514%, (0.059362 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] gradient: cluster size  took: 18.7406%, (0.048471 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] gradient: cluster center took: 6.6421%, (0.017179 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] batch data loading with context took: 4.0785%, (0.010549 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] collect from kv store took: 3.9665%, (0.010259 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] splitting centers key-value pair took: 3.2907%, (0.008511 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] compute all data-center distances: point norm took: 3.2720%, (0.008463 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] gradient: one_hot took: 2.8511%, (0.007374 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] update state and report convergance took: 2.8434%, (0.007354 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] compute all data-center distances: center norm took: 0.8898%, (0.002301 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] predict minus dist took: 0.1355%, (0.000350 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] update set-up time took: 0.0800%, (0.000207 secs)\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] TOTAL took: 0.258642673492\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 300.4438877105713, \"sum\": 300.4438877105713, \"min\": 300.4438877105713}, \"initialize.time\": {\"count\": 1, \"max\": 37.05096244812012, \"sum\": 37.05096244812012, \"min\": 37.05096244812012}, \"model.serialize.time\": {\"count\": 1, \"max\": 0.1380443572998047, \"sum\": 0.1380443572998047, \"min\": 0.1380443572998047}, \"update.time\": {\"count\": 1, \"max\": 255.11598587036133, \"sum\": 255.11598587036133, \"min\": 255.11598587036133}, \"epochs\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"state.serialize.time\": {\"count\": 1, \"max\": 1.4908313751220703, \"sum\": 1.4908313751220703, \"min\": 1.4908313751220703}, \"_shrink.time\": {\"count\": 1, \"max\": 298.7639904022217, \"sum\": 298.7639904022217, \"min\": 298.7639904022217}}, \"EndTime\": 1564361842.556131, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1564361841.949236}\n",
      "\u001b[0m\n",
      "\u001b[31m[07/29/2019 00:57:22 INFO 140106530510656] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 671.81396484375, \"sum\": 671.81396484375, \"min\": 671.81396484375}, \"setuptime\": {\"count\": 1, \"max\": 14.061927795410156, \"sum\": 14.061927795410156, \"min\": 14.061927795410156}}, \"EndTime\": 1564361842.556472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KMeansWebscale\"}, \"StartTime\": 1564361842.55622}\n",
      "\u001b[0m\n",
      "\n",
      "2019-07-29 00:57:31 Uploading - Uploading generated training model\n",
      "2019-07-29 00:57:31 Completed - Training job completed\n",
      "Billable seconds: 68\n",
      "CPU times: user 1.78 s, sys: 18.7 ms, total: 1.8 s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TOOD: Create a training job and time it. Running this code cell will send a training job request to AWS Sagemaker\n",
    "kmeans.fit(kmeans.record_set(data_train), job_name= job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations** on building and training a model on the cloud using unsupervised machine learning algorithm and saving it! Next we are going to deserialise the model so that we can use its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Deserialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deserialise the compressed model output saved on our S3 bucket we need to import the following packages.\n",
    "\n",
    "- **Boto** is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services, such as EC2 and S3. Boto provides an easy to use, object-oriented API, as well as low-level access to AWS service. See [Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "\n",
    "> **Exercise**: Import `boto3` package, then use the AWS Python SDK boto3 to download the compressed model from the S3 bucket to a file. You will need to construct a url to the model and save it to `path_to_model` variable. Then pass `path_to_model` to the following command `boto3.resource(\"s3\").Bucket(bucket).download_file(path_to_model, file_name_to_save_to)`. - See [boto3 Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html?highlight=s3.object#S3.Client.download_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import boto3\n",
    "import boto3\n",
    "\n",
    "# Construct a url to the model. Compressed model is saved under the model-artifacts folder\n",
    "path_to_model = \"model-artifacts/\" + job_name + \"/output/model.tar.gz\"\n",
    "\n",
    "# TODO: Use the AWS Python SDK boto3 to download the compressed model output from S3 bucket onto `model.tar.gz` file.\n",
    "boto3.resource(\"s3\").Bucket(bucket).download_file(path_to_model, \"model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deserialise the compressed model output saved on our S3 bucket we need to import the following packages.\n",
    "\n",
    "- **Python's Built-in module** `os` - See [Documentation](https://docs.python.org/2/library/os.html#os.system)\n",
    "\n",
    "Python's built-in system module `os.system()` can be used to execute a shell command `tar -zxvf` on the `model.tar.gz` compressed gzipped file. This command shell helps to extract tar files out a `tar.gz` archives. The `-zxvf` flags can passed in to `os.system()` to perform the following commands: \n",
    "\n",
    "- `-z` - The file is a “gzipped” file\n",
    "- `-x` - Extract files\n",
    "- `-v` - Verbose, print the file names as they are extracted one by one\n",
    "- `-f` - Use the following tar archive for the operation\n",
    "\n",
    "\n",
    "See [Linux's tar Man Pages](https://linux.die.net/man/1/tar) for more details on the `tar` shell command. \n",
    "\n",
    "> **Exercise:** Use `os.system()` method to run the `tar` command on the compressed gzip file `model.tar.gz` with the above flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Import the required packages for deserialisation\n",
    "import os\n",
    "\n",
    "# TODO: Use Python's built-in os package to open the compressed model output\n",
    "os.system(\"tar -zxvf model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`os.system()` later can be used to execute the `unzip` shell command on `model_algo-1`. `unzip` shell command lists, tests, or extracts files from a ZIP archive. See [Linux unzip Man Pages](https://linux.die.net/man/1/unzip) for more details on the `unzip` command.\n",
    "\n",
    "> **Exercise:** Use `os.system()` method to unzip `model_algo-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Use Python's built-in os package to unzip model_algo-1 file. \n",
    "os.system(\"unzip model_algo-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the unzipped model output parameters, we need to install `mxnet` package.\n",
    "\n",
    "> **Exercise**: Use `!pip install` to install `mxnet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.5.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from mxnet) (2.20.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from mxnet) (1.17.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet) (1.23)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# TODO: Install mxnet package\n",
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the model output parameters we need to import the following package:\n",
    "\n",
    "- **MXNet**: A flexible and efficient library for deep learning. - See [Documentation](https://mxnet.apache.org/versions/master/api/python/index.html) \n",
    "\n",
    "> **Exercise**: Use `mxnet`'s `.ndarray.load()` method to load the model output parameters and assign it to `Kmeans_model_params` variable - See [Documentation](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import mxnet\n",
    "import mxnet as mx\n",
    "\n",
    "# TODO: Use mxnet to load the model parameters\n",
    "Kmeans_model_params = mx.ndarray.load(\"model_algo-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Convert the model parameters to a dataframe called `cluster_centroids_kmeans` using `pd.DataFrame()`. You can grab the model output parameters using `Kmeans_model_params[0].asnumpy()` to pass to `pd.DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    latitude   longitude\n",
      "0  35.379860 -118.177162\n",
      "1  41.521103  -74.812103\n",
      "2  51.608204    0.121513\n",
      "3 -11.612000  128.658752\n",
      "4  47.705780 -122.042778\n",
      "5  35.611134  -98.932304\n",
      "6  31.191694  -82.532051\n",
      "7  28.319733   37.477905\n",
      "8  41.149517  -87.080086\n",
      "9 -18.685837  -53.455894\n"
     ]
    }
   ],
   "source": [
    "# TODO: Convert the Kmeans_model_params to a dataframe using pandas and numpy: cluster_centroids_kmeans\n",
    "cluster_centroids_kmeans = pd.DataFrame(Kmeans_model_params[0].asnumpy())\n",
    "\n",
    "# TODO: Set the column names of the cluster_centroids_kmeans dataframe to match the df_geo column names\n",
    "cluster_centroids_kmeans.columns = df_geo.columns\n",
    "\n",
    "# Print cluster_centroids_kmeans\n",
    "print(cluster_centroids_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the content of the model output using An in-memory stream for text I/O we need to import the following package:\n",
    "\n",
    "- **Python's Built-in Package** `io` - See [Documentation](https://docs.python.org/3/library/io.html#io.StringIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'A80CA456D349AA28',\n",
       "  'HostId': 'tHzEAdFgatji4gI50xrA2L31eCImX7RQVFa1R3M3E/tdwGrAUrsoywBv74FMzoxw7X5wCwWWJ/Y=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'tHzEAdFgatji4gI50xrA2L31eCImX7RQVFa1R3M3E/tdwGrAUrsoywBv74FMzoxw7X5wCwWWJ/Y=',\n",
       "   'x-amz-request-id': 'A80CA456D349AA28',\n",
       "   'date': 'Sun, 25 Aug 2019 21:25:44 GMT',\n",
       "   'etag': '\"2477206b3fc6b0706e3cd0fde0ca6337\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"2477206b3fc6b0706e3cd0fde0ca6337\"'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Import Python's built-on package io\n",
    "import io\n",
    "\n",
    "# When a csv_buffer object is created, it is initialized using StringIO() constructor\n",
    "# Here no string is given to the StringIO() so the csv_buffer object is empty.\n",
    "csv_buffer = io.StringIO()\n",
    "\n",
    "# TODO: Use pandas .to_csv() method to weite the cluster_centroids_kmeans dataframe to a csv file\n",
    "cluster_centroids_kmeans.to_csv(csv_buffer, index = False)\n",
    "\n",
    "# TODO: Let's use Amazon S3\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "\n",
    "# Use the .Object() method to upload an object in the given `bucket`\n",
    "# Save the content of the csv_buffer file using the .put() method\n",
    "s3_resource.Object(bucket, \"results/ten_locations_kmeans.csv\").put(Body = csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly visualise where these top 10 coordinates are! We will use **AWS Quicksights** later on to for reporting these locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the top 10 locations in the world most likely to have UFO Sightings\n",
    "fig = px.scatter_geo(cluster_centroids_kmeans, lat=\"latitude\", lon = \"longitude\", \n",
    "                     title=\"Top 10 Locations in the world mostly likely to have UFO Sightings\", color = \"longitude\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/LHRIP1L.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the top locations in the US most likely to have UFO Sightings\n",
    "data = [dict(\n",
    "        type = 'scattergeo',\n",
    "        locationmode = 'USA-states',\n",
    "        lat = cluster_centroids_kmeans.iloc[:, 0],\n",
    "        lon = cluster_centroids_kmeans.iloc[:, 1],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 5.5,\n",
    "            opacity = 0.75,\n",
    "            color = 'rgb(0, 163, 81)',\n",
    "            line = dict(color = 'rgb(255, 255, 255)', width = 1))\n",
    "        )]\n",
    "\n",
    "layout = dict(\n",
    "         title = 'Top locations in the United States most likely to have UFO Sightings',\n",
    "         geo = dict(\n",
    "             scope = 'usa',\n",
    "             projection = dict(type = 'albers usa'),\n",
    "             showland = True,\n",
    "             landcolor = 'rgb(250, 250, 250)',\n",
    "             subunitwidth = 1,\n",
    "             subunitcolor = 'rgb(217, 217, 217)',\n",
    "             countrywidth = 1,\n",
    "             countrycolor = 'rgb(217, 217, 217)',\n",
    "             showlakes = True,\n",
    "             lakecolor = 'rgb(255, 255, 255)')\n",
    "        )\n",
    "\n",
    "figure = dict(data = data, layout = layout)\n",
    "iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/dJakSLk.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting findings! Now answer the following questions:\n",
    "\n",
    "- Which cities are the closest to these top 10 locations?\n",
    "- Which states in the United States are these top coordinates located in?\n",
    "- What landmarks - airports, research centres, etc. - do these coordinates correlate with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your answers here\n",
    "cities = [\"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\"]\n",
    "us_states = [\"___\", \"___\", \"___\", \"___\", \"___\", \"___\"]\n",
    "landmarks = [\"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\", \"___\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONGRATULATIONS!!!\n",
    "Well done on completing this difficult part of the assignment. All is now left for you to do is to visualise the model outputs you have saved in the `ten_locations_kmeans.csv` file in your S3 bucket on a map. Simply create an **AWS Quicksight** account and use the `my-manifest.json` file under the `quicksight` folder of [BML github repo](https://github.com/beginners-machine-learning-london/intro_to_unsupervised_ml_with_AWS_Sagemaker/tree/master/exercises/quicksight) to configure AWS Quicksight.\n",
    "\n",
    "Again, Well done on completing the above assignments! This was a hard exercise. You have learned how to use AWS Sagemaker to train an unsupervised machine learning model in the cloud. We hope that you enjoyed this **Introduction to unsupervised machine learning with AWS** Workshop. To learn more about AWS Sagemaker and machine learning in the cloud check out a few resources we have provided in our repo's [README.md](https://github.com/beginners-machine-learning-london/intro_to_unsupervised_ml_with_AWS_Sagemaker).\n",
    "\n",
    "Also make sure to join our meetup group to be informed of future workshops! [London Beginners Machine Learning Meetup](https://www.meetup.com/beginners-machine-learning-london/).\n",
    "\n",
    "And join our [slack channel](https://join.slack.com/t/beginnersmach-wlf5812/shared_invite/enQtNzAzODA4OTY3MTcyLWU2ZDMzNGU2YTQ4ZDk5ZjY3OTk1YWU2OGU5NWRmMjM1NzkwM2MwYjk5MDNhZWE1YWVmNzY1MjgzZDk4OGE1OGE) to ask questions, discuss ML with other BML community members and suggest the topics of future workshops."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
