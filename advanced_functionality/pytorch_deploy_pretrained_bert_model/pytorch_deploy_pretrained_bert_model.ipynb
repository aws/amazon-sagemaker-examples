{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa032c8b",
   "metadata": {},
   "source": [
    "# Deploy a pretrained PyTorch BERT model from Hugging Face Hub on Amazon SageMaker for sentiment analysis\n",
    "\n",
    "Sentiment analysis is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n",
    "\n",
    "BERT was trained on `BookCorpus` and English Wikipedia data, which contain 800 million words and 2,500 million words, respectively. Training BERT from scratch would be prohibitively expensive. By taking advantage of transfer learning, one can quickly fine tune BERT for another use case with a relatively small amount of training data to achieve state-of-the-art results for common NLP tasks, such as text classification and question answering.\n",
    "\n",
    "Amazon SageMaker is a fully managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models. The SageMaker Python SDK provides open source APIs and containers that make it easy to train and deploy models in Amazon SageMaker with several machine learning and deep learning frameworks.\n",
    "\n",
    "Our customers often ask for quick fine-tuning and easy deployment of their NLP models.\n",
    "\n",
    "In this notebook, you will deploy a pretrained PyTorch BERT model from Hugging Face Hub on Amazon SageMaker for sentiment analysis.\n",
    "\n",
    "You'll execute the following steps:\n",
    " - Initiate a [`Huggingface pipeline`](https://huggingface.co/transformers/main_classes/pipelines.html) and save the model and config on the local file system.\n",
    " - Tar GZIP the model and config files, and upload `model.tar.gz` to a `S3` bucket.\n",
    " - Deploy the model to a SageMaker Endpoint and make few inference requests.\n",
    " - Optional cleanup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b67037",
   "metadata": {},
   "source": [
    "## Install Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1553e29",
   "metadata": {},
   "source": [
    "If you run this notebook in SageMaker Studio, you need to make sure `ipywidgets` is installed and restart the kernel, so please uncomment the code in the next cell, and run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# import IPython\n",
    "# import sys\n",
    "\n",
    "# !{sys.executable} -m pip install ipywidgets\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)  # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27530bc3",
   "metadata": {},
   "source": [
    "Then you'll install `Transformers`, a state-of-the-art Natural Language Processing for `Jax`, `Pytorch` and `TensorFlow`.\n",
    "\n",
    "[Transformers](https://huggingface.co/transformers/) (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (`BERT`, `GPT-2`, `RoBERTa`, `XLM`, `DistilBert`, `XLNet`) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between `Jax`, `PyTorch` and `TensorFlow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467f6a0",
   "metadata": {},
   "source": [
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for the model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb29c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"sagemaker/pytorch-bert-sentiment-analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f93ca3",
   "metadata": {},
   "source": [
    "## Initiate a `Huggingface pipeline`\n",
    "\n",
    "The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the [task summary](https://huggingface.co/transformers/task_summary.html) for examples of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041533e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69ee7b",
   "metadata": {},
   "source": [
    "Save the pre-trained model on file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d7c1dc",
   "metadata": {},
   "source": [
    "## Package the pre-trained model and upload it to S3\n",
    "\n",
    "No you can see that there is a pretrained BERT model under `model` directory by listing the files in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rtlh ./model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e27641",
   "metadata": {},
   "source": [
    "Now you'll create a `model.tar.gz` file to be used by SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd model && tar czvf ../model.tar.gz *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48742f30",
   "metadata": {},
   "source": [
    "Upload the `model.tar.gz` to the bucket in S3 you previously set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fObj = open(\"model.tar.gz\", \"rb\")\n",
    "key = os.path.join(prefix, \"model.tar.gz\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(key).upload_fileobj(fObj)\n",
    "print(os.path.join(bucket, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae760728",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_data = \"s3://{}/{}\".format(bucket, key)\n",
    "pretrained_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a7c78",
   "metadata": {},
   "source": [
    "## Write the Inference Script\n",
    "\n",
    "To deploy a pretrained `PyTorch` model, you'll need to use the `PyTorch` estimator object to create a `PyTorchModel` object and set a different `entry_point`.\n",
    "\n",
    "You'll use the `PyTorchModel` object to deploy a `PyTorchPredictor`. This creates a `SageMaker` Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "An implementation of `model_fn` is required for inference script. We are going to use default implementations of `input_fn`, `predict_fn`, `output_fn` and `model_fn` defined in [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers).\n",
    "\n",
    "Here's an example of the inference script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08adb772",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14587c",
   "metadata": {},
   "source": [
    "## Create a model object\n",
    "\n",
    "You define the model object by using the SageMaker Python SDK's `PyTorchModel` and pass in the model from the `estimator` and the `entry_point`. The endpoint's entry point for inference is defined by `model_fn` as seen in the following code block that prints out `inference.py`. The function loads the model and sets it to use a GPU, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b89a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=pretrained_model_data,\n",
    "    role=role,\n",
    "    framework_version=\"1.7.1\",\n",
    "    source_dir=\"code\",\n",
    "    py_version=\"py3\",\n",
    "    entry_point=\"inference.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd0e51",
   "metadata": {},
   "source": [
    "## Deploy the model in SageMaker endpoint\n",
    "\n",
    "The arguments to the `deploy` function allow us to set the number and type of instances that will be used for the Endpoint.\n",
    "\n",
    "Here you will deploy the model to a single `ml.m5.large` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d645d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9755531",
   "metadata": {},
   "source": [
    "Since in the `input_fn` we declared that the incoming requests are json-encoded, we need to use a `json serializer`,\n",
    "To encode the incoming data into a json string. Also, we declared the return content type to be json string, we\n",
    "Need to use a `json deserializer` to parse the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d27710",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "Using few samples, you can now invoke the SageMaker endpoint to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(\"Never allow the same bug to bite you twice.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20966211",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(\n",
    "    \"The best part of Amazon SageMaker is that it makes machine learning easy.\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173aa51",
   "metadata": {},
   "source": [
    "You can also invoke the endpoint with a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(\n",
    "    [\n",
    "        \"Never allow the same bug to bite you twice.\",\n",
    "        \"The best part of Amazon SageMaker is that it makes machine learning easy.\",\n",
    "    ]\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03dc4e8",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Endpoints should be deleted when no longer in use, since (per the [SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/)) they're billed by time deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082ae5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
