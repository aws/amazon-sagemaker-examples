{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GluonCV YoloV3 training and optimizing using Neo\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Preparation](#Data-Preparation)\n",
    "  1. [Download data](#Download-data)\n",
    "  2. [Convert data into RecordIO](#Convert-data-into-RecordIO)\n",
    "  3. [Upload data to S3](#Upload-data-to-S3)\n",
    "4. [Training](#Training)\n",
    "5. [Compile the trained model using SageMaker Neo](#Compile-the-trained-model-using-SageMaker-Neo)\n",
    "6. [Deploy the compiled model and request Inferences](#Deploy-the-compiled-model-and-request-Inferences)\n",
    "7. [Delete the Endpoint](#Delete-the-Endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is an end-to-end example of GluonCV YoloV3 model training inside of Amazon SageMaker notebook and then compile the trained model using Neo runtime. In this demo, we will demonstrate how to train and to host an MXNet model on the [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/) using the YoloV3 algorithm. We will also demonstrate how to optimize this trained model using Neo.\n",
    "\n",
    "***This notebook is for demonstration purpose only. Please fine tune the training parameters based on your own dataset.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To train the YoloV3 MXNet model on Amazon SageMaker, we need to setup and authenticate the use of AWS services.\n",
    "\n",
    "To start, we need to upgrade the [SageMaker SDK for Python](https://sagemaker.readthedocs.io/en/stable/v2.html) to v2.33.0 or greater and restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/anaconda3/envs/mxnet_p36/bin/pip install --upgrade 'sagemaker>=2.33.0'\n",
    "!~/anaconda3/envs/mxnet_p36/bin/pip install --upgrade opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need an AWS account role with SageMaker access. This role is used to give SageMaker access to your data in S3. We also create a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the S3 bucket that is used for training, and storing the tranied model artifacts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket()\n",
    "folder = \"DEMO-ObjectDetection-YOLOv3-MXNet\"\n",
    "custom_code_sub_folder = folder + \"/custom-code\"\n",
    "training_data_sub_folder = folder + \"/training-data\"\n",
    "validation_data_sub_folder = folder + \"/validation-data\"\n",
    "training_output_sub_folder = folder + \"/training-output\"\n",
    "compilation_output_sub_folder = folder + \"/compilation-output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily visualize the detection outputs we also define the following function. The function visualizes the high-confidence predictions with bounding box by filtering out low-confidence detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def visualize_detection(img_file, dets, classes=[], thresh=0.6):\n",
    "    \"\"\"\n",
    "    visualize detections in one image\n",
    "    Parameters:\n",
    "    ----------\n",
    "    img_file : numpy.array\n",
    "        image, in bgr format\n",
    "    dets : numpy.array\n",
    "        yolo detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n",
    "        each row is one object\n",
    "    classes : tuple or list of str\n",
    "        class names\n",
    "    thresh : float\n",
    "        score threshold\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.image as mpimg\n",
    "    from matplotlib.patches import Rectangle\n",
    "\n",
    "    img = mpimg.imread(img_file)\n",
    "    plt.imshow(img)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    colors = dict()\n",
    "    klasses = dets[0][0]\n",
    "    scores = dets[1][0]\n",
    "    bbox = dets[2][0]\n",
    "    for i in range(len(classes)):\n",
    "        klass = klasses[i][0]\n",
    "        score = scores[i][0]\n",
    "        x0, y0, x1, y1 = bbox[i]\n",
    "        if score < thresh:\n",
    "            continue\n",
    "        cls_id = int(klass)\n",
    "        if cls_id not in colors:\n",
    "            colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "        xmin = int(x0 * width / 320)\n",
    "        ymin = int(y0 * height / 320)\n",
    "        xmax = int(x1 * width / 320)\n",
    "        ymax = int(y1 * height / 320)\n",
    "        rect = Rectangle(\n",
    "            (xmin, ymin),\n",
    "            xmax - xmin,\n",
    "            ymax - ymin,\n",
    "            fill=False,\n",
    "            edgecolor=colors[cls_id],\n",
    "            linewidth=3.5,\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "        class_name = str(cls_id)\n",
    "        if classes and len(classes) > cls_id:\n",
    "            class_name = classes[cls_id]\n",
    "        plt.gca().text(\n",
    "            xmin,\n",
    "            ymin - 2,\n",
    "            \"{:s} {:.3f}\".format(class_name, score),\n",
    "            bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n",
    "            fontsize=12,\n",
    "            color=\"white\",\n",
    "        )\n",
    "    plt.tight_layout(rect=[0, 0, 2, 2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing object categories\n",
    "object_categories = [\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]\n",
    "\n",
    "# Setting a threshold 0.02 will only plot detection results that have a confidence score greater than 0.02\n",
    "threshold = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we load the test image into the memory. The test image used in this notebook is from [PEXELS](https://www.pexels.com/) which remains unseen until the time of preditcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "test_file = \"test.jpg\"\n",
    "test_image = PIL.Image.open(test_file)\n",
    "test_image = np.asarray(test_image.resize((320, 320)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) was a popular computer vision challenge and they released annual challenge datasets for object detection from 2005 to 2012. In this notebook, we will use the data sets from 2007 and 2012, named as VOC07 and VOC12 respectively. Cumulatively, we have more than 20,000 images containing about 50,000 annotated objects. These annotated objects are grouped into 20 categories.\n",
    "\n",
    "***Notes:***\n",
    "1. While using the Pascal VOC dataset, please be aware of the database usage rights. The VOC data includes images obtained from flickr's website. Use of these images must respect the corresponding terms of use: https://www.flickr.com/help/terms\n",
    "2. Default EBS Volume size for SageMaker Notebook instances is 5GB. While performing this step if you run out of storage then consider increasing the volume size. One way to do so is by using AWS CLI as documented [here](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/sagemaker/update-notebook-instance.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Download the Pascal VOC datasets from 2007 and 2012 from Oxford University's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Download and extract the datasets\n",
    "![ ! -f /tmp/pascal-voc.tgz ] && { wget -P /tmp https://s3.amazonaws.com/fast-ai-imagelocal/pascal-voc.tgz; }\n",
    "![ ! -d VOCdevkit ] && { tar -xf /tmp/pascal-voc.tgz; mv pascal-voc VOCdevkit; }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data into RecordIO\n",
    "[RecordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html) is a highly efficient binary data format from [MXNet](https://mxnet.incubator.apache.org/). Using this format, dataset is simple to prepare and transfer to the instance that will run the training job. Please refer to [object_detection_recordio_format](https://github.com/awslabs/amazon-sagemaker-examples/blob/80333fd4632cf6d924d0b91c33bf80da3bdcf926/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb) for more information about how to prepare RecordIO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tools/prepare_dataset.py --dataset pascal --year 2007,2012 --set trainval --target VOCdevkit/train.lst\n",
    "!python tools/prepare_dataset.py --dataset pascal --year 2007 --set test --target VOCdevkit/val.lst --no-shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "Upload the data to the S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the RecordIO files to train and validation channels\n",
    "sess.upload_data(path=\"VOCdevkit/train.rec\", bucket=bucket, key_prefix=training_data_sub_folder)\n",
    "sess.upload_data(path=\"VOCdevkit/train.idx\", bucket=bucket, key_prefix=training_data_sub_folder)\n",
    "\n",
    "sess.upload_data(path=\"VOCdevkit/val.rec\", bucket=bucket, key_prefix=validation_data_sub_folder)\n",
    "sess.upload_data(path=\"VOCdevkit/val.idx\", bucket=bucket, key_prefix=validation_data_sub_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to setup training and compilation output locations in S3, where the respective model artifacts will be dumped. We also setup the s3 location for training data, validation data and custom code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Location where the training data is stored in the previous step\n",
    "s3_training_data_location = \"s3://{}/{}\".format(bucket, training_data_sub_folder)\n",
    "\n",
    "# S3 Location to save the model artifact after training\n",
    "s3_training_output_location = \"s3://{}/{}\".format(bucket, training_output_sub_folder)\n",
    "\n",
    "# S3 Location where the training data is stored in the previous step\n",
    "s3_validation_data_location = \"s3://{}/{}\".format(bucket, validation_data_sub_folder)\n",
    "\n",
    "# S3 Location to save the model artifact after compilation\n",
    "s3_compilation_output_location = \"s3://{}/{}\".format(bucket, compilation_output_sub_folder)\n",
    "\n",
    "# S3 Location to save your custom code in tar.gz format\n",
    "s3_custom_code_upload_location = \"s3://{}/{}\".format(bucket, custom_code_sub_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sagemaker.MXNet`` object. This estimator will launch the training job. It may take some time for training job to complete. To make it faster `num-epochs` can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "yolo_estimator = MXNet(\n",
    "    entry_point=\"train_yolo.py\",\n",
    "    role=role,\n",
    "    output_path=s3_training_output_location,\n",
    "    code_location=s3_custom_code_upload_location,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.16xlarge\",\n",
    "    framework_version=\"1.8.0\",\n",
    "    py_version=\"py37\",\n",
    "    hyperparameters={\n",
    "        \"num-epochs\": 10,\n",
    "        \"data-shape\": 320,\n",
    "        \"gpus\": \"0,1,2,3,4,5,6,7\",\n",
    "        \"network\": \"mobilenet1.0\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yolo_estimator.fit({\"train\": s3_training_data_location, \"val\": s3_validation_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the trained model using SageMaker Neo\n",
    "\n",
    "After training the model we can use SageMaker Neo's ``compile_model()`` API to compile the trained model. When calling ``compile_model()`` user is expected to provide all the correct input shapes required by the model for successful compilation. We also specify the target instance family, the name of our IAM execution role, S3 bucket to which the compiled model would be stored and we set ``MMS_DEFAULT_RESPONSE_TIMEOUT`` environment variable to 500. \n",
    "\n",
    "For this example, we will choose `ml_p3` as the target instance family while compiling the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "compiled_model = yolo_estimator.compile_model(\n",
    "    target_instance_family=\"ml_p3\",\n",
    "    input_shape={\"data\": [1, 3, 320, 320]},\n",
    "    role=role,\n",
    "    output_path=s3_compilation_output_location,\n",
    "    framework=\"mxnet\",\n",
    "    framework_version=\"1.8\",\n",
    "    env={\"MMS_DEFAULT_RESPONSE_TIMEOUT\": \"500\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the compiled model and request Inferences\n",
    "\n",
    "We have to deploy the compiled model on one of the instance family for which the trained model was compiled for. Since we have compiled for `ml_p3` we can deploy to any `ml.p3` instance type. For this example we will choose `ml.p3.2xlarge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "neo_object_detector = compiled_model.deploy(initial_instance_count=1, instance_type=\"ml.p3.2xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = neo_object_detector.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the detections.\n",
    "visualize_detection(test_file, response, object_categories, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Endpoint name: \" + neo_object_detector.endpoint_name)\n",
    "neo_object_detector.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
