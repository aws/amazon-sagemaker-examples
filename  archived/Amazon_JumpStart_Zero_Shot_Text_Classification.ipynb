{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c61f54",
   "metadata": {},
   "source": [
    "# Introduction to JumpStart - Zero Shot Text classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987d75d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad97efd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Welcome to Amazon SageMaker JumpStart! You can use JumpStart to solve many Machine Learning tasks through one-click in SageMaker Studio, or through SageMaker JumpStart API. In this demo notebook, we demonstrate how to use the JumpStart API to do zero shot text classification. \n",
    "\n",
    "In supervised classification, natural language processing (NLP) models can only classify text that belong to classes in the training data. Zero-shot classification is a paradigm where a model can classify new, unseen examples that belong to classes that were not present in the training data. For example, a text classification model that is trained to classify new year resolutions tweets on 2 classes ‘career’ and ‘health’, can be used to classify resolutions to a category ‘finance’ that the model has not been trained on .\n",
    "\n",
    "In this notebook, you will learn how to deploy the pre-trained model for Zero Shot Text classification, run inference and clean up resources. Furthermore, we benchmark the a zero shot text classification model, BART LARGE MNLI model on [New Year's Resolutions dataset](https://data.world/crowdflower/2015-new-years-resolutions).\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db28351",
   "metadata": {},
   "source": [
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Select a model](#2.-Select-a-model)\n",
    "3. [Retrieve JumpStart Artifacts & Deploy an Endpoint](#3.-Retrieve-JumpStart-Artifacts-&-Deploy-an-Endpoint)\n",
    "4. [Query endpoint and parse response](#4.-Query-endpoint-and-parse-response)\n",
    "5. [Benchmarking](#5.-Benchmarking)\n",
    "6. [Clean up the endpoint](#6.-Clean-up-the-endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce462973",
   "metadata": {},
   "source": [
    "Note: This notebook was tested on ml.t3.medium instance in Amazon SageMaker Studio with Python 3 (Data Science) kernel and in Amazon SageMaker Notebook instance with conda_python3 kernel.\n",
    "\n",
    "Note: After you’re done running the notebook, make sure to delete all resources so that all the resources that you created in the process are deleted and your billing is stopped. Code in [Clean up the endpoint](#5.-Clean-up-the-endpoint) deletes model and endpoints that are created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea47727",
   "metadata": {},
   "source": [
    "### 1. Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b91e81",
   "metadata": {},
   "source": [
    "---\n",
    "Before executing the notebook, there are some initial steps required for set up.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317073ae-da63-4735-a59e-ac2625d4f9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48370155",
   "metadata": {},
   "source": [
    "#### Permissions and environment variables\n",
    "\n",
    "---\n",
    "To host on Amazon SageMaker, we need to set up and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook as the AWS account role with SageMaker access. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90518e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5bfaa1",
   "metadata": {},
   "source": [
    "### 2. Select a model\n",
    "\n",
    "***\n",
    "You can continue with the default model, or can choose a different model from the dropdown generated upon running the next cell. A complete list of SageMaker pre-trained models can also be accessed at [Sagemaker pre-trained Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#). [Default model](https://huggingface.co/facebook/bart-large-mnli) is the base BART Large model further trained on the [MNLI](https://huggingface.co/datasets/multi_nli) dataset. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119115c",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    model_id,\n",
    "    model_version,\n",
    ") = (\n",
    "    \"huggingface-zstc-facebook-bart-large-mnli\",\n",
    "    \"*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1f5f4",
   "metadata": {},
   "source": [
    "***\n",
    "[Optional] Here, we filter-out all the zero shot text classification models and select a model for inference.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecb929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "filter_value = \"task == zstc\"\n",
    "zstc_models = list_jumpstart_models(filter=filter_value)\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "model_dropdown = Dropdown(\n",
    "    options=zstc_models,\n",
    "    value=model_id,\n",
    "    description=\"Select a model\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71255f0",
   "metadata": {},
   "source": [
    "#### Chose a model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b423969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0927746c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_version=\"*\" fetches the latest version of the model\n",
    "model_id, model_version = model_dropdown.value, \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3ab601",
   "metadata": {},
   "source": [
    "### 3. Retrieve Artifacts & Deploy an Endpoint\n",
    "\n",
    "***\n",
    "\n",
    "Using SageMaker, we can perform inference on the pre-trained model, even without fine-tuning it first on a new dataset. We start by retrieving the `deploy_image_uri`, `deploy_source_uri`, and `model_uri` for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it. This may take a few minutes.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a79ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters, instance_types\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-infer-{model_id}\")\n",
    "\n",
    "# Retrieve the default inference instance type. You can replace it with other instance types compatible with the model.\n",
    "inference_instance_type = instance_types.retrieve_default(\n",
    "    region=None, model_id=model_id, model_version=model_version, scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri. This includes all dependencies and scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Retrieve the model uri. This includes the pre-trained model and parameters.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    model_data=model_uri,\n",
    "    entry_point=\"inference.py\",  # entry point file in source_dir and present in deploy_source_uri\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665dfbe4-2857-484e-8179-3ad11307822c",
   "metadata": {},
   "source": [
    "### 4. Query endpoint and parse response\n",
    "\n",
    "---\n",
    "Input to the endpoint is a sequence and a set of candidate labels to chose from. It is in a json format and encoded in `utf-8` format. Output of the endpoint is a `json` with predicted labels and the scores. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5f775-c73c-4a19-a581-9f9db30238d6",
   "metadata": {},
   "source": [
    "Next we write some helper function for querying the endpoint and parsing the endpoint response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4921df0-5b9e-4dfb-a9b0-d689c36a339d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sequence, candidate_labels = \"one day I will see the world\", [\"travel\", \"cooking\", \"dancing\"]\n",
    "\n",
    "newline = \"\\n\"\n",
    "bold = \"\\033[1m\"\n",
    "unbold = \"\\033[0m\"\n",
    "\n",
    "\n",
    "def query_endpoint(input_json):\n",
    "    \"\"\"Query the model predictor.\"\"\"\n",
    "    query_response = model_predictor.predict(\n",
    "        json.dumps(input_json).encode(\"utf-8\"),\n",
    "        {\n",
    "            \"ContentType\": \"application/json\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        },\n",
    "    )\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    \"\"\"Parse response and return the labels, scores and the predicted label (highest score).\"\"\"\n",
    "    model_predictions = json.loads(query_response)\n",
    "    scores = model_predictions[\"scores\"]\n",
    "    labels = model_predictions[\"labels\"]\n",
    "    predicted_label_index = np.argmax(scores)\n",
    "    predicted_label = labels[predicted_label_index]\n",
    "    return labels, scores, predicted_label\n",
    "\n",
    "\n",
    "input_query = {\"sequences\": sequence, \"candidate_labels\": candidate_labels}\n",
    "query_response = query_endpoint(input_query)\n",
    "labels, scores, predicted_label = parse_response(query_response)\n",
    "\n",
    "print(\n",
    "    f\"Inference:{newline}\"\n",
    "    f\"Sequence: {bold}{sequence}{unbold}{newline}\"\n",
    "    f\"Labels: {bold}{labels}{unbold}{newline}\"\n",
    "    f\"Scores: {bold}{scores}{unbold}{newline}\"\n",
    "    f\"Predicted Label: {bold}{predicted_label}{unbold}{newline}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6dc6f5-0e8e-4c40-a094-c6ea39e26a4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "Zero Shot text classification models also support multi-class classification i.e. predict multiple labels for a single input.  To predict more than one label, set `multi_class` to `True`. By default, this parameter is `False`.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d18283-a937-4820-aa53-8ce3771561d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ml_sequence, ml_candidate_labels = \"one day I will see the world\", [\n",
    "    \"travel\",\n",
    "    \"cooking\",\n",
    "    \"dancing\",\n",
    "    \"exploration\",\n",
    "]\n",
    "\n",
    "ml_input_query = {\n",
    "    \"sequences\": ml_sequence,\n",
    "    \"candidate_labels\": ml_candidate_labels,\n",
    "    \"multi_class\": True,\n",
    "}\n",
    "ml_query_response = query_endpoint(ml_input_query)\n",
    "ml_labels, ml_scores, _ = parse_response(ml_query_response)\n",
    "\n",
    "print(\n",
    "    f\"Inference:{newline}\"\n",
    "    f\"Sequence: {bold}{ml_sequence}{unbold}{newline}\"\n",
    "    f\"Labels: {bold}{ml_candidate_labels}{unbold}{newline}\"\n",
    "    f\"Multi-label scores: {bold}{ml_scores}{unbold}{newline}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c065c0e6-f40a-4111-9f87-9d85d4ac1e4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Benchmarking\n",
    "---\n",
    "In this section, we will benchmark the BART LARGE MNLI model on the [New Year's Resolutions dataset](https://data.world/crowdflower/2015-new-years-resolutions). We classify each resolution as one of the following categories:\n",
    "\n",
    "- Health\n",
    "- Humor\n",
    "- Personal Growth\n",
    "- Philanthropy\n",
    "- Leisure\n",
    "- Career\n",
    "- Finance\n",
    "- Education\n",
    "- Time Management\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49cde6-6486-4b11-a78f-389da7a71d6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.1. Data download and inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c930bba-57d0-4f25-a69d-9d2455c2e4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "s3_bucket = f\"sagemaker-solutions-prod-{region}\"\n",
    "key_prefix = \"0.2.0/Zero-shot-text-classification/1.0.0/artifacts/data\"\n",
    "sample_tweets_file_name = \"jumpstart-soln-zero-shot-text-clf-data.csv\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "s3.download_file(s3_bucket, f\"{key_prefix}/{sample_tweets_file_name}\", sample_tweets_file_name)\n",
    "\n",
    "# Get on overview of the dataset.\n",
    "# Resolution category: is the actual label of the text\n",
    "# Text: is the actual tweet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sample_tweets = pd.read_csv(sample_tweets_file_name)\n",
    "sample_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccccc68-6907-4fc8-b25f-69dbcf479ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unique categories\n",
    "list(sample_tweets[\"Resolution_Category\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf3884a-10b9-4330-9c55-8bdd23d31a5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "Remap provided categories to the newly defined categories.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3a04e-5298-4b13-8603-fe8cd7c262c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_remap = {\n",
    "    \"Health & Fitness\": \"Health\",\n",
    "    \"Recreation & Leisure\": \"Leisure\",\n",
    "    \"Philanthropic\": \"Philanthropy\",\n",
    "    \"Time Management/Organization\": \"Time Management\",\n",
    "    \"Education/Training\": \"Education\",\n",
    "}\n",
    "\n",
    "sample_tweets[\"Resolution_Category\"] = sample_tweets[\"Resolution_Category\"].replace(category_remap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6691c5-4182-42ae-8d57-940f3f9c7707",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "Drop the family, friends, and relationship categories because the original Family/Friends/Relationship cannot be mapped to a single category\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e4cb4-443d-48de-b0e2-b70a3e5f0d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_tweets = sample_tweets.loc[\n",
    "    sample_tweets[\"Resolution_Category\"] != \"Family/Friends/Relationships\"\n",
    "]\n",
    "sample_tweets = sample_tweets.reset_index(drop=True)\n",
    "sample_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1ec30-14ed-4336-a688-b02e2979b91f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unique categories\n",
    "classification_categories = list(sample_tweets[\"Resolution_Category\"].unique())\n",
    "classification_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1066e-fd8d-4935-8958-6b55e6587455",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.2. Dataset Clean up\n",
    "---\n",
    "Before running inference the text is cleaned by removing links, emojis, and media.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455b8ca-2246-463c-8ee9-6116ded705ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class TweetPreprocessor:\n",
    "    @staticmethod\n",
    "    def remove_links(tweet):\n",
    "        \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "        tweet = re.sub(r\"http\\S+\", \"\", tweet)  # remove http links\n",
    "        tweet = re.sub(r\"bit.ly/\\S+\", \"\", tweet)  # remove bitly links\n",
    "        tweet = re.sub(r\"pic.twitter\\S+\", \"\", tweet)\n",
    "        return tweet\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_users(tweet):\n",
    "        \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "        tweet = re.sub(\"(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+):*\", \"\", tweet)  # remove retweet\n",
    "        tweet = re.sub(\"(@[A-Za-z]+[A-Za-z0-9-_]+):*\", \"\", tweet)  # remove tweeted at\n",
    "        return tweet\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_hashtags(tweet):\n",
    "        \"\"\"Takes a string and removes any hashtags\"\"\"\n",
    "        tweet = re.sub(\"(#[A-Za-z]+[A-Za-z0-9-_]+)\", \"\", tweet)  # remove hashtags\n",
    "        return tweet\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_av(tweet):\n",
    "        \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "        tweet = re.sub(\"VIDEO:\", \"\", tweet)  # remove 'VIDEO:' from start of tweet\n",
    "        tweet = re.sub(\"AUDIO:\", \"\", tweet)  # remove 'AUDIO:' from start of tweet\n",
    "        return tweet\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(tweet):\n",
    "        # tweet = tweet.encode('latin1', 'ignore').decode('latin1')\n",
    "        tweet = tweet.lower()\n",
    "        # tweet = TweetPreprocessor.remove_users(tweet)\n",
    "        tweet = TweetPreprocessor.remove_links(tweet)\n",
    "        # tweet = TweetPreprocessor.remove_hashtags(tweet)\n",
    "        tweet = TweetPreprocessor.remove_av(tweet)\n",
    "        tweet = \" \".join(tweet.split())  # Remove extra spaces\n",
    "        return tweet.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_hash_tags(tweet):\n",
    "        return re.findall(r\"#(\\w+)\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84992c-a18c-49b7-9770-422a35c68282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642e794-adc1-4246-a0c6-dcde88111118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import demoji\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "\n",
    "sample_tweets[\"text_clean\"] = sample_tweets[\"text\"].map(\n",
    "    TweetPreprocessor.preprocess\n",
    ")  # Preprocess text.\n",
    "sample_tweets[\"text_clean\"] = sample_tweets[\"text_clean\"].map(demoji.replace)  # Remove emojis.\n",
    "sample_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7de4b-fb6a-4803-b73b-f06e07eed8e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.3. Run inference\n",
    "\n",
    "---\n",
    "Select the number of samples you want to use for inference. The estimated time depends on the instance type you choose. On `ml.p3.2xlarge`, running inference on 1000 samples take roughly 5 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6600e-298e-448a-9cb2-eef66cdbd56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "sample_tweets = sample_tweets.iloc[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c4e06-5adf-4229-8596-c0415056a753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequences = sample_tweets[\"text_clean\"].tolist()\n",
    "\n",
    "predicted_labels = []\n",
    "for tweet in sequences:\n",
    "    endpoint_response = query_endpoint(\n",
    "        {\n",
    "            \"sequences\": tweet,\n",
    "            \"candidate_labels\": classification_categories,\n",
    "        }\n",
    "    )\n",
    "    _, _, predicted_label = parse_response(endpoint_response)\n",
    "    predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0513202-fc75-4982-978b-645a4190d9d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_tweets[\"zero-shot-class\"] = predicted_labels\n",
    "sample_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d0a304-b29b-4484-bc03-8a38e9b3dcc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.4. Compute metrics\n",
    "\n",
    "---\n",
    "\n",
    "Next, we compare the predicted label with the ground truth labels. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be020ce-2e3b-4214-8072-7e6315809133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(sample_tweets[\"Resolution_Category\"], sample_tweets[\"zero-shot-class\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d1173",
   "metadata": {},
   "source": [
    "### 6. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3d98d-ebbb-4fa7-862b-2db0c27a3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker model and endpoint\n",
    "model_predictor.delete_model()\n",
    "model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcff8b7",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/introduction_to_amazon_algorithms|jumpstart_zstc|Amazon_JumpStart_Zero_Shot_Text_Classification.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
