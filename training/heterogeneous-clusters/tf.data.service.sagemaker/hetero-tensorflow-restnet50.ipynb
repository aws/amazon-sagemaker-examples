{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow's tf.data.service with Amazon SageMaker Training Heterogeneous Clusters\n",
    "\n",
    "---\n",
    "### Intro\n",
    "\n",
    "Heterogeneous clusters enables launching training jobs that use multiple instance types in a single job. This  capability can improve your training cost and speed by running different parts of the model training on the most suitable instance type. This use case typically happens in computer vision DL training, where training is bottlnecked on CPU resources needed for data augmentation, leaving the expensive GPU underutilized. Heterogeneous clusters allows you to add more CPU resources to fully utilize GPUs to increase training speed and cost-efficiency. For more details, you can find the documentation of this feature [here](https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html).\n",
    "\n",
    "This notebook demonstrates how to use Heterogeneous Clusters feature of SageMaker Training with TensorFlow's [tf.data.service](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service).\n",
    "\n",
    "In this sample notebook, we'll be training a CPU intensive Deep Learning computer vision workload. We'll be comparing between a homogeneous and a heterogeneous training configurations. Both jobs we'll run train with the same data, pre-processing, and other relevant parameters:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td width=43%><b>Homogeneous Training Job</b><br/>\n",
    "    In a Homogeneous training job the ml.p4d.24xlarge instance GPUs are under-utilized due to a CPU bottleneck.</td>\n",
    "    <td width=57%><b>Heterogeneous Training Job</b><br/>\n",
    "    In a Heterogeneous training job, we add two ml.c5.18xlarge instances with extra CPU cores, to reduce the CPU bottleneck and drive up GPU usage, to improve training speed cost-efficiency.\n",
    "    </td>\n",
    "   </tr> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <img src=images/basic-homogeneous-job.png alt=\"homogeneous training job\" /></td>\n",
    "    <td><img src=images/basic-heterogeneous-job.png alt=\"Heterogeneous training job\" /></td>\n",
    "   </tr> \n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workload Details\n",
    "Training data is stored in TFRecord files in `data` folder, and generated from [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset using `generate_cifar10_tfrecords.py` script. The data pre-processing pipeline includes: parsing images, dilation, blur filtering, and a number of [TensorFlow preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers). We'll use a [Resnet50](https://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50) architecture. The job runs on an 8 GPUs instance, p4d.24xlarge, and uses Horovod for data parallelizaion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Heterogeneous clusters Training\n",
    "We then define `instance_groups` in the TensorFlow estimator to enable training jobs to leverage Heterogeneous Cluster features. \n",
    "The data pre-processing code runs on the CPU nodes (here referred as **data_group or data group**), whereas the Deep Neural Network training code runs on the GPU nodes (here referred as **dnn_group or dnn group**). In this example, the inter-node communication between CPU and GPU instance_groups is implemented using [TensorFlow data service feature](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service). This feature was introduced in TensorFlow version 2.3 which provides APIs for defining dedicated worker machines for performing data preprocessing. Note that SageMaker's Heterogeneous cluster does not provide out-of-the-box support for inter-instance_group communication. \n",
    "\n",
    "\n",
    "The training script (`launcher.py`) runs on all the nodes regardless of which instance_group it belongs to. However, it has the logic to detect (using SageMaker's `instance_group` environment variables) whether the node it is running on belongs to data_group or dnn_group. \n",
    "\n",
    "If it is data_group, it spawns a separate process by executing `train_data.py`. This script runs a dispatcher server service on the first node of the instance group. And, on all the nodes in the same instance_group, it runs the worker server service. A dispatch server is responsible for distributing preprocessing tasks to one, or more, worker servers, each of which load the raw data directly from storage, and send the processed data to the GPU device. A dispatcher server listens on port 6000, whereas the worker server listens on port 6001. By applying `tf.data.experimental.service.distribute` to your dataset, you can program the dataset to run all preprocessing operations up to the point of application, on the workers. TFRecord files are copied over to instances in this group, as the workers load the raw data from those files. In this example, we are using 2 instances of ml.c5.18xlarge in the data_group. While dispatching the data to the dnn_group, the main entrypoint script `launcher.py` listens on port 16000 for a shutdown request coming from the data group. The train_data.py waits for shutdown action from the parent process.\n",
    "\n",
    "If the node belongs to dnn_group, the main training script (`launcher.py`) spawns a separate set of processes by executing `train_dnn.py`. This script contains a deep neural network algorithm/code. And, in some cases, can host additional data-preprocessing components to maximize the use of CPUs on dnn nodes. The set of processes running DNN training consumes a stream of processed dataset from the Dispatcher server (the first node in the data_group at port 6000), and runs model training. The dnn_group can also run distributed training on multiple nodes defined by parameter instance_count (see details under **Setting up the training environment** section of this notebook). Once the model is trained on the dataset, the dnn_group establishes a connection back to the dispatcher server on port 16000 to signal shutdown request. \n",
    "\n",
    "\n",
    "A graphical view of how the data flows is shown below in Heterogeneous Cluster training with tf.data.service:\n",
    "\n",
    "**NEED TO BE UPDATED**\n",
    "\n",
    "<img src=images/tf.data.service-diagram.png width=600px>\n",
    "\n",
    "\n",
    "This notebook refers following files and folders:\n",
    "\n",
    "- Folders: \n",
    "  - `code`: this has the training scripts, grpc client-server code, \n",
    "  - `images`: contains images referred in notebook\n",
    "- Files: \n",
    "  - `launcher.py`: entry point training script. This script is executed on all the nodes irrespective of which group it belongs to. Explained above. \n",
    "  - `train_data.py`: this script runs on the data_group nodes and responsible for setting up dispatcher and worker servers\n",
    "  - `train_dnn.py`: this script runs on the dnn_group nodes, and responsible for DNN training code/algorithm. \n",
    "  - `requirements.txt`: defines package required for tensorflow-addon and protobuf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "As an alternative to following this notebook, you follow (readme.md)[./readme.md] which allows you to setup and launch the training job from an IDE or command line.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "At a high level, the notebook covers:\n",
    "-  A Setting up Amazon SageMaker Studio Notebook \n",
    "-  Preparing Training dataset and uploading to Amazon S3\n",
    "-  Setting up the Training environment\n",
    "-  Submitting the Training job\n",
    "-  Monitor and visualize the CloudWatch metrics\n",
    "-  Comparing time-to-train and cost-to-train\n",
    "-  Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Setting up SageMaker Studio notebook\n",
    "#### Before you start\n",
    "Ensure you have selected Python 3 (_TensorFlow 2.6 Python 3.8 CPU Optimized_) image for your SageMaker Studio Notebook instance, and running on _ml.t3.medium_ instance type.\n",
    "\n",
    "#### Step 1 - Upgrade SageMaker SDK and dependent packages \n",
    "Heterogeneous Clusters for Amazon SageMaker model training was [announced](https://aws.amazon.com/about-aws/whats-new/2022/07/announcing-heterogeneous-clusters-amazon-sagemaker-model-training) on 07/08/2022. As a first step, ensure you have updated SageMaker SDK, PyTorch, and Boto3 client that enables this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m pip install --upgrade boto3 botocore awscli sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Restart the notebook kernel \n",
    "From the Jupyter Lab menu bar **Kernel > Restart Kernel...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Valdiate SageMaker Python SDK and Tensorflow versions\n",
    "Ensure the output of the cell below reflects:\n",
    "\n",
    "- SageMaker Python SDK version 2.98.0 or above, \n",
    "- boto3 1.24 or above \n",
    "- botocore 1.27 or above \n",
    "- TensorFlow 2.6 or above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sagemaker\n",
      "Version: 2.109.0\n",
      "---\n",
      "Name: boto3\n",
      "Version: 1.24.72\n",
      "---\n",
      "Name: botocore\n",
      "Version: 1.27.72\n",
      "---\n",
      "Name: tensorflow\n",
      "Version: 2.8.0\n",
      "---\n",
      "Name: protobuf\n",
      "Version: 3.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip show sagemaker boto3 botocore tensorflow protobuf |egrep 'Name|Version|---'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Preparing Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download cifar10 dataset and convert them into tfrecord\n",
    "The training data set is stored in TFRecord files in `data` folder, and generated from CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 ./generate_cifar10_tfrecords.py --data-dir ./data\n",
    "rm -rf /tmp/data.old && mv data data.old && mkdir data && cp data.old/train/train.tfrecords ./data/train.1.tfrecords && cp data.old/train/train.tfrecords ./data/train.2.tfrecords && mv data.old /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Setting up training environment\n",
    "#### Step 1: Import SageMaker components and setup the IAM role and S3 bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "output_path = \"s3://\" + sess.default_bucket() + \"/cifar10-tfrecord\"\n",
    "print(f\"role={role}\")\n",
    "print(f\"output_path={output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 2: Upload the tfrecord training data to S3 bucket and define training input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"cifar10-tfrecord\"\n",
    "bucket = sess.default_bucket()\n",
    "print(f\"Uploading data from ./data to s3://{bucket}/{prefix}/\")\n",
    "s3path = sess.upload_data(path=\"./data\", bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "from sagemaker import TrainingInput\n",
    "data_uri = TrainingInput(\n",
    "    s3path,\n",
    "    # instance_groups=['data_group'], # we don't need to restrict training channel to a specific group as we have data workers in both groups\n",
    "    input_mode=\"FastFile\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Running a homogeneous training job\n",
    "In this step we define and submit a homogeneous training job. It use a single instance type (p4d.24xlarge) with 8 GPUs, and analysis will show its CPU bound causing its GPUs to be underutilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import os\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": 10,\n",
    "    \"steps_per_epoch\": 500,\n",
    "    \"batch_size\": 1024,\n",
    "    \"tf_data_mode\": \"local\",  # We won't be using tf.data.service ('service') for this homogeneous job\n",
    "    \"num_of_data_workers\": 0,  # We won't be using tf.data.service ('service') for this homogeneous job\n",
    "}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"launcher.py\",\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"2.9.1\",\n",
    "    py_version=\"py39\",\n",
    "    role=role,\n",
    "    volume_size=10,\n",
    "    max_run=1800,  # 30 minutes\n",
    "    disable_profiler=True,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    instance_count=1,\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": 8,  # 8 GPUs per host\n",
    "            \"custom_mpi_options\": \"--NCCL_DEBUG WARN\",\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Submit the training job\n",
    "\n",
    "Note: For the logs, click on **View logs** from the **Training Jobs** node in **Amazon SageMaker Console**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs=data_uri,\n",
    "    job_name=\"homogeneous-\" + datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Analyzing the homogeneous training job throughput and resource usage\n",
    "We'll examine: CPU and GPU usage. Epoch time and step time\n",
    "\n",
    "#### CPU and GPU usage analysis\n",
    "In the screenshot below we observe that close to all the 96 vCPU of the instance is utilized. While GPU utilization is only ~40%. Clearly if we had more vCPUs we could increase GPU usage signifiantly to increase job throughput\n",
    "\n",
    "Note: To view your own job Click on **View instance metrics** from the **Training jobs** node in **Amazon SageMaker Console**. Then to rescale the CloudWatch Metrics to 100% on CPU utilization for algo-1 and algo-2, use CloudWatch \"Add Math\" feature and average it out by no. of vCPUs/GPUs on those instance types.  \n",
    "<img src=\"images/metrics homogeneous cpu and gpu usage.png\" width=75%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch time and step time analysis\n",
    "For 2nd and 3rd epochs the below should printout: 105s/epoch - 209ms/step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture homogeneous_logs\n",
    "estimator.sagemaker_session.logs_for_job(estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing step time for epochs and steps for homogenous-20220909T140047Z\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 132s - loss: 1.9972 - lr: 0.0033 - 132s/epoch - 263ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 105s - loss: 1.8961 - lr: 0.0057 - 105s/epoch - 209ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 105s - loss: 1.8536 - lr: 0.0080 - 105s/epoch - 211ms/step\n"
     ]
    }
   ],
   "source": [
    "print(f\"Printing step time for epochs and steps for {estimator.latest_training_job.name}\")\n",
    "for line in homogeneous_logs.stdout.split(\"\\n\"):\n",
    "    if \"mpirank:0\" in line and \"/epoch\" in line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Running a Heterogeneous Training Job\n",
    "We'll now run a training job in heterogeneous clusters mode.  \n",
    "Note the changes from the homogeneous cluster job:  \n",
    "- We define two new instance groups that are provided to the `estimator` as the `instance_groups` parameter that replaces the homogeneous paramters `instance_type` and `instance_count`.\n",
    "- In the `distribution` parameter for Horovod we added a new parameter `instance_groups` that is used to limit the MPI cluster to run in the  `dnn_group`. The MPI cluster should include only the GPU nodes that run Horovod (which needs MPI). The `data_group` instances should not be part of the MPI cluster, as they set up their on `tf.data.service` cluster.\n",
    "\n",
    "More on the two instance groups config we use:\n",
    "- `data_group` - two ml.c5.18xlarge instances, each with 72 vCPUs to handle data preprocessing. Reading data from S3, preprocessing it, and forwarding it to the `dnn_group`.\n",
    "- `dnn_group` - a single p4d.24xlarge instance, with 8 GPUs and 96 vCPUs. to handle deep neural network optimization (forward backword passes) releing on 8 GPUs and some of the 96 vPCUs. To fully utilize 96 vCPUs in the `dnn_group`, we'll be starting data workers on all instances in both groups, therefore we have 240 vCPUs (96+72+72) in total available for preprocessing (minus vCPUs used for the neural network optimization process).\n",
    "\n",
    "There are three Python scripts to know about:\n",
    "The 1st is `train_dnn.py` - This is your training script for the neural network, you should edit it to match your own use case. Note how this script isn't aware of the Heterogeneous clusters setup, except when it initializes the tf.data dataset calling this line: `ds = ds.apply(tf.data.experimental.service.distribute(...)`.  \n",
    "The 2nd and 3rd scripts, which you're not suppose to edit when adapting to your own use case, do the heavy lifting required for using tf.data.service over the Heterogeneous clusters feature.  \n",
    "`train_data.py` include functions to start/stop tf.service.data process like a dispatcher and WorkerServer. \n",
    "`launcher.py` has several responsibilities: \n",
    "- a single entrypoint script for all instances in all instance groups (SageMaker will start the same script on all instances).\n",
    "- Identify which instance group the node belong to, and start the relevant script accordingly (`train_dnn.py` or  `train_data.py` or sometimes both).\n",
    "- Takes measures to ensure that tf.data.sevice processes shutdown when training completes, as the training job completes only when all instances exit. Remember that training job.\n",
    "- Allow to start more than one process (for example, on the dnn_gruop instances we'll run both the `train_dnn.py` and a tf.data.service worker to utilize the instance CPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import os\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": 10,\n",
    "    \"steps_per_epoch\": 500,\n",
    "    \"batch_size\": 1024,\n",
    "    \"tf_data_mode\": \"service\",  # We'll be using tf.data.service for this Heterogeneous clusters job\n",
    "    \"num_of_data_workers\": 2,  # We won't be using tf.data.service for this Heterogeneous clusters job\n",
    "}\n",
    "\n",
    "# Group for CPU instances to run tf.data.service dispatcher/workers processes.\n",
    "data_group = InstanceGroup(\"data_group\", \"ml.c5.18xlarge\", 3)\n",
    "# Group for deep neural network (dnn) with accleartors (e.g., GPU, FPGA, etc.)\n",
    "dnn_group = InstanceGroup(\"dnn_group\", \"ml.p4d.24xlarge\", 1)\n",
    "\n",
    "estimator2 = TensorFlow(\n",
    "    entry_point=\"launcher.py\",\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"2.9.1\",\n",
    "    py_version=\"py39\",\n",
    "    role=role,\n",
    "    volume_size=10,\n",
    "    max_run=1800,  # 30 minutes\n",
    "    disable_profiler=True,\n",
    "    # instance_type='ml.p4d.24xlarge',\n",
    "    # instance_count=1,\n",
    "    instance_groups=[data_group, dnn_group],\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": 8,  # 8 GPUs per host\n",
    "            \"custom_mpi_options\": \"--NCCL_DEBUG WARN\",\n",
    "        },\n",
    "        \"instance_groups\": [dnn_group],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Submit the training job\n",
    "\n",
    "Note1: For the logs, click on **View logs** from the **Training Jobs** node in **Amazon SageMaker Console**. \n",
    "Note2: Ignore the 0 billable seconds shown below. See actual billable seconds in the AWS web console > SageMaker > Training Jobs > this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-13 21:29:25 Starting - Starting the training job."
     ]
    }
   ],
   "source": [
    "estimator2.fit(\n",
    "    inputs=data_uri,\n",
    "    job_name=\"heterogenous-\" + datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Analyzing the Heterogeneous training job throughput and resource usage\n",
    "We'll examine: CPU and GPU usage. Epoch time and step time.\n",
    "\n",
    "#### CPU and GPU usage analysis\n",
    " In the screenshot below we observe that GPU usage has increase to 73% (compared to ~40% in the homogeneous training run) which is what we were aiming for. The CPU usage on all 3 instances are close to 90% CPU uage.  \n",
    " \n",
    "Note: To view your own job Click on **View instance metrics** from the **Training jobs** node in **Amazon SageMaker Console**. Then to rescale the CloudWatch Metrics to 100% on CPU utilization for algo-1 and algo-2, use CloudWatch \"Add Math\" feature and average it out by no. of vCPUs/GPUs on those instance types.  \n",
    "<img src=\"images/metrics Heterogeneous cpu and gpu usage.png\" width=75%/>\n",
    "\n",
    "#### Epoch time and step time analysis\n",
    "For 2nd epoch onwards you should see this printout in the logs of the dnn_gruop instance (p4d.24xlarge): 45s/epoch - 89ms/step.\n",
    "Note that the instances are named: Algo1, Algo2, Algo3 randomly on each execution, so you'll need to open all instances logs to find the dnn_group instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Comparing time-to-train and cost-to-train\n",
    "The table below summarizes both jobs. We can see that:\n",
    "- The Heterogeneous job is <b>2.4x faster to train</b> (86ms/step) than the homogeneous job (208ms/step).\n",
    "- The Heterogeneous job is <b>50% cheaper to train</b> than the homogeneous job. This is despite the heterogenous costs more per hour ($45/hour vs $37/hour), due to the two extra c5.18xlarge instances included in the heterogenous job `($45 = $37.7 + 2 * $3.67` \n",
    "The cost-to-train formula we used: change in houly price `($45/$37.7) ` times `reduction-in-time-to-train (86ms/208ms)`  =  50% = `($45/$37.7) * (86ms/208ms)`. \n",
    "\n",
    "<img src=images/homogeneous-vs-heterogeneous-results-table.png alt=\"results table\" />\n",
    "\n",
    "## F. Conclusion\n",
    "In this notebook, we demonstrated how to leverage Heterogeneous cluster feature of SageMaker Training, with TensorFlow to achieve better price performance and increase training speed. To get started you can copy this example project and only change the `train_dnn.py` script. To run the job, you could use this notebook, or the `start_job.py`."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "77c0de85c2cb739aa5100af7b92fb9d2075368f0e653f4148499a56c989df5f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
