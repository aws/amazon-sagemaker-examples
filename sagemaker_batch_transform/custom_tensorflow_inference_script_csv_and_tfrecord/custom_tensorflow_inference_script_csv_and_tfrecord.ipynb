{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Batch Transform custom TensorFlow inference.py (CSV & TFRecord)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "We will train a simple classifier on the iris dataset, training locally from where this notebook is being run. We will then write a custom `inference.py` script for `CSV` and `TFRecord` data that will be used when hosting our model in a Batch Transform Job.\n",
    "\n",
    "Consider the following model definition for IRIS classification. This mode uses the ``tensorflow.estimator.DNNClassifier`` which is a pre-defined estimator module for its model definition.\n",
    "\n",
    "* [Prequisites](#prequisites)\n",
    "* [Training the Network Locally](#training-the-network-locally)\n",
    "* [Set the model up for hosting ](#hosting)\n",
    "* [Write a custom inference.py script](#inference-script)\n",
    "* [ Create Batch Transform Job](#transform)\n",
    "\n",
    "\n",
    "## Prequisites  <a class=\"anchor\" id=\"prequisites\"></a>\n",
    "### Packages and Permissions\n",
    "\n",
    "Here we set up the specific TensorFlow version we will use to Train and Host our model. The SageMaker SDK will use S3 defualt buckets when needed. If the ``get_execution_role``  does not return a role with the appropriate permissions, you'll need to specify an IAM role arn that does. Please make use of an execustion role with `SageMakerFullAccess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.3.1\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "import shutil\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from datetime import datetime \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "bucket_name = sm_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we'll use a very simple network architecture, with three densely-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_mlp(metrics):\n",
    "    ### Setup loss and output node activation\n",
    "    output_activation = \"softmax\"\n",
    "    loss = \"sparse_categorical_crossentropy\"\n",
    "\n",
    "    input = Input(shape=(4,), name=\"input\")\n",
    "\n",
    "    x = Dense(\n",
    "        units=10,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        activation=\"relu\",\n",
    "        name=\"dense_layer1\",\n",
    "    )(input)\n",
    "\n",
    "    x = Dense(\n",
    "        units=20,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        activation=\"relu\",\n",
    "        name=\"dense_layer2\",\n",
    "    )(x)\n",
    "\n",
    "    x = Dense(\n",
    "        units=10,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        name=\"dense_layer3\",\n",
    "    )(x)\n",
    "\n",
    "    output = Dense(units=3, activation=output_activation)(x)\n",
    "\n",
    "    ### Compile the model\n",
    "    model = tf.keras.Model(input, output)\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the pre-processed iris training and test data stored in the `sagemaker-sample-files` public S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download iris test and train data sets from S3\n",
    "SOURCE_DATA_BUCKET = \"sagemaker-sample-files\"\n",
    "SOURCE_DATA_PREFIX = \"datasets/tabular/iris\"\n",
    "sm_session.download_data(\".\", bucket=SOURCE_DATA_BUCKET, key_prefix=SOURCE_DATA_PREFIX)\n",
    "\n",
    "# Load the training and test data from .csv to a Pandas data frame.\n",
    "train_df = pd.read_csv(\n",
    "    \"iris_train.csv\",\n",
    "    header=0,\n",
    "    names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"],\n",
    ")\n",
    "test_df = pd.read_csv(\n",
    "    \"iris_test.csv\",\n",
    "    header=0,\n",
    "    names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"],\n",
    ")\n",
    "\n",
    "# Pop the record labels into N x 1 Numpy arrays\n",
    "train_labels = np.array(train_df.pop(\"class\"))\n",
    "test_labels = np.array(test_df.pop(\"class\"))\n",
    "\n",
    "# Save the remaining features as Numpy arrays\n",
    "train_np = np.array(train_df)\n",
    "test_np = np.array(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network Locally <a class=\"anchor\" id=\"training-the-network-locally\"></a> \n",
    "Here, we train the network using the Tensorflow .fit method, just like if we were using our local computers. This should only take a few seconds because the model is very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EARLY_STOPPING = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", mode=\"auto\", restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Instantiate classifier\n",
    "classifier = iris_mlp(metrics=[\"accuracy\", \"binary_accuracy\"])\n",
    "\n",
    "# Fit classifier\n",
    "history = classifier.fit(\n",
    "    x=train_np,\n",
    "    y=train_labels,\n",
    "    validation_data=(test_np, test_labels),\n",
    "    callbacks=[EARLY_STOPPING],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model up for hosting  <a class=\"anchor\" id=\"hosting\"></a>\n",
    "\n",
    "### Export the model from TensorFlow\n",
    "SageMaker TensorFlow Serving container expects the model artifacts in the following format:\n",
    "\n",
    "```\n",
    "1\n",
    "├── keras_metadata.pb\n",
    "├── saved_model.pb\n",
    "└── variables\n",
    "    ├── variables.data-00000-of-00001\n",
    "    └── variables.index\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"1\")\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a new sagemaker session and upload the model on to the default S3 bucket. We can use the ``sagemaker.Session.upload_data`` method to do this. We need the location of where we exported the model from TensorFlow and where in our default bucket we want to store the model(``/model``). The default S3 bucket can be found using the ``sagemaker.Session.default_bucket`` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we upload the model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_response = sm_session.upload_data(\"model.tar.gz\", bucket=bucket_name, key_prefix=\"model\")\n",
    "s3_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model input tensor shape\n",
    "We can use the the `saved_model_cli` to view the model's input tensors which will help us in building our custom inference.py script.\n",
    "\n",
    "As we can see our model expects input in the shape of (-1, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir {\"1\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Example Batch Data\n",
    "Below we view the sample CSV data that will be used as input to our Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head Data/batch-iris-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_csv_data = \"s3://{}/datasets/batch-iris-data.csv\".format(bucket_name)\n",
    "s3_csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload CSV input data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp Data/batch-iris-data.csv $s3_csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord Example Batch Data\n",
    "We will use the CSV data to generate TFRecord data that we will also use as inference.\n",
    "I.e the CSV data and TFRecord identical they are just of different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import tensorflow as tf\n",
    "csv = pandas.read_csv(\"Data/batch-iris-data.csv\", header=None).values\n",
    "\n",
    "with tf.io.TFRecordWriter(\"Data/batch-iris-data.tfrecords\") as writer:\n",
    "    for row in csv:\n",
    "        features = row[:]\n",
    "        example = tf.train.Example()\n",
    "        example.features.feature[\"features\"].float_list.value.extend(features)\n",
    "      \n",
    "        writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(\"Data/batch-iris-data.tfrecords\")\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_tf_record_data = \"s3://{}/datasets/batch-iris-data.tfrecords\".format(bucket_name)\n",
    "s3_tf_record_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload TFRecord input data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp Data/batch-iris-data.tfrecords $s3_tf_record_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a custom inference.py script <a class=\"anchor\" id=\"inference-script\"></a>\n",
    "\n",
    "Our model accepts a tensor of (-1, 4). Hence, we will create an input handler for each file type (text/csv, application/x-tfrecord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "import json\n",
    "import os \n",
    "os.system(\"pip install numpy tensorflow crcmod\")\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "\n",
    "import crcmod\n",
    "\n",
    "def _masked_crc32c(value):\n",
    "    crc = crcmod.predefined.mkPredefinedCrcFun(\"crc-32c\")(value)\n",
    "    return (((crc >> 15) | (crc << 17)) + 0xA282EAD8) & 0xFFFFFFFF\n",
    "\n",
    "def read_tfrecords(tfrecords):\n",
    "    import io\n",
    "    import struct\n",
    "    tfrecords_bytes = io.BytesIO(tfrecords)\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    while True:\n",
    "        length_header = 12\n",
    "        buf = tfrecords_bytes.read(length_header)\n",
    "        if not buf:\n",
    "            # reached end of tfrecord buffer, return examples\n",
    "            return examples\n",
    "\n",
    "        if len(buf) != length_header:\n",
    "            raise ValueError(\"TFrecord is fewer than %d bytes\" % length_header)\n",
    "        length, length_mask = struct.unpack(\"<QI\", buf)\n",
    "        length_mask_actual = _masked_crc32c(buf[:8])\n",
    "        if length_mask_actual != length_mask:\n",
    "            raise ValueError(\"TFRecord does not contain a valid length mask\")\n",
    "\n",
    "        length_data = length + 4\n",
    "        buf = tfrecords_bytes.read(length_data)\n",
    "        if len(buf) != length_data:\n",
    "            raise ValueError(\"TFRecord data payload has fewer bytes than specified in header\")\n",
    "        data, data_mask_expected = struct.unpack(\"<%dsI\" % length, buf)\n",
    "        data_mask_actual = _masked_crc32c(data)\n",
    "        if data_mask_actual != data_mask_expected:\n",
    "            raise ValueError(\"TFRecord has an invalid data crc32c\")\n",
    "\n",
    "        # Deserialize the tf.Example proto\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(data)\n",
    "        example_features = MessageToDict(example)['features']['feature']['features']['floatList']['value']\n",
    "        # Extract a feature map from the example object\n",
    "        examples.append(example_features)\n",
    "        \n",
    "    return examples\n",
    "\n",
    "def read_csv(csv):\n",
    "      return np.array([[float(j) for j in i.split(',')] for i in csv.splitlines()])\n",
    "\n",
    "\n",
    "def input_handler(data, context):\n",
    "    \"\"\" Pre-process request input before it is sent to TensorFlow Serving REST API\n",
    "    Args:\n",
    "        data (obj): the request data stream\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (dict): a JSON-serializable dict that contains request body and headers\n",
    "    \"\"\"\n",
    "    \n",
    "    if context.request_content_type == 'text/csv':\n",
    "\n",
    "        payload = data.read().decode(\"utf-8\")\n",
    "        inputs = read_csv(payload)\n",
    "      \n",
    "        input = {\n",
    "            'inputs': inputs.tolist()\n",
    "            }\n",
    "        \n",
    "     \n",
    "        return json.dumps(input)\n",
    "    \n",
    "    if context.request_content_type == \"application/x-tfrecord\":\n",
    "    \n",
    "        payload = data.read()\n",
    "        examples = read_tfrecords(payload)\n",
    "        \n",
    "        input = {\n",
    "            'inputs': examples\n",
    "            }\n",
    "        \n",
    "   \n",
    "        return json.dumps(input)\n",
    "        \n",
    "\n",
    "    raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(\n",
    "            context.request_content_type or \"unknown\"))\n",
    "\n",
    "    \n",
    "def output_handler(data, context):\n",
    "    \"\"\"Post-process TensorFlow Serving output before it is returned to the client.\n",
    "    Args:\n",
    "        data (obj): the TensorFlow serving response\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (bytes, string): data to return to client, response content type\n",
    "    \"\"\"\n",
    "\n",
    "    if data.status_code != 200:\n",
    "        raise ValueError(data.content.decode('utf-8'))\n",
    "\n",
    "    response_content_type = context.accept_header\n",
    "    prediction = data.content\n",
    "    return prediction, response_content_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm the input_handler for TFRecord and CSV return the same output\n",
    "Due to the fact that our TFRecord and CSV input data is the same, this means the input_handler should have identical output for each format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inference\n",
    "class Context:\n",
    "    def __init__(self, request_content_type):\n",
    "        self.request_content_type = request_content_type\n",
    "\n",
    "  \n",
    "tfrecord_bytes = open(\"Data/batch-iris-data.tfrecords\", \"rb\")\n",
    "tfrecord_input = inference.input_handler(tfrecord_bytes, Context(\"application/x-tfrecord\"))\n",
    "\n",
    "csv_file_bytes = open(\"Data/batch-iris-data.csv\", \"rb\")\n",
    "csv_input = inference.input_handler(csv_file_bytes, Context(\"text/csv\"))\n",
    "\n",
    "assert csv_input == tfrecord_input, \"CSV and TFRecord output do not match!\"\n",
    "print(\"CSV and TFRecord output match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batch Transform Job <a class=\"anchor\" id=\"transform\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the SageMaker TensorFlow Model\n",
    "First we need to create a `TensorFlowModel` that points to our S3 model tar ball in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "tensorflow_serving_model_batch = TensorFlowModel(\n",
    "    model_data=f\"s3://{bucket_name}/model/model.tar.gz\",\n",
    "    entry_point='inference.py',\n",
    "    role=role,\n",
    "    framework_version=\"2.3.1\",\n",
    "    sagemaker_session=sm_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Input Transform Job \n",
    "Create the Transform Job by specifying the CSV input data S3 location and `content_type` as `text/csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date=datetime.now().strftime('%Y-%m-%d-%H-%m-%S')\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_path_batch = \"s3://{}/output/batch_iris/\".format(bucket_name)\n",
    "output_data_path = output_data_path_batch\n",
    "batch_instance_count = 1\n",
    "batch_instance_type = \"ml.m5.4xlarge\"\n",
    "concurrency = 5\n",
    "max_payload_in_mb = 1\n",
    "split_type = \"Line\"\n",
    "batch_strategy =\"MultiRecord\"\n",
    "TFRecord_job_name = \"tensorflow-inference-TFRecord-{}\".format(date)\n",
    "\n",
    "transformer = tensorflow_serving_model_batch.transformer(\n",
    "  \n",
    "    instance_count=batch_instance_count,\n",
    "    instance_type=batch_instance_type,\n",
    "    max_concurrent_transforms=concurrency,\n",
    "    max_payload=max_payload_in_mb,\n",
    "    strategy=batch_strategy,\n",
    "    output_path=output_data_path\n",
    ")\n",
    "\n",
    "transformer.transform(data=s3_csv_data, content_type=\"text/csv\", split_type=split_type,  wait=False,   job_name=TFRecord_job_name)\n",
    "print(TFRecord_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord Input Transform Job \n",
    "### CSV Input Transform Job\n",
    "Create the Transform Job by specifying the TFRecord input data S3 location and `content_type` as `application/x-tfrecord`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_path_batch = \"s3://{}/output/batch_iris/\".format(bucket_name)\n",
    "\n",
    "output_data_path = output_data_path_batch\n",
    "batch_instance_count = 1\n",
    "batch_instance_type = \"ml.m5.4xlarge\"\n",
    "concurrency = 5\n",
    "max_payload_in_mb = 1\n",
    "split_type=\"TFRecord\"\n",
    "batch_strategy=\"MultiRecord\"\n",
    "CSV_job_name=\"tensorflow-inference-CSV-{}\".format(date)\n",
    "\n",
    "transformer = tensorflow_serving_model_batch.transformer(\n",
    "    instance_count=batch_instance_count,\n",
    "    instance_type=batch_instance_type,\n",
    "    max_concurrent_transforms=concurrency,\n",
    "    max_payload=max_payload_in_mb,\n",
    "    strategy=batch_strategy,\n",
    "    output_path=output_data_path\n",
    ")\n",
    "\n",
    "transformer.transform(data=s3_tf_record_data, content_type=\"application/x-tfrecord\", split_type=split_type,  wait=False, job_name=CSV_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Kindly navigate to the SageMaker Console to monitor the Job status.*"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
