{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R API Serving Examples\n",
    "\n",
    "In this example, we compare the runtimes of three methods for serving a model from an R hosted API:\n",
    "\n",
    "* **Plumber**\n",
    " * Website: [https://www.rplumber.io/](https://www.rplumber.io/)\n",
    " * SageMaker Example: [r_byo_algo_with_plumber](../r_byo_algo_with_plumber)\n",
    "* **RestRServe**\n",
    " * Website: [https://restrserve.org](https://restrserve.org/)\n",
    " * SageMaker Example: [r_byo_algo_with_restrserve](../r_byo_algo_with_restrserve)\n",
    "* **FastAPI** (reticulated from Python)\n",
    " * Website: [https://fastapi.tiangolo.com](https://fastapi.tiangolo.com/)\n",
    " * SageMaker Example: [r_byo_algo_with_fastapi](../r_byo_algo_with_fastapi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Docker Images for Serving\n",
    "\n",
    "First, let's build each docker image from the provided SageMaker Examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plumber Serving Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  121.9kB\n",
      "Step 1/9 : FROM r-base:3.6.3\n",
      " ---> cec2502269fb\n",
      "Step 2/9 : MAINTAINER Amazon SageMaker Examples <amazon-sagemaker-examples@amazon.com>\n",
      " ---> Using cache\n",
      " ---> d5c7ee17124e\n",
      "Step 3/9 : RUN apt-get -y update && apt-get install -y --no-install-recommends     wget     apt-transport-https     ca-certificates     libcurl4-openssl-dev     libsodium-dev\n",
      " ---> Using cache\n",
      " ---> 9045f663fee5\n",
      "Step 4/9 : RUN R -e \"install.packages(c('xgboost','plumber'), repos='https://cloud.r-project.org')\"\n",
      " ---> Using cache\n",
      " ---> f3e0508834a6\n",
      "Step 5/9 : COPY xgb.model /opt/ml/xgb.model\n",
      " ---> Using cache\n",
      " ---> 48db5ef0c627\n",
      "Step 6/9 : COPY endpoints.R /opt/ml/endpoints.R\n",
      " ---> Using cache\n",
      " ---> 00b19c2b8b4f\n",
      "Step 7/9 : COPY deploy.R /opt/ml/deploy.R\n",
      " ---> Using cache\n",
      " ---> 3732e470829d\n",
      "Step 8/9 : WORKDIR /opt/ml\n",
      " ---> Using cache\n",
      " ---> aa8b88ff67c2\n",
      "Step 9/9 : ENTRYPOINT [\"/usr/bin/Rscript\", \"/opt/ml/deploy.R\", \"--no-save\"]\n",
      " ---> Using cache\n",
      " ---> afc7d7621ec2\n",
      "Successfully built afc7d7621ec2\n",
      "Successfully tagged r-plumber:latest\n"
     ]
    }
   ],
   "source": [
    "!cd .. && docker build -t r-plumber -f r_byo_algo_with_plumber/Dockerfile r_byo_algo_with_plumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RestRServe Serving Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  107.5kB\n",
      "Step 1/7 : FROM r-base:3.6.3\n",
      " ---> cec2502269fb\n",
      "Step 2/7 : MAINTAINER Amazon SageMaker Examples <amazon-sagemaker-examples@amazon.com>\n",
      " ---> Using cache\n",
      " ---> d5c7ee17124e\n",
      "Step 3/7 : RUN R -e \"install.packages(c('RestRserve','xgboost','dplyr'), repos='https://cloud.r-project.org')\"\n",
      " ---> Using cache\n",
      " ---> ebcf2f81ff2d\n",
      "Step 4/7 : COPY xgb.model /opt/ml/xgb.model\n",
      " ---> Using cache\n",
      " ---> d3c15c4582c7\n",
      "Step 5/7 : COPY restrserve.R /opt/ml/restrserve.R\n",
      " ---> Using cache\n",
      " ---> 558d27d04a7e\n",
      "Step 6/7 : WORKDIR /opt/ml\n",
      " ---> Using cache\n",
      " ---> fd6a29e89e0b\n",
      "Step 7/7 : ENTRYPOINT [\"/usr/bin/Rscript\", \"/opt/ml/restrserve.R\", \"--no-save\"]\n",
      " ---> Using cache\n",
      " ---> 9cfd01394754\n",
      "Successfully built 9cfd01394754\n",
      "Successfully tagged r-restrserve:latest\n"
     ]
    }
   ],
   "source": [
    "!cd .. && docker build -t r-restrserve -f r_byo_algo_with_restrserve/Dockerfile r_byo_algo_with_restrserve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastAPI Serving Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon    129kB\n",
      "Step 1/11 : FROM r-base:3.6.3\n",
      " ---> cec2502269fb\n",
      "Step 2/11 : MAINTAINER Amazon SageMaker Examples <amazon-sagemaker-examples@amazon.com>\n",
      " ---> Using cache\n",
      " ---> d5c7ee17124e\n",
      "Step 3/11 : RUN apt-get -y update && apt-get install -y --no-install-recommends     wget     r-base     r-base-dev     apt-transport-https     ca-certificates     python3 python3-dev pip\n",
      " ---> Using cache\n",
      " ---> 627800afb90b\n",
      "Step 4/11 : RUN pip install fastapi uvicorn numpy\n",
      " ---> Using cache\n",
      " ---> 34ca8248acac\n",
      "Step 5/11 : RUN R -e \"install.packages('reticulate', repos='https://cloud.r-project.org')\"\n",
      " ---> Using cache\n",
      " ---> 9ec2537f2bc4\n",
      "Step 6/11 : RUN R -e \"install.packages(c('xgboost'), repos='https://cloud.r-project.org')\"\n",
      " ---> Using cache\n",
      " ---> e9dc3ef54fec\n",
      "Step 7/11 : COPY endpoints.py /opt/ml/endpoints.py\n",
      " ---> Using cache\n",
      " ---> c93e0e4cfa76\n",
      "Step 8/11 : COPY deploy.R /opt/ml/deploy.R\n",
      " ---> Using cache\n",
      " ---> fcc54e3ec14b\n",
      "Step 9/11 : COPY xgb.model /opt/ml/xgb.model\n",
      " ---> Using cache\n",
      " ---> 0f2b1f97e7a4\n",
      "Step 10/11 : WORKDIR /opt/ml\n",
      " ---> Using cache\n",
      " ---> 2b0d89d5ea53\n",
      "Step 11/11 : ENTRYPOINT [\"/usr/bin/Rscript\", \"/opt/ml/deploy.R\", \"--no-save\"]\n",
      " ---> Using cache\n",
      " ---> bffd71d8e8ca\n",
      "Successfully built bffd71d8e8ca\n",
      "Successfully tagged r-fastapi:latest\n"
     ]
    }
   ],
   "source": [
    "!cd .. && docker build -t r-fastapi -f r_byo_algo_with_fastapi/Dockerfile r_byo_algo_with_fastapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Serving Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will launch each search container. The containers will be launch on the following ports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports = {\n",
    "    \"plumber\": 5000,\n",
    "    \"restrserve\": 5001,\n",
    "    \"fastapi\": 5002,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Plumber\n",
      "34d7ca67f79b0dfdb4447292abf8e2721fddd7950fa0de06b92741a48ed00dde\n",
      "Launching RestRServer\n",
      "b4eb855317938b81cbdaad271f620b7a37bd5e1bd4a173b50a476702b7bcd54e\n",
      "Launching FastAPI\n",
      "5a3d76225727ce7eae1d34239543278322d8e0781520e91df68e26f2fd572d7e\n"
     ]
    }
   ],
   "source": [
    "!bash launch.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                  PORTS                    NAMES\n",
      "5a3d76225727        r-fastapi           \"/usr/bin/Rscript /o…\"   1 second ago        Up Less than a second   0.0.0.0:5002->8080/tcp   agitated_ganguly\n",
      "b4eb85531793        r-restrserve        \"/usr/bin/Rscript /o…\"   2 seconds ago       Up 1 second             0.0.0.0:5001->8080/tcp   musing_grothendieck\n",
      "34d7ca67f79b        r-plumber           \"/usr/bin/Rscript /o…\"   2 seconds ago       Up 1 second             0.0.0.0:5000->8080/tcp   flamboyant_maxwell\n"
     ]
    }
   ],
   "source": [
    "!docker container list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Simple Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(examples, instance=requests, port=5000):\n",
    "    payload = { \"features\": examples }\n",
    "    return instance.post(f\"http://127.0.0.1:{port}/invocations\", json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_health(instance=requests, port=5000):\n",
    "    instance.get(f\"http://127.0.0.1:{port}/ping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Example Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's a define an example from the [iris.csv](iris.csv) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features = iris[['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = iris_features.values[:1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_examples = iris_features.values[:100].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test how each API server performs under stress.\n",
    "\n",
    "First, let's test how each one performs when each request generates a new connection. \n",
    "\n",
    "We will test the performance on following situations:\n",
    "* 1000 requests of a single example\n",
    "* 1000 requests of 100 examples\n",
    "* 1000 pings for health status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(example,port=ports[\"plumber\"]).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 127.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(example,port=ports[\"plumber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 99.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(many_examples, port=ports[\"plumber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 337.57it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    get_health(port=ports[\"plumber\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RestRserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outputs': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(example,port=ports[\"restrserve\"]).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:21<00:00, 46.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(example,port=ports[\"restrserve\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:24<00:00, 41.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(many_examples,port=ports[\"restrserve\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 130.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    get_health(port=ports[\"restrserve\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 0.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(example,port=ports[\"fastapi\"]).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 212.88it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(example,port=ports[\"fastapi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 165.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(many_examples,port=ports[\"fastapi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 451.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    get_health(port=ports[\"fastapi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep Alive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test how each one performs when each request reuses a session connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse the session for each post and get request\n",
    "instance=requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:52<00:00, 19.01it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(example, instance=instance, port=ports[\"plumber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:53<00:00, 18.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(many_examples, instance=instance, port=ports[\"plumber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:44<00:00, 22.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    get_health(instance=instance, port=ports[\"plumber\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RestRserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 191.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(example, instance=instance, port=ports[\"restrserve\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 163.42it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(many_examples, instance=instance, port=ports[\"restrserve\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 460.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    get_health(instance=instance, port=ports[\"restrserve\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 262.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(example, instance=instance, port=ports[\"fastapi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 193.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    _ = get_predictions(many_examples, instance=instance, port=ports[\"fastapi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 585.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    get_health(instance=instance, port=ports[\"fastapi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop All Serving Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's shutdown the serving containers we launched for the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5a3d76225727\n",
      "b4eb85531793\n",
      "34d7ca67f79b\n"
     ]
    }
   ],
   "source": [
    "!docker kill $(docker ps -q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
