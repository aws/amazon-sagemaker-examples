{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8589658e-8fa7-48ec-953d-27d2fbd5049e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train Text Classification Model using BERT and Smart Sifting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58d1c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293f7ad-a1e7-4405-9f6f-80cfc2c219fb",
   "metadata": {},
   "source": [
    "In this notebook we will train a text classification model using BERT (Transformers) and Smart Sifting library. BERT is a transformers encoder model pretrained on a large corpus of English data in a self-supervised fashion. This model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification, and question answering. We will be building an BERT based Sentiment Analysis model using SST datasaet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a533980e-69e9-4a98-bf01-abd45de27a9f",
   "metadata": {},
   "source": [
    "## 1.Introduction to Smart Sifting \n",
    "\n",
    "Smart Sifting is a framework to speed up training of PyTorch models. The framework implements a set of algorithms that filter out inconsequential training examples during training, reducing the computational cost and accelerating the training process. It is configuration-driven and extensible, allowing users to add custom logic to transform their training examples into a filterable format. Smart sifting provides a generic utility for any DNN model, and can reduce the training cost by up to 35% in infrastructure cost.\n",
    "\n",
    "![image](sifting_flow.png)\n",
    "\n",
    "\n",
    "\n",
    "Smart sifting’s task is to sift through your training data during the training process and only feed the more informative samples to the model. During typical training with PyTorch, data is iteratively sent in batches to the training loop and to accelerator devices (e.g. GPUs or Trainium chips) by the PyTorch data loader. Smart sifting is implemented at this data loading stage and is thus independent of any upstream data preprocessing in your training pipeline. Smart sifting uses your live model and a user specified loss function to do an evaluative forward pass of each data sample as it is loaded. Samples which are high loss will materially impact model training and thus are included in training data; meanwhile data samples which are relatively low loss are already well represented by the model and so are set aside and excluded from training. A key input to smart sifting is the proportion of data to exclude: for example, by setting the proportion to 25%, samples in approximately the bottom quartile of loss of each batch will be excluded from training. Once enough high-loss samples have been identified to complete a batch, the data is sent through the full training loop and the model learns and trains normally. Customers don’t need to make any downstream changes to their training loop when smart sifting is enabled.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20242de-818a-47eb-977a-7df7c54d0908",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset\n",
    "\n",
    "For this training we will be using [SST2](https://huggingface.co/datasets/sst2). SST2 consists of positive/negative sentiment texts with roughly about 11k sentences extracted from movie reviews.\n",
    "\n",
    "Lets start by downloading and extracting the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a415ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp --recursive s3://sagemaker-example-files-prod-us-west-2/datasets/text/SST2/ ./text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80279f68",
   "metadata": {},
   "source": [
    "We will convert the dataset into a tsv file which will be uploaded to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"text/sst2.train\", \"r\")\n",
    "input_lines = train_file.readlines()\n",
    "train_data = []\n",
    "for line in input_lines:\n",
    "    label = line[0]\n",
    "    text = line[2:].strip()\n",
    "    data = {}\n",
    "    data[\"label\"] = label\n",
    "    data[\"sentence\"] = text\n",
    "    train_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trainDF = pd.DataFrame(train_data)\n",
    "\n",
    "trainDF.to_csv(\"train.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69024932",
   "metadata": {},
   "source": [
    "Upload the TSV file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d385a23-74ad-4c2f-80bf-23e67f9505a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990a77e-26db-4532-a0d4-c7b5f91880d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8341f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to s3\n",
    "\n",
    "train_data_url = sess.upload_data(\n",
    "    path=\"train.tsv\",\n",
    "    key_prefix=\"data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6999be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data uploaded at {train_data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb08e02",
   "metadata": {},
   "source": [
    "## 3. Run training Job using SageMaker Training.\n",
    "\n",
    "\n",
    "Adding Sifting library to the Image classification code involves following the below steps\n",
    "\n",
    "1. **Define Loss Function** - For Image classification we use CrossEntropy loss defined as below\n",
    "    ````\n",
    "\n",
    "    class BertLoss(Loss):\n",
    "    \"\"\"\n",
    "    This is an implementation of the Loss interface for the BERT model\n",
    "    required for Smart Sifting. Use Cross-Entropy loss with 2 classes\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.celoss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def loss(\n",
    "            self,\n",
    "            model: torch.nn.Module,\n",
    "            transformed_batch: SiftingBatch,\n",
    "            original_batch: Any = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # get original batch onto model device. Note that we are assuming the model is on the right device here   already\n",
    "        # Pytorch lightning takes care of this under the hood with the model thats passed in.\n",
    "        # TODO: ensure batch and model are on the same device in SiftDataloader so that the customer\n",
    "        #  doesn't have to implement this\n",
    "        device = next(model.parameters()).device\n",
    "        batch = [t.to(device) for t in original_batch]\n",
    "\n",
    "        # compute loss\n",
    "        outputs = model(batch)\n",
    "        return self.celoss(outputs.logits, batch[2])\n",
    "\n",
    "    ````\n",
    "2. **Define Transformation Function** to convert input batch to sifting format.\n",
    "\n",
    "````\n",
    "class BertListBatchTransform(SiftingBatchTransform):\n",
    "    \"\"\"\n",
    "    This is an implementation of the data transforms for the BERT model\n",
    "    required for Smart Sifting. Transform to and from ListBatch\n",
    "    \"\"\"\n",
    "    def transform(self, batch: Any):\n",
    "        inputs = []\n",
    "        for i in range(len(batch[0])):\n",
    "            inputs.append((batch[0][i], batch[1][i]))\n",
    "\n",
    "        labels = batch[2].tolist()  # assume the last one is the list of labels\n",
    "        return ListBatch(inputs, labels)\n",
    "\n",
    "    def reverse_transform(self, list_batch: ListBatch):\n",
    "        inputs = list_batch.inputs\n",
    "        input_ids = [iid for (iid, _) in inputs]\n",
    "        masks = [mask for (_, mask) in inputs]\n",
    "        a_batch = [torch.stack(input_ids), torch.stack(masks), torch.tensor(list_batch.labels, dtype=torch.int64)]\n",
    "        return a_batch\n",
    "````\n",
    "\n",
    "\n",
    "3. **Define sifting config** - Define configuration for sifting. \n",
    "\n",
    "    Beta_value depicts the proportion of samples to keep , higher the value more samples are sifted. \n",
    "    loss_history_length - Depicts the window of samples to include when evaluating relative loss.\n",
    "\n",
    "````\n",
    "   sift_config = RelativeProbabilisticSiftConfig(\n",
    "            beta_value=3,\n",
    "            loss_history_length=500,\n",
    "            loss_based_sift_config=LossConfig(\n",
    "                 sift_config=SiftingBaseConfig(sift_delay=10)\n",
    "            )\n",
    "        )\n",
    "````\n",
    "4. **Wrap the Pytorch Data Loader with Sifting Data loader**.\n",
    "   As a last step we wrap the Pytroch Dataloader with siftingDataLoader passing config, transformation and loss functions.\n",
    "\n",
    "```\n",
    "   SiftingDataloader(\n",
    "            sift_config = sift_config,\n",
    "            orig_dataloader=DataLoader(self.train, self.batch_size, shuffle=True),\n",
    "            loss_impl=BertLoss(),\n",
    "            model=self.model,\n",
    "            batch_transforms=BertListBatchTransform()\n",
    "        )   \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9fffb-3067-4e9b-a647-c8ba1fe3dace",
   "metadata": {},
   "source": [
    "We define few metrics to be tracked inorder to monitor sifting. This are optional metrics useful to debug and understand sifting performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a4759-26d9-40e3-b4da-3e64437b86fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "\n",
    "SAGEMAKER_METRIC_DEFINITIONS = [\n",
    "    # Sifting Metrics\n",
    "    {\"Name\": \"num_samples_batch:count\", \"Regex\": \"num_orig_samples_seen: ([0-9.]+)\"},\n",
    "    {\"Name\": \"num_kept_batch:count\", \"Regex\": \"num_orig_samples_kept: ([0-9.]+)\"},\n",
    "    {\"Name\": \"batch_size:count\", \"Regex\": \"Batch size for next sifted batch: ([0-9.]+)\"},\n",
    "    {\n",
    "        \"Name\": \"sifted_batch_creation:latency\",\n",
    "        \"Regex\": \"sifted_batch_creation_latency: ([0-9.e+\\-]+)\",\n",
    "    },\n",
    "    {\"Name\": \"sifting_batch_transform:latency\", \"Regex\": \"func:transform latency: ([0-9.e+\\-]+)\"},\n",
    "    {\n",
    "        \"Name\": \"calc_rel_vals:latency\",\n",
    "        \"Regex\": \"func:calculate_relevance_values latency: ([0-9.e+\\-]+)\",\n",
    "    },\n",
    "    {\"Name\": \"should_sift:latency\", \"Regex\": \"func:should_sift latency: ([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"sift:latency\", \"Regex\": \"func:sift latency: ([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"accumulate:latency\", \"Regex\": \"func:accumulate latency: ([0-9.e+\\-]+)\"},\n",
    "    {\n",
    "        \"Name\": \"orig_batch_transform:latency\",\n",
    "        \"Regex\": \"func:reverse_transform latency: ([0-9.e+\\-]+)\",\n",
    "    },\n",
    "    # The following two metrics (rel_vals:number, kept_rel_vals:number) have multiple data points per log line, but\n",
    "    # this metric capturing method can only capture one data point per log line.\n",
    "    # TODO: capture all data points once we have a custom reporting API for sifting.\n",
    "    {\"Name\": \"rel_vals:number\", \"Regex\": \"relevance_values: \\[([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"kept_rel_vals:number\", \"Regex\": \"relevance_values_kept: \\[([0-9.e+\\-]+)\"},\n",
    "    # Training Metrics\n",
    "    {\"Name\": \"train_step:count\", \"Regex\": \"train step count: ([0-9.]+)\"},\n",
    "    {\"Name\": \"train_fp:latency\", \"Regex\": \"train forward pass latency: ([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"train_bp:latency\", \"Regex\": \"train backprop latency: ([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"train_optim:latency\", \"Regex\": \"train optimizer step latency: ([0-9.e+\\-]+)\"},\n",
    "    {\"Name\": \"train_total_step:latency\", \"Regex\": \"train total step latency: ([0-9.e+\\-]+)\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2993d2",
   "metadata": {},
   "source": [
    "We will launch the training job using G5.2xlarge instance. Sifting library is part of the SageMaker Pytorch Deep Learning containers starting version 2.0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d175d0-3d77-4bb6-84cd-6390cff39ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    source_dir=\"./scripts\",\n",
    "    entry_point=\"train_bert.py\",\n",
    "    role=role,\n",
    "    framework_version=\"2.0.1\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    disable_profiler=True,\n",
    "    metric_definitions=SAGEMAKER_METRIC_DEFINITIONS,\n",
    "    hyperparameters={\n",
    "        \"epochs\": 2,\n",
    "        \"num_nodes\": 1,\n",
    "        \"log_level\": 20,  # 10 is debug, 20 is info\n",
    "        \"log_batches\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4ad26",
   "metadata": {},
   "source": [
    "Launch the training job with Data in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea95edd-77eb-4a37-aa51-2fef6b102191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "estimator.fit(\n",
    "    inputs={\"data\": train_data_url},\n",
    "    job_name=f\"bert-cola-sifting-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10235e1e",
   "metadata": {},
   "source": [
    "In this notebook, we looked at how to use smart sifting library to train an Text classification (Sentiment analysis) model. Smart sifting helps in reducing training time upto 40% without any reduction in Model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed1e24d",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/training|smart_sifting|Text_Classification_BERT|Train_text_classification.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
