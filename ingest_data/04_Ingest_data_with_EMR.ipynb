{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Data with EMR\n",
    "\n",
    "This notebook demonstrates how to read the data from the EMR cluster.\n",
    "We are going to use the data we load into S3 in the previous notebook [011_Ingest_tabular_data.ipynb](011_Ingest_tabular_data_v1.ipynb).\n",
    "\n",
    "Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Notebook\n",
    "First, we are going to make sure we have the EMR Cluster set up and the connection between EMR and Sagemaker Notebook set up correctly. You can follow the [documentation](https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/) and [procedure](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-lifecycle-config-emr.html) to set up this notebook. Once you are done with setting up, restart the kernel and run the following command to check if you set up the EMR and Sagemaker connection correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import sagemaker\n",
    "from sklearn.datasets import *\n",
    "import pandas as pd\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3 = sagemaker_session.boto_session.resource('s3')\n",
    "bucket = sagemaker_session.default_bucket() #replace with your own bucket name if you have one\n",
    "prefix = 'data/tabular/boston_house'\n",
    "filename = 'boston_house.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from online resources and write data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "#helper functions to upload data to s3\n",
    "def write_to_s3(filename, bucket, prefix):\n",
    "    #put one file in a separate folder. This is helpful if you read and prepare data with Athena\n",
    "    filename_key = filename.split('.')[0]\n",
    "    key = \"{}/{}/{}\".format(prefix,filename_key,filename)\n",
    "    return s3.Bucket(bucket).upload_file(filename,key)\n",
    "\n",
    "def upload_to_s3(bucket, prefix, filename):\n",
    "    url = 's3://{}/{}/{}'.format(bucket, prefix, filename)\n",
    "    print('Writing to {}'.format(url))\n",
    "    write_to_s3(filename, bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "tabular_data = load_boston()\n",
    "tabular_data_full = pd.DataFrame(tabular_data.data, columns=tabular_data.feature_names)\n",
    "tabular_data_full['target'] = pd.DataFrame(tabular_data.target)\n",
    "tabular_data_full.to_csv('boston_house.csv', index = False)\n",
    "\n",
    "upload_to_s3(bucket, 'data/tabular', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "data_s3_path = 's3://{}/{}/{}'.format(bucket, prefix, filename)\n",
    "print ('this is path to your s3 files: '+data_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the S3 bucket file path\n",
    "The S3 bucket file path is required to read the data on EMR Spark. Copy and paste the path string shown above into the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace this path string with your path shown in last step\n",
    "data_s3_path = 's3://sagemaker-us-east-2-060356833389/data/tabular/boston_house/boston_house.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data in EMR spark Cluster\n",
    "\n",
    "Once we have a path to our data in S3, we can use `spark s3 select` to read data with the following command. You can specify a data format, schema is not necessary but recommended, and in options you can specify `compression`, `delimiter`, `header`, etc. For more details, please see [documentation on using S3 select with Spark](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-s3select.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR cell\n",
    "schema = ' CRIM double, ZN double, INDUS double,\\\n",
    "CHAS double, NOX double, RM double,  AGE double, DIS double,  RAD double,  TAX double, PTRATIO double, \\\n",
    "B double,  LSTAT double, target double'\n",
    "df = spark.read.format('csv').schema(schema).options(header='true').load(data_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Now that you have read in the data, you can pre-process the data with Spark in an EMR cluster, build an ML pipeline, and train models in scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "Boston Housing data,  Harrison, D. and Rubinfeld, D.L. `Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
