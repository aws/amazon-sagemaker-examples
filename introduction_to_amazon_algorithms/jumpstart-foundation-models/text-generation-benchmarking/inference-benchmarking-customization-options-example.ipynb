{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart Foundation Models - Benchmark Latency and Throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Welcome to Amazon [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)! You can use SageMaker JumpStart to solve many Machine Learning tasks through one-click in SageMaker Studio, or through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#use-prebuilt-models-with-sagemaker-jumpstart).\n",
    "\n",
    "When testing a large language model for production use cases, common questions arise, such as: \n",
    "- What is the inference latency for my expected payloads?\n",
    "- How much throughput does this model configuration provide for my expected payloads?\n",
    "- What is the inference throughput and latency for my expected concurrency load, i.e., the number of concurrent requests that have invoked the endpoint?\n",
    "- How much does it cost to generate 1 million tokens?\n",
    "- How does instance type selection (e.g., `ml.g5.2xlarge`) affect latency and throughput?\n",
    "- How does modification of the deployment configuration (e.g., tensor parallel degree) affect latency and throughput?\n",
    "\n",
    "Given these questions, you may notice that inference latency and throughput depend on numerous factors, to include payload, number of concurrent requests, instance type, deployment configuration, and more. In this notebook, we demonstrate how you can run your own latency and throughput benchmark for SageMaker JumpStart endpoints. This benchmarking process involves running load tests with various concurrent request values for each payload and deployed endpoint.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet sagemaker transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The primary inputs to this benchmarking tool include the models to benchmark and the payloads used to invoke endpoints.\n",
    "- **`MODELS`**: A dictionary mapping a unique name to benchmarking configuration. The model can be defined in 3 different ways. Each model value should be a dictionary with the following keys:\n",
    "  - **`jumpstart_model_specs` key**: requires `model_args` and optionally `deploy_args` definitions to use with a SageMaker SDK `JumpStartModel` constructor and deploy methods, respectfully. This should be used to deploy and benchmark a JumpStart model.\n",
    "  - **`model_specs` key**: requires `image_uri_args`, `model_args`, and `deploy_args` definitions to use with a SageMaker SDK `Model` constructor and deploy methods. This should be used to deploy and benchmark a non-JumpStart model.\n",
    "  - **`endpoint_name` key**: provide the endpoint name of a pre-deployed model to benchmark.\n",
    "  - **`huggingface_model_id` key**: to compute metrics with respect to model tokens, provide the HuggingFace Model ID with an appropriate tokenizer to use.\n",
    "- **`PAYLOADS`**: A dictionary mapping a unique name to a payload of interest. The benchmarking tool will serially run a concurrency probe against each payload.\n",
    "\n",
    "For this notebook, we deploy Falcon 7B using `JumpStartModel`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarking.payload import create_test_payload\n",
    "\n",
    "\n",
    "PAYLOADS = {\n",
    "    \"input_128_output_128\": create_test_payload(input_words=128, output_tokens=128),\n",
    "    \"input_512_output_128\": create_test_payload(input_words=512, output_tokens=128),\n",
    "}\n",
    "\n",
    "MODELS = {\n",
    "    \"falcon-7b-jumpstart\": {\n",
    "        \"jumpstart_model_specs\": {\"model_args\": {\"model_id\": \"huggingface-llm-falcon-7b-bf16\"}},\n",
    "        \"huggingface_model_id\": \"tiiuae/falcon-7b\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The default concurrency probe will iteratively produce loads to the endpoint with concurrent request values of $2^x$ for $x\\ge 0$ and stop once the endpoint produces an error, most often a SageMaker 60s endpoint invocation timeout. Here, we show how to create a custom concurrency probe iterator object with a different concurrent request schedule and an additional stop iteration criteria when latency goes above an undesirable threshold.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarking.concurrency_probe import ConcurrentProbeIteratorBase\n",
    "\n",
    "\n",
    "class CustomConcurrencyProbeIterator(ConcurrentProbeIteratorBase):\n",
    "    \"\"\"A custom concurrency probe iterator to explore concurrent request multiples with max latency stop criteria.\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.concurrent_requests = 20\n",
    "        self.increment_value = 20\n",
    "        self.max_latency_per_token_ms = 100.0\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> int:\n",
    "        if self.exception is not None:\n",
    "            e = self.exception\n",
    "            self.stop_reason = \"\".join([type(e).__name__, f\": {e}\" if str(e) else \"\"])\n",
    "            raise StopIteration\n",
    "\n",
    "        if self.result is None:\n",
    "            return self.concurrent_requests\n",
    "\n",
    "        last_latency_per_token_ms = self.result[\"LatencyPerToken\"][\"p90\"]\n",
    "        if last_latency_per_token_ms > self.max_latency_per_token_ms:\n",
    "            self.stop_reason = (\n",
    "                f\"Last p90 latency = {last_latency_per_token_ms} > {self.max_latency_per_token_ms}.\"\n",
    "            )\n",
    "            raise StopIteration\n",
    "\n",
    "        self.concurrent_requests = self.concurrent_requests + self.increment_value\n",
    "\n",
    "        return self.concurrent_requests\n",
    "\n",
    "\n",
    "def num_invocation_scaler_with_minimum(\n",
    "    concurrent_requests: int, factor: int = 5, max_invocations: int = 200\n",
    ") -> int:\n",
    "    return min(concurrent_requests * factor, max_invocations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Now create a `Benchmarker` object and run benchmarking for all models. This will first concurrently create a `Predictor` for all models. If `endpoint_name` is specified in the `MODELS` definition or provided in the JSON metrics file of a previous run, the endpoint will be attached to a `Predictor`. Otherwise, a new endpoint will be deployed. Once an endpoint is in service, it will begin the load test concurrency probe. A concurrency probe will be executed concurrently for all models. For each model, the probe will sweep concurrent request values, performing a load test at each unique value, until an error occurs. These errors may be validation checks (e.g., endpoint is overloaded, input sequence length unsupported, etc.), SageMaker invocation timeout, or any other potential model error. The concurrency probe for each specified payload will run serially for each model. When the probe has completed, all computed metrics will be saved in a JSON file for downstream analysis.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarking.runner import Benchmarker\n",
    "\n",
    "\n",
    "benchmarker = Benchmarker(\n",
    "    payloads=PAYLOADS,\n",
    "    run_concurrency_probe=True,\n",
    "    concurrency_probe_concurrent_request_iterator_cls=CustomConcurrencyProbeIterator,\n",
    "    concurrency_probe_num_invocation_hook=num_invocation_scaler_with_minimum,\n",
    ")\n",
    "metrics = benchmarker.run_multiple_models(models=MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Now that benchmarking is complete, let's load the results into a Pandas DataFrame and create a pivot table that shows throughput, p90 latency, and cost to generate one million tokens. Many variations of these metrics are recorded in the DataFrame, so please extract any information relevant to your benchmarking effort.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from benchmarking.runner import Benchmarker\n",
    "\n",
    "\n",
    "try:\n",
    "    df = Benchmarker.load_metrics_pandas()\n",
    "    df_pivot = Benchmarker.create_concurrency_probe_pivot_table(df)\n",
    "\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.max_colwidth\", 0)\n",
    "    pd.set_option(\"display.max_rows\", 500)\n",
    "    display(df_pivot)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Finally, please remember to clean up all model and endpoint resources to avoid incurring additional costs after your benchmarking is complete.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarker.clean_up_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|text-generation-benchmarking|inference-benchmarking-customization-options-example.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
