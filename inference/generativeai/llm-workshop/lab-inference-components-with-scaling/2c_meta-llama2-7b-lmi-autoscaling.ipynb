{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a387c0a0-961d-427b-aa7a-7a0e5e5b9bc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Llama2-7b\n",
    "In this notebook we will create and deploy a Llama2-7b using inference components on the endpoint you created in the first notebook. For this model we will be using  the SageMaker Large Model Inference (LMI) container.  We will also be using one GPU for each model copy of the inference component we create. After creating the inference component we also show you how to set auto scaling policies to manage the number of copies of your inference component. We also use managed instance scaling which will scale the number of instances in your endpoint properlly in relation to your inference componenets. This is the 4rth notebook in a series of 5 notebooks used to deploy a model against the endpoint you created in the first notebook. The last notebook will show you other apis available and clean up the artifacts created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b797b9-7f60-463d-8a93-5a5b38df99d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b4f64-ad6a-43c9-838f-c8ce850ba62c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tested using the `Python 3 (Data Science)` kernel on SageMaker Studio and `conda_python3` kernel on SageMaker Notebook Instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffcfa8-02c5-4492-af5c-fdcf60f369db",
   "metadata": {},
   "source": [
    "# Licence agreement\n",
    " - View license information https://huggingface.co/meta-llama before using the model.\n",
    " - This notebook is a sample notebook and not intended for production use. Please refer to the licence at https://github.com/aws/mit-0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f64cae0-b6d5-4fae-888c-c4a1422c23f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install dependencies\n",
    "\n",
    "Upgrade the SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30361a-4aef-4bf7-8280-05d5eea8c00d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 huggingface_hub --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d581f-b82e-4e4a-855e-edaa7db5c8d3",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0f6e8-8ecb-4c9e-88af-34fa773074c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42f66a-b6cf-4e09-bd77-c8b8ce1e6ab6",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b4ea4-98aa-43d6-b98e-d7f0e00d81a6",
   "metadata": {},
   "source": [
    "`REPLACE` the `endpoint_name` value with the created endpoint from the first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd8b740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r \\\n",
    "endpoint_name\n",
    "\n",
    "if \"endpoint_name\" not in locals():\n",
    "    print(\"Please specify the endpoint_name before proceed.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Endpoint name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc2994-e847-4783-b136-e3436a94bf69",
   "metadata": {},
   "source": [
    "We first by creating the objects we will need for our notebook. In particular, the boto3 library to create the various clients we will need to interact with SageMaker and other variables that will be referenced later in our notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19782ebf-8124-4d52-afff-38807496fd45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d465f3f-2ad6-430f-a064-402e4c0b76f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "cloudwatch_client = sess.boto_session.client(\"cloudwatch\")\n",
    "aas_client = sess.boto_session.client(\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b5181-d018-4564-9762-fa8770a9672f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bucket = sess.default_bucket()  # bucket to house model artifacts\n",
    "\n",
    "s3_model_prefix = \"hf-large-model-djl/meta-llama/Llama-2-7b-fp16/model\"  # folder within bucket where model artifact will go\n",
    "s3_code_prefix = \"hf-large-model-djl/meta-llama/Llama-2-7b-fp16/code\"\n",
    "\n",
    "default_bucket_prefix = sess.default_bucket_prefix\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if default_bucket_prefix:\n",
    "    s3_model_prefix = f\"{default_bucket_prefix}/{s3_model_prefix}\"\n",
    "    s3_code_prefix = f\"{default_bucket_prefix}/{s3_code_prefix}\"\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913de41-193a-4bfb-b42e-581c7677d0ed",
   "metadata": {},
   "source": [
    "## Create SageMaker compatible Model artifact,  upload Model to S3 and bring your own inference script.\n",
    "\n",
    "SageMaker Large Model Inference containers can be used to host models without providing your own inference code. This is extremely useful when there is no custom pre-processing of the input data or postprocessing of the model's predictions.\n",
    "\n",
    "SageMaker needs the model artifacts to be in a Tarball format. In this example, we provide the following files - serving.properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f5d44-6388-49fa-bc87-2e82bed226f3",
   "metadata": {},
   "source": [
    "#### Create serving.properties \n",
    "This is a configuration file to indicate to DJL Serving which model parallelization and inference optimization libraries you would like to use. Depending on your need, you can set the appropriate configuration.\n",
    "\n",
    "Here is a list of settings that we use in this configuration file -\n",
    "\n",
    "    engine: The engine for DJL to use. In this case, we have set it to MPI.\n",
    "    option.model_id: The model id of a pretrained model hosted inside a model repository on huggingface.co (https://huggingface.co/models) or S3 path to the model artefacts. \n",
    "    option.tensor_parallel_degree: Set to the number of GPU devices over which Accelerate needs to partition the model. This parameter also controls the no of workers per model which will be started up when DJL serving runs. As an example if we have a 4 GPU machine and we are creating 4 partitions then we will have 1 worker per model to serve the requests.\n",
    "\n",
    "For more details on the configuration options and an exhaustive list, you can refer the documentation - https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2691f5b-4e98-4f7a-887c-49c05bbf7a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf llama2_7b_fp16\n",
    "!mkdir -p llama2_7b_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b040a8-2180-46cd-aa5c-b1f6d50c2dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile llama2_7b_fp16/serving.properties\n",
    "engine = MPI\n",
    "option.tensor_parallel_degree = 1\n",
    "option.rolling_batch = auto\n",
    "option.max_rolling_batch_size = 8\n",
    "option.model_loading_timeout = 3600\n",
    "option.model_id = s3://sagemaker-example-files-prod-us-west-2/models/llama-2/fp16/7B/\n",
    "option.paged_attention = true\n",
    "option.trust_remote_code = true\n",
    "option.dtype = fp16\n",
    "option.enable_streaming=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9db9c4-5023-4125-a413-7c5afa135218",
   "metadata": {},
   "source": [
    "**Image URI for the DJL container is being used here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfabe0a-04f8-486d-94ab-7d6066680954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905903de-a4b9-4f41-8cc2-564416ae5d5f",
   "metadata": {},
   "source": [
    "**Create the Tarball and then upload to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005aa2e-ec1a-4ccf-8f39-67bcbabd0309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz llama2_7b_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83ba3b-2ea5-4297-8e85-f16dd4c7c13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", model_bucket, s3_code_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82153c94-ab0e-4ff6-99af-6a61c0043b24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "\n",
    "model_name = f\"{prefix}-model\"\n",
    "print(f\"Test model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893ecb0-e0fe-450a-8bb1-5709d07b6e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check to see the status of our SageMaker endpoint\n",
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428cd1ea-27fe-4093-a07f-427d899c9b0b",
   "metadata": {},
   "source": [
    "## Create Inference Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f555f4d-a836-450d-be15-65cd2cb0b848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_component_name = f\"{prefix}-inference-component-0\"\n",
    "print(f\"Test inference component name: {inference_component_name}\")\n",
    "\n",
    "\n",
    "initial_copy_count = 1\n",
    "# inference component names if we deploy multiple of them\n",
    "max_copy_count_per_instance = 4  # up to 4 llama2 7b fp16 model\n",
    "inference_component_names = [\n",
    "    f\"{prefix}-inference-component-{i}\" for i in range(max_copy_count_per_instance)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ec2d7-6ef8-4383-ac50-6a4984af5c03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Inference Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356471f-6a04-458c-ba12-a99b2d3a2342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cf1e0-10ea-4dfb-8213-446d4992b824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variant_name = \"AllTraffic\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "min_memory_required_in_mb = 1024  # max memory util is up to 85%\n",
    "number_of_accelerator_devices_required = 1\n",
    "\n",
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        \"StartupParameters\": {\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            # \"NumberOfCpuCoresRequired\": number_of_cpu_cores_required,\n",
    "            \"MinMemoryRequiredInMb\": min_memory_required_in_mb,\n",
    "            \"NumberOfAcceleratorDevicesRequired\": number_of_accelerator_devices_required,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\n",
    "        \"CopyCount\": initial_copy_count,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168be22-697d-4cd9-af65-6f6c40963d97",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    desc = sm_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae49518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ic3_name = inference_component_name\n",
    "%store \\\n",
    "ic3_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aed031-b68c-4e23-8fa7-24483ec189d1",
   "metadata": {},
   "source": [
    "#### Leverage the Boto3 to invoke the endpoint. \n",
    "\n",
    "This is a generative model so we pass in a Text as a prompt and Model will complete the sentence and return the results.\n",
    "\n",
    "You can pass a prompt as input to the model. This done by setting inputs to a prompt. The model then returns a result for each prompt. The text generation can be configured using appropriate parameters.\n",
    "These parameters need to be passed to the endpoint as a dictionary of kwargs. Refer this documentation - https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig for more details.\n",
    "\n",
    "The below code sample illustrates the invocation of the endpoint using a text prompt and also sets some parameters. \n",
    "\n",
    "Note that we also apply an InferenceComponentName input to determine whch Inference Component the request should be directed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54753e-d261-43db-ba18-7cde5c3ea6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": \"The diamondback terrapin was the first reptile to be\",\n",
    "            \"parameters\": {\n",
    "                \"do_sample\": True,\n",
    "                \"max_new_tokens\": 256,\n",
    "                \"min_new_tokens\": 256,\n",
    "                \"temperature\": 0.3,\n",
    "                \"watermark\": True,\n",
    "            },\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242fbb1-5b51-4406-8c49-c8dc14f2032d",
   "metadata": {},
   "source": [
    "#### Scalable Target\n",
    "AAS creates two alarms for each autoscaling target\n",
    "* one to trigger scale-out: 3 minutes (3 one-minute data points)\n",
    "* another one to trigger scale-in: 15 minutes (15 one-minute data points)\n",
    "\n",
    "The time to trigger is usually 1 to 2 minutes longer than those because it take time for the endpoint to publish metrics to CloudWatch, and it also takes time for AAS to react."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660df6b-10f5-4741-8c73-5b2e84c1c564",
   "metadata": {},
   "source": [
    "# Application Auto Scaling\n",
    "In the following cells we will go through how to use Application Auto Scaling to scale your inference component copies. In addition, please note that in our first notebook we set `ManagedInstanceScaling` to be enabled. By doing this SageMaker will automatically scale your endpoint based on the needs of your inference components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa1605-040b-4acb-abda-d900b1f16d10",
   "metadata": {},
   "source": [
    "We can first start by setting the number of desired initial and max copies for an inference component. We will also specify a folder for our test results for our scaling test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a3c9f-07ec-4fa9-90df-e68825d10066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set  to a value of '5' to account for the number of accelerators remainin for two instances of ml.g5.12xlarge\n",
    "max_copy_count = 3\n",
    "print(f\"Initial copy count: {initial_copy_count}\")\n",
    "print(f\"Max copy county {max_copy_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5af7c0-c005-4598-affb-7d63e360ac51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_results_folder = \"test-results\"\n",
    "print(f\"Test results will be saved to folder {test_results_folder}\")\n",
    "test_start_time = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3145a96-8cfd-44d2-96ec-0eedf1f35a20",
   "metadata": {},
   "source": [
    "We can now set the values we will need to register a scalable target (in this case an inference component) with Application Auto Scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913178d2-4f25-42d2-90a8-7b8a455463ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Autoscaling parameters\n",
    "resource_id = f\"inference-component/{inference_component_name}\"\n",
    "service_namespace = \"sagemaker\"\n",
    "scalable_dimension = \"sagemaker:inference-component:DesiredCopyCount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f30f6-f8d9-422b-b2ab-e1b7439e46aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aas_client.register_scalable_target(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    MinCapacity=initial_copy_count,\n",
    "    MaxCapacity=max_copy_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a035316-9dae-4b27-8819-d29156e75175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aas_client.describe_scalable_targets(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceIds=[resource_id],\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559485c-8f9d-47ae-be82-04ad2e87925d",
   "metadata": {},
   "source": [
    "#### Scalable Policy\n",
    "Now that we have registered our scalable targets we can specify a scaling policy for our target. NOTE: If the scale-out cooldown is shorter than that the endpoint update time then it takes no effect, as it is not possible to update a SageMaker endpoint which is already in “Updating” status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560fc8d-af1c-4745-b32c-bbcb6cb46618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=endpoint_name,\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerInferenceComponentInvocationsPerCopy\",\n",
    "        },\n",
    "        # Low TPS + load TPS\n",
    "        \"TargetValue\": (4.0 / max_copy_count_per_instance)\n",
    "        + 1,  # you need to adjust this value based on your use case\n",
    "        \"ScaleInCooldown\": 300,  # default\n",
    "        \"ScaleOutCooldown\": 300,  # default\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5643d7b-3536-4a8f-b8af-a4dffcbc0b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aas_client.describe_scaling_policies(\n",
    "    PolicyNames=[endpoint_name],\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163ecc55-5a3a-4bef-9d88-654d2eab2096",
   "metadata": {},
   "source": [
    "### Run The Test\n",
    "We can now run a test to see the behavior of instance and managed auto scaling on SageMaker endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350665c-8c74-4742-b4a7-3babc5a2ba53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define some helper functions\n",
    "from dataclasses import dataclass\n",
    "import threading\n",
    "\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 2\n",
    "print(f\"Initial instance count: {initial_instance_count}\")\n",
    "print(f\"Max instance count: {max_instance_count}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AutoscalingStatus:\n",
    "    status_name: str  # endpoint status or inference component status\n",
    "    start_time: datetime  # when was the status changed\n",
    "    current_instance_count: int\n",
    "    desired_instance_count: int\n",
    "    current_copy_count: int\n",
    "    desired_copy_count: int\n",
    "\n",
    "\n",
    "class WorkerThread(threading.Thread):\n",
    "    def __init__(self, do_run, *args, **kwargs):\n",
    "        super(WorkerThread, self).__init__(*args, **kwargs)\n",
    "        self.__do_run = do_run\n",
    "        self.__terminate_event = threading.Event()\n",
    "\n",
    "    def terminate(self):\n",
    "        self.__terminate_event.set()\n",
    "\n",
    "    def is_terminated(self):\n",
    "        return self.__terminate_event.is_set()\n",
    "\n",
    "    def run(self):\n",
    "        while not self.__terminate_event.is_set():\n",
    "            self.__do_run(self.__terminate_event)\n",
    "\n",
    "\n",
    "invoke_endpoint_sanity_check_sample = {\n",
    "    \"inputs\": \"The diamondback terrapin was the first reptile to be\",\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"min_new_tokens\": 100,\n",
    "        \"temperature\": 0.3,\n",
    "        \"watermark\": True,\n",
    "    },\n",
    "}\n",
    "invoke_endpoint_sanity_check_payload = json.dumps(invoke_endpoint_sanity_check_sample)\n",
    "\n",
    "\n",
    "def invoke_endpoint_sanity_check(\n",
    "    sagemaker_runtime_client, endpoint_name, container_names=None, inference_component_name=None\n",
    "):\n",
    "    try:\n",
    "        parameters = {\n",
    "            \"EndpointName\": endpoint_name,\n",
    "            \"ContentType\": \"application/json\",\n",
    "            \"Body\": invoke_endpoint_sanity_check_payload,\n",
    "        }\n",
    "        if container_names is not None:\n",
    "            for container_name in container_names:\n",
    "                parameters[\"TargetContainerHostname\"] = container_name\n",
    "                response = sagemaker_runtime_client.invoke_endpoint(**parameters)\n",
    "        else:\n",
    "            if inference_component_name is not None:\n",
    "                parameters[\"InferenceComponentName\"] = inference_component_name\n",
    "            response = sagemaker_runtime_client.invoke_endpoint(**parameters)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to invoke {endpoint_name}: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1b9b3-0eba-4887-a911-08cc1092bc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_endpoint(terminate_event):\n",
    "    start_time = datetime.utcnow()\n",
    "    for _ in range(max_copy_count_per_instance * max_instance_count * 2):\n",
    "        invoke_endpoint_sanity_check(\n",
    "            smr_client, endpoint_name, inference_component_name=inference_component_name\n",
    "        )\n",
    "        time.sleep(0.1)\n",
    "    elapsed_seconds = (datetime.utcnow() - start_time).total_seconds()\n",
    "    if terminate_event.is_set():\n",
    "        return\n",
    "    if elapsed_seconds < 60:\n",
    "        time.sleep(60 - elapsed_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bd024-c073-45e2-99ee-8e2dd81aa1f1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keep invoking the endpoint with test data\n",
    "invoke_endpoint_thread = WorkerThread(do_run=invoke_endpoint)\n",
    "invoke_endpoint_thread.start()\n",
    "\n",
    "statuses = []\n",
    "while True:\n",
    "    endpoint_desc = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = endpoint_desc[\"EndpointStatus\"]\n",
    "    current_instance_count = endpoint_desc[\"ProductionVariants\"][0][\"CurrentInstanceCount\"]\n",
    "    desired_instance_count = endpoint_desc[\"ProductionVariants\"][0][\"DesiredInstanceCount\"]\n",
    "    ic_desc = sm_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    ic_status = ic_desc[\"InferenceComponentStatus\"]\n",
    "    current_copy_count = ic_desc[\"RuntimeConfig\"][\"CurrentCopyCount\"]\n",
    "    desired_copy_count = ic_desc[\"RuntimeConfig\"][\"DesiredCopyCount\"]\n",
    "    status_name = f\"{status}_{ic_status}\"\n",
    "    if not statuses or statuses[-1].status_name != status_name:\n",
    "        statuses.append(\n",
    "            AutoscalingStatus(\n",
    "                status_name=status_name,\n",
    "                start_time=datetime.utcnow(),\n",
    "                current_instance_count=current_instance_count,\n",
    "                desired_instance_count=desired_instance_count,\n",
    "                current_copy_count=current_copy_count,\n",
    "                desired_copy_count=desired_copy_count,\n",
    "            )\n",
    "        )\n",
    "        print(statuses[-1])\n",
    "    if status_name == \"InService_InService\":\n",
    "        if current_copy_count == 5:\n",
    "            invoke_endpoint_thread.terminate()\n",
    "        elif current_copy_count == initial_copy_count:\n",
    "            if invoke_endpoint_thread.is_terminated():\n",
    "                break\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25689efa-d96c-4409-8ff6-9ed8cee0605d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "invoke_endpoint_thread.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eafdf9c-fe17-4359-bf99-220e297a91db",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "We can delete and deregisterer our scaling policy and targets with Application Auto Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0e37e-5fd7-4b73-b89e-db7d4e25976c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aas_client.delete_scaling_policy(\n",
    "    PolicyName=endpoint_name,\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb65276-8b5b-4098-b74c-d74d555fa3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aas_client.deregister_scalable_target(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509493e2-af99-420c-a8a4-6831111906da",
   "metadata": {},
   "source": [
    "Thats it! You can now proceed to the third notebook where we will show you some miscellaneous functions and clean up our resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa3893-a084-4bee-8a5d-0bcb9b31b126",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c940a72-427e-4768-8bb4-c14dd0f0ca7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
