{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a485b55-4b06-4fa9-8148-0bb37603eb6a",
   "metadata": {},
   "source": [
    "# Build Predictor (XGBoost) Model\n",
    "\n",
    "We demonstrate building an ML application to predict the rings of Abalone.\n",
    "\n",
    "After the model is hosted for inference, the payload will be sent as a raw (untransformed) csv string to a real-time endpoint.\n",
    "The raw payload is first received by the preprocessor container. The raw payload is then transformed (feature-engineering) by the preprocessor, and the transformed record (float values) are returned as a csv string by the preprocessor container.\n",
    "\n",
    "The transformed record is then passed to the predictor container (XGBoost model). The predictor then converts the transformed record into [`XGBoost DMatrix`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix) format, loads the model, calls `booster.predict(input_data)` and returns the predictions (Rings) in a JSON format.\n",
    "\n",
    "![Abalone Predictor](../images/byoc-predictor.png)\n",
    "\n",
    "We use [nginx](https://nginx.org/) as the reverse proxy, [gunicorn](https://gunicorn.org/#deployment) as the web server gateway interface and the inference code as python [\"Flask\"](https://flask.palletsprojects.com/en/2.3.x/tutorial/factory/) app.\n",
    "\n",
    "## Dataset and model\n",
    "\n",
    "For this example, we use a pre-trained [XGBoost](https://xgboost.readthedocs.io) model on [UCI Abalone dataset](https://archive.ics.uci.edu/ml/datasets/abalone).\n",
    "Trained [xgboost-model](./models/xgboost-model) accepts input in `text/csv` format and returns prediction results in `application/json`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52a4c584-a793-4d1c-9e7d-692a9a8f3483",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "Ensure [`featurizer.ipynb`](../featurizer/featurizer.ipynb) is run first. \n",
    "We use `abalone_test_predictions.csv` file generated by [`featurizer.ipynb`](../featurizer/featurizer.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "912ecbf3-e4a5-494b-92da-cc3ded85f973",
   "metadata": {},
   "source": [
    "### Inference script for a predictor (XGBoost) model\n",
    "\n",
    "- In this example, we use a trained XGBoost model on the UCI abalone dataset. Trained model with name `xgboost-model` is available under `./models` directory\n",
    "- The inference code is implemented in [`code/inference.py`](./code/inference.py). The [Flask](https://flask.palletsprojects.com/) app implementation is as follows:\n",
    "  - Implement routes for `/ping` and `/invocations`\n",
    "  - Implement functions to handle preprocessing, model loading and prediction\n",
    "  - Predictions will be returned from `/invocations` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137679d4-58bb-464d-9acd-e3df30405266",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize ./code/inference.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6838fec2-f74a-4186-95b6-6b486afe1a7e",
   "metadata": {},
   "source": [
    "### Build and test custom inference image locally\n",
    "\n",
    " - [Dockerfile](./Dockerfile) implementation\n",
    "   - Set required LABEL `LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true`\n",
    "   - Installs required software and python packages ([nginx](https://nginx.org/), [gunicorn](https://gunicorn.org/#deployment), [flask](https://flask.palletsprojects.com/en/2.3.x/tutorial/factory/), [xgboost](https://xgboost.readthedocs.io/) etc ,; )\n",
    "   - Copies all files under code directory to `/opt/program`\n",
    "   - Sets `ENTRYPOINT` to `[\"python\"]`\n",
    "   - Sets `CMD` to `[\"serve\"]` (python script that launches nginx, gunicorn in the background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38730055-fd66-40ce-a520-dd44bcb16ab7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba8ab1-4e0e-41f5-ad4e-e647b2a13efc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build image locally\n",
    "!docker build -t abalone/predictor ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69cd93d6-e00a-44ce-a742-79e1bc5b4076",
   "metadata": {},
   "source": [
    "### Launch and test custom Inference container locally\n",
    "\n",
    "Launch a new terminal and run the below docker command\n",
    "\n",
    "```docker\n",
    "docker run --rm -v $(pwd)/models:/opt/ml/model -p 8080:8080 abalone/predictor\n",
    "```\n",
    "\n",
    "- This command mounts the [models](./models) directory to `/opt/ml/model` directory inside the container and maps container port `8080` to host port `8080`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef0220-ae37-4d72-b230-a292498c73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a terminal and cd into the predictor directory (the location where predictor.ipynb is located) run the following command\n",
    "# run this command to launch container locally\n",
    "# docker run --rm -v $(pwd)/models:/opt/ml/model -p 8080:8080 abalone/predictor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "113c4680-48ab-4ce2-aaaf-168da80bc0b0",
   "metadata": {},
   "source": [
    "#### Check container health by invoking `/ping`\n",
    "\n",
    "If the `/ping` was successful you should see a response similar to `\"GET /ping HTTP/1.1\" 200 1` in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd99bed-c9a6-4194-b41f-54501aba6816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ping local inference endpoint\n",
    "!curl http://localhost:8080/ping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9893e277-20ea-40e5-86a9-e5b5809ac892",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Verify container logs locally (using docker logs)\n",
    "\n",
    "- To inspect a running container to view container config values or IP address we use `docker inspect <CONTAINER_ID_OR_NAME>`\n",
    "- To view and tail logs generated in the container we use `docker logs --follow <NUM_OF_LINES> <CONTAINER_ID_OR_NAME>`\n",
    "- SageMaker publishes container logs to CloudWatch. CloudWatch logs for a given endpoint are published to the following log stream path\n",
    "`/aws/sagemaker/Endpoints/ENDPOINT_NAME/VARIANT_NAME/CONTAINER_NAME`\n",
    "\n",
    "**NOTE:** \n",
    "1. Run this command in a terminal as running this inside a cell would hang execution.\n",
    "1. the below command assumes there is only one running container. If you have more, then use command with container name `docker inspect <CONTAINER_ID_OR_NAME>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292186ed-22a4-4f4f-a23c-acfb138e2ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RUN THE BELOW IN A SEPARATE NEW TERMINAL\n",
    "# docker ps --format \"{{.Names}}\" | xargs -n1 -I{} docker logs --follow --tail 50 {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c79be964-d4dd-4cff-ac62-ef57c6dd0627",
   "metadata": {},
   "source": [
    "### Troubleshooting container locally (using logs)\n",
    "\n",
    "`!docker logs abalone/featurizer`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "902de0e4-03da-4592-82c0-461d1ee17b51",
   "metadata": {},
   "source": [
    "#### Test records for inference\n",
    "\n",
    "Grab a test record from [abalone_test_predictor.csv](../featurizer/data/abalone_test_predictor.csv), generated by [`featurizer.ipynb`](../featurizer/featurizer.ipynb), format it as a CSV record, and send it as raw data to the endpoint `http://localhost:8080/invocations` path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365d9de-51fe-46dc-9e5e-a82b05685bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send test records to /invocations on the endpoint\n",
    "!curl --data-raw '-1.3317586042173168,-1.1425409076053987,-1.0579488602777858,-1.177706547272754,-1.130662184748842,-1.1493955859050584,-1.139968767909096,0.0,1.0,0.0' \\\n",
    "-H 'Content-Type: text/csv; charset=utf-8' \\\n",
    "-v http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86286333-a056-4619-be76-2b65d492cd08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send test records to /invocations on the endpoint\n",
    "!curl --data-raw '0.7995425613971686,0.877965470587042,1.326659055767273,1.398563012556441,0.9896192483949702,1.509166873607132,2.01650402614155,0.0,0.0,1.0' \\\n",
    "-H 'Content-Type: text/csv; charset=utf-8' \\\n",
    "-v http://localhost:8080/invocations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bad32979-3897-42e7-94f4-8537e12c720a",
   "metadata": {},
   "source": [
    "### Tag and push the local image to private ECR\n",
    "\n",
    "- Tag the `abalone/predictor` local image to `{account_id}.dkr.ecr.{region}.amazonaws.com/{imagename}:{tag}` format\n",
    "- Run [./build_n_push.sh](./build_n_push.sh) shell script with image name `nginx` as parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863efc3-4dfd-4ecc-8206-39f7b51ea690",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x ./build_n_push.sh\n",
    "!./build_n_push.sh abalone/predictor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a336a71-b05e-484c-8609-2d26cd1f7a1a",
   "metadata": {},
   "source": [
    "### Optional: Test predictor inference image by deploying to a real-time endpoint\n",
    "\n",
    "- **Step 1:** SageMaker session initialize\n",
    "- **Step 2:** Compress your model in `./models/xgboost-model` to `model.tar.gz` format and upload to s3\n",
    "- **Step 3:** Create Model object with your custom inference image \n",
    "- **Step 4:** Deploy model\n",
    "- **Step 5:** Send test inference request to deployed endpoint\n",
    "- **Step 6:** Cleanup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1957a055-ae4d-4003-8bd3-4a8dbb4267d2",
   "metadata": {},
   "source": [
    "#### **Step 1:** Initialize Session and upload model artifacts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bff45f-56e3-4bc1-90e8-0d9e31def130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "import tarfile\n",
    "from sagemaker import get_execution_role, session\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader, s3_path_join\n",
    "\n",
    "sm_session = session.Session()\n",
    "region = sm_session._region_name\n",
    "role = get_execution_role()\n",
    "bucket = sm_session.default_bucket()\n",
    "\n",
    "prefix = \"sagemaker/abalone/models/byoc\"\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "model_s3uri = s3_path_join(f\"s3://{bucket}/{prefix}\", \"predictor\")\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "\n",
    "print(f\"Listing files under: {model_s3uri}\")\n",
    "S3Downloader.list(model_s3uri)\n",
    "\n",
    "model_path = os.path.join(\"./models\", \"xgboost-model\")\n",
    "model_output_path = os.path.join(\"./models\", \"model.tar.gz\")\n",
    "\n",
    "# SageMaker expects model artifacts to be compressed to `model.tar.gz`\n",
    "if not os.path.exists(model_output_path):\n",
    "    print(f\"Compressing model to {model_output_path}\")\n",
    "    tar = tarfile.open(model_output_path, \"w:gz\")\n",
    "    tar.add(model_path, arcname=\"xgboost-model\")\n",
    "    tar.close()\n",
    "else:\n",
    "    print(f\"Model file exists: {model_output_path}\")\n",
    "\n",
    "# Upload compressed model artifact to S3 using S3Uploader utility class\n",
    "model_data_url = S3Uploader.upload(\n",
    "    local_path=model_output_path,\n",
    "    desired_s3_uri=model_s3uri,\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "print(f\"Uploaded predictor model.tar.gz to {model_data_url}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04630460-d038-4690-bd6d-37a2b3838b11",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Step 2:** Create model object with custom inference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c37340-474b-4c37-b7af-d3063235c57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "from sagemaker.model import Model\n",
    "\n",
    "image_name = \"abalone/predictor\"\n",
    "ecr_image = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{image_name}:latest\"\n",
    "print(f\"model_image_uri: {ecr_image}\")\n",
    "\n",
    "suffix = f\"{str(uuid4())[:5]}-{datetime.now().strftime('%d%b%Y')}\"\n",
    "model_name = f\"AbaloneXGB-predictor-{suffix}\"\n",
    "\n",
    "print(f\"Creating model : {model_name} with custom image:\\n{ecr_image}\")\n",
    "predictor_model = Model(\n",
    "    image_uri=ecr_image,\n",
    "    name=model_name,\n",
    "    model_data=model_data_url,\n",
    "    role=role,\n",
    "    sagemaker_session=sm_session,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b29ddc7-7562-4a55-b96d-5fab1f3b2bfa",
   "metadata": {},
   "source": [
    "#### **Step 3:** Deploy model to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51248e7f-8c38-4b6b-a3db-7d6b61c5a04a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = f\"Abalone-nginx-ep-{suffix}\"\n",
    "\n",
    "print(f\"Deploying model to endpoint: {endpoint_name}\")\n",
    "predictor = predictor_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58952b53-ca5d-45c4-b3e7-3c73f47eb071",
   "metadata": {},
   "source": [
    "### Wait for endpoint to be `InService`\n",
    "\n",
    "Get a waiter on the endpoint and wait for endpoint to be `InService`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358fb5a-8c43-48e1-88f7-e1f4db2047bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get endpoint status using describe endpoint\n",
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint {endpoint_name} - Status: {status}\")\n",
    "\n",
    "# Get waiter object\n",
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "# Apply waiter on the endpoint\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "# Get endpoint status using describe endpoint\n",
    "status = sm_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "print(f\"Endpoint {endpoint_name} - Status: {status}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9786cc6d-3298-42c6-ac83-423474011eed",
   "metadata": {},
   "source": [
    "### **Step 4:** Send test inference requests to deployed endpoint\n",
    "\n",
    "- Open [abalone_test_predictions.csv](../data/abalone_test_predictions.csv), read each line a do the following:\n",
    "  - Ignore the header row\n",
    "  - join the values to a csv record to form payload\n",
    "  - send payload to endpoint by calling `invoke_endpoint` using SageMaker run-time client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717ff77-2700-4af7-b143-0de0e67bff2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "LOCALDIR = \"../data\"\n",
    "local_test_dataset = f\"{LOCALDIR}/abalone_test_predictions.csv\"\n",
    "\n",
    "limit = 100\n",
    "i = 0\n",
    "\n",
    "with open(local_test_dataset, \"r\") as _f:\n",
    "    lines = _f.readlines()\n",
    "    for row in lines:\n",
    "        # Skip headers\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "        elif i <= limit:\n",
    "            row = row.rstrip(\"\\n\")\n",
    "            splits = row.split(\",\")\n",
    "            input_cols = \",\".join(s for s in splits)\n",
    "            prediction = None\n",
    "            try:\n",
    "                prediction = runtime_sm_client.invoke_endpoint(\n",
    "                    EndpointName=endpoint_name,\n",
    "                    ContentType=\"text/csv; charset=utf-8\",\n",
    "                    Body=input_cols,\n",
    "                )\n",
    "                response = prediction[\"Body\"].read().decode(\"utf-8\")\n",
    "                print(response)\n",
    "                i += 1\n",
    "                sleep(0.15)\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {e}\")\n",
    "                pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5ed1695-39b8-497d-a260-027ac6de23d6",
   "metadata": {},
   "source": [
    "### View logs emitted by the endpoint in CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dd365-391a-4797-b4b4-5d74e75dfd85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "logs_client = boto3.client(\"logs\")\n",
    "end_time = datetime.utcnow()\n",
    "start_time = end_time - timedelta(minutes=15)\n",
    "\n",
    "log_group_name = f\"/aws/sagemaker/Endpoints/{endpoint_name}\"\n",
    "log_streams = logs_client.describe_log_streams(logGroupName=log_group_name)\n",
    "log_stream_name = log_streams[\"logStreams\"][0][\"logStreamName\"]\n",
    "\n",
    "# Retrieve the logs\n",
    "logs = logs_client.get_log_events(\n",
    "    logGroupName=log_group_name,\n",
    "    logStreamName=log_stream_name,\n",
    "    startTime=int(start_time.timestamp() * 1000),\n",
    "    endTime=int(end_time.timestamp() * 1000),\n",
    ")\n",
    "\n",
    "# Print the logs\n",
    "for event in logs[\"events\"]:\n",
    "    print(f\"{datetime.fromtimestamp(event['timestamp'] // 1000)}: {event['message']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e05d4caf-e665-477d-965d-1ba8cb91bd0a",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Cleanup resources. Delete endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ffc175-ee55-4dc7-83ec-b306a75978e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete endpoint\n",
    "try:\n",
    "    print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting EP: {endpoint_name}\\n{e}\")\n",
    "    pass\n",
    "# Delete model\n",
    "try:\n",
    "    print(f\"Deleting model: {model_name}\")\n",
    "    sm_client.delete_model(ModelName=model_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting Model: {model_name}\\n{e}\")\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_aiml",
   "language": "python",
   "name": "conda_aiml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
