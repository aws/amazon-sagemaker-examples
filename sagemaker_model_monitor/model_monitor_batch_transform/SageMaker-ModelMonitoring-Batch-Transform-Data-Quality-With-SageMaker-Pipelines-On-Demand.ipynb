{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c41240-c6cd-4dce-882d-770a8f0eea33",
   "metadata": {},
   "source": [
    "# SageMaker Data Quality Model Monitor for Batch Transform with SageMaker Pipelines On-demand\n",
    "\n",
    "In this notebook, we use SageMaker Pipelines and SageMaker Model Monitor to monitor the data quality of a batch transform job.\n",
    "\n",
    "Data quality monitoring automatically monitors machine learning (ML) models in production and notifies you when data quality issues arise. ML models in production have to make predictions on real-life data that is not carefully curated like most training datasets. If the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was trained on, the model begins to lose accuracy in its predictions.\n",
    "\n",
    "We introduce a new step type called `MonitorBatchTransformStep` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6967670f-22f7-40c1-aed6-708ce81bf0f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "! pip install --upgrade pip\n",
    "!{sys.executable} -m pip install sagemaker==2.114.0\n",
    "!{sys.executable} -m pip install -U boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcf7a0-fbef-4d09-bc1c-fcc8e4f780a5",
   "metadata": {},
   "source": [
    "If you run this notebook in SageMaker Studio, you need to make sure latest python SDK is installed and restart the kernel, so please uncomment the code in the next cell, and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acef961-5c67-43c5-84cc-504a27a2207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)  # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaadc74-5506-4185-bd91-1dcb2b5987a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from sagemaker import get_execution_role, session\n",
    "import pandas as pd\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411aa33-dabf-436a-af20-c49e8e5c75ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = session.Session(boto3.Session()).default_bucket()\n",
    "\n",
    "print(\"Demo Bucket: {}\".format(bucket))\n",
    "prefix = f\"sagemaker/demo-model-monitor-batch-transform/data-quality/{int(time.time())}\"\n",
    "\n",
    "reports_prefix = \"{}/reports\".format(prefix)\n",
    "s3_report_path = \"s3://{}/{}\".format(bucket, reports_prefix)\n",
    "\n",
    "transform_output_path = \"s3://{}/{}/transform-outputs\".format(bucket, prefix)\n",
    "\n",
    "print(\"Transform Output path: {}\".format(transform_output_path))\n",
    "print(\"Report path: {}\".format(s3_report_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff366b-54a5-4ede-809f-7f7fbbcc6272",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Construct a SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffae6da-67cb-4a1c-a753-189ca9953bef",
   "metadata": {},
   "source": [
    "Amazon SageMaker Model Building Pipelines is a tool for building machine learning pipelines that take advantage of direct SageMaker integration. We can leverage it to run batch transform job with monitoring on-demand.\n",
    "\n",
    "In this notebook, we showcase how to use SageMaker Pipeline to orchestrate the on-demand batch inference monitoring. In summary, we create and execute a pipeline to:\n",
    "\n",
    "- Create a model\n",
    "- Run a batch inference with the model\n",
    "- Run a model monitoring job to evaluate the inference inputs/outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a46fa-bde2-40f5-ae5b-a6c6faf1d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.model_step import ModelStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b206fcb-0245-40d8-aefe-fa8b0c0577f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_session = PipelineSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bc26dc-f7a6-4589-ad7a-dd7afa2270bb",
   "metadata": {},
   "source": [
    "### Create a model\n",
    "\n",
    "Here we take a pretrained model and upload it to S3. We use this model in our batch transform step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76ffc3-1c82-4db9-8629-367cb33d0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"xgb-churn-prediction-model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078b203-9fee-4929-94da-1bc9ff136675",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp model/{model_file_name} s3://{bucket}/{prefix}/{model_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fafd7-45d9-4623-aec4-d0ebcfd96998",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DEMO-xgb-churn-pred-model-monitor-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = \"https://{}.s3-{}.amazonaws.com/{}/{}\".format(bucket, region, prefix, model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d110cfd-f016-4c30-a187-9cfc65970bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = retrieve(\"xgboost\", boto3.Session().region_name, \"0.90-1\")\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_url,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "create_model_step = ModelStep(\n",
    "    name=\"CreateXGBoostModelStep\",\n",
    "    step_args=model.create(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03588123-091b-42b0-97c3-10f8d6e8b632",
   "metadata": {},
   "source": [
    "### Configure a transformer\n",
    "\n",
    "We must first upload the dataset used to generate predictions to S3. We then define a transformer object to be used in the `MonitorBatchTransformStep`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20afd87-d93e-4572-a625-7141729d03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset used to get predictions\n",
    "\n",
    "!aws s3 cp test_data/validation.csv s3://{bucket}/{prefix}/transform_input/validation/validation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4addb-9b77-4993-91ec-826851233ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.parameters import ParameterString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383df859-3025-4586-b4e8-062712eb29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    model_name=create_model_step.properties.ModelName,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    accept=\"text/csv\",\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=transform_output_path,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1588b0-a7f5-43e2-bba3-a32140f1f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_input_param = ParameterString(\n",
    "    name=\"transform_input\",\n",
    "    default_value=f\"s3://{bucket}/{prefix}/transform_input/validation\",\n",
    ")\n",
    "\n",
    "transform_arg = transformer.transform(\n",
    "    transform_input_param,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\",\n",
    "    # exclude the ground truth (first column) from the validation set\n",
    "    # when doing inference.\n",
    "    input_filter=\"$[1:]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43b32c-26dd-4007-87f8-02c739dfacf9",
   "metadata": {},
   "source": [
    "### Configure data quality monitoring\n",
    "\n",
    "In this section, we first run a baseline job, and use the suggested constraints and statistics as the baseline for running the data quality monitoring job during pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a5bf6c-c7c2-4f2e-9de2-fe10dd59beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "from sagemaker.workflow.quality_check_step import DataQualityCheckConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fdf781-0a11-4e7c-9038-3ab23c421520",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = \"s3://{}/{}\".format(bucket, baseline_data_prefix)\n",
    "baseline_results_uri = \"s3://{}/{}\".format(bucket, baseline_results_prefix)\n",
    "print(\"Baseline data uri: {}\".format(baseline_data_uri))\n",
    "print(\"Baseline results uri: {}\".format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded86498-5b8b-4521-bea8-9588dfa051cc",
   "metadata": {},
   "source": [
    "### Generate a baseline for Model Monitor\n",
    "\n",
    "We use the training dataset called `training-dataset-with-header.csv` to generate a baseline that will be used by the Data Quality Monitor. To do this, we use the `suggest_baseline` method. The purpose of this is to generate a set of `statistics` and `constraints` file. These files will be used by Model Monitor to compare the data passed to the Transform job and report any violations that are detected.\n",
    "\n",
    "The `suggest_baseline` method has an argument called `baseline_dataset`. This is typically the dataset used during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4158a-0ec4-49dc-9aa2-89164e2029fe",
   "metadata": {},
   "source": [
    "We upload the dataset used for baselining and the data used for inference to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc63bfa-5435-48df-a2ca-a91b037f36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = \"test_data/training-dataset-with-header.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3800a0-a072-4a8e-8b44-6b29b5f38d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset used to generate statistics and constraints file\n",
    "\n",
    "!aws s3 cp {training_data_file} {baseline_data_uri}/training-dataset-with-header.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fd94a-ee4b-4c70-80f4-46747aa89a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri + \"/training-dataset-with-header.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c65099-df97-4eb5-a227-4f6b811da40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa753e9-ff61-40af-93bd-bfe636b521ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_path = \"{}/statistics.json\".format(baseline_results_uri)\n",
    "constraints_path = \"{}/constraints.json\".format(baseline_results_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf07482-518b-4322-9087-97040791b2f9",
   "metadata": {},
   "source": [
    "### Configure the Data Quality Check\n",
    "\n",
    "There are two configurations we create here, one is `CheckJobConfig` and the other is `DataQualityCheckConfig`. The `CheckJobConfig` is used to configure the underlying processing job used by Model Monitor. This is where users can specify the role, instance type, etc. \n",
    "\n",
    "The `DataQualityCheckConfig` is used to configure how Model Monitor runs the data quality check. It accepts an argument called `baseline_dataset`. This is the dataset that is passed to the transform job. The dataset passed here is compared against the baseline and statistics file generated by the `suggest_baseline` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bad4f-a072-4118-9ac3-17094f888cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = CheckJobConfig(role=role)\n",
    "data_quality_config = DataQualityCheckConfig(\n",
    "    baseline_dataset=transform_input_param,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=s3_report_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c052dc-64ad-463a-b2fd-89299f15efd9",
   "metadata": {},
   "source": [
    "### Use the `MonitorBatchTransformStep` to monitor the transform job\n",
    "\n",
    "This step runs a batch transform job using the transformer object configured above and monitors the data passed to the transformer before executing the job.\n",
    "\n",
    "The baselines calculated above must be passed to this step so that the incoming data can be compared against them to detect violations.\n",
    "\n",
    "You can configure the step to fail if a violation to Data Quality is found by toggling the `fail_on_violation` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1b39c4-443b-4f7f-a1d0-96cf4daff097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.monitor_batch_transform_step import MonitorBatchTransformStep\n",
    "\n",
    "transform_and_monitor_step = MonitorBatchTransformStep(\n",
    "    name=\"MonitorCustomerChurnDataQuality\",\n",
    "    transform_step_args=transform_arg,\n",
    "    monitor_configuration=data_quality_config,\n",
    "    check_job_configuration=job_config,\n",
    "    # since this is for data quality monitoring,\n",
    "    # you could choose to run the monitoring job before the batch inference.\n",
    "    monitor_before_transform=True,\n",
    "    # if violation is detected in the monitoring, you can skip it and continue running batch transform\n",
    "    fail_on_violation=False,\n",
    "    supplied_baseline_statistics=statistics_path,\n",
    "    supplied_baseline_constraints=constraints_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e4c78-9864-4a13-88d2-ba5023a60091",
   "metadata": {},
   "source": [
    "### Create and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e11f94-4b8c-4269-8295-e56267db1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"MonitorDataQualityBatchTransformPipeline\",\n",
    "    parameters=[transform_input_param],\n",
    "    steps=[create_model_step, transform_and_monitor_step],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca86add-96c1-4aad-bfe0-ddd8baa6f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e997e3d6-6789-441b-9637-e9f3c9223199",
   "metadata": {},
   "source": [
    "### Start a pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8632c-4a19-4ba8-ac76-604621500a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf5872-9bf8-49c3-9c9e-8a7f8f8d98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b253f9-3877-44a8-9e69-336aa6e5b925",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read the model monitor reports\n",
    "\n",
    "You must wait for the pipeline to finish executing before you can read the violation reports.\n",
    "\n",
    "This pipeline succeeds even though violations are found by model monitor because `fail_on_violation` is set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d204e74-d139-4910-98c9-94cf40139c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import MonitoringExecution\n",
    "\n",
    "monitoring_step = [step for step in execution.list_steps() if \"QualityCheck\" in step[\"Metadata\"]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2d72f-54aa-4b05-9ed1-f40c15b4dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring = MonitoringExecution.from_processing_arn(\n",
    "    sagemaker_session=pipeline_session,\n",
    "    processing_job_arn=monitoring_step[\"Metadata\"][\"QualityCheck\"][\"CheckJobArn\"],\n",
    ")\n",
    "violation = monitoring.constraint_violations(file_name=\"constraint_violations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ff42f-b4e9-4e53-ab16-b14cc11e2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "\n",
    "constraints_df = pd.io.json.json_normalize(violation.body_dict[\"violations\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472492c0-670e-4ab8-b413-a1f08b8dd12a",
   "metadata": {},
   "source": [
    "### Other commands\n",
    "We can also start and stop the monitoring schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c6c6a-2590-4ee5-b162-bf658a8720ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_default_monitor.stop_monitoring_schedule()\n",
    "# my_default_monitor.start_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21380232-4d1c-4aa4-8b36-6db336b11992",
   "metadata": {},
   "source": [
    "### Delete the resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52768bb8-bcb5-450d-92ce-8b6ec8fd6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_default_monitor.stop_monitoring_schedule()\n",
    "# my_default_monitor.delete_monitoring_schedule()\n",
    "# time.sleep(60)  # actually wait for the deletion"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
