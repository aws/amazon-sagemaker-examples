{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeef4885-a6e6-41af-b6e0-9efd5894e86c",
   "metadata": {},
   "source": [
    "# Build and deploy a serial inference application to SageMaker real-time endpoints\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "- We build a fully custom ML serial inference application that encapsulates the following:\n",
    "  1. A [\"featurizer\"](./featurizer/) model (data pre-processing container) built using `SKLearn` column transformer\n",
    "     - The model transforms raw csv input data to features and returns the transformed data as output\n",
    "  1. A [predictor](./predictor/) `XGBoost` model trained on UCI Abalone dataset that accepts transformed features (generated by \"featurizer\" model) and returns predictions in JSON format.\n",
    "\n",
    "![ Abalone Predictor Pipeline ](./images/serial-inference-pipeline.png)\n",
    "\n",
    "## Building Custom inference containers\n",
    "\n",
    "1. To build, test and host \"featurizer\" container locally Refer to [`featurizer.ipynb`](./featurizer/featurizer.ipynb) Notebook\n",
    "1.  To build, test and host \"predictor\" container locally - Refer to [`predictor.ipynb`](./predictor/predictor.ipynb) Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd608469-ee3e-4086-9793-019e71057226",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisite\n",
    "\n",
    "**NOTE:** Ensure both [featurizer.ipynb](./featurizer/featurizer.ipynb) and [predictor.ipynb](./predictor/predictor.ipynb) are completed before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c5e55-8ee4-4d9c-a830-5891ef29c0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U awscli boto3 sagemaker watermark scikit-learn tqdm --quiet\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -p awscli,boto3,sagemaker,scikit-learn,tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cacad-d445-4312-98b1-5b7ed992de65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from sagemaker import session, get_execution_role\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader, s3_path_join\n",
    "\n",
    "# account id for constructing ECR repo uri\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "\n",
    "sm_session = session.Session()\n",
    "region = sm_session.boto_region_name\n",
    "role = get_execution_role()\n",
    "bucket = sm_session.default_bucket()\n",
    "\n",
    "prefix = \"sagemaker/abalone/models/byoc\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "abalone_s3uri = (\n",
    "    f\"s3://sagemaker-example-files-prod-{region}/datasets/tabular/uci_abalone/abalone.csv\"\n",
    ")\n",
    "\n",
    "pretrained_xgboost_model_s3uri = (\n",
    "    f\"s3://sagemaker-example-files-prod-{region}/models/xgb-abalone/xgboost-model\"\n",
    ")\n",
    "\n",
    "\n",
    "base_dir = Path(\"./data\")\n",
    "featurizer_dir = Path(\"./featurizer\").absolute()\n",
    "predictor_dir = Path(\"./predictor\").absolute()\n",
    "\n",
    "S3Downloader.download(s3_uri=abalone_s3uri, local_path=base_dir, sagemaker_session=sm_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8768bc6-4bb7-4403-b493-6178986e4b32",
   "metadata": {},
   "source": [
    "### Build \"featurizer\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564f79d-20b1-4e56-8c3b-f2258cc50b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(featurizer_dir)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b4b81-c22d-4947-a934-3292604be85c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "featurizer_model_dir = featurizer_dir.joinpath(\"models\")\n",
    "\n",
    "DATA_DIR = Path(\"../data\").resolve()\n",
    "DATA_FILE = DATA_DIR.joinpath(\"abalone.csv\")\n",
    "\n",
    "if not DATA_FILE.exists():\n",
    "    raise ValueError(f\"{DATA_FILE} doesn't exist\")\n",
    "\n",
    "if not featurizer_model_dir.exists():\n",
    "    featurizer_model_dir.mkdir(parents=True)\n",
    "\n",
    "# As we get a headerless CSV file, we specify the column names here.\n",
    "feature_columns_names = [\n",
    "    \"sex\",\n",
    "    \"length\",\n",
    "    \"diameter\",\n",
    "    \"height\",\n",
    "    \"whole_weight\",\n",
    "    \"shucked_weight\",\n",
    "    \"viscera_weight\",\n",
    "    \"shell_weight\",\n",
    "]\n",
    "label_column = \"rings\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"sex\": str,\n",
    "    \"length\": np.float64,\n",
    "    \"diameter\": np.float64,\n",
    "    \"height\": np.float64,\n",
    "    \"whole_weight\": np.float64,\n",
    "    \"shucked_weight\": np.float64,\n",
    "    \"viscera_weight\": np.float64,\n",
    "    \"shell_weight\": np.float64,\n",
    "}\n",
    "label_column_dtype = {\"rings\": np.float64}\n",
    "\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    DATA_FILE,\n",
    "    header=None,\n",
    "    names=feature_columns_names + [label_column],\n",
    "    dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype),\n",
    ")\n",
    "\n",
    "print(\"Splitting raw dataset to train and test datasets..\")\n",
    "\n",
    "(df_train_val, df_test) = train_test_split(df, random_state=42, test_size=0.1)\n",
    "\n",
    "\n",
    "df_test.to_csv(f\"{DATA_DIR.joinpath('abalone_test.csv')}\", index=False)\n",
    "\n",
    "print(f\"Test dataset written to {str(DATA_DIR.resolve())}/abalone_test.csv\")\n",
    "\n",
    "\n",
    "numeric_features = list(feature_columns_names)\n",
    "numeric_features.remove(\"sex\")\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_features = [\"sex\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Call fit on ColumnTransformer to fit all transformers to X, y\n",
    "preprocessor = preprocess.fit(df_train_val)\n",
    "\n",
    "# Save the processor model to featurizer/models directory\n",
    "joblib.dump(preprocess, featurizer_model_dir.joinpath(\"preprocess.joblib\"))\n",
    "print(f\"Saved preprocessor model to {featurizer_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd088d-d633-4a69-907d-4c4a09dc7179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "os.chdir(featurizer_model_dir.absolute())\n",
    "\n",
    "featurizer_model_path = featurizer_model_dir.absolute().joinpath(\"model.tar.gz\")\n",
    "\n",
    "if featurizer_model_path.exists():\n",
    "    featurizer_model_path.unlink()\n",
    "\n",
    "tar_cmd = \"tar -czvf model.tar.gz preprocess.joblib ../code/\"\n",
    "result = subprocess.run(tar_cmd, shell=True, capture_output=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"{featurizer_model_path} archive created successfully!\")\n",
    "    os.chdir(featurizer_dir)\n",
    "else:\n",
    "    os.chdir(featurizer_dir)\n",
    "    print(\"An error occurred:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec80d6-674d-4a03-8d17-d00b911aefb4",
   "metadata": {},
   "source": [
    "### Build and push \"featurizer\" docker image to private ECR repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed485387-2e95-42ef-b711-b1658796a3ed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "featurizer_image_name = \"abalone/featurizer\"\n",
    "\n",
    "# build featurizer image\n",
    "!docker build -t $featurizer_image_name .\n",
    "\n",
    "# change file permissions\n",
    "!chmod +x build_n_push.sh\n",
    "\n",
    "# push image to ecr repo\n",
    "!./build_n_push.sh $featurizer_image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e538b-26ab-4918-9087-b6d3dfd327ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Full name of the ECR repository\n",
    "featurizer_ecr_repo_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{featurizer_image_name}\"\n",
    "\n",
    "print(featurizer_ecr_repo_uri)\n",
    "\n",
    "# Upload featurizer model to s3\n",
    "featurizer_s3_uri = s3_path_join(f\"s3://{bucket}/{prefix}\", \"featurizer\")\n",
    "\n",
    "if featurizer_model_path.exists():\n",
    "    featurizer_model_data = S3Uploader.upload(\n",
    "        local_path=str(featurizer_model_path),\n",
    "        desired_s3_uri=featurizer_s3_uri,\n",
    "        sagemaker_session=sm_session,\n",
    "    )\n",
    "else:\n",
    "    print(f\"{featurizer_model_path} not found!\")\n",
    "\n",
    "print(f\"featurizer model uploaded to to {featurizer_model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183dfc2-2812-4f8f-a298-f2bde597e0c4",
   "metadata": {},
   "source": [
    "### Build predictor model\n",
    "\n",
    "We downlaod and use the pre-trained `xgboost` model from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55562119-bf71-4a04-837a-1a4d65ee4bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step out of featurizer directory\n",
    "os.chdir(current_dir)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f696b-9c90-4e37-88bb-c7e3eda31472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_model_dir = predictor_dir.joinpath(\"models\").absolute()\n",
    "if not predictor_model_dir.exists():\n",
    "    predictor_model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba350d57-a807-44b2-bd64-65c2d9490c58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(predictor_dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d172947-8006-4c4f-98bf-35f71f12e4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp $pretrained_xgboost_model_s3uri $predictor_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d44ee7-e56f-432a-b702-b3490946be05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(predictor_model_dir)\n",
    "predictor_model_path = predictor_model_dir.joinpath(\"model.tar.gz\")\n",
    "\n",
    "if predictor_model_path.exists():\n",
    "    predictor_model_path.unlink()\n",
    "\n",
    "tar_cmd = \"tar -czvf model.tar.gz xgboost-model ../code/\"\n",
    "result = subprocess.run(tar_cmd, shell=True, capture_output=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"Tar archive created successfully!\")\n",
    "    print(predictor_model_path)\n",
    "    os.chdir(predictor_dir)\n",
    "else:\n",
    "    os.chdir(predictor_model_dir)\n",
    "    print(\"An error occurred:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae225b1b-81e6-4ac8-b5a8-9161f5ec40a4",
   "metadata": {},
   "source": [
    "### Build and push \"predictor\" docker image to private ECR repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefad799-5bb5-4475-ba78-47ccbc75b7f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_image_name = \"abalone/predictor\"\n",
    "\n",
    "!docker build -t $predictor_image_name .\n",
    "\n",
    "!chmod +x build_n_push.sh\n",
    "\n",
    "!./build_n_push.sh $predictor_image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1ecfc-aa0d-4558-8cbf-33a880475e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Full name of the ECR repository\n",
    "predictor_ecr_repo_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{predictor_image_name}\"\n",
    "\n",
    "print(predictor_ecr_repo_uri)\n",
    "\n",
    "# Upload featurizer model to s3\n",
    "predictor_s3_uri = s3_path_join(f\"s3://{bucket}/{prefix}\", \"predictor\")\n",
    "\n",
    "if predictor_model_path.exists():\n",
    "    print(f\"Uploading predictor model to {predictor_s3_uri}\")\n",
    "    predictor_model_data = S3Uploader.upload(\n",
    "        local_path=str(predictor_model_path),\n",
    "        desired_s3_uri=predictor_s3_uri,\n",
    "        sagemaker_session=sm_session,\n",
    "    )\n",
    "else:\n",
    "    print(f\"{predictor_model_path} not found!\")\n",
    "\n",
    "os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afca977-3f3d-4676-b339-ee19e1cd1025",
   "metadata": {},
   "source": [
    "### Create Models and Pipeline Model\n",
    "\n",
    "Now, we create two model objects to be combined later to a Pipeline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616caf67-66f6-4333-aef5-b9e7f0740bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "from sagemaker.model import Model\n",
    "\n",
    "suffix = f\"{str(uuid4())[:5]}-{datetime.now().strftime('%d%b%Y')}\"\n",
    "\n",
    "# Featurizer Model (SKLearn Model)\n",
    "image_name = \"abalone/featurizer\"\n",
    "sklearn_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{image_name}:latest\"\n",
    "\n",
    "featurizer_model_name = f\"AbaloneXGB-featurizer-{suffix}\"\n",
    "print(f\"Creating Featurizer model: {featurizer_model_name}\")\n",
    "sklearn_model = Model(\n",
    "    image_uri=featurizer_ecr_repo_uri,\n",
    "    name=featurizer_model_name,\n",
    "    model_data=featurizer_model_data,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# Predictor Model (XGBoost Model)\n",
    "predictor_model_name = f\"AbaloneXGB-Predictor-{suffix}\"\n",
    "print(f\"Creating Predictor model: {predictor_model_name}\")\n",
    "xgboost_model = Model(\n",
    "    image_uri=predictor_ecr_repo_uri,\n",
    "    name=predictor_model_name,\n",
    "    model_data=predictor_model_data,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a43ba0-b6c7-448e-9723-407363b7f54e",
   "metadata": {},
   "source": [
    "### Create Pipeline Model\n",
    "\n",
    "1. Create a Pipeline model with `sklearn_model` and `xgboost_model` to act a serial inference pipeline.\n",
    "1. Deploy Pipeline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d113217c-e048-4f16-b645-504c82f310ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pipeline import PipelineModel\n",
    "\n",
    "pipeline_model_name = f\"Abalone-pipeline-{suffix}\"\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    name=pipeline_model_name,\n",
    "    role=role,\n",
    "    models=[sklearn_model, xgboost_model],\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "\n",
    "print(f\"Deploying pipeline model {pipeline_model_name}...\")\n",
    "predictor = pipeline_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e90fac-cae7-4a9c-a4ed-90a7a9fe4f09",
   "metadata": {},
   "source": [
    "### Test inference on Endpoint with Pipeline Model\n",
    "\n",
    "- Instantiate a `Predictor` class from `sagemaker.predictor` module\n",
    "- Use `CSVSerialzier` to serialize payload\n",
    "- and `JSONDeSerializer` for deserializing output (JSON) from the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d6992-4f0d-461b-bd66-0c12d99b6ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Use the endpoint_name you specified when deploying the pipeline\n",
    "endpoint_name = pipeline_model_name\n",
    "\n",
    "# Let's use the test dataset in featurizer/data directory\n",
    "local_test_dataset = DATA_DIR.joinpath(\"abalone_test.csv\").resolve()\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sm_session,\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "\n",
    "limit = 15\n",
    "i = 0\n",
    "\n",
    "with open(local_test_dataset, \"r\") as _f:\n",
    "    for row in _f:\n",
    "        # Skip headers row\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "        elif i <= limit:\n",
    "            row = row.rstrip(\"\\n\")\n",
    "            splits = row.split(\",\")\n",
    "            # Remove the target column (last column)\n",
    "            label = splits.pop(-1)\n",
    "            input_cols = \",\".join(s for s in splits)\n",
    "            prediction = None\n",
    "            try:\n",
    "                response = predictor.predict(input_cols)\n",
    "                print(f\"True value: {label} | Predicted: {response['result'][0]}\")\n",
    "                i += 1\n",
    "                sleep(0.15)\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {e}\")\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f4b74-6cb0-4020-8cdf-6c36b1d665b7",
   "metadata": {},
   "source": [
    "### (Optional) Verify Logs emitted by the endpoint in CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc7226-f527-4e94-984e-37836d5cc8a9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "logs_client = boto3.client(\"logs\")\n",
    "end_time = datetime.utcnow()\n",
    "start_time = end_time - timedelta(minutes=15)\n",
    "\n",
    "log_group_name = f\"/aws/sagemaker/Endpoints/{endpoint_name}\"\n",
    "log_streams = logs_client.describe_log_streams(logGroupName=log_group_name)\n",
    "log_stream_name = log_streams[\"logStreams\"][0][\"logStreamName\"]\n",
    "\n",
    "# Retrieve the logs\n",
    "logs = logs_client.get_log_events(\n",
    "    logGroupName=log_group_name,\n",
    "    logStreamName=log_stream_name,\n",
    "    startTime=int(start_time.timestamp() * 1000),\n",
    "    endTime=int(end_time.timestamp() * 1000),\n",
    ")\n",
    "\n",
    "# Print the logs\n",
    "for event in logs[\"events\"]:\n",
    "    print(f\"{datetime.fromtimestamp(event['timestamp'] // 1000)}: {event['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3865d9-7743-4841-aa21-68fe011db86a",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba487c-5606-4d4d-879f-f1c6512a768a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete model, endpoint\n",
    "try:\n",
    "    print(f\"Deleting model: {pipeline_model_name}\")\n",
    "    predictor.delete_model()\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting model: {pipeline_model_name}\\n{e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "    predictor.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting EP: {endpoint_name}\\n{e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f37039",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|structured|realtime|byoc|byoc-nginx-python|serial-inference-pipeline.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
