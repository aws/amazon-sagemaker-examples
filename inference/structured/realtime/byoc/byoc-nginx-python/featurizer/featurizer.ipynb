{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a6a1fb-1684-4f0f-9ec9-3123b65de4ea",
   "metadata": {},
   "source": [
    "# Build preprocessing model and custom inference image\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "We demonstrate building an ML application to predict the rings of Abalone.\n",
    "\n",
    "After the model is hosted for inference, the payload will be sent as a raw (untransformed) csv string to a real-time endpoint.\n",
    "The raw payload is first received by the preprocessing container. The raw payload is then transformed (feature-engineering) by the \"preprocessor\", and the transformed record (float values) are returned as a csv string by the preprocessing container.\n",
    "\n",
    "The transformed record is then passed to the predictor container (hosting an XGBoost model). The predictor then converts the input data (from preprocessing container) into [XGBoost DMatrix](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix) format, loads the model, calls `booster.predict(input_data)` and returns the predictions (Rings) in a JSON format.\n",
    "\n",
    "In this notebook, we build the \"preprocessor\" model that transforms the raw csv records using the following transformations with [scikit-learn](https://scikit-learn.org) \n",
    " - [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) for handling missing values, \n",
    " - [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for normalizing numerical columns, and\n",
    " - [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) for transforming categorical columns. After fitting the transformer, we save the fitted model to disk in [`joblib`](https://joblib.readthedocs.io/en/latest/persistence.html) format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84db376",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Upgrade the below packages to the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b997c6-f0ea-4be5-b2a0-d3bd1b726f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U awscli boto3 sagemaker watermark scikit-learn tqdm --quiet\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -p awscli,boto3,sagemaker,scikit-learn,tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c56ce-6408-4e1f-b7d9-38a06240ca54",
   "metadata": {},
   "source": [
    "## Build preprocessor model\n",
    "\n",
    "Here are the high level transformation steps (feature-engineering) we perform on the columns in the dataset\n",
    "\n",
    "- Target column here is \"Rings\"\n",
    "- Convert all numeric columns to dtype of `float64`\n",
    "  - Handle missing values with a `SimpleImputer` w/ strategy (mean)\n",
    "  - Scale numerical values using a `StandardScaler`\n",
    "- One hot encode \"sex\" column\n",
    "  - Handle missing values with a `SimpleImputer` w/ strategy (most common)\n",
    "- Process the columns using [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn-compose-columntransformer)\n",
    "- Serialize the transformer model `preprocess.joblib` to disk\n",
    "\n",
    "![](../images/byoc-featurizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ab0c2-441a-4b8b-baba-01ffa7256dd2",
   "metadata": {},
   "source": [
    "## Download UCI Abalone dataset from S3\n",
    "\n",
    "We download the UCI abalone dataset from sagemaker samples repository in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182a85f-224a-4054-9196-dc73e2abc420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from sagemaker import session, get_execution_role\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader, s3_path_join\n",
    "from pathlib import Path\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "sm_session = session.Session()\n",
    "region = sm_session.boto_region_name\n",
    "role = get_execution_role()\n",
    "bucket = sm_session.default_bucket()\n",
    "prefix = \"sagemaker/abalone/models/byoc\"\n",
    "default_bucket_prefix = sm_session.default_bucket_prefix\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if default_bucket_prefix:\n",
    "    prefix = f\"{default_bucket_prefix}/{prefix}\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "abalone_s3uri = (\n",
    "    f\"s3://sagemaker-example-files-prod-{region}/datasets/tabular/uci_abalone/abalone.csv\"\n",
    ")\n",
    "\n",
    "featurizer_image_name = \"abalone/featurizer\"\n",
    "\n",
    "base_dir = Path(\"../data\").resolve()\n",
    "featurizer_model_dir = Path(\"./models\").absolute()\n",
    "\n",
    "if not base_dir.joinpath(\"abalone.csv\").exists():\n",
    "    S3Downloader.download(s3_uri=abalone_s3uri, local_path=base_dir, sagemaker_session=sm_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459be145",
   "metadata": {},
   "source": [
    "### Fitting the preprocessor model\n",
    "\n",
    "First, we split the training data to train and test sets and save the test data to `abalone_test.csv` file in the data directory.\n",
    "\n",
    "We use [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn-compose-columntransformer) to impute, scale and one-hot encode the columns, then fit the [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn-compose-columntransformer) on input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f3466-8686-49ef-92b2-d1d14ce7d8f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "DATA_FILE = base_dir.joinpath(\"abalone.csv\")\n",
    "\n",
    "if not DATA_FILE.exists():\n",
    "    raise ValueError(f\"{DATA_FILE} doesn't exist\")\n",
    "\n",
    "if not featurizer_model_dir.exists():\n",
    "    featurizer_model_dir.mkdir(parents=True)\n",
    "\n",
    "# As we get a headerless CSV file, we specify the column names here.\n",
    "feature_columns_names = [\n",
    "    \"sex\",\n",
    "    \"length\",\n",
    "    \"diameter\",\n",
    "    \"height\",\n",
    "    \"whole_weight\",\n",
    "    \"shucked_weight\",\n",
    "    \"viscera_weight\",\n",
    "    \"shell_weight\",\n",
    "]\n",
    "label_column = \"rings\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"sex\": str,\n",
    "    \"length\": np.float64,\n",
    "    \"diameter\": np.float64,\n",
    "    \"height\": np.float64,\n",
    "    \"whole_weight\": np.float64,\n",
    "    \"shucked_weight\": np.float64,\n",
    "    \"viscera_weight\": np.float64,\n",
    "    \"shell_weight\": np.float64,\n",
    "}\n",
    "label_column_dtype = {\"rings\": np.float64}\n",
    "\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    DATA_FILE,\n",
    "    header=None,\n",
    "    names=feature_columns_names + [label_column],\n",
    "    dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype),\n",
    ")\n",
    "\n",
    "print(f\"Splitting raw dataset to train and test datasets..\")\n",
    "(df_train_val, df_test) = train_test_split(df, random_state=42, test_size=0.1)\n",
    "df_test.to_csv(f\"{base_dir}/abalone_test.csv\", index=False)\n",
    "print(f\"Test dataset written to {base_dir}/abalone_test.csv\")\n",
    "\n",
    "\n",
    "numeric_features = list(feature_columns_names)\n",
    "numeric_features.remove(\"sex\")\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_features = [\"sex\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Call fit on ColumnTransformer to fit all transformers to X, y\n",
    "preprocessor = preprocess.fit(df_train_val)\n",
    "\n",
    "# Save the processor model to disk\n",
    "\n",
    "joblib.dump(preprocess, featurizer_model_dir.joinpath(\"preprocess.joblib\"))\n",
    "print(f\"Saved preprocessor model to {featurizer_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a41d0e-de99-4690-80f0-64afcf00efb2",
   "metadata": {},
   "source": [
    "## Build custom inference container with preprocessor\n",
    "\n",
    "As our preprocessor model is now ready, we now build our own container to host the preprocessor model. This is also referred to as Bring-Your-Own-Container (BYOC) mode in Amazon SageMaker.\n",
    "\n",
    "In BYOC mode, we bring our own serving stack. We use [nginx](https://nginx.org/), [\"gunicorn\"](https://gunicorn.org/#deployment) and [\"flask\"](https://flask.palletsprojects.com/en/2.3.x/tutorial/factory/) app as our serving stack.\n",
    "\n",
    "First, we create an inference script [`preprocessing.py`](./code/preprocessing.py) that implements the following:\n",
    "\n",
    "1. Implement `/ping` and `/invocations` routes\n",
    "1. Define functions to load the serialized model (`./models/preprocess.joblib`) from disk and to transform the received input with `model.transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8fb65c-f1be-428d-acd8-9a39e54adf0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize code/preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bde474",
   "metadata": {},
   "source": [
    "Next, we create configuration files for \n",
    "- [Nginx](https://nginx.org/) (reverse-proxy), [`nginx.conf`](code/nginx.conf)\n",
    "- [Gunicorn](https://gunicorn.org/#deployment) (webserver-gateway interface), [`wsgi.py`](code/wsgi.py)\n",
    "- [serve](code/serve) (python script to launch nginx, [`gunicorn`](https://gunicorn.org/#deployment) processes)\n",
    "\n",
    "For convenience, we place all the above files along with [`inference.py`](./code/inference.py) under `code/` directory\n",
    "\n",
    "Next, build and test the custom image locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7343a-8ee0-40d2-8883-2d0432fee7db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build and test docker image locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff65d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100956c",
   "metadata": {},
   "source": [
    "- Build [Dockerfile](./Dockerfile) using command `docker build -t abalone/featurizer .` command\n",
    "- Next, we launch the inference container locally by mounting `./models` directory to `/opt/ml/model` in the container and exposing port `8080`\n",
    "  ```docker\n",
    "  docker run --rm -v $(pwd)/models:/opt/ml/model -p 8080:8080 abalone/featurizer\n",
    "  ```\n",
    "- We then test the running container by invoking `/ping` and `/invocations` endpoint with some test data\n",
    "- Finally, we push local image to ECR using [./build_n_push.sh](./build_n_push.sh) shell script\n",
    "\n",
    ">NOTE: We set `abalone/featurizer` as the docker image name.\n",
    "\n",
    "Sample commands are saved in [commands.txt](./commands.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ee93e-cf19-4bb3-bf76-96c983c6354c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker build -t $featurizer_image_name ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c495637",
   "metadata": {},
   "source": [
    "**NOTE:** Do not run the below cell from the notebook, as the container will be in serving mode, the execution hangs.\n",
    "\n",
    "Open a terminal and change directory into the `featurizer/` directory where `models/` folder exists and run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13bff1-62a7-453a-bec2-0d397f0c9bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# docker run --rm -v $(pwd)/models:/opt/ml/model -p 8080:8080 abalone/featurizer\n",
    "\n",
    "# NOTE: If you run this from the NB, the cell will be continue with execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39f343",
   "metadata": {},
   "source": [
    "#### Invoke endpoint /ping and /invocations\n",
    "\n",
    "Test serving container by sending HTTP requests using curl to both `/ping` and `/invocations` endpoints\n",
    "\n",
    "Uncomment below cells to test inference on local container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5da8a5-4eb2-4c7e-84e8-d1ca0246a597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Health check by invoking  /ping\n",
    "# !curl http://localhost:8080/ping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658552be-ad2b-4016-a237-3af7fa6f81e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send test records\n",
    "# !curl --data-raw 'I,0.365,0.295,0.095,0.25,0.1075,0.0545,0.08,9.0' \\\n",
    "# -H 'Content-Type: text/csv' \\\n",
    "# -v http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792017e-c4b7-45c7-873d-3d05e6e32327",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (Optional) View container logs locally (using docker logs)\n",
    "\n",
    "- To inspect a running container to view container config values or IP address we use `docker inspect <CONTAINER_ID_OR_NAME>`\n",
    "- To view and tail logs generated in the container we use `docker logs --follow <NUM_OF_LINES> <CONTAINER_ID_OR_NAME>`\n",
    "- SageMaker publishes container logs to CloudWatch. CloudWatch logs for a given endpoint are published to the following log stream path\n",
    "`/aws/sagemaker/Endpoints/ENDPOINT_NAME/VARIANT_NAME/CONTAINER_NAME`\n",
    "\n",
    "**NOTE:** \n",
    "1. Run this command in a terminal as running this inside a cell would hang execution.\n",
    "1. the below command assumes there is only one running container. If you have more, then use command with container name `docker inspect <CONTAINER_ID_OR_NAME>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe1cbf-70c0-4f21-9cb4-55c10b39c7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RUN THE BELOW IN A TERMINAL\n",
    "# docker ps --format \"{{.Names}}\" | xargs -n1 -I{} docker logs --follow --tail 50 {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb2be3-7956-4e32-aa72-88dc00379d7f",
   "metadata": {},
   "source": [
    "### Tag and push the local image to private ECR\n",
    "\n",
    "- Tag the `abalone/featurizer` local image to `{account_id}.dkr.ecr.{region}.amazonaws.com/{imagename}:{tag}` format\n",
    "- Run [./build_n_push.sh](./build_n_push.sh) shell script with image name `abalone/featurizer` as parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d088a-ff13-44d3-8771-f8b833bbfedc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize ./build_n_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e8bf8-2750-43f6-9ef8-d116111c2aa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# push image to private ECR\n",
    "!chmod +x ./build_n_push.sh\n",
    "\n",
    "!./build_n_push.sh $featurizer_image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764c2ce7-51fa-426e-b55c-e89561ea86ec",
   "metadata": {},
   "source": [
    "### Test preprocessor inference image by deploying to a real-time endpoint\n",
    "\n",
    "1. We first compress and upload preprocessor model artifact to S3\n",
    "2. Create SageMaker [`Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) object with `image_uri` pointing to the custom image in ECR and s3 location of the model artifact from step 1\n",
    "3. Deploy model to an Amazon SageMaker real-time endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c71c8-5ffc-41a1-9cf0-ca6e81b6edbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compress and upload model artifact to S3\n",
    "\n",
    "Generated preprocessor model (under `./models` directory) is compressed to a `tar.gz` format and uploaded to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68824559-7780-475c-b32d-83c626366408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.chdir(current_dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e11a0a-73dd-46d4-bd21-43f14cb3598f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "os.chdir(featurizer_model_dir)\n",
    "\n",
    "model_s3uri = s3_path_join(f\"s3://{bucket}/{prefix}\", \"featurizer\")\n",
    "\n",
    "featurizer_model_path = featurizer_model_dir.joinpath(\"model.tar.gz\")\n",
    "\n",
    "if featurizer_model_path.exists():\n",
    "    featurizer_model_path.unlink()\n",
    "\n",
    "# SageMaker expects model artifacts to be compressed to `model.tar.gz`\n",
    "tar_cmd = \"tar -czvf model.tar.gz preprocess.joblib ../code/\"\n",
    "result = subprocess.run(tar_cmd, shell=True, capture_output=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"{featurizer_model_path} archive created successfully!\")\n",
    "    os.chdir(current_dir)\n",
    "else:\n",
    "    os.chdir(featurizer_model_dir)\n",
    "    print(\"An error occurred:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a204945-e9ff-4379-8df3-2592a3eadd0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload compressed model artifact to S3 using S3Uploader utility class\n",
    "model_data_url = S3Uploader.upload(\n",
    "    local_path=featurizer_model_path.absolute(),\n",
    "    desired_s3_uri=model_s3uri,\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "print(f\"Uploaded predictor model.tar.gz to {model_data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930892e-a7fe-46c9-8ea5-1aa307e7c9b4",
   "metadata": {},
   "source": [
    "### Deploy model to endpoint\n",
    "\n",
    "We deploy the model to a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc3275-67f3-4c5c-bb5a-1fef24de6bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing Scikit-learn model\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "from sagemaker.model import Model\n",
    "\n",
    "suffix = f\"{str(uuid4())[:5]}-{datetime.now().strftime('%d%b%Y')}\"\n",
    "\n",
    "featurizer_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{featurizer_image_name}:latest\"\n",
    "\n",
    "model_name = f\"AbaloneXGB-featurizer-{suffix}\"\n",
    "print(f\"Creating featurizer model: {model_name}\")\n",
    "sklearn_model = Model(\n",
    "    image_uri=featurizer_image_uri,\n",
    "    name=model_name,\n",
    "    model_data=model_data_url,\n",
    "    role=role,\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "\n",
    "endpoint_name = f\"AbaloneXGB-featurizer-ep-{suffix}\"\n",
    "print(f\"Deploying model {model_name} to Endpoint: {endpoint_name}\")\n",
    "\n",
    "predictor = sklearn_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a2b08-7c9b-485f-abd3-81b7be778d35",
   "metadata": {},
   "source": [
    "### Send test payload as inference\n",
    "\n",
    "- Use records from `./data/abalone_test.csv` as payload to \"featurizer\" model\n",
    "- Save responses (transformations) to `./data/abalone_featurizer_predictions.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39685371-f030-47fa-81c1-9d0c03a645c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "raw_dataset = base_dir.joinpath(\"abalone_test.csv\")\n",
    "transformed_dataset = base_dir.joinpath(\"abalone_featurizer_predictions.csv\")\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sm_session)\n",
    "predictor.content_type = \"text/csv\"\n",
    "predictor.serializer = CSVSerializer()\n",
    "predictor.accept = \"text/csv\"\n",
    "predictor.deserializer = CSVDeserializer()\n",
    "\n",
    "\n",
    "# Send 50 records for inference\n",
    "limit = 10\n",
    "i = 0\n",
    "\n",
    "# Remove file if exists\n",
    "if transformed_dataset.exists():\n",
    "    transformed_dataset.unlink()\n",
    "\n",
    "transformations = []\n",
    "with open(raw_dataset, \"r\") as _f:\n",
    "    lines = _f.readlines()\n",
    "    for row in lines:\n",
    "        # Skip headers\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "        elif i <= limit:\n",
    "            row = row.rstrip(\"\\n\")\n",
    "            splits = row.split(\",\")\n",
    "            # Remove the target column (last column)\n",
    "            label = splits.pop(-1)\n",
    "            input_cols = \",\".join(s for s in splits)\n",
    "            prediction = None\n",
    "            try:\n",
    "                # print(input_cols)\n",
    "                response = predictor.predict(input_cols)\n",
    "                response = \",\".join(map(str, response[0]))\n",
    "                print(response)\n",
    "                transformations.append(response)\n",
    "                i += 1\n",
    "                sleep(0.15)\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {e}\")\n",
    "                pass\n",
    "\n",
    "with open(transformed_dataset, \"w\") as csvfile:\n",
    "    for line in transformations:\n",
    "        csvfile.write(f\"{line}\\n\")\n",
    "\n",
    "csvfile.close()\n",
    "\n",
    "print(f\"Saved transformed records to {transformed_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147b81e-8722-457b-9bf9-3cddb4528426",
   "metadata": {},
   "source": [
    "### View Logs emitted by the endpoint in CloudWatch\n",
    "\n",
    "Logs from the Amazon SageMaker real-time endpoint that are written to `stdout` and `stderr` streams are automatically streamed to Amazon CloudWatch.\n",
    "\n",
    "We can verify the logs by reading them from the CloudWatch log stream for the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d456719-6d5c-40c7-8e1a-dfb1940cb933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Get endpoint logs from CloudWatch log stream (last 15minutes)\n",
    "logs_client = boto3.client(\"logs\")\n",
    "end_time = datetime.utcnow()\n",
    "start_time = end_time - timedelta(minutes=15)\n",
    "\n",
    "log_group_name = f\"/aws/sagemaker/Endpoints/{endpoint_name}\"\n",
    "log_streams = logs_client.describe_log_streams(logGroupName=log_group_name)\n",
    "log_stream_name = log_streams[\"logStreams\"][0][\"logStreamName\"]\n",
    "\n",
    "# Retrieve the logs\n",
    "logs = logs_client.get_log_events(\n",
    "    logGroupName=log_group_name,\n",
    "    logStreamName=log_stream_name,\n",
    "    startTime=int(start_time.timestamp() * 1000),\n",
    "    endTime=int(end_time.timestamp() * 1000),\n",
    ")\n",
    "\n",
    "# Print the logs\n",
    "for event in logs[\"events\"]:\n",
    "    print(f\"{datetime.fromtimestamp(event['timestamp'] // 1000)}: {event['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23f187-7998-461d-a615-74dcb7433888",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Finally, Cleanup resources. Delete endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab7bcd-96eb-4ac7-8ad7-209addb73d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete endpoint\n",
    "try:\n",
    "    print(f\"Deleting model: {model_name}\")\n",
    "    predictor.delete_model()\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting model: {model_name}\\n{e}\")\n",
    "    pass\n",
    "\n",
    "# Delete model\n",
    "try:\n",
    "    print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "    predictor.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint: {endpoint_name}\\n{e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62c340",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|featurizer.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
