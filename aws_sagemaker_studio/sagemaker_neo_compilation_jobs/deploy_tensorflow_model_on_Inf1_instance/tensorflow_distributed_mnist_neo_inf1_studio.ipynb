{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Deploy a TensorFlow model on Inf1 instances\n",
    "\n",
    "Amazon SageMaker supports Inf1 instances for high performance and cost-effective inferences. Inf1 instances are ideal for large scale machine learning inference applications like image recognition, speech recognition, natural language processing, personalization, and fraud detection. In this example, train a classification model on the MNIST dataset using TensorFlow, compile it using Amazon SageMaker Neo, deploy the model on Inf1 instances on a SageMaker endpoint, and use the Neo Deep Learning Runtime to make inferences in real-time and with low latency. \n",
    "\n",
    "## Inf 1 instances \n",
    "Inf1 instances are built from the ground up to support machine learning inference applications and feature up to 16 AWS Inferentia chips, which are high-performance machine learning inference chips designed and built by AWS. The Inferentia chips are coupled with the latest custom 2nd generation Intel® Xeon® Scalable processors and up to 100 Gbps networking to enable high throughput inference. With 1 to 16 AWS Inferentia chips per instance, Inf1 instances can scale in performance to up to 2000 Tera Operations per Second (TOPS) and deliver extremely low latency for real-time inference applications. The large on-chip memory on AWS Inferentia chips used in Inf1 instances allows caching of machine learning models directly on the chip. This eliminates the need to access outside memory resources during inference, enabling low latency without impacting bandwidth. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* SageMaker Studio with Python 3 (Data Science) kernel\n",
    "* SageMaker SDK version 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the required version of SageMaker and TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "if sagemaker.__version__ >= '2':\n",
    "    orig_sm_version = sagemaker.__version__\n",
    "    with open('orig_sm_version.txt', \"w\") as f:\n",
    "        f.write(orig_sm_version)\n",
    "    %pip install \"sagemaker>=1.14.2,<2\"\n",
    "\n",
    "if sagemaker.__version__ >= '2':\n",
    "    print(f\"WARNING: The current running version of the SageMaker SDK is {sagemaker.__version__}, which will cause this notebook to fail. \"\n",
    "          f\"Restart the kernel to run the required version of the SDK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow==1.15.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a SageMaker session and get the excecution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "\n",
    "utils.convert_to(data_sets.train, 'train', 'data')\n",
    "utils.convert_to(data_sets.validation, 'validation', 'data')\n",
    "utils.convert_to(data_sets.test, 'test', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data to Amazon Simple Storage Service (Amazon S3)\n",
    "Use the `sagemaker.Session.upload_data` function to upload datasets to an S3 location. The return value is the location, which is used when the training job is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a script for distributed training \n",
    "\n",
    "To see the code for the network model, either browse to `mnist.py` in the File Browser or run the following command to show it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is an adaptation of the [TensorFlow MNIST example](https://github.com/tensorflow/models/tree/master/official/vision/image_classification). It provides a `model_fn(features, labels, mode)` function that is used for training, evaluation and inference. For more details, see [TensorFlow MNIST distributed training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_script_mode_training_and_serving/tensorflow_script_mode_training_and_serving.ipynb).\n",
    "\n",
    "At the end of the training script, there are two additional functions that are used with Neo Deep Learning Runtime:\n",
    "\n",
    "* `neo_preprocess(payload, content_type)`: takes the payload and Content-Type of each incoming request and returns a NumPy array.\n",
    "* `neo_postprocess(result)`: takes the prediction results produced by Deep Learning Runtime and returns the response body.\n",
    "\n",
    "LeCun, Y., Cortes, C., & Burges, C. (2010). MNIST handwritten digit databaseATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job\n",
    "\n",
    "Use the `sagemaker.TensorFlow` estimator to create a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             framework_version='1.11.0',\n",
    "                             training_steps=1000, \n",
    "                             evaluation_steps=100,\n",
    "                             train_instance_count=2,\n",
    "                             train_instance_type='ml.c5.xlarge',\n",
    "                             sagemaker_session=sagemaker_session)\n",
    "\n",
    "mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method creates a training job in two **ml.c5.xlarge** instances. The logs from `fit` show the instances training, evaluating, and incrementing the number of **training steps**. \n",
    "\n",
    "At the end of the training, the training job generates a saved model for compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model\n",
    "\n",
    "Deploy the model to an Inf1 instance for real-time inferences. Once the training is complete, compile the model using SageMaker Neo to optimize performance for the desired deployment target. SageMaker Neo enables you to train machine learning models once and run them anywhere in the cloud and at the edge. To compile the trained model for deployment to Inf1 instances, use the  `TensorFlowEstimator.compile_model` method and select `ml_inf1` as the deployment target. The compiled model is deployed on an endpoint that uses Inf1 instances in SageMaker.\n",
    "\n",
    "### Compile the model \n",
    "\n",
    "The `input_shape` is the definition for the model's input tensor and `output_path` is where the compiled model is stored in S3.\n",
    "\n",
    "> Note: If `compile_model` results in a permission error, verify that the execution role returned previously by `get_execution_role()` has access to the Amazon S3 bucket specified in `output_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/'.join(mnist_estimator.output_path.split('/')[:-1])\n",
    "mnist_estimator.framework_version='1.15.0'\n",
    "\n",
    "optimized_estimator = mnist_estimator.compile_model(target_instance_family='ml_inf1', \n",
    "                              input_shape={'data':[1, 784]},  # Batch size 1, 3 channels, 224x224 Images.\n",
    "                              output_path=output_path,\n",
    "                              framework='tensorflow', framework_version='1.15.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to a SageMaker endpoint\n",
    "\n",
    "Deploy the compiled model to an Amazon SageMaker endpoint. This example uses the Inf1 `ml.inf1.xlarge` instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_predictor = optimized_estimator.deploy(initial_instance_count = 1,\n",
    "                                                 instance_type = 'ml.inf1.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure a serializer for `application/vnd+python.numpy+binary` Content-Type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def numpy_bytes_serializer(data):\n",
    "    f = io.BytesIO()\n",
    "    np.save(f, data)\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "optimized_predictor.content_type = 'application/vnd+python.numpy+binary'\n",
    "optimized_predictor.serializer = numpy_bytes_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the endpoint\n",
    "\n",
    "When the endpoint is ready, send requests to it and receive inference results in real time with low latency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from IPython import display\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(10):\n",
    "    data = mnist.test.images[i]\n",
    "    # Display image\n",
    "    im = PIL.Image.fromarray(data.reshape((28,28))*255).convert('L')\n",
    "    display.display(im)\n",
    "    # Invoke endpoint with image\n",
    "    predict_response = optimized_predictor.predict(data)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response\n",
    "    print(\"prediction is {}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(optimized_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rollback the SageMaker Python SDK version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rollback the SageMaker Python SDK to the kernel's original version\n",
    "if os.path.exists('orig_sm_version.txt'):\n",
    "    with open('orig_sm_version.txt', 'r') as f:\n",
    "        orig_sm_version = f.read()\n",
    "    print(f\"Original version: {orig_sm_version}\")\n",
    "    print(f\"Current version: {sagemaker.__version__}\")\n",
    "    %pip install sagemaker=={orig_sm_version}\n",
    "    os.remove('orig_sm_version.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the kernel to run the updated version of the SDK."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
