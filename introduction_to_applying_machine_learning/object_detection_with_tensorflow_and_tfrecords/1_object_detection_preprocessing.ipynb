{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build an Object Detection Model on TensorFlow and SageMaker: Overview and Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Background\n",
    "\n",
    "This notebook is one of a sequence of notebooks that show you how to use various SageMaker functionalities to build, train, and test the object detection model, including data pre-processing steps like ingestion, cleaning and processing, training, and test the model. There are two parts of the demo: \n",
    "\n",
    "1. Overview and Data Preparation (current notebook) - you will preprocess the data, then create a json file from the cleaned data. By the end of part 1, you will have a complete data set that contains all features used on Object selection to be ingested by a data loader in *[TensorFlow](https://github.com/tensorflow/tensorflow)* using 'TFRecords'.\n",
    "1. Data loader creation and Model Training - you will use the data set built from part 1 to create a data loader for Tensorflow using *[Keras CV](https://github.com/keras-team/keras-cv)*, train the model and then test the model predictability with the test data. \n",
    "\n",
    "\n",
    "## Content\n",
    "* [Overview](#Overview)\n",
    "* [Data Selection](#Data-Selection)\n",
    "* [Prepare Data](#Prepare-Data)\n",
    "* [Preprocessing the Dataset with SageMaker](#Preprocessing-the-Dataset-with-SageMaker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "### What is Object Detection, and why is it important?\n",
    "\n",
    "Object detection refers to detecting instances of objects from certain classes in images or videos. It allows for multiple objects to be detected and localized in an image. Object detection is commonly used in applications such as self-driving cars, face detection, video surveillance, etc.  \n",
    "\n",
    "### Use Cases for Object Detection\n",
    "\n",
    "Some common use cases of object detection include:\n",
    "\n",
    "- Self driving cars - detect pedestrians, cars, traffic signs, etc.\n",
    "- Face detection - detect faces in images and videos for applications like security and tagging people in images.\n",
    "- Video surveillance - detect suspicious activities or objects.\n",
    "- Medical imaging - detect anomalies, tumors, etc. in medical scans.\n",
    "- Retail - detect objects on shelves for inventory management.\n",
    "\n",
    "### Define the Machine Learning Problem  \n",
    "\n",
    "Object detection can be formulated as a supervised machine learning problem:\n",
    "\n",
    "- Given a set of labelled images containing objects from certain classes, train a model to detect the presence and location of those objects in new images.\n",
    "\n",
    "- The model needs to identify the class of objects present and draw bounding boxes around them indicating their locations.\n",
    "\n",
    "### Data Requirements\n",
    "\n",
    "- Large dataset of images with object annotation - Object locations are annotated using bounding boxes around them.\n",
    "\n",
    "- Variety of images - Objects captured under different conditions of illumination, scales, occlusion, viewpoints etc. \n",
    "\n",
    "### Challenges\n",
    "\n",
    "- Data annotation - Time consuming and expensive process.\n",
    "\n",
    "- Class imbalance - Models tend to perform better for classes with more examples.\n",
    "\n",
    "- Viewpoint variation - Objects look different from different angles and viewpoints. \n",
    "\n",
    "- Background clutter - Objects may blend with their surroundings.\n",
    "\n",
    "- Small objects - Harder to detect smaller objects.\n",
    "\n",
    "- Occlusion - Objects hidden behind other things are tougher to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Data Selection\n",
    "\n",
    "The dataset that will be used for object detection is the *[PASCAL VOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html)* dataset. This dataset contains images, annotations, and evaluation tools for object detection.\n",
    "\n",
    "### Dataset Structure\n",
    "The folder structure is:\n",
    "\n",
    "- `Annotations` - contains XML annotation files \n",
    "- `ImageSets` - contains text files that specify train, val, test splits\n",
    "- `JPEGImages` - contains JPEG images\n",
    "- `SegmentationClass` - contains segmentation class PNGs \n",
    "- `SegmentationObject` - contains segmented objects PNGs\n",
    "\n",
    "### Annotation Format\n",
    "\n",
    "The XML annotation files contain information about objects present in each image:\n",
    "\n",
    "- `<filename>`: name of the JPEG image\n",
    "- `<size>`: image width, height, depth\n",
    "- `<object>`: contains information about each object instance\n",
    "    - `<name>`: object class name \n",
    "    - `<pose>`: orientation of object (left, right, frontal, rear) \n",
    "    - `<truncated>`: whether object is truncated \n",
    "    - `<difficult>`: whether object is difficult to detect\n",
    "    - `<bndbox>`: bounding box of object \n",
    "        - contains `<xmin>, <ymin>, <xmax>, <ymax>` coordinates\n",
    "\n",
    "### Classes\n",
    "\n",
    "There are 20 classes representing objects:\n",
    "\n",
    "'person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep', 'airplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train', 'bottle', 'chair', 'dining table', 'potted plant', 'sofa', 'tv/monitor'\n",
    "\n",
    "### Data Splits\n",
    "\n",
    "The data is split into train, validation, and test sets defined in the 'ImageSets' folder text files.\n",
    "\n",
    "- Train - 5717 images \n",
    "- Validation - 5823 images\n",
    "- Test - 10991 images\n",
    "\n",
    "This dataset will be used to train an object detection model to detect and localize the defined classes.\n",
    " * the data will be downloaded from an [Amazon Simple Storage Service](https://aws.amazon.com/s3/) (Amazon S3) bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For this specific use case, you will focus on a solution to detect and localize objects in images. Some possible expansions of the work include:\n",
    "\n",
    "- Detect additional classes by expanding the dataset with more images and annotations\n",
    "- Improve localization accuracy by generating more precise bounding boxes \n",
    "- Add image attributes (lighting, orientation, occlusion) and object attributes (size, color) to the data\n",
    "- Expand to video data and perform object tracking over frames\n",
    "- Perform instance segmentation instead of bounding box detection\n",
    "- Develop a real-time object detection application \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set Up Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install --upgrade sagemaker boto3 --quiet\n",
    "!pip install keras-cv tensorflow~=2.13.0 --upgrade --quiet\n",
    "#!pip install keras-cv --upgrade --quiet\n",
    "#!pip install pickleshare --upgrade --quiet\n",
    "#!pip install opencv-python\n",
    "#!pip install matplotlib\n",
    "#!conda install opencv\n",
    "\n",
    "# tensorflow 2.15.0 is needed for Keras CV in Keras 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Parameters \n",
    "The following lists configurable parameters that are used throughout the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# Get the default S3 bucket associated with your SageMaker session\n",
    "bucket = sagemaker_session.default_bucket()  # replace with your own bucket name if you have one\n",
    "# Create an S3 resource client\n",
    "s3 = boto3.resource(\"s3\")\n",
    "# Get the AWS region name\n",
    "region = boto3.Session().region_name\n",
    "# Get the execution role for SageMaker\n",
    "role = sagemaker.get_execution_role()\n",
    "# Create a SageMaker client\n",
    "smclient = boto3.Session().client(\"sagemaker\")\n",
    "# Set a prefix for your S3 objects\n",
    "prefix = \"object-detection-tensorflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ingest Data\n",
    "\n",
    "We ingest the dataset from a public SageMaker S3 training bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### Alternative: you can copy data from public S3 bucket to your own bucket\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "BUCKET_NAME = f\"sagemaker-example-files-prod-{region}\"  # dataset bucket name\n",
    "KEY = \"datasets/image/VOC2012/VOCtrainval_11-May-2012.tar\"  # dataset object key\n",
    "\n",
    "# Define the local path where the dataset will be downloaded\n",
    "raw_dataset_folder = \"./data/VOCtrainval_11-May-2012.tar\"\n",
    "\n",
    "# Define the target S3 bucket and prefix where the dataset will be copied\n",
    "s3_target_bucket = f\"s3://{bucket}/{prefix}/data\"\n",
    "\n",
    "try:\n",
    "    # Download the dataset from the source bucket to the local path\n",
    "    s3.Bucket(BUCKET_NAME).download_file(KEY, raw_dataset_folder)\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    # Handle exceptions related to the S3 download operation\n",
    "    if e.response[\"Error\"][\"Code\"] == \"404\":\n",
    "        print(\"The object does not exist.\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload local tar file to s3_target_bucket\n",
    "S3_dataset_bucket = sagemaker.s3.S3Uploader.upload(raw_dataset_folder, s3_target_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# this step can take a while if you use depending ec2 instance,\n",
    "# untar the partitioned data files into the data folder\n",
    "!tar -xf {raw_dataset_folder} -C ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove local tar file\n",
    "!rm {raw_dataset_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# location of tar file on S3 bucket for later processing\n",
    "S3_dataset_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Due to the size and complexity of the data (images, XML, text files, size of 2GB), you will start exploring our data by using the text files with the train, val and test splits. \n",
    "The image metadata is in the XML files, so you need to create a file that is easier to process in the data loader. \n",
    "For this case, you will use JSON Lines format to store a list of JSON files that can then be used to create the TensorFlow data loaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Remove irrelevant features\n",
    "\n",
    "Upon initial inspection of the XML structure, features such as 'folder', 'source', 'object/pose', 'object/truncated', 'object/difficult', 'object/part' are not relevant and can be excluded. You will need to write a function that parses the XML files, extracts the relevant features, and creates a dictionary of these features that can then be used to generate a JSON Lines format files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from defusedxml.ElementTree import parse\n",
    "\n",
    "# this variable helps to track class names\n",
    "class_ids = set()\n",
    "\n",
    "\n",
    "# function that parse XML File and returns a dict\n",
    "def xml_data_parser(xml_file):\n",
    "    if os.path.isfile(xml_file):\n",
    "        with open(xml_file) as f:\n",
    "            tree = parse(f)\n",
    "            root = tree.getroot()\n",
    "        annotation_dict = {}\n",
    "        filename = root.findall(\"filename\")[0].text\n",
    "        width = root.findall(\"size\")[0].find(\"width\").text\n",
    "        height = root.findall(\"size\")[0].find(\"height\").text\n",
    "        depth = root.findall(\"size\")[0].find(\"depth\").text\n",
    "        annotation_dict[\"image\"] = filename\n",
    "        annotation_dict[\"width\"] = width\n",
    "        annotation_dict[\"height\"] = height\n",
    "        annotation_dict[\"annotation\"] = []\n",
    "        for i in range(len(root.findall(\"object\"))):\n",
    "            annotation = {}\n",
    "            xmin = root.findall(\"object\")[i].find(\"bndbox\").find(\"xmin\").text\n",
    "            ymin = root.findall(\"object\")[i].find(\"bndbox\").find(\"ymin\").text\n",
    "            xmax = root.findall(\"object\")[i].find(\"bndbox\").find(\"xmax\").text\n",
    "            ymax = root.findall(\"object\")[i].find(\"bndbox\").find(\"ymax\").text\n",
    "            name = root.findall(\"object\")[i].find(\"name\").text\n",
    "            annotation[\"category\"] = name\n",
    "            annotation[\"bbox\"] = [xmin, ymin, xmax, ymax]\n",
    "            annotation_dict[\"annotation\"].append(annotation)\n",
    "            class_ids.add(name)\n",
    "        return annotation_dict\n",
    "    else:\n",
    "        print(f\"{xml_file} doesnt exists\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# This function parses XML file and returns a dict structure\n",
    "xml_data_parser(\"./data/VOCdevkit/VOC2012/Annotations/2007_000027.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Exploration\n",
    "To avoid repeatedly loading the individual XML files, we will create a JSON Lines format file containing the training and validation datasets. The VOC2012 dataset includes TXT files for object detection that contain the names of the corresponding XML and JPEG files. We can use these names as IDs when generating the JSON Lines format files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# load file and create a list with name of files\n",
    "def parse_dataset_files(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        dataset_lines = f.readlines()\n",
    "        dataset_files = [i.strip() for i in dataset_lines]\n",
    "\n",
    "    xml_files = [\n",
    "        f\"./data/VOCdevkit/VOC2012/Annotations/{dataset_files[i]}.xml\"\n",
    "        for i in range(len(dataset_files))\n",
    "    ]\n",
    "\n",
    "    result = [xml_data_parser(xml_file) for xml_file in xml_files]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Dumps the list of dicts to a JSONL file\n",
    "def dump_and_load_dataset(dataset, filename):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        json.dump(dataset, outfile)\n",
    "    # Loads the JSONL file to a list of dicts structure\n",
    "    with open(filename, \"r\") as f:\n",
    "        output_dataset = json.load(f)\n",
    "    return output_dataset\n",
    "\n",
    "\n",
    "# creation of datasets(list of dicts)\n",
    "train_dataset = parse_dataset_files(\"./data/VOCdevkit/VOC2012/ImageSets/Main/train.txt\")\n",
    "val_dataset = parse_dataset_files(\"./data/VOCdevkit/VOC2012/ImageSets/Main/val.txt\")\n",
    "training_dataset_size = len(train_dataset)\n",
    "validation_dataset_size = len(val_dataset)\n",
    "print(\n",
    "    f\"{training_dataset_size} samples for training and {validation_dataset_size} samples for validation before loading\"\n",
    ")\n",
    "\n",
    "# Dump of dicts to jsonl files and load for testing purposes\n",
    "train_filename = \"./data/train_labels_VOC.jsonl\"\n",
    "# val_filename=\"./data/val_labels_VOC.jsonl\"\n",
    "train_dataset = dump_and_load_dataset(train_dataset, train_filename)\n",
    "# val_dataset=dump_and_load_dataset(val_dataset,val_filename)\n",
    "\n",
    "# creates validation dataset from training dataset with test_size=0.33 and random_state=42\n",
    "training_dataset, val_dataset = train_test_split(train_dataset, test_size=0.33, random_state=42)\n",
    "\n",
    "# store S3bucket string from preprocessing\n",
    "%store training_dataset_size\n",
    "%store validation_dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creation of a dict to map classes with class ids\n",
    "class_ids = sorted(list(class_ids))\n",
    "# Creating a dictionary that maps numerical class IDs to their corresponding class names\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "# Creating an inverse dictionary that maps class names to their corresponding numerical class IDs\n",
    "class_mapping_by_label = dict(zip(class_ids, range(len(class_ids))))\n",
    "\n",
    "\n",
    "# This function takes a class label (string) as input\n",
    "# and returns the corresponding numerical class ID\n",
    "def class_text_to_int(label):\n",
    "    if label in class_mapping_by_label.keys():\n",
    "        return class_mapping_by_label[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the class mapping in the notebook's metadata\n",
    "%store class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prints the first element of the traing JSONL fle\n",
    "print(train_dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Evaluating the Object Detection Data\n",
    "\n",
    "To validate that our data is correct and that the bounding boxes are in the proper format, we can load and visualize the first 5 images in our dataset. This allows us to inspect the current image along with the bounding boxes and labels. By sampling these initial images, we can confirm the data is formatted appropriately before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "# Function to plot bounding boxes on an image\n",
    "def plot_boxes(image_path, bboxes, width, height):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    # print(bboxes)\n",
    "\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    # Load a font for drawing labels\n",
    "    # font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "\n",
    "    # Loop over the bounding boxes and draw them on the image\n",
    "    for bbox in bboxes:\n",
    "        # x1, y1, x2, y2 = bbox\n",
    "        x1 = float(bbox[\"bbox\"][0])\n",
    "        y1 = float(bbox[\"bbox\"][1])\n",
    "        x2 = float(bbox[\"bbox\"][2])\n",
    "        y2 = float(bbox[\"bbox\"][3])\n",
    "        draw.rectangle([(x1, y1), (x2, y2)], outline=\"red\", width=2)\n",
    "\n",
    "        # Draw the label\n",
    "        text_width, text_height = draw.textsize(bbox[\"category\"])\n",
    "        draw.rectangle([(x1, y1), (x1 + text_width + 10, y1 - text_height - 5)], fill=\"red\")\n",
    "        draw.text((x1 + 5, y1 - text_height), bbox[\"category\"], fill=\"white\")\n",
    "\n",
    "    # Display the image\n",
    "    image.show()\n",
    "\n",
    "\n",
    "# Loop through first 5 images in the dataset and plot bounding boxes\n",
    "for i in range(5):\n",
    "    boxes = [i for i in train_dataset[i][\"annotation\"]]\n",
    "    width = float(train_dataset[i][\"width\"])\n",
    "    height = float(train_dataset[i][\"height\"])\n",
    "    plot_boxes(\n",
    "        f\"./data/VOCdevkit/VOC2012/JPEGImages/{train_dataset[i]['image']}\", boxes, width, height\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Addressing Imbalance Classes\n",
    "Imbalanced class distributions, where some classes are much more frequent than others, are very common in object detection datasets.  Useful tactics for handling imbalance include under sampling, oversampling, and augmentation. In this case, we will utilize data augmentation in the Keras preprocessing layers inside our data loader to improve balance. Image augmentation via random scaling, cropping, flipping, and rotation can generate synthetic minority class examples. This helps equalize class frequencies so models better learn the rare classes too. Their implementation demonstrates augmentation nearly doubling 'mAP' performance on an imbalanced detection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "# Define a function to count the number of instances for each class in the dataset\n",
    "def count_dataset(dataset, name):\n",
    "    categories = []\n",
    "    for sample in dataset:\n",
    "        for category in sample[\"annotation\"]:\n",
    "            categories.append(category[\"category\"])\n",
    "    class_counts = collections.Counter(categories)\n",
    "    class_df = pd.DataFrame.from_dict(class_counts, orient=\"index\").reset_index()\n",
    "    class_df.columns = [\"label\", \"count\"]\n",
    "    class_df[\"type\"] = name\n",
    "\n",
    "    return class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datasets = {\"train\": train_dataset, \"val\": val_dataset}\n",
    "for key in datasets:\n",
    "    dataset_df = count_dataset(datasets[key], key)\n",
    "    ax = sns.catplot(\n",
    "        data=dataset_df,\n",
    "        x=\"count\",\n",
    "        y=\"label\",\n",
    "        col=\"type\",\n",
    "        kind=\"bar\",\n",
    "        height=6,\n",
    "        aspect=0.8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In preprocessing, you have:\n",
    "* Removed irrelevant features from the data\n",
    "\n",
    "* Generated JSON Lines format files and created list of dictionary datasets for training and validation\n",
    "\n",
    "* Visually validated bounding boxes, labels, and images to ensure proper formatting\n",
    "\n",
    "* Identified imbalanced class distributions that may need augmentation or sampling to improve\n",
    "\n",
    "Overall, you have cleaned and formatted the raw data into train and validation sets, confirmed integrity via sampling, and identified potential areas for improvement via balancing. Our data is now ready for model training after these key preprocessing steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 'TFRecords' dataset processing\n",
    "When working with large datasets in TensorFlow, it is common to use 'TFRecords' format. 'TFRecords' are a simple format for storing data serialized as protobuf messages. The benefits of using 'TFRecords' include:\n",
    "* More efficient I/O performance - reading/writing protobuf messages is much faster than parsing raw images and annotations.\n",
    "* Optimization - 'TFRecord' files contain serialized data, allowing for pre-processing and data augmentation during parsing.\n",
    "* Portability - 'TFRecord' files can be used across different environments.\n",
    "* Compact - 'TFRecord' files take up less space compared to uncompressed images.\n",
    "For object detection datasets, we can store images, bounding boxes, classes, etc. in 'TFExample' protocol buffer messages in a 'TFRecord' file.\n",
    "\n",
    "'TFRecords' work nicely with TensorFlow's input pipeline for reading and parsing data efficiently during training. They can also be used with SageMaker Pipe Mode for large datasets. Pipe Mode allows us to stream data directly from Amazon S3 via 'TFRecord' files during training without needing to store the full dataset locally. This enables training on datasets that are larger than local storage capacity.\n",
    ". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Creating 'TFRecord' Files\n",
    "'TFRecord' is a simple format for storing machine learning data. It allows us to store image and annotation data together in a single file.\n",
    "\n",
    "To create 'TFRecord' files for our object detection dataset, we first need to encode the image data and bounding box information from our dataset into 'tf.train.Example' protocol buffers. Each Example 'proto' contains the following fields:\n",
    "* 'height': Image height\n",
    "* 'width': Image width\n",
    "* 'filename': Filename of the image\n",
    "* 'image': Encoded image bytes\n",
    "* 'object/bbox/xmin': Normalized left x coordinate of bounding box\n",
    "* 'object/bbox/xmax': Normalized right x coordinate of bounding box\n",
    "* 'object/bbox/ymin': Normalized top y coordinate of bounding box\n",
    "* 'object/bbox/ymax': Normalized bottom y coordinate of bounding box\n",
    "* 'object/label': Class label index\n",
    "* 'object/text': Class label text\n",
    "\n",
    "We can use the tf.io.encode_jpeg() function to encode the image bytes and the bounding box data can be obtained from the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# TFRecords helper functions\n",
    "def image_feature(value):\n",
    "    # This function takes a value (likely an image) and encodes it as a JPEG image\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    # This function takes an integer value and returns a tf.train.Feature object\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def int64_list_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def bytes_list_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def float_list_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def create_example(image, image_path, group):\n",
    "    # This function takes an image, its file path, and a group of annotations\n",
    "    # It creates a tf.train.Example object with various features extracted from the data\n",
    "\n",
    "    width = int(group[\"width\"])\n",
    "    height = int(group[\"height\"])\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "\n",
    "    # Iterate over the annotations in the group\n",
    "    for index, row in enumerate(group[\"annotation\"]):\n",
    "        # Extract bounding box coordinates and class information\n",
    "        # print(row)\n",
    "        # np.array([i])\n",
    "        xmin.append(float(row[\"bbox\"][0]))\n",
    "        ymin.append(float(row[\"bbox\"][1]))\n",
    "        xmax.append(float(row[\"bbox\"][2]))\n",
    "        ymax.append(float(row[\"bbox\"][3]))\n",
    "        classes.append(class_text_to_int(row[\"category\"]))\n",
    "        classes_text.append(row[\"category\"].encode(\"utf8\"))\n",
    "\n",
    "    # Create a tf.train.Example object with various features\n",
    "    feature = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                \"height\": int64_feature(height),\n",
    "                \"width\": int64_feature(width),\n",
    "                \"filename\": bytes_feature(group[\"image\"].encode(\"utf8\")),\n",
    "                \"image\": image_feature(image),\n",
    "                \"object/bbox/xmin\": float_list_feature(xmin),\n",
    "                \"object/bbox/xmax\": float_list_feature(xmax),\n",
    "                \"object/bbox/ymin\": float_list_feature(ymin),\n",
    "                \"object/bbox/ymax\": float_list_feature(ymax),\n",
    "                \"object/text\": bytes_list_feature(classes_text),\n",
    "                \"object/label\": int64_list_feature(classes),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have encoded the data into Examples, we can write them to a 'TFRecord' file using 'tf.io.TFRecordWriter'. Multiple Examples are serialized and written to each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creates output folder\n",
    "tfrecords_dir = \"./data/tfrecords\"\n",
    "# create TFrecord folders\n",
    "if not os.path.exists(tfrecords_dir + \"/train\"):\n",
    "    os.makedirs(tfrecords_dir + \"/train\")\n",
    "if not os.path.exists(tfrecords_dir + \"/val\"):\n",
    "    os.makedirs(tfrecords_dir + \"/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_samples = 1024  # number of samples on each TFRecord file\n",
    "num_tfrecords_train = len(train_dataset) // num_samples\n",
    "num_tfrecords_val = len(val_dataset) // num_samples\n",
    "if len(train_dataset) % num_samples:\n",
    "    num_tfrecords_train += 1  # add one record if there are any remaining samples\n",
    "if len(val_dataset) % num_samples:\n",
    "    num_tfrecords_val += 1  # add one record if there are any remaining samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the directory containing the images\n",
    "images_dir = \"data/VOCdevkit/VOC2012/JPEGImages\"\n",
    "# Loop through the specified number of TFRecord files for the training dataset\n",
    "for tfrec_num in range(num_tfrecords_train):\n",
    "    # Select a subset of samples from the training dataset based on the current TFRecord file index\n",
    "    samples = train_dataset[(tfrec_num * num_samples) : ((tfrec_num + 1) * num_samples)]\n",
    "    # Open a TFRecordWriter to create a new TFRecord file\n",
    "    with tf.io.TFRecordWriter(\n",
    "        tfrecords_dir + \"/train/file_%.2i-%i.tfrec\" % (tfrec_num, len(samples))\n",
    "    ) as writer:\n",
    "        # Iterate through the selected samples\n",
    "        for sample in samples:\n",
    "            # Construct the full path to the image file\n",
    "            image_path = f\"{images_dir}/{sample['image']}\"\n",
    "            # Read the image file and decode the JPEG data\n",
    "            image = tf.io.decode_jpeg(tf.io.read_file(image_path))\n",
    "            # Create a TFExample from the image data and sample metadata\n",
    "            example = create_example(image, image_path, sample)\n",
    "            # Write the TFExample to the TFRecord file\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through the specified number of TFRecord files for the validation dataset\n",
    "for tfrec_num in range(num_tfrecords_val):\n",
    "    samples = val_dataset[(tfrec_num * num_samples) : ((tfrec_num + 1) * num_samples)]\n",
    "    with tf.io.TFRecordWriter(\n",
    "        tfrecords_dir + \"/val/file_%.2i-%i.tfrec\" % (tfrec_num, len(samples))\n",
    "    ) as writer:\n",
    "        for sample in samples:\n",
    "            image_path = f\"{images_dir}/{sample['image']}\"\n",
    "            image = tf.io.decode_jpeg(tf.io.read_file(image_path))\n",
    "            example = create_example(image, image_path, sample)\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we can read the image and label data efficiently from the 'TFRecord' files using 'tf.data.TFRecordDataset' and parse the Examples back into tensors. This provides an optimized input pipeline for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to parse TFRecordDataset back to tensors\n",
    "def parse_tfrecord_fn(example):\n",
    "    # Define the structure of the TFRecord file\n",
    "    feature_description = {\n",
    "        \"height\": tf.io.FixedLenFeature((), tf.int64),\n",
    "        \"width\": tf.io.FixedLenFeature((), tf.int64),\n",
    "        \"filename\": tf.io.FixedLenFeature((), tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature((), tf.string),\n",
    "        \"object/bbox/xmin\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"object/bbox/xmax\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"object/bbox/ymin\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"object/bbox/ymax\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"object/text\": tf.io.VarLenFeature(tf.string),\n",
    "        \"object/label\": tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "    # Parse the example from the TFRecord file\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    # Decode the JPEG image data and convert it to a float32 tensor\n",
    "    example[\"image\"] = tf.cast(tf.io.decode_jpeg(example[\"image\"], channels=3), tf.float32)\n",
    "    # Convert the filename to a string tensor\n",
    "    example[\"filename\"] = tf.cast(example[\"filename\"], tf.string)\n",
    "    # Convert the sparse tensors to dense tensors\n",
    "    example[\"object/bbox/xmin\"] = tf.sparse.to_dense(example[\"object/bbox/xmin\"])\n",
    "    example[\"object/bbox/xmax\"] = tf.sparse.to_dense(example[\"object/bbox/xmax\"])\n",
    "    example[\"object/bbox/ymin\"] = tf.sparse.to_dense(example[\"object/bbox/ymin\"])\n",
    "    example[\"object/bbox/ymax\"] = tf.sparse.to_dense(example[\"object/bbox/ymax\"])\n",
    "    example[\"object/text\"] = tf.sparse.to_dense(example[\"object/text\"])\n",
    "    example[\"object/label\"] = tf.sparse.to_dense(example[\"object/label\"])\n",
    "\n",
    "    # Combine the bounding box coordinates into a single tensor\n",
    "    example[\"object/bbox\"] = tf.stack(\n",
    "        [\n",
    "            example[\"object/bbox/xmin\"],\n",
    "            example[\"object/bbox/ymin\"],\n",
    "            example[\"object/bbox/xmax\"],\n",
    "            example[\"object/bbox/ymax\"],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "# Helper function to prepare sample in the next format\n",
    "# {\"images\": image, \"bounding_boxes\": {\n",
    "#        \"classes\": label_list,\n",
    "#        \"boxes\": boxes_list,\n",
    "#    }\n",
    "# }\n",
    "def prepare_sample(inputs):\n",
    "    image = inputs[\"image\"]  # Get the image tensor\n",
    "    boxes = inputs[\"object/bbox\"]  # Get the bounding box tensor\n",
    "    bounding_boxes = {\n",
    "        \"classes\": inputs[\"object/label\"],  # Get the object labels\n",
    "        \"boxes\": boxes,  # Get the bounding box coordinates\n",
    "    }\n",
    "    return {\n",
    "        \"images\": image,\n",
    "        \"bounding_boxes\": bounding_boxes,\n",
    "    }  # Return the sample in the desired format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Validating the 'TFRecord' Files\n",
    "After creating the 'TFRecord' files, it's important to validate that they were created correctly before training.\n",
    "\n",
    "We can load and parse a sample of the data and visualize the bounding boxes overlaid on the images. This allows us to verify that the image data and annotations match up properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "# Function to plot bounding boxes on an image\n",
    "def plot_boxes_tfrecords(features):\n",
    "    # Open the image\n",
    "    data = features[\"images\"].numpy().astype(np.uint8)\n",
    "    image = Image.fromarray(data, \"RGB\")\n",
    "\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Iterate over the bounding boxes in the features dictionary\n",
    "    for index in range(len(features[\"bounding_boxes\"][\"boxes\"])):\n",
    "        # print(index)\n",
    "        # Extract the coordinates of the current bounding box\n",
    "        box = features[\"bounding_boxes\"][\"boxes\"][index]\n",
    "        x1, y1, x2, y2 = box\n",
    "        category = class_mapping[int(features[\"bounding_boxes\"][\"classes\"][index])]\n",
    "        draw.rectangle([(x1, y1), (x2, y2)], outline=\"red\", width=2)\n",
    "        # Draw the label\n",
    "        # text_width, text_height = draw.textsize(category)\n",
    "        # draw.rectangle([(x1, y1), (x1 + text_width + 10, y1 - text_height - 5)], fill=\"red\")\n",
    "        # draw.text((x1 + 5, y1 - text_height), category, fill=\"white\")\n",
    "\n",
    "    # Display the image\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This plots the image and overlays the bounding box coordinates loaded from the 'TFRecord' file.\n",
    "\n",
    "By visualizing several examples in this way, we can verify that the 'TFRecord' files contain the correct data before training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Loads the first TFRecord file for testing\n",
    "raw_dataset_sample = tf.data.TFRecordDataset(f\"{tfrecords_dir}/train/file_00-{num_samples}.tfrec\")\n",
    "# This line applies a function `parse_tfrecord_fn` to each example in the dataset.\n",
    "raw_dataset_sample = raw_dataset_sample.map(parse_tfrecord_fn)\n",
    "# This line applies another function `prepare_sample` to each example in the dataset.\n",
    "parsed_dataset_sample = raw_dataset_sample.map(lambda x: prepare_sample(x))\n",
    "# This line shuffles the dataset by creating a buffer of `BATCH_SIZE * 4` elements and randomly sampling from that buffer.\n",
    "sample_ds = parsed_dataset_sample.shuffle(BATCH_SIZE * 4)\n",
    "# This line takes one batch of data from the shuffled dataset.\n",
    "data = next(iter(sample_ds.take(1)))\n",
    "# visualize the sample\n",
    "plot_boxes_tfrecords(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing the Dataset with SageMaker\n",
    "\n",
    "\n",
    "One of the benefits of using SageMaker for preprocessing data is that we can leverage powerful compute instances to speed up our data preparation scripts. Although for this use case we could run the preprocessing locally, for larger datasets it is useful to execute our scripts on optimized SageMaker instances.\n",
    "\n",
    "We will use a custom script that implements the object detection data preprocessing steps discussed previously. By containerizing this script, we can execute it on SageMaker processing jobs and save the outputs to S3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'TensorFlowProcessor' is a SageMaker component that allows you to run TensorFlow scripts or Docker containers as processing jobs on SageMaker. It provides a convenient way to preprocess data, perform feature engineering, or run any other data processing tasks using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlowProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Get the current AWS region\n",
    "region = boto3.session.Session().region_name\n",
    "# Get the execution role for SageMaker\n",
    "role = get_execution_role()\n",
    "\n",
    "# Initialize the TensorFlowProcessor\n",
    "tp = TensorFlowProcessor(\n",
    "    framework_version=\"2.3\",  # TensorFlow version to use\n",
    "    role=role,  # AWS IAM role for SageMaker to access AWS resources\n",
    "    instance_type=\"ml.m5.xlarge\",  # Instance type for the processing job\n",
    "    instance_count=1,  # Number of instances to use for the processing job\n",
    "    base_job_name=\"frameworkprocessor-TF\",  # Base name for the processing job\n",
    "    py_version=\"py37\",  # Python version to use\n",
    "    sagemaker_session=sagemaker_session,  # SageMaker session object\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the name of the output file for the processing job\n",
    "processing_job_output_name = \"processing_job_output.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we will package our data preparation code into a Docker container. The script handles tasks like splitting the dataset into train and validation sets, converting labels to the required format, and encoding the image data.\n",
    "\n",
    "Next, we will configure and launch a SageMaker processing job to run this containerized script on the input dataset from S3. We can select ml.p3.2xlarge instances to parallelize and accelerate the data preprocessing.\n",
    "\n",
    "The output preprocessed object detection datasets are saved back to S3. Now we have an efficient way to preprocess large volumes of data while leveraging SageMaker's managed compute resources. Running on capable instances improves the speed of our data pipeline.\n",
    "\n",
    "This demonstrates how SageMaker processing jobs allow us to customize and automate preprocessing on scalable infrastructure for computer vision tasks like object detection.\n",
    "You can find a complete guide to the SageMaker Processing job in [this blog](https://aws.amazon.com/blogs/aws/amazon-sagemaker-processing-fully-managed-data-processing-and-model-evaluation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creates local dir\n",
    "!mkdir -p preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile preprocessing/preprocessing_dataset.py\n",
    "# Import necessary libraries\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import argparse\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import pathlib\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from sagemaker.session import Session\n",
    "from sklearn.model_selection import train_test_split\n",
    "from defusedxml.ElementTree import parse\n",
    "\n",
    "start_time = time.time()  # Record the start time for timing purposes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "# This set will store unique class names from the dataset\n",
    "class_ids = set()\n",
    "\n",
    "\n",
    "# Function to parse an XML file and return a dictionary with image and annotation details\n",
    "def xml_data_parser(xml_file):\n",
    "    # Check if the XML file exists\n",
    "    if os.path.isfile(xml_file):\n",
    "        with open(xml_file) as f:\n",
    "            # tree = ET.parse(f)\n",
    "            tree = parse(f)\n",
    "            root = tree.getroot()\n",
    "\n",
    "        annotation_dict = {}\n",
    "        filename = root.findall(\"filename\")[0].text\n",
    "        width = root.findall(\"size\")[0].find(\"width\").text\n",
    "        height = root.findall(\"size\")[0].find(\"height\").text\n",
    "        depth = root.findall(\"size\")[0].find(\"depth\").text\n",
    "        annotation_dict[\"image\"] = filename\n",
    "        annotation_dict[\"width\"] = width\n",
    "        annotation_dict[\"height\"] = height\n",
    "        annotation_dict[\"annotation\"] = []\n",
    "        for i in range(len(root.findall(\"object\"))):\n",
    "            annotation = {}\n",
    "            xmin = root.findall(\"object\")[i].find(\"bndbox\").find(\"xmin\").text\n",
    "            ymin = root.findall(\"object\")[i].find(\"bndbox\").find(\"ymin\").text\n",
    "            xmax = root.findall(\"object\")[i].find(\"bndbox\").find(\"xmax\").text\n",
    "            ymax = root.findall(\"object\")[i].find(\"bndbox\").find(\"ymax\").text\n",
    "            name = root.findall(\"object\")[i].find(\"name\").text\n",
    "            annotation[\"category\"] = name\n",
    "            annotation[\"bbox\"] = [xmin, ymin, xmax, ymax]\n",
    "            annotation_dict[\"annotation\"].append(annotation)\n",
    "            class_ids.add(name)  # Add the class name to the set\n",
    "        return annotation_dict\n",
    "    else:\n",
    "        print(f\"{xml_file} doesnt exists\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Function to parse the dataset files and return a list of dictionaries\n",
    "def parse_dataset_files(base_path, filename):\n",
    "    # Load the file and create a list of filenames\n",
    "    with open(filename, \"r\") as f:\n",
    "        dataset_lines = f.readlines()\n",
    "        dataset_files = [i.strip() for i in dataset_lines]\n",
    "\n",
    "    # Create a list of XML file paths\n",
    "    xml_files = [\n",
    "        f\"{base_path}/VOCdevkit/VOC2012/Annotations/{dataset_files[i]}.xml\"\n",
    "        for i in range(len(dataset_files))\n",
    "    ]\n",
    "    result = [xml_data_parser(xml_file) for xml_file in xml_files]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Function to dump and load a dataset from/to a JSONL file\n",
    "def dump_and_load_dataset(dataset, filename):\n",
    "    # Dumps the list of dicts to a JSONL file\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        json.dump(dataset, outfile)\n",
    "    # Loads the JSONL file to a list of dicts structure\n",
    "    with open(filename, \"r\") as f:\n",
    "        output_dataset = json.load(f)\n",
    "    return output_dataset\n",
    "\n",
    "\n",
    "# Function to map class labels to integers\n",
    "def class_text_to_int(label):\n",
    "    if label in class_mapping_by_label.keys():\n",
    "        return class_mapping_by_label[label]\n",
    "\n",
    "\n",
    "# TFRecords helper functions\n",
    "def image_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def int64_list_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def bytes_list_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def float_list_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "# Function to create a TFRecord example from a sample\n",
    "def create_example(image, image_path, group):\n",
    "    width = int(group[\"width\"])\n",
    "    height = int(group[\"height\"])\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "\n",
    "    for index, row in enumerate(group[\"annotation\"]):\n",
    "        xmin.append(float(row[\"bbox\"][0]))\n",
    "        ymin.append(float(row[\"bbox\"][1]))\n",
    "        xmax.append(float(row[\"bbox\"][2]))\n",
    "        ymax.append(float(row[\"bbox\"][3]))\n",
    "        classes.append(class_text_to_int(row[\"category\"]))\n",
    "        classes_text.append(row[\"category\"].encode(\"utf8\"))\n",
    "\n",
    "    feature = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                \"height\": int64_feature(height),\n",
    "                \"width\": int64_feature(width),\n",
    "                \"filename\": bytes_feature(group[\"image\"].encode(\"utf8\")),\n",
    "                \"image\": image_feature(image),\n",
    "                \"object/bbox/xmin\": float_list_feature(xmin),\n",
    "                \"object/bbox/xmax\": float_list_feature(xmax),\n",
    "                \"object/bbox/ymin\": float_list_feature(ymin),\n",
    "                \"object/bbox/ymax\": float_list_feature(ymax),\n",
    "                \"object/text\": bytes_list_feature(classes_text),\n",
    "                \"object/label\": int64_list_feature(classes),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "# Helper function to parse TFRecordDataset back to tensors\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        \"height\": tf.io.FixedLenFeature((), tf.int64),\n",
    "        \"width\": tf.io.FixedLenFeature((), tf.int64),\n",
    "        \"filename\": tf.io.FixedLenFeature((), tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature((), tf.string),\n",
    "        \"object/bbox/xmin\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"object/bbox/xmax\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"object/bbox/ymin\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"object/bbox/ymax\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"object/text\": tf.io.VarLenFeature(tf.string),\n",
    "        \"object/label\": tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "    # Parse the single example\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    # Preprocess the image and bounding box data\n",
    "    example[\"image\"] = tf.cast(tf.io.decode_jpeg(example[\"image\"], channels=3), tf.float32)\n",
    "    example[\"filename\"] = tf.cast(example[\"filename\"], tf.string)\n",
    "    example[\"object/bbox/xmin\"] = tf.sparse.to_dense(example[\"object/bbox/xmin\"])\n",
    "    example[\"object/bbox/xmax\"] = tf.sparse.to_dense(example[\"object/bbox/xmax\"])\n",
    "    example[\"object/bbox/ymin\"] = tf.sparse.to_dense(example[\"object/bbox/ymin\"])\n",
    "    example[\"object/bbox/ymax\"] = tf.sparse.to_dense(example[\"object/bbox/ymax\"])\n",
    "    example[\"object/text\"] = tf.sparse.to_dense(example[\"object/text\"])\n",
    "    example[\"object/label\"] = tf.sparse.to_dense(example[\"object/label\"])\n",
    "    # Combine the bounding box coordinates into a single tensor\n",
    "    example[\"object/bbox\"] = tf.stack(\n",
    "        [\n",
    "            example[\"object/bbox/xmin\"],\n",
    "            example[\"object/bbox/ymin\"],\n",
    "            example[\"object/bbox/xmax\"],\n",
    "            example[\"object/bbox/ymax\"],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "# Helper function to prepare sample in the expected format\n",
    "def prepare_sample(inputs):\n",
    "    image = inputs[\"image\"]\n",
    "    boxes = inputs[\"object/bbox\"]\n",
    "    bounding_boxes = {\n",
    "        \"classes\": inputs[\"object/label\"],\n",
    "        \"boxes\": boxes,\n",
    "    }\n",
    "    return {\"images\": image, \"bounding_boxes\": bounding_boxes}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args, _ = parser.parse_known_args()\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    pathlib.Path(f\"{base_dir}/input\").mkdir(parents=True, exist_ok=True)\n",
    "    output_dir = base_dir + \"/output\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger.info(\"downloading file\")\n",
    "\n",
    "    input_path = f\"{base_dir}/input/\"\n",
    "\n",
    "    # untar the partitioned data files into the data folder\n",
    "    logger.info(\"extracting files\")\n",
    "    subprocess.run([\"tar\", \"-xf\", f\"{input_path}/VOCtrainval_11-May-2012.tar\", \"-C\", input_path])\n",
    "\n",
    "    # Creation of datasets (list of dicts)\n",
    "    logger.info(\"Creating Json files\")\n",
    "    train_dataset = parse_dataset_files(\n",
    "        input_path, f\"{input_path}/VOCdevkit/VOC2012/ImageSets/Main/train.txt\"\n",
    "    )\n",
    "    val_dataset = parse_dataset_files(\n",
    "        input_path, f\"{input_path}/VOCdevkit/VOC2012/ImageSets/Main/val.txt\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{len(train_dataset)} samples for training and {len(val_dataset)} samples for validation before loading\"\n",
    "    )\n",
    "\n",
    "    # Dump of dicts to jsonl files and load for testing purposes\n",
    "    train_filename = f\"{output_dir}/train_labels_VOC.jsonl\"\n",
    "    val_filename = f\"{output_dir}/val_labels_VOC.jsonl\"\n",
    "    train_dataset = dump_and_load_dataset(train_dataset, train_filename)\n",
    "    # val_dataset=dump_and_load_dataset(val_dataset,val_filename)\n",
    "\n",
    "    # Split the train dataset into train and validation sets\n",
    "    training_dataset, val_dataset = train_test_split(train_dataset, test_size=0.33, random_state=42)\n",
    "\n",
    "    print(\n",
    "        f\"{len(train_dataset)} samples for training and {len(val_dataset)} samples for validation after loading\"\n",
    "    )\n",
    "\n",
    "    # creation of a dict to map classes with class ids\n",
    "    class_ids = sorted(list(class_ids))\n",
    "    class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "    class_mapping_by_label = dict(zip(class_ids, range(len(class_ids))))\n",
    "\n",
    "    logger.info(\"Creating TFRecords\")\n",
    "    # creates output folders\n",
    "    tfrecords_dir = f\"{output_dir}/tfrecords\"\n",
    "    if not os.path.exists(tfrecords_dir + \"/train\"):\n",
    "        os.makedirs(tfrecords_dir + \"/train\")\n",
    "    if not os.path.exists(tfrecords_dir + \"/val\"):\n",
    "        os.makedirs(tfrecords_dir + \"/val\")\n",
    "\n",
    "    num_samples = 1024  # number of samples on each TFRecord file\n",
    "    num_tfrecords_train = len(train_dataset) // num_samples\n",
    "    num_tfrecords_val = len(val_dataset) // num_samples\n",
    "    if len(train_dataset) % num_samples:\n",
    "        num_tfrecords_train += 1  # add one record if there are any remaining samples\n",
    "    if len(val_dataset) % num_samples:\n",
    "        num_tfrecords_val += 1  # add one record if there are any remaining samples\n",
    "\n",
    "    images_dir = f\"{input_path}/VOCdevkit/VOC2012/JPEGImages\"\n",
    "    logger.info(\"Creating Training Records\")\n",
    "    for tfrec_num in range(num_tfrecords_train):\n",
    "        samples = train_dataset[(tfrec_num * num_samples) : ((tfrec_num + 1) * num_samples)]\n",
    "\n",
    "        with tf.io.TFRecordWriter(\n",
    "            tfrecords_dir + \"/train/file_%.2i-%i.tfrec\" % (tfrec_num, len(samples))\n",
    "        ) as writer:\n",
    "            for sample in samples:\n",
    "                image_path = f\"{images_dir}/{sample['image']}\"\n",
    "                image = tf.io.decode_jpeg(tf.io.read_file(image_path))\n",
    "                example = create_example(image, image_path, sample)\n",
    "                writer.write(example.SerializeToString())\n",
    "    logger.info(\"Creating validation records\")\n",
    "    for tfrec_num in range(num_tfrecords_val):\n",
    "        samples = val_dataset[(tfrec_num * num_samples) : ((tfrec_num + 1) * num_samples)]\n",
    "\n",
    "        with tf.io.TFRecordWriter(\n",
    "            tfrecords_dir + \"/val/file_%.2i-%i.tfrec\" % (tfrec_num, len(samples))\n",
    "        ) as writer:\n",
    "            for sample in samples:\n",
    "                image_path = f\"{images_dir}/{sample['image']}\"\n",
    "                image = tf.io.decode_jpeg(tf.io.read_file(image_path))\n",
    "                example = create_example(image, image_path, sample)\n",
    "                writer.write(example.SerializeToString())\n",
    "    logger.info(f\"Processed data save on {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_path = processing_output_filename\n",
    "s3_dataset_uri = S3_dataset_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Define the S3 path where the output of the processing job will be stored\n",
    "processing_job_output_path = f\"s3://{bucket}/{prefix}/data/processing/output\"\n",
    "# Run the processing job\n",
    "tp.run(\n",
    "    # Specify the Python script to run for preprocessing\n",
    "    code=\"preprocessing_dataset.py\",\n",
    "    # Specify the Python script folder for preprocessing\n",
    "    source_dir=\"preprocessing\",\n",
    "    # Define the input data for the processing job\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=s3_dataset_uri,  # The S3 URI of the input data\n",
    "            destination=\"/opt/ml/processing/input\",  # The local path where the input data will be copied\n",
    "        )\n",
    "    ],\n",
    "    # Define the output location for the processed data\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"processed_data\",  # A name for the output\n",
    "            source=\"/opt/ml/processing/output\",  # The local path where the processed data will be saved\n",
    "            destination=processing_job_output_path,  # The S3 path where the processed data will be uploaded\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Describe the processing job to get details about its status, output, etc.\n",
    "preprocessing_job_description = tp.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessing_job_description[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Congratulations! You have preprocessed the data. Now you can find the processed data at the S3 URI from the preprocessing job outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Accessing Processing Job Results\n",
    "\n",
    "Once a SageMaker processing job has finished running, the results including any output artifacts are stored in an S3 location specified when the job was created. To determine where these results are located, we can view the 'ProcessingOutputConfig' field returned when describing the processing job: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Find the output of Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "processing_job_output_uri = preprocessing_job_description[\"ProcessingOutputConfig\"][\"Outputs\"][0][\n",
    "    \"S3Output\"\n",
    "][\"S3Uri\"]\n",
    "processing_job_output_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the S3 bucket string from the preprocessing step\n",
    "# This value will be available for use in a second Jupyter notebook\n",
    "# It allows you to access the output from the preprocessing job\n",
    "# without having to hardcode the S3 path again\n",
    "%store processing_job_output_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/introduction_to_applying_machine_learning|object_detection_with_tensorflow_and_tfrecords|1_object_detection_preprocessing.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "TensorFlow 2.10.0 Python 3.9 CPU Optimized",
   "language": "python",
   "name": "TensorFlow 2.10.0 Python 3.9 CPU Optimized"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
