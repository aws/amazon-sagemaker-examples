{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Experiments applied to NYC TLC trip data\n",
    "\n",
    "In this example, we will do SageMaker Experiments to train a model using  [Amazon SageMaker XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to predict NYC TLC trip duration in minutes based on feature vector that includes ride pickup source zone, destination zone, month, day of the week and hour of the day for the ride. \n",
    "\n",
    "The data used to train the XGBoost model was prepared using PySpark cluster as described in this [companion notebook](emr-pyspark-nyc-tlc.ipynb) and saved in S3 bucket. So, if you have not executed the companion notebook, please do that before executing this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Amazon SageMaker Execution Role\n",
    "\n",
    "This notebook requires a Python 3 kernel with SageMaker support, for example `conda_python3` kernel.\n",
    "\n",
    "The first step in using Amazon SageMaker is to create an execution role that encapsulates permissions used by Amazon SageMaker to access other AWS services, so we do that as the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "# Create SageMaker role \n",
    "role = get_execution_role()\n",
    "print(f'Amazon SageMaker execution role: {role}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get XGBoost Amazon ECR URI\n",
    "Every Amazon SageMaker model training requires an [Amazon ECR](https://aws.amazon.com/ecr/) image that provides an [Amazon SageMaker training container](https://docs.aws.amazon.com/sagemaker/latest/dg/amazon-sagemaker-containers.html). So, next, we specify the URI for the Amazon ECR docker image for Amazon SageMaker XGBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the url to the container image for using linear-learner\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "xgboost_image = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')\n",
    "print(f'Amazon SageMaker XGBoost Algorithm ECR Image URI: {xgboost_image}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training data into training, validation and test\n",
    "\n",
    "At the end of the data preparation step executed in [companion notebook](emr-pyspark-nyc-tlc.ipynb), we get multiple CSV files saved in your S3 bucket. Each CSV file contains data with columns for trip duration in minutes, origin zone, destination zone, and pickup month, ordinal day of the week, and hour of day. \n",
    "\n",
    "Specify `source_bucket` S3 bucket where you saved the prepared data in the [companion notebook](emr-pyspark-nyc-tlc.ipynb). Specify `dest_bucket` S3 bucket where you want to load the split training, validaiton and test datasets. The `source_bucket` and `dest_bucket` can be the same S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source bucket with CSV files produced by Amazon EMR spar cluster data preparation\n",
    "source_bucket=\n",
    "source_prefix='emr/output/uber_nyc/v1'\n",
    "\n",
    "# destination bucket to upload SageMaker training input data files\n",
    "# can be the same as source_bucket\n",
    "dest_bucket=source_bucket\n",
    "dest_prefix = 'sagemaker/input/uber_nyc/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split input data into <b>training, validation and test</b> datasets for SageMaker XGBoost model training. We split the data by downloading the input CSV data files from `source_bucket` and separating the downloaded files into three folders. We reserve 80% of the downloaded files for training, 15% for validation and 5% for test, and upload the separated data files using `training`, `validation` and `test` prefixes into the `dest_bucket` S3 bucket to stage the data for SageMaker XGBoost model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import csv\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response=s3.list_objects_v2(Bucket=source_bucket, Prefix=source_prefix)\n",
    "contents=response['Contents']\n",
    "count=len(contents)\n",
    "\n",
    "sbucket = boto3.resource('s3').Bucket(source_bucket)\n",
    "ntrain=int(count*0.80)\n",
    "nval = int(count*0.15)\n",
    "ntest = count - ntrain - nval\n",
    "\n",
    "def stage_data(start, end, name):  \n",
    "    for i in range(start, end, 1):\n",
    "        item=contents[i]\n",
    "        key =item['Key']    \n",
    "        if not key.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        with tempfile.NamedTemporaryFile(mode='w+b', suffix='.csv', prefix='data-', delete=True) as csv_file:\n",
    "            print(f'Download {key} file')\n",
    "            sbucket.download_fileobj(key, csv_file)\n",
    "            \n",
    "            with open(csv_file.name, 'rb') as data_reader:\n",
    "                dest_key = f'{dest_prefix}/{name}/part-{i}.csv'\n",
    "                print(f'upload {dest_key} file')\n",
    "                s3.upload_fileobj(data_reader, dest_bucket, dest_key)\n",
    "                data_reader.close()\n",
    "        \n",
    "            csv_file.close()\n",
    "            \n",
    "stage_data(0, ntrain, 'train')\n",
    "stage_data(ntrain, ntrain+nval, 'validation')\n",
    "stage_data(ntrain+nval, count, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Amazon SageMaker data input channels\n",
    "\n",
    "Next, we  define <b>train and validation</b> input channels for Amazon SageMaker XGBoost model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import s3_input\n",
    "\n",
    "s3_train = s3_input(s3_data=f's3://{dest_bucket}/{dest_prefix}/train', content_type='csv')\n",
    "s3_validation = s3_input(s3_data=f's3://{dest_bucket}/{dest_prefix}/validation', content_type='csv')\n",
    "\n",
    "output_path=f's3://{dest_bucket}/sagemaker/output/uber_nyc/xgboost'\n",
    "data_channels = {'train': s3_train, 'validation': s3_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Amazon SageMaker Session\n",
    "\n",
    "Next, we create Amazon SageMaker Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import Session\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Experiment\n",
    "\n",
    "To define SageMaker Experiment, we first install `sagemaker-experiments` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the SageMaker Experiment modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a Tracker for tracking input data used in the SageMaker Trials in this Experiment. Specify the S3 URL of your dataset in the value below and change the name of the dataset if you are using a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    # we can log the s3 uri to the dataset used for training\n",
    "    tracker.log_input(name=\"nyc-tlc-dataset\", \n",
    "                      media_type=\"s3/uri\", \n",
    "                      value= \"s3://aws-ajayvohra-ml-data/emr/output/uber_nyc/v1\" # specify S3 URL to your dataset\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a SageMaker Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_experiment = Experiment.create(\n",
    "    experiment_name=f\"xgb-nyc-tlc-experiment-{int(time.time())}\", \n",
    "   description=\"XGBoost NYC TLC experiment\", \n",
    "   sagemaker_boto_client=sm)\n",
    "print(xgb_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training Job ###\n",
    "After specifying best values for `num_round` and `max_depth` hyper-parameters, we are ready to start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_params = [ (110, 11), (100, 10), (90, 9), (80, 8)]\n",
    "sagemaker_session=Session(sess, sm)\n",
    "for num_round, max_depth in trial_params:\n",
    "    \n",
    "    trial_name = f\"xgb-nyc-tlc-{int(time.time())}\"\n",
    "    xgb_trial = Trial.create(\n",
    "                        trial_name=trial_name, \n",
    "                        experiment_name=xgb_experiment.experiment_name,\n",
    "                        sagemaker_boto_client=sm,\n",
    "    )\n",
    "    \n",
    "    # associate the proprocessing trial component with the current trial\n",
    "    xgb_trial.add_trial_component(tracker.trial_component)\n",
    "    print(xgb_trial)\n",
    "\n",
    "    xgb = Estimator(image_name=xgboost_image,\n",
    "                            role=role, \n",
    "                            train_instance_count=2, \n",
    "                            train_instance_type='ml.c5.4xlarge',\n",
    "                            output_path=output_path,\n",
    "                            sagemaker_session=sagemaker_session)\n",
    "    \n",
    "    xgb.set_hyperparameters(objective='reg:linear',\n",
    "                        grow_policy='depthwise',\n",
    "                        tree_method = 'approx',\n",
    "                        num_round=num_round, \n",
    "                        max_depth=max_depth)\n",
    "    \n",
    "    \n",
    "    xgb.fit(inputs=data_channels, \n",
    "                        logs=True,  \n",
    "                        experiment_config={\"TrialName\": xgb_trial.trial_name, \n",
    "                                           \"TrialComponentDisplayName\": \"Training\"},\n",
    "                        wait=False)\n",
    "    print(xgb)\n",
    "\n",
    "    # sleep in between starting two trials\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the model training runs for an experiment\n",
    "\n",
    "Now we will use the analytics capabilities of Python SDK to query and compare the training runs for identifying the best model produced by our experiment. You can retrieve trial components by using a search expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression = {\n",
    "    \"Filters\":[\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    experiment_name=xgb_experiment.experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.validation:rmse.min\",\n",
    "    sort_order=\"Ascending\",\n",
    "    metric_names=['validation:accuracy', 'train:rmse'],\n",
    "    parameter_names=['max_depth', 'num_round']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic_table = trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
