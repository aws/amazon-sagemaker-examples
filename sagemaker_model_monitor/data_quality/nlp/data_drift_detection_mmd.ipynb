{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Data Drift in NLP using Maximum Mean Discrepancy\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Detecting data drift in NLP is a challenging task due to complex nature of human language. NLP deep learning models are trained on certain corpus of text data, data drift occurs when distribution changes between the training and inference data. Model monitoring is an important aspect in production NLP applications, any change in data distribution(drift) during inference can cause model performance degradation(model decay). Please refer this [blog](https://aws.amazon.com/blogs/machine-learning/detect-nlp-data-drift-using-custom-amazon-sagemaker-model-monitor/) for more details on NLP data drift.\n",
    "\n",
    "In this notebook, we will implement a data drift detection technique using [Maximum Mean Discrepancy](https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions) (MMD) distance measure. MMD is a kernel based two-sample testing method to measure distance between distributions. We will compare MMD distance between training and inference sentence embeddings to determine if there is a data drift using Amazon SageMaker custom model monitoring. You can establish a custom baseline such as sentence embeddings and evaluate inference data points against it for any potential data drift. \n",
    "\n",
    "## Solution Architecture\n",
    "\n",
    "In this notebook, we will implement an end to end solution to detect data drift in NLP using Amazon SageMaker model monitoring feature. Below are the highlevel steps involved in this architecture\n",
    "\n",
    "1. Fine tune `distilroberta-base` model using HuggingFace Estimators\n",
    "2. Deploy the model to SageMaker real time endpoint\n",
    "3. Create a baseline using `sentence-transformer` sentence embeddings\n",
    "4. Detect data drift using Maximum Mean Discrepancy technique\n",
    "5. Define a model monitoring schedule and inspect violation reports\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nlp-data-drift-mmd.png\" alt=\"nlp-data-drfit\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "To start, we install and upgrade required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install torch --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install \"sagemaker>=2.48.0\" --upgrade --quiet\n",
    "!pip install -U sentence-transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "#sagemaker\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "import time\n",
    "from sagemaker import TrainingJobAnalytics\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import DataCaptureConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sagemaker_session_bucket=None\n",
    "current_timestamp = strftime('%m-%d-%H-%M', gmtime())\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "region = sess.boto_region_name\n",
    "\n",
    "local_train_dataset = \"train.json\"\n",
    "local_test_dataset = \"test.json\"\n",
    "remote_train_dataset = f\"s3://{sagemaker_session_bucket}/nlp-data-drift-mmd/data\"\n",
    "remote_test_dataset = f\"s3://{sagemaker_session_bucket}/nlp-data-drift-mmd/data\"\n",
    "base_name = \"nlp-data-drift-mmd\"\n",
    "report_file_name = \"constraints_violations.json\"\n",
    "\n",
    "train_job_name = f'{base_name}-{current_timestamp}'\n",
    "model_monitor_job_name = f'{base_name}-{current_timestamp}'\n",
    "endpoint_name = f'{base_name}-{current_timestamp}'\n",
    "monitor_schedule_name = f'{base_name}-{current_timestamp}'\n",
    "processing_job_name = f'{base_name}-{current_timestamp}'\n",
    "\n",
    "prefix = \"sagemaker/nlp-data-drift-mmd\"\n",
    "data_capture_prefix = f\"{prefix}/data_capture\"\n",
    "s3_capture_upload_path = f\"s3://{sagemaker_session_bucket}/{data_capture_prefix}\"\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session_bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use Corpus of Linguistic Acceptability (CoLA) dataset (https://nyu-mll.github.io/CoLA/), a dataset of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. In the script below, we do\n",
    "\n",
    "1. Download the data as a zip file\n",
    "2. Extract content to a folder\n",
    "3. Read `sentence` and `label` columns from a tsv file\n",
    "4. Split dataset into train and test files\n",
    "5. Store the data in JSON format, each data point will be in a seperate line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pygmentize ./scripts/create_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the script to create training, testing data in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!mkdir -p ./data/\n",
    "!python ./scripts/create_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload train and test files to Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# upload datasets\n",
    "S3Uploader.upload(os.path.join('./data', local_train_dataset),remote_train_dataset)\n",
    "S3Uploader.upload(os.path.join('./data',local_test_dataset),remote_test_dataset)\n",
    "\n",
    "print(f\"train dataset uploaded to: {remote_train_dataset}/{local_train_dataset}\")\n",
    "print(f\"test dataset uploaded to: {remote_test_dataset}/{local_test_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning distilroberta-base model using HuggingFace Estimators\n",
    "\n",
    "In order to create our sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles all end-to-end Amazon SageMaker training and deployment tasks. In the Estimator we define, fine-tuning script (`entry_point`), `instance_type` to launch training job, `hyperparameters` with model name, epoch and training batch size\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.12',\n",
    "                            pytorch_version='1.9',\n",
    "                            py_version='py38',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilroberta-base'\n",
    "                                               \n",
    "                                                })\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the entry point training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pygmentize ./scripts/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set below hyperparameters to be passed to the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={'epochs': 1,                          # number of training epochs\n",
    "                 'train_batch_size': 32,               # batch size for training\n",
    "                 'eval_batch_size': 64,                # batch size for evaluation\n",
    "                 'learning_rate': 3e-5,                # learning rate used during training\n",
    "                 'model_id':'distilroberta-base', # pre-trained model\n",
    "                 'fp16': True,                         # Whether to use 16-bit (mixed) precision training\n",
    "                 'train_file': local_train_dataset,    # training dataset\n",
    "                 'test_file': local_test_dataset,      # test dataset\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define metrics for training job by specifying a name and a regular expression for each metric that the training job monitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "    {'Name': 'eval_loss',               'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_accuracy',           'Regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_f1',                 'Regex': \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_precision',          'Regex': \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below estimator runs a Hugging Face training script in a SageMaker training environment. The estimator initiates the SageMaker-managed Hugging Face environment by using the pre-built Hugging Face Docker container and runs the Hugging Face training script that user provides through the entry_point argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',        # fine-tuning script used in training jon\n",
    "    source_dir           = './scripts',       # directory where fine-tuning script is stored\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = train_job_name,    # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version = '4.12',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.9',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py38',            # the python version used in the training job\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    "    metric_definitions   = metric_definitions,# the metrics regex definitions to extract logs\n",
    "    disable_profiler     = True               # disable sagemaker debugger profiler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After configuring the estimator class, use the class method `fit()` to start a training job. This step takes around 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {\n",
    "    'train': remote_train_dataset,\n",
    "    'test': remote_test_dataset\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(training_data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch training metrics data from CloudWatch Metrics for a specific training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = huggingface_estimator.latest_training_job.name\n",
    "print(f\"Training jobname: {training_job_name}\")\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture real-time inference data from Amazon SageMaker endpoints. To enable data capture for monitoring the model data quality, specify the new capture option called `DataCaptureConfig` when deploying to an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the trained model to an Amazon SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\", endpoint_name=endpoint_name, data_capture_config=data_capture_config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name=endpoint_name, \n",
    "                                                   sagemaker_session=sess,\n",
    "                                                   serializer=JSONSerializer(),\n",
    "                                                   deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy trained model to SageMaker real-time endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deploy()` method returns a predictor that provides `predict()` which can be used to send requests to the Amazon SageMaker endpoint and obtain inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read validation data, sample random 10 records and send it to the precitor\n",
    "df_validation = pd.read_json(\"./data/validation.json\", lines = True)\n",
    "\n",
    "sentiment_input = {}\n",
    "for row in df_validation.sample(n=10).iterrows():\n",
    "    val_data = row[1][1]\n",
    "    sentiment_input[\"inputs\"] = val_data\n",
    "    pred_resp = predictor.predict(sentiment_input)\n",
    "    print(f\"Model prediction : {pred_resp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Captured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View captured data by listing the data capture files stored in Amazon S3. Expect to see different files from different time periods, organized based on the hour when the invocation occurred. Please note it may take few minutes for the data cpature files to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.Session().client('s3')\n",
    "time.sleep(120)\n",
    "\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=sagemaker_session_bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=sagemaker_session_bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the captured inoput data format,  model monitoring schedule evaluates the captured data in string format. Preprocessing logic is included in the `evalaution.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish Sentence Embedding Baseline\n",
    "\n",
    "In this section, we will establish a baseline from the training data using `SentenceTransformers`. [SentenceTransformers](https://www.sbert.net/) is a Python framework for state-of-the-art sentence, text and image embeddings.\n",
    "\n",
    "We create a SentenceTransformer pretrained model `all-distilroberta-v1`, that can be used to map sentences / text to embeddings. Transformer model's (BERT/ RoBERTa etc) runtime and the memory requirement grows quadratic with the input length. So we limit input text length to 128 tokens, longer inputs will be truncated. Compute sentence embeddings with `encode()` method and append the sentence embedding to a list. We will represent this baseline as as numpy object and use it during model monitoring schedule execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-distilroberta-v1')\n",
    "model.max_seq_length = 128\n",
    "print(f\"Max Sequence Length: {model.max_seq_length}\")\n",
    "\n",
    "df_sentences = pd.read_json('./data/train.json', lines=True)\n",
    "sentences = df_sentences['sentence'].values\n",
    "\n",
    "sentence_embeddings = []\n",
    "counter = 0\n",
    "for sent in sentences:\n",
    "    print(f\"Encoding sentence #: {counter}\")\n",
    "    sentence_embedding = model.encode(sent)\n",
    "    sentence_embeddings.append(sentence_embedding[None])\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `sentence_embeddings_list` list as a numpy file that will be uploaded to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.save('./data/embeddings.npy', sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the .npy baseline data to Amazon S3, we will read this in the evaluation script in the later section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!aws s3 cp ./data/embeddings.npy s3://{sagemaker_session_bucket}/{prefix}/baseline/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Mean Discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical estimation of MMD: We represent emperical estimation of MMD using following formula:\n",
    "\n",
    "\n",
    "$$ MMD^{2}(X,Y) = \\underbrace{\\frac{1}{m (m-1)} \\sum_{i} \\sum_{j \\neq i} k(\\mathbf{x_{i}}, \\mathbf{x_{j}})}_\\text{A} - \\underbrace{2 \\frac{1}{m.m} \\sum_{i} \\sum_{j} k(\\mathbf{x_{i}}, \\mathbf{y_{j}})}_\\text{B} + \\underbrace{\\frac{1}{m (m-1)} \\sum_{i} \\sum_{j \\neq i} k(\\mathbf{y_{i}}, \\mathbf{y_{j}})}_\\text{C} \\tag{1} $$\n",
    "\n",
    "$\\mathbf{x_{i}}$'s are the data points in first distribution and $\\mathbf{y_{i}}$'s are the data points in second distribution, so that MMD score guides us towards the evaluation of underlying distributions. In the `evaluation.py` file, we will demonstrate how to implement above equation in pytorch and use the MMD as a distance measure to ensure inference distributions doesn't diverge from the training data.\n",
    "\n",
    "Given $X,Y$ maximum mean discrepancy is the distance between feature means of $X,Y$:\n",
    "\n",
    "$$ MMD^{2}(X,Y) = \\Vert \\mu_{X} - \\mu_{Y} \\Vert^{2} _{\\mathcal{F}} \\tag{2} $$ \n",
    "First, we obtain the similarity matrices between  $X$ and $X$, $X$ and $Y$, finally $Y$ and $Y$ with given distance metric, then plugging the results to kernel specific function such as exponential. \n",
    "\n",
    "For example, let's say kernel in question is gaussian meaning \n",
    "\n",
    "$$ k(\\mathbf{x_{i}}, \\mathbf{x_{j}}) = \\exp \\left(\\frac{- \\Vert \\mathbf{x_{i}} - \\mathbf{x_{j}} \\Vert^{2}}{2\\sigma^{2}}\\right) = \\exp \\left(\\frac{-1}{\\sigma^{2}} [\\mathbf{x_{i}}^\\intercal \\mathbf{x_{i}} - 2 \\mathbf{x_{i}}^\\intercal \\mathbf{x_{j}} + \\mathbf{x_{j}}^\\intercal \\mathbf{x_{j}}]\\right) $$\n",
    "\n",
    "If we can construct a matrix with elements such that for every i and j corresponding element is $[\\mathbf{x_{i}}^\\intercal \\mathbf{x_{i}} - 2 \\mathbf{x_{i}}^\\intercal \\mathbf{x_{j}} + \\mathbf{x_{j}}^\\intercal \\mathbf{x_{j}}]$, then it is possible to just plug that matrix into `pytorch.exp()` for the result.\n",
    "\n",
    "We will implement MMD logic in model monitor evaluation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pygmentize docker/evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Model monitor evaluation Script\n",
    "\n",
    "Amazon SageMaker Model Monitor provides a prebuilt container with ability to analyze the data captured from endpoints for tabular datasets. If you would like to bring your own container, Model Monitor provides extension points which you can leverage. \n",
    "\n",
    "Under the hood, when you create a MonitoringSchedule, Model Monitor ultimately kicks off processing jobs. Hence the container needs to be aware of the processing job contract \n",
    "\n",
    "We need to create an evaluation script that is compatible with container contract inputs and outputs\n",
    "\n",
    "[Container Contract Inputs](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-contract-inputs.html)\n",
    "\n",
    "[Container Contract Outputs](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-contract-outputs.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Push Image to ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below script shows how to build the Docker image and push it to ECR to be ready for use by SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ecr_repository = f'{base_name}-{current_timestamp}'\n",
    "tag = ':latest'\n",
    "\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "processing_repository_uri = f'{account_id}.dkr.ecr.{region}.{uri_suffix}/{ecr_repository + tag}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the ECR repository and pushing the container image\n",
    "\n",
    "# SageMaker Classic Notebook Instance:\n",
    "!docker build -t $ecr_repository docker\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri\n",
    "\n",
    "# SageMaker Studio:\n",
    "# !cd docker && sm-docker build . --repository $ecr_repository$tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model Monitor for detetcing data drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModelMonitor class assist in creating monitoring schedules for data captured by SageMaker Endpoints. Use this class  to provide your own container image containing the code to evaluate the captured data and detect and potential drift\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Note </strong>\n",
    "The threshold value for average MMD could vary depending on the data and use case. You can run experiments with MMD distance measure and specify a threshold value in the ModelMonitor that fits your use case\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import ModelMonitor\n",
    "\n",
    "monitor = ModelMonitor(\n",
    "    base_job_name=model_monitor_job_name,\n",
    "    role=role,\n",
    "    image_uri=processing_repository_uri,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    env={ 'THRESHOLD':'6.0', 'bucket': sagemaker_session_bucket },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a monitoring schedule to monitor an Amazon SageMaker Endpoint. We have established a sentence embedding baseline and stored in S3 as a numpy file. During model monitoring schedule execution, S3 file will be downloaded and MMD distance metrics will be calculated for every data point captured part of data capture config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator, MonitoringOutput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "destination = f's3://{sagemaker_session_bucket}/{prefix}/{endpoint_name}/monitoring_schedule'\n",
    "\n",
    "processing_output = ProcessingOutput(\n",
    "    output_name='result',\n",
    "    source='/opt/ml/processing/resultdata',\n",
    "    destination=destination,\n",
    "    s3_upload_mode=\"EndOfJob\"\n",
    ")\n",
    "output = MonitoringOutput(source=processing_output.source, destination=processing_output.destination, s3_upload_mode=\"EndOfJob\")\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=monitor_schedule_name,\n",
    "    output=output,\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe and inspect the schedule, note that the MonitoringScheduleStatus changes from 'Pending' to 'Scheduled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Even for an hourly schedule, Amazon SageMaker has a buffer period of 20 minutes to schedule your execution. You might see your execution start in anywhere from zero to ~20 minutes from the hour boundary. This is expected and done for load balancing on the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mon_executions = monitor.list_executions()\n",
    "print(\n",
    "    \"We created a hourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\\nWe will have to wait till we hit the hour...\"\n",
    ")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = monitor.list_executions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the latest monitring schedule execution and check the `ProcessingJobStatus` of the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = mon_executions[\n",
    "    -1\n",
    "]  # latest execution's index is -1, second to last is -2 and so on..\n",
    "time.sleep(60)\n",
    "latest_execution.wait(logs=False)\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()[\"ProcessingJobStatus\"]))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()[\"ExitMessage\"]))\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if latest_job[\"ProcessingJobStatus\"] != \"Completed\":\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model monitoring schedule excution will create violation reports if there any data points that violates MMD threshold indicating a data drift. Reports are uploaded to S3 location below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_uri = latest_execution.output.destination\n",
    "print(\"Report Uri: {}\".format(report_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip(\"/\")\n",
    "print(\"Report bucket: {}\".format(report_bucket))\n",
    "print(\"Report key: {}\".format(report_key))\n",
    "\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the violations report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s3.download_file(report_bucket, report_key+'/'+report_file_name, report_file_name)\n",
    "    with open(report_file_name, 'r') as handle:\n",
    "        parsed = json.load(handle)\n",
    "    \n",
    "    print(json.dumps(parsed, indent=4, sort_keys=True))\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually execute the processing job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test SageMaker model monitoring schedule execution manually by launching a processing job with the ECR image, this can help with reducing test cycles and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "\n",
    "processor = Processor(\n",
    "    base_job_name=processing_job_name,\n",
    "    role=role,\n",
    "    image_uri=processing_repository_uri,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    env={ 'THRESHOLD':'6.0','bucket': sagemaker_session_bucket },\n",
    ")\n",
    "    \n",
    "processor.run(\n",
    "    [ProcessingInput(\n",
    "        input_name='endpointdata',\n",
    "        source = \"s3://{}/{}/{}\".format(sagemaker_session_bucket, data_capture_prefix,endpoint_name),\n",
    "        #source=f's3://{sagemaker_session.default_bucket()}/{s3_prefix}/endpoint/data_capture',\n",
    "        destination = '/opt/ml/processing/input/endpoint',\n",
    "    )],\n",
    "    [ProcessingOutput(\n",
    "        output_name='result',\n",
    "        source='/opt/ml/processing/resultdata',\n",
    "        destination=destination,\n",
    "    )],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Lastly, please remember to delete the monitoring schedule and Amazon SageMaker endpoint to avoid charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Delete the monitoring schedule\n",
    "monitor.delete_monitoring_schedule()\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Delete endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we discussed how to leverage Maximum mean discrepacy distance measure to detect data drift in NLP applications using Amazon SageMaker custom model monitoring. You can use this pattern with any other distance measure such as cosine distance. Give this a try !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
