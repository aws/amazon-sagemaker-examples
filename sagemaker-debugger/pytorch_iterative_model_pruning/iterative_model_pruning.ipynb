{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SageMaker Debugger and SageMaker Experiments for iterative model pruning\n",
    "\n",
    "This notebook demonstrates how we can use SageMaker Debugger and SageMaker Experiments to perform iterative model pruning. Let's start first with a quick introduction into model pruning.\n",
    "\n",
    "State of the art deep learning models consist of millions of parameters and are trained on very large datasets. For transfer learning we take a pre-trained model and fine-tune it on a new and typically much smaller dataset. The new dataset may even consist of different classes, so the model is basically learning a new task. This process allows us to quickly achieve state of the art results without having to design and train our own model from scratch. However it may happen that a much smaller and simpler model would also perform well on our dataset. With model pruning we identify the importance of weights during training and remove the weights that are contributing negligibly to the learning process. We can do this in an iterative way where we remove let's say 5% of weights in each iteration. \n",
    "\n",
    "We use SageMaker Debugger to get weights, activation outputs and gradients during training. These tensors are used to compute the importance of weights. We will use SageMaker Experiments to keep track of each pruning iteration: if we prune too much we may degrade model accuracy, so we will monitor number of parameters versus validation accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sagemaker\n",
    "! pip install sagemaker-experiments\n",
    "! pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and save AlexNet model\n",
    "\n",
    "First we get a pre-trained AlexNet model from PyTorch model zoo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from pytorch_iterative_model_pruning import model_alexnet\n",
    "\n",
    "model = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained on CIFAR10 so we set the number of output classes to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[6] = nn.Linear(4096, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we store the model definition and weights in an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'model': model,\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'src/checkpoint_model_pruned')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell creates a SageMaker experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "sagemaker_boto_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "mnist_autoencoder = Experiment.create(\n",
    "    experiment_name=\"model-pruning-experiment\", \n",
    "    description=\"Iterative model pruning of AlexNet trained on CIFAR10\", \n",
    "    sagemaker_boto_client=sagemaker_boto_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell defines a list of tensor names that are considered for pruning. The list contains all convolutional layers and their biases. It also includes the fully-connected layers of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_outputs = model_alexnet.activation_outputs\n",
    "gradients = model_alexnet.gradients\n",
    "weights = model_alexnet.weights\n",
    "biases = model_alexnet.biases\n",
    "classifier_weights = model_alexnet.classifier_weights\n",
    "classifier_biases = model_alexnet.classifier_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "\n",
    "sagemaker_boto_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "experiment_config = { \"ExperimentName\": \"model-pruning-experiment\", \n",
    "                      \"TrialName\": None,\n",
    "                      \"TrialComponentDisplayName\": \"Training\"}\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "LOCATION_IN_BUCKET = 'smdebug-model-pruning-example'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative model pruning: step by step\n",
    "\n",
    "Before we jump into the code for running the iterative model pruning we will walk through the code step by step. First we create a new trial for each pruning iteration. That allows us to track our training jobs and see which models have the lowest number of parameters and best accuracy. We use the `smexperiments` library to create a trial within an experiment named `model-pruning-experiment`.\n",
    "\n",
    "```python\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.tracker import Tracker\n",
    "import boto3\n",
    "\n",
    "sagemaker_boto_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "trial = Trial.create(\n",
    "        experiment_name=\"model-pruning-experiment\",\n",
    "        sagemaker_boto_client=sagemaker_boto_client\n",
    "    )\n",
    "```\n",
    "\n",
    "Next we define the `experiment_config` which is a dictionary that will be passed to the SageMaker training. It associates the training job with a trial and an experiment. If we don't specify an `experiment_config` the training job will appear in SageMaker Experiments under `Unassigned trial components`\n",
    "\n",
    "```python \n",
    "experiment_config = {\n",
    "                        \"ExperimentName\": \"model-pruning-experiment\", \n",
    "                        \"TrialName\": trial.trial_name,\n",
    "                        \"TrialComponentDisplayName\": \"Training\",\n",
    "                    }\n",
    "```\n",
    "\n",
    "We use the SageMaker default bucket, to store the tensors emitted by Debugger. \n",
    "\n",
    "``` python \n",
    "\n",
    "import sagemaker \n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "LOCATION_IN_BUCKET = 'model-pruning-example'\n",
    "TRIAL_NAME = trial.trial_name\n",
    "\n",
    "s3_bucket_for_tensors = 's3://{BUCKET_NAME}/{LOCATION_IN_BUCKET}/{TRIAL_NAME}'.format(BUCKET_NAME=BUCKET_NAME, LOCATION_IN_BUCKET=LOCATION_IN_BUCKET, TRIAL_NAME=TRIAL_NAME)\n",
    "\n",
    "```\n",
    "We create a debugger hook configuration to define a custom collection of tensors to be emitted. The custom collection contains all weights and biases of the model. It also includes individual layer outputs and their gradients which will be used to compute filter ranks. Tensors are saved every 100th iteration where an iteration represents one forward and backward pass.\n",
    "\n",
    "```python\n",
    "debugger_hook_config = DebuggerHookConfig(\n",
    "              s3_output_path=s3_bucket_for_tensors,  \n",
    "              collection_configs=[ \n",
    "                  CollectionConfig(\n",
    "                        name=\"custom_collection\",\n",
    "                        parameters={ \"include_regex\": \".*output_0|.*weight|.*bias\",\n",
    "                                 \"save_interval\": \"100\" })])\n",
    " ```                                    \n",
    "Now we define the SageMaker PyTorch Estimator. We will train the model on an `ml.p2.xlarge` instance. The model definition plus training is defined in the entry_point file `train.py`. \n",
    "\n",
    "```python\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(role=sagemaker.get_execution_role(),\n",
    "                  train_instance_count=1,\n",
    "                  train_instance_type='ml.p2.xlarge',\n",
    "                  train_volume_size=400,\n",
    "                  source_dir='src',\n",
    "                  entry_point='train.py',\n",
    "                  framework_version='1.3.1',\n",
    "                  py_version='py3',\n",
    "                  metric_definitions=[ {'Name':'train:loss', 'Regex':'loss:(.*?)'}, {'Name':'eval:acc', 'Regex':'acc:(.*?)'} ],\n",
    "                  enable_sagemaker_metrics=True,\n",
    "                  hyperparameters = {'epochs': 10},\n",
    "                  debugger_hook_config=debugger_hook_config\n",
    "        )\n",
    "```\n",
    "Once we have the estimator object we can call `fit` which creates a `ml.p2.xlarge` instance on which it starts the training.\n",
    "\n",
    "```python\n",
    "estimator.fit(experiment_config=experiment_config)\n",
    "```\n",
    "\n",
    "Once the training job has finished, we will retrieve its tensors, such as gradients, weights and biases. We use the `smdebug` library which provides functions to read and filter tensors. First we create a trial that is reading the tensors from S3.\n",
    "\n",
    "```python\n",
    "from smdebug.trials import create_trial\n",
    "\n",
    "smdebug_trial = create_trial(s3_bucket_for_tensors)\n",
    "```\n",
    "\n",
    "\n",
    "To access tensor values, we only need to call `smdebug_trial.tensor()`. For instance to get the value of the first fully connected layer at step 0 we run  `smdebug_trial.tensor('AlexNet_classifier.1.weight').value(0, mode=modes.TRAIN)`. Next we compute a filter rank for the convolutions. \n",
    "\n",
    "To recap: a filter is a collection of kernels (one kernel for every single input channel) and a filter produces one feature map (output channel). In the image below the convolution creates 64 feature maps (output channels) and uses a kernel of 5x5. By pruning a filter, an entire feature map will be removed. So in the example image below the number of feature maps (output channels) would shrink to 63 and the number of learnable parameters (weights) would be reduced by 1x5x5.\n",
    "\n",
    "![](images/convolution.png) \n",
    "\n",
    "\n",
    "In this noteook we compute filter ranks as described in the article [\"Pruning Convolutional Neural Networks for Resource Efficient Inference\"](https://arxiv.org/pdf/1611.06440.pdf) \n",
    "In the following code we retrieve activation outputs and gradients and compute the 1st order Taylor series that is used to measure the filter rank. \n",
    "\n",
    "```python\n",
    "    filters = {}\n",
    "    for activation_output_name, gradient_name in zip(activation_outputs, gradients):\n",
    "        for step in smdebug_trial.steps(mode=modes.TRAIN):\n",
    "            activation_output = smdebug_trial.tensor(activation_output_name).value(step, mode=modes.TRAIN)\n",
    "            gradient = smdebug_trial.tensor(gradient_name).value(step, mode=modes.TRAIN)\n",
    "            rank = activation_output * gradient\n",
    "            rank = np.mean(rank, axis=(0,2,3))\n",
    "\n",
    "            if activation_output_name not in filters:\n",
    "                filters[activation_output_name] = 0\n",
    "            filters[activation_output_name] += rank\n",
    "```\n",
    "\n",
    "Next we normalize the filters:\n",
    "```python\n",
    "    rank = np.abs(filters[activation_output_name])\n",
    "    rank = rank / np.sqrt(np.sum(rank * rank))\n",
    "    filters[activation_output_name] = rank\n",
    "```\n",
    "\n",
    "We create a list of filters, sort it by rank and retrieve the smallest values:\n",
    "\n",
    "```python\n",
    "filters_list = []\n",
    "for layer_name in sorted(filters.keys()):\n",
    "    for channel in range(filters[layer_name].shape[0]): \n",
    "        filters_list.append((layer_name, channel, filters[layer_name][channel], ))\n",
    "\n",
    "filters_list.sort(key = lambda x: x[2])\n",
    "filters_list = filters_list[:100]\n",
    "print(\"The 100 smallest filters\", filters_list)\n",
    "```\n",
    "Next we prune the model, where we remove filters and their corresponding weights. The new model definition and weights are saved under `src` and will be used by the next training job.\n",
    "\n",
    "```python\n",
    "model = model_alexnet.prune(model, \n",
    "                    activation_outputs, \n",
    "                    weights, \n",
    "                    biases, \n",
    "                    classifier_weights, \n",
    "                    classifier_biases, \n",
    "                    filters_dict, \n",
    "                    trial)\n",
    "\n",
    "checkpoint = {'model': model,\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'src/checkpoint_model_pruned')  \n",
    "del model\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall workflow looks like the following:\n",
    " ![](images/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run iterative model pruning\n",
    "\n",
    "After having gone through the code step by step, we are ready to run the full worfklow. The following cell runs 10 pruning iterations: in each iteration of the pruning a new SageMaker training job is started, where it emits gradients and activation outputs to Amazon S3. Once the job has finished, filter ranks are computed and the 100 smallest filters are removed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from smexperiments.trial import Trial\n",
    "from smdebug.trials import create_trial\n",
    "from smdebug import modes\n",
    "from sagemaker.debugger import DebuggerHookConfig, CollectionConfig\n",
    "from torchsummary import summary\n",
    "\n",
    "# start iterative pruning\n",
    "for pruning_step in range(10):\n",
    "    \n",
    "    #create new trial for this pruning step\n",
    "    smexperiments_trial = Trial.create(\n",
    "        experiment_name=\"model-pruning-experiment\",\n",
    "        sagemaker_boto_client=sagemaker_boto_client\n",
    "    )\n",
    "    experiment_config[\"TrialName\"] = smexperiments_trial.trial_name\n",
    "\n",
    "    # s3 path where tensors will be stored\n",
    "    s3_bucket_for_tensors = 's3://{BUCKET_NAME}/{LOCATION_IN_BUCKET}/{TRIAL_NAME}'.format(BUCKET_NAME=BUCKET_NAME, LOCATION_IN_BUCKET=LOCATION_IN_BUCKET, TRIAL_NAME=smexperiments_trial.trial_name)\n",
    "\n",
    "    print(\"Created new trial\", smexperiments_trial.trial_name, \"for pruning step\", pruning_step)\n",
    "    \n",
    "    #debugger hook configuration for custom collection\n",
    "    debugger_hook_config = DebuggerHookConfig(\n",
    "                  s3_output_path=s3_bucket_for_tensors,  \n",
    "                  collection_configs=[ \n",
    "                      CollectionConfig(\n",
    "                            name=\"custom_collection\",\n",
    "                            parameters={ \"include_regex\": \".*output|.*weight|.*bias\",\n",
    "                                         \"save_interval\": \"100\" })])\n",
    "        \n",
    "    # train the models\n",
    "    estimator = PyTorch(role=sagemaker.get_execution_role(),\n",
    "                  train_instance_count=1,\n",
    "                  train_instance_type='ml.p2.xlarge',\n",
    "                  train_volume_size=400,\n",
    "                  source_dir='src',\n",
    "                  entry_point='train.py',\n",
    "                  framework_version='1.3.1',\n",
    "                  py_version='py3',\n",
    "                  metric_definitions=[ {'Name':'train:loss', 'Regex':'loss:(.*?)'}, {'Name':'eval:acc', 'Regex':'acc:(.*?)'} ],\n",
    "                  enable_sagemaker_metrics=True,\n",
    "                  hyperparameters = {'epochs': 10},\n",
    "                  debugger_hook_config = debugger_hook_config\n",
    "        )\n",
    "    \n",
    "    #start training job\n",
    "    estimator.fit(experiment_config=experiment_config)\n",
    "\n",
    "    print(\"Training job\", estimator.latest_training_job.name , \"finished. Read tensors from \", s3_bucket_for_tensors)\n",
    "    \n",
    "    # read tensors\n",
    "    path = estimator.latest_job_debugger_artifacts_path()\n",
    "    smdebug_trial = create_trial(path)\n",
    "    \n",
    "    # compute filter ranks\n",
    "    filters = {}\n",
    "    for activation_output_name, gradient_name in zip(activation_outputs, gradients):\n",
    "        for step in smdebug_trial.steps(mode=modes.TRAIN):\n",
    "            activation_output = smdebug_trial.tensor(activation_output_name).value(step, mode=modes.TRAIN)\n",
    "            gradient = smdebug_trial.tensor(gradient_name).value(step, mode=modes.TRAIN)\n",
    "            rank = activation_output * gradient\n",
    "            rank = np.mean(rank, axis=(0,2,3))\n",
    "\n",
    "            if activation_output_name not in filters:\n",
    "                filters[activation_output_name] = 0\n",
    "            filters[activation_output_name] += rank\n",
    "        \n",
    "        #normalize\n",
    "        rank = np.abs(filters[activation_output_name])\n",
    "        rank = rank / np.sqrt(np.sum(rank * rank))\n",
    "        filters[activation_output_name] = rank\n",
    "        \n",
    "    # find lowest ranked filters\n",
    "    filters_list = []\n",
    "    for layer_name in sorted(filters.keys()):\n",
    "        for channel in range(filters[layer_name].shape[0]): \n",
    "            filters_list.append((layer_name, channel, filters[layer_name][channel], ))\n",
    "\n",
    "    filters_list.sort(key = lambda x: x[2])\n",
    "    filters_list = filters_list[:100]\n",
    "    print(\"The 100 smallest filters\", filters_list)\n",
    "        \n",
    "    #load previous model \n",
    "    checkpoint = torch.load(\"src/checkpoint_model_pruned\")\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    #print parameters per layer\n",
    "    print(\"Pruning iteration:\", pruning_step)\n",
    "    print(summary(model, (3, 64, 64)))\n",
    "    \n",
    "    #prune model\n",
    "    model = model_alexnet.prune(model, \n",
    "                        activation_outputs, \n",
    "                        weights, \n",
    "                        biases, \n",
    "                        classifier_weights, \n",
    "                        classifier_biases, \n",
    "                        filters_list, \n",
    "                        smdebug_trial, \n",
    "                        step)\n",
    "    \n",
    "    print(\"Saving pruned model\")\n",
    "    \n",
    "    # save pruned model\n",
    "    checkpoint = {'model': model,\n",
    "                  'state_dict': model.state_dict()}\n",
    "    torch.save(checkpoint, 'src/checkpoint_model_pruned')\n",
    "    \n",
    "    #clean up\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the iterative model pruning is running, we can track and visualize our experiment in SageMakwer Studio. The following image shows the number of parameters versus the model accuracy. The first iteration of the model consisted of 57 million parameters. After 10 iterations the number of parameters was reduced to 18 million, while accuracy increased to 97% and then dropped after the 6th pruning iteration. This means that the best accuracy can be reached if the model has a size of about 20 million parameters.\n",
    "\n",
    "![](images/results.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The following animation shows the number of parameters per layer for each pruning iteration. We can see that most of the parameters are pruned in the last convolutional layers. The model starts with 57 million parameters and a size of 218 MB. After 10 iterations it consists of only 18 million parameters and 70 MB. Less parameters means smaller model size, and hence faster training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/results.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:environment/pytorch-cpu-optimized"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
