{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Lab 6:  Deploy a token streaming solution on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767e0e3",
   "metadata": {},
   "source": [
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel. We recommend to use the Data Science 3.0 kernel.\n",
    "\n",
    "In this notebook, you will deploy a small solution using the [AWS Cloud Development Kit (CDK)](https://docs.aws.amazon.com/cdk/v2/guide/home.html) which includes an Amazon SageMaker endpoint that serves the [`cerebras/Cerebras-GPT-2.7B`](https://huggingface.co/cerebras/Cerebras-GPT-2.7B) on a `ml.g5.2xlarge` (single-GPU instance type) using a [Large Model Inference container](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html) container (HuggingFace Accelerate engine). This solution will demonstrate a streaming experience using [AWS Lambda's Response Streaming](https://docs.aws.amazon.com/lambda/latest/dg/configuration-response-streaming.html) with the generated tokens being returned to a HTTP client as they get generated.\n",
    "\n",
    "**Notices:**\n",
    "* Make sure that the `ml.g5.2xlarge` instance type is available in your AWS Region. If not, fallback on a similar instance type (e.g. `ml.g4dn.2xlarge`).\n",
    "* Make sure that the value of your \"+instance_type+ for endpoint usage\" Amazon SageMaker service quota allows you to deploy one Endpoint using the chosen instance type.\n",
    "\n",
    "### License information\n",
    "* The `cerebras/Cerebras-GPT-2.7B` model is under the Apache 2.0 license.\n",
    "* This notebook is a sample notebook and not intended for production use and is under the [MIT-0 license](https://github.com/aws/mit-0).\n",
    "\n",
    "### Permissions\n",
    "This lab involves the deployment of a AWS CloudFormation stack using the AWS CDK. Make sure that you are able to bootstrap a CDK environment in your account and that you are allowed to create and delete all the resources of this lab's stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182c6d9",
   "metadata": {},
   "source": [
    "## 1. Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.111 requires botocore==1.29.111, but you have botocore 1.29.162 which is incompatible.\n",
      "awscli 1.27.111 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker boto3 huggingface_hub --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa88a7",
   "metadata": {},
   "source": [
    "### 1.1 Imports & global variables assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb53e996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import huggingface_hub\n",
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84627e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SM_DEFAULT_EXECUTION_ROLE_ARN = sagemaker.get_execution_role()\n",
    "SM_SESSION = sagemaker.session.Session()\n",
    "SM_ARTIFACT_BUCKET_NAME = SM_SESSION.default_bucket()\n",
    "\n",
    "REGION_NAME = SM_SESSION._region_name\n",
    "S3_CLIENT = boto3.client(\"s3\", region_name=REGION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd57c186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HOME_DIR = os.environ[\"HOME\"]\n",
    "\n",
    "# HuggingFace local model storage\n",
    "HF_LOCAL_CACHE_DIR = Path(HOME_DIR) / \".cache\" / \"huggingface\" / \"hub\"\n",
    "HF_LOCAL_DOWNLOAD_DIR = Path.cwd() / \"model_repo\"\n",
    "HF_LOCAL_DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Selected HuggingFace model\n",
    "HF_HUB_MODEL_NAME = \"cerebras/Cerebras-GPT-2.7B\"\n",
    "\n",
    "# HuggingFace remote model storage (Amazon S3)\n",
    "HF_MODEL_KEY_PREFIX = f\"hf-llm-djl-serving/{HF_HUB_MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb716e52",
   "metadata": {},
   "source": [
    "### 1.2 Storage utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f14b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_s3_objects(bucket: str, key_prefix: str) -> List[Dict[str, Any]]:\n",
    "    paginator = S3_CLIENT.get_paginator(\"list_objects\")\n",
    "    operation_parameters = {\"Bucket\": bucket, \"Prefix\": key_prefix}\n",
    "    page_iterator = paginator.paginate(**operation_parameters)\n",
    "    return [obj for page in page_iterator for obj in page[\"Contents\"]]\n",
    "\n",
    "\n",
    "def delete_s3_objects(bucket: str, keys: str) -> None:\n",
    "    S3_CLIENT.delete_objects(Bucket=bucket, Delete={\"Objects\": [{\"Key\": key} for key in keys]})\n",
    "\n",
    "\n",
    "def get_local_model_cache_dir(hf_model_name: str) -> str:\n",
    "    for dir_name in os.listdir(HF_LOCAL_CACHE_DIR):\n",
    "        if dir_name.endswith(hf_model_name.replace(\"/\", \"--\")):\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"Could not find HF local cache directory for model {hf_model_name}\")\n",
    "    return HF_LOCAL_CACHE_DIR / dir_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d665b9",
   "metadata": {},
   "source": [
    "## 2. Model upload to Amazon S3\n",
    "Models served by a LMI container can be downloaded to the container in different ways:\n",
    "* Like all the SageMaker Inference containers, having the container to download the model from Amazon S3 as a single `model.tar.gz` file. In the case of LLMs, this approach is discouraged since downloading and decompression times can become unreasonably high.\n",
    "* Having the container to download the model directly from the HuggingFace Hub for you. This option may involve high download times too and requires access to the public Internet.\n",
    "* Having the container to download the uncompressed model from Amazon S3 with maximal throughput by using the [`s5cmd`](https://github.com/peak/s5cmd) utility. This option is specific to LMI containers and is the recommended one. It requires however, that the model has been previously uploaded to a S3 Bucket. \n",
    "\n",
    "In this section, you will:\n",
    "1. Download the model from the HuggingFace Hub to your local host,\n",
    "2. Upload the downloaded model to a S3 Bucket. This notebook uses the SageMaker's default regional Bucket. Feel free to upload the model to the Bucket of your choice by modifying the `SM_ARTIFACT_BUCKET_NAME` global variable accordingly.\n",
    "\n",
    "Each operation takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439c218",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2ad59a97104ea4a8ec94525a2b39a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cff22f9709471b8107ef672261a834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e6cbf7c1/config.json:   0%|          | 0.00/361 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8a37b1857344dfb7937ff86096ac7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)1e6cbf7c1/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a31cd09072140cb819f3cbfebe050a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)1e6cbf7c1/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156de59a6e9b4fa4a5e76226683c87f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/10.7G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=HF_HUB_MODEL_NAME,\n",
    "    revision=\"main\",\n",
    "    local_dir=HF_LOCAL_DOWNLOAD_DIR,\n",
    "    local_dir_use_symlinks=\"auto\",  # Files larger than 5MB are actually symlinked to the local HF cache\n",
    "    allow_patterns=[\n",
    "        \"*.json\",\n",
    "        \"*.pt\",\n",
    "        \"*.bin\",\n",
    "        \"*.txt\",\n",
    "        \"*.model\",\n",
    "        \"*.py\",\n",
    "    ],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b2fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = SM_SESSION.upload_data(\n",
    "    path=HF_LOCAL_DOWNLOAD_DIR.as_posix(),\n",
    "    bucket=SM_ARTIFACT_BUCKET_NAME,\n",
    "    key_prefix=HF_MODEL_KEY_PREFIX,\n",
    ")\n",
    "print(f\"Model artifacts have been successfully uploaded to: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e50c0-d9cd-42d4-b72d-75fed23acfdd",
   "metadata": {},
   "source": [
    "**Save the returned location for later.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400b85b",
   "metadata": {},
   "source": [
    "The `huggingface_hub.snapshot_download` function downloaded the model repository to a cache located in your home directory. Downloaded files were duplicated in the target local download directory. Large files (larger than 5 MB) were not duplicated however but simply symlinked. Still, uncompressed LLM artifacts consume disk space. The two following cells removes the downloaded files from your local host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1399a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - Remove HF model artifacts from the local download directory\n",
    "shutil.rmtree(HF_LOCAL_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - Remove HF model artifacts from the local HF cache directory\n",
    "hf_local_cache_dir = get_local_model_cache_dir(hf_model_name=HF_HUB_MODEL_NAME)\n",
    "shutil.rmtree(hf_local_cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bb604",
   "metadata": {},
   "source": [
    "## 3. Deploy the streaming solution\n",
    "### 3.1. Custom inference handler code\n",
    "The following custom Python code will be deployed to the Endpoint. First let's gather all the custom endpoint Python code in a `code` directory by executing the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"code\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec5502",
   "metadata": {},
   "source": [
    "The content of the `code` directory consist of:\n",
    "* A `requirements.txt` file that list the Python dependencies required by the custom Python code. It can include Python dependencies specific to the chosen model.\n",
    "* A DJLServing-specific `serving.properties` which allow to inject configuration values to both the DJL model server and to the custom Python handler.\n",
    "* Python source files (`cache.py` and `handler.py`), the `handler.py` file being used by the DJL server as entry point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "boto3==1.26.161\n",
    "transformers==4.27.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab338ce-2a5e-4b48-b8c2-19a55eb73af4",
   "metadata": {},
   "source": [
    "For more information about the content of the `serving.properties` file, refer to the [Configuration and settings](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html) page from the SageMaker service documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/serving.properties\n",
    "engine = Python\n",
    "option.entryPoint = handler.py\n",
    "option.task = text - generation\n",
    "option.low_cpu_mem_usage = true\n",
    "option.post_every_x_tokens = 1\n",
    "option.torch_dtype = fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48057713-2499-455e-a429-079b3080bd08",
   "metadata": {},
   "source": [
    "The `cache.py` module provides a dedicated object which posts the generated tokens to the Amazon DynamoDB table every time a new token is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/cache.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from types import TracebackType\n",
    "from typing import Any, Dict, List, Type\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CacheState:\n",
    "    counter: int = 0\n",
    "    buffer: List[str] = field(default_factory=list)\n",
    "    session_id: str = \"\"\n",
    "    has_eos_been_generated: bool = False\n",
    "    is_generation_finished: bool = False\n",
    "\n",
    "\n",
    "class DynamoDBSequenceCache:\n",
    "    def __init__(self, config: Dict[str, Any], eos_token: str) -> None:\n",
    "        self._client = boto3.client(\"dynamodb\", region_name=config[\"region_name\"])\n",
    "        # Cache config\n",
    "        self._table_name = config[\"table_name\"]\n",
    "        self._table_primary_key_name = config[\"table_primary_key_name\"]\n",
    "        self._post_every_x_tokens = config[\"post_every_x_tokens\"]\n",
    "        self._eos_token = eos_token\n",
    "        # Cache state\n",
    "        self._state = CacheState()\n",
    "\n",
    "    def __call__(self, session_id: str) -> DynamoDBSequenceCache:\n",
    "        self._state.session_id = session_id\n",
    "        return self\n",
    "\n",
    "    def __enter__(self) -> DynamoDBSequenceCache:\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type: Type, exc_value: BaseException, exc_tb: TracebackType) -> None:\n",
    "        # Exiting the context manager means that the token generation has stopped:\n",
    "        # - Either because the EOS token has been generated (state.has_eos_been_generated set to True),\n",
    "        # - Or because the nb of generated tokens exceeds max_new_tokens (state.has_eos_been_generated remains False)\n",
    "        self._state.is_generation_finished = True\n",
    "        self._flush_to_cache()\n",
    "        self._state = CacheState()\n",
    "\n",
    "    def put(self, token: str) -> str:\n",
    "        if token != self._eos_token:\n",
    "            self._state.buffer.append(token)\n",
    "        else:\n",
    "            self._state.has_eos_been_generated = True\n",
    "        self._state.counter += 1\n",
    "        if self._do_flush():\n",
    "            self._flush_to_cache()\n",
    "\n",
    "    def _do_flush(self) -> bool:\n",
    "        return (self._state.counter % self._post_every_x_tokens) == 0\n",
    "\n",
    "    def _flush_to_cache(self) -> None:\n",
    "        self._client.update_item(\n",
    "            TableName=self._table_name,\n",
    "            Key={self._table_primary_key_name: {\"S\": self._state.session_id}},\n",
    "            AttributeUpdates={\n",
    "                \"generated_sequence\": {\n",
    "                    \"Value\": {\"S\": \"\".join(self._state.buffer)},\n",
    "                    \"Action\": \"PUT\",\n",
    "                },\n",
    "                \"is_generation_finished\": {\n",
    "                    \"Value\": {\"BOOL\": self._state.is_generation_finished},\n",
    "                    \"Action\": \"PUT\",\n",
    "                },\n",
    "                \"has_eos_been_generated\": {\n",
    "                    \"Value\": {\"BOOL\": self._state.has_eos_been_generated},\n",
    "                    \"Action\": \"PUT\",\n",
    "                },\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729fc42",
   "metadata": {},
   "source": [
    "The custom entrypoint `handler.py` uses the utilities provided by the DJL-Python toolkit (DJLServing version 0.22.1 and later) available in the LMI container (`djl_python.streaming_utils`) to create an iterator object from the loaded model artifacts and request payload. On each iteration (i.e. every time the iterator is passed to `next`), it performs a forward pass and returns the generated token.\n",
    "\n",
    "**Notice**: Token iterators from DJL-Python support batching, i.e. an iterator can be created not only from a single prompt but from a list of input sequences. On each iteration, the iterator returns a list of tokens, one per input sequence. In the present case, batch size is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0380fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/handler.py\n",
    "import os\n",
    "import uuid\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "from djl_python import Input, Output\n",
    "from djl_python.streaming_utils import StreamingUtils\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "from cache import DynamoDBSequenceCache\n",
    "\n",
    "\n",
    "def get_torch_dtype_from_str(dtype: str) -> Optional[torch.dtype]:\n",
    "    if dtype == \"fp32\":\n",
    "        return torch.float32\n",
    "    if dtype == \"fp16\":\n",
    "        return torch.float16\n",
    "    if dtype == \"bf16\":\n",
    "        return torch.bfloat16\n",
    "    if dtype == \"int8\":\n",
    "        return torch.int8\n",
    "    if dtype is None:\n",
    "        return None\n",
    "    raise ValueError(f\"Data type cannot be parsed as valid Torch data type: {dtype}\")\n",
    "\n",
    "\n",
    "def parse_request(inputs: Input) -> Tuple[List[str], Dict[str, Any]]:\n",
    "    body = inputs.get_as_json()\n",
    "    inputs = body[\"inputs\"]\n",
    "    if isinstance(inputs, list):\n",
    "        prompt, *_ = inputs\n",
    "    generation_config = body[\"parameters\"]\n",
    "    return [prompt], generation_config\n",
    "\n",
    "\n",
    "class ConfigFactory:\n",
    "    def __init__(self, properties: Dict[str, str]) -> None:\n",
    "        self._properties = properties\n",
    "\n",
    "    def get_tokenizer_config(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"trust_remote_code\": (\n",
    "                self._properties.get(\"trust_remote_code\", \"false\").lower() == \"true\"\n",
    "            ),\n",
    "            \"revision\": self._properties.get(\"revision\", \"main\"),\n",
    "            \"padding_side\": \"left\",\n",
    "        }\n",
    "\n",
    "    def get_model_config(self) -> Dict[str, Any]:\n",
    "        dtype = self._properties.get(\"torch_dtype\")\n",
    "        return {\n",
    "            \"low_cpu_mem_usage\": (\n",
    "                self._properties.get(\"low_cpu_mem_usage\", \"true\").lower() == \"true\"\n",
    "            ),\n",
    "            \"torch_dtype\": get_torch_dtype_from_str(dtype=dtype),\n",
    "            \"trust_remote_code\": (\n",
    "                self._properties.get(\"trust_remote_code\", \"false\").lower() == \"true\"\n",
    "            ),\n",
    "            \"revision\": self._properties.get(\"revision\", \"main\"),\n",
    "        }\n",
    "\n",
    "    def get_accelerate_config(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"load_in_8bit\": (self._properties.get(\"load_in_8bit\", \"false\").lower() == \"true\"),\n",
    "            \"device_map\": self._properties.get(\"device_map\", \"auto\"),\n",
    "        }\n",
    "\n",
    "    def get_cache_config(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"table_name\": os.environ[\"CACHE_TABLE_NAME\"],\n",
    "            \"table_primary_key_name\": os.environ[\"CACHE_TABLE_PRIMARY_KEY_NAME\"],\n",
    "            \"region_name\": os.environ[\"REGION_NAME\"],\n",
    "            \"post_every_x_tokens\": int(self._properties.get(\"post_every_x_tokens\", 1)),\n",
    "        }\n",
    "\n",
    "    def get_default_generation_config(self) -> Dict[str, Any]:\n",
    "        return {\"max_new_tokens\": int(self._properties.get(\"max_new_tokens\", 256))}\n",
    "\n",
    "\n",
    "class AccelerateInferenceService:\n",
    "    engine = \"Accelerate\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.initialized = False\n",
    "        self._model_location = None\n",
    "        self._tokenizer = None\n",
    "        self._model = None\n",
    "        self._config = None\n",
    "        self._generation_request_handler = None\n",
    "\n",
    "    def _load_tokenizer(self) -> None:\n",
    "        tokenizer_config = self._config.get_tokenizer_config()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self._model_location, **tokenizer_config)\n",
    "        if not tokenizer.pad_token:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        # Since the token generator is able to handle multiple input sequences at once, the length of the input\n",
    "        # sequences must be normalized. We instruct the tokenizer to add padding tokens to the left of input sequences\n",
    "        # shorter than the longest input sequence. We therefore make sure that a padding token is set for the tokenizer.\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def _load_model(self) -> None:\n",
    "        model_config = self._config.get_model_config()\n",
    "        accelerate_config = self._config.get_accelerate_config()\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self._model_location, **model_config, **accelerate_config\n",
    "        )\n",
    "        self._model = model\n",
    "\n",
    "    def initialize(self, properties: Dict[str, str]) -> None:\n",
    "        print(f\"properties: {properties}\")\n",
    "        self._config = ConfigFactory(properties=properties)\n",
    "        # model_id can point to huggingface model_id or local directory.\n",
    "        # If option.model_id points to a s3 bucket, we download it and set model_id to the download directory.\n",
    "        # Otherwise we assume model artifacts are in the model_dir (/opt/ml/model, which is also the cwd)\n",
    "        self._model_location = properties.get(\"model_id\") or properties.get(\"model_dir\")\n",
    "        self._load_tokenizer()\n",
    "        self._load_model()\n",
    "        cache_config = self._config.get_cache_config()\n",
    "        self._generation_request_handler = create_generation_request_handler(\n",
    "            tokenizer=self._tokenizer,\n",
    "            model=self._model,\n",
    "            cache_config=cache_config,\n",
    "            engine=self.engine,\n",
    "        )\n",
    "        self.initialized = True\n",
    "\n",
    "    def handle_generation_request(self, inputs: Input) -> Output:\n",
    "        try:\n",
    "            input_seqs, request_generation_config = parse_request(inputs=inputs)\n",
    "            generation_config = self._config.get_default_generation_config()\n",
    "            generation_config.update(request_generation_config)\n",
    "            session_id = str(uuid.uuid4())\n",
    "            print(f\"inputs: {input_seqs}\")\n",
    "            print(f\"generation_config: {generation_config}\")\n",
    "            output = (\n",
    "                Output(code=200)\n",
    "                .add({\"session_id\": session_id})\n",
    "                .finalize(\n",
    "                    self._generation_request_handler, session_id, input_seqs, generation_config\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            output = Output(code=500, message=str(e))\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_generation_request_handler(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    cache_config: Dict[str, Any],\n",
    "    engine: str,\n",
    ") -> Callable:\n",
    "    \"\"\"Creates a generation handler (closure) function\"\"\"\n",
    "    cache = DynamoDBSequenceCache(config=cache_config, eos_token=tokenizer.eos_token)\n",
    "\n",
    "    def generation_request_handler(\n",
    "        session_id: str, input_seqs: List[str], generation_config: Dict[str, Any]\n",
    "    ) -> None:\n",
    "        stream_generator = StreamingUtils.get_stream_generator(engine)\n",
    "        token_iterator = stream_generator(model, tokenizer, input_seqs, **generation_config)\n",
    "        with cache(session_id=session_id):\n",
    "            for token_batch in token_iterator:\n",
    "                # The iterator supports multi-sequence generation (i.e. batch_size>1).\n",
    "                # Here batch_size=1, token_batch is a one-element list.\n",
    "                token, *_ = token_batch\n",
    "                cache.put(token=token)\n",
    "\n",
    "    return generation_request_handler\n",
    "\n",
    "\n",
    "_service = AccelerateInferenceService()\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> Optional[Output]:\n",
    "    if not _service.initialized:\n",
    "        _service.initialize(properties=inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "\n",
    "    return _service.handle_generation_request(inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6780a32",
   "metadata": {},
   "source": [
    "### 3.2. Stack deployment\n",
    "In this section, you will use the AWS CDK to deploy the solution described in the following diagram:\n",
    "\n",
    "![Token-streaming-solution](img/stream-llm-tokens-arch-diagram-light.png)\n",
    "\n",
    "\n",
    "**Notice:** Before performing any operation, the AWS CDK first retrieves credentials [like the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#configure-precedence). You can for example create a dedicated named profile for this lab and then add a `--profile <profile-name>` to all `cdk` commands.\n",
    "\n",
    "Now, let's open a terminal window and:\n",
    "1. Cd into the lab's directory\n",
    "2. Extract the archive content: `tar xzvf token-streaming-project.tar.gz`\n",
    "3. Cd into the newly created `token-streaming-project` directory: `cd token-streaming-project`\n",
    "4. Create a Python virtualenv: `python3 -m venv .venv`\n",
    "5. Activate the Python virtualenv: `source .venv/bin/activate`\n",
    "6. Install the required Python dependencies: `pip install -r requirements.txt`\n",
    "7. If not already done in the past for your account and Region, bootstrap your AWS CDK environment: `cdk bootstrap`\n",
    "8. To deploy the stack, substitute the content of the `MODEL_ID` variable into the following command and run it (This operation takes a few minutes):\n",
    "\n",
    "```bash\n",
    "cdk deploy --context ModelArtifactsUri=<model-artifacts-s3-uri> \\\n",
    "--context EndpointInstanceType=ml.g5.2xlarge \\\n",
    "--context CodeSourceDirPath=../code \\\n",
    "--context EndpointStartupTimeoutInSec=360\n",
    "```\n",
    "\n",
    "9. Write down the stack's output values. Using these values, fill the following variables appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT = \"<account-id>\"\n",
    "IDENTITY_POOL_ID = \"<cognito-id-pool-id>\"\n",
    "FUNCTION_NAME = \"<function-name>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb9b8d",
   "metadata": {},
   "source": [
    "## 4. Test the streaming solution\n",
    "### 4.1 Post your generation request programmatically using `boto3`\n",
    "In this section you will invoke the streaming function using `boto3`'s `invoke_with_response_stream` function. You will first retrieve temporary credentials from the identity pool, then invoke the function and get a response stream to iterate on in return. \n",
    "\n",
    "**Warning:** The very first request sent to the stack may return an empty stream. Don't hesitate to retry.\n",
    "\n",
    "Notice that the response body has been designed to have a similar structure to the responses resturned by the HuggingFace streaming APIs:\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"token\":{\n",
    "      \"id\":14,\n",
    "      \"text\":\"\\n\",\n",
    "      \"logprob\":null,\n",
    "      \"special\":false\n",
    "   }\n",
    "}\n",
    "```\n",
    "\n",
    "Only the `id` and `text` fields are actively used in this example. An additional `details` field is added on completion. Example: `\"details\":{\"FinishReason\":\"length\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINISH_MESSAGES = {\n",
    "    \"eos_token\": \"Sequence generation completed successfully: EOS token was generated\",\n",
    "    \"length\": \"Sequence generation completed successfully: Maximum sequence length was reached\",\n",
    "}\n",
    "\n",
    "\n",
    "def running_invocation_handler(chunk: Dict[str, Any]) -> None:\n",
    "    deserialized_payload = json.loads(chunk[\"PayloadChunk\"][\"Payload\"])\n",
    "    print(deserialized_payload[\"token\"][\"text\"], end=\"\")\n",
    "    try:\n",
    "        finish_reason = deserialized_payload[\"details\"][\"FinishReason\"]\n",
    "        print(\"\\n\" + FINISH_MESSAGES[finish_reason])\n",
    "    except TypeError:  # \"details\" value is None\n",
    "        pass\n",
    "\n",
    "\n",
    "def completed_invocation_handler(chunk: Dict[str, Any]) -> None:\n",
    "    try:\n",
    "        serialized_error_details = chunk[\"InvokeComplete\"][\"ErrorDetails\"]\n",
    "        error_details = json.loads(serialized_error_details)\n",
    "        error_message = error_details[\"errorMessage\"]\n",
    "        print(f\"Resquest raised the following error: {error_message}\")\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "COGNITO_ID_CLIENT = boto3.client(\"cognito-identity\", region_name=REGION_NAME)\n",
    "\n",
    "get_id_response = COGNITO_ID_CLIENT.get_id(\n",
    "    AccountId=ACCOUNT,\n",
    "    IdentityPoolId=IDENTITY_POOL_ID,\n",
    ")\n",
    "\n",
    "get_creds_response = COGNITO_ID_CLIENT.get_credentials_for_identity(\n",
    "    IdentityId=get_id_response[\"IdentityId\"]\n",
    ")\n",
    "\n",
    "credentials = {\n",
    "    \"aws_access_key_id\": get_creds_response[\"Credentials\"][\"AccessKeyId\"],\n",
    "    \"aws_secret_access_key\": get_creds_response[\"Credentials\"][\"SecretKey\"],\n",
    "    \"aws_session_token\": get_creds_response[\"Credentials\"][\"SessionToken\"],\n",
    "}\n",
    "\n",
    "LAMBDA_CLIENT = boto3.client(\"lambda\", region_name=REGION_NAME, **credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is Amazon? Be concise.\"\n",
    "\n",
    "generation_parameters = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "request_body = {\"inputs\": prompt, \"parameters\": generation_parameters}\n",
    "\n",
    "response = LAMBDA_CLIENT.invoke_with_response_stream(\n",
    "    FunctionName=FUNCTION_NAME, InvocationType=\"RequestResponse\", Payload=json.dumps(request_body)\n",
    ")\n",
    "\n",
    "for chunk in response[\"EventStream\"]:\n",
    "    if \"InvokeComplete\" not in chunk:\n",
    "        running_invocation_handler(chunk=chunk)\n",
    "    else:\n",
    "        completed_invocation_handler(chunk=chunk)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec2298",
   "metadata": {},
   "source": [
    "### 4.2 Post your generation request using the command line and `cURL`\n",
    "If you have `cURL` (or any HTTP client) and the AWS CLI installed on your local host, you can post your generation requests and get streamed tokens using the command line:\n",
    "1. First retrieve the identity pool ID from the CDK stack's output and substitute it into the following command to get an ID from the Cognito identity pool: `aws cognito-identity get-id --identity-pool-id <cognito-id-pool-id>`\n",
    "2. Get temporary credentials for your identity: `aws cognito-identity get-credentials-for-identity --identity-id <id>`. The request should return a `Credentials` dictionary with an access key Id, a secret access key and a session token.\n",
    "3. To post your request, first substitute the credentials and the Lambda function's URL (part of the CDK stack's outputs) into the following cURL command and then run it:\n",
    "\n",
    "```bash\n",
    "curl -X POST \"<token-streaming-function-url>\" \\\n",
    "-d '{\"inputs\": \"What is Amazon? Be concise.\", \"parameters\": {\"max_new_tokens\": 128, \"do_sample\": true}}' \\\n",
    "--user \"<access-key-id>:<secret-access-key-id>\" \\\n",
    "--header \"x-amz-security-token: <session-token>\" \\\n",
    "--aws-sigv4 \"aws:amz:<region-name>:lambda\" \\\n",
    "--no-buffer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d85273",
   "metadata": {},
   "source": [
    "## 5. Clean-up\n",
    "Once done, execute the following commands to remove the lab's artifacts and resources from both you local host and AWS account:\n",
    "1. Destroy the lab's CDK stack: `cdk destroy`\n",
    "2. If you want to destroy the `CDKToolkit` stack created at the bootstrapping step, see [this comment](https://github.com/aws/aws-cdk/issues/986#issuecomment-644602463).\n",
    "3. Deactivate the virtualenv: `deactivate`\n",
    "4. Cd back in the lab's root directory: `cd ..`\n",
    "5. Remove the `token-streaming-stack.tar.gz` file: `rm token-streaming-project.tar.gz`\n",
    "6. Remove the CDK's application directory `rm -rf token-streaming-project` (includes the removal of the virtualenv).\n",
    "7. Remove the code directory: `rm -rf code`\n",
    "8. Remove the model artifacts from Amazon S3 by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81804b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HF model artifacts from S3\n",
    "hf_s3_objects = list_s3_objects(bucket=SM_ARTIFACT_BUCKET_NAME, key_prefix=HF_MODEL_KEY_PREFIX)\n",
    "hf_s3_objects_keys = [obj[\"Key\"] for obj in hf_s3_objects]\n",
    "delete_s3_objects(bucket=SM_ARTIFACT_BUCKET_NAME, keys=hf_s3_objects_keys)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
