{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Deploy, and Monitor the Music Recommender Model using SageMaker Pipelines\n",
    "\n",
    "----\n",
    "## Background\n",
    "\n",
    "In this notebook, we'll build an end-to-end pipeline to create a music recommender using [SageMaker Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html), which will automate  the entire modeling process from the beginning of data ingestion to monitoring the model. SageMaker Pipelines is a tool for building machine learning pipelines that take advantage of direct SageMaker integration. Because of this integration, you can create a pipeline and set up SageMaker Projects for orchestration using a tool that handles much of the step creation and management for you.\n",
    "\n",
    "If you want to learn more about each step of the pipeline, feel free to look at the series of notebooks listed below. It basically implements the same process in this notebook in a manual way with more detailed descriptions of what each step does. Please see the [README.md](README.md) for more information about the use case implemented by this sequence of notebooks. \n",
    "\n",
    "1. [Music Recommender Data Exploration](01_data_exploration.ipynb)\n",
    "1. [Music Recommender Data Preparation with SageMaker Feature Store and SageMaker Data Wrangler](02_export_feature_groups.ipynb)\n",
    "1. [Train, Deploy, and Monitor the Music Recommender Model using SageMaker SDK](03_train_deploy_debugger_explain_monitor_registry.ipynb)\n",
    "\n",
    "----\n",
    "## Contents\n",
    "1. [Architecture: Create a SageMaker Pipeline to Automate All the Steps from Data Prep to Model Deployment](#Architecture:-Create-a-SageMaker-Pipeline-to-Automate-All-the-Steps-from-Data-Prep-to-Model-Deployment)\n",
    "1. [SageMaker Pipeline Overview](#SageMaker-Pipeline-Overview)\n",
    "1. [Clean Up](#Clean-Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required and/or update third-party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -Uq pip\n",
    "!python -m pip install -q sagemaker==2.45.0 imbalanced-learn awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pathlib\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set region and boto3 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "\n",
    "sys.path.insert(1, \"./code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"music-recommendation-pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dir = \"/opt/ml/processing\"\n",
    "\n",
    "# Output name is auto-generated from the select node's ID + output name from the flow file.\n",
    "# You can change to a different node ID to export a different step in the flow file\n",
    "output_name_tracks = \"19ad8e80-2002-4ee9-9753-fe9a384b1166.default\"  # tracks node in flow file\n",
    "output_name_user_preferences = (\n",
    "    \"7a6dad19-2c80-43e3-b03d-ec23c3842ae9.default\"  # joined node in flow file\"\n",
    ")\n",
    "output_name_ratings = \"9a283380-91ca-478e-be99-6ba3bf57c680.default\"  # ratings node in flow file\n",
    "\n",
    "# ======> variables used for parameterizing the notebook run\n",
    "flow_instance_count = 1\n",
    "flow_instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "deploy_model_instance_type = \"ml.m4.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture: Create a SageMaker Pipeline to Automate All the Steps from Data Prep to Model Deployment\n",
    "\n",
    "----\n",
    "\n",
    "![arch diagram](./images/music-rec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prereqs: Get Data \n",
    "----\n",
    "\n",
    "Here we will download the music data from a public S3 bucket that we'll be using for this demo and uploads it to your default S3 bucket that was created for you when you initially created a SageMaker Studio workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_helpers import get_data, get_model, update_data_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data folder\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public S3 bucket that contains our music data\n",
    "s3_bucket_music_data = \"s3://sagemaker-sample-files/datasets/tabular/synthetic-music\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_paths = get_data(\n",
    "    s3_client,\n",
    "    [f\"{s3_bucket_music_data}/tracks.csv\", f\"{s3_bucket_music_data}/ratings.csv\"],\n",
    "    bucket,\n",
    "    prefix,\n",
    "    sample_data=0.70,\n",
    ")\n",
    "print(new_data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the new file paths located on your SageMaker Studio default s3 storage bucket\n",
    "tracks_data_source = f\"s3://{bucket}/{prefix}/tracks.csv\"\n",
    "ratings_data_source = f\"s3://{bucket}/{prefix}/ratings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will provide the processed data you need to complete this task. But you are free to take a look at how we processed the data:\n",
    "\n",
    "* If you are curious as to how `tracks_new.csv` and `ratings_new.csv` are generated, see [Music Recommender Data Exploration](01_data_exploration.ipynb)\n",
    "* If you are curious as to how the rest of the files are generated, see [Music Recommender Data Preparation with SageMaker Feature Store and SageMaker Data Wrangler](02_export_feature_groups.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_download = [\n",
    "    f\"sample_tracks.csv\",\n",
    "    f\"sample_user.csv\",\n",
    "    f\"train_data_headers.csv\",\n",
    "    f\"train_data.zip\",\n",
    "    f\"val_data_headers.csv\",\n",
    "    f\"val_data.zip\",\n",
    "    f\"tracks_new.csv\",\n",
    "    f\"ratings_new.csv\",\n",
    "]\n",
    "\n",
    "for file in files_to_download:\n",
    "    s3_client.download_file(\n",
    "        f\"sagemaker-sample-files\", f\"datasets/tabular/synthetic-music/{file}\", f\"./data/{file}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -o './data/*.zip' -d './data'\n",
    "! rm ./data/*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload train and validation datasets as well\n",
    "s3_client.upload_file(\"data/tracks_new.csv\", bucket, f\"{prefix}/data/tracks_new.csv\")\n",
    "s3_client.upload_file(\"data/ratings_new.csv\", bucket, f\"{prefix}/data/ratings_new.csv\")\n",
    "s3_client.upload_file(\"data/train_data.csv\", bucket, f\"{prefix}/data/train/train_data.csv\")\n",
    "s3_client.upload_file(\"data/val_data.csv\", bucket, f\"{prefix}/data/val/val_data.csv\")\n",
    "\n",
    "\n",
    "train_data_uri = f\"s3://{bucket}/{prefix}/data/train/train_data.csv\"\n",
    "val_data_uri = f\"s3://{bucket}/{prefix}/data/val/val_data.csv\"\n",
    "print(f\"Saving training data to {train_data_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Pipeline Overview\n",
    "\n",
    "---- \n",
    "\n",
    "### List of Steps\n",
    "\n",
    "1. [Step 1: Data Wrangler Preprocessing Step](#Step-1:-Data-Wrangler-Preprocessing-Step)\n",
    "1. [Step 2: Create Dataset and Train/Test Split](#Step-2:-Create-Dataset-and-Train/Test-Split)\n",
    "1. [Step 3: Train XGBoost Model](#Step-3:-Train-XGBoost-Model)\n",
    "1. [Step 4: Model Pre-Deployment Step](#Step-4:-Model-Pre-Deployment-Step)\n",
    "1. [Step 5: Register Model](#Step-5:-Register-Model)\n",
    "1. [Step 6: Deploy Model](#Step-6:-Deploy-Model)\n",
    "1. [Step 7: Monitor Model Deployed to SageMaker Hosted Endpoint](#Step-7:-Monitor-Model-Deployed-to-SageMaker-Hosted-Endpoint)\n",
    "1. [Combine Steps and Run Pipeline](#Combine-Steps-and-Run-Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that you've manually done each step in our machine learning workflow, you can certain steps to allow for faster model experimentation without sacrificing transparncy and model tracking. In this section you will create a pipeline which trains a new model, persists the model in SageMaker and then adds the model to the registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline parameters\n",
    "An important feature of SageMaker Pipelines is the ability to define the steps ahead of time, but be able to change the parameters to those steps at execution without having to re-define the pipeline. This can be achieved by using ParameterInteger, ParameterFloat or ParameterString to define a value upfront which can be modified when you call `pipeline.start(parameters=parameters)` later. Only certain parameters can be defined this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_param = ParameterString(\n",
    "    name=\"TrainingInstance\",\n",
    "    default_value=\"ml.m4.xlarge\",\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Wrangler Preprocessing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the data source in the `.flow` file\n",
    "The `01_music_datapred.flow` file is a JSON file containing instructions for where to find your data sources and how to transform the data. We'll be updating the object telling Data Wrangler where to find the input data on S3. We will set this to your default S3 bucket. With this update to the `.flow` file it now points to your new S3 bucket as the data source used by SageMaker Data Wrangler.\n",
    "\n",
    "Make sure the `.flow` file is closed before running this next step or it won't update the new s3 file locations in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "update_data_sources(\"01_music_dataprep.flow\", tracks_data_source, ratings_data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload flow to S3\n",
    "This will become an input to the first step and, as such, needs to be in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"01_music_dataprep.flow\"\n",
    "\n",
    "s3_client.upload_file(\n",
    "    Filename=flow_file_name, Bucket=bucket, Key=f\"{prefix}/dataprep-notebooks/music_dataprep.flow\"\n",
    ")\n",
    "flow_s3_uri = f\"s3://{bucket}/{prefix}/dataprep-notebooks/music_dataprep.flow\"\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Data Wrangler step's inputs\n",
    "In this step, new data from `source=` will be transformed according to the SageMaker Data Wrangler `.flow` file and later added to the existing feature groups we created in the `02` notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = []\n",
    "\n",
    "## Input - S3 Source: tracks.csv\n",
    "data_sources.append(\n",
    "    ProcessingInput(\n",
    "        source=f\"s3://{bucket}/{prefix}/data/tracks_new.csv\",  # You can override this to point to another dataset on S3\n",
    "        destination=f\"{processing_dir}/data/tracks_new.csv\",\n",
    "        input_name=\"tracks_new.csv\",\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    )\n",
    ")\n",
    "\n",
    "## Input - S3 Source: ratings.csv\n",
    "data_sources.append(\n",
    "    ProcessingInput(\n",
    "        source=f\"s3://{bucket}/{prefix}/data/ratings_new.csv\",  # You can override this to point to another dataset on S3\n",
    "        destination=f\"{processing_dir}/data/ratings_new.csv\",\n",
    "        input_name=\"ratings_new.csv\",\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    )\n",
    ")\n",
    "\n",
    "## Input - Flow: 01_music_dataprep.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=f\"{processing_dir}/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define outputs for the Data Wranger step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature group names we previously created in notebooks 02a-c\n",
    "fg_name_tracks = \"track-features-music-rec\"\n",
    "fg_name_ratings = \"ratings-features-music-rec\"\n",
    "fg_name_user_preferences = \"user-5star-track-features-music-rec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_output_tracks = sagemaker.processing.ProcessingOutput(\n",
    "    output_name=output_name_tracks,\n",
    "    app_managed=True,\n",
    "    feature_store_output=sagemaker.processing.FeatureStoreOutput(feature_group_name=fg_name_tracks),\n",
    ")\n",
    "\n",
    "flow_output_user_preferences = sagemaker.processing.ProcessingOutput(\n",
    "    output_name=output_name_user_preferences,\n",
    "    app_managed=True,\n",
    "    feature_store_output=sagemaker.processing.FeatureStoreOutput(\n",
    "        feature_group_name=fg_name_user_preferences\n",
    "    ),\n",
    ")\n",
    "\n",
    "flow_output_ratings = sagemaker.processing.ProcessingOutput(\n",
    "    output_name=output_name_ratings,\n",
    "    app_managed=True,\n",
    "    feature_store_output=sagemaker.processing.FeatureStoreOutput(\n",
    "        feature_group_name=fg_name_ratings\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output configuration used as processing job container arguments\n",
    "output_config_tracks = {output_name_tracks: {\"content_type\": \"CSV\"}}\n",
    "\n",
    "output_config_user_preferences = {output_name_user_preferences: {\"content_type\": \"CSV\"}}\n",
    "\n",
    "output_config_ratings = {output_name_ratings: {\"content_type\": \"CSV\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define processor and processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.network import NetworkConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangler Container URL\n",
    "# You can also find the proper container uri by exporting your Data Wrangler flow to a pipeline notebook\n",
    "\n",
    "container_uri = sagemaker.image_uris.retrieve(framework=\"data-wrangler\", region=region)\n",
    "\n",
    "\n",
    "flow_processor = sagemaker.processing.Processor(\n",
    "    role=sagemaker_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=flow_instance_count,\n",
    "    instance_type=flow_instance_type,\n",
    "    volume_size_in_gb=30,\n",
    "    network_config=NetworkConfig(enable_network_isolation=False),\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "flow_step_tracks = ProcessingStep(\n",
    "    name=\"DataWranglerStepTracks\",\n",
    "    processor=flow_processor,\n",
    "    inputs=[flow_input] + data_sources,\n",
    "    outputs=[flow_output_tracks],\n",
    "    job_arguments=[f\"--output-config '{json.dumps(output_config_tracks)}'\"],\n",
    ")\n",
    "\n",
    "flow_step_ratings = ProcessingStep(\n",
    "    name=\"DataWranglerStepRatings\",\n",
    "    processor=flow_processor,\n",
    "    inputs=[flow_input] + data_sources,\n",
    "    outputs=[flow_output_ratings],\n",
    "    job_arguments=[f\"--output-config '{json.dumps(output_config_ratings)}'\"],\n",
    ")\n",
    "\n",
    "flow_step_user_preferences = ProcessingStep(\n",
    "    name=\"DataWranglerStepUserPref\",\n",
    "    processor=flow_processor,\n",
    "    inputs=[flow_input] + data_sources,\n",
    "    outputs=[flow_output_user_preferences],\n",
    "    job_arguments=[f\"--output-config '{json.dumps(output_config_user_preferences)}'\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Dataset and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"./code/create_datasets.py\", Bucket=bucket, Key=f\"{prefix}/code/create_datasets.py\"\n",
    ")\n",
    "create_dataset_script_uri = f\"s3://{bucket}/{prefix}/code/create_datasets.py\"\n",
    "\n",
    "create_dataset_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    instance_count=2,\n",
    "    volume_size_in_gb=100,\n",
    "    base_job_name=\"music-rec-pipeline-split-data\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "create_dataset_step = ProcessingStep(\n",
    "    name=\"SplitData\",\n",
    "    processor=create_dataset_processor,\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"train_data\", source=f\"{processing_dir}/output/train\"\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"test_data\", source=f\"{processing_dir}/output/test\"\n",
    "        ),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--feature-group-name-tracks\",\n",
    "        fg_name_tracks,\n",
    "        \"--feature-group-name-ratings\",\n",
    "        fg_name_ratings,\n",
    "        \"--feature-group-name-user-preferences\",\n",
    "        fg_name_user_preferences,\n",
    "        \"--bucket-name\",\n",
    "        bucket,\n",
    "        \"--bucket-prefix\",\n",
    "        prefix,\n",
    "        \"--region\",\n",
    "        region,\n",
    "    ],\n",
    "    code=create_dataset_script_uri,\n",
    "    depends_on=[flow_step_tracks.name, flow_step_ratings.name, flow_step_user_preferences.name],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train XGBoost Model\n",
    "In this step we use the ParameterString `train_instance_param` defined at the beginning of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"4\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"100\",\n",
    "}\n",
    "\n",
    "save_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_estimator = Estimator(\n",
    "    role=sagemaker_role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    volume_size=60,\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", region, \"0.90-2\"),\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=f\"s3://{bucket}/{prefix}/training_jobs\",\n",
    "    base_job_name=\"xgb-music-rec-pipeline-model\",\n",
    "    max_run=1800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = TrainingStep(\n",
    "    name=\"TrainStep\",\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        \"train\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=create_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train_data\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=create_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test_data\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model Pre-Deployment Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.model.Model(\n",
    "    name=\"music-rec-pipeline-xgboost-model\",\n",
    "    image_uri=train_step.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(instance_type=\"ml.m4.xlarge\")\n",
    "\n",
    "create_model_step = CreateModelStep(name=\"CreateModel\", model=model, inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Register Model\n",
    "In this step you will use the ParameterString `model_approval_status` defined at the outset of the pipeline code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_step = RegisterModel(\n",
    "    name=\"XgboostRegisterModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=prefix,\n",
    "    approval_status=model_approval_status,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"./code/deploy_model.py\", Bucket=bucket, Key=f\"{prefix}/code/deploy_model.py\"\n",
    ")\n",
    "deploy_model_script_uri = f\"s3://{bucket}/{prefix}/code/deploy_model.py\"\n",
    "pipeline_endpoint_name = \"music-rec-pipeline-endpoint\"\n",
    "\n",
    "deploy_model_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size_in_gb=60,\n",
    "    base_job_name=\"music-recommender-deploy-model\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "deploy_step = ProcessingStep(\n",
    "    name=\"DeployModel\",\n",
    "    processor=deploy_model_processor,\n",
    "    job_arguments=[\n",
    "        \"--model-name\",\n",
    "        create_model_step.properties.ModelName,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-instance-type\",\n",
    "        deploy_model_instance_type,\n",
    "        \"--endpoint-name\",\n",
    "        pipeline_endpoint_name,\n",
    "    ],\n",
    "    code=deploy_model_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Monitor Model Deployed to SageMaker Hosted Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"./code/model_monitor.py\", Bucket=bucket, Key=f\"{prefix}/code/model_monitor.py\"\n",
    ")\n",
    "model_monitor_script_uri = f\"s3://{bucket}/{prefix}/code/model_monitor.py\"\n",
    "mon_schedule_name_base = \"music-rec-pipeline-daily-monitor\"\n",
    "\n",
    "\n",
    "model_monitor_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size_in_gb=60,\n",
    "    base_job_name=\"music-rec-pipeline-model-monitor\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "monitor_model_step = ProcessingStep(\n",
    "    name=\"ModelMonitor\",\n",
    "    processor=model_monitor_processor,\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"model_baseline\", source=f\"{processing_dir}/output/baselineresults\"\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--baseline-data-uri\",\n",
    "        val_data_uri,\n",
    "        \"--bucket-name\",\n",
    "        bucket,\n",
    "        \"--bucket-prefix\",\n",
    "        prefix,\n",
    "        \"--endpoint\",\n",
    "        pipeline_endpoint_name,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--schedule-name\",\n",
    "        mon_schedule_name_base,\n",
    "    ],\n",
    "    code=model_monitor_script_uri,\n",
    "    depends_on=[deploy_step.name],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Steps and Run Pipeline\n",
    "\n",
    "Once all of our steps are defined, we can put them together using the SageMaker `Pipeline` object. While we pass the steps in order so that it is easier to read, technically the order that we pass them does not matter since the pipeline DAG will parse it out properly based on any dependencies between steps. If the input of one step is the output of another step, the Pipelines understands which must come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_name = f\"MusicRecommendationPipeline\"\n",
    "dataprep_pipeline_name = f\"MusicRecommendationDataPrepPipeline\"\n",
    "train_deploy_pipeline_name = f\"MusicRecommendationTrainDeployPipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: The Entire Pipeline End to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f\"MusicRecommendationPipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[train_instance_param, model_approval_status],\n",
    "    steps=[\n",
    "        flow_step_tracks,\n",
    "        flow_step_user_preferences,\n",
    "        flow_step_ratings,\n",
    "        create_dataset_step,\n",
    "        train_step,\n",
    "        create_model_step,\n",
    "        register_step,\n",
    "        deploy_step,\n",
    "        monitor_model_step,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: You may want to refrain from running an entire data prep and train deploy pipeline as one long pipeline and rather consider breaking the pipeline into two parts:\n",
    "\n",
    "#### Data Prep Pipeline\n",
    "\n",
    "Sometimes we may want to run a number of data prep steps and split the data, getting it ready for training and beyond. This may require multiple iterations. We can separate this process from the rest of the pipeline by including only these data prep steps in their own smaller data prep pipeline.\n",
    "\n",
    "#### Train Deploy Monitor Pipeline\n",
    "\n",
    "This allows you to have separation of concerns around the preparation of data distinct from that of training, tuning, deployment, inference and monitoring until you want to kick off a retraining only, data prep only, or the complete pipeline. With SageMaker Pipelines you have the flexibility of doing any one of these in a modular and iterative manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Data Prep Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dataprep = Pipeline(\n",
    "    name=dataprep_pipeline_name,\n",
    "    parameters=[train_instance_param, model_approval_status],\n",
    "    steps=[flow_step_tracks, flow_step_user_preferences, flow_step_ratings, create_dataset_step],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3: Train Deploy Monitor Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_step_no_depend = ProcessingStep(\n",
    "    name=\"SplitData\",\n",
    "    processor=create_dataset_processor,\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"train_data\", source=f\"{processing_dir}/output/train\"\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"test_data\", source=f\"{processing_dir}/output/test\"\n",
    "        ),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--feature-group-name-tracks\",\n",
    "        fg_name_tracks,\n",
    "        \"--feature-group-name-ratings\",\n",
    "        fg_name_ratings,\n",
    "        \"--feature-group-name-user-preferences\",\n",
    "        fg_name_user_preferences,\n",
    "        \"--bucket-name\",\n",
    "        bucket,\n",
    "        \"--bucket-prefix\",\n",
    "        prefix,\n",
    "        \"--region\",\n",
    "        region,\n",
    "    ],\n",
    "    code=create_dataset_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_train_deploy_monitor = Pipeline(\n",
    "    name=train_deploy_pipeline_name,\n",
    "    parameters=[train_instance_param, model_approval_status],\n",
    "    steps=[\n",
    "        create_dataset_step_no_depend,\n",
    "        train_step,\n",
    "        create_model_step,\n",
    "        register_step,\n",
    "        deploy_step,\n",
    "        monitor_model_step,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline definition to the SageMaker Pipeline service\n",
    "Note: If an existing pipeline has the same name it will be overwritten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's choose the pipeline we want to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=sagemaker_role)\n",
    "pipeline_dataprep.upsert(role_arn=sagemaker_role)\n",
    "pipeline_train_deploy_monitor.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the entire pipeline definition\n",
    "Viewing the pipeline definition will all the string variables interpolated may help debug pipeline bugs. It is commented out here due to length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# json.loads(pipeline.describe()['PipelineDefinition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "Note this will take about 1 hour to complete. You can watch the progress of the Pipeline Job on your SageMaker Studio Components panel\n",
    "\n",
    "<!-- ![image.png](attachment:image.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special pipeline parameters can be defined or changed here\n",
    "parameters = {\"TrainingInstance\": \"ml.m5.4xlarge\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in the notebook, we defines several `ProcessingStep()`s and a `TrainingStep()` which our `Pipeline()` instance here will reference and kick off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start_response = pipeline.start(parameters=parameters)\n",
    "# start_response = pipeline_dataprep.start(parameters=parameters)\n",
    "# start_response = pipeline_train_deploy_monitor.start(parameters=parameters)\n",
    "start_response.wait(delay=60, max_attempts=1000)\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completion we can use Sagemaker Studio's **Components and Registries** tab to see our Pipeline graph and any further error or log messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demo_helpers\n",
    "\n",
    "demo_helpers.delete_project_resources(\n",
    "    sagemaker_boto_client=sagemaker_boto_client,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    endpoint_names=[pipeline_endpoint_name],\n",
    "    pipeline_names=[pipeline_name, dataprep_pipeline_name, train_deploy_pipeline_name],\n",
    "    prefix=prefix,\n",
    "    delete_s3_objects=True,\n",
    "    bucket_name=bucket,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}