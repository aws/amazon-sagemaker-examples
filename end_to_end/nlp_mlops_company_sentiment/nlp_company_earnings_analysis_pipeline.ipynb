{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Trends in Company Valuation with NLP\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Orchestrating company earnings trend analysis, using SEC filings, news sentiment with the Hugging Face transformers, and Amazon SageMaker Pipelines\n",
    "\n",
    "In this notebook, we demonstrate how to summarize and derive sentiments out of Security and Exchange Commission reports filed by a publicly traded organization. We will derive the overall market sentiments about the said organization through financial news articles within the same financial period to present a fair view of the organization vs. market sentiments and outlook about the company's overall valuation and performance. In addition to this we will also identify the most popular keywords and entities within the news articles about that organization.\n",
    "\n",
    "In order to achieve the above we will be using multiple SageMaker Hugging Face based NLP transformers for the downstream NLP tasks of Summarization (e.g., of the news and SEC MDNA sections) and Sentiment Analysis (of the resulting summaries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Using SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Pipelines is the first purpose-built, easy-to-use continuous integration and continuous delivery (CI/CD) service for machine learning (ML). With SageMaker Pipelines, you can create, automate, and manage end-to-end ML workflows at scale. \n",
    "\n",
    "Orchestrating workflows across each step of the machine learning process (e.g. exploring and preparing data, experimenting with different algorithms and parameters, training and tuning models, and deploying models to production) can take months of coding.\n",
    "\n",
    "Since it is purpose-built for machine learning, SageMaker Pipelines helps you automate different steps of the ML workflow, including data loading, data transformation, training and tuning, and deployment. With SageMaker Pipelines, you can build dozens of ML models a week, manage massive volumes of data, thousands of training experiments, and hundreds of different model versions. You can share and re-use workflows to recreate or optimize models, helping you scale ML throughout your organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Understanding trends in company valuation (or similar) with NLP\n",
    "\n",
    "**Natural language processing (NLP)** is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. (Source: [Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing))\n",
    "\n",
    "We are going to demonstrate how to summarize and derive sentiments out of Security and Exchange Commission reports filed by a publicly traded organization. We are also going to derive the overall market sentiments about the said organization through financial news articles within the same financial period to present a fair view of the organization vs. market sentiments and outlook about the company's overall valuation and performance. In addition to this we will also identify the most popular keywords and entities within the news articles about that organization.\n",
    "\n",
    "In order to achieve the above we will be using multiple SageMaker Hugging Face based NLP transformers with summarization and sentiment analysis downstream tasks.\n",
    "\n",
    "* <b> Summarization of financial text from SEC reports and news articles </b> will be done via [Pegasus for Financial Summarization model](https://huggingface.co/human-centered-summarization/financial-summarization-pegasus) based on the paper [Towards Human-Centered Summarization: A Case Study on Financial News](https://aclanthology.org/2021.hcinlp-1.4/). \n",
    "* Sentiment analysis on summarized SEC financial report and news articles will be done via pre-trained NLP model to analyze sentiment of financial text called [FinBERT](https://huggingface.co/ProsusAI/finbert). Paper: [ FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063)\n",
    "\n",
    "---\n",
    "\n",
    "### SEC Dataset\n",
    "\n",
    "The starting point for a vast amount of financial NLP is text in SEC filings. The SEC requires companies to report different types of information related to various events involving companies. The full list of SEC forms is here: https://www.sec.gov/forms.\n",
    "\n",
    "SEC filings are widely used by financial services companies as a source of information about companies in order to make trading, lending, investment, and risk management decisions. Because these filings are required by regulation, they are of high quality and veracity. They contain forward-looking information that helps with forecasts and are written with a view to the future, required by regulation. In addition, in recent times, the value of historical time-series data has degraded, since economies have been structurally transformed by trade wars, pandemics, and political upheavals. Therefore, text as a source of forward-looking information has been increasing in relevance. \n",
    "\n",
    "#### Obtain the dataset using the SageMaker JumpStart Industry Python SDK\n",
    "\n",
    "Downloading SEC filings is done from the SEC's Electronic Data Gathering, Analysis, and Retrieval (EDGAR) website, which provides open data access. EDGAR is the primary system under the U.S. Securities And Exchange Commission (SEC) for companies and others submitting documents under the Securities Act of 1933, the Securities Exchange Act of 1934, the Trust Indenture Act of 1939, and the Investment Company Act of 1940. EDGAR contains millions of company and individual filings. The system processes about 3,000 filings per day, serves up 3,000 terabytes of data to the public annually, and accommodates 40,000 new filers per year on average.\n",
    "\n",
    "There are several ways to download the data, and some open source packages available to extract the text from these filings. However, these require extensive programming and are not always easy-to-use. We provide a simple one-API call that will create a dataset in a few lines of code, for any period of time and for numerous tickers.\n",
    "\n",
    "We have wrapped the extraction functionality into a SageMaker processing container and provide this notebook to enable users to download a dataset of filings with metadata such as dates and parsed plain text that can then be used for machine learning using other SageMaker tools. This is included in the [SageMaker Industry Jumpstart Industry](https://aws.amazon.com/blogs/machine-learning/use-pre-trained-financial-language-models-for-transfer-learning-in-amazon-sagemaker-jumpstart/) library for financial language models. Users only need to specify a date range and a list of ticker symbols, and the library will take care of the rest.\n",
    "\n",
    "As of now, the solution supports extracting a popular subset of SEC forms in plain text (excluding tables): 10-K, 10-Q, 8-K, 497, 497K, S-3ASR, and N-1A. For each of these, we provide examples throughout this notebook and a brief description of each form. For the 10-K and 10-Q forms, filed every year or quarter, we also extract the Management Discussion and Analysis (MDNA) section, which is the primary forward-looking section in the filing. This is the section that has been most widely used in financial text analysis. Therefore, we provide this section automatically in a separate column of the dataframe alongside the full text of the filing.\n",
    "\n",
    "The extracted dataframe is written to S3 storage and to the local notebook instance. \n",
    "\n",
    "---\n",
    "\n",
    "### News articles related to the stock symbol -- dataset\n",
    "\n",
    "We will use the MIT Licensed [NewsCatcher API](https://docs.newscatcherapi.com/) to grab top 4-5 articles about the specific organization using filters, however other sources such as Social media feeds, RSS Feeds can also be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the pipeline is to fetch the SEC report from the EDGAR database using the [SageMaker Industry Jumpstart Industry](https://aws.amazon.com/blogs/machine-learning/use-pre-trained-financial-language-models-for-transfer-learning-in-amazon-sagemaker-jumpstart/) library for Financial language models. This library provides us an easy to use functionality to obtain either one or multiple SEC reports for one or more Ticker symbols or CIKs. The ticker or CIK number will be passed to the SageMaker Pipeline using Pipeline parameter `inference_ticker_cik`. For demo purposes of this Pipeline we will focus on a single Ticker/CIK number at a time and the MDNA section of the 10-K form. The first processing will extract the MDNA from the 10-K form for a company and will also gather few news articles related to the company from the NewsCatcher API. This data will ultimately be used for summarization and then finally sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## MLOps for NLP using SageMaker Pipelines\n",
    "\n",
    "We will set up the following SageMaker Pipeline. The Pipleline has two flows depending on what the value for `model_register_deploy` Pipeline parameter is set to. If the value is set to `Y` we want the pipeline to register the model and deploy the latest version of the model from the model registry to the SageMaker endpoint. If the value is set to `N` then we simply want to run inferences using the FinBert and the Pegasus models using the Ticker symbol (or CIK number) that is passed to the pipeline using the `inference_ticker_cik` Pipeline parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pipeline.png\" alt=\"Pipeline\" style=\"width: 800px;\"/>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Container\n",
    "\n",
    "To achieve that, you first have to build a docker image and push it to an [ECR (Elastic Container Registry)](https://aws.amazon.com/ecr/) repo in your account. Typically, this can be done using the `docker` CLI and `aws cli` in your local machine pretty easily. However, SageMaker makes it even easier to use this in the studio environment to build, create, and push any custom container to your ECR repository using a purpose-built tool known as `sagemaker-studio-image-build`, and use the custom container image in your notebooks for your ML projects. \n",
    "\n",
    "For more information on this, see [Using the Amazon SageMaker Studio Image Build CLI to build container images from your Studio notebooks](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/).\n",
    "\n",
    "Next, install this required CLI tool into the SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "MY_ACCOUNT = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "# CONTAINER_IMAGE_URI=\"738335684114.dkr.ecr.us-east-1.amazonaws.com/nlp-script-processor:1.0\"\n",
    "nlp_script_processor = f\"nlp-script-processor:1.0\"\n",
    "\n",
    "CONTAINER_IMAGE_URI = f\"{MY_ACCOUNT}.dkr.ecr.{region}.amazonaws.com/{nlp_script_processor}\"\n",
    "CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install sagemaker-studio-image-build CLI tool\n",
    "!pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Grant appropriate permissions to SageMaker\n",
    "\n",
    "In order to use `sagemaker-studio-image-build`, we need to first add permissions to SageMaker's IAM role so that it may perform actions on your behalf. Specifically, you would add Amazon ECR and Amazon CodeBuild permissions to it. Add the <font color='green'> AmazonEC2ContainerRegistryFullAccess </font> and <font color='green'> AWSCodeBuildAdminAccess </font> policies to your SageMaker default role.\n",
    "\n",
    "<img src=\"./images/iam-perm.png\" width=\"500\" height=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to this, you will also have to add the `iam:PassRole` permission to the SageMaker Studio execution role. Add the following policy as an inline policy to the SageMaker Studio Execution role using the AWS IAM console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"arn:aws:iam::*:role/*\",\n",
    "            \"Condition\": {\n",
    "                \"StringLikeIfExists\": {\n",
    "                    \"iam:PassedToService\": \"codebuild.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you must add a trust relationship in the SageMaker Studio Execution role to allow CodeBuild to assume this role. To add a trust relationship, do the following: \n",
    "\n",
    "1. Navigate to IAM Console\n",
    "2. Search for your SageMaker execution role. (You can find your SageMaker execution role name from SageMaker Studio console)\n",
    "3. Click on the \"Trust Relationships\" tab > Click the \"Edit Trust relationship\" button\n",
    "4. Add the following Trust relationship to any pre-existing trust relationship\n",
    "\n",
    "    ```json\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"codebuild.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a typical situation, your final trust relationship should look something like the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"sagemaker.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"codebuild.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "<img src=\"./images/trust.png\" width=\"500\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> <h4 color=\"red\"><strong>ðŸ’¡ NOTE </strong></h4>\n",
    "IAM Policies described in this notebook can be overly permissive. Please practice caution in setting up IAM Roles with them. For more information about fine-grained permissions for the sagemaker-studio-image-build tool, see <a href=\"https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/\">Using the Amazon SageMaker Studio Image Build CLI to build container images from your Studio notebooks</a>. For best practices on SageMaker security, IAM roles, and policies, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_id-based-policy-examples.html#security_iam_service-with-iam-policy-best-practices\">Policy Best Practices</a> in the Amazon SageMaker Developer Guide.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Build a custom Docker image\n",
    "\n",
    "We now build a custom [Dockerfile](https://docs.docker.com/engine/reference/builder/) and use the CLI tool to build the image from the Dockerfile. Our docker image is going to be pretty simple, it will be a copy of the open source [python:3.7-slim-buster](https://github.com/docker-library/python/blob/117d4e375b86cdbe1853930478d0d07d7d5701f7/3.7/buster/slim/Dockerfile) image and contain an installation of [Boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html), [SageMaker SDK](https://github.com/aws/sagemaker-python-sdk), Pandas, and NumPy.\n",
    "\n",
    "For our NLP pipeline, we have a number of tasks that depend on Boto3 and SageMaker SDK. We will also use the [SageMaker JumpStart Industry Python SDK](https://sagemaker-jumpstart-industry-pack.readthedocs.io/en/latest/notebooks/finance/notebook1/SEC_Retrieval_Summarizer_Scoring.html) to download 10k/10Q reports from SEC's EDGAR system. We install all of these dependencies in the container, and use the custom container in our [`ScriptProcessor` step](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-container-run-scripts.html) in our pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.7-slim-buster\n",
    "\n",
    "RUN pip3 install smjsindustry==1.0.0 requests botocore boto3>=1.15.0 sagemaker pandas numpy transformers typing sentencepiece nltk\n",
    "RUN python3 -c \"import nltk; nltk.download('punkt')\"\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above will create a `Dockerfile` in the local project's directory. We can then run the `sm-docker build` command to build, and publish our image. This single command will take care of building the Docker image and publishing it to a [private ECR Repository](https://docs.aws.amazon.com/AmazonECR/latest/userguide/Repositories.html) in your current region (i.e. your SageMaker Studio's default Region). \n",
    "\n",
    "<font color='red'> NOTE: </font> You must execute the code cell above to run the following cells. the `sm-docker build` command reads the `Dockerfile` to create the docker image. To ensure that the code above ran successfully, please verify that you have a file named `Dockerfile` is under the same directory where this notebook is located in the left navigation pane of Studio. This project already includes the Dockerfile, however, if you modify the code cell above, it would be a good idea to verify if the contents of the Dockerfile were updated correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!sm-docker build . --repository $nlp_script_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the command in the preceding code cell prints log lines in the notebook ending with three lines like the following example:\n",
    "\n",
    "```sh\n",
    "[Container] 2021/05/15 03:19:43 Phase complete: POST_BUILD State: SUCCEEDED\n",
    "[Container] 2021/05/15 03:19:43 Phase context status code:  Message:\n",
    "Image URI: <ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com/sagemaker-studio-d-xxxxxxxxx:default-<xxxxxxxxxxx>\n",
    "```\n",
    "\n",
    "We will need the `Image URI` for our SageMaker pipeline setup. You can also find this image URI from the [ECR Console](https://console.aws.amazon.com/ecr/repositories) (make sure the correct region is selected in the ECR console)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Set Up SageMaker Project\n",
    "\n",
    "<a id='setup-project'></a>\n",
    "\n",
    "\n",
    "### Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q sagemaker==2.91.1\n",
    "\n",
    "!pip install transformers\n",
    "!pip install typing\n",
    "!pip install sentencepiece\n",
    "!pip install fiscalyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install SageMaker Jumpstart Industry\n",
    "!pip install smjsindustry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "NOTE: After installing an updated version of SageMaker and PyTorch, save the notebook and then restart your kernel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "print(f\"SageMaker version: {sagemaker.__version__}\")\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.sklearn.processing import ScriptProcessor\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters that you'll use throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_role = role\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"nlp-e2e-mlops\"\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sagemaker_boto_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "\n",
    "# deploy_model_instance_type = \"ml.m4.8xlarge\"\n",
    "deploy_model_instance_type = \"ml.m4.xlarge\"\n",
    "inference_instances = [\n",
    "    \"ml.t2.medium\",\n",
    "    \"ml.m5.xlarge\",\n",
    "    \"ml.m5.2xlarge\",\n",
    "    \"ml.m5.4xlarge\",\n",
    "    \"ml.m5.12xlarge\",\n",
    "]\n",
    "transform_instances = [\"ml.m5.xlarge\"]\n",
    "PROCESSING_INSTANCE = \"ml.m4.4xlarge\"\n",
    "ticker = \"AMZN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"s3://{default_bucket}/{prefix}/code/model_deploy.py\")\n",
    "print(f\"SageMaker Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters to parametrize Pipeline Execution\n",
    "\n",
    "Using SageMaker Pipelines, we can define the steps to be included in a pipeline but then use parameters to modify that pipeline when we go to execute the pipeline, without having to modify the pipeline definition. We'll provide some default parameter values that can be overridden on pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some default parameters:\n",
    "\n",
    "# specify default number of instances for processing step\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "# specify default instance type for processing step\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=PROCESSING_INSTANCE\n",
    ")\n",
    "\n",
    "# specify location of inference data for data processing step\n",
    "inference_input_data = f\"s3://{default_bucket}/{prefix}/nlp-pipeline/inf-data\"\n",
    "\n",
    "# Specify the Ticker CIK for the pipeline\n",
    "inference_ticker_cik = ParameterString(\n",
    "    name=\"InferenceTickerCik\",\n",
    "    default_value=ticker,\n",
    ")\n",
    "\n",
    "# specify default method for model approval\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "# specify if new model needs to be registered and deployed\n",
    "model_register_deploy = ParameterString(name=\"ModelRegisterDeploy\", default_value=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='analyze-sec'></a>\n",
    "\n",
    "## Preparing SEC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive right into setting up the pipeline, let's take a look at how the SageMaker Jumpstart Industry SDK for Financial language model helps obtain the dataset from SEC forms and what are the features available for us to use. \n",
    "\n",
    "**Note:** The code cells in this section are completely optional and for information purposes only; we will use the SageMaker JumpStart Industry SDK directly in the pipeline. \n",
    "\n",
    "Let's install the required dependencies first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the SageMaker JumpStart Industry SDK\n",
    "\n",
    "The functionality is delivered through a client-side SDK. The first step requires pip installing a Python package that interacts with a SageMaker processing container. The retrieval, parsing, transforming, and scoring of text is a complex process and uses different algorithms and packages. In order to make this seamless and stable for the user, the functionality is packaged into a SageMaker container. This lifts the load of installation and maintenance of the workflow, reducing the user effort down to a pip install followed by a single API call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-index smjsindustry==1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will try to pull AMZN ticker 10k/10q filings from EDGAR and write the data as CSV to S3. Below is the single block of code that contains the API call. The options are all self-explanatory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from smfinance import SECDataSetConfig, DataLoader\n",
    "from smjsindustry.finance import DataLoader\n",
    "from smjsindustry.finance.processor_config import EDGARDataSetConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted reports will be saved to an S3 bucket for us to review. This code will also be used in the Pipeline to fetch the report for the Ticker or CIK number passed to the SageMaker Pipeline. Executing the following code cell will run a processing job which will fetch the SEC reports from the EDGAR database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='use-smjsindustry'></a>\n",
    "\n",
    "### Obtain SEC data using the SageMaker JumpStart Industry SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dataset_config = EDGARDataSetConfig(\n",
    "    tickers_or_ciks=[\"amzn\", \"goog\", \"27904\", \"FB\"],  # list of stock tickers or CIKs\n",
    "    form_types=[\"10-K\", \"10-Q\"],  # list of SEC form types\n",
    "    filing_date_start=\"2019-01-01\",  # starting filing date\n",
    "    filing_date_end=\"2020-12-31\",  # ending filing date\n",
    "    email_as_user_agent=\"test-user@test.com\",\n",
    ")  # user agent email\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    role=sagemaker.get_execution_role(),  # loading job execution role\n",
    "    instance_count=1,  # instances number, limit varies with instance type\n",
    "    instance_type=\"ml.c5.2xlarge\",  # instance type\n",
    "    volume_size_in_gb=30,  # size in GB of the EBS volume to use\n",
    "    volume_kms_key=None,  # KMS key for the processing volume\n",
    "    output_kms_key=None,  # KMS key ID for processing job outputs\n",
    "    max_runtime_in_seconds=None,  # timeout in seconds. Default is 24 hours.\n",
    "    sagemaker_session=sagemaker.Session(),  # session object\n",
    "    tags=None,\n",
    ")  # a list of key-value pairs\n",
    "\n",
    "data_loader.load(\n",
    "    dataset_config,\n",
    "    \"s3://{}/{}\".format(\n",
    "        default_bucket, \"sample-sec-data\"\n",
    "    ),  # output s3 prefix (both bucket and folder names are required)\n",
    "    \"dataset_10k_10q.csv\",  # output file name\n",
    "    wait=True,\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "\n",
    "The output of the `data_loader` processing job is a `CSV` file. We see the filings for different quarters. \n",
    "\n",
    "The filing date comes within a month of the end date of the reporting period. Both these dates are collected and displayed in the dataframe. The column `text` contains the full text of the report, but the tables are not extracted. The values in the tables in the filings are balance-sheet and income-statement data (numeric/tabular) and are easily available elsewhere as they are reported in numeric databases. The last column of the dataframe comprises the Management Discussion & Analysis section, the column is named `mdna`, which is the primary forward-looking section in the filing. This is the section that has been most widely used in financial text analysis. Therefore, we will use the `mdna` text to derive the sentiment of the overall filing in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "print(f\"{default_bucket}/{prefix}/\")\n",
    "s3_client.download_file(\n",
    "    default_bucket,\n",
    "    \"{}/{}\".format(f\"sample-sec-data\", f\"dataset_10k_10q.csv\"),\n",
    "    f\"./data/dataset_10k_10q.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_10k_10q = pd.read_csv(f\"./data/dataset_10k_10q.csv\")\n",
    "data_frame_10k_10q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Set Up Your MLOps NLP Pipeline with SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipe-pre-deploy'></a>\n",
    "\n",
    "### Step 1: Data pre-processing - extract SEC data and news about the company\n",
    "\n",
    "#### Define a processing step to prepare SEC data for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a processing step to extract 10K and 10Q forms for a specific Organization either using the company [Stock Ticker](https://www.investopedia.com/ask/answers/12/what-is-a-stock-ticker.asp) Symbol or [CIK (Central Index Key)](https://www.sec.gov/edgar/searchedgar/cik.htm) used to lookup reports in SEC's EDGAR System. You can find the company Stock Ticker Symbol to CIK Number mapping [here](https://www.sec.gov/include/ticker.txt). This step will also collect news article snippets related to the company using the NewsCatcher API.\n",
    "\n",
    "#### **<span style=\"color:lightgreen\">Important</span>**:\n",
    "\n",
    "It is recommended to use CIKs as the input. The tickers will be internally converted to CIKs according to the [mapping file](https://www.sec.gov/include/ticker.txt).  \n",
    "One ticker may map to multiple CIKs, but we only support the latest ticker to CIK mapping. Please provide the old CIKs in the input when you want historical filings. Also note that even though the Client side SDK allows you to download multiple SEC reports for multiple CIKs at a time, we will set up our data preprocessing step to grab exactly 1 SEC Report for 1 CIK (Company/Organization). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we used store magic in the previous note book script-processor-custom-container.ipynb \n",
    "to instantiate the container in the region of choice \n",
    "\"\"\"\n",
    "CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_instance_type = \"ml.c5.2xlarge\"\n",
    "create_dataset_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=CONTAINER_IMAGE_URI,\n",
    "    role=role,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a processing step to process the SEC data for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_script_uri = f\"s3://{default_bucket}/{prefix}/code/data-processing.py\"\n",
    "s3_client.upload_file(\n",
    "    Filename=\"./scripts/data-processing.py\",\n",
    "    Bucket=default_bucket,\n",
    "    Key=f\"{prefix}/code/data-processing.py\",\n",
    ")\n",
    "\n",
    "create_dataset_step = ProcessingStep(\n",
    "    name=\"HFSECFinBertCreateDataset\",\n",
    "    processor=create_dataset_processor,\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"report_data\",\n",
    "            source=\"/opt/ml/processing/output/10k10q\",\n",
    "            destination=f\"{inference_input_data}/10k10q\",\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"article_data\",\n",
    "            source=\"/opt/ml/processing/output/articles\",\n",
    "            destination=f\"{inference_input_data}/articles\",\n",
    "        ),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--ticker-cik\",\n",
    "        inference_ticker_cik,\n",
    "        \"--instance-type\",\n",
    "        loader_instance_type,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--bucket\",\n",
    "        default_bucket,\n",
    "        \"--prefix\",\n",
    "        prefix,\n",
    "        \"--role\",\n",
    "        role,\n",
    "    ],\n",
    "    code=create_dataset_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipe-pre-deploy'></a>\n",
    "\n",
    "### Step 2: Create models for summarization and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model_name = \"HFSECFinbertModel\"\n",
    "summarization_model_name = \"HFSECPegasusModel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the `finBert` model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained model using HuggingFaceModel class\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "hub = {\"HF_MODEL_ID\": \"ProsusAI/finbert\", \"HF_TASK\": \"text-classification\"}\n",
    "\n",
    "# create Hugging Face Model Class (documentation here: https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model)\n",
    "sentiment_huggingface_model = HuggingFaceModel(\n",
    "    name=sentiment_model_name,\n",
    "    transformers_version=\"4.6.1\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(instance_type=\"ml.m4.xlarge\")\n",
    "\n",
    "create_sentiment_model_step = CreateModelStep(\n",
    "    name=\"HFSECFinBertCreateModel\",\n",
    "    model=sentiment_huggingface_model,\n",
    "    inputs=inputs,\n",
    "    #     depends_on=['HFSECFinBertCreateDataset']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Pegasus summarization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = {\n",
    "    \"HF_MODEL_ID\": \"human-centered-summarization/financial-summarization-pegasus\",\n",
    "    \"HF_TASK\": \"summarization\",\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class (documentation here: https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model)\n",
    "summary_huggingface_model = HuggingFaceModel(\n",
    "    name=summarization_model_name,\n",
    "    transformers_version=\"4.6.1\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "create_summary_model_step = CreateModelStep(\n",
    "    name=\"HFSECPegasusCreateModel\",\n",
    "    model=summary_huggingface_model,\n",
    "    inputs=inputs,\n",
    "    #     depends_on=['HFSECFinBertCreateDataset']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-registry'></a>\n",
    "\n",
    "### Step 3: Register model\n",
    "\n",
    "Use HuggingFace register method to register Hugging Face Model for deployment. Set up step as a custom processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model_package_group_name = \"HuggingFaceSECSentimentModelPackageGroup\"\n",
    "summary_model_package_group_name = \"HuggingFaceSECSummaryModelPackageGroup\"\n",
    "model_approval_status = \"Approved\"\n",
    "\n",
    "register_sentiment_model_step = RegisterModel(\n",
    "    name=\"HFSECFinBertRegisterModel\",\n",
    "    model=sentiment_huggingface_model,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m4.4xlarge\"],\n",
    "    transform_instances=[\"ml.m4.4xlarge\"],\n",
    "    model_package_group_name=sentiment_model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    depends_on=[\"HFSECFinBertCreateModel\"],\n",
    ")\n",
    "\n",
    "register_summary_model_step = RegisterModel(\n",
    "    name=\"HFSECPegasusRegisterModel\",\n",
    "    model=summary_huggingface_model,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m4.4xlarge\"],\n",
    "    transform_instances=[\"ml.m4.4xlarge\"],\n",
    "    model_package_group_name=summary_model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    depends_on=[\"HFSECPegasusCreateModel\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deploy'></a>\n",
    "\n",
    "### Step 4: Deploy model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deploy the FinBert and Pegasus models from the model registry. \n",
    "\n",
    "**NOTE:** The models in the model registry are the pre-trained version from HuggingFace Model Hub. Each of the deployment step will attempt to deploy a SageMaker Endpoint with the model and will write a property file upon successful completion. The Pipeline will make use of these property files to decide whether to execute the subsequent summarization and sentiment analysis inference steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_model_instance_type = \"ml.m4.4xlarge\"\n",
    "deploy_model_instance_count = \"1\"\n",
    "\n",
    "sentiment_endpoint_name = \"HFSECFinBertModel-endpoint\"\n",
    "summarization_endpoint_name = \"HFSECPegasusModel-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"./scripts/model_deploy_v2.py\",\n",
    "    Bucket=default_bucket,\n",
    "    Key=f\"{prefix}/code/model_deploy_v2.py\",\n",
    ")\n",
    "deploy_model_script_uri = f\"s3://{default_bucket}/{prefix}/code/model_deploy_v2.py\"\n",
    "\n",
    "\n",
    "deploy_model_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=CONTAINER_IMAGE_URI,\n",
    "    role=role,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    ")\n",
    "\n",
    "sentiment_deploy_response = PropertyFile(\n",
    "    name=\"SentimentPropertyFile\",\n",
    "    output_name=\"sentiment_deploy_response\",\n",
    "    path=\"success.json\",  # the property file generated by the script\n",
    ")\n",
    "\n",
    "sentiment_deploy_step = ProcessingStep(\n",
    "    name=\"HFSECFinBertDeployModel\",\n",
    "    processor=deploy_model_processor,\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"sentiment_deploy_response\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"s3://{default_bucket}/{prefix}/nlp-pipeline/sentimentResponse\",\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--initial-instance-count\",\n",
    "        deploy_model_instance_count,\n",
    "        \"--endpoint-instance-type\",\n",
    "        deploy_model_instance_type,\n",
    "        \"--endpoint-name\",\n",
    "        sentiment_endpoint_name,\n",
    "        \"--model-package-group-name\",\n",
    "        sentiment_model_package_group_name,\n",
    "        \"--role\",\n",
    "        role,\n",
    "        \"--region\",\n",
    "        region,\n",
    "    ],\n",
    "    property_files=[sentiment_deploy_response],\n",
    "    code=deploy_model_script_uri,\n",
    "    depends_on=[\"HFSECFinBertRegisterModel\"],\n",
    ")\n",
    "\n",
    "\n",
    "summary_deploy_response = PropertyFile(\n",
    "    name=\"SummaryPropertyFile\",\n",
    "    output_name=\"summary_deploy_response\",\n",
    "    path=\"success.json\",  # the property file generated by the script\n",
    ")\n",
    "\n",
    "summary_deploy_step = ProcessingStep(\n",
    "    name=\"HFSECPegasusDeployModel\",\n",
    "    processor=deploy_model_processor,\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"summary_deploy_response\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"s3://{default_bucket}/{prefix}/nlp-pipeline/summaryResponse\",\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--initial-instance-count\",\n",
    "        deploy_model_instance_count,\n",
    "        \"--endpoint-instance-type\",\n",
    "        deploy_model_instance_type,\n",
    "        \"--endpoint-name\",\n",
    "        summarization_endpoint_name,\n",
    "        \"--model-package-group-name\",\n",
    "        summary_model_package_group_name,\n",
    "        \"--role\",\n",
    "        role,\n",
    "        \"--region\",\n",
    "        region,\n",
    "    ],\n",
    "    property_files=[summary_deploy_response],\n",
    "    code=deploy_model_script_uri,\n",
    "    depends_on=[\"HFSECPegasusRegisterModel\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pipeline conditions to check if the Endpoint deployments were successful\n",
    "\n",
    "We will define a condition that checks to see if our model deployment was successful based on the property files generated by the deployment steps of both the FinBert and Pegasus Models. If both the conditions evaluates to `True` then we will run or subsequent inferences for Summarization and Sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionEquals\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "summarize_script_uri = f\"s3://{default_bucket}/{prefix}/code/summarize.py\"\n",
    "\n",
    "sentiment_condition_eq = ConditionEquals(\n",
    "    left=JsonGet(  # the left value of the evaluation expression\n",
    "        step_name=\"HFSECFinBertDeployModel\",  # the step from which the property file will be grabbed\n",
    "        property_file=sentiment_deploy_response,  # the property file instance that was created earlier in Step 4\n",
    "        json_path=\"model_created\",  # the JSON path of the property within the property file success.json\n",
    "    ),\n",
    "    right=\"Y\",  # the right value of the evaluation expression, i.e. the AUC threshold\n",
    ")\n",
    "\n",
    "summary_condition_eq = ConditionEquals(\n",
    "    left=JsonGet(  # the left value of the evaluation expression\n",
    "        step_name=\"HFSECPegasusDeployModel\",  # the step from which the property file will be grabbed\n",
    "        property_file=summary_deploy_response,  # the property file instance that was created earlier in Step 4\n",
    "        json_path=\"model_created\",  # the JSON path of the property within the property file success.json\n",
    "    ),\n",
    "    right=\"Y\",  # the right value of the evaluation expression, i.e. the AUC threshold\n",
    ")\n",
    "\n",
    "summarize_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=CONTAINER_IMAGE_URI,\n",
    "    role=role,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    ")\n",
    "\n",
    "summarize_step_2 = ProcessingStep(\n",
    "    name=\"HFSECPegasusSummarizer_2\",\n",
    "    processor=summarize_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"summary_data\",\n",
    "            source=f\"{inference_input_data}/10k10q\",\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"summarized_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{inference_input_data}/10k10q/summary\",\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--ticker-cik\",\n",
    "        inference_ticker_cik,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-name\",\n",
    "        summarization_endpoint_name,\n",
    "    ],\n",
    "    code=summarize_script_uri,\n",
    ")\n",
    "\n",
    "deploy_condition_step = ConditionStep(\n",
    "    name=\"HFSECFinBertDeployConditionCheck\",\n",
    "    conditions=[\n",
    "        sentiment_condition_eq,\n",
    "        summary_condition_eq,\n",
    "    ],  # the equal to conditions defined above\n",
    "    if_steps=[\n",
    "        summarize_step_2\n",
    "    ],  # if the condition evaluates to true then run the summarization step\n",
    "    else_steps=[],  # there are no else steps so we will keep it empty\n",
    "    depends_on=[\n",
    "        \"HFSECFinBertDeployModel\",\n",
    "        \"HFSECPegasusDeployModel\",\n",
    "    ],  # dependencies on both Finbert and Pegasus Deployment steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Summarize SEC report step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is to make use of the Pegasus Summarizer model endpoint to summarize the MDNA text from the SEC report. Because the MDNA text is usually large, we want to derive a short summary of the overall text to be able to determine the overall sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=CONTAINER_IMAGE_URI,\n",
    "    role=role,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"./scripts/summarize.py\", Bucket=default_bucket, Key=f\"{prefix}/code/summarize.py\"\n",
    ")\n",
    "\n",
    "summarize_step_1 = ProcessingStep(\n",
    "    name=\"HFSECPegasusSummarizer_1\",\n",
    "    processor=summarize_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"summary_data\",\n",
    "            source=f\"{inference_input_data}/10k10q\",\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"summarized_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{inference_input_data}/10k10q/summary\",\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--ticker-cik\",\n",
    "        inference_ticker_cik,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-name\",\n",
    "        summarization_endpoint_name,\n",
    "    ],\n",
    "    code=summarize_script_uri,\n",
    ")\n",
    "\n",
    "summarize_step_2 = ProcessingStep(\n",
    "    name=\"HFSECPegasusSummarizer_2\",\n",
    "    processor=summarize_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"summary_data\",\n",
    "            source=f\"{inference_input_data}/10k10q\",\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"summarized_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{inference_input_data}/10k10q/summary\",\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--ticker-cik\",\n",
    "        inference_ticker_cik,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-name\",\n",
    "        summarization_endpoint_name,\n",
    "    ],\n",
    "    code=summarize_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Sentiment inference step - SEC summary and news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step uses the MDNA summary (determined by the previous step) and the news articles to find out the sentiment of the company's financial and what the Market trends are indicating. This would help us understand the overall position of the company's financial outlook and current position without leaning solely on the company's forward-looking statements and bring objective market opinions into the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=CONTAINER_IMAGE_URI,\n",
    "    role=role,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_script_uri = f\"s3://{default_bucket}/{prefix}/code/sentiment.py\"\n",
    "s3_client.upload_file(\n",
    "    Filename=\"./scripts/sentiment.py\", Bucket=default_bucket, Key=f\"{prefix}/code/sentiment.py\"\n",
    ")\n",
    "\n",
    "sentiment_step_1 = ProcessingStep(\n",
    "    name=\"HFSECFinBertSentiment_1\",\n",
    "    processor=summarize_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"sec_summary\",\n",
    "            source=f\"{inference_input_data}/10k10q/summary\",\n",
    "            destination=\"/opt/ml/processing/input/10k10q\",\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"articles\",\n",
    "            source=f\"{inference_input_data}/articles\",\n",
    "            destination=\"/opt/ml/processing/input/articles\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"sentiment_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{inference_input_data}/sentiment\",\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--ticker-cik\",\n",
    "        inference_ticker_cik,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-name\",\n",
    "        sentiment_endpoint_name,\n",
    "    ],\n",
    "    code=sentiment_script_uri,\n",
    "    depends_on=[\"HFSECPegasusSummarizer_1\"],\n",
    ")\n",
    "\n",
    "sentiment_step_2 = ProcessingStep(\n",
    "    name=\"HFSECFinBertSentiment_2\",\n",
    "    processor=summarize_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"sec_summary\",\n",
    "            source=f\"{inference_input_data}/10k10q/summary\",\n",
    "            destination=\"/opt/ml/processing/input/10k10q\",\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"articles\",\n",
    "            source=f\"{inference_input_data}/articles\",\n",
    "            destination=\"/opt/ml/processing/input/articles\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"sentiment_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{inference_input_data}/sentiment\",\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--ticker-cik\",\n",
    "        inference_ticker_cik,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-name\",\n",
    "        sentiment_endpoint_name,\n",
    "    ],\n",
    "    code=sentiment_script_uri,\n",
    "    depends_on=[\"HFSECPegasusSummarizer_2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condition Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained earlier, this is a top level condition step. This step will determine based on the value of the pipeline parameter `model_register_deploy` on whether we want to register and deploy a new version of the models and then run inference, or to simply run inference using the existing endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionEquals\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "\n",
    "condition_eq = ConditionEquals(left=model_register_deploy, right=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the condition step\n",
    "condition_step = ConditionStep(\n",
    "    name=\"HFSECFinBertConditionCheck\",\n",
    "    conditions=[condition_eq],  # the parameter is Y\n",
    "    if_steps=[\n",
    "        create_sentiment_model_step,\n",
    "        register_sentiment_model_step,\n",
    "        sentiment_deploy_step,\n",
    "        create_summary_model_step,\n",
    "        register_summary_model_step,\n",
    "        summary_deploy_step,\n",
    "    ],  # if the condition evaluates to true then create model, register, and deploy\n",
    "    else_steps=[summarize_step_1],\n",
    "    depends_on=[\"HFSECFinBertCreateDataset\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Pipeline steps and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"FinbertSECDeploymentPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        model_register_deploy,\n",
    "        inference_ticker_cik,\n",
    "        inference_input_data,\n",
    "    ],\n",
    "    steps=[\n",
    "        create_dataset_step,\n",
    "        condition_step,\n",
    "        deploy_condition_step,\n",
    "        sentiment_step_1,\n",
    "        sentiment_step_2,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start_response = pipeline.start()\n",
    "start_response.wait(delay=60, max_attempts=200)\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image shows a successful execution of the NLP end-to-end Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"./images/pipeline_execution_graph.png\" alt=\"Successful Pipeline Execution\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## View Evaluation Results\n",
    "\n",
    "Once the pipeline execution completes, we can download the evaluation data from S3 and view it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.download_file(\n",
    "    default_bucket,\n",
    "    f\"{prefix}/nlp-pipeline/inf-data/sentiment/{ticker}_sentiment_result.csv\",\n",
    "    f\"./data/{ticker}_sentiment_result.csv\",\n",
    ")\n",
    "sentiment_df = pd.read_csv(f\"./data/{ticker}_sentiment_result.csv\")\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the SageMaker Pipeline and the SageMaker Endpoints created by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_resources():\n",
    "    pipeline.delete()\n",
    "    sagemaker_boto_client.delete_endpoint(EndpointName=sentiment_endpoint_name)\n",
    "    sagemaker_boto_client.delete_endpoint(EndpointName=summarization_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}