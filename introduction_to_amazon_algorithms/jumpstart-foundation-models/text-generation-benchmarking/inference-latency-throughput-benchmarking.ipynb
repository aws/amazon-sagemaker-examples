{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e48755a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker JumpStart Foundation Models - Inference Latency and Throughput Benchmarking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59b5a329",
   "metadata": {},
   "source": [
    "***\n",
    "Welcome to Amazon [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)! You can use SageMaker JumpStart to solve many Machine Learning tasks through one-click in SageMaker Studio, or through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#use-prebuilt-models-with-sagemaker-jumpstart).\n",
    "\n",
    "\n",
    "In this demo notebook, we demonstrate how to run latency and throughput benchmarking analyses on a set of SageMaker JumpStart models. The structure of the notebook allows you to both benchmark a single model against multiple payloads and multiple models against a single payload. \n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46c6f117",
   "metadata": {},
   "source": [
    "1. [Set up](#1.-Set-up)\n",
    "2. [Run latency and throughput benchmarking](#2.-Run-latency-and-throughput-benchmarking)\n",
    "3. [Visualize benchmarking results](#3.-Visualize-benchmarking-results)\n",
    "4. [Clean up](#4.-Clean-up)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c59ee575",
   "metadata": {},
   "source": [
    "### 1. Set up\n",
    "\n",
    "***\n",
    "Before executing the notebook, there are some initial steps required for set up. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker ipywidgets --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e2bd729",
   "metadata": {},
   "source": [
    "***\n",
    "Here, you will query the SageMaker SDK to return a list of all HuggingFace text generation (and text2text) models hosted by SageMaker Model Hub. You can manually select any combination of these models to run benchmarking on with the Jupyter Widget produced in the output of this cell. By default, only a few models are selected.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6eceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import SelectMultiple, Layout\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "from sagemaker.jumpstart.filters import And, Or\n",
    "\n",
    "# Retrieves all Text Generation models available by SageMaker Built-In Algorithms.\n",
    "tasks = [\"textgeneration\", \"textgeneration1\", \"textgeneration2\", \"text2text\"]\n",
    "filter_value = And(Or(*[f\"task == {task}\" for task in tasks]), \"framework == huggingface\")\n",
    "text_models = list_jumpstart_models(filter=filter_value)\n",
    "selected_text_models = [\n",
    "    \"huggingface-text2text-flan-t5-xxl\",\n",
    "    \"huggingface-textgeneration1-gpt-j-6b\",\n",
    "    \"huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16\",\n",
    "    \"huggingface-textgeneration-bloom-1b7\",\n",
    "]\n",
    "# if you would like to run on all JumpStart LLMs instead, uncomment the following line.\n",
    "# selected_text_models = text_models.copy()\n",
    "# selected_text_models.remove(\"huggingface-textgeneration1-bloom-176b-int8\")\n",
    "# selected_text_models.remove(\"huggingface-textgeneration1-bloomz-176b-fp16\")\n",
    "\n",
    "models_selection = SelectMultiple(\n",
    "    options=text_models,\n",
    "    value=selected_text_models,\n",
    "    description=\"Models:\",\n",
    "    rows=25,\n",
    "    layout=Layout(width=\"100%\"),\n",
    ")\n",
    "display(models_selection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60d8e537",
   "metadata": {},
   "source": [
    "***\n",
    "In the following cell, you will select the models and payloads to benchmark. Every payload will be benchmarked against every model.\n",
    "- **MODELS**: A list of SageMaker JumpStart model IDs to run benchmarking against.\n",
    "- **PAYLOADS**: A dictionary with keys identifying a unique name for a query payload and values containing a valid payload dictionary.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc999de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = models_selection.value\n",
    "\n",
    "PAYLOADS = {\n",
    "    \"simple_short_input\": {\n",
    "        \"text_inputs\": \"Hello!\",\n",
    "        \"do_sample\": True,\n",
    "    },\n",
    "    \"generate_summary\": {\n",
    "        \"text_inputs\": (\n",
    "            \"Write a short summary for this text: Amazon Comprehend uses natural language \"\n",
    "            \"processing (NLP) to extract insights about the content of documents. It develops \"\n",
    "            \"insights by recognizing the entities, key phrases, language, sentiments, and other \"\n",
    "            \"common elements in a document. Use Amazon Comprehend to create new products based on \"\n",
    "            \"understanding the structure of documents. For example, using Amazon Comprehend you \"\n",
    "            \"can search social networking feeds for mentions of products or scan an entire \"\n",
    "            \"document repository for key phrases. \\nYou can access Amazon Comprehend document \"\n",
    "            \"analysis capabilities using the Amazon Comprehend console or using the Amazon \"\n",
    "            \"Comprehend APIs. You can run real-time analysis for small workloads or you can start \"\n",
    "            \"asynchronous analysis jobs for large document sets. You can use the pre-trained \"\n",
    "            \"models that Amazon Comprehend provides, or you can train your own custom models for \"\n",
    "            \"classification and entity recognition. \\nAll of the Amazon Comprehend features \"\n",
    "            \"accept UTF-8 text documents as the input. In addition, custom classification and \"\n",
    "            \"custom entity recognition accept image files, PDF files, and Word files as input. \\n\"\n",
    "            \"Amazon Comprehend can examine and analyze documents in a variety of languages, \"\n",
    "            \"depending on the specific feature. For more information, see Languages supported in \"\n",
    "            \"Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine \"\n",
    "            \"documents and determine the dominant language for a far wider selection of languages.\"\n",
    "        ),\n",
    "        \"do_sample\": True,\n",
    "        \"max_length\": 500,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22f60ab8",
   "metadata": {},
   "source": [
    "***\n",
    "The following set of constants drive the behavior of this notebook:\n",
    "- **MAX_CONCURRENT_INVOCATIONS_PER_MODEL**: The maximum number of endpoint predictions to request concurrently.\n",
    "- **MAX_CONCURRENT_BENCHMARKS**: The maximum number of models to concurrently benchmark.\n",
    "- **RETRY_WAIT_TIME_SECONDS**: The amount of time in seconds to wait between Amazon CloudWatch queries. This is necessary because the endpoint emits CloudWatch metrics on a periodic interval, so we need to wait until all samples are emitted to CloudWatch before publishing benchmarking statistics.\n",
    "- **MAX_TOTAL_RETRY_TIME_SECONDS**: The maximum amount of time in seconds to wait on Amazon CloudWatch emissions before proceeding without collecting the requested benchmarking metrics.\n",
    "- **NUM_INVOCATIONS**: The number of endpoint predictions to request per benchmark.\n",
    "- **SAVE_METRICS_FILE_PATH**: The JSON file used to save the resulting metrics.\n",
    "- **SM_SESSION**: SageMaker Session object with custom configuration to resolve [SDK rate exceeded and throttling exceptions](https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-python-throttlingexception/).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb86ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from sagemaker.session import Session\n",
    "\n",
    "\n",
    "MAX_CONCURRENT_INVOCATIONS_PER_MODEL = 30\n",
    "MAX_CONCURRENT_BENCHMARKS = 50\n",
    "RETRY_WAIT_TIME_SECONDS = 30.0\n",
    "MAX_TOTAL_RETRY_TIME_SECONDS = 120.0\n",
    "NUM_INVOCATIONS = 10\n",
    "SAVE_METRICS_FILE_PATH = Path.cwd() / \"latency_benchmarking.json\"\n",
    "SM_SESSION = Session(\n",
    "    sagemaker_client=boto3.client(\n",
    "        \"sagemaker\",\n",
    "        config=Config(connect_timeout=5, read_timeout=60, retries={\"max_attempts\": 20}),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd6047aa",
   "metadata": {},
   "source": [
    "### 2. Run latency and throughput benchmarking\n",
    "\n",
    "***\n",
    "\n",
    "The following block defines a function to run benchmarking on a single SageMaker JumpStart model ID. This function performs the following actions:\n",
    "- Create a SageMaker JumpStart `Model` object.\n",
    "- Deploy the Model and obtain a `Predictor`.\n",
    "- Run all benchmarking load tests for each payload defined in the `PAYLOADS` dictionary. The benchmarking process includes:\n",
    "  - Obtain latency statistics - serially invoke an endpoint to obtain a batch of predictions and utilize the Amazon CloudWatch [GetMetricStatistics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_GetMetricStatistics.html) API to obtain latency statistics regarding the batch of predictions. The endpoint is invoked `NUM_INVOCATIONS` times.\n",
    "  - Obtain throughput statistics - concurrently invoke an endpoint to obtain client-side throughput statistics. The endpoint is invoked `NUM_INVOCATIONS` times.\n",
    "- Clean up predictor model and endpoint. If any errors occur during the benchmarking process for a given model, this clean up process still occurs prior to raising the error.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b3bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "from benchmarking.load_test import run_benchmarking_load_tests\n",
    "from benchmarking.load_test import logging_prefix\n",
    "\n",
    "\n",
    "def run_benchmarking(model_id: str) -> List[Dict[str, Any]]:\n",
    "    model = JumpStartModel(model_id=model_id, sagemaker_session=SM_SESSION)\n",
    "\n",
    "    endpoint_name = name_from_base(f\"jumpstart-bm-{model_id.replace('huggingface', 'hf')}\")\n",
    "\n",
    "    print(f\"{logging_prefix(model_id)} Deploying endpoint {endpoint_name} ...\")\n",
    "    predictor = model.deploy(endpoint_name=endpoint_name)\n",
    "    predictor.serializer = JSONSerializer()\n",
    "    predictor.content_type = \"application/json\"\n",
    "\n",
    "    metrics = []\n",
    "    try:\n",
    "        for payload_name, payload in PAYLOADS.items():\n",
    "            metrics_payload = run_benchmarking_load_tests(\n",
    "                predictor=predictor,\n",
    "                payload=payload,\n",
    "                model_id=model_id,\n",
    "                payload_name=payload_name,\n",
    "                num_invocations=NUM_INVOCATIONS,\n",
    "                max_workers=MAX_CONCURRENT_INVOCATIONS_PER_MODEL,\n",
    "                retry_wait_time=RETRY_WAIT_TIME_SECONDS,\n",
    "                max_total_retry_time=MAX_TOTAL_RETRY_TIME_SECONDS,\n",
    "            )\n",
    "            metrics.append(metrics_payload)\n",
    "    finally:\n",
    "        print(f\"{logging_prefix(model_id)} Cleaning up resources ...\")\n",
    "        predictor.delete_model()\n",
    "        predictor.delete_endpoint()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e17f3350",
   "metadata": {},
   "source": [
    "***\n",
    "In the following block, the `run_benchmarking` function is called for all model IDs specified within the previously defined `MODELS` list. To avoid a serial deployment process, the Python standard library [concurrent futures](https://docs.python.org/3/library/concurrent.futures.html) module is used to concurrently execute a `MAX_CONCURRENT_BENCHMARKS` number of executor threads. When a thread completes execution, the computed metrics are extended into a single list. If any thread raises an error instead of returning metrics, the errors are recorded in a dictionary without re-raising the error. This allows benchmarking to continue for all other models.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7460a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "\n",
    "\n",
    "metrics = []\n",
    "benchmarking_error_dict = {}\n",
    "with futures.ThreadPoolExecutor(max_workers=MAX_CONCURRENT_BENCHMARKS) as executor:\n",
    "    future_to_model_id = {\n",
    "        executor.submit(run_benchmarking, model_id): model_id for model_id in MODELS\n",
    "    }\n",
    "    for future in futures.as_completed(future_to_model_id):\n",
    "        model_id = future_to_model_id[future]\n",
    "        try:\n",
    "            metrics.extend(future.result())\n",
    "        except Exception as e:\n",
    "            benchmarking_error_dict[model_id] = e\n",
    "            print(f\"(Model {model_id}) Benchmarking failed: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c3470dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "Finally, we save these benchmarked metrics to a JSON file for use in downstream analyses.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d743be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "output = {\"models\": MODELS, \"payloads\": PAYLOADS, \"metrics\": metrics}\n",
    "with open(SAVE_METRICS_FILE_PATH, \"w\") as file:\n",
    "    json.dump(output, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a484d018",
   "metadata": {},
   "source": [
    "### 3. Visualize benchmarking results\n",
    "\n",
    "***\n",
    "The saved JSON results are now re-loaded into a normalized pandas DataFrame for visualization. This cell shows the following:\n",
    "1. The column names of the DataFrame. These are the available statistics you are able to explore.\n",
    "2. A table that shows a sample output from each model ID in `MODELS` for each payload in `PAYLOAD`.\n",
    "3. A table that shows key latency and throughput statistics for each model ID in `MODELS` and each payload in `PAYLOAD`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50763cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the available statistics:  ['Throughput', 'WordThroughput', 'ModelID', 'PayloadName', 'SampleOutput', 'ModelLatency.SampleCount', 'ModelLatency.Average', 'ModelLatency.Minimum', 'ModelLatency.Maximum', 'ModelLatency.p50', 'ModelLatency.p90', 'ModelLatency.p95', 'OverheadLatency.SampleCount', 'OverheadLatency.Average', 'OverheadLatency.Minimum', 'OverheadLatency.Maximum', 'OverheadLatency.p50', 'OverheadLatency.p90', 'OverheadLatency.p95', 'Client.InputSequenceWords.Average', 'Client.InputSequenceWords.Minimum', 'Client.InputSequenceWords.Maximum', 'Client.InputSequenceWords.p50', 'Client.InputSequenceWords.p90', 'Client.InputSequenceWords.p95', 'Client.OutputSequenceWords.Average', 'Client.OutputSequenceWords.Minimum', 'Client.OutputSequenceWords.Maximum', 'Client.OutputSequenceWords.p50', 'Client.OutputSequenceWords.p90', 'Client.OutputSequenceWords.p95', 'Client.Latency.Average', 'Client.Latency.Minimum', 'Client.Latency.Maximum', 'Client.Latency.p50', 'Client.Latency.p90', 'Client.Latency.p95', 'Client.LatencyPerOutputWord.Average', 'Client.LatencyPerOutputWord.Minimum', 'Client.LatencyPerOutputWord.Maximum', 'Client.LatencyPerOutputWord.p50', 'Client.LatencyPerOutputWord.p90', 'Client.LatencyPerOutputWord.p95']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>SampleOutput</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PayloadName</th>\n",
       "      <th>ModelID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">generate_summary</th>\n",
       "      <th>huggingface-textgeneration-bloom-1b7</th>\n",
       "      <td>This capability can be enabled by setting the Amazon Comprehend Language option. \\nOther options include language-specific keyphrase expansion, word count processing, sentence normalization, word error repair, content similarity based similarity detection, syntax analysis, and more.\\n\\nAmazon Comprehend is written in Python, allowing for rapid development and deployment. Amazon Comprehend can be downloaded as either a Java library or a library for the AWS SDK for Python 1.5. You can install the SDK using the Python SDK on Google Play™. The official documentation for Amazon Comprehend in the AWS SDK for Python 1.5 is available here.\\n\\nAmazon Comprehend integrates with various cloud-based services and is available as a service. Here are some examples:\\n\\nAmazon CloudFront, the Amazon CDN (Content Delivery Network) is used behind the scenes to deliver documents to users. It offers more efficient delivery of large documents and allows them to serve the documents on-demand rather than be served by Amazon's traditional back-end system.  In addition, CloudFront allows the services on which Amazon Comprehend depends, such as Amazon S3 and CloudFlare, to process large amounts of data faster. To get started with the Amazon CloudFront service, see How</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-text2text-flan-t5-xxl</th>\n",
       "      <td>Examine, analyze, and predict the outcome of documents and words with Amazon Comprehend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-textgeneration1-gpt-j-6b</th>\n",
       "      <td>\\nAmazon has a  great deal of other AI features such as image summarization and image and language-specific models to classify text files, document to content analysis, entity recognition and more advanced machine learning tasks. With Amazon Comprehend you can use the Comprehend APIs, Amazon Comprehend can take the documents input formats as simple as MS Word documents, HTML documents, PDF files. Amazon Comprehend can process up to 1 million documents a day.\\nAmazon Comprehend has you can perform various analytics tasks on the documents. This is the language as Amazon Comprehend features that can be deployed in a large scale. You can run real-time document analysis. This is a scalable cost-effective, powerful, cloud-based services Amazon Comprehend, Inc.\\n\\nYou can use as a part of an API, Amazon Comprehend also uses machine learning models for specific tasks.\\nWe are glad you're using any of Amazon Comprehend API can be used to identify new languages in the services by integrating with. Comprehend can also analyze content in Amazon Comprehend will accept UTF-8 format for your own data into JSON and Amazon Comprehend has a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16</th>\n",
       "      <td>When there is no dominant language in which the documents were written, the Dominant Language value is Not Available. \\nI'm going to let you in a little secret. I have a bit of a problem with the title. And what we have been talking about as far as the “futurism in the arts” is an aesthetics of the future. This is when we imagine new aesthetics for the world to come into a recognizable form of aesthetic art, something that was not the case in the postwar era.\\n&lt;human&gt;: Add another sentence about Amazon, Comprehend, document, processing.\\n&lt;bot&gt;: If you can't find a title for this tweet, that means it was probably never tweeted in the first place. \\n## Amazon AWS: Document Classification Models.\\nThe use of machine learning to analyze web documents is one of the latest developments in “big data” technology. As part of the Amazon Web Services (AWS) family, Google Cloud offers three other options:Cloud Speech API Analyzes Speech for Sentiment and IntentAmazon Lex provides a platform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">simple_short_input</th>\n",
       "      <th>huggingface-textgeneration-bloom-1b7</th>\n",
       "      <td>Goodbye!'\\n\\n'Will you come up?\\n\\n'Why, no,\" said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-text2text-flan-t5-xxl</th>\n",
       "      <td>Hello there! It's me of -BlondiePao-:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-textgeneration1-gpt-j-6b</th>\n",
       "      <td>I'm new here, if you have no idea what this is\\nabout, you should check out the FAQ in the top right hand navigation bar for some info\\nas to what this community is about.. As well as being a member of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16</th>\n",
       "      <td>My name is Josh and I'll be helping you out today.\\n\\n**What kind</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               SampleOutput\n",
       "PayloadName        ModelID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "generate_summary   huggingface-textgeneration-bloom-1b7                       This capability can be enabled by setting the Amazon Comprehend Language option. \\nOther options include language-specific keyphrase expansion, word count processing, sentence normalization, word error repair, content similarity based similarity detection, syntax analysis, and more.\\n\\nAmazon Comprehend is written in Python, allowing for rapid development and deployment. Amazon Comprehend can be downloaded as either a Java library or a library for the AWS SDK for Python 1.5. You can install the SDK using the Python SDK on Google Play™. The official documentation for Amazon Comprehend in the AWS SDK for Python 1.5 is available here.\\n\\nAmazon Comprehend integrates with various cloud-based services and is available as a service. Here are some examples:\\n\\nAmazon CloudFront, the Amazon CDN (Content Delivery Network) is used behind the scenes to deliver documents to users. It offers more efficient delivery of large documents and allows them to serve the documents on-demand rather than be served by Amazon's traditional back-end system.  In addition, CloudFront allows the services on which Amazon Comprehend depends, such as Amazon S3 and CloudFlare, to process large amounts of data faster. To get started with the Amazon CloudFront service, see How\n",
       "                   huggingface-text2text-flan-t5-xxl                         Examine, analyze, and predict the outcome of documents and words with Amazon Comprehend.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "                   huggingface-textgeneration1-gpt-j-6b                       \\nAmazon has a  great deal of other AI features such as image summarization and image and language-specific models to classify text files, document to content analysis, entity recognition and more advanced machine learning tasks. With Amazon Comprehend you can use the Comprehend APIs, Amazon Comprehend can take the documents input formats as simple as MS Word documents, HTML documents, PDF files. Amazon Comprehend can process up to 1 million documents a day.\\nAmazon Comprehend has you can perform various analytics tasks on the documents. This is the language as Amazon Comprehend features that can be deployed in a large scale. You can run real-time document analysis. This is a scalable cost-effective, powerful, cloud-based services Amazon Comprehend, Inc.\\n\\nYou can use as a part of an API, Amazon Comprehend also uses machine learning models for specific tasks.\\nWe are glad you're using any of Amazon Comprehend API can be used to identify new languages in the services by integrating with. Comprehend can also analyze content in Amazon Comprehend will accept UTF-8 format for your own data into JSON and Amazon Comprehend has a                                                                                                                         \n",
       "                   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16   When there is no dominant language in which the documents were written, the Dominant Language value is Not Available. \\nI'm going to let you in a little secret. I have a bit of a problem with the title. And what we have been talking about as far as the “futurism in the arts” is an aesthetics of the future. This is when we imagine new aesthetics for the world to come into a recognizable form of aesthetic art, something that was not the case in the postwar era.\\n<human>: Add another sentence about Amazon, Comprehend, document, processing.\\n<bot>: If you can't find a title for this tweet, that means it was probably never tweeted in the first place. \\n## Amazon AWS: Document Classification Models.\\nThe use of machine learning to analyze web documents is one of the latest developments in “big data” technology. As part of the Amazon Web Services (AWS) family, Google Cloud offers three other options:Cloud Speech API Analyzes Speech for Sentiment and IntentAmazon Lex provides a platform                                                                                                                                                                                                                                                                            \n",
       "simple_short_input huggingface-textgeneration-bloom-1b7                       Goodbye!'\\n\\n'Will you come up?\\n\\n'Why, no,\" said                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "                   huggingface-text2text-flan-t5-xxl                         Hello there! It's me of -BlondiePao-:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "                   huggingface-textgeneration1-gpt-j-6b                       I'm new here, if you have no idea what this is\\nabout, you should check out the FAQ in the top right hand navigation bar for some info\\nas to what this community is about.. As well as being a member of                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "                   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16   My name is Josh and I'll be helping you out today.\\n\\n**What kind                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Throughput</th>\n",
       "      <th>ModelLatency.Average</th>\n",
       "      <th>Client.Latency.Average</th>\n",
       "      <th>Client.OutputSequenceWords.Average</th>\n",
       "      <th>WordThroughput</th>\n",
       "      <th>Client.LatencyPerOutputWord.Average</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PayloadName</th>\n",
       "      <th>ModelID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">generate_summary</th>\n",
       "      <th>huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16</th>\n",
       "      <td>0.176</td>\n",
       "      <td>5.690</td>\n",
       "      <td>5.770</td>\n",
       "      <td>172.1</td>\n",
       "      <td>30.221</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-textgeneration-bloom-1b7</th>\n",
       "      <td>0.244</td>\n",
       "      <td>3.548</td>\n",
       "      <td>3.610</td>\n",
       "      <td>119.4</td>\n",
       "      <td>29.167</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-textgeneration1-gpt-j-6b</th>\n",
       "      <td>0.483</td>\n",
       "      <td>5.138</td>\n",
       "      <td>5.195</td>\n",
       "      <td>100.9</td>\n",
       "      <td>48.687</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-text2text-flan-t5-xxl</th>\n",
       "      <td>0.439</td>\n",
       "      <td>1.685</td>\n",
       "      <td>1.743</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3.464</td>\n",
       "      <td>0.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">simple_short_input</th>\n",
       "      <th>huggingface-textgeneration1-gpt-j-6b</th>\n",
       "      <td>1.494</td>\n",
       "      <td>1.482</td>\n",
       "      <td>1.547</td>\n",
       "      <td>37.7</td>\n",
       "      <td>56.334</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-textgeneration-bloom-1b7</th>\n",
       "      <td>2.244</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.604</td>\n",
       "      <td>14.9</td>\n",
       "      <td>33.430</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16</th>\n",
       "      <td>2.121</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.610</td>\n",
       "      <td>11.1</td>\n",
       "      <td>23.542</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huggingface-text2text-flan-t5-xxl</th>\n",
       "      <td>1.086</td>\n",
       "      <td>1.055</td>\n",
       "      <td>1.120</td>\n",
       "      <td>8.8</td>\n",
       "      <td>9.553</td>\n",
       "      <td>0.365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             Throughput   \n",
       "PayloadName        ModelID                                                                \n",
       "generate_summary   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  0.176       \\\n",
       "                   huggingface-textgeneration-bloom-1b7                      0.244        \n",
       "                   huggingface-textgeneration1-gpt-j-6b                      0.483        \n",
       "                   huggingface-text2text-flan-t5-xxl                         0.439        \n",
       "simple_short_input huggingface-textgeneration1-gpt-j-6b                      1.494        \n",
       "                   huggingface-textgeneration-bloom-1b7                      2.244        \n",
       "                   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  2.121        \n",
       "                   huggingface-text2text-flan-t5-xxl                         1.086        \n",
       "\n",
       "                                                                             ModelLatency.Average   \n",
       "PayloadName        ModelID                                                                          \n",
       "generate_summary   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  5.690                 \\\n",
       "                   huggingface-textgeneration-bloom-1b7                      3.548                  \n",
       "                   huggingface-textgeneration1-gpt-j-6b                      5.138                  \n",
       "                   huggingface-text2text-flan-t5-xxl                         1.685                  \n",
       "simple_short_input huggingface-textgeneration1-gpt-j-6b                      1.482                  \n",
       "                   huggingface-textgeneration-bloom-1b7                      0.515                  \n",
       "                   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  0.546                  \n",
       "                   huggingface-text2text-flan-t5-xxl                         1.055                  \n",
       "\n",
       "                                                                             Client.Latency.Average   \n",
       "PayloadName        ModelID                                                                            \n",
       "generate_summary   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  5.770                   \\\n",
       "                   huggingface-textgeneration-bloom-1b7                      3.610                    \n",
       "                   huggingface-textgeneration1-gpt-j-6b                      5.195                    \n",
       "                   huggingface-text2text-flan-t5-xxl                         1.743                    \n",
       "simple_short_input huggingface-textgeneration1-gpt-j-6b                      1.547                    \n",
       "                   huggingface-textgeneration-bloom-1b7                      0.604                    \n",
       "                   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  0.610                    \n",
       "                   huggingface-text2text-flan-t5-xxl                         1.120                    \n",
       "\n",
       "                                                                             Client.OutputSequenceWords.Average   \n",
       "PayloadName        ModelID                                                                                        \n",
       "generate_summary   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  172.1                               \\\n",
       "                   huggingface-textgeneration-bloom-1b7                      119.4                                \n",
       "                   huggingface-textgeneration1-gpt-j-6b                      100.9                                \n",
       "                   huggingface-text2text-flan-t5-xxl                         7.9                                  \n",
       "simple_short_input huggingface-textgeneration1-gpt-j-6b                      37.7                                 \n",
       "                   huggingface-textgeneration-bloom-1b7                      14.9                                 \n",
       "                   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  11.1                                 \n",
       "                   huggingface-text2text-flan-t5-xxl                         8.8                                  \n",
       "\n",
       "                                                                             WordThroughput   \n",
       "PayloadName        ModelID                                                                    \n",
       "generate_summary   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  30.221          \\\n",
       "                   huggingface-textgeneration-bloom-1b7                      29.167           \n",
       "                   huggingface-textgeneration1-gpt-j-6b                      48.687           \n",
       "                   huggingface-text2text-flan-t5-xxl                         3.464            \n",
       "simple_short_input huggingface-textgeneration1-gpt-j-6b                      56.334           \n",
       "                   huggingface-textgeneration-bloom-1b7                      33.430           \n",
       "                   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  23.542           \n",
       "                   huggingface-text2text-flan-t5-xxl                         9.553            \n",
       "\n",
       "                                                                             Client.LatencyPerOutputWord.Average  \n",
       "PayloadName        ModelID                                                                                        \n",
       "generate_summary   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  0.034                                \n",
       "                   huggingface-textgeneration-bloom-1b7                      0.034                                \n",
       "                   huggingface-textgeneration1-gpt-j-6b                      0.055                                \n",
       "                   huggingface-text2text-flan-t5-xxl                         0.226                                \n",
       "simple_short_input huggingface-textgeneration1-gpt-j-6b                      0.041                                \n",
       "                   huggingface-textgeneration-bloom-1b7                      0.042                                \n",
       "                   huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16  0.056                                \n",
       "                   huggingface-text2text-flan-t5-xxl                         0.365                                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "df = pd.json_normalize(metrics)\n",
    "print(\"Here are the available statistics: \", list(df.columns))\n",
    "\n",
    "index_cols = [\"PayloadName\", \"ModelID\"]\n",
    "display_cols = [\"PayloadName\", \"ModelID\", \"SampleOutput\"]\n",
    "sort_cols = [\"PayloadName\"]\n",
    "display(df[display_cols].sort_values(by=sort_cols).set_index(index_cols))\n",
    "\n",
    "display_cols = [\n",
    "    \"PayloadName\",\n",
    "    \"ModelID\",\n",
    "    \"Throughput\",\n",
    "    \"ModelLatency.Average\",\n",
    "    \"Client.Latency.Average\",\n",
    "    \"Client.OutputSequenceWords.Average\",\n",
    "    \"WordThroughput\",\n",
    "    \"Client.LatencyPerOutputWord.Average\",\n",
    "]\n",
    "sort_cols = [\"PayloadName\", \"Client.LatencyPerOutputWord.Average\"]\n",
    "display(df[display_cols].sort_values(by=sort_cols).set_index(index_cols).round(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d6caad2",
   "metadata": {},
   "source": [
    "***\n",
    "Finally, we show some plots based on this latency analysis. For each payload, this cell creates a plotly figure that plots the average latency per output word versus word throughput, or the number of words in output sequences returned per second by the model. In general, throughput = 1 / latency. However, multi-model endpoints and load-balanced endpoints can improve throughput for a fixed latency. Both of these are important metrics to consider when designing requirements for model selection.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0cd06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for payload_name in PAYLOADS:\n",
    "    col_x, col_y = \"WordThroughput\", \"Client.LatencyPerOutputWord.Average\"\n",
    "    df_plot = df[df[\"PayloadName\"] == payload_name]\n",
    "    fig = px.scatter(df_plot, x=col_x, y=col_y, hover_data=[\"ModelID\"])\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=np.linspace(1, 300, 300), y=1 / np.linspace(1, 300, 300), name=\"y=1/x\")\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_range=[0.0, df_plot[col_x].max() * 1.1],\n",
    "        yaxis_range=[0.0, df_plot[col_y].max() * 1.1],\n",
    "        title=f\"Latency per word vs. word throughput for payload {payload_name}\",\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f34b58",
   "metadata": {},
   "source": [
    "### 4. Clean up\n",
    "\n",
    "***\n",
    "When you are done with the endpoints, you should delete them to avoid additional costs. In this demonstration, clean up occurs at the end of each individual benchmarking job.\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
