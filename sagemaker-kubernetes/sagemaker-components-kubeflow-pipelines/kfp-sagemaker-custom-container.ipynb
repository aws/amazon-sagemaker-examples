{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp --upgrade\n",
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Components for Kubeflow Pipelines Example - custom container\n",
    "In this example we'll build a Kubeflow pipeline where every component call a different Amazon SageMaker feature.\n",
    "Our simple pipeline will perform:\n",
    "\n",
    "1. Hyperparameter optimization \n",
    "1. Select best hyperparameters and increase epochs\n",
    "1. Training model on the best hyperparameters \n",
    "1. Create an Amazon SageMaker model\n",
    "1. Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp import dsl\n",
    "import time, os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Component URLs are available here: <br>\n",
    "https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_hpo_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/hyperparameter_tuning/component.yaml')\n",
    "sagemaker_train_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/train/component.yaml')\n",
    "sagemaker_model_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/model/component.yaml')\n",
    "sagemaker_deploy_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/deploy/component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = boto3.Session()\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "sm   = sess.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training datasets and upload to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "job_folder      = 'jobs'\n",
    "dataset_folder  = 'datasets'\n",
    "local_dataset = 'cifar10'\n",
    "\n",
    "# TensorFlow is required to download and convert the dataset to TFRecord format\n",
    "!python generate_cifar10_tfrecords.py --data-dir {local_dataset}\n",
    "datasets = sagemaker_session.upload_data(path='cifar10', key_prefix='datasets/cifar10-dataset')\n",
    "\n",
    "# If dataset is already in S3 use the dataset's path:\n",
    "# datasets = 's3://{bucket_name}/{dataset_folder}/cifar10-dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build your Docker container and push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------\n",
    "**STOP:** Open **`/docker/build_docker_push_to_ecr.ipynb`** and follow steps to build and push container to Amazon ECR before proceeding\n",
    "\n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a custom pipeline op\n",
    "Takes the results from a hyperparameter tuning job and increases the number of epochs for the next training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_best_model_hyperparams(hpo_results, best_model_epoch = \"80\") -> str:\n",
    "    import json\n",
    "    r = json.loads(str(hpo_results))\n",
    "    return json.dumps(dict(r,epochs=best_model_epoch))\n",
    "\n",
    "get_best_hyp_op = func_to_container_op(update_best_model_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='cifar10 hpo train deploy pipeline',\n",
    "    description='cifar10 hpo train deploy pipeline using sagemaker'\n",
    ")\n",
    "def cifar10_hpo_train_deploy(region='us-west-2',\n",
    "                           training_input_mode='File',\n",
    "                           train_image=f'{account}.dkr.ecr.us-west-2.amazonaws.com/sagemaker-kubernetes:latest',\n",
    "                           serving_image='763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:1.15.2-cpu',\n",
    "                           volume_size='50',\n",
    "                           max_run_time='86400',\n",
    "                           instance_type='ml.p3.2xlarge',\n",
    "                           network_isolation='False',\n",
    "                           traffic_encryption='False',\n",
    "                           spot_instance='False',\n",
    "                           channels='[ \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"train\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"s3://'+bucket_name+'/datasets/cifar10-dataset/train\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    }, \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"validation\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"s3://'+bucket_name+'/datasets/cifar10-dataset/validation\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    }, \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"eval\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"s3://'+bucket_name+'/datasets/cifar10-dataset/eval\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    } \\\n",
    "                ]'\n",
    "                          ):\n",
    "    # Component 1\n",
    "    hpo = sagemaker_hpo_op(\n",
    "        region=region,\n",
    "        image=train_image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        strategy='Bayesian',\n",
    "        metric_name='val_acc',\n",
    "        metric_definitions='{\"val_acc\": \"val_acc: ([0-9\\\\\\\\.]+)\"}',\n",
    "        metric_type='Maximize',\n",
    "        static_parameters='{ \\\n",
    "            \"epochs\": \"1\", \\\n",
    "            \"momentum\": \"0.9\", \\\n",
    "            \"weight-decay\": \"0.0002\", \\\n",
    "            \"model_dir\":\"s3://'+bucket_name+'/jobs\", \\\n",
    "            \"sagemaker_region\": \"us-west-2\" \\\n",
    "        }',\n",
    "        continuous_parameters='[ \\\n",
    "            {\"Name\": \"learning-rate\", \"MinValue\": \"0.0001\", \"MaxValue\": \"0.1\", \"ScalingType\": \"Logarithmic\"} \\\n",
    "        ]',\n",
    "        categorical_parameters='[ \\\n",
    "            {\"Name\": \"optimizer\", \"Values\": [\"sgd\", \"adam\"]}, \\\n",
    "            {\"Name\": \"batch-size\", \"Values\": [\"32\", \"128\", \"256\"]}, \\\n",
    "            {\"Name\": \"model-type\", \"Values\": [\"resnet\", \"custom\"]} \\\n",
    "        ]',\n",
    "        channels=channels,\n",
    "        output_location=f's3://{bucket_name}/jobs',\n",
    "        instance_type=instance_type,\n",
    "        instance_count='1',\n",
    "        volume_size=volume_size,\n",
    "        max_num_jobs='1',\n",
    "        max_parallel_jobs='1',\n",
    "        max_run_time=max_run_time,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=spot_instance,\n",
    "        role=role\n",
    "    )\n",
    "    \n",
    "    # Component 2\n",
    "    training_hyp = get_best_hyp_op(hpo.outputs['best_hyperparameters'])\n",
    "    \n",
    "    # Component 3\n",
    "    training = sagemaker_train_op(\n",
    "        region=region,\n",
    "        image=train_image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        hyperparameters=training_hyp.output,\n",
    "        channels=channels,\n",
    "        instance_type=instance_type,\n",
    "        instance_count='1',\n",
    "        volume_size=volume_size,\n",
    "        max_run_time=max_run_time,\n",
    "        model_artifact_path=f's3://{bucket_name}/jobs',\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=spot_instance,\n",
    "        role=role,\n",
    "    )\n",
    "\n",
    "    # Component 4\n",
    "    create_model = sagemaker_model_op(\n",
    "        region=region,\n",
    "        model_name=training.outputs['job_name'],\n",
    "        image=serving_image,\n",
    "        model_artifact_url=training.outputs['model_artifact_url'],\n",
    "        network_isolation=network_isolation,\n",
    "        role=role\n",
    "    )\n",
    "\n",
    "    # Component 5\n",
    "    prediction = sagemaker_deploy_op(\n",
    "        region=region,\n",
    "        model_name_1=create_model.output,\n",
    "        instance_type_1='ml.m5.large'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(cifar10_hpo_train_deploy,'sm-hpo-train-deploy-pipeline.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "aws_experiment = client.create_experiment(name='aws')\n",
    "\n",
    "exp_name    = f'cifar10-hpo-train-deploy-kfp-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}'\n",
    "my_run = client.run_pipeline(aws_experiment.id, exp_name, 'sm-hpo-train-deploy-pipeline.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, boto3\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "file_name = '1000_dog.png'\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName='Endpoint-20200502070427-8KDX', \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=payload)\n",
    "print(response['Body'].read())\n",
    "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_kfp)",
   "language": "python",
   "name": "conda_kfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
