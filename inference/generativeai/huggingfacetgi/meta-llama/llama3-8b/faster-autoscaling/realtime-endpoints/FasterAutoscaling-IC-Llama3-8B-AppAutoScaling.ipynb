{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f236c9",
   "metadata": {},
   "source": [
    "# Faster autoscaling on Amazon SageMaker realtime endpoints with inference components (Application Autoscaling)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we show how the new faster autoscaling feature helps scale sagemaker inference endpoints by almost 6x faster than earlier.\n",
    "\n",
    "We deploy Meta's `Llama3-8B-Instruct` model to an Amazon SageMaker realtime endpoint using Text Generation Inference (TGI) Deep Learning Container (DLC) and apply <span style='color:green'><b>Application Autoscaling</b></span> scaling policies to the endpoint.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Please select <b>m5.2xlarge</b> or larger instance types when running this on Amazon SageMaker Notebook Instance.<br/>\n",
    "    Select <b>conda_pytorch_p310</b> kernel when running this notebook on Amazon SageMaker Notebook Instance. <br/><br/>\n",
    "    Ensure python version for the kernel is <b>3.10.x</b> (3.11 is not supported). <br/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"border: 1px solid #f00; border-radius: 5px; padding: 10px; background-color: #fee;\">\n",
    "Before using this notebook please ensure you have access to an active access token from HuggingFace and have accepted the license agreement from Meta.\n",
    "\n",
    "- **Step 1:** Create user access token in HuggingFace (HF). Refer [here](https://huggingface.co/docs/hub/security-tokens) on how to create HF tokens.\n",
    "- **Step 2:** Login to [HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/tree/main) and navigate to *Meta-Llama-3-8B-Instruct** home page.\n",
    "- **Step 3:** Accept META LLAMA 3 COMMUNITY LICENSE AGREEMENT by following the instructions [here](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/tree/main)\n",
    "- **Step 4:** Wait for the approval email from META (Approval may take any where b/w 1-3 hrs)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a241652-091a-4769-9480-ba64b9e30c9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install packages using uv, an extremely fast python package installer\\\n",
    "Read more about uv here <https://astral.sh/blog/uv>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767c519-29c9-4794-8a4e-67cb43779697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ensure python version of the selected kernel is not greater than 3.10\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d260cb1-1355-448e-8fd9-3eebb1584ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install uv && uv pip install -U ipywidgets\n",
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abe082-5ce0-4a26-bae8-68f9bff4104c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ec770-d05e-474d-80da-d2f2bab63db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load rich extension\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97edd4-8bba-4806-bce5-c559e23da05d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import time\n",
    "from getpass import getpass\n",
    "import boto3\n",
    "import sagemaker\n",
    "from rich import print\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02d12b-2109-4f01-8da8-8972ba493398",
   "metadata": {},
   "source": [
    "## Initiate sagemaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a795d-df01-494e-b5fa-5f14971a1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = sess.sagemaker_client\n",
    "sagemaker_runtime_client = sess.sagemaker_runtime_client\n",
    "cloudwatch_client = boto3.client(\"cloudwatch\", region_name=region)\n",
    "\n",
    "hf_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# retrieve the llm image uri\n",
    "# tgi_dlc = f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\"\n",
    "tgi_dlc = get_huggingface_llm_image_uri(\"huggingface\", version=\"2.0.0\")\n",
    "\n",
    "print(f\"TGI DLC: \\n[b i green]{tgi_dlc}[/b i green]\")\n",
    "print(f\"Region: [b blue]{region}[/b blue]\")\n",
    "print(f\"Role: [b red]{role}[/b red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d06b2c-dec7-4b42-af1a-423d39f211d6",
   "metadata": {},
   "source": [
    "## Create Endpoint\n",
    "\n",
    "1. Create `EndpointConfiguration`\n",
    "2. Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8a39a-0f45-4e0e-a678-25c158a268c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an unique endpoint config name\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"llama3\")\n",
    "print(f\"prefix: {prefix}\")\n",
    "\n",
    "endpoint_config_name = f\"{prefix}-endpoint-config\"\n",
    "print(f\"Endpoint config name: {endpoint_config_name}\")\n",
    "\n",
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 2\n",
    "print(f\"Initial instance count: {initial_instance_count}\")\n",
    "print(f\"Max instance count: {max_instance_count}\")\n",
    "\n",
    "epc_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": initial_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "print(epc_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17259935-6a85-46af-89dd-b10e064a1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "\n",
    "ep_response = sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "# print(ep_response)\n",
    "print(f\"Creating endpoint: [b blue]{endpoint_name}...\")\n",
    "sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f7d75-de75-4687-a2e2-ab4aa7168ef6",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "Create and deploy model using Amazon SageMaker HuggingFace TGI DLC\n",
    "\n",
    "<https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>NOTE:</b> Remember to copy your Hugging Face Access Token from <a href=\"https://hf.co/\">https://hf.co/</a> before running the below cell.<br/><br/>\n",
    "Refer <a href=\"https://huggingface.co/docs/hub/security-tokens\">here</a> to learn about creating HF tokens.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74991ddd-3b2c-4f07-b35b-55f5d8c19ada",
   "metadata": {},
   "source": [
    "## Configure container and environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89da4d-9ce7-4e5b-a02a-3f2c690cd26d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print ecr image uri\n",
    "print(f\"llm image uri: [b green]{tgi_dlc}\")\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGING_FACE_HUB_TOKEN\") or getpass(\"Enter HUGGINGFACE Access Token: \")\n",
    "\n",
    "llama3model = {\n",
    "    \"Image\": tgi_dlc,\n",
    "    \"Environment\": {\n",
    "        \"HF_MODEL_ID\": \"meta-llama/Meta-Llama-3-8B-Instruct\",  # model_id from hf.co/models\n",
    "        \"SM_NUM_GPUS\": \"1\",  # Number of GPU used per replica\n",
    "        \"MAX_INPUT_LENGTH\": \"2048\",  # Max length of input text\n",
    "        \"MAX_TOTAL_TOKENS\": \"4096\",  # Max length of the generation (including input text)\n",
    "        \"MAX_BATCH_TOTAL_TOKENS\": \"8192\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "        \"MESSAGES_API_ENABLED\": \"true\",  # Enable the messages API\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": HF_TOKEN,\n",
    "    },\n",
    "}\n",
    "\n",
    "# create Model\n",
    "deployment_name = \"sm\"\n",
    "model_name = f\"{deployment_name}-model-llama3\"\n",
    "\n",
    "print(f\"Creating model: [b green]{model_name}...\")\n",
    "model_response = sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[llama3model],\n",
    ")\n",
    "\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93cea6-703e-4eb9-8b4d-d92b60597a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to Amazon SageMaker Inference Component\n",
    "inference_component_name_llama3b = f\"{prefix}-IC-llama3b\"\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "ic_response = sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name_llama3b,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": f\"{deployment_name}-model-llama3\",\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")\n",
    "\n",
    "# print(ic_response)\n",
    "\n",
    "# Wait for IC to come InService\n",
    "print(f\"InferenceComponent [b magenta]{inference_component_name_llama3b}...\")\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name_llama3b\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1af5c-e713-4cf8-bc23-1c96f1e61327",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Invoke and test endpoint using messages API. Refer to HF [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa5f76-5498-47a1-a443-5483c3077172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictor object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    component_name=inference_component_name_llama3b,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9ecc2-fffe-4ff1-b78b-1222fe6d32de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": hf_model_id,  # model id is required\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "chat = predictor.predict({\"messages\": messages, **parameters})\n",
    "\n",
    "# Unpack and print response\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7a5ab-0264-4b12-8243-b4aa649335b7",
   "metadata": {},
   "source": [
    "## Apply Autoscaling policies to the endpoint\n",
    "\n",
    "Apply Application Autoscaling Policy to endpoint\n",
    "\n",
    "1. Register Scalable Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf762f-beec-42ed-9ff8-5b06f76269ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "as_min_capacity = 1\n",
    "as_max_capacity = 2\n",
    "\n",
    "resource_id = f\"inference-component/{inference_component_name_llama3b}\"\n",
    "\n",
    "autoscaling_client = boto3.client(\"application-autoscaling\", region_name=region)\n",
    "\n",
    "# Register scalable target\n",
    "scalable_target = autoscaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\",\n",
    "    MinCapacity=as_min_capacity,\n",
    "    MaxCapacity=as_max_capacity,  # Replace with your desired maximum instances\n",
    ")\n",
    "\n",
    "scalable_target_arn = scalable_target[\"ScalableTargetARN\"]\n",
    "print(f\"Resource ID: [b blue]{resource_id}\")\n",
    "print(f\"Scalable_target_arn:\\n[b green]{scalable_target_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2e234-d1c7-4575-b943-5291c70c326d",
   "metadata": {},
   "source": [
    "## Use the latest high-resolution Metrics to trigger auto-scaling\n",
    "\n",
    "- New feature introduces a new <span style='color:green'><b>PredefinedMetricType</b></span> for scaling policy configuration i.e. <span style='color:green'><b>SageMakerVariantConcurrentRequestsPerModelHighResolution</b></span> to trigger scaling actions.\n",
    "- Creating a scaling policy with this metric type will create cloudwatch alarms that track a new metric called <span style='color:green'><b>ConcurrentRequestsPerModel</b></span>.\n",
    "- These high-resolution metrics are published at sub-minute intervals (10s intervals to CW + any additional jitter + delays)\n",
    "- We should observe significant improvement in scale out times with this new metric\n",
    "\n",
    "\n",
    "### Steps to create Application autoscaling policy\n",
    "\n",
    "- Create scaling policy\n",
    "  - Set `PolicyType` to `TargetTrackingScaling`\n",
    "  - Set `TargetValue` to `5.0`. i.e., Scaling triggers when endpoint receives 5 `ConcurrentRequestsPerModel`\n",
    "  - Set `PredefinedMetricType` to `SageMakerVariantConcurrentRequestsPerModelHighResolution`\n",
    "  - Set `ScaleInCoolDown` and `ScaleOutCoolDown` values to `300` seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc8c60-37cd-4852-a03d-e08149ccad17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Target Tracking Scaling Policy\n",
    "target_tracking_policy_response = autoscaling_client.put_scaling_policy(\n",
    "    PolicyName=\"SageMakerICScalingPolicy\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 5.0,  # Scaling triggers when endpoint receives 5 ConcurrentRequestsPerModel\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution\"\n",
    "        },\n",
    "        \"ScaleInCooldown\": 300,  # Cooldown period after scale-in activity\n",
    "        \"ScaleOutCooldown\": 300,  # Cooldown period after scale-out activity\n",
    "    },\n",
    ")\n",
    "\n",
    "# print(target_tracking_policy_response)\n",
    "print(f\"Policy ARN: [i blue]{target_tracking_policy_response['PolicyARN']}\")\n",
    "\n",
    "# print Cloudwatch Alarms\n",
    "alarms = target_tracking_policy_response[\"Alarms\"]\n",
    "\n",
    "for alarm in alarms:\n",
    "    print(f\"[b]Alarm Name:[/b] [b magenta]{alarm['AlarmName']}\")\n",
    "    # print(f\"[b]Alarm ARN:[/b] [i green]{alarm['AlarmARN']}[/i green]\")\n",
    "    print(\"===\" * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2d5b0-dc4b-40e3-8ada-ceddecfdac1a",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "- Deregister scalable target. This automatically deletes associated cloudwatch alarms.\n",
    "- Delete model\n",
    "- Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacabc3-2b60-41c6-b903-9de5e31fc8e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Deregister the scalable target for AAS\n",
    "    autoscaling_client.deregister_scalable_target(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    )\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] deregistered. ✅\")\n",
    "except autoscaling_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] not found!.\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting inference components: [b magenta]{inference_component_name_llama3b} ✅\")\n",
    "    # Delete inference component\n",
    "    sagemaker_client.delete_inference_component(\n",
    "        InferenceComponentName=inference_component_name_llama3b\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting model: [b magenta]{deployment_name}-model-llama3 ✅\")\n",
    "    predictor.delete_model()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting endpoint: [b magenta]{predictor.endpoint_name} ✅\")\n",
    "    predictor.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdee8f2",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
