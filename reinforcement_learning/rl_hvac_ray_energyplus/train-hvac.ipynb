{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing building HVAC with Amazon SageMaker RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "from sagemaker.rl import RLEstimator\n",
    "\n",
    "from source.common.docker_utils import build_and_push_docker_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.session.Session()\n",
    "\n",
    "# SageMaker SDK creates a default bucket. Change this bucket to your own bucket, if needed.\n",
    "s3_bucket = sm_session.default_bucket()\n",
    "\n",
    "s3_output_path = f\"s3://{s3_bucket}\"\n",
    "print(f\"S3 bucket path: {s3_output_path}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set additional training parameters\n",
    "\n",
    "### Set instance type\n",
    "\n",
    "Set `cpu_or_gpu` to either `'cpu'` or `'gpu'` for using CPU or GPU instances.\n",
    "\n",
    "### Configure the framework you want to use\n",
    "\n",
    "Set `framework` to `'tf'` or `'torch'` for TensorFlow or PyTorch, respectively.\n",
    "\n",
    "You will also have to edit your entry point i.e., `train-sagemaker-distributed.py` with the configuration parameter `\"use_pytorch\"` to match the framework that you have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix = \"energyplus-hvac-ray\"\n",
    "\n",
    "cpu_or_gpu = \"gpu\"  # has to be either cpu or gpu\n",
    "if cpu_or_gpu != \"cpu\" and cpu_or_gpu != \"gpu\":\n",
    "    raise ValueError(\"cpu_or_gpu has to be either cpu or gpu\")\n",
    "\n",
    "framework = \"tf\"\n",
    "\n",
    "instance_type = \"ml.g4dn.16xlarge\"  # g4dn.16x large has 1 GPU and 64 cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your homogeneous scaling job here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit the training code\n",
    "\n",
    "The training code is written in the file `train-sagemaker-distributed.py` which is uploaded in the /source directory.\n",
    "\n",
    "*Note that ray will automatically set `\"ray_num_cpus\"` and `\"ray_num_gpus\"` in `_get_ray_config`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize source/train-sagemaker-distributed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RL model using the Python SDK Script mode\n",
    "\n",
    "When using SageMaker for distributed training, you can select a GPU or CPU instance. The RLEstimator is used for training RL jobs.\n",
    "\n",
    "1. Specify the source directory where the environment, presets and training code is uploaded.\n",
    "2. Specify the entry point as the training code\n",
    "3. Specify the image (CPU or GPU) to be used for the training environment.\n",
    "4. Define the training parameters such as the instance count, job name, S3 path for output and job name.\n",
    "5. Define the metrics definitions that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build image\n",
    "\n",
    "repository_short_name = f\"sagemaker-hvac-ray-{cpu_or_gpu}\"\n",
    "docker_build_args = {\n",
    "    \"CPU_OR_GPU\": cpu_or_gpu,\n",
    "    \"AWS_REGION\": boto3.Session().region_name,\n",
    "    \"FRAMEWORK\": framework,\n",
    "}\n",
    "\n",
    "image_name = build_and_push_docker_image(repository_short_name, build_args=docker_build_args)\n",
    "print(\"Using ECR image %s\" % image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {\n",
    "        \"Name\": \"training_iteration\",\n",
    "        \"Regex\": \"training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"episodes_total\",\n",
    "        \"Regex\": \"episodes_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"num_steps_trained\",\n",
    "        \"Regex\": \"num_steps_trained: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"timesteps_total\",\n",
    "        \"Regex\": \"timesteps_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"training_iteration\",\n",
    "        \"Regex\": \"training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"episode_reward_max\",\n",
    "        \"Regex\": \"episode_reward_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"episode_reward_mean\",\n",
    "        \"Regex\": \"episode_reward_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"episode_reward_min\",\n",
    "        \"Regex\": \"episode_reward_min: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray homogeneous scaling - Specify `train_instance_count` > 1\n",
    "\n",
    "Homogeneous scaling allows us to use multiple instances of the same type.\n",
    "\n",
    "Spot instances are unused EC2 instances that could be used at 90% discount compared to On-Demand prices (more information about spot instances can be found [here](https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&cards.sort-order=asc) and [here](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html))\n",
    "\n",
    "To use spot instances, set `train_use_spot_instances = True`. To use On-Demand instances, `train_use_spot_instances = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    # no. of days to simulate. Remember to adjust the dates in RunPeriod of\n",
    "    # 'source/eplus/envs/buildings/MediumOffice/RefBldgMediumOfficeNew2004_Chicago.idf' to match simulation days.\n",
    "    \"n_days\": 365,\n",
    "    \"n_iter\": 50,  # no. of training iterations\n",
    "    \"algorithm\": \"APEX_DDPG\",  # only APEX_DDPG and PPO are tested\n",
    "    \"multi_zone_control\": True,  # if each zone temperature set point has to be independently controlled\n",
    "    \"energy_temp_penalty_ratio\": 10,\n",
    "}\n",
    "\n",
    "# Set additional training parameters\n",
    "training_params = {\n",
    "    \"base_job_name\": job_name_prefix,\n",
    "    \"train_instance_count\": 1,\n",
    "    \"tags\": [{\"Key\": k, \"Value\": str(v)} for k, v in hyperparameters.items()],\n",
    "}\n",
    "\n",
    "# Defining the RLEstimator\n",
    "estimator = RLEstimator(\n",
    "    entry_point=f\"train-sagemaker-hvac.py\",\n",
    "    source_dir=\"source\",\n",
    "    dependencies=[\"source/common/\"],\n",
    "    image_uri=image_name,\n",
    "    role=role,\n",
    "    train_instance_type=instance_type,\n",
    "    #                         train_instance_type='local',\n",
    "    output_path=s3_output_path,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    **training_params,\n",
    ")\n",
    "\n",
    "estimator.fit(wait=False)\n",
    "\n",
    "print(\" \")\n",
    "print(estimator.latest_training_job.job_name)\n",
    "print(\"type=\", instance_type, \"count=\", training_params[\"train_instance_count\"])\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
