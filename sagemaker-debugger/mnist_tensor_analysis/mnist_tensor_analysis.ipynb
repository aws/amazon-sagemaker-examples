{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor analysis using Amazon SageMaker Debugger\n",
    "\n",
    "Looking at the distributions of activation inputs/outputs, gradients and weights per layer can give useful insights. For instance, it helps to understand whether the model runs into problems like neuron saturation, whether there are layers in your model that are not learning at all or whether the network consists of too many layers etc. \n",
    "\n",
    "The following animation shows the distribution of gradients of a convolutional layer from an example application  as the training progresses. We can see that it starts as Gaussian distribution but then becomes more and more narrow. We can also see that the range of gradients starts very small (order of $1e-5$) and becomes even tinier as training progresses. If tiny gradients are observed from the start of training, it is an indication that we should check the hyperparameters of our model. \n",
    "\n",
    "![](images/example.gif)\n",
    "\n",
    "In this notebook we will train a poorly configured neural network and use Amazon SageMaker Debugger with custom rules to aggregate and analyse specific tensors. Before we proceed let us install the smdebug binary which allows us to perform interactive analysis in this notebook. After installing it, please restart the kernel, and when you come back skip this cell.\n",
    "\n",
    "### Installing smdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!  python -m pip install smdebug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the inputs for the training job\n",
    "\n",
    "Now we'll call the Sagemaker MXNet Estimator to kick off a training job . The `entry_point_script` points to the MXNet training script. The users can create a custom *SessionHook* in their training script. If they chose not to create such hook in the training script (similar to the one we will be using in this example) Amazon SageMaker Debugger will create the appropriate *SessionHook* based on specified *DebugHookConfig* parameters.\n",
    "\n",
    "The `hyperparameters` are the parameters that will be passed to the training script. We choose `Uniform(1)` as initializer and learning rate of `0.001`. This leads to the model not training well because the model is poorly initialized.\n",
    "\n",
    "The goal of a good intialization is \n",
    "- to break the symmetry such that parameters do not receive same gradients and updates\n",
    "- to keep variance similar across layers\n",
    "\n",
    "A bad intialization may lead to vanishing or exploiding gradients and the model not training at all. Once the training is running we will look at the distirbutions of activation inputs/outputs, gradients and weights across the training to see how these hyperparameters influenced the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "entry_point_script = \"mnist.py\"\n",
    "bad_hyperparameters = {\"initializer\": 2, \"lr\": 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker.debugger import DebuggerHookConfig, CollectionConfig\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "estimator = MXNet(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    base_job_name=\"mxnet\",\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.m5.xlarge\",\n",
    "    train_volume_size=400,\n",
    "    source_dir=\"src\",\n",
    "    entry_point=entry_point_script,\n",
    "    hyperparameters=bad_hyperparameters,\n",
    "    framework_version=\"1.6.0\",\n",
    "    py_version=\"py3\",\n",
    "    debugger_hook_config=DebuggerHookConfig(\n",
    "        collection_configs=[\n",
    "            CollectionConfig(name=\"all\", parameters={\"include_regex\": \".*\", \"save_interval\": \"100\"})\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get S3 location of tensors\n",
    "\n",
    "We can get information related to the training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "description = client.describe_training_job(TrainingJobName=job_name)\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the S3 location of the tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "path = estimator.latest_job_debugger_artifacts_path()\n",
    "print(\"Tensors are stored in: \", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status of our training job, by executing `describe_training_job`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "print(\"Training job name: {}\".format(job_name))\n",
    "\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "\n",
    "description = client.describe_training_job(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the tensors from S3 once the training job is in status `Training` or `Completed`. In the following code cell we check the job status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if description[\"TrainingJobStatus\"] != \"Completed\":\n",
    "    while description[\"SecondaryStatus\"] not in {\"Training\", \"Completed\"}:\n",
    "        description = client.describe_training_job(TrainingJobName=job_name)\n",
    "        primary_status = description[\"TrainingJobStatus\"]\n",
    "        secondary_status = description[\"SecondaryStatus\"]\n",
    "        print(\n",
    "            \"Current job status: [PrimaryStatus: {}, SecondaryStatus: {}]\".format(\n",
    "                primary_status, secondary_status\n",
    "            )\n",
    "        )\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is in status `Training` or `Completed`, we can create the trial that allows us to access the tensors in Amazon S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "trial1 = create_trial(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the available steps. A step presents one forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial1.steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As training progresses more steps will become available. \n",
    "\n",
    "Next we will access specific tensors like weights, gradients and activation outputs and plot their distributions.  We will use Amazon SageMaker Debugger and define custom rules to retrieve certain tensors. Rules are supposed to return True or False. However in this notebook we will use custom rules to store dictionaries of aggregated tensors per layer and step, which we then plot afterwards.\n",
    "\n",
    "A custom rule inherits from the smdebug Rule class and implements the function `invoke_at_step`. This function is called everytime tensors of a new step become available:\n",
    "\n",
    "```\n",
    "\n",
    "from smdebug.rules.rule import Rule\n",
    "\n",
    "class MyCustomRule(Rule):\n",
    "    def __init__(self, base_trial):\n",
    "        super().__init__(base_trial)\n",
    "        \n",
    "    def invoke_at_step(self, step):  \n",
    "        if np.max(self.base_trial.tensor('conv0_relu_output_0').value(step) < 0.001:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "```   \n",
    "\n",
    "Above example rule checks if the first convolutional layer outputs only small values. If so the rule returns `True` which corresponds to an `Issue found`, otherwise False `No Issue found`.\n",
    "\n",
    "\n",
    "### Activation outputs\n",
    "This rule will use Amazon SageMaker Debugger to retrieve tensors from the ReLU output layers. It sums the activations across batch and steps. If there is a large fraction of ReLUs outputing 0 across many steps it means that the neuron is dying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "from smdebug.rules.rule_invoker import invoke_rule\n",
    "from smdebug.exceptions import NoMoreData\n",
    "from smdebug.rules.rule import Rule\n",
    "import numpy as np\n",
    "import utils\n",
    "import collections\n",
    "import os\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ActivationOutputs(Rule):\n",
    "    def __init__(self, base_trial):\n",
    "        super().__init__(base_trial)\n",
    "        self.tensors = collections.OrderedDict()\n",
    "\n",
    "    def invoke_at_step(self, step):\n",
    "        for tname in self.base_trial.tensor_names(regex=\".*relu_output\"):\n",
    "            if \"gradients\" not in tname:\n",
    "                try:\n",
    "                    tensor = self.base_trial.tensor(tname).value(step)\n",
    "                    if tname not in self.tensors:\n",
    "                        self.tensors[tname] = collections.OrderedDict()\n",
    "                    if step not in self.tensors[tname]:\n",
    "                        self.tensors[tname][step] = 0\n",
    "                    neg_values = np.where(tensor <= 0)[0]\n",
    "                    if len(neg_values) > 0:\n",
    "                        self.logger.info(\n",
    "                            f\" Step {step} tensor  {tname}  has {len(neg_values)/tensor.size*100}% activation outputs which are smaller than 0 \"\n",
    "                        )\n",
    "                    batch_over_sum = np.sum(tensor, axis=0) / tensor.shape[0]\n",
    "                    self.tensors[tname][step] += batch_over_sum\n",
    "                except:\n",
    "                    self.logger.warning(f\"Can not fetch tensor {tname}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "rule = ActivationOutputs(trial1)\n",
    "try:\n",
    "    invoke_rule(rule)\n",
    "except NoMoreData:\n",
    "    print(\n",
    "        \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "utils.create_interactive_matplotlib_histogram(\n",
    "    rule.tensors, filename=\"images/activation_outputs.gif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"images/activation_outputs.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Inputs\n",
    "In this rule we look at the inputs into activation function, rather than the output. This can be helpful to understand if there are extreme negative or positive values that saturate the activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ActivationInputs(Rule):\n",
    "    def __init__(self, base_trial):\n",
    "        super().__init__(base_trial)\n",
    "        self.tensors = collections.OrderedDict()\n",
    "\n",
    "    def invoke_at_step(self, step):\n",
    "        for tname in self.base_trial.tensor_names(regex=\".*relu_input\"):\n",
    "            if \"gradients\" not in tname:\n",
    "                try:\n",
    "                    tensor = self.base_trial.tensor(tname).value(step)\n",
    "                    if tname not in self.tensors:\n",
    "                        self.tensors[tname] = {}\n",
    "                    if step not in self.tensors[tname]:\n",
    "                        self.tensors[tname][step] = 0\n",
    "                    neg_values = np.where(tensor <= 0)[0]\n",
    "                    if len(neg_values) > 0:\n",
    "                        self.logger.info(\n",
    "                            f\" Tensor  {tname}  has {len(neg_values)/tensor.size*100}% activation inputs which are smaller than 0 \"\n",
    "                        )\n",
    "                    batch_over_sum = np.sum(tensor, axis=0) / tensor.shape[0]\n",
    "                    self.tensors[tname][step] += batch_over_sum\n",
    "                except:\n",
    "                    self.logger.warning(f\"Can not fetch tensor {tname}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "rule = ActivationInputs(trial1)\n",
    "try:\n",
    "    invoke_rule(rule)\n",
    "except NoMoreData:\n",
    "    print(\n",
    "        \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "utils.create_interactive_matplotlib_histogram(rule.tensors, filename=\"images/activation_inputs.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that second convolutional layer `conv1_relu_input_0` receives only negative input values, which means that all ReLUs in this layer output 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"images/activation_inputs.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "The following code retrieves the gradients and plots their distribution. If variance is tiny, that means that the model parameters do not get updated effectively with each training step or that the training has converged to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GradientsLayer(Rule):\n",
    "    def __init__(self, base_trial):\n",
    "        super().__init__(base_trial)\n",
    "        self.tensors = collections.OrderedDict()\n",
    "\n",
    "    def invoke_at_step(self, step):\n",
    "        for tname in self.base_trial.tensor_names(regex=\".*gradient\"):\n",
    "            try:\n",
    "                tensor = self.base_trial.tensor(tname).value(step)\n",
    "                if tname not in self.tensors:\n",
    "                    self.tensors[tname] = {}\n",
    "\n",
    "                self.logger.info(\n",
    "                    f\" Tensor  {tname}  has gradients range: {np.min(tensor)} {np.max(tensor)} \"\n",
    "                )\n",
    "                self.tensors[tname][step] = tensor\n",
    "            except:\n",
    "                self.logger.warning(f\"Can not fetch tensor {tname}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "rule = GradientsLayer(trial1)\n",
    "try:\n",
    "    invoke_rule(rule)\n",
    "except NoMoreData:\n",
    "    print(\n",
    "        \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "utils.create_interactive_matplotlib_histogram(rule.tensors, filename=\"images/gradients.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"images/gradients.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check variance across layers\n",
    "The rule retrieves gradients, but this time we compare variance of gradient distribution across layers. We want to identify if there is a large difference between the min and max variance per training step. For instance, very deep neural networks may suffer from vanishing gradients the deeper we go. By checking this ratio we can determine if we run into such a situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GradientsAcrossLayers(Rule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_trial,\n",
    "    ):\n",
    "        super().__init__(base_trial)\n",
    "        self.tensors = collections.OrderedDict()\n",
    "\n",
    "    def invoke_at_step(self, step):\n",
    "        for tname in self.base_trial.tensor_names(regex=\".*gradient\"):\n",
    "            try:\n",
    "                tensor = self.base_trial.tensor(tname).value(step)\n",
    "                if step not in self.tensors:\n",
    "                    self.tensors[step] = [np.inf, 0]\n",
    "                variance = np.var(tensor.flatten())\n",
    "                if variance < self.tensors[step][0]:\n",
    "                    self.tensors[step][0] = variance\n",
    "                elif variance > self.tensors[step][1]:\n",
    "                    self.tensors[step][1] = variance\n",
    "                self.logger.info(\n",
    "                    f\" Step {step} current ratio: {self.tensors[step][0]} {self.tensors[step][1]} Ratio: {self.tensors[step][1] / self.tensors[step][0]}\"\n",
    "                )\n",
    "            except:\n",
    "                self.logger.warning(f\"Can not fetch tensor {tname}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "rule = GradientsAcrossLayers(trial1)\n",
    "try:\n",
    "    invoke_rule(rule)\n",
    "except NoMoreData:\n",
    "    print(\n",
    "        \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check min and max values of the gradients across layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for step in rule.tensors:\n",
    "    print(\n",
    "        \"Step\",\n",
    "        step,\n",
    "        \"variance of gradients: \",\n",
    "        rule.tensors[step][0],\n",
    "        \" to \",\n",
    "        rule.tensors[step][1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of weights\n",
    "This rule retrieves the weight tensors and checks the variance. If the distribution does not change much across steps it may indicate that the learning rate is too low, that gradients are too small or that the training has converged to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class WeightRatio(Rule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_trial,\n",
    "    ):\n",
    "        super().__init__(base_trial)\n",
    "        self.tensors = collections.OrderedDict()\n",
    "\n",
    "    def invoke_at_step(self, step):\n",
    "        for tname in self.base_trial.tensor_names(regex=\".*weight\"):\n",
    "            if \"gradient\" not in tname:\n",
    "                try:\n",
    "                    tensor = self.base_trial.tensor(tname).value(step)\n",
    "                    if tname not in self.tensors:\n",
    "                        self.tensors[tname] = {}\n",
    "\n",
    "                    self.logger.info(\n",
    "                        f\" Tensor  {tname}  has weights with variance: {np.var(tensor.flatten())} \"\n",
    "                    )\n",
    "                    self.tensors[tname][step] = tensor\n",
    "                except:\n",
    "                    self.logger.warning(f\"Can not fetch tensor {tname}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "rule = WeightRatio(trial1)\n",
    "try:\n",
    "    invoke_rule(rule)\n",
    "except NoMoreData:\n",
    "    print(\n",
    "        \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "utils.create_interactive_matplotlib_histogram(rule.tensors, filename=\"images/weights.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"images/weights.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "This rule retrieves layer inputs excluding activation inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Inputs(Rule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_trial,\n",
    "    ):\n",
    "        super().__init__(base_trial)\n",
    "        self.tensors = collections.OrderedDict()\n",
    "\n",
    "    def invoke_at_step(self, step):\n",
    "        for tname in self.base_trial.tensor_names(regex=\".*input\"):\n",
    "            if \"relu\" not in tname:\n",
    "                try:\n",
    "                    tensor = self.base_trial.tensor(tname).value(step)\n",
    "                    if tname not in self.tensors:\n",
    "                        self.tensors[tname] = {}\n",
    "\n",
    "                    self.logger.info(\n",
    "                        f\" Tensor  {tname}  has inputs with variance: {np.var(tensor.flatten())} \"\n",
    "                    )\n",
    "                    self.tensors[tname][step] = tensor\n",
    "                except:\n",
    "                    self.logger.warning(f\"Can not fetch tensor {tname}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "rule = Inputs(trial1)\n",
    "try:\n",
    "    invoke_rule(rule)\n",
    "except NoMoreData:\n",
    "    print(\n",
    "        \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_interactive_matplotlib_histogram(rule.tensors, filename=\"images/layer_inputs.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"images/layer_inputs.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer outputs\n",
    "This rule retrieves outputs of layers excluding activation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Outputs(Rule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_trial,\n",
    "    ):\n",
    "        super().__init__(base_trial)\n",
    "        self.tensors = collections.OrderedDict()\n",
    "\n",
    "    def invoke_at_step(self, step):\n",
    "        for tname in self.base_trial.tensor_names(regex=\".*output\"):\n",
    "            if \"relu\" not in tname:\n",
    "                try:\n",
    "                    tensor = self.base_trial.tensor(tname).value(step)\n",
    "                    if tname not in self.tensors:\n",
    "                        self.tensors[tname] = {}\n",
    "\n",
    "                    self.logger.info(\n",
    "                        f\" Tensor  {tname}  has inputs with variance: {np.var(tensor.flatten())} \"\n",
    "                    )\n",
    "                    self.tensors[tname][step] = tensor\n",
    "                except:\n",
    "                    self.logger.warning(f\"Can not fetch tensor {tname}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "rule = Outputs(trial1)\n",
    "try:\n",
    "    invoke_rule(rule)\n",
    "except NoMoreData:\n",
    "    print(\n",
    "        \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "utils.create_interactive_matplotlib_histogram(rule.tensors, filename=\"images/layer_outputs.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Image(url=\"images/layer_outputs.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison \n",
    "In the previous section we have looked at the distribution of gradients, activation outputs and weights of a model that has not trained well due to poor initialization. Now we will compare some of these distributions with a model that has been well intialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point_script = \"mnist.py\"\n",
    "hyperparameters = {\"lr\": 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MXNet(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    base_job_name=\"mxnet\",\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.m5.xlarge\",\n",
    "    train_volume_size=400,\n",
    "    source_dir=\"src\",\n",
    "    entry_point=entry_point_script,\n",
    "    hyperparameters=hyperparameters,\n",
    "    framework_version=\"1.6.0\",\n",
    "    py_version=\"py3\",\n",
    "    debugger_hook_config=DebuggerHookConfig(\n",
    "        collection_configs=[\n",
    "            CollectionConfig(name=\"all\", parameters={\"include_regex\": \".*\", \"save_interval\": \"100\"})\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get S3 path where tensors have been stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = estimator.latest_job_debugger_artifacts_path()\n",
    "print(\"Tensors are stored in: \", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "print(\"Training job name: {}\".format(job_name))\n",
    "\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "\n",
    "description = client.describe_training_job(TrainingJobName=job_name)\n",
    "\n",
    "if description[\"TrainingJobStatus\"] != \"Completed\":\n",
    "    while description[\"SecondaryStatus\"] not in {\"Training\", \"Completed\"}:\n",
    "        description = client.describe_training_job(TrainingJobName=job_name)\n",
    "        primary_status = description[\"TrainingJobStatus\"]\n",
    "        secondary_status = description[\"SecondaryStatus\"]\n",
    "        print(\n",
    "            \"Current job status: [PrimaryStatus: {}, SecondaryStatus: {}]\".format(\n",
    "                primary_status, secondary_status\n",
    "            )\n",
    "        )\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new trial object `trial2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "trial2 = create_trial(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients\n",
    "\n",
    "Lets compare distribution of gradients of the convolutional layers of both trials. `trial` is the trial object of the first training job, `trial2` is the trial object of second training job. We can now easily compare tensors from both training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = GradientsLayer(trial1)\n",
    "try:\n",
    "    invoke_rule(rule)\n",
    "except NoMoreData:\n",
    "    print(\n",
    "        \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_gradients = {}\n",
    "dict_gradients[\"gradient/conv0_weight_bad_hyperparameters\"] = rule.tensors[\"gradient/conv0_weight\"]\n",
    "dict_gradients[\"gradient/conv1_weight_bad_hyperparameters\"] = rule.tensors[\"gradient/conv1_weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = GradientsLayer(trial2)\n",
    "try:\n",
    "    invoke_rule(rule)\n",
    "except NoMoreData:\n",
    "    print(\n",
    "        \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_gradients[\"gradient/conv0_weight_good_hyperparameters\"] = rule.tensors[\"gradient/conv0_weight\"]\n",
    "dict_gradients[\"gradient/conv1_weight_good_hyperparameters\"] = rule.tensors[\"gradient/conv1_weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_interactive_matplotlib_histogram(\n",
    "    dict_gradients, filename=\"images/gradients_comparison.gif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the poorly initalized model, gradients are fluctuating a lot leading to very high variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"images/gradients_comparison.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation inputs\n",
    "\n",
    "Lets compare distribution of activation inputs of both trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = ActivationInputs(trial1)\n",
    "try:\n",
    "    invoke_rule(rule)\n",
    "except NoMoreData:\n",
    "    print(\n",
    "        \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_activation_inputs = {}\n",
    "dict_activation_inputs[\"conv0_relu_input_0_bad_hyperparameters\"] = rule.tensors[\n",
    "    \"conv0_relu_input_0\"\n",
    "]\n",
    "dict_activation_inputs[\"conv1_relu_input_0_bad_hyperparameters\"] = rule.tensors[\n",
    "    \"conv1_relu_input_0\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = ActivationInputs(trial2)\n",
    "try:\n",
    "    invoke_rule(rule)\n",
    "except NoMoreData:\n",
    "    print(\n",
    "        \"The training has ended and there is no more data to be analyzed. This is expected behavior.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_activation_inputs[\"conv0_relu_input_0_good_hyperparameters\"] = rule.tensors[\n",
    "    \"conv0_relu_input_0\"\n",
    "]\n",
    "dict_activation_inputs[\"conv1_relu_input_0_good_hyperparameters\"] = rule.tensors[\n",
    "    \"conv1_relu_input_0\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_interactive_matplotlib_histogram(\n",
    "    dict_activation_inputs, filename=\"images/activation_inputs_comparison.gif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of activation inputs into first activation layer `conv0_relu_input_0` look quite similar in both trials. However in the case of the second layer they drastically differ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"images/activation_inputs_comparison.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
