{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74b1250",
   "metadata": {},
   "source": [
    "For ease of use, we advice to open this notebook in an Amazon SageMaker instance and use the conda_pytorch_latest_p36 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "!pip install datasets\n",
    "!pip install py7zr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eccbff",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5dc3f",
   "metadata": {},
   "source": [
    "One way to prepare your dataset for training on Amazon SageMaker is to have your training, validation and test datasets saved separately. This enables to effectively decouple data preparation from training in an architecture and for example ensure that the same datasets can be reused by different models with the same split. In this example we download the [samsum dataset](https://arxiv.org/pdf/1911.12237.pdf) and prepare it for HuggingFace using the [datasets](https://github.com/huggingface/datasets) library. Any dataset containing text and summaries could work here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd5f1f",
   "metadata": {},
   "source": [
    "We first import required packages and define the prefix where to save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import io, boto3, sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, filesystems, DatasetDict\n",
    "\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "session = sagemaker.Session()\n",
    "session_bucket = session.default_bucket()\n",
    "\n",
    "s3_prefix = 'samsum-dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba2d28",
   "metadata": {},
   "source": [
    "Download the samsum dataset using curl. If you would like to use your own custom dataset, you do not require to run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed4b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir corpus && cd corpus\n",
    "curl https://arxiv.org/src/1911.12237v2/anc/corpus.7z --output corpus.7z\n",
    "py7zr x corpus.7z\n",
    "rm corpus.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667fdd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the json files to jsonlines in order to save it in Hugging Face dataset format for optimal speed and efficiency\n",
    "\n",
    "data_path = 'corpus/'\n",
    "\n",
    "frames = []\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith('.json'):\n",
    "        with open(os.path.join(data_path, file)) as f:\n",
    "            json_dict = json.load(f)\n",
    "            with open(os.path.join(data_path, file.replace('.json', '.jsonl')), 'w') as f:\n",
    "                f.write('\\n'.join(map(json.dumps, json_dict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO USE WITH YOUR OWN CUSTOM DATASET PLEASE UNCOMMENT\n",
    "# If you would like to use your own custom dataset (single CSV/JSON), you can use the datasets.Dataset.train_test_split() method  to shuffle and split your data. \n",
    "# The splits will be shuffled by default. You can deactivate this behavior by setting shuffle=False\n",
    "\n",
    "\n",
    "# # For single JSON file\n",
    "# dataset_json = load_dataset('json', data_files='path_to_your_file', split ='train') #\n",
    "\n",
    "# # Replace type to 'csv' if you are using a single CSV file, the rest of the steps are exactly the same\n",
    "# # dataset_csv = load_dataset('csv', data_files='path_to_your_file', split ='train') # path to your file\n",
    "\n",
    "\n",
    "# # Split into 70% train, 30% test + validation\n",
    "# train_test_validation = dataset_json.train_test_split(test_size=0.3)\n",
    "\n",
    "# # Split 30% test + validation into half test, half validation\n",
    "# test_validation = train_test_validation['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# # Gather the splits  to have a single DatasetDict\n",
    "\n",
    "# train_test_valid_dataset = DatasetDict({\n",
    "#     'train': train_test_validation['train'],\n",
    "#     'validation': test_validation['train'],\n",
    "#     'test': test_validation['test'],})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb33ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using the samsum dataset that is already split, you can simply load the separate files\n",
    "\n",
    "dataset = load_dataset('json', data_files={'train': ['corpus/train.jsonl'],\n",
    "                                              'validation' : 'corpus/val.jsonl',\n",
    "                                              'test': 'corpus/test.jsonl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DIALOGUE\\n{dialogue}'.format(dialogue=dataset['train']['dialogue'][0]))\n",
    "print('\\nSUMMARY\\n{summary}'.format(summary=dataset['train']['summary'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5262b",
   "metadata": {},
   "source": [
    "Finally we write the training, validation and test dataframes to separate CSVs and upload them to S3.\n",
    "\n",
    "This will then be used in the 02_finetune_deploy.ipynb notebook for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51006ac",
   "metadata": {},
   "source": [
    "##### Use the save_to_disk method to directly save your dataset to S3 in Hugging Face dataset format. The format is backed by the Apache Arrow format which enables processing of large datasets with zero-copy reads without any memory constraints for optimal speed and efficiency.  You can use the load_to_disk method in your train script to directly load the dataset in the format it was saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = filesystems.S3FileSystem()\n",
    "dataset.save_to_disk(f's3://{session_bucket}/{s3_prefix}/train/', fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00247b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
