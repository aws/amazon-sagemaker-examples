{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model on Tabular Data using Amazon SageMaker\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to train a machine learning model via Amazon SageMaker using tabular data. In this notebook you can train either an XGBoost or Linear Learner (regression) model on tabular data in Amazon SageMaker. \n",
    "\n",
    "#### Prerequisite\n",
    "This notebook is a sequel to the [01_preprocessing_tabular_data.ipynb](01_preprocessing_tabular_data.ipynb)  and [02_feature_selection_tabular_data.ipynb](02_feature_selection_tabular_data.ipynb) notebooks. Before running this notebook, run [01_preprocessing_tabular_data.ipynb](01_preprocessing_tabular_data.ipynb) to preprocess the data used in this notebook and then [02_feature_selection_tabular_data.ipynb](02_feature_selection_tabular_data.ipynb) that selects important features.\n",
    "\n",
    "#### Tabular Data Sets\n",
    "* [california house data](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)\n",
    "* [diabetes data ](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html)\n",
    "\n",
    "\n",
    "#### Library Dependencies:\n",
    "* sagemaker >= 2.0.0\n",
    "* numpy \n",
    "* pandas\n",
    "* plotly\n",
    "* sklearn \n",
    "* matplotlib \n",
    "* seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets import *\n",
    "import ast\n",
    "import sklearn.model_selection\n",
    "\n",
    "## SageMaker dependencies\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "## This instantiates a SageMaker session that we will be operating in.\n",
    "session = sagemaker.Session()\n",
    "\n",
    "## This object represents the IAM role that we are assigned.\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Relevant Variables from preprocessing_tabular_data.ipynb and feature_engineering_tabular_data.ipynb (Required for this notebook)\n",
    "Here we load in our training, test, and validation data sets. We preprocessed this data in the [01_preprocessing_tabular_data.ipynb](01_preprocessing_tabular_data.ipynb) and did feature selection [02_feature_selection_tabular_data.ipynb](02_feature_selection_tabular_data.ipynb) in persisted it using storemagic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant dataframes and variables from 01_preprocessing_tabular_data.ipynb required for this notebook\n",
    "%store -r X_train\n",
    "%store -r X_test\n",
    "%store -r X_val\n",
    "%store -r Y_train\n",
    "%store -r Y_test\n",
    "%store -r Y_val\n",
    "%store -r choosen_data_set\n",
    "%store -r sagemaker_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Uploading the data to S3\n",
    "Here we upload our training and validation data to an S3 bucket. This is a critical step because we will be specifying this S3 bucket's location during the training step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\" + choosen_data_set\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "prefix = choosen_data_set + \"-deploy-hl\"\n",
    "pd.concat([Y_train, X_train], axis=1).to_csv(\n",
    "    os.path.join(data_dir, \"train.csv\"), header=False, index=False\n",
    ")\n",
    "pd.concat([Y_val, X_val], axis=1).to_csv(\n",
    "    os.path.join(data_dir, \"validation.csv\"), header=False, index=False\n",
    ")\n",
    "\n",
    "val_location = session.upload_data(os.path.join(data_dir, \"validation.csv\"), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, \"train.csv\"), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a pointer to our training and validation data sets stored in an S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = TrainingInput(s3_data=train_location, content_type=\"text/csv\")\n",
    "s3_input_validation = TrainingInput(s3_data=val_location, content_type=\"text/csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Select and Train the Model\n",
    "Select between the XGBoost or Linear Learner algorithm by assigning model_selected to either 'xgboost' or 'linear-learner'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select between xgboost or linear-learner (regression)\n",
    "models = [\"xgboost\", \"linear-learner\"]\n",
    "model_selected = \"xgboost\"\n",
    "assert model_selected in models\n",
    "print(\"Selected model:\", model_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we retrieve our container and instantiate our model object using the Estimator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = retrieve(framework=model_selected, region=session.boto_region_name, version=\"latest\")\n",
    "\n",
    "model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(session.default_bucket(), prefix),\n",
    "    sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set hyperparameters\n",
    "Thus far, we have instantiated our model with our container and uploaded our preprocessed data to our S3 bucket. \n",
    "Next, we set our hyperparameters for our choosen model. We note that both [XGBoost](https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/xgboost_hyperparameters.html) and [linear learner](https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/ll_hyperparameters.html) have different hyperparameters that can be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_selected == \"xgboost\":\n",
    "    model.set_hyperparameters(\n",
    "        max_depth=5,\n",
    "        eta=0.2,\n",
    "        gamma=4,\n",
    "        min_child_weight=6,\n",
    "        subsample=0.8,\n",
    "        objective=\"reg:linear\",\n",
    "        early_stopping_rounds=10,\n",
    "        num_round=1,\n",
    "    )\n",
    "\n",
    "if model_selected == \"linear-learner\":\n",
    "    model.set_hyperparameters(\n",
    "        feature_dim=X_train.shape[1], predictor_type=\"regressor\", mini_batch_size=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our estimator object is instantiated with hyperparameter settings, now it is time to train! To do this we specify our S3 bucket's location that is storing our training data and validation data and pass it via a dictionary to the fit method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit({\"train\": s3_input_train, \"validation\": s3_input_validation}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Save Trained Model\n",
    "The model has been trained. Below we select and download the model we just trained above. \n",
    "\n",
    "To download the last trained model we assign the `s3_uri` parameter to be `model.model_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.s3.S3Downloader.download(s3_uri=model.model_data, local_path=\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we safe guard your kernel environment by installing your original sagemaker version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sagemaker_version != None:\n",
    "    !{sys.executable} -m pip install -qU sagemaker_version"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
