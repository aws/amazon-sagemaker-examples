{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Trends in Company Valuation with NLP - Part 3: NLP Company Earnings Analysis using SageMaker Asynchronous Inference and AWS Lambda\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Orchestrating company earnings trend analysis, using SEC filings, news sentiment with the Hugging Face transformers, and Amazon SageMaker Pipelines\n",
    "\n",
    "In this notebook, we demonstrate how to summarize and derive sentiments out of Security and Exchange Commission reports filed by a publicly traded organization. We will derive the overall market sentiments about the said organization through financial news articles within the same financial period to present a fair view of the organization vs. market sentiments and outlook about the company's overall valuation and performance. In addition to this we will also identify the most popular keywords and entities within the news articles about that organization.\n",
    "\n",
    "In order to achieve the above we will be using multiple SageMaker Hugging Face based NLP transformers for the downstream NLP tasks of Summarization (e.g., of the news and SEC MDNA sections) and Sentiment Analysis (of the resulting summaries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Using SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Pipelines is the first purpose-built, easy-to-use continuous integration and continuous delivery (CI/CD) service for machine learning (ML). With SageMaker Pipelines, you can create, automate, and manage end-to-end ML workflows at scale. \n",
    "\n",
    "Orchestrating workflows across each step of the machine learning process (e.g. exploring and preparing data, experimenting with different algorithms and parameters, training and tuning models, and deploying models to production) can take months of coding.\n",
    "\n",
    "Since it is purpose-built for machine learning, SageMaker Pipelines helps you automate different steps of the ML workflow, including data loading, data transformation, training and tuning, and deployment. With SageMaker Pipelines, you can build dozens of ML models a week, manage massive volumes of data, thousands of training experiments, and hundreds of different model versions. You can share and re-use workflows to recreate or optimize models, helping you scale ML throughout your organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Understanding trends in company valuation (or similar) with NLP\n",
    "\n",
    "**Natural language processing (NLP)** is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. (Source: [Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing))\n",
    "\n",
    "We are going to demonstrate how to summarize and derive sentiments out of Security and Exchange Commission reports filed by a publicly traded organization. We are also going to derive the overall market sentiments about the said organization through financial news articles within the same financial period to present a fair view of the organization vs. market sentiments and outlook about the company's overall valuation and performance. In addition to this we will also identify the most popular keywords and entities within the news articles about that organization.\n",
    "\n",
    "In order to achieve the above we will be using multiple SageMaker Hugging Face based NLP transformers with summarization and sentiment analysis downstream tasks.\n",
    "\n",
    "* <b> Summarization of financial text from SEC reports and news articles </b> will be done via [Pegasus for Financial Summarization model](https://huggingface.co/human-centered-summarization/financial-summarization-pegasus) based on the paper [Towards Human-Centered Summarization: A Case Study on Financial News](https://aclanthology.org/2021.hcinlp-1.4/). \n",
    "* Sentiment analysis on summarized SEC financial report and news articles will be done via pre-trained NLP model to analyze sentiment of financial text called [FinBERT](https://huggingface.co/ProsusAI/finbert). Paper: [ FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063)\n",
    "\n",
    "---\n",
    "\n",
    "### SEC Dataset\n",
    "\n",
    "The starting point for a vast amount of financial NLP is text in SEC filings. The SEC requires companies to report different types of information related to various events involving companies. The full list of SEC forms is here: https://www.sec.gov/forms.\n",
    "\n",
    "SEC filings are widely used by financial services companies as a source of information about companies in order to make trading, lending, investment, and risk management decisions. Because these filings are required by regulation, they are of high quality and veracity. They contain forward-looking information that helps with forecasts and are written with a view to the future, required by regulation. In addition, in recent times, the value of historical time-series data has degraded, since economies have been structurally transformed by trade wars, pandemics, and political upheavals. Therefore, text as a source of forward-looking information has been increasing in relevance. \n",
    "\n",
    "#### Obtain the dataset using the SageMaker JumpStart Industry Python SDK\n",
    "\n",
    "Downloading SEC filings is done from the SEC's Electronic Data Gathering, Analysis, and Retrieval (EDGAR) website, which provides open data access. EDGAR is the primary system under the U.S. Securities And Exchange Commission (SEC) for companies and others submitting documents under the Securities Act of 1933, the Securities Exchange Act of 1934, the Trust Indenture Act of 1939, and the Investment Company Act of 1940. EDGAR contains millions of company and individual filings. The system processes about 3,000 filings per day, serves up 3,000 terabytes of data to the public annually, and accommodates 40,000 new filers per year on average.\n",
    "\n",
    "There are several ways to download the data, and some open source packages available to extract the text from these filings. However, these require extensive programming and are not always easy-to-use. We provide a simple one-API call that will create a dataset in a few lines of code, for any period of time and for numerous tickers.\n",
    "\n",
    "We have wrapped the extraction functionality into a SageMaker processing container and provide this notebook to enable users to download a dataset of filings with metadata such as dates and parsed plain text that can then be used for machine learning using other SageMaker tools. This is included in the [SageMaker Industry Jumpstart Industry](https://aws.amazon.com/blogs/machine-learning/use-pre-trained-financial-language-models-for-transfer-learning-in-amazon-sagemaker-jumpstart/) library for financial language models. Users only need to specify a date range and a list of ticker symbols, and the library will take care of the rest.\n",
    "\n",
    "As of now, the solution supports extracting a popular subset of SEC forms in plain text (excluding tables): 10-K, 10-Q, 8-K, 497, 497K, S-3ASR, and N-1A. For each of these, we provide examples throughout this notebook and a brief description of each form. For the 10-K and 10-Q forms, filed every year or quarter, we also extract the Management Discussion and Analysis (MDNA) section, which is the primary forward-looking section in the filing. This is the section that has been most widely used in financial text analysis. Therefore, we provide this section automatically in a separate column of the dataframe alongside the full text of the filing.\n",
    "\n",
    "The extracted dataframe is written to S3 storage and to the local notebook instance. \n",
    "\n",
    "---\n",
    "\n",
    "### News articles related to the stock symbol -- dataset\n",
    "\n",
    "We will use the MIT Licensed [NewsCatcher API](https://docs.newscatcherapi.com/) to grab top 4-5 articles about the specific organization using filters, however other sources such as Social media feeds, RSS Feeds can also be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the pipeline is to fetch the SEC report from the EDGAR database using the [SageMaker Industry Jumpstart Industry](https://aws.amazon.com/blogs/machine-learning/use-pre-trained-financial-language-models-for-transfer-learning-in-amazon-sagemaker-jumpstart/) library for Financial language models. This library provides us an easy to use functionality to obtain either one or multiple SEC reports for one or more Ticker symbols or CIKs. The ticker or CIK number will be passed to the SageMaker Pipeline using Pipeline parameter `inference_ticker_cik`. For demo purposes of this Pipeline we will focus on a single Ticker/CIK number at a time and the MDNA section of the 10-K form. The first processing will extract the MDNA from the 10-K form for a company and will also gather few news articles related to the company from the NewsCatcher API. This data will ultimately be used for summarization and then finally sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## MLOps for NLP using SageMaker Pipelines\n",
    "\n",
    "We will set up the following SageMaker Pipeline. The Pipleline has two flows depending on what the value for `model_register_deploy` Pipeline parameter is set to. If the value is set to `Y` we want the pipeline to register the model and deploy the latest version of the model from the model registry to the SageMaker endpoint. If the value is set to `N` then we simply want to run inferences using the [FinBERT](https://huggingface.co/ProsusAI/finbert) and the [Pegasus](https://huggingface.co/transformers/model_doc/pegasus.html) models using the Ticker symbol (or CIK number) that is passed to the pipeline using the `inference_ticker_cik` Pipeline parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pipeline_lambda_execution_graph.png\" alt=\"Pipeline\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> You must execute the <code>01_nlp_setup_company_earnings_analysis.ipynb</code> notebook before you can set up the SageMaker Pipeline. In that notebook we create a custom Docker image and register it in Amazon Elastic Container Registry (Amazon ECR) for the pipeline to use. The image contains of all the required dependencies.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Set Up SageMaker Project\n",
    "\n",
    "<a id='setup-project'></a>\n",
    "\n",
    "\n",
    "### Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install updated version of SageMaker\n",
    "# !pip install -q sagemaker==2.49\n",
    "!pip install sagemaker --upgrade\n",
    "# !pip install transformers\n",
    "# !pip install typing\n",
    "# !pip install sentencepiece\n",
    "# !pip install fiscalyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install SageMaker JumpStart Industry\n",
    "!pip install smjsindustry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "NOTE: After installing an updated version of SageMaker and PyTorch, save the notebook and then restart your kernel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "print(f\"SageMaker version: {sagemaker.__version__}\")\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.sklearn.processing import ScriptProcessor\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters that you'll use throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_role = role\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"nlp-e2e-mlops\"\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sagemaker_boto_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "\n",
    "# deploy_model_instance_type = \"ml.m4.8xlarge\"\n",
    "deploy_model_instance_type = \"ml.m4.xlarge\"\n",
    "inference_instances = [\n",
    "    \"ml.t2.medium\",\n",
    "    \"ml.m5.xlarge\",\n",
    "    \"ml.m5.2xlarge\",\n",
    "    \"ml.m5.4xlarge\",\n",
    "    \"ml.m5.12xlarge\",\n",
    "]\n",
    "transform_instances = [\"ml.m5.xlarge\"]\n",
    "PROCESSING_INSTANCE = \"ml.m4.4xlarge\"\n",
    "ticker = \"AMZN\"\n",
    "\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"s3://{default_bucket}/{prefix}/code/model_deploy.py\")\n",
    "print(f\"SageMaker Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters to parametrize Pipeline Execution\n",
    "\n",
    "Using SageMaker Pipelines, we can define the steps to be included in a pipeline but then use parameters to modify that pipeline when we go to execute the pipeline, without having to modify the pipeline definition. We'll provide some default parameter values that can be overridden on pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some default parameters:\n",
    "\n",
    "# specify default number of instances for processing step\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "# specify default instance type for processing step\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=PROCESSING_INSTANCE\n",
    ")\n",
    "\n",
    "# specify location of inference data for data processing step\n",
    "inference_input_data = ParameterString(\n",
    "    name=\"InferenceData\",\n",
    "    default_value=f\"s3://{default_bucket}/{prefix}/nlp-pipeline/inf-data\",\n",
    ")\n",
    "\n",
    "# Specify the Ticker CIK for the pipeline\n",
    "inference_ticker_cik = ParameterString(\n",
    "    name=\"InferenceTickerCik\",\n",
    "    default_value=ticker,\n",
    ")\n",
    "\n",
    "# specify default method for model approval\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "# specify if new model needs to be registered and deployed\n",
    "model_register_deploy = ParameterString(name=\"ModelRegisterDeploy\", default_value=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store\n",
    "# These are the stored variables, the container is created in the\n",
    "# previous notebook 01_script-processor-custom-container.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='analyze-sec'></a>\n",
    "\n",
    "## Preparing SEC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive right into setting up the pipeline, let's take a look at how the SageMaker Jumpstart Industry SDK for Financial language model helps obtain the dataset from SEC forms and what are the features available for us to use. \n",
    "\n",
    "**Note:** The code cells in this section are completely optional and for information purposes only; we will use the SageMaker JumpStart Industry SDK directly in the pipeline. \n",
    "\n",
    "Let's install the required dependencies first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the SageMaker JumpStart Industry SDK\n",
    "\n",
    "The functionality is delivered through a client-side SDK. The first step requires pip installing a Python package that interacts with a SageMaker processing container. The retrieval, parsing, transforming, and scoring of text is a complex process and uses different algorithms and packages. In order to make this seamless and stable for the user, the functionality is packaged into a SageMaker container. This lifts the load of installation and maintenance of the workflow, reducing the user effort down to a pip install followed by a single API call. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will try to pull AMZN ticker 10k/10q filings from EDGAR and write the data as CSV to S3. Below is the single block of code that contains the API call. The options are all self-explanatory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from smfinance import SECDataSetConfig, DataLoader\n",
    "from smjsindustry.finance import DataLoader\n",
    "from smjsindustry.finance.processor_config import EDGARDataSetConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted reports will be saved to an S3 bucket for us to review. This code will also be used in the Pipeline to fetch the report for the Ticker or CIK number passed to the SageMaker Pipeline. Executing the following code cell will run a processing job which will fetch the SEC reports from the EDGAR database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='use-smjsindustry'></a>\n",
    "\n",
    "### Obtain SEC data using the SageMaker JumpStart Industry SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dataset_config = EDGARDataSetConfig(\n",
    "    tickers_or_ciks=[\"amzn\", \"goog\", \"27904\", \"FB\"],  # list of stock tickers or CIKs\n",
    "    form_types=[\"10-K\", \"10-Q\"],  # list of SEC form types\n",
    "    filing_date_start=\"2019-01-01\",  # starting filing date\n",
    "    filing_date_end=\"2020-12-31\",  # ending filing date\n",
    "    email_as_user_agent=\"test-user@test.com\",\n",
    ")  # user agent email\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    role=sagemaker.get_execution_role(),  # loading job execution role\n",
    "    instance_count=1,  # instances number, limit varies with instance type\n",
    "    instance_type=\"ml.c5.2xlarge\",  # instance type\n",
    "    volume_size_in_gb=30,  # size in GB of the EBS volume to use\n",
    "    volume_kms_key=None,  # KMS key for the processing volume\n",
    "    output_kms_key=None,  # KMS key ID for processing job outputs\n",
    "    max_runtime_in_seconds=None,  # timeout in seconds. Default is 24 hours.\n",
    "    sagemaker_session=sagemaker.Session(),  # session object\n",
    "    tags=None,\n",
    ")  # a list of key-value pairs\n",
    "\n",
    "data_loader.load(\n",
    "    dataset_config,\n",
    "    \"s3://{}/{}\".format(\n",
    "        default_bucket, \"sample-sec-data\"\n",
    "    ),  # output s3 prefix (both bucket and folder names are required)\n",
    "    \"dataset_10k_10q.csv\",  # output file name\n",
    "    wait=True,\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output\n",
    "\n",
    "The output of the `data_loader` processing job is a `CSV` file. We see the filings for different quarters. \n",
    "\n",
    "The filing date comes within a month of the end date of the reporting period. Both these dates are collected and displayed in the dataframe. The column `text` contains the full text of the report, but the tables are not extracted. The values in the tables in the filings are balance-sheet and income-statement data (numeric/tabular) and are easily available elsewhere as they are reported in numeric databases. The last column of the dataframe comprises the Management Discussion & Analysis section, the column is named `mdna`, which is the primary forward-looking section in the filing. This is the section that has been most widely used in financial text analysis. Therefore, we will use the `mdna` text to derive the sentiment of the overall filing in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{default_bucket}/{prefix}/\")\n",
    "s3_client.download_file(\n",
    "    default_bucket,\n",
    "    \"{}/{}\".format(f\"sample-sec-data\", f\"dataset_10k_10q.csv\"),\n",
    "    f\"./data/dataset_10k_10q.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_10k_10q = pd.read_csv(f\"./data/dataset_10k_10q.csv\")\n",
    "data_frame_10k_10q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Set Up Your MLOps NLP Pipeline with SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipe-pre-deploy'></a>\n",
    "\n",
    "### Step 1: Data pre-processing - extract SEC data and news about the company\n",
    "\n",
    "#### Define a processing step to prepare SEC data for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a processing step to extract 10K and 10Q forms for a specific Organization either using the company [Stock Ticker](https://www.investopedia.com/ask/answers/12/what-is-a-stock-ticker.asp) Symbol or [CIK (Central Index Key)](https://www.sec.gov/edgar/searchedgar/cik.htm) used to lookup reports in SEC's EDGAR System. You can find the company Stock Ticker Symbol to CIK Number mapping [here](https://www.sec.gov/include/ticker.txt). This step will also collect news article snippets related to the company using the NewsCatcher API.\n",
    "\n",
    "#### **<span style=\"color:lightgreen\">Important</span>**:\n",
    "\n",
    "It is recommended to use CIKs as the input. The tickers will be internally converted to CIKs according to the [mapping file](https://www.sec.gov/include/ticker.txt).  \n",
    "One ticker may map to multiple CIKs, but we only support the latest ticker to CIK mapping. Please provide the old CIKs in the input when you want historical filings. Also note that even though the Client side SDK allows you to download multiple SEC reports for multiple CIKs at a time, we will set up our data preprocessing step to grab exactly 1 SEC Report for 1 CIK (Company/Organization). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we used store magic in the previous note book script-processor-custom-container.ipynb \n",
    "to instantiate the container in the region of choice \n",
    "\"\"\"\n",
    "CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_instance_type = \"ml.c5.2xlarge\"\n",
    "create_dataset_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=CONTAINER_IMAGE_URI,\n",
    "    role=role,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a processing step to process the SEC data for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_script_uri = f\"s3://{default_bucket}/{prefix}/code/data-processing.py\"\n",
    "s3_client.upload_file(\n",
    "    Filename=\"./scripts/data-processing.py\",\n",
    "    Bucket=default_bucket,\n",
    "    Key=f\"{prefix}/code/data-processing.py\",\n",
    ")\n",
    "\n",
    "create_dataset_step = ProcessingStep(\n",
    "    name=\"nlp-e2e-FinBertCreateDataset\",\n",
    "    processor=create_dataset_processor,\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"report_data\",\n",
    "            source=\"/opt/ml/processing/output/10k10q\",\n",
    "            destination=f\"{inference_input_data}/10k10q\",\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"article_data\",\n",
    "            source=\"/opt/ml/processing/output/articles\",\n",
    "            destination=f\"{inference_input_data}/articles\",\n",
    "        ),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--ticker-cik\",\n",
    "        inference_ticker_cik,\n",
    "        \"--instance-type\",\n",
    "        loader_instance_type,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--bucket\",\n",
    "        default_bucket,\n",
    "        \"--prefix\",\n",
    "        prefix,\n",
    "        \"--role\",\n",
    "        role,\n",
    "    ],\n",
    "    code=create_dataset_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipe-pre-deploy'></a>\n",
    "\n",
    "### Step 2: Create models for summarization and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model_name = \"nlp-e2e-FinbertModel\"\n",
    "summarization_model_name = \"nlp-e2e-PegasusModel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the `finBert` model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained model using HuggingFaceModel class\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "hub = {\"HF_MODEL_ID\": \"ProsusAI/finbert\", \"HF_TASK\": \"text-classification\"}\n",
    "\n",
    "# create Hugging Face Model Class (documentation here: https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model)\n",
    "sentiment_huggingface_model = HuggingFaceModel(\n",
    "    name=sentiment_model_name,\n",
    "    transformers_version=\"4.6.1\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(instance_type=\"ml.m4.xlarge\")\n",
    "\n",
    "# This may not be needed since we are going to use Model registry\n",
    "# create_sentiment_model_step = CreateModelStep(\n",
    "#     name=\"nlp-e2e-FinBertCreateModel\",\n",
    "#     model=sentiment_huggingface_model,\n",
    "#     inputs=inputs\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Pegasus summarization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = {\n",
    "    \"HF_MODEL_ID\": \"human-centered-summarization/financial-summarization-pegasus\",\n",
    "    \"HF_TASK\": \"summarization\",\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class (documentation here: https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model)\n",
    "summary_huggingface_model = HuggingFaceModel(\n",
    "    name=summarization_model_name,\n",
    "    transformers_version=\"4.6.1\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# This may not be needed since we are going to use Model registry\n",
    "# create_summary_model_step = CreateModelStep(\n",
    "#     name=\"nlp-e2e-PegasusCreateModel\",\n",
    "#     model=summary_huggingface_model,\n",
    "#     inputs=inputs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-registry'></a>\n",
    "\n",
    "### Step 3: Register model\n",
    "\n",
    "Use HuggingFace register method to register Hugging Face Model for deployment. Set up step as a custom processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model_package_group_name = \"HuggingFaceSECSentimentModelPackageGroup\"\n",
    "summary_model_package_group_name = \"HuggingFaceSECSummaryModelPackageGroup\"\n",
    "model_approval_status = \"Approved\"\n",
    "\n",
    "register_sentiment_model_step = RegisterModel(\n",
    "    name=\"nlp-e2e-FinBertRegisterModel\",\n",
    "    model=sentiment_huggingface_model,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=inference_instances,\n",
    "    transform_instances=transform_instances,\n",
    "    model_package_group_name=sentiment_model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    #     depends_on=['nlp-e2e-FinBertCreateModel']\n",
    ")\n",
    "\n",
    "register_summary_model_step = RegisterModel(\n",
    "    name=\"nlp-e2e-PegasusRegisterModel\",\n",
    "    model=summary_huggingface_model,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=inference_instances,\n",
    "    transform_instances=transform_instances,\n",
    "    model_package_group_name=summary_model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    #     depends_on=['nlp-e2e-PegasusCreateModel']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deploy'></a>\n",
    "\n",
    "### Step 4: Deploy model using Lambda step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deploy the FinBert and Pegasus models from the model registry. \n",
    "\n",
    "**NOTE:** The models in the model registry are the pre-trained version from HuggingFace Model Hub. Each of the deployment step will attempt to deploy a SageMaker Endpoint with the model and will write a property file upon successful completion. The Pipeline will make use of these property files to decide whether to execute the subsequent summarization and sentiment analysis inference steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also deploy a HuggingFace model similar to how we deployed the FinBert sentiment model above but using SageMaker Pipelines `Lambda` step. The SageMaker SDK provides a Lambda helper class that can be used to create a Lambda function. This function is provided to the Lambda step for invocation via the pipeline. Alternatively, a predefined Lambda function can be provided to the Lambda step. \n",
    "\n",
    "The SageMaker Execution Role requires the policy `AmazonSageMakerPipelinesIntegrations` to create the Lambda function, and the Lambda function needs a role with policies allowing creation of a SageMaker endpoint. The AWS Lambda execution role must have `AWSLambdaBasicExecutionRole` and `AmazonSageMakerFullAccess` managed Policies. The IAM role must also have the trust policy shown below. For More information on how to create a AWS Lambda execution role, refer to [documentation](https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html).\n",
    "\n",
    "<code style=\"color:#00E676;font-size:12px\">\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"lambda.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "</code>\n",
    "\n",
    "<div class=\"alert alert-warning\"> <h4 color=\"red\"><strong>💡 NOTE </strong></h4>\n",
    "The IAM Policies described above can be overly permissive. Please practice caution in setting up IAM Roles with them. For more information about fine-grained permissions for the sagemaker-studio-image-build tool, see <a href=\"https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/\">Using the Amazon SageMaker Studio Image Build CLI to build container images from your Studio notebooks</a>. For best practices on SageMaker security, IAM roles, and policies, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_id-based-policy-examples.html#security_iam_service-with-iam-policy-best-practices\">Policy Best Practices</a> in the Amazon SageMaker Developer Guide.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, specify the name of AWS Lambda execution role. You may replace it with the role name you specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_role_name = \"<sagemaker-pipelines-lambda-step-role>\"\n",
    "lambda_role = boto3.client(\"iam\").get_role(RoleName=lambda_role_name)[\"Role\"][\"Arn\"]\n",
    "lambda_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a few variables to be used for setting up the LambdaStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_model_instance_type = \"ml.m4.4xlarge\"\n",
    "deploy_model_instance_count = 1\n",
    "\n",
    "sentiment_endpoint_name = \"nlp-e2e-FinBertModel-endpoint\"\n",
    "summarization_endpoint_name = \"nlp-e2e-PegasusModel-endpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lambda function code is contained in a a script called `lambda_deployer.py` in the `/scripts` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the Lambda step.  After defining some object names, we use the Lambda helper class to create the actual Lambda function, then pass it to the Lambda step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current time to define unique names for the resources created\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "function_name = \"hfnlp-demo-lambda-deploy-step-\" + current_time\n",
    "\n",
    "# Lambda helper class can be used to create the Lambda function\n",
    "func = Lambda(\n",
    "    function_name=function_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"./lambda/lambda_deployer.py\",\n",
    "    handler=\"lambda_deployer.lambda_handler\",\n",
    "    timeout=600,\n",
    "    memory_size=10240,\n",
    ")\n",
    "\n",
    "# The dictionary retured by the Lambda function is captured by LambdaOutput, each key in the dictionary corresponds to a\n",
    "# LambdaOutput\n",
    "\n",
    "output_param_1 = LambdaOutput(output_name=\"statusCode\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name=\"body\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_3 = LambdaOutput(\n",
    "    output_name=\"endpoint_created\", output_type=LambdaOutputTypeEnum.String\n",
    ")\n",
    "\n",
    "sentiment_deploy_lambda_step = LambdaStep(\n",
    "    name=\"LambdaStepFinbertDeploy\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"endpoint_name\": sentiment_endpoint_name,\n",
    "        \"deploy_model_instance_type\": deploy_model_instance_type,\n",
    "        \"deploy_model_instance_count\": deploy_model_instance_count,\n",
    "        \"model_package_group_name\": sentiment_model_package_group_name,\n",
    "        \"role\": role,\n",
    "    },\n",
    "    outputs=[output_param_1, output_param_2, output_param_3],\n",
    "    depends_on=[\"nlp-e2e-FinBertRegisterModel\"],\n",
    ")\n",
    "\n",
    "# The inputs provided to the Lambda function can be retrieved via the `event` object within the `lambda_handler` function\n",
    "# in the Lambda\n",
    "summary_deploy_lambda_step = LambdaStep(\n",
    "    name=\"LambdaStepPegasusDeploy\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"endpoint_name\": summarization_endpoint_name,\n",
    "        \"deploy_model_instance_type\": deploy_model_instance_type,\n",
    "        \"deploy_model_instance_count\": deploy_model_instance_count,\n",
    "        \"model_package_group_name\": summary_model_package_group_name,\n",
    "        \"role\": role,\n",
    "    },\n",
    "    outputs=[output_param_1, output_param_2, output_param_3],\n",
    "    depends_on=[\"nlp-e2e-PegasusRegisterModel\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pipeline conditions to check if the Endpoint deployments were successful\n",
    "\n",
    "We will define a condition that checks to see if our model deployment was successful based on the response returned by the Lambda function in the Lambda deployment steps of both the FinBert and Pegasus Models. If both the conditions evaluates to `True` then we will run or subsequent inferences for Summarization and Sentiment analysis. \n",
    "\n",
    "At this time we will simply define the conditions, and later when we are ready with the inference steps we will setup the `ConditionStep` using `sentiment_condition_eq` and `summary_condition_eq` conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionEquals\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "sentiment_condition_eq = ConditionEquals(\n",
    "    left=sentiment_deploy_lambda_step.properties.Outputs[\"endpoint_created\"],\n",
    "    right=\"Y\",  # the right value of the evaluation expression, i.e. endpoint_created='Y'\n",
    ")\n",
    "\n",
    "summary_condition_eq = ConditionEquals(\n",
    "    left=summary_deploy_lambda_step.properties.Outputs[\"endpoint_created\"],\n",
    "    right=\"Y\",  # the right value of the evaluation expression, i.e. endpoint_created='Y'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Summarize SEC report step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is to make use of the Pegasus Summarizer model endpoint to summarize the MDNA text from the SEC report. Because the MDNA text is usually large, we want to derive a short summary of the overall text to be able to determine the overall sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=CONTAINER_IMAGE_URI,\n",
    "    role=role,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_script_uri = f\"s3://{default_bucket}/{prefix}/code/summarize.py\"\n",
    "s3_client.upload_file(\n",
    "    Filename=\"./scripts/summarize.py\", Bucket=default_bucket, Key=f\"{prefix}/code/summarize.py\"\n",
    ")\n",
    "\n",
    "summarize_step_1 = ProcessingStep(\n",
    "    name=\"nlp-e2e-PegasusSummarizer_1\",\n",
    "    processor=summarize_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"summary_data\",\n",
    "            source=f\"{inference_input_data}/10k10q\",\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"summarized_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{inference_input_data}/10k10q/summary\",\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--ticker-cik\",\n",
    "        inference_ticker_cik,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-name\",\n",
    "        summarization_endpoint_name,\n",
    "    ],\n",
    "    code=summarize_script_uri,\n",
    ")\n",
    "\n",
    "summarize_step_2 = ProcessingStep(\n",
    "    name=\"nlp-e2e-PegasusSummarizer_2\",\n",
    "    processor=summarize_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"summary_data\",\n",
    "            source=f\"{inference_input_data}/10k10q\",\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"summarized_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{inference_input_data}/10k10q/summary\",\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--ticker-cik\",\n",
    "        inference_ticker_cik,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-name\",\n",
    "        summarization_endpoint_name,\n",
    "    ],\n",
    "    code=summarize_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Sentiment inference step - SEC summary and news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step uses the MDNA summary (determined by the previous step) and the news articles to find out the sentiment of the company's financial and what the Market trends are indicating. This would help us understand the overall position of the company's financial outlook and current position without leaning solely on the company's forward-looking statements and bring objective market opinions into the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=CONTAINER_IMAGE_URI,\n",
    "    role=role,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_script_uri = f\"s3://{default_bucket}/{prefix}/code/sentiment.py\"\n",
    "s3_client.upload_file(\n",
    "    Filename=\"./scripts/sentiment.py\", Bucket=default_bucket, Key=f\"{prefix}/code/sentiment.py\"\n",
    ")\n",
    "\n",
    "sentiment_step_1 = ProcessingStep(\n",
    "    name=\"nlp-e2e-FinBertSentiment_1\",\n",
    "    processor=summarize_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"sec_summary\",\n",
    "            source=f\"{inference_input_data}/10k10q/summary\",\n",
    "            destination=\"/opt/ml/processing/input/10k10q\",\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"articles\",\n",
    "            source=f\"{inference_input_data}/articles\",\n",
    "            destination=\"/opt/ml/processing/input/articles\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"sentiment_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{inference_input_data}/sentiment\",\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--ticker-cik\",\n",
    "        inference_ticker_cik,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-name\",\n",
    "        sentiment_endpoint_name,\n",
    "    ],\n",
    "    code=sentiment_script_uri,\n",
    "    depends_on=[\"nlp-e2e-PegasusSummarizer_1\"],\n",
    ")\n",
    "\n",
    "sentiment_step_2 = ProcessingStep(\n",
    "    name=\"nlp-e2e-FinBertSentiment_2\",\n",
    "    processor=summarize_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"sec_summary\",\n",
    "            source=f\"{inference_input_data}/10k10q/summary\",\n",
    "            destination=\"/opt/ml/processing/input/10k10q\",\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"articles\",\n",
    "            source=f\"{inference_input_data}/articles\",\n",
    "            destination=\"/opt/ml/processing/input/articles\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"sentiment_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{inference_input_data}/sentiment\",\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--ticker-cik\",\n",
    "        inference_ticker_cik,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-name\",\n",
    "        sentiment_endpoint_name,\n",
    "    ],\n",
    "    code=sentiment_script_uri,\n",
    "    depends_on=[\"nlp-e2e-PegasusSummarizer_2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Condition Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained earlier, this is a top level condition step. This step will determine based on the value of the pipeline parameter `model_register_deploy` on whether we want to register and deploy a new version of the models and then run inference, or to simply run inference using the existing endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionEquals\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "\n",
    "condition_eq = ConditionEquals(left=model_register_deploy, right=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the condition step\n",
    "condition_step = ConditionStep(\n",
    "    name=\"nlp-e2e-FinBertConditionCheck\",\n",
    "    conditions=[condition_eq],  # the parameter is Y\n",
    "    if_steps=[\n",
    "        #               create_sentiment_model_step, #CreateModel Step\n",
    "        register_sentiment_model_step,\n",
    "        sentiment_deploy_lambda_step,\n",
    "        #               create_summary_model_step,  #CreateModel Step\n",
    "        register_summary_model_step,\n",
    "        summary_deploy_lambda_step,\n",
    "    ],  # if the condition evaluates to true then create model, register, and deploy\n",
    "    else_steps=[summarize_step_1],\n",
    "    depends_on=[\"nlp-e2e-FinBertCreateDataset\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also setup the `ConditionStep` using the two conditions we set up earlier - `sentiment_condition_eq` and `summary_condition_eq`. This condition will make sure that the inference steps are only executed if the endpoint deployment/update is successful (done by the LambdaStep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_condition_step = ConditionStep(\n",
    "    name=\"nlp-e2e-EndpointDeployConditionCheck\",\n",
    "    conditions=[\n",
    "        sentiment_condition_eq,\n",
    "        summary_condition_eq,\n",
    "    ],  # the equal to conditions defined above\n",
    "    if_steps=[\n",
    "        summarize_step_2\n",
    "    ],  # if the condition evaluates to true then run the summarization step\n",
    "    else_steps=[],  # there are no else steps so we will keep it empty\n",
    "    depends_on=[\n",
    "        \"LambdaStepFinbertDeploy\",\n",
    "        \"LambdaStepPegasusDeploy\",\n",
    "    ],  # dependencies on both Finbert and Pegasus Deployment steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Pipeline steps and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"Lambda-Company-Earnings-Sentiment-Async-Pipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        model_register_deploy,\n",
    "        inference_ticker_cik,\n",
    "        inference_input_data,\n",
    "    ],\n",
    "    steps=[\n",
    "        create_dataset_step,\n",
    "        condition_step,\n",
    "        deploy_condition_step,\n",
    "        sentiment_step_1,\n",
    "        sentiment_step_2,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start_response = pipeline.start()\n",
    "start_response.wait(delay=60, max_attempts=200)\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image shows a successful execution of the NLP end-to-end Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"./images/pipeline_lambda_execution_graph.png\" alt=\"Successful Pipeline Execution\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## View Evaluation Results\n",
    "\n",
    "Once the pipeline execution completes, we can download the evaluation data from S3 and view it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.download_file(\n",
    "    default_bucket,\n",
    "    f\"{prefix}/nlp-pipeline/inf-data/sentiment/{ticker}_sentiment_result.csv\",\n",
    "    f\"./data/{ticker}_sentiment_result.csv\",\n",
    ")\n",
    "sentiment_df = pd.read_csv(f\"./data/{ticker}_sentiment_result.csv\")\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the SageMaker Pipeline and the SageMaker Endpoints created by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_resources():\n",
    "    pipeline.delete()\n",
    "    sagemaker_boto_client.delete_endpoint(EndpointName=sentiment_endpoint_name)\n",
    "    sagemaker_boto_client.delete_endpoint(EndpointName=summarization_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
