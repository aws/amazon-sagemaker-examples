{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Amazon SageMaker Autopilot is an automated machine learning (commonly referred to as AutoML) solution for tabular datasets. You can use SageMaker Autopilot in different ways: on autopilot (hence the name) or with human guidance, without code through SageMaker Studio, or using the AWS SDKs. This notebook will use the AWS SDKs to simply create and deploy a machine learning model.\n",
    "\n",
    "In this notebook we will work through an example of credit card fraud detection using SageMaker AutoPilot.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "This notebook was created and tested on an ml.m4.xlarge notebook instance. Also ensure that this notebook uses the older version(older than 2.0.0) of the SageMaker SDK. Below we have code to check this so you don't have to.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regex with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sagemaker\n",
    "import time\n",
    "! pip install --upgrade pip \n",
    "#install s3fs - this package is used by pandas to read file from s3\n",
    "!pip install --upgrade s3fs\n",
    "#install arff package, this package is used to read the bankruptcy data which is in ARFF format\n",
    "!pip install --upgrade arff\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "#### Please make sure to download kaggle.json file into the .kaggle directory. Please take a look here for further details https://www.kaggle.com/docs/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import gmtime, strftime, sleep\n",
    "from sagemaker import get_execution_role\n",
    "from urllib.parse import urlparse\n",
    "from sagemaker.automl.automl import AutoML\n",
    "import botocore\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook, you will need to donwload the Credit Card Fraud dataset from Kaggle first. \n",
    "\n",
    "We downloaded the fraud data set from Kaggle site(https://www.kaggle.com/mlg-ulb/creditcardfraud). Below cell will download credit card fraud dataset from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d mlg-ulb/creditcardfraud -o -q\n",
    "!unzip -q creditcardfraud.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a quick look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, class 0 = No Fraud, class 1  = Fraud. As we can see, other than Amount, other columns are anonymized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we select all attributes except “*Class*'” as predictor/training feature into X and “Class” as target attribute y.\n",
    "\n",
    "We also need to split data into train data set and test data set. We will keep **70%** of the data as training and **30%** of the data as test. We are going to use Scikit Learn utility train_test_split for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'Class'\n",
    "print (fraud_df[target_variable].value_counts())\n",
    "train, test = train_test_split(fraud_df, test_size=.3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that the binary label column *Class* is highly imbalanced, a typical occurrence in financial use cases.**  \n",
    "We will verify how well Autopilot handle this highly imbalanced data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will configure Sagemaker AutoPilot. \n",
    "We give a job name **automl-creditcard-fraud**, create a session with Sagemaker client. We need to have a **s3** bucket to store train/test data and all other artifacts Autopilot will produce. We are using default **s3** bucket, you can create your own bucket. Training and Test data is used from the \n",
    "previous steps and uploaded to **s3** bucket under \"train\" and \"test\" respectively. training_data['Class'] has the target (credit card fraud 0/1). **S3Uri** field in input_data_config points Autopilot to the training data location. **TargetAttributeName** indicates target variable for the training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ml_job_name = 'automl-creditcard-fraud'\n",
    "sm = boto3.client('sagemaker')\n",
    "session = sagemaker.Session()\n",
    "\n",
    "prefix = 'sagemaker/' + auto_ml_job_name\n",
    "bucket = session.default_bucket()\n",
    "training_data = train\n",
    "X_test = test.drop(columns = [target_variable])\n",
    "y_test = test[target_variable]\n",
    "test_data = X_test\n",
    "\n",
    "train_file = 'train_data.csv';\n",
    "training_data.to_csv(train_file, index=False, header=True)\n",
    "train_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"/train\")\n",
    "print('Train data uploaded to: ' + train_data_s3_path)\n",
    "\n",
    "test_file = 'test_data.csv';\n",
    "test_data.to_csv(test_file, index=False, header=False)\n",
    "test_data_s3_path = session.upload_data(path=test_file, key_prefix=prefix + \"/test\")\n",
    "print('Test data uploaded to: ' + test_data_s3_path)\n",
    "input_data_config = [{\n",
    "      'DataSource': {\n",
    "        'S3DataSource': {\n",
    "          'S3DataType': 'S3Prefix',\n",
    "          'S3Uri': 's3://{}/{}/train'.format(bucket,prefix)\n",
    "        }\n",
    "      },\n",
    "      'TargetAttributeName': target_variable\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to create the Autopilot job.We set the maximum candidate models (attribute max_candidates ) with different parameters to 200. We also set ProblemType='BinaryClassification'.Please note you do not need to set ProblemType and MetricName.If you do not set these 2 field, Autopilot will automatically determine the type of supervised learning problem by analyzing the data(for binary classification problem - default metric is F1). If you do not set this field, Autopilot will automatically determine the type of supervised learning problem by analyzing the data. We set MetricName(parameter job_objective) to AUC/F1(eval_obj). You can find out all options for the job configuration here (https://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-auto-ml-job.html).\n",
    "\n",
    "Note that depending on the number of candidates you run (here we set it to 200), this may take a couple hours to run both F1 and AUC scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_automl_object(eval_obj,base_job_name):\n",
    "    \n",
    "    target_attribute_name = target_variable\n",
    "    role = get_execution_role()\n",
    "    automl = AutoML(role=role,\n",
    "                    target_attribute_name=target_attribute_name,\n",
    "                    base_job_name=base_job_name,\n",
    "                    sagemaker_session=session,\n",
    "                    problem_type='BinaryClassification',\n",
    "                    job_objective={'MetricName': eval_obj},\n",
    "                    max_candidates=200) # Including a max candidates will let you limit the number of AutoPilot jobs to run\n",
    "    return automl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the AutoML object is created, we call the fit() function to train the AutoML object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_fit(automl,base_job_name):\n",
    "    automl.fit(train_file, job_name=base_job_name, wait=False, logs=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We check the status of the Autopilot Job for every 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status(automl):\n",
    "    describe_response = automl.describe_auto_ml_job()\n",
    "    print (describe_response)\n",
    "    job_run_status = describe_response['AutoMLJobStatus']\n",
    "\n",
    "    while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "        describe_response = automl.describe_auto_ml_job()\n",
    "        job_run_status = describe_response['AutoMLJobStatus']\n",
    "        print (job_run_status)\n",
    "        sleep(30)\n",
    "    print ('completed')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After the job is complete, we select the best candidate job and check the AUC or F1 (based on the MetricName in the create_automl_object() function )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_candidate(automl):\n",
    "    best_candidate = automl.describe_auto_ml_job()['BestCandidate']\n",
    "    best_candidate_name = best_candidate['CandidateName']\n",
    "    print(best_candidate)\n",
    "    print('\\n')\n",
    "    print(\"CandidateName: \" + best_candidate_name)\n",
    "    print(\"FinalAutoMLJobObjectiveMetricName: \" + best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName'])\n",
    "    print(\"FinalAutoMLJobObjectiveMetricValue: \" + str(best_candidate['FinalAutoMLJobObjectiveMetric']['Value']))\n",
    "    return best_candidate_name,best_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a model from the **best candidate**. In addition to predicted label, we want **probability** of the prediction - this probability will be used later to plot AUC and Precision/Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(automl,best_candidate_name,best_candidate,timestamp_suffix):\n",
    "    model_name = 'automl-cardfraud-model-' + timestamp_suffix\n",
    "    inference_response_keys = ['predicted_label', 'probability']\n",
    "    model = automl.create_model(name=best_candidate_name,\n",
    "                                candidate=best_candidate,inference_response_keys=inference_response_keys)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is created, we run a Transform job to get inference (i.e Prediction about the default) from the test data set and save to S3. It is worth noting that when you deploy the model as an endpoint or create a Transform job, SageMaker handles the deployment of the feature engineering pipeline and the ML algorithm, so end users can send the data in its raw format for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer(model,best_candidate,eval_obj):\n",
    "    s3_transform_output_path = 's3://{}/{}/inference-results/'.format(bucket, prefix);\n",
    "    output_path = s3_transform_output_path + best_candidate['CandidateName'] +'/'\n",
    "    transformer=model.transformer(instance_count=1, \n",
    "                              instance_type='ml.m5.xlarge',\n",
    "                              assemble_with='Line',\n",
    "                              output_path=output_path)\n",
    "    transformer.transform(data=test_data_s3_path, split_type='Line', content_type='text/csv', wait=False)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This function will return predicted output as dataframe. This function will read the file from s3 (generated from create_transformer), create a Data Frame for label(predicted 0/1) and probability(probability of the prediction 0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pred_df(transformer):\n",
    "    print ('***predict output path ***')\n",
    "    print (transformer.output_path, '{}.out'.format(test_file))\n",
    "    pred_csv = get_csv_from_s3(transformer.output_path,'{}.out'.format(test_file))\n",
    "    data=pd.read_csv(io.StringIO(pred_csv), header=None)\n",
    "    data.columns= ['label', 'proba']    \n",
    "    return data\n",
    "def get_csv_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:].strip('/')\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = None \n",
    "    loop = True\n",
    "    while (loop):\n",
    "        try:\n",
    "            obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n",
    "            pred_body  = obj.get()[\"Body\"].read().decode('utf-8')    \n",
    "            print ('predict file is avilable s3')    \n",
    "            loop = False\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            print('prediction file still not avilable in s3 sleeping for 2 minutes')\n",
    "            time.sleep(120)\n",
    "    return pred_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download Candidate Definition notebook from the following s3 location.\n",
    "We can download data exploration notebook to see details of AutoPilot data analysis. This report provides insights about the dataset you provided as input to the AutoML job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_notebooks(automl,eval_obj):\n",
    "    print (\"download CandidateDefinitionNotebookLocation for \" + eval_obj)\n",
    "    print (automl.describe_auto_ml_job()['AutoMLJobArtifacts']['CandidateDefinitionNotebookLocation'])\n",
    "    print (\"download DataExplorationNotebookLocation for \" + eval_obj)\n",
    "    print (automl.describe_auto_ml_job()['AutoMLJobArtifacts']['DataExplorationNotebookLocation'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We create a wrapper function run_automl_process for objective metrics AUC and F1. This wrapper function calls multiple functions to creare AutoML object, run training process, create model from best trained job and finally return predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_automl_process(eval_obj):\n",
    "    timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "    base_job_name = 'automl-card-fraud-' + eval_obj + timestamp_suffix\n",
    "    print (base_job_name)\n",
    "    automl = create_automl_object(eval_obj,base_job_name)\n",
    "    automl_fit(automl,base_job_name)\n",
    "    check_status(automl)\n",
    "    best_candidate_name,best_candidate=get_best_candidate(automl)\n",
    "    model = create_model(automl,best_candidate_name,best_candidate,timestamp_suffix)\n",
    "    transformer=create_transformer(model,best_candidate,eval_obj)\n",
    "    pred_df = return_pred_df(transformer)\n",
    "    download_notebooks(automl,eval_obj)\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run the Autopilot pilot job. We call the wrapper function run_automl_process with objective AUC and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('*********running with eval objective AUC***********')\n",
    "data_auc =run_automl_process('AUC')\n",
    "print ('*********running with eval objective F1***********')\n",
    "data_f1 = run_automl_process('F1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot ROC - the Area under the Curve (AUC) for true positives (in this data set Fraud) vs false positives (predicted as Fraud but not Fraud in the ground truth). The higher the prediction quality of the classification model, the more the AUC curve is skewed to the top left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "colors = ['blue','green']\n",
    "model_names = ['Objective : AUC','Objective : F1']\n",
    "models = [data_auc,data_f1]\n",
    "for i in range(0,len(models)):\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, models[i]['proba'])\n",
    "    fpr, tpr, _  = metrics.roc_curve(y_test, models[i]['proba'])\n",
    "    auc_score = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=str('Auto Pilot {:.2f} '+ model_names[i]).format(auc_score),color=colors[i]) \n",
    "        \n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('ROC Cuve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The precision-recall curve compares the trade-off between precision and recall, with the best models having a precision-recall curve that is flat initially, dropping steeply as the recall approaches 1. The higher precision + recall, more the curve will be skewed towards upper right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue','green']\n",
    "model_names = ['Objective : AUC','Objective : F1']\n",
    "models = [data_auc,data_f1]\n",
    "\n",
    "print ('model ', 'F1 ', 'precision ', 'recall ')\n",
    "for i in range(0,len(models)):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, models[i]['proba'])\n",
    "    print (model_names[i],f1_score(y_test, np.array(models[i]['label'])),precision_score(y_test, models[i]['label']),recall_score(y_test, models[i]['label']) )\n",
    "    plt.plot(recall,precision,color=colors[i],label=model_names[i])\n",
    "        \n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion <a name=\"Conclusion\"></a>\n",
    "We can see that with very little data science knowledge, we are able to create a highly accurate prediction for credit card fruad dataset. From the AUC and Precision+Recall plots, we can also see that Auto Pilot handled highly imbalanced data well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup <a name=\"Cleanup\"></a>\n",
    "\n",
    "The Autopilot job creates many underlying artifacts such as dataset splits, preprocessing scripts, or preprocessed data, etc. This code, when un-commented, deletes them. This operation deletes all the generated models and the auto-generated notebooks as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.resource('s3')\n",
    "# s3_bucket = s3.Bucket(bucket)\n",
    "\n",
    "# s3_bucket.objects.filter(Prefix=prefix).delete()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally, we can delete the models by calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
