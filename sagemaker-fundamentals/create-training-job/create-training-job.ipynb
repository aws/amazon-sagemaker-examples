{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Job\n",
    "\n",
    "An Amazon SageMaker *training job* is a compute process that trains an ML model in an containerized environment. In this notebook, you will create a training job with your own custom container on Amazon SageMaker. To read more about training job, refer to the [official docs](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html)\n",
    "\n",
    "The outline of this notebook is:\n",
    "- create an service execution for SageMaker to run a training job\n",
    "- build a light-weighted container based on continuumio/miniconda\n",
    "- test your container locally\n",
    "- push your container to Elastic Container Registry (ECR)\n",
    "- upload your training data to an S3 bucket\n",
    "- create a training job with everything you did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "file=$(ls . | grep iam_helpers.py)\n",
    "\n",
    "if [ -f \"$file\" ]\n",
    "then\n",
    "    rm $file\n",
    "fi\n",
    "\n",
    "wget https://raw.githubusercontent.com/hsl89/amazon-sagemaker-examples/master/sagemaker-fundamentals/execution-role/iam_helpers.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-03 19:44:56--  https://raw.githubusercontent.com/hsl89/amazon-sagemaker-examples/master/sagemaker-fundamentals/execution-role/iam_helpers.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3350 (3.3K) [text/plain]\n",
      "Saving to: ‘iam_helpers.py.1’\n",
      "\n",
      "iam_helpers.py.1    100%[===================>]   3.27K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-03-03 19:45:02 (70.4 MB/s) - ‘iam_helpers.py.1’ saved [3350/3350]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "!wget https://raw.githubusercontent.com/hsl89/amazon-sagemaker-examples/master/sagemaker-fundamentals/execution-role/iam_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "iam = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "def current_time():\n",
    "    ct = datetime.datetime.now() \n",
    "    return str(ct.now()).replace(\":\", \"-\").replace(\" \", \"-\")[:19]\n",
    "\n",
    "def account_id():\n",
    "    return boto3.client('sts').get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an IAM service role\n",
    "\n",
    "To review IAM role, see the [notebook on execution role](https://github.com/hsl89/amazon-sagemaker-examples/blob/execution-role/sagemaker-fundamentals/execution-role/execution-role.ipynb)\n",
    "\n",
    "The service role is intended to be assumed by the SageMaker service. For simplicity, we will give it `AmazonSageMakerFullAccess` permission. However, in order to do what we need in this notebook, we do not need such a comprehensive permission. You are highly encouraged to play with the helper functions we provide in `iam_helpers.py` to figure out what are the minimum permissions needed to run this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Path': '/', 'RoleName': 'sm', 'RoleId': 'AROA2ATYEUMKISEX5EO2V', 'Arn': 'arn:aws:iam::688520471316:role/sm', 'CreateDate': datetime.datetime(2021, 3, 3, 23, 37, 7, tzinfo=tzlocal()), 'AssumeRolePolicyDocument': {'Version': '2012-10-17', 'Statement': [{'Effect': 'Allow', 'Principal': {'AWS': 'arn:aws:iam::688520471316:user/hongshan', 'Service': ['sagemaker.amazonaws.com']}, 'Action': 'sts:AssumeRole'}]}}\n"
     ]
    }
   ],
   "source": [
    "from iam_helpers import create_execution_role, attach_permission\n",
    "\n",
    "role_name='sm' \n",
    "role = create_execution_role(role_name=role_name)['Role']\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'HTTPHeaders': {'content-length': '212',\n",
      "                                      'content-type': 'text/xml',\n",
      "                                      'date': 'Wed, 03 Mar 2021 23:37:08 GMT',\n",
      "                                      'x-amzn-requestid': '8549d495-f371-41bd-ba92-974326a857f9'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '8549d495-f371-41bd-ba92-974326a857f9',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# attach AmazonSageMakerFullAccess\n",
    "res = iam.attach_role_policy(\n",
    "    RoleName=role['RoleName'],\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    ")\n",
    "\n",
    "pp.pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the training environement into a docker image\n",
    "\n",
    "Before creating a training job on Amazon SageMaker, you need to package the entire runtime environment of your ML project into a docker image and push the image into the Elastic Container Registry (ECR) under your account. \n",
    "\n",
    "When triggering a training job, your requested SageMaker instance will pull that image from your ECR and execute it with the data you specified in an S3 URI. \n",
    "\n",
    "It important to know how SageMaker runs your image. For **training job**, SageMaker runs your image like\n",
    "```\n",
    "docker run <image> train\n",
    "```\n",
    "i.e. your image needs to have an executable `train` and it is the executable that starts the model training process. You will see later in the notebook how to create it. \n",
    "\n",
    "The next natural thing to ask is how does the image running on SageMaker instance access the data that the model needs to be trained on? SageMaker requires you to reserve `/opt/ml` directory inside your image for it to provide training information. When you trigger a training job, you will need to specify the location of your training data, and the SageMaker instance running your image will mount your data into `/opt/ml/input`. \n",
    "\n",
    "To read more about SageMaker uses `/opt/ml` to provide training information, refer to the [official docs](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM continuumio/miniconda:latest \r\n",
      "\r\n",
      "# SageMaker uses /opt/ml for input / output data \r\n",
      "# throughout the training \r\n",
      "RUN mkdir -p /opt/ml\r\n",
      "\r\n",
      "# Copy the training script into /usr/bin \r\n",
      "# as an executable\r\n",
      "COPY train.py /usr/bin/train\r\n",
      "\r\n",
      "# make /opt/ml/program/train an executable\r\n",
      "RUN chmod +x /usr/bin/train\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# View the Dockerfile\n",
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination\n",
    "\n",
    "`train.py` in `container/` is the main script for training the model. We copied it into `/usr/bin`, renamed it as `train` and made it an executable in the docker image. This way when the container is executed as \n",
    "```\n",
    "docker run <image> train\n",
    "```\n",
    "The script in `/usr/bin/train` (in the container) will run. \n",
    "\n",
    "Note that this is one way to run the training logic on SageMaker. As long as the command \n",
    "```\n",
    "docker run <image> train \n",
    "```\n",
    "triggers your training logic you can do whatever you want. \n",
    "\n",
    "Next, we build the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  18.43kB\r",
      "\r\n",
      "Step 1/4 : FROM continuumio/miniconda:latest\n",
      " ---> b8ea69b5c41c\n",
      "Step 2/4 : RUN mkdir -p /opt/ml\n",
      " ---> Using cache\n",
      " ---> a170cc3fed03\n",
      "Step 3/4 : COPY train.py /usr/bin/train\n",
      " ---> Using cache\n",
      " ---> 315ae4eff0a2\n",
      "Step 4/4 : RUN chmod +x /usr/bin/train\n",
      " ---> Using cache\n",
      " ---> 0213a62c189a\n",
      "Successfully built 0213a62c189a\n",
      "Successfully tagged example-image:latest\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# build the image\n",
    "cd container/\n",
    "\n",
    "# tag it as example-image:latest\n",
    "docker build -t example-image:latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect what's in the training script `container/train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/usr/bin/env python\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# A sample script for training an ML model\u001b[39;49;00m\r\n",
      "\u001b[37m# It does 2 things\u001b[39;49;00m\r\n",
      "\u001b[37m# load csv data in /opt/ml/data\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# where SageMaker injects training data inside container\u001b[39;49;00m\r\n",
      "data_dir=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# SageMaker treat \"/opt/ml/model\" as checkpoint direcotry\u001b[39;49;00m\r\n",
      "\u001b[37m# and it will send everything there to S3 output path you \u001b[39;49;00m\r\n",
      "\u001b[37m# specified \u001b[39;49;00m\r\n",
      "model_dir=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m== Files in train channel ==\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m f \u001b[35min\u001b[39;49;00m os.listdir(os.path.join(data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)):\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(f)\r\n",
      "    \r\n",
      "    \u001b[37m# define your training logic here\u001b[39;49;00m\r\n",
      "    \u001b[37m# import tensorflow as pd\u001b[39;49;00m\r\n",
      "    \u001b[37m# import pandas as tf\u001b[39;49;00m\r\n",
      "\r\n",
      "    model = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# validate / test your model\u001b[39;49;00m\r\n",
      "    \u001b[37m# using test data\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m== Files in the test channel ==\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m f \u001b[35min\u001b[39;49;00m os.listdir(os.path.join(data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)):\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(f)\r\n",
      "    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m== Saving model checkpoint ==\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        pickle.dump(model, f)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m== training completed ==\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize container/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination\n",
    "\n",
    "It is a skeleton of a typical ML training logic. The main function fetches training data in `/opt/ml/input/data/train`. To verify we indeed have access to the data, we will print out the names of the files in `/opt/ml/input/data/train`. When you actually run this training logic on SageMaker, you can view the stdout through CloudWatch. We will discuss this in more detail later in this notebook. \n",
    "\n",
    "When the main function finishes model training, it saves the model checkpoint in `/opt/ml/model`. The SageMaker Instance running your container will then upload everything in `/opt/ml/model` to an S3 URI that you will later configure yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your container\n",
    "\n",
    "It is a good practice to test your container before sending it to SageMaker, because you can debug and iterate much faster on your local machine. \n",
    "\n",
    "You are strongly encouraged to read through the section on [How Amazon SageMaker Provides Training Information](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html) from the official doc and figure out local testing environment that replicates how SageMaker provides training information to your container. \n",
    "\n",
    "We will use docker python client to execute the container. To see our implementation of local testing environment, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# This script tests the your own container before running\u001b[39;49;00m\r\n",
      "\u001b[37m# on SageMaker infrastructure. It mimics how SageMaker provides\u001b[39;49;00m\r\n",
      "\u001b[37m# training info to your container and how it executes it. \u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdocker\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\r\n",
      "dirname = os.path.dirname(\r\n",
      "    os.path.realpath(\u001b[31m__file__\u001b[39;49;00m)\r\n",
      "    )\r\n",
      "\r\n",
      "client = docker.from_env()\r\n",
      "\r\n",
      "container = client.containers.run(\r\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33mexample-image:latest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m# docker run example-image:latest train \u001b[39;49;00m\r\n",
      "    volumes={\r\n",
      "        \u001b[37m# mount ml/ to /opt/ml as volume\u001b[39;49;00m\r\n",
      "        \u001b[37m# it's a mechanism for the operating \u001b[39;49;00m\r\n",
      "        \u001b[37m# system to communicate with inside of\u001b[39;49;00m\r\n",
      "        \u001b[37m# a docker container\u001b[39;49;00m\r\n",
      "        os.path.join(dirname, \u001b[33m'\u001b[39;49;00m\u001b[33mml\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) : {\u001b[33m'\u001b[39;49;00m\u001b[33mbind\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmode\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mrw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m}, \r\n",
      "        },\r\n",
      "    stderr=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "    detach=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "\r\n",
      "\u001b[37m# wait the execution to finish\u001b[39;49;00m\r\n",
      "container.wait()\r\n",
      "\r\n",
      "\u001b[37m# retrieve logs\u001b[39;49;00m\r\n",
      "byte_str=container.logs()\r\n",
      "\r\n",
      "\u001b[37m# decode byte string to utf-8 encoding\u001b[39;49;00m\r\n",
      "\u001b[36mprint\u001b[39;49;00m(byte_str.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize container/local_test/test_container.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination \n",
    "\n",
    "Our testing script runs the docker image `example-image:latest` with `train` command, mimicking how SageMaker runs your container for a training job. It mounts the local directory `container/local_test/ml/` to `/opt/ml` in the docker image, mimicking how SageMaker provides the training information to the container. \n",
    "\n",
    "The directory `container/local_test/ml` looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container/local_test/ml:\r\n",
      "input  model  output\r\n",
      "\r\n",
      "container/local_test/ml/input:\r\n",
      "data\r\n",
      "\r\n",
      "container/local_test/ml/input/data:\r\n",
      "test  train\r\n",
      "\r\n",
      "container/local_test/ml/input/data/test:\r\n",
      "test_data_batch1.csv  test_data_batch2.csv  test_data_batch3.csv\r\n",
      "\r\n",
      "container/local_test/ml/input/data/train:\r\n",
      "data_batch1.csv  data_batch2.csv  data_batch3.csv\r\n",
      "\r\n",
      "container/local_test/ml/model:\r\n",
      "model.pkl\r\n",
      "\r\n",
      "container/local_test/ml/output:\r\n",
      "failure\r\n",
      "\r\n",
      "container/local_test/ml/output/failure:\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R container/local_test/ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directories `container/local_test/ml/input/data/train` and `container/local_test/ml/input/data/test` contains some csv files, which will be available in `/opt/ml/input/data/train` and `/opt/ml/input/data/test` as the training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Files in train channel ==\r\n",
      "data_batch2.csv\r\n",
      "data_batch3.csv\r\n",
      "data_batch1.csv\r\n",
      "== Files in the test channel ==\r\n",
      "test_data_batch1.csv\r\n",
      "test_data_batch2.csv\r\n",
      "test_data_batch3.csv\r\n",
      "== Saving model checkpoint ==\r\n",
      "== training completed ==\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# run the test\n",
    "!python container/local_test/test_container.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see a model checkpoint in `container/local_test/ml/model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls container/local_test/ml/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push your docker image to ECR\n",
    "\n",
    "Now, you have build your image tested it locally. Next thing you need to do is to push it to the Elastic Container Registry under your account. Later, when you trigger a training job, the SageMaker instance you requested will pull that image. \n",
    "\n",
    "To do so, you will need to create a repo in your ECR to host it. You might have guess that this operation requires some permission on your ECR resources. That's right. You (the principal running this notebook) needs permission to create repository in ECR and get authorization token from it and the role you created before (which you will later pass to SageMaker) needs permission to get authorization token (and pull the image). \n",
    "\n",
    "If you have `AdministratorAccess` then you have permisssion to do everything on your AWS resources. For the service role `sm` we created at begining of this notebook, we attached `AmazonSageMakerFullAccess` to it and you might have guessed that this permission is kind of strong and common actions like pulling an image from ECR is included. You are right. But it is still interesting to verify that you and your agent (service role) have the necessary permissions.\n",
    "\n",
    "To do so, you can use `SimulatePrincipalPolicy` API from IAM. You guessed right, it simulates the principal's policy and tells you if certain actions are allowed. For more detail on `SimulatePrincipalPolicy`, refer to the [API reference](https://docs.aws.amazon.com/IAM/latest/APIReference/API_SimulatePrincipalPolicy.html) in the official docs or its [python equivalent](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.simulate_principal_policy) in boto3 documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== User's Permission Evaluation ==\n",
      "[{'EvalActionName': 'ecr:GetAuthorizationToken',\n",
      "  'EvalDecision': 'allowed',\n",
      "  'EvalResourceName': '*',\n",
      "  'MatchedStatements': [{'EndPosition': {'Column': 10, 'Line': 149},\n",
      "                         'SourcePolicyId': 'AmazonSageMakerFullAccess',\n",
      "                         'SourcePolicyType': 'IAM Policy',\n",
      "                         'StartPosition': {'Column': 10, 'Line': 43}},\n",
      "                        {'EndPosition': {'Column': 6, 'Line': 8},\n",
      "                         'SourcePolicyId': 'AdministratorAccess',\n",
      "                         'SourcePolicyType': 'IAM Policy',\n",
      "                         'StartPosition': {'Column': 17, 'Line': 3}},\n",
      "                        {'EndPosition': {'Column': 6, 'Line': 11},\n",
      "                         'SourcePolicyId': 'AmazonEC2ContainerRegistryFullAccess',\n",
      "                         'SourcePolicyType': 'IAM Policy',\n",
      "                         'StartPosition': {'Column': 17, 'Line': 3}}],\n",
      "  'MissingContextValues': []},\n",
      " {'EvalActionName': 'ecr:CreateRepository',\n",
      "  'EvalDecision': 'allowed',\n",
      "  'EvalResourceName': '*',\n",
      "  'MatchedStatements': [{'EndPosition': {'Column': 6, 'Line': 11},\n",
      "                         'SourcePolicyId': 'AmazonEC2ContainerRegistryFullAccess',\n",
      "                         'SourcePolicyType': 'IAM Policy',\n",
      "                         'StartPosition': {'Column': 17, 'Line': 3}},\n",
      "                        {'EndPosition': {'Column': 10, 'Line': 149},\n",
      "                         'SourcePolicyId': 'AmazonSageMakerFullAccess',\n",
      "                         'SourcePolicyType': 'IAM Policy',\n",
      "                         'StartPosition': {'Column': 10, 'Line': 43}},\n",
      "                        {'EndPosition': {'Column': 6, 'Line': 8},\n",
      "                         'SourcePolicyId': 'AdministratorAccess',\n",
      "                         'SourcePolicyType': 'IAM Policy',\n",
      "                         'StartPosition': {'Column': 17, 'Line': 3}}],\n",
      "  'MissingContextValues': []}]\n"
     ]
    }
   ],
   "source": [
    "user_arn = boto3.client('sts').get_caller_identity()['Arn'] # you\n",
    "\n",
    "user_prp = iam.simulate_principal_policy(\n",
    "    PolicySourceArn=user_arn,\n",
    "    ActionNames=['ecr:GetAuthorizationToken', 'ecr:CreateRepository']\n",
    ")\n",
    "print(\"== User's Permission Evaluation ==\")\n",
    "pp.pprint(user_prp['EvaluationResults'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Service Role Permission Evaluation ==\n",
      "[{'EvalActionName': 'ecr:GetAuthorizationToken',\n",
      "  'EvalDecision': 'allowed',\n",
      "  'EvalResourceName': '*',\n",
      "  'MatchedStatements': [{'EndPosition': {'Column': 10, 'Line': 149},\n",
      "                         'SourcePolicyId': 'AmazonSageMakerFullAccess',\n",
      "                         'SourcePolicyType': 'IAM Policy',\n",
      "                         'StartPosition': {'Column': 10, 'Line': 43}}],\n",
      "  'MissingContextValues': []}]\n"
     ]
    }
   ],
   "source": [
    "role_arn=role['Arn'] # your agent \n",
    "\n",
    "role_prp = iam.simulate_principal_policy(\n",
    "    PolicySourceArn=role_arn,\n",
    "    ActionNames=['ecr:GetAuthorizationToken']\n",
    ")\n",
    "print(\"== Service Role Permission Evaluation ==\")\n",
    "pp.pprint(role_prp['EvaluationResults'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you do not have enough permissions on the ECR resources under your organization's account. Then the admin of the account needs to grant you the ECR permissions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a repository in your ECR\n",
    "\n",
    "Suppose you have enough ECR permissions, we now create a repository in your ECR to host the image `example-image:latest`. It is convenient to set the name of the repository should be the same as the name of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'example-image' already exists in the registry with id '688520471316'\n"
     ]
    }
   ],
   "source": [
    "ecr = boto3.client('ecr')\n",
    "\n",
    "try:\n",
    "    # The repository might already exist\n",
    "    # in your ECR\n",
    "    cr_res = ecr.create_repository(\n",
    "        repositoryName='example-image')\n",
    "    pp.pprint(cr_res)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have a repository called `example-image`, then there are two ways you can continue\n",
    "* Delete the repository can create new one with the same name\n",
    "* Create a repository using a name other than `example-image`\n",
    "\n",
    "We will provide code for the second route below. But you will need to run it with caution, because the repository `example-image` is probably used by your org for production, and it happens to coincides with our choice of repository name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you want to delete the `example-image` repository,\n",
    "Change this cell from markdown to python, then run it. \n",
    "\"\"\"\n",
    "try:\n",
    "    ecr.delete_repository(\n",
    "        repositoryName='example-image')\n",
    "    \n",
    "    ecr.create_repository(\n",
    "        repositoryName='example-image')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag your image and push to ECR\n",
    "\n",
    "Now, let's tag the image with the full address of the repository we just created and push it there. Before doing that, you will need to grant docker access to your ECR. Refer to the [registry authentication section](https://docs.aws.amazon.com/AmazonECR/latest/userguide/registry_auth.html) from the ECR documentation for more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "The push refers to repository [688520471316.dkr.ecr.us-west-2.amazonaws.com/example-image]\n",
      "16c0bf8a256b: Preparing\n",
      "94149d717f86: Preparing\n",
      "88674bdc7fd9: Preparing\n",
      "78db50750faa: Preparing\n",
      "805309d6b0e2: Preparing\n",
      "2db44bce66cd: Preparing\n",
      "2db44bce66cd: Waiting\n",
      "94149d717f86: Pushed\n",
      "88674bdc7fd9: Pushed\n",
      "16c0bf8a256b: Pushed\n",
      "2db44bce66cd: Pushed\n",
      "78db50750faa: Pushed\n",
      "805309d6b0e2: Pushed\n",
      "latest: digest: sha256:64caa0b2f89c35a24e9648f1644d3efc7634054f2779460ea1064d87bb06c8af size: 1574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "account=$(aws sts get-caller-identity --query Account | sed -e 's/^\"//' -e 's/\"$//')\n",
    "region=$(aws configure get region)\n",
    "ecr_account=${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Give docker your ECR login password\n",
    "aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $ecr_account\n",
    "\n",
    "# Fullname of the repo\n",
    "fullname=$ecr_account/example-image:latest\n",
    "\n",
    "#echo $fullname\n",
    "# Tag the image with the fullname\n",
    "docker tag example-image:latest $fullname\n",
    "\n",
    "# Push to ECR\n",
    "docker push $fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'HTTPHeaders': {'content-length': '399',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Wed, 03 Mar 2021 19:56:36 GMT',\n",
      "                                      'x-amzn-requestid': 'c33de26f-f577-4fac-9566-d5e904225ac9'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'c33de26f-f577-4fac-9566-d5e904225ac9',\n",
      "                      'RetryAttempts': 0},\n",
      " 'imageDetails': [{'artifactMediaType': 'application/vnd.docker.container.image.v1+json',\n",
      "                   'imageDigest': 'sha256:64caa0b2f89c35a24e9648f1644d3efc7634054f2779460ea1064d87bb06c8af',\n",
      "                   'imageManifestMediaType': 'application/vnd.docker.distribution.manifest.v2+json',\n",
      "                   'imagePushedAt': datetime.datetime(2021, 3, 3, 19, 50, 30, tzinfo=tzlocal()),\n",
      "                   'imageSizeInBytes': 150950302,\n",
      "                   'imageTags': ['latest'],\n",
      "                   'registryId': '688520471316',\n",
      "                   'repositoryName': 'example-image'}]}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the ECR repository\n",
    "repo_res = ecr.describe_images(\n",
    "    repositoryName='example-image')\n",
    "pp.pprint(repo_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data\n",
    "\n",
    "SageMaker provides training data to your image through an S3 bucket that your service role has read access to. This means before triggering a training job, you need to make your training available in such an S3 bucket.\n",
    "\n",
    "In this notebook, we will use preloaded data on a public bucket `sagemaker-sample-files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/\n",
      "datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/churn.txt\n",
      "datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/identity.csv\n",
      "datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_identity.csv\n",
      "datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_transactions.csv\n",
      "datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/transaction.csv\n"
     ]
    }
   ],
   "source": [
    "# inspect the bucket\n",
    "public_bucket = \"sagemaker-sample-files\"\n",
    "s3 = boto3.client('s3')\n",
    "obj_res = s3.list_objects_v2(\n",
    "    Bucket=\"sagemaker-sample-files\")\n",
    "\n",
    "# print out object keys compactly\n",
    "for obj in obj_res['Contents']:\n",
    "    if '/tabular/fraud_detection/synthethic_fraud_detection_SA' in obj['Key']:\n",
    "        print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend the data under `datasets/tabular/synthetic_fraud_detection_SA` is the data for your ML project.\n",
    "\n",
    "The public bucket `sagemaker-sample-files` is located in us-east-1. We first need to copy the data to a bucket of yours that share the same region with the SageMaker instance you will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bucket\n",
    "def create_tmp_bucket():\n",
    "    \"\"\"Create an S3 bucket that is intended to be used for short term\"\"\"\n",
    "    bucket = \"{}-{}\".format(account_id(), current_time())\n",
    "    region = boto3.Session().region_name\n",
    "    boto3.client('s3').create_bucket(\n",
    "        Bucket=bucket,\n",
    "        CreateBucketConfiguration={\n",
    "            'LocationConstraint': region\n",
    "        })\n",
    "    return bucket\n",
    "\n",
    "bucket = create_tmp_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bucket is created by you. By default, all objects in the bucket are private and are accessible by you. But later you will need SageMaker to read input data from write model artifact to it. Therefore, you will need to grant read and write access to the bucket to the execution role `sm`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:Get*\" # Allow the role to perform list related actions, i.e read access\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    \"arn:aws:s3:us-west-2:688520471316-2021-03-03-22-46-41:*\" \n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    \n",
    "# create a new policy\n",
    "policy_name='s3get'\n",
    "policy = iam.create_policy(\n",
    "    PolicyName=policy_name,\n",
    "    PolicyDocument=json.dumps(get))['Policy']\n",
    "\n",
    "# attach the policy to the role\n",
    "res = iam.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=policy['Arn']\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'HTTPHeaders': {'content-length': '212',\n",
      "                                      'content-type': 'text/xml',\n",
      "                                      'date': 'Thu, 04 Mar 2021 02:13:07 GMT',\n",
      "                                      'x-amzn-requestid': 'b517f83e-13a7-4426-bd2e-defefdcb9272'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'b517f83e-13a7-4426-bd2e-defefdcb9272',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "bucket_arn = \"arn:aws:s3:::{}/*\".format(bucket)\n",
    "\n",
    "get_put = {\n",
    "    \"Version\":\"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:Get*\",\n",
    "                \"s3:Put*\"\n",
    "            ],\n",
    "            \"Resource\": bucket_arn\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "pm_res = attach_permission(\n",
    "    role_name=role['RoleName'],\n",
    "    policy_name='get_put',\n",
    "    policy_doc=get_put)\n",
    "\n",
    "pp.pprint(pm_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prefix = 'input_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from sagemaker-samplef-files to {bucket}\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# copy remote csv files to local\n",
    "files = []\n",
    "data_dir = '/tmp'\n",
    "for obj in obj_res['Contents']:\n",
    "    if '/tabular/fraud_detection/synthethic_fraud_detection_SA' in obj['Key']:\n",
    "        key = obj['Key']\n",
    "        if key.endswith('.csv'):\n",
    "            filename=key.split('/')[-1]\n",
    "            files.append(filename)\n",
    "            with open(os.path.join(data_dir, filename), 'wb') as f:\n",
    "                s3.download_fileobj(public_bucket, key, f)\n",
    "\n",
    "# upload from local to the bucket you just created\n",
    "for fname in files:\n",
    "    with open(os.path.join(data_dir, fname), 'rb') as f:\n",
    "        key = input_prefix + fname\n",
    "        s3.upload_fileobj(f, bucket, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/identity.csv\n",
      "input_data/sampled_identity.csv\n",
      "input_data/sampled_transactions.csv\n",
      "input_data/transaction.csv\n"
     ]
    }
   ],
   "source": [
    "# inspect your bucket\n",
    "obj_res = s3.list_objects_v2(\n",
    "    Bucket=bucket)\n",
    "\n",
    "for obj in obj_res['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare an S3 URI for saving model artifact\n",
    "\n",
    "After your image is done with model training, it needs to write the trained model artifact into `/opt/ml/model`. This is directory where SageMaker looks for the trained model artifact and upload it to an S3 URI you will configure later. Naturally, the execution role `sm` needs to have write permission to this S3 URI. \n",
    "\n",
    "Refer to the section on [How Amazon SageMaker Processes Training Output](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-output.html) in the official docs for more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put everything together\n",
    "\n",
    "Now, you have everything you need to create a training job. Let's review what you have done. you have \n",
    "* created an execution role for SageMaker service\n",
    "* built and tested a docker image that includes the runtime and logic of your model training\n",
    "* made the image accessible to SageMaker by hosting it on ECR\n",
    "* made the training data available to SageMaker by hosting it on S3\n",
    "* pointed SageMaker to an S3 bucket to write output \n",
    "\n",
    "Let pull the trigger and create a training job. We will invoke `CreateTrainingJob` API via boto3. You are strongly encouraged to read through the [description of the API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job) before moving on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "\n",
    "# name training job\n",
    "training_job_name = 'example-training-job-{}'.format(current_time())\n",
    "\n",
    "# input data prefix\n",
    "data_path = \"s3://\" + bucket + '/' + input_prefix\n",
    "\n",
    "# location that SageMaker saves the model artifacts\n",
    "output_prefix = 'example/output/'\n",
    "output_path = \"s3://\" + bucket + '/' + output_prefix\n",
    "\n",
    "# ECR URI of your image\n",
    "region = boto3.Session().region_name\n",
    "account = account_id()\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/example-image:latest\".format(account, region)\n",
    "\n",
    "algorithm_specification = {\n",
    "    'TrainingImage': image_uri,\n",
    "    'TrainingInputMode': 'File',\n",
    "}\n",
    "\n",
    "\n",
    "input_data_config = [\n",
    "    {\n",
    "        'ChannelName': 'train',\n",
    "            'DataSource':{\n",
    "                'S3DataSource':{\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': data_path,\n",
    "                    'S3DataDistributionType': 'FullyReplicated',\n",
    "                }\n",
    "        }\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'ChannelName': 'test',\n",
    "        'DataSource':{\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': data_path,\n",
    "                'S3DataDistributionType': 'FullyReplicated',\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "output_data_config = {\n",
    "    'S3OutputPath': output_path\n",
    "}\n",
    "\n",
    "resource_config = {\n",
    "    'InstanceType': 'ml.m5.large',\n",
    "    'InstanceCount':1,\n",
    "    'VolumeSizeInGB':10\n",
    "}\n",
    "\n",
    "stopping_condition={\n",
    "    'MaxRuntimeInSeconds':120,\n",
    "    #'MaxWaitTimeInSeconds': 123\n",
    "}\n",
    "\n",
    "enable_network_isolation=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_res = sm_boto3.create_training_job(\n",
    "    TrainingJobName=training_job_name,\n",
    "    AlgorithmSpecification=algorithm_specification,\n",
    "    RoleArn=role_arn,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    ResourceConfig=resource_config,\n",
    "    StoppingCondition=stopping_condition,\n",
    "    EnableNetworkIsolation=enable_network_isolation,\n",
    "    EnableManagedSpotTraining=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['TrainingJobName', 'TrainingJobArn', 'ModelArtifacts', 'TrainingJobStatus', 'SecondaryStatus', 'AlgorithmSpecification', 'RoleArn', 'InputDataConfig', 'OutputDataConfig', 'ResourceConfig', 'StoppingCondition', 'CreationTime', 'TrainingStartTime', 'TrainingEndTime', 'LastModifiedTime', 'SecondaryStatusTransitions', 'EnableNetworkIsolation', 'EnableInterContainerTrafficEncryption', 'EnableManagedSpotTraining', 'TrainingTimeInSeconds', 'BillableTimeInSeconds', 'ProfilingStatus', 'ResponseMetadata'])\n"
     ]
    }
   ],
   "source": [
    "# View the status of the training job\n",
    "tj_state = sm_boto3.describe_training_job(\n",
    "    TrainingJobName=training_job_name)\n",
    "pp.pprint(tj_state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job completed\n"
     ]
    }
   ],
   "source": [
    "# check training job status every 30 seconds\n",
    "stopped = False\n",
    "while not stopped:\n",
    "    tj_state = sm_boto3.describe_training_job(\n",
    "        TrainingJobName=training_job_name)\n",
    "    if tj_state['TrainingJobStatus'] in ['Completed', 'Stopped', 'Failed']:\n",
    "        stopped=True\n",
    "    else:\n",
    "        print(\"Training in progress\")\n",
    "        time.sleep(30)\n",
    "\n",
    "if tj_state['TrainingJobStatus'] == 'Failed':\n",
    "    print(\"Training job failed \")\n",
    "    print(\"Failed Reason: {}\".tj_state['FailedReason'])\n",
    "else:\n",
    "    print(\"Training job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the trained model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Output config:\n",
      "{'KmsKeyId': '', 'S3OutputPath': 's3://688520471316-2021-03-03-22-46-41/example/output/'}\n",
      "\n",
      "== Model artifact:\n",
      "{'Contents': [{'ETag': '\"cea072960b7b3a427bebf56f5dca5071\"',\n",
      "               'Key': 'example/output/example-training-job-2021-03-04-02-16-41/output/model.tar.gz',\n",
      "               'LastModified': datetime.datetime(2021, 3, 4, 2, 20, 17, tzinfo=tzlocal()),\n",
      "               'Size': 120,\n",
      "               'StorageClass': 'STANDARD'}],\n",
      " 'EncodingType': 'url',\n",
      " 'IsTruncated': False,\n",
      " 'KeyCount': 1,\n",
      " 'MaxKeys': 1000,\n",
      " 'Name': '688520471316-2021-03-03-22-46-41',\n",
      " 'Prefix': 'example/output/',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-type': 'application/xml',\n",
      "                                      'date': 'Thu, 04 Mar 2021 02:31:07 GMT',\n",
      "                                      'server': 'AmazonS3',\n",
      "                                      'transfer-encoding': 'chunked',\n",
      "                                      'x-amz-bucket-region': 'us-west-2',\n",
      "                                      'x-amz-id-2': 'GVxmJumcT7MW9u50/XQOAi778b1uNi3Nn0d+5T0qzu9h8FvfDy5n6MM5USgl+fk6UMAwcJvavJA=',\n",
      "                                      'x-amz-request-id': 'JTB67ESMSWP70QJY'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'HostId': 'GVxmJumcT7MW9u50/XQOAi778b1uNi3Nn0d+5T0qzu9h8FvfDy5n6MM5USgl+fk6UMAwcJvavJA=',\n",
      "                      'RequestId': 'JTB67ESMSWP70QJY',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(\"== Output config:\")\n",
    "print(tj_state['OutputDataConfig'])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"== Model artifact:\")\n",
    "pp.pprint(s3.list_objects_v2(Bucket=bucket, Prefix=output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Files in train channel ==\n",
      "transaction.csv\n",
      "sampled_identity.csv\n",
      "sampled_transactions.csv\n",
      "identity.csv\n",
      "== Files in the test channel ==\n",
      "transaction.csv\n",
      "sampled_identity.csv\n",
      "sampled_transactions.csv\n",
      "identity.csv\n",
      "== Saving model checkpoint ==\n",
      "== training completed ==\n"
     ]
    }
   ],
   "source": [
    "logs = boto3.client('logs')\n",
    "\n",
    "log_res= logs.describe_log_streams(\n",
    "    logGroupName='/aws/sagemaker/TrainingJobs',\n",
    "    logStreamNamePrefix=training_job_name)\n",
    "\n",
    "for log_stream in log_res['logStreams']:\n",
    "    # get one log event\n",
    "    log_event = logs.get_log_events(\n",
    "        logGroupName='/aws/sagemaker/TrainingJobs',\n",
    "        logStreamName=log_stream['logStreamName'])\n",
    "    \n",
    "    # print out messages from the log event\n",
    "    for ev in log_event['events']:\n",
    "        for k, v in ev.items():\n",
    "            if k == 'message':\n",
    "                print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You now understand the basics of a training job on SageMaker. It's funny to think that after this long notebook, you get a trained model artifact, which is a pickled None instance. But keep in mind that you can follow the exact same process to train a state-of-art model with billions of parameters and the compute cost is proportional to how long you train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the ECR repo\n",
    "del_repo_res = ecr.delete_repository(\n",
    "    repositoryName='example-image',\n",
    "    force=True)\n",
    "\n",
    "pp.pprint(del_repo_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'HTTPHeaders': {'date': 'Thu, 04 Mar 2021 03:09:12 GMT',\n",
      "                                      'server': 'AmazonS3',\n",
      "                                      'x-amz-id-2': 'Trw1jLE9sIZbTSBm4VQD3Gpio1DIBdiDrJ5y4IvynC5dnu+0VSUoNYQ5TLJpowZYMFlwxUQlhxM=',\n",
      "                                      'x-amz-request-id': '6CR3EN9H0WNG6SK2'},\n",
      "                      'HTTPStatusCode': 204,\n",
      "                      'HostId': 'Trw1jLE9sIZbTSBm4VQD3Gpio1DIBdiDrJ5y4IvynC5dnu+0VSUoNYQ5TLJpowZYMFlwxUQlhxM=',\n",
      "                      'RequestId': '6CR3EN9H0WNG6SK2',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# delete the S3 bucket\n",
    "def delete_bucket_force(bucket_name):\n",
    "    objs = s3.list_objects_v2(Bucket=bucket_name)['Contents']\n",
    "    for obj in objs:\n",
    "        s3.delete_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=obj['Key'])\n",
    "    \n",
    "    return s3.delete_bucket(Bucket=bucket_name)\n",
    "\n",
    "del_buc_res = delete_bucket_force(bucket)\n",
    "\n",
    "pp.pprint(del_buc_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
