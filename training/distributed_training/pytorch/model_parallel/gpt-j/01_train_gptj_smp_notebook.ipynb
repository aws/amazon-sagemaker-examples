{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train EleutherAI GPT-J with PyTorch 1.8.1 and Pipeline Parallelism Using the SageMaker Model Parallelism Library\n",
    "\n",
    "**Please run this notebook with Data Science-> Python 3 Kernel on SageMaker Studio Notebook**\n",
    "\n",
    "This notebook walks you through how to train the [EleutherAI's](https://www.eleuther.ai/) [GPT-J](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/) model with SageMaker's model parallelism.\n",
    "EleutherAI released GPT-J 6B, an open-source alternative to [OpenAIs GPT-3](https://openai.com/blog/gpt-3-apps/). [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6B) is the 6 billion parameter successor to EleutherAIs GPT-NEO family, a family of transformer-based language models based on the GPT architecture for text generation.\n",
    "\n",
    "EleutherAI's primary goal is to train a model that is equivalent in size to GPT⁠-⁠3 and make it available to the public under an open license.\n",
    "Over the last few months, GPT-J gained a lot of interest from Researchers, Data Scientists, and even Software Developers, but it remained very challenging to fine tune GPT-J.\n",
    "\n",
    "The weights of the 6 billion parameter model represent a ~24GB memory footprint. To load it in float32, one would need at least 2x model size CPU RAM: 1x for initial weights and another 1x to load the checkpoint. Apart from the model parameters, there are the gradients, optimizer states, and activations taking memory, so the actual memory usage might be significantly higher than 48GB. Just as an example, with Adam optimizer and FP32 training, the use from parameters, gradients and optimizer states might be 96GB+, and activation memory footprint would be even more than this, so the total memory usage might be easily larger than 200 GB.\n",
    "\n",
    "In this notebook, you will learn how to easily fine tune GPT-J using Amazon SageMaker and Hugging Face on NVIDIA GPU instances.\n",
    "\n",
    "This notebook depends on the following files and folders:\n",
    "\n",
    "1. `train_gptj_smp_script.py`: This is an entrypoint script that is passed to the PyTorch estimator in the notebook instructions. This script is responsible for end to end training of the GPT-J model with SMP. The script has additional comments at places where the SMP API is used.\n",
    "2. `fp16`: This folder is used for 16-bit float training, which contains a fp16 optimizer and various fp16 utilities.\n",
    "3. `learning_rates.py`: This contains the functions for learning rate schedule.\n",
    "4. `requirements.txt`: This will install the dependencies, like the right version of huggingface transformers.\n",
    "5. `preprocess.py`: This will download and preprocess the sst2/glue dataset.\n",
    "6. `args.py`: collection of difference arguments like training, data, SageMaker Model Parallel related args.\n",
    "7. `smp_trainer.py`.py: Defines the SageMaker Model Parallel Trainer class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Distributed Training \n",
    "\n",
    "SageMaker provides distributed training libraries for data parallelism and model parallelism. The libraries are optimized for the SageMaker training environment, help adapt your distributed training jobs to SageMaker, and improve training speed and throughput.\n",
    "\n",
    "### Approaches\n",
    "\n",
    "![SageMaker Distributed Training Approaches](img/TypesOfDistributedTraining.png)\n",
    "\n",
    "\n",
    "### SageMaker Model Parallel\n",
    "\n",
    "Model parallelism is the process of splitting a model up between multiple devices or nodes (such as GPU-equipped instances) and creating an efficient pipeline to train the model across these devices to maximize GPU utilization.\n",
    "\n",
    "Increasing deep learning model size (layers and parameters) can result in better accuracy. However, there is a limit to the maximum model size you can fit in a single GPU. When training deep learning models, GPU memory limitations can be a bottleneck in the following ways:\n",
    "\n",
    "1. They can limit the size of the model you train. Given that larger models tend to achieve higher accuracy, this directly translates to trained model accuracy.\n",
    "\n",
    "2. They can limit the batch size you train with, leading to lower GPU utilization and slower training.\n",
    "\n",
    "To overcome the limitations associated with training a model on a single GPU, you can use model parallelism to distribute and train your model on multiple computing devices.\n",
    "\n",
    "### Core features of SageMaker Model Parallel \n",
    "\n",
    "1. [Automated Model Splitting](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html): When you use SageMaker's model parallel library, you can take advantage of automated model splitting, also referred to as automated model partitioning. The library uses a partitioning algorithm that balances memory, minimizes communication between devices, and optimizes performance. You can configure the automated partitioning algorithm to optimize for speed or memory.\n",
    "\n",
    "2. [Pipeline Execution Schedule](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html): A core feature of SageMaker's distributed model parallel library is pipelined execution, which determines the order in which computations are made and data is processed across devices during model training. Pipelining is a technique to achieve true parallelization in model parallelism, by having the GPUs compute simultaneously on different data samples, and to overcome the performance loss due to sequential computation.\n",
    "\n",
    "Pipelining is based on splitting a mini-batch into microbatches, which are fed into the training pipeline one-by-one and follow an execution schedule defined by the library runtime. A microbatch is a smaller subset of a given training mini-batch. The pipeline schedule determines which microbatch is executed by which device for every time slot.\n",
    "\n",
    "In addition to its [core features](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html), the SageMaker distributed model parallel library offers [memory-saving features](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch.html) for training deep learning models with PyTorch: [tensor parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-tensor-parallelism.html), [optimizer state sharding](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-optimizer-state-sharding.html), [activation checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html), and [activation offloading](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-offloading.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Model Parallel configuration\n",
    "\n",
    "Please refer to all the [configuration parameters](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html) related to SageMaker Distributed Training.\n",
    "\n",
    "As we are going to use PyTorch and Hugging Face for training GPT-J, it is important to understand all the SageMaker Distributed configuration parameters specific to PyTorch [here](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#pytorch-specific-parameters).\n",
    "\n",
    "#### Important\n",
    "\n",
    "`process_per_host` must not be greater than the number of GPUs per instance and typically will be equal to the number of GPUs per instance.\n",
    "\n",
    "For example, if you use one instance with 4-way pipeline parallelism and 2-way data parallelism, then processes_per_host should be 2 x 4 = 8. Therefore, you must choose an instance that has at least 8 GPUs, such as an ml.p3.16xlarge.\n",
    "\n",
    "The following image illustrates how 4-way data parallelism and 2-way pipeline parallelism is distributed across 8 GPUs: the models is partitioned across 2 GPUs, and each partition is added to 4 GPUs.\n",
    "\n",
    "It is also important to understand how the [ranking mechanism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-ranking-mechanism.html) of model parallelism works with tensor parallelism. This is extended from the Ranking Basics for Core Features of the SageMaker Model Parallel Library.\n",
    "\n",
    "![SageMaker Distributed Training Approaches](img/SMP-Pipeline-Parallel-DDP.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Resources\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to learn more about SMP and using SageMaker with PyTorch.\n",
    "\n",
    "1. To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "2. To learn more about using the SageMaker Python SDK with PyTorch, see Using [PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "3. To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon SageMaker Initialization\n",
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment, such as your AWS account ID, the AWS Region, and the ARN of your Amazon SageMaker execution role.\n",
    "\n",
    "Upgrade SageMaker SDK to the latest version.\n",
    "\n",
    "NOTE: This step might require a kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.21.5)\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (2.83.0)\n",
      "Requirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.1.35)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
      "Collecting torchnet\n",
      "  Using cached torchnet-0.0.4.tar.gz (23 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[20 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 36, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-wsp0gjsm/torchnet_b4e0d323615649fe827e0702f2929ccf/setup.py\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(**setup_info)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/__init__.py\", line 87, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_distutils/core.py\", line 109, in setup\n",
      "  \u001b[31m   \u001b[0m     _setup_distribution = dist = klass(attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 466, in __init__\n",
      "  \u001b[31m   \u001b[0m     for k, v in attrs.items()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_distutils/dist.py\", line 293, in __init__\n",
      "  \u001b[31m   \u001b[0m     self.finalize_options()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 885, in finalize_options\n",
      "  \u001b[31m   \u001b[0m     for ep in sorted(loaded, key=by_order):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 884, in <lambda>\n",
      "  \u001b[31m   \u001b[0m     loaded = map(lambda e: e.load(), filtered)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_vendor/importlib_metadata/__init__.py\", line 196, in load\n",
      "  \u001b[31m   \u001b[0m     return functools.reduce(getattr, attrs, module)\n",
      "  \u001b[31m   \u001b[0m AttributeError: type object 'Distribution' has no attribute '_finalize_feature_opts'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: urllib3\n",
      "Version: 1.26.9\n",
      "Summary: HTTP library with thread-safe connection pooling, file post, and more.\n",
      "Home-page: https://urllib3.readthedocs.io/\n",
      "Author: Andrey Petrov\n",
      "Author-email: andrey.petrov@shazow.net\n",
      "License: MIT\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: \n",
      "Required-by: botocore, requests, responses\n"
     ]
    }
   ],
   "source": [
    "!pip show urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.83.0)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.88.1.tar.gz (527 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[22 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /opt/conda/lib/python3.7/site-packages/setuptools/dist.py:760: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "  \u001b[31m   \u001b[0m   % (opt, underscore_opt)\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 36, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-rntygq22/sagemaker_c3114472923647b897609f65de9486fd/setup.py\", line 102, in <module>\n",
      "  \u001b[31m   \u001b[0m     \"sagemaker-upgrade-v2=sagemaker.cli.compatibility.v2.sagemaker_upgrade_v2:main\",\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/__init__.py\", line 87, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_distutils/core.py\", line 109, in setup\n",
      "  \u001b[31m   \u001b[0m     _setup_distribution = dist = klass(attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 466, in __init__\n",
      "  \u001b[31m   \u001b[0m     for k, v in attrs.items()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_distutils/dist.py\", line 293, in __init__\n",
      "  \u001b[31m   \u001b[0m     self.finalize_options()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 885, in finalize_options\n",
      "  \u001b[31m   \u001b[0m     for ep in sorted(loaded, key=by_order):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 884, in <lambda>\n",
      "  \u001b[31m   \u001b[0m     loaded = map(lambda e: e.load(), filtered)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_vendor/importlib_metadata/__init__.py\", line 196, in load\n",
      "  \u001b[31m   \u001b[0m     return functools.reduce(getattr, attrs, module)\n",
      "  \u001b[31m   \u001b[0m AttributeError: type object 'Distribution' has no attribute '_finalize_feature_opts'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker: 2.83.0\n",
      "transformers: 4.18.0\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import sagemaker\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "sagemaker_session = LocalSession()\n",
    "sagemaker_session.config = {\"local\": {\"local_code\": True}}\n",
    "\n",
    "print(f\"sagemaker: {sagemaker.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role: arn:aws:iam::232838030412:role/service-role/AmazonSageMaker-ExecutionRole-20211204T182243\n",
      "AWS account: 232838030412\n",
      "AWS region: us-west-2\n",
      "\n",
      "Default bucket for this session:  sagemaker-us-west-2-232838030412\n",
      "CPU times: user 333 ms, sys: 33.3 ms, total: 367 ms\n",
      "Wall time: 895 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import boto3\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = f\"s3://sagemaker-{region}-{account}/smp-model-parallel-outputdir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script fine-tunes GPT-J on the `sst2` dataset. \n",
    "\n",
    "#### DataLoader \n",
    "\n",
    "The DataLoader and Sampler is defined in `smp_trainer.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Hyperparameters\n",
    "We will train on 4-node p3.16xlarge cluster.  Total number of GPUs in the cluster will be 32.\n",
    "We will use 16-way pipeline parallelism and 2-way data parallel. Please note the `ddp=True` enables PyTorch's Distributed Data Parallel (DDP).\n",
    "\n",
    "`(pipeline parallelism degree) x (data parallelism degree) = total number of GPUs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"EleutherAI/gpt-j-6B\"\n",
    "ddp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_s3 = False\n",
    "if load_from_s3:\n",
    "    %store -r model_location\n",
    "    model_name_or_path = model_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"dataset_name\": \"glue\",\n",
    "    \"dataset_config_name\": \"sst2\",\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"load_from_s3\": False,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"output_dir\": \"./temp\",\n",
    "    \"model_name_or_path\": model_name_or_path,\n",
    "    \"load_from_s3\": load_from_s3,\n",
    "    \"max_steps\": 100,\n",
    "    \"seed\": 12345,\n",
    "    \"lr\": 2.0e-4,\n",
    "    \"lr_decay_iters\": 125000,\n",
    "    \"min_lr\": 0.00001,\n",
    "    \"warmup\": 0.01,\n",
    "    \"shard_optimizer_state\": 1,\n",
    "    \"activation_checkpointing\": 1,\n",
    "    \"activation_strategy\": \"each\",\n",
    "    \"optimize\": \"memory\",\n",
    "    \"ddp\": ddp,\n",
    "    \"cache_dir\": \"/tmp\",\n",
    "    \"save_final_full_model\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \"gpt-j-6B\"\n",
    "\n",
    "if model_config == \"gpt-j-6B\":\n",
    "    model_params = {\n",
    "        \"tensor_parallel_degree\": 1,\n",
    "        \"pipeline_parallel_degree\": 8,\n",
    "        \"prescaled_batch\": 0,#was 1\n",
    "    }\n",
    "    \n",
    "for k, v in model_params.items():\n",
    "    hyperparameters[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_per_host = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "mpioptions += \"-x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    "mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\"\n",
    "\n",
    "\n",
    "mpi = {\n",
    "    \"enabled\": True,\n",
    "    \"processes_per_host\": process_per_host,\n",
    "    \"custom_mpi_options\": mpioptions,\n",
    "}\n",
    "\n",
    "smdistributed = {\n",
    "    \"modelparallel\": {\n",
    "        \"enabled\": True,\n",
    "        \"parameters\": {\n",
    "            \"ddp\": hyperparameters[\"ddp\"],\n",
    "            \"microbatches\": 2,\n",
    "            # partitions is a required param in the current SM SDK so it needs to be passed,\n",
    "            # these two map to the same config\n",
    "            \"partitions\": hyperparameters[\"pipeline_parallel_degree\"],\n",
    "            \"shard_optimizer_state\": hyperparameters[\"shard_optimizer_state\"] > 0,\n",
    "            \"prescaled_batch\": hyperparameters[\"prescaled_batch\"] > 0,\n",
    "            \"optimize\": hyperparameters[\"optimize\"],\n",
    "            \"auto_partition\": True,\n",
    "            \"default_partition\": 0,\n",
    "            \"offload_activations\": True,\n",
    "            \"active_microbatches\": 2,\n",
    "            \"optimize\": hyperparameters[\"optimize\"],\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "distribution = {\"mpi\": mpi, \"smdistributed\": smdistributed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SageMaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import datetime\n",
    "\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "volume_size = 900\n",
    "instance_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Specify your experiment name\n",
    "experiment_name = \"smp-gptj-model-parallel\"\n",
    "# Specify your trial name\n",
    "trial_name = f'{experiment_name}-trial1' \n",
    "\n",
    "all_experiment_names = [exp.experiment_name for exp in Experiment.list()]\n",
    "# Load the experiment if it exists, otherwise create \n",
    "if experiment_name not in all_experiment_names:\n",
    "    experiment = Experiment.create(experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client)\n",
    "else:\n",
    "    experiment = Experiment.load(experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client)\n",
    "\n",
    "# Create the trial\n",
    "trial = Trial.create(\n",
    "        trial_name=\"smp-{}-{}\".format(trial_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())),\n",
    "        experiment_name=experiment.experiment_name,\n",
    "        sagemaker_boto_client=sm_boto_client,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_str = instance_type.split('.')[1] + instance_type.split('.')[2][:3]\n",
    "pp_degree = hyperparameters['pipeline_parallel_degree']\n",
    "tp_degree = hyperparameters['tensor_parallel_degree']\n",
    "base_job_name = f'smp-{model_config}-{machine_str}-tp{tp_degree}-pp{pp_degree}-bs{hyperparameters[\"per_device_train_batch_size\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "\n",
    "smp_estimator = HuggingFace(\n",
    "        entry_point=\"train_gptj_smp_script.py\",\n",
    "        source_dir=os.getcwd(),\n",
    "        role=role,\n",
    "        instance_type=instance_type,\n",
    "        volume_size=volume_size,\n",
    "        instance_count=instance_count,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        distribution=distribution,\n",
    "        pytorch_version='1.10.2',\n",
    "        transformers_version='4.17.0',\n",
    "        py_version='py38',\n",
    "        output_path=s3_output_bucket,\n",
    "        hyperparameters=hyperparameters,\n",
    "        debugger_hook_config=False,\n",
    "        disable_profiler=True,\n",
    "        base_job_name=base_job_name,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: smp-gpt-j-6B-p4d24x-tp1-pp8-bs2-2022-04-28-13-36-17-108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-28 13:36:17 Starting - Starting the training job......\n",
      "2022-04-28 13:37:17 Starting - Preparing the instances for training....."
     ]
    }
   ],
   "source": [
    "smp_estimator.fit(experiment_config={\n",
    "                    \"ExperimentName\": experiment.experiment_name,\n",
    "                    \"TrialName\": trial.trial_name,\n",
    "                    \"TrialComponentDisplayName\": \"Training\",\n",
    "                  },\n",
    "                  logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Training Logs\n",
    "\n",
    "You can access the training logs from [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html). Make sure to look at the logs of **algo-1** because that is the main node whose output stream will have the training job logs.\n",
    "\n",
    "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see [SageMaker Jobs and Endpoint Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-jobs) in the Amazon SageMaker Developer Guide.\n",
    "\n",
    "If you are a new user of CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html). \n",
    "\n",
    "For additional information on monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html).\n",
    "\n",
    "## Deploying Trained Model for Inference\n",
    "\n",
    "In most cases, a trained model can be deployed on a single device for inference because inference only requires a small amount of memory. You can use the SMP API to create a single, unified model after training: the [smp.DistributedModel.save_model()](https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/latest/smd_model_parallel_tensorflow.html#smp.DistributedModel.save_model) method for TensorFlow, and the [smp.save()](https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/latest/smd_model_parallel_pytorch.html#apis-for-saving-and-loading) function for PyTorch.\n",
    "\n",
    "After you build and train your models, you can deploy them to get predictions in one of two ways:\n",
    "\n",
    "* To set up a persistent endpoint to get predictions from your models, use SageMaker hosting services. For an overview on deploying a single model or multiple models with SageMaker hosting services, see [Deploy a Model on SageMaker Hosting Services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html#how-it-works-hosting).\n",
    "* To get predictions for an entire dataset, use SageMaker batch transform. For an overview on deploying a model with SageMaker Batch Transform, see [Get Inferences for an Entire Dataset with Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html).\n",
    "\n",
    "To learn more about deploying models for inference using SageMaker, see [Deploy Models for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html). \n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
