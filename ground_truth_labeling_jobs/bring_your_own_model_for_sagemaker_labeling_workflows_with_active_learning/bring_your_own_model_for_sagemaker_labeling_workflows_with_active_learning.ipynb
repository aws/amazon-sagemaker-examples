{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Active Learning Workflow using Amazon SageMaker Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part 1: Create Resources Needed for an Active Learning Workflow\n",
    "\n",
    "Use this part of the notebook to create the resources required to create an automated labeling workflow for a text-classification labeling job. Specifically, we will create:\n",
    "\n",
    "* An input manifest file using the UCI News Dataset with 20% of the data labeled\n",
    "* A CreateLabelingJob request\n",
    "\n",
    "**This notebook is intended to be used along side the blog post [Bring your own model for SageMaker labeling workflows with Active Learning](https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-for-amazon-sagemaker-labeling-workflows-with-active-learning/),  Part 1: Create an Active Learning Workflow with BlazingText**.\n",
    "\n",
    "While following along with this blog post, we recommend that you leave most of the cells unmodified. However, the notebook will indicate where you can modify variables to create the resources needed for a custom labeling job.\n",
    "\n",
    "If you plan to customize the Ground Truth labeling job request configuration below, you will also need the resources required to create a labeling job. For more information, see [Use Amazon SageMaker Ground Truth for Data Labeling](https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html). \n",
    "\n",
    "### Using this Notebook\n",
    "\n",
    "Please set the kernel to *conda_tensorflow_p36* when running this notebook.\n",
    "\n",
    "Run the code cells in this notebook to configure a Labeling Job request in JSON format. This request JSON can be used in an active learning workflow and will determine how your labeling job task appears to human workers. \n",
    "\n",
    "To customize this notebook, you will need to modify the the cells below and configure the Ground Truth labeling job request (`human_task_config`) to meet your requirements. To learn how to create a Ground Truth labeling job using the Amazon SageMaker API, see [CreateLabelingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateLabelingJob.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, we will set up our environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, sagemaker, tensorflow as tf, pandas as pd, boto3, numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sess.boto_session.region_name\n",
    "bucket = sess.default_bucket()\n",
    "key = \"sagemaker-byoal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Prepare labeling input manifest file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We will create an input manifest file for our active learning workflow using the newsCorpora.csv file from the [UCI News Dataset](https://archive.ics.uci.edu/ml/datasets/News+Aggregator). This dataset contains a list of about 420,000 articles that fall into one of four categories: Business (b), Science & Technology (t), Entertainment (e) and Health & Medicine (m). We will randomly choose 10,000 articles from that file to create our dataset.\n",
    "\n",
    "For the active learning loop to start, 20% of the data must be labeled. To quickly test the active learning component, we will include 20% (`labeled_count`) of the original labels provided in the dataset in our input manifest. We use this partially-labeled dataset as the input to the active learning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip --no-check-certificate && unzip NewsAggregatorDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "column_names = [\"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
    "manifest_file = \"partially-labeled.manifest\"\n",
    "news_data_all = pd.read_csv(\"newsCorpora.csv\", names=column_names, header=None, delimiter=\"\\t\")\n",
    "news_data = news_data_all.sample(n=10000, random_state=42)\n",
    "news_data = news_data[[\"TITLE\", \"CATEGORY\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We will clean our data set using *pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "news_data[\"TITLE\"].replace('\"', \"\", inplace=True, regex=True)\n",
    "news_data[\"TITLE\"].replace(\"[^\\w\\s]\", \"\", inplace=True, regex=True)\n",
    "news_data[\"TITLE\"] = news_data[\"TITLE\"].str.split(\"\\n\").str[0]\n",
    "news_data[\"CATEGORY\"] = news_data[\"CATEGORY\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "fixed = news_data[\"TITLE\"].str.lower().replace('\"', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "news_data.to_csv(\"news_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The following cell will create our partially-labeled input manifest file, and push it to our S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "total = len(news_data)\n",
    "labeled_count = int(total / 5)  # 20% of the dataset is labeled.\n",
    "label_map = {\n",
    "    \"b\": \"Business\",\n",
    "    \"e\": \"Entertainment\",\n",
    "    \"m\": \"Health & Medicine\",\n",
    "    \"t\": \"Science and Technology\",\n",
    "}\n",
    "labeled_series = pd.Series(\n",
    "    data=news_data.iloc[:labeled_count].TITLE.values,\n",
    "    index=news_data.iloc[:labeled_count].CATEGORY.values,\n",
    ")\n",
    "annotation_metadata = b\"\"\"{ \"category-metadata\" : { \"confidence\": 1.0, \"human-annotated\": \"yes\", \"type\": \"groundtruth/text-classification\"} }\"\"\"\n",
    "annotation_metadata_dict = json.loads(annotation_metadata)\n",
    "with open(manifest_file, \"w\") as outfile:\n",
    "    for items in labeled_series.iteritems():\n",
    "        labeled_record = dict()\n",
    "        labeled_record[\"source\"] = items[1]\n",
    "        labeled_record[\"category\"] = int(items[0])\n",
    "        labeled_record.update(annotation_metadata_dict)\n",
    "        outfile.write(json.dumps(labeled_record) + \"\\n\")\n",
    "\n",
    "unlabeled_series = pd.Series(\n",
    "    data=news_data.iloc[labeled_count:].TITLE.values,\n",
    "    index=news_data.iloc[labeled_count:].CATEGORY.values,\n",
    ")\n",
    "with open(manifest_file, \"a\") as outfile:\n",
    "    for items in unlabeled_series.iteritems():\n",
    "        outfile.write('{\"source\":\"' + items[1] + '\"}\\n')\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).upload_file(manifest_file, key + \"/\" + manifest_file)\n",
    "manifest_file_uri = \"s3://{}/{}\".format(bucket, key + \"/\" + manifest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Use s3 client to upload relevant json strings to s3.\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This cell will specify the labels that workers will use to categorize the articles. To customize your labeling job, add your own labels here. To learn more, see [LabelCategoryConfigS3Uri](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateLabelingJob.html#sagemaker-CreateLabelingJob-request-LabelCategoryConfigS3Uri)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "label_file_name = \"class_labels.json\"\n",
    "label_file = \"\"\"{\n",
    "    \"document-version\": \"2018-11-28\",\n",
    "    \"labels\": [\n",
    "        {\n",
    "            \"label\": \"Business\"\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"Entertainment\"\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"Health & Medicine\"\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"Science and Technology\"\n",
    "        }\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "s3_client.put_object(Body=label_file, Bucket=bucket, Key=key + \"/\" + label_file_name)\n",
    "label_file_uri = \"s3://{}/{}\".format(bucket, key + \"/\" + label_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The following cell will specify our custom worker task template. This template will configure the UI that workers will see when they open our text classification labeling job tasks. To learn how to customize this cell, see  [Creating your custom labeling task template](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates-step2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "template_file_name = \"instructions.template\"\n",
    "template_file = r\"\"\"\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "<crowd-form>\n",
    "  <crowd-classifier\n",
    "    name=\"crowd-classifier\"\n",
    "    categories=\"{{ task.input.labels | to_json | escape }}\"\n",
    "    header=\"Select the news title corresponding to the 4 categories. (b) for Business, (e) for Entertainment, (m) for Health and Medicine and (t) for Science and Technology.\"\n",
    "  >\n",
    "    <classification-target> {{ task.input.taskObject }} </classification-target>\n",
    "    <full-instructions header=\"Classifier instructions\">\n",
    "      <ol><li><strong>Read</strong> the text carefully.</li><li><strong>Read</strong> the examples to understand more about the options.</li><li><strong>Choose</strong> the appropriate label that best suits the text.</li></ol>\n",
    "    </full-instructions>\n",
    "    <short-instructions>\n",
    "      <p>Example Business title:</p><p>US open: Stocks fall after Fed official hints at accelerated tapering.</p><p><br>\n",
    "      </p><p>Example Entertainment title:</p><p>CBS negotiates three more seasons for The Big Bang Theory</p><p><br>\n",
    "      </p><p>Example Health & Medicine title:</p><p>Blood Test Could Predict Alzheimer's. Good News? </p><p><br>\n",
    "      </p><p>Example Science and Technology (t) title:</p><p>Elephants tell human friend from foe by voice.</p><p><br>\n",
    "      </p>\n",
    "    </short-instructions>\n",
    "  </crowd-classifier>\n",
    "</crowd-form>\n",
    "\"\"\"\n",
    "\n",
    "s3_client.put_object(Body=template_file, Bucket=bucket, Key=key + \"/\" + template_file_name)\n",
    "template_file_uri = \"s3://{}/{}\".format(bucket, key + \"/\" + template_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "To use a private work team to labeling your data objects, set `USE_PRIVATE_WORKFORCE` to `True` and input your work team ARN for `private_workteam_arn`. You must have a private workforce in the same AWS Region as your labeling job task request to use a private work team. To learn more see [Use a Private Workforce](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-private.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "USE_PRIVATE_WORKFORCE = False\n",
    "private_workteam_arn = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This cell will automatically configure a public workforce ARN and pre- and post-annotation ARNs (`prehuman_arn` and `acs_arn` respectively). If `USE_PRIVATE_WORKFORCE` is `False` a public workforce will be used to create your labeling job request. \n",
    "\n",
    "To customize your labeling job task type, you will need to modify `prehuman_arn` and `acs_arn`. \n",
    "\n",
    "If you are using one of the Ground Truth built-in task types, you can find pre- and post-annotation lambda ARNs using the following links. \n",
    "* Pre-annotation lambda ARNs for built in task types can be found in [HumanTaskConfig](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_HumanTaskConfig.html#API_HumanTaskConfig_Contents).\n",
    "* Post-annotation lambda ARNs (Annotation Consolidation Lambda) for built in task types can be found in [AnnotationConsolidationConfig](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AnnotationConsolidationConfig.html#sagemaker-Type-AnnotationConsolidationConfig-AnnotationConsolidationLambdaArn).\n",
    "\n",
    "If you are creating a custom labeling job task, see [Step 3: Processing with AWS Lambda\n",
    "](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates-step3.html) learn how to create custom pre- and post-annotation lambda ARNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Specify ARNs for resources needed to run a text classification job.\n",
    "ac_arn_map = {\n",
    "    \"us-west-2\": \"081040173940\",\n",
    "    \"us-east-1\": \"432418664414\",\n",
    "    \"us-east-2\": \"266458841044\",\n",
    "    \"eu-west-1\": \"568282634449\",\n",
    "    \"ap-northeast-1\": \"477331159723\",\n",
    "}\n",
    "\n",
    "public_workteam_arn = \"arn:aws:sagemaker:{}:394669845002:workteam/public-crowd/default\".format(\n",
    "    region\n",
    ")\n",
    "prehuman_arn = \"arn:aws:lambda:{}:{}:function:PRE-TextMultiClass\".format(region, ac_arn_map[region])\n",
    "acs_arn = \"arn:aws:lambda:{}:{}:function:ACS-TextMultiClass\".format(region, ac_arn_map[region])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The following cell specifies our labeling job name, the description workers see, and tags that workers can use to find our labeling job task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "job_name_prefix = \"byoal-news\"\n",
    "task_description = \"Classify news title to one of these 4 categories.\"\n",
    "task_keywords = [\"text\", \"classification\", \"humans\", \"news\"]\n",
    "task_title = task_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Modify the following request to customize your labeling job request. For more information on the parameters below, see [CreateLabelingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateLabelingJob.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "human_task_config = {\n",
    "    \"AnnotationConsolidationConfig\": {\n",
    "        \"AnnotationConsolidationLambdaArn\": acs_arn,\n",
    "    },\n",
    "    \"PreHumanTaskLambdaArn\": prehuman_arn,\n",
    "    \"MaxConcurrentTaskCount\": 200,  # 200 texts will be sent at a time to the workteam.\n",
    "    \"NumberOfHumanWorkersPerDataObject\": 1,  # 1 workers will be enough to label each text.\n",
    "    \"TaskAvailabilityLifetimeInSeconds\": 21600,  # Your work team has 6 hours to complete all pending tasks.\n",
    "    \"TaskDescription\": task_description,\n",
    "    \"TaskKeywords\": task_keywords,\n",
    "    \"TaskTimeLimitInSeconds\": 300,  # Each text must be labeled within 5 minutes.\n",
    "    \"TaskTitle\": task_title,\n",
    "    \"UiConfig\": {\n",
    "        \"UiTemplateS3Uri\": template_file_uri,\n",
    "    },\n",
    "}\n",
    "\n",
    "if not USE_PRIVATE_WORKFORCE:\n",
    "    human_task_config[\"PublicWorkforceTaskPrice\"] = {\n",
    "        \"AmountInUsd\": {\n",
    "            \"Dollars\": 0,\n",
    "            \"Cents\": 1,\n",
    "            \"TenthFractionsOfACent\": 2,\n",
    "        }\n",
    "    }\n",
    "    human_task_config[\"WorkteamArn\"] = public_workteam_arn\n",
    "else:\n",
    "    human_task_config[\"WorkteamArn\"] = private_workteam_arn\n",
    "\n",
    "ground_truth_request = {\n",
    "    \"InputConfig\": {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"ManifestS3Uri\": manifest_file_uri,\n",
    "            }\n",
    "        },\n",
    "        \"DataAttributes\": {\n",
    "            \"ContentClassifiers\": [\"FreeOfPersonallyIdentifiableInformation\", \"FreeOfAdultContent\"]\n",
    "        },\n",
    "    },\n",
    "    \"OutputConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/output/\".format(bucket, key),\n",
    "    },\n",
    "    \"HumanTaskConfig\": human_task_config,\n",
    "    \"LabelingJobNamePrefix\": job_name_prefix,\n",
    "    \"RoleArn\": role,\n",
    "    \"LabelAttributeName\": \"category\",\n",
    "    \"LabelCategoryConfigS3Uri\": label_file_uri,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print(json.dumps(ground_truth_request, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Do the following steps to trigger the Active Learning loop.\n",
    "\n",
    "1.\tOpen the AWS Step Functions console: http://console.aws.amazon.com/states\n",
    "2.\tThe Cloud Formation stack provided in the blog post has generated two step function in the State Machines section: **ActiveLearningLoop-*** and **ActiveLearning-*** where * will be replaced with the name you used when you launched your Cloud Formation stack. \n",
    "3.\tSelect **ActiveLearningLoop**-*. \n",
    "4.\tChoose **Start Execution**.\n",
    "5.\tPaste the JSON above in **Input – optional code-block**.\n",
    "6.\tSelect **Start execution**. \n",
    "\n",
    "    \n",
    "These manual steps could be automated by using the data science SDK. Please refer to the details [here](https://aws.amazon.com/about-aws/whats-new/2019/11/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "On successful completion of the active learning loop, the state machine will output the final output manifest file and the latest trained model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part 2: Bring Your Own Model to an Active Learning Workflow \n",
    "\n",
    "Use this part of the notebook to learn how to containerize your own Machine Learning model and push it to [Amazon Elastic Container Registry (ERC)](https://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html). This notebook will produce an ECR ID that you can use to integrate your model into an active learning workflow.\n",
    "\n",
    "**This notebook is intended to be used along side the blog post [Bring your own model for Amazon SageMaker labeling workflows with Active Learning](https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-for-amazon-sagemaker-labeling-workflows-with-active-learning/), Part 2: Create a Custom Model and Integrate it into an Active Learning Workflow**.\n",
    "\n",
    "### Permissions\n",
    "\n",
    "**Please update your role with AmazonEC2ContainerRegistryFullAccess before proceeding**\n",
    "\n",
    "Running this notebook requires permissions in addition to the normal SageMakerFullAccess permissions. This is because it creates new repositories in Amazon ECR. The easiest way to add these permissions is simply to add the managed policy **AmazonEC2ContainerRegistryFullAccess** to the role that you used to start your notebook instance. There's no need to restart your notebook instance when you do this, the new permissions will be available immediately. To access the role associated with your notebook instance, select \"Notebook instances\" from the SageMaker console, select the name of your instance, and finally select the link under “IAM role ARN” in the “Permissions and encryption” section.\n",
    "\n",
    "### To Use this Notebook\n",
    "\n",
    "We use this notebook to tokenize our dataset and create a training dataset, add a containerized model to ERC, and train the model. The notebook will produce an image name in ECR which can be used for training and inference across Amazon SageMaker. \n",
    "\n",
    "We use a Keras deep learning model for demonstration purposes only. The methodology for developing and containerizing our model was inspired by the tutorial [Take an ML from idea to production using Amazon SageMaker](https://github.com/aws-samples/amazon-sagemaker-keras-text-classification) and is not included in the notebook. \n",
    "\n",
    "To customize this notebook, you will need to create your own machine learning model and add it to a Docker container. Use the blog post above to learn how to do this with Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First we will set up our environment and extract our account number. We will use the account number to define an image name for the Elastic Container Repository (ECR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "region = sess.boto_session.region_name\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "image = \"{}.dkr.ecr.{}.amazonaws.com/news-classifier\".format(account, region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Preprocessing and Tokenizing the data\n",
    "\n",
    "First we read the csv news dataset using pandas and clean the data:\n",
    "\n",
    "* We make all alphanumeric characters lowercase and replace undesired characters. \n",
    "* We remove stop words and empty records. \n",
    "\n",
    "The result is saved into a JSON formatted file.\n",
    "\n",
    "Next, we use the [Keras Tokenizer class](https://keras.io/preprocessing/text/) to tokenize our dataset and upload it to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "stop_words = stop_words.ENGLISH_STOP_WORDS\n",
    "import os, sys, sagemaker, tensorflow as tf, pandas as pd, boto3, numpy as np\n",
    "\n",
    "train_s3_key = \"sagemaker/news_subset.csv\"\n",
    "boto3.resource(\"s3\").Bucket(bucket).upload_file(\"news_subset.csv\", train_s3_key)\n",
    "\n",
    "column_names = [\"TITLE\", \"CATEGORY\"]\n",
    "tf_train = pd.read_csv(\n",
    "    \"news_subset.csv\", names=column_names, header=None, skiprows=[0], delimiter=\",\"\n",
    ")\n",
    "tf_train = tf_train[column_names]\n",
    "\n",
    "tf_train[\"TITLE\"] = tf_train[\"TITLE\"].str.lower().replace(\"[^\\w\\s]\", \"\")\n",
    "tf_train[\"TITLE\"] = tf_train[\"TITLE\"].apply(\n",
    "    lambda x: \" \".join([word for word in x.split() if word not in (stop_words)])\n",
    ")\n",
    "tf_train.dropna(inplace=True)\n",
    "\n",
    "cat = tf_train[\"CATEGORY\"].astype(\"category\").cat.categories\n",
    "tf_train[\"CATEGORY\"] = tf_train[\"CATEGORY\"].astype(\"category\").cat.codes\n",
    "y = tf_train[\"CATEGORY\"].values\n",
    "\n",
    "\n",
    "max_features = 5000  # we set maximum number of words to 5000\n",
    "maxlen = 100  # and maximum sequence length to 100\n",
    "embedding_dim = 50  # this is the final dimension of the embedding space.\n",
    "tok = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)  # tokenizer step\n",
    "tok.fit_on_texts(list(tf_train[\"TITLE\"]))  # fit to cleaned text\n",
    "with open(\"tokenizer.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "boto3.resource(\"s3\").Bucket(bucket).upload_file(\"tokenizer.pickle\", key + \"/tokenizer.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The next cell will update the value of *tokenizer_bucket* in our training and prediction scripts within the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def inplace_string_replace(filename, old_string, new_string):\n",
    "    with open(filename) as f:\n",
    "        updated_text = f.read().replace(old_string, new_string)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(updated_text)\n",
    "\n",
    "\n",
    "old_code = \"tokenizer_bucket = '<Update tokenizer bucket here>'\"\n",
    "new_code = \"tokenizer_bucket = '{}'\".format(bucket)\n",
    "inplace_string_replace(\"./container/news-classifier/train\", old_code, new_code)\n",
    "inplace_string_replace(\"./container/news-classifier/predictor.py\", old_code, new_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We extract the first 1000 entries for training and add them to a manifest file. Then, we save our training manifest file in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "column_names = [\"TITLE\", \"CATEGORY\"]\n",
    "\n",
    "tf_train = pd.read_csv(\n",
    "    \"news_subset.csv\", names=column_names, header=None, skiprows=[0], delimiter=\",\"\n",
    ")\n",
    "tf_train = tf_train[[\"TITLE\", \"CATEGORY\"]]\n",
    "tf_train[\"TITLE\"] = tf_train[\"TITLE\"].str.replace('\"', \"\").replace(\"\\r\", \"\")\n",
    "tf_train[\"CATEGORY\"] = tf_train[\"CATEGORY\"].astype(\"category\").cat.codes\n",
    "\n",
    "val_file = \"validation-manifest\"\n",
    "series = pd.Series(\n",
    "    data=tf_train.iloc[:1000].TITLE.values, index=tf_train.iloc[:1000].CATEGORY.values\n",
    ")\n",
    "with open(val_file, \"w\") as outfile:\n",
    "    for items in series.iteritems():\n",
    "        outfile.write('{\"category\":' + str(items[0]) + ',\"source\":\"' + items[1] + '\"}\\n')\n",
    "boto3.resource(\"s3\").Bucket(bucket).upload_file(val_file, key + \"/\" + val_file)\n",
    "valdiate_s3_uri = \"s3://{}/{}\".format(bucket, key + \"/\" + val_file)\n",
    "\n",
    "train_file = \"train-manifest\"\n",
    "series = pd.Series(\n",
    "    data=tf_train.iloc[1000:7000].TITLE.values, index=tf_train.iloc[1000:7000].CATEGORY.values\n",
    ")\n",
    "with open(train_file, \"w\") as outfile:\n",
    "    for items in series.iteritems():\n",
    "        outfile.write('{\"category\":' + str(items[0]) + ',\"source\":\"' + items[1] + '\"}\\n')\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).upload_file(train_file, key + \"/\" + train_file)\n",
    "train_s3_uri = \"s3://{}/{}\".format(bucket, key + \"/\" + train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Adding the Containerized ML Model to ECR\n",
    "\n",
    "The next cell will create a repository in ECR (if it does not exist already), build our docker image locally, and then [push it to ECR](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=news-classifier\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x ${algorithm_name}/train\n",
    "chmod +x ${algorithm_name}/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Confirm** the push to the ecr repository happened successfully before proceeding to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Training our Model\n",
    "\n",
    "We train our model on the training data that we extracted above and see the accuracy returned by our algorithm in Amazon SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "estimator = Estimator(\n",
    "    image_name=\"news-classifier\", role=role, train_instance_count=1, train_instance_type=\"local\"\n",
    ")\n",
    "\n",
    "estimator.fit({\"training\": train_s3_uri, \"validation\": valdiate_s3_uri})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Print the Image name in ECR\n",
    "\n",
    "The cell below will print our image's name in ECR. This image can now be used for both training and inference across Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "To add this image to an active learning workflow follow the instructions in *Step 1: Update the container ECR reference* in the blog. \n",
    "\n",
    "The active learning workflow resources produced by the Cloud Formation Stack provided in **Bring your own model for SageMaker labeling workflows with Active Learning** defaults to a `MultiRecord` [batch strategy](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html#sagemaker-CreateTransformJob-request-BatchStrategy). If your model only support a `SingleRecord` batch strategy, change your batch strategy by following the instructions in *Step 2: Change batch strategy*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
