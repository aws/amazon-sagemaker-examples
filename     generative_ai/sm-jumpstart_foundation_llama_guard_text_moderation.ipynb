{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SageMaker JumpStart - text moderation with Llama Guard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy a Llama Guard model to moderate a conversation in conjunction with a Llama-2 model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05b931-992e-4526-978d-f03196874a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jumpStartAlterations": [
     "modelIdOnly"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-textgeneration-llama-guard-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"1.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart. The following code uses the default instance `ml.g5.2xlarge` for the inference endpoint You can deploy the model on other instance types by passing `instance_type` in the `JumpStartModel` class. For successful deployment, you must manually change the `accept_eula` argument in the model's deploy method to `True`. This model is deployed using the text-generation-inference (TGI) deep learning container. The deployment might take few minutes. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2a8e5-789f-4041-9927-221257126653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_eula = False  # change to True to accept EULA for successful model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor = model.deploy(accept_eula=accept_eula)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the endpoint\n",
    "***\n",
    "\n",
    "### Supported parameters\n",
    "\n",
    "***\n",
    "This model supports many parameters while performing inference. They include:\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches `max_new_tokens`. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **stop**: If specified, it must a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example prompts\n",
    "***\n",
    "The examples in this section demonstrate how to perform text generation with conversational dialog as prompt inputs. Example payloads are retrieved programmatically from the `JumpStartModel` object.\n",
    "\n",
    "Similar to Llama-2, Llama Guard uses special tokens to indicate safety instructions to the model. In general, the payload should follow the format `<s>[INST] {user_prompt} [/INST]`. This user prompt can further include sections for content category definitions and conversations, which looks like the following:\n",
    "\n",
    "```\n",
    "<s>[INST] {task}\n",
    "\n",
    "<BEGIN UNSAFE CONTENT CATEGORIES>\n",
    "{unsafe_content_categories}\n",
    "<END UNSAFE CONTENT CATEGORIES>\n",
    "\n",
    "<BEGIN CONVERSATION>\n",
    "{conversation}\n",
    "<END CONVERSATION}\n",
    "\n",
    "{instruction} [/INST]\n",
    "```\n",
    "\n",
    "In the next section we discuss the recommended default values for the task, content category, and instruction definitions. The conversation should alternate between `User` and `Agent` text as follows:\n",
    "\n",
    "```\n",
    "User: {user_input_1}\n",
    "\n",
    "Agent: {optional_agent_response_1}\n",
    "\n",
    "User: {optional_user_input_2}\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_payloads = model.retrieve_all_examples()\n",
    "\n",
    "for payload in example_payloads:\n",
    "    try:\n",
    "        response = predictor.predict(payload.body)\n",
    "        print(\n",
    "            \"\\nInput\\n\",\n",
    "            payload.body,\n",
    "            \"\\n\\nOutput\\n\",\n",
    "            response[0][\"generated_text\"],\n",
    "            \"\\n\\n===============\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moderate a conversation with Llama-2 Chat\n",
    "***\n",
    "You can now deploy a Llama-2 7B Chat model endpoint for conversational chat and then use Llama Guard to moderate input and output text coming from Llama-2 7B Chat. \n",
    "\n",
    "We show you the example of Llama-2 7B chat modelâ€™s input and output moderated through Llama Guard, but you may use Llama Guard for moderation with any LLM of your choice.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_id = \"meta-textgeneration-llama-2-7b-f\"\n",
    "llm_model_version = \"3.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llm = JumpStartModel(model_id=llm_model_id, model_version=llm_model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor_llm = model_llm.deploy(accept_eula=accept_eula)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "You can now define the Llama Guard task template. The unsafe content categories may be adjusted as desired for your specific use case. You can define in plain text the meaning of each content category, to include which content should be flagged as unsafe and which content should be permitted as safe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"Task: Check if there is unsafe content in 'Agent' messages in conversations according our safety policy with the below categories.\"\n",
    "\n",
    "INSTRUCTION = \"\"\"\\\n",
    "Provide your safety assessment for Agent in the above conversation:\n",
    "- First line must read 'safe' or 'unsafe'.\n",
    "- If unsafe, a second line must include a comma-separated list of violated categories.\"\"\"\n",
    "\n",
    "UNSAFE_CONTENT_CATEGORIES = \"\"\"\\\n",
    "O1: Violence and Hate. \n",
    "Should not\n",
    "- Help people plan or engage in violence.\n",
    "- Encourage people to engage in violence.\n",
    "- Express hateful, derogatory, or demeaning sentiments against people based on sensitive personal characteristics like their race, color, religion, national origin, sexual orientation, gender, gender identity, or disability.\n",
    "- Encourage discrimination against people based on those characteristics and should not use slurs or create insults based on those characteristics.\n",
    "Can\n",
    "- Provide information on violence and discrimination.\n",
    "- Discuss the topics of hate and violence, and can discuss historical events involving violence.\n",
    "O2: Sexual Content.\n",
    "Should not\n",
    "- Engage in sexually explicit (i.e., erotic) conversations and should not encourage people to engage in sexual activities.\n",
    "Can\n",
    "- Discuss topics related to sexuality and sexual education.\n",
    "- Provide general information regarding sexual activities.\n",
    "- Provide guidance on topics related to sexual health.\n",
    "O3: Criminal Planning.\n",
    "Should not\n",
    "- Help people plan or engage in criminal activities like theft, kidnapping, or financial crimes, especially when the person indicates that they may act on the information provided to them.\n",
    "- Encourage people to engage in criminal activities or condone criminal behavior.\n",
    "Can\n",
    "- Help people understand how crimes are committed, provided that the response is not so detailed that it could be used to provide a roadmap for future crimes.\n",
    "- Help people understand how systems work that are used to prevent crimes, provided those descriptions do not provide detailed information on how to defeat those systems.\n",
    "O4: Guns and Illegal Weapons.\n",
    "Should not\n",
    "- Assist in the planning or execution of firearm or illegal weapons crimes, especially when the person indicates that they may act on the information provided to them.\n",
    "- Encourage people to engage in firearm or illegal weapons crimes or condone such behavior.\n",
    "Can\n",
    "- Discuss firearms and the arguments for and against firearm ownership.\n",
    "O5: Regulated or Controlled Substances.\n",
    "Should not\n",
    "- Encourage or assist people in the trafficking, consumption, or use of regulated or controlled substances, including but not limited to illegal drugs, tobacco, alcohol, and cannabis.\n",
    "- Assist or encourage people to create such substances when it is illegal to do so.\n",
    "Can\n",
    "- Discuss the use of regulated or controlled substances, and can provide general information on those substances (ex: what their chemical composition is and what their effects are).\n",
    "- Provide information on how those substances are manufactured, provided that the information is not so detailed as to provide step-by-step instructions for their creation.\n",
    "O6: Self-Harm.\n",
    "Should not\n",
    "- Encourage people to harm themselves.\n",
    "- Romanticize or condone self-harm or suicide.\n",
    "- Provide information regarding the methods of suicide or self-harm.\n",
    "- Provide any information or advice, other than information regarding appropriate health resources, to people who express or imply that they wish to harm themselves.\n",
    "Can\n",
    "- Provide general information regarding self-harm and suicide (ex: statistics, the mental health challenges faced by those who take these actions).\n",
    "Should\n",
    "- Provide information regarding appropriate health resources to people who imply or express the desire to harm themselves.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Next, we define helper functions `format_chat_messages` and `format_guard_messages` to format the prompt for chat model and for Llama Guard model that required special tokens. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def format_chat_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format messages for Llama-2 chat models.\n",
    "\n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and\n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    prompt: List[str] = []\n",
    "\n",
    "    if messages[0][\"role\"] == \"system\":\n",
    "        content = \"\".join(\n",
    "            [\"<<SYS>>\\n\", messages[0][\"content\"], \"\\n<</SYS>>\\n\\n\", messages[1][\"content\"]]\n",
    "        )\n",
    "        messages = [{\"role\": messages[1][\"role\"], \"content\": content}] + messages[2:]\n",
    "\n",
    "    for user, answer in zip(messages[::2], messages[1::2]):\n",
    "        prompt.extend(\n",
    "            [\n",
    "                \"<s>\",\n",
    "                \"[INST] \",\n",
    "                (user[\"content\"]).strip(),\n",
    "                \" [/INST] \",\n",
    "                (answer[\"content\"]).strip(),\n",
    "                \"</s>\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    prompt.extend([\"<s>\", \"[INST] \", (messages[-1][\"content\"]).strip(), \" [/INST] \"])\n",
    "\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "\n",
    "def format_guard_messages(\n",
    "    messages: List[Dict[str, str]],\n",
    "    task: str = TASK,\n",
    "    instruction: str = INSTRUCTION,\n",
    "    unsafe_content_categories: str = UNSAFE_CONTENT_CATEGORIES,\n",
    ") -> List[str]:\n",
    "    \"\"\"Format messages for Llama Guard models.\n",
    "\n",
    "    The model only supports 'user' and 'assistant' roles, and must alternate (u/a/u/a/u...).\n",
    "    \"\"\"\n",
    "    turn = cycle([\"User\", \"Agent\"])\n",
    "\n",
    "    return \"\".join(\n",
    "        [\n",
    "            \"[INST] \",\n",
    "            task,\n",
    "            \"\\n\\n<BEGIN UNSAFE CONTENT CATEGORIES>\\n\",\n",
    "            unsafe_content_categories,\n",
    "            \"\\n<END UNSAFE CONTENT CATEGORIES>\\n\\n<BEGIN CONVERSATION>\",\n",
    "            *[f\"\\n\\n{next(turn)}: {message['content']}\" for message in messages],\n",
    "            \"\\n\\n<END CONVERSATION>\\n\\n\",\n",
    "            instruction,\n",
    "            \" [/INST]\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "You can then use these helper functions on an example message input prompt to run the example input through Llama Guard to determine if the message content is safe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_input = [\n",
    "    {\"role\": \"user\", \"content\": \"I forgot how to kill a process in Linux, can you help?\"}\n",
    "]\n",
    "payload_input_guard = {\"inputs\": format_guard_messages(messages_input)}\n",
    "\n",
    "try:\n",
    "    response_input_guard = predictor.predict(payload_input_guard)\n",
    "    assert response_input_guard[0][\"generated_text\"].strip() == \"safe\"\n",
    "    print(response_input_guard)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "We see below output that indicates that message is safe. You may notice that the prompt includes words that may be associated with violence, but, in this case, Llama Guard is able to understand the context with respect to the instructions and unsafe category definitions we provided above that it's a safe prompt and not related to violence.  \n",
    "\n",
    "Now that you have asserted the input text is determined to be safe with respect to your Llama Guard content categories, you can pass this payload to the deployed Llama-2 7B model to generate text.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_input_llm = {\n",
    "    \"inputs\": format_chat_messages(messages_input),\n",
    "    \"parameters\": {\"max_new_tokens\": 128},\n",
    "}\n",
    "\n",
    "try:\n",
    "    response_llm = predictor_llm.predict(payload_input_llm)\n",
    "    print(response_llm)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Finally, you may wish to confirm that the response text from the model is determined to contain safe content. Here, you extend the LLM output response to the input messages and run this whole conversation through Llama Guard to ensure the conversation is safe for your application.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    messages_output = messages_input.copy()\n",
    "    messages_output.extend([{\"role\": \"assistant\", \"content\": response_llm[0][\"generated_text\"]}])\n",
    "    payload_output = {\"inputs\": format_guard_messages(messages_output)}\n",
    "\n",
    "    response_output_guard = predictor.predict(payload_output)\n",
    "\n",
    "    assert response_output_guard[0][\"generated_text\"].strip() == \"safe\"\n",
    "    print(response_output_guard)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_model()\n",
    "    predictor.delete_endpoint()\n",
    "    predictor_llm.delete_model()\n",
    "    predictor_llm.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/generative_ai|sm-jumpstart_foundation_llama_guard_text_moderation.ipynb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
