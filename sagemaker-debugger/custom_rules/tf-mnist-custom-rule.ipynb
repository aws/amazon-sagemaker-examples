{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker - Debugging With Custom Rules\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed platform to build, train, and deploy machine learning models quickly. Amazon SageMaker Debugger offers the capability to debug machine learning models during training by identifying and detecting problems with the models in near real time. \n",
    "\n",
    "## How does Amazon SageMaker Debugger work?\n",
    "\n",
    "Amazon SageMaker Debugger lets you go beyond just looking at scalars like losses and accuracies during training. It gives you full visibility into all tensors flowing through the graph during training. Furthermore, it helps you monitor your training in near real time using rules. It also provides alerts once it has detected an inconsistency in training flow.\n",
    "\n",
    "### Concepts\n",
    "* **Tensors**: These are the artifacts that define the state of the training job at any particular instant in its lifecycle.\n",
    "* **Debug Hook**: Captures the tensors flowing through the training computational graph every N steps.\n",
    "* **Debugging Rule**: Logic to analyze the tensors captured by the hook and report anamolies, if at all.\n",
    "\n",
    "With these concepts in mind, let's understand the overall flow of things which the Debugger uses to orchestrate debugging\n",
    "\n",
    "### Storage of tensors\n",
    "\n",
    "The tensors captured by the debug hook are stored in an Amazon Simple Storage Service (Amazon S3) location specified by you. There are two ways you can configure the Debugger for storage:\n",
    "\n",
    "#### With no changes to your training script\n",
    "If you use any Amazon SageMaker-provided [deep learning containers](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) then you don't need to make any changes to your training script for the tensors to be stored. Amazon SageMaker Debugger uses the configuration you provide in the framework `Estimator` to save the tensors in the fashion you specify.\n",
    "\n",
    "#### Orchestrating your script to store tensors\n",
    " The Debugger exposes a library that allows you to capture these tensors and save them for analysis. It's highly customizable. You can save the specific tensors you want at different frequencies and possibly with other configurations. For more information about how to use the Debugger library with your choice of framework in your training script, see [Developer Guide](https://github.com/awslabs/sagemaker-debugger/tree/master/sagemaker-docs)\n",
    "\n",
    "### Analysis of tensors\n",
    "\n",
    "Once the tensors are saved, the Debugger can be configured to run debugging ***Rules*** on them. On a very broad level, a rule is a python code used to detect certain conditions during training. Some of the conditions that a data scientist training an algorithm may care about are monitoring for gradients getting too large or too small, detecting overfitting, and so on. Sagemaker-Debugger will come pre-packaged with certain first-party (1P) rules. Users can write their own rules using the Sagemaker-Debugger APIs. You can also analyze raw tensor data outside of the Rules construct in say, a Sagemaker notebook, using Sagemaker-Debugger's full set of APIs.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Follow this one time setup to get your notebook up-and-running to use Amazon SageMaker Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install smdebug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the setup out of the way, start training the MXNet model in Amazon SageMaker with the debugger enabled\n",
    "\n",
    "## Training TensorFlow models with Amazon SageMaker Debugger\n",
    "\n",
    "### Amazon SageMaker TensorFlow as a framework\n",
    "\n",
    "Train a TensorFlow model in this notebook with Amazon Sagemaker Debugger enabled and monitor the training jobs with rules. This is done using Amazon SageMaker [TensorFlow 1.15.0](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) Container as a framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries needed for the demo of Amazon SageMaker Debugger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, rule_configs, DebuggerHookConfig, TensorBoardOutputConfig, CollectionConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the entry point for the training script. Use image recognition by using MNIST dataset as the training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the entrypoint script\n",
    "entrypoint_script='scripts/mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Estimator\n",
    "\n",
    "Now it's time to setup our TensorFlow estimator. There are new parameters with the estimator to enable your training job for debugging through Amazon SageMaker Debugger. These new parameters are explained below\n",
    "\n",
    "* **debugger_hook_config**: This new parameter accepts a local path where you wish your tensors to be written to and also accepts the S3 Uri where you wish your tensors to be uploaded to\n",
    "* **rules**: This new parameter will accept a list of rules you wish to evaluate against the tensors output by this training job. For rules, SageMaker Debugger supports two types:\n",
    " * **Amazon SageMaker Rules**: These are rules curated by the Amazon SageMaker team and you can choose to evaluate them against your training job.\n",
    " * **Custom Rules**: You can optionally choose to write your own rule as a Python source file and have it evaluated against your training job. To provide SageMaker Debugger to evaluate this rule, you would have to provide the S3 location of the rule source and the evaluator image.\n",
    " \n",
    "#### Using your own custom rule\n",
    " \n",
    "In this example you see how to use your own custom rule logic to be evaluated against your training.\n",
    "\n",
    "For demonstration purposes, use a rule that is similar to Amazon SageMaker's vanishing gradients rule but with the logic tweaked a bit. The rule attempts to evaluate if they're vanishing gradients by analyzing the tensors emitted by your training job\n",
    "\n",
    "##### **Summary of what the custom rule evaluates**\n",
    "\n",
    "The custom rule looks at the tensors in the collection \"gradients\" saved during training and attempt to get the absolute value of the gradients in each step of the training. If the mean of the absolute values of gradients in any step is greater than a specified threshold, mark the rule as 'triggering'. Look at how to structure the rule source\n",
    "\n",
    "Any custom rule logic you want to be evaluated should extend the `Rule` interface provided by Amazon SageMaker \n",
    "\n",
    "```python\n",
    "from smdebug.rules.rule import Rule\n",
    "\n",
    "class CustomGradientRule(Rule):\n",
    "```\n",
    "\n",
    "Now implement the class methods for the rule. Doing this allows Amazon SageMaker to understand the intent of the rule and evaluate it against your training tensors.\n",
    "\n",
    "##### **Rule class constructor**\n",
    "\n",
    "In order for Amazon SageMaker to instantiate your rule, your rule class constructor must conform to the following signature.\n",
    "```python\n",
    "__init__(base_trial, other_trials, <other parameters>)\n",
    "```\n",
    "`base_trial (Trial)`: This defines the primary [Trial](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/analysis.md#trial) that your rule is anchored to. This is an object of class type `Trial`.\n",
    "\n",
    "`other_trials (list[Trial])`: *(Optional)* This defines a list of 'other' trials you want your rule to look at. This is useful in the scenarios when you're comparing tensors from the base_trial to tensors from some other trials. \n",
    "`<other parameters>`: This is similar to `**kwargs` where you can pass in however many string parameters in your constructor signature. Note that SageMaker would only be able to support supplying string types for these values at runtime (see how, later).\n",
    "\n",
    "##### `set_required_tensors()`\n",
    "\n",
    "Overriding this method helps Amazon SageMaker to look for only the necessary tensors for a particular step. In your case you're only concerned with the collection \"gradients\" so specify that as the required tensor.\n",
    "```python\n",
    "def set_required_tensors(self, step):\n",
    "        for tname in self.base_trial.tensor_names(collection=\"gradients\"):\n",
    "            self.req_tensors.add(tname, steps=[step])\n",
    "            \n",
    "```\n",
    "\n",
    "##### `invoke_at_step()`:\n",
    "\n",
    "This defines the logic to invoked for each step. Essentially, this is where you decide whether the rule should trigger or not. In this case, you're concerned about the gradients getting too large. So, get the [tensor reduction]() \"mean\" for each step and see if it's value is larger than a threshold.\n",
    "\n",
    "```python\n",
    "def invoke_at_step(self, step):\n",
    "        for t in self.req_tensors.get():\n",
    "            abs_mean = t.reduction_value(step, \"mean\", abs=True)\n",
    "            if abs_mean > self.threshold:\n",
    "                return True\n",
    "        return False\n",
    "```\n",
    "\n",
    "See the [documentation](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/analysis.md) to learn more about structuring your rules and other related concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_rule = Rule.custom(name='MyCustomRule', # used to identify the rule\n",
    "                          image_uri='490809245908.dkr.ecr.us-west-2.amazonaws.com/sagemaker-debugger-rule-evaluator:latest', # rule evaluator container image\n",
    "                          instance_type='ml.t3.medium', # instance type to run the rule evaluation on\n",
    "                          source='rules/my_custom_rule.py', # path to the rule source file\n",
    "                          rule_to_invoke='CustomGradientRule', # name of the class to invoke in the rule source file\n",
    "                          volume_size_in_gb=30, # EBS volume size required to be attached to the rule evaluation instance\n",
    "                          collections_to_save=[CollectionConfig(name='gradients')], # collections to be analyzed by the rule\n",
    "                          rule_parameters={\n",
    "                              \"threshold\": \"20.0\" # this will be used to intialize 'threshold' param in your constructor\n",
    "                          })\n",
    "\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    base_job_name='smdebugger-demo-mnist-tensorflow',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m4.xlarge',\n",
    "    entry_point=entrypoint_script,\n",
    "    framework_version='1.15',\n",
    "    py_version='py3',\n",
    "    train_max_run=3600,\n",
    "    script_mode=True,\n",
    "    sagemaker_session=sess,\n",
    "    ## New parameter\n",
    "    rules = [custom_rule]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you proceed and create our training job, explore the new parameters in the TensorFlow estimator above\n",
    "\n",
    "* `name`: This is used to identify this particular rule among the suite of rules you specified to be evaluated.\n",
    "* `image_uri`: This is the image of the container that has the logic of understanding your custom rule sources and evaluating them against the collections you save in the training job. You can get the list of open sourced SageMaker rule evaluator images [here]()\n",
    "* `instance_type`: The type of the instance you want to run the rule evaluation on\n",
    "* `source`: This is the local path or the S3 Uri of your rule source file.\n",
    "* `rule_to_invoke`: This specifies the particular Rule class implementation in your source file which you want to be evaluated. SageMaker supports only 1 rule to be evaluated at a time in a rule job. Your source file can have multiple Rule class implementations, though.\n",
    "* `collections_to_save`: This specifies which collections are necessary for this rule.\n",
    "* `rule_parameters`: This provides the runtime values of the parameter in your constructor. You can still choose to pass in other values which may be necessary for your rule to be evaluated. Any value in this map is available as an environment variable and can be accessed by your rule script using `$<rule_parameter_key>`\n",
    "\n",
    "You can read more about custom rule evaluation in Amazon SageMaker in this [documentation](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/analysis.md)\n",
    "\n",
    "\n",
    "Let's call `fit()` on our estimator to start the training job and the parallel custom rule evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-28 04:56:10 Starting - Starting the training job...\n",
      "2019-11-28 04:56:33 Starting - Launching requested ML instances\n",
      "********* Debugger Rule Status *********\n",
      "*\n",
      "*       MyCustomRule: InProgress        \n",
      "*\n",
      "****************************************\n",
      "...\n",
      "2019-11-28 04:57:09 Starting - Preparing the instances for training......\n",
      "2019-11-28 04:58:05 Downloading - Downloading input data...\n",
      "2019-11-28 04:58:35 Training - Downloading the training image.....\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[0m\n",
      "\u001b[34m2019-11-28 04:59:21,058 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2019-11-28 04:59:21,065 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-11-28 04:59:21,371 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-11-28 04:59:21,388 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-11-28 04:59:21,406 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-11-28 04:59:21,418 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-11-28-04-45-33-451/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"smdebugger-demo-mnist-tensorflow-2019-11-28-04-56-10-291\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-11-28-04-56-10-291/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-11-28-04-45-33-451/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-11-28-04-56-10-291/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-11-28-04-45-33-451/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"smdebugger-demo-mnist-tensorflow-2019-11-28-04-56-10-291\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-11-28-04-56-10-291/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-11-28-04-45-33-451/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-11-28-04-45-33-451/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 mnist.py --model_dir s3://sagemaker-us-west-2-072677473360/smdebugger-demo-mnist-tensorflow-2019-11-28-04-45-33-451/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[0m\n",
      "\u001b[34mDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\u001b[0m\n",
      "\u001b[34m#015    8192/11490434 [..............................] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 2244608/11490434 [====>.........................] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 4202496/11490434 [=========>....................] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 8396800/11490434 [====================>.........] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01511493376/11490434 [==============================] - 0s 0us/step\u001b[0m\n",
      "\n",
      "2019-11-28 04:59:36 Training - Training image download completed. Training in progress.\u001b[34mWARNING:tensorflow:From mnist.py:113: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n",
      "\u001b[0m\n",
      "\u001b[34m[2019-11-28 04:59:25.480 ip-10-0-71-23.us-west-2.compute.internal:26 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[0m\n",
      "\u001b[34m[2019-11-28 04:59:25.504 ip-10-0-71-23.us-west-2.compute.internal:26 INFO hook.py:196] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist.py:50: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.keras.layers.Conv2D` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease use `layer.__call__` method instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist.py:54: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse keras.layers.MaxPooling2D instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist.py:64: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse keras.layers.Dense instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist.py:66: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse keras.layers.dropout instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist.py:84: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.where in 2.0, which has the same broadcast rule as np.where\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist.py:85: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist.py:89: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist.py:92: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\u001b[0m\n",
      "\u001b[34m[2019-11-28 04:59:26.096 ip-10-0-71-23.us-west-2.compute.internal:26 INFO hook.py:325] Monitoring the collections: sm_metrics, gradients, losses, metrics\u001b[0m\n",
      "\u001b[34m[2019-11-28 04:59:26.110 ip-10-0-71-23.us-west-2.compute.internal:26 INFO base_hook.py:182] Writing graph\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1075: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse standard file utilities to get mtimes.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py:888: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/smdebug/tensorflow/session.py:310: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.compat.v1.graph_util.extract_sub_graph`\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From mnist.py:97: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
      "\u001b[0m\n",
      "\u001b[34m[2019-11-28 05:04:15.514 ip-10-0-71-23.us-west-2.compute.internal:26 INFO hook.py:325] Monitoring the collections: sm_metrics, gradients, losses, metrics\u001b[0m\n",
      "\u001b[34m[2019-11-28 05:04:15.522 ip-10-0-71-23.us-west-2.compute.internal:26 INFO base_hook.py:182] Writing graph\u001b[0m\n",
      "\u001b[34m[2019-11-28 05:04:19.874 ip-10-0-71-23.us-west-2.compute.internal:26 INFO utils.py:27] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2019-11-28 05:04:20,306 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[34mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[34mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[34m2019-11-28 05:04:20,306 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-11-28 05:04:41 Uploading - Uploading generated training model\n",
      "2019-11-28 05:04:41 Completed - Training job completed\n",
      "\n",
      "********* Debugger Rule Status *********\n",
      "*\n",
      "*       MyCustomRule: NoIssuesFound     \n",
      "*\n",
      "****************************************\n",
      "Training seconds: 378\n",
      "Billable seconds: 378\n"
     ]
    }
   ],
   "source": [
    "# After calling fit, Amazon SageMaker spins off one training job and one rule job for you.\n",
    "# The rule evaluation status is visible in the training logs\n",
    "# at regular intervals\n",
    "\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result \n",
    "\n",
    "As a result of calling the `fit()` Amazon SageMaker debugger kicked off a rule evaluation job for our custom gradient logic in-parallel with the training job that was monitoring the tensors output by the training job. As you can see, in the summary, there was no step in the training that reported vanishing gradients in the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "description = client.describe_training_job(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RuleConfigurationName': 'MyCustomRule',\n",
       "  'RuleEvaluationJobArn': 'arn:aws:sagemaker:us-west-2:072677473360:processing-job/smdebugger-demo-mnist-tens-mycustomrule-1223c902',\n",
       "  'RuleEvaluationStatus': 'NoIssuesFound',\n",
       "  'LastModifiedTime': datetime.datetime(2019, 11, 28, 5, 4, 41, 853000, tzinfo=tzlocal())}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description['DebugRuleEvaluationStatuses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having these kind of analyses run through the Amazon SageMaker Debugger in-parallel with the training job is beneficial. You can react to the status transitions of the rule evaluations by configuring Amazon CloudWatch events."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
