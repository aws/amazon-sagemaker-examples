{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can download an NVIDIA model from torchhub\n",
    "\n",
    "#### ... and then compress it as a tar.gz file, and use this for deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-18 19:37:01--  https://api.ngc.nvidia.com/v2/models/nvidia/ssdpyt_fp32/versions/2/files/nvidia_ssdpyt_fp32_190826.pt\n",
      "Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 52.35.21.100, 35.161.41.62\n",
      "Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com)|52.35.21.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 \n",
      "Location: https://s3.us-west-2.amazonaws.com/prod-model-registry-ngc-bucket/org/nvidia/models/ssdpyt_fp32/versions/2/files/nvidia_ssdpyt_fp32_190826.pt?response-content-disposition=attachment%3B%20filename%3D%22nvidia_ssdpyt_fp32_190826.pt%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQDQgkQcQ9pDRllmFXUkmQyrW6dohDEsW0WJJLAgyaxO%2BgIhAPfQV9WZeKSAzXRZwTT3q3ZjKW5GGQ%2Bk0LSSk70QiEkEKr0DCMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAhoMNzg5MzYzMTM1MDI3IgxSXrpcKj5tL8heQiUqkQONTLPWf%2Bhetcq1bgG5mZ0jBrR4Hz3uN03%2BA8d8zT%2Fy9tgCLha9kXrTx1hhgqE5ZyiaoO7e9T62JWbkPCQbzq0EZLES%2FUcr6nHgfDflXle%2FGaqy1UhnADkMvAqL1AoRokmGw19warcbQI82AWif2DdCvlI0LZbmtoOdrAOuqif3BPt%2FjgK7Bt2FNNBbdKy5mvchsfBtDof%2FWEN0CaLqSvI7JGLl4m0%2ByA4mlQ8J6dSMpGyIE3XX%2BMx1L8UEGtLrY%2BoSUT2GlAVt9rGULN3GYMaKfek4YK8bFiPMFvYX%2B052DtZK1WdHtJlfrhMaHdORppWOwjlC870y1ORz5nht7gCY994dq1Dh9ADsWh57Qe0UYrsovEDf8l%2FD5YTQFoF6hid0lSJXwhe1%2Ff2XBdmOCzQHljcEk47j5QLYOe%2FZwMnuhe7H3uSVl7FO8%2FwC3gFo8X5g8%2FsrBlI%2FGWJx6XCN%2FsdEwETpPFjDoKjZWLoPFZ6nvl8KEJwa4Zt%2FcGr8%2FSmpIf%2F%2FOVdrv2k1Q7WAifpWpMIOgjCg4snzBTrqAebRDH4Ouzdv52YOMyUueVfRKGGyb1nDELh2QPaF2%2FtfILKFFn0%2F7RTGP2Pq5vrhwQgutIpG2PGLLIfO3SS2Za%2FL5NhxoOXHey1ztL7%2BKUJB32OXX8qNHJ264Nh%2Bnc4w5onnNlS3PX991FwecUgIKCI5O%2FxWVz1msNFB356mGusvEju%2F7CjqmqcCtn9WC9kRlO6CzoB8bhAKOicXY%2F24sKCMgUx4ZxRCx7uwNu0ogdGX5IePN2Zc2m8%2FkgdLBRJ6Ohv%2FqeCR%2FXMIUvhsxT5d4GOjJGjTGDVd36GE%2FleaaIR%2BR%2F%2FpthJev0ysEA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200318T193702Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=ASIA3PSNVSIZTQLSHG62%2F20200318%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=16b1524b52a14760a71ed552a3f433a30232d63708eb3dd1af2ff6d85ed651e8 [following]\n",
      "--2020-03-18 19:37:02--  https://s3.us-west-2.amazonaws.com/prod-model-registry-ngc-bucket/org/nvidia/models/ssdpyt_fp32/versions/2/files/nvidia_ssdpyt_fp32_190826.pt?response-content-disposition=attachment%3B%20filename%3D%22nvidia_ssdpyt_fp32_190826.pt%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQDQgkQcQ9pDRllmFXUkmQyrW6dohDEsW0WJJLAgyaxO%2BgIhAPfQV9WZeKSAzXRZwTT3q3ZjKW5GGQ%2Bk0LSSk70QiEkEKr0DCMT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAhoMNzg5MzYzMTM1MDI3IgxSXrpcKj5tL8heQiUqkQONTLPWf%2Bhetcq1bgG5mZ0jBrR4Hz3uN03%2BA8d8zT%2Fy9tgCLha9kXrTx1hhgqE5ZyiaoO7e9T62JWbkPCQbzq0EZLES%2FUcr6nHgfDflXle%2FGaqy1UhnADkMvAqL1AoRokmGw19warcbQI82AWif2DdCvlI0LZbmtoOdrAOuqif3BPt%2FjgK7Bt2FNNBbdKy5mvchsfBtDof%2FWEN0CaLqSvI7JGLl4m0%2ByA4mlQ8J6dSMpGyIE3XX%2BMx1L8UEGtLrY%2BoSUT2GlAVt9rGULN3GYMaKfek4YK8bFiPMFvYX%2B052DtZK1WdHtJlfrhMaHdORppWOwjlC870y1ORz5nht7gCY994dq1Dh9ADsWh57Qe0UYrsovEDf8l%2FD5YTQFoF6hid0lSJXwhe1%2Ff2XBdmOCzQHljcEk47j5QLYOe%2FZwMnuhe7H3uSVl7FO8%2FwC3gFo8X5g8%2FsrBlI%2FGWJx6XCN%2FsdEwETpPFjDoKjZWLoPFZ6nvl8KEJwa4Zt%2FcGr8%2FSmpIf%2F%2FOVdrv2k1Q7WAifpWpMIOgjCg4snzBTrqAebRDH4Ouzdv52YOMyUueVfRKGGyb1nDELh2QPaF2%2FtfILKFFn0%2F7RTGP2Pq5vrhwQgutIpG2PGLLIfO3SS2Za%2FL5NhxoOXHey1ztL7%2BKUJB32OXX8qNHJ264Nh%2Bnc4w5onnNlS3PX991FwecUgIKCI5O%2FxWVz1msNFB356mGusvEju%2F7CjqmqcCtn9WC9kRlO6CzoB8bhAKOicXY%2F24sKCMgUx4ZxRCx7uwNu0ogdGX5IePN2Zc2m8%2FkgdLBRJ6Ohv%2FqeCR%2FXMIUvhsxT5d4GOjJGjTGDVd36GE%2FleaaIR%2BR%2F%2FpthJev0ysEA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200318T193702Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=ASIA3PSNVSIZTQLSHG62%2F20200318%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=16b1524b52a14760a71ed552a3f433a30232d63708eb3dd1af2ff6d85ed651e8\n",
      "Resolving s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)... 52.218.253.16\n",
      "Connecting to s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)|52.218.253.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 183409492 (175M) [application/octet-stream]\n",
      "Saving to: ‘nvidia_ssdpyt_fp32_190826.pt.1’\n",
      "\n",
      "nvidia_ssdpyt_fp32_ 100%[===================>] 174.91M  20.4MB/s    in 9.0s    \n",
      "\n",
      "2020-03-18 19:37:11 (19.4 MB/s) - ‘nvidia_ssdpyt_fp32_190826.pt.1’ saved [183409492/183409492]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://api.ngc.nvidia.com/v2/models/nvidia/ssdpyt_fp32/versions/2/files/nvidia_ssdpyt_fp32_190826.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('nvidia_ssdpyt_fp32_190826.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = sess.upload_data(\n",
    "    path='model.tar.gz', bucket=bucket,\n",
    "    key_prefix='sagemaker-pytorch/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-497456752804/sagemaker-pytorch/input/model.tar.gz'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transform_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile transform_script.py\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = torch.load(os.path.join(model_dir, 'nvidia_ssdpyt_fp32_190826.pt')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = PyTorchModel(model_data=modelpath, role=role,\n",
    "                             entry_point='transform_script.py',\n",
    "                             framework_version='1.4.0')\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also, you can download the model from torchhub with this API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ec2-user/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "precision = 'fp32'\n",
    "ssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', model_math=precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.tar.gz\t\t\ttmp.tar.gz\r\n",
      "nvidia_ssdpyt_fp32_190826.pt\ttransform_script_hub.py\r\n",
      "nvidia_ssdpyt_fp32_190826.pt.1\ttransform_script.py\r\n",
      "Pytorch BYOM from NGC.ipynb\tu.item\r\n",
      "tmp\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or better, download the model from torch hub on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transform_script_hub.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile transform_script_hub.py\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', model_math='fp32',map_location='cpu')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp\n",
      "---------------!"
     ]
    }
   ],
   "source": [
    "#PyTorchModel requires a non-empty, model_data file\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "!echo \"tmp content\" > tmp\n",
    "!tar -zcvf ./tmp.tar.gz tmp\n",
    "pytorch_model = PyTorchModel(model_data = 'file://tmp.tar.gz',\n",
    "                             role=role,\n",
    "                             entry_point='./transform_script_hub.py',\n",
    "                             framework_version='1.4.0')\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1, endpoint_name='nvidia-ssd-pytorch-cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "\n",
    "# METHOD #1: OpenCV, NumPy, and urllib\n",
    "def url_to_image(url):\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/nvidia-ssd-pytorch-cpu in account 497456752804 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-fc9340cf95f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://upload.wikimedia.org/wikipedia/commons/2/25/Postmen_Office_Room.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/nvidia-ssd-pytorch-cpu in account 497456752804 for more information."
     ]
    }
   ],
   "source": [
    "predictor.predict(url_to_image('https://upload.wikimedia.org/wikipedia/commons/2/25/Postmen_Office_Room.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
