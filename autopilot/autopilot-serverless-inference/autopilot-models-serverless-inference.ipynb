{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Autopilot models to serverless inference endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Serverless Inference is a purpose-built inference option that makes it easy for customers to deploy and scale ML models. Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless endpoints also automatically launch compute resources and scale them in and out depending on traffic, eliminating the need to choose instance types or manage scaling policies.\n",
    "\n",
    "In this notebook we'll use models generated with Amazon SageMaker Autopilot and then deploy these models to serverless endpoints.\n",
    "\n",
    "We will be using the public [UCI Direct Marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing) dataset for this example.\n",
    "\n",
    "**Notebook Settings:**\n",
    "\n",
    "- **SageMaker Classic Notebook Instance:** `ml.t3.xlarge` Notebook Instance & `conda_python3` Kernel\n",
    "- **SageMaker Studio:** `Python 3 (Data Science 2.0) Kernel`\n",
    "- **Regions Available:** SageMaker Serverless Inference is currently available in the following regions: \n",
    "        US East (Northern Virginia), US East (Ohio), US West (Oregon), EU (Ireland), Asia Pacific (Tokyo) and Asia Pacific (Sydney)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Let's ensure we have the latest packages installed. For this notebook, we'll need the below versions of sagemaker and boto3\n",
    "1. sagemaker current version >= `2.110.0`\n",
    "1. boto3 version >= `boto3-1.24.84`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "print(f\"SageMaker Version: {sagemaker.__version__}\")\n",
    "print(f\"Boto3 Version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Skip the below cell if installed sagemaker current version >= `2.110.0` and boto3 version >= `boto3-1.24.84`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U awscli sagemaker boto3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SageMaker Version: {sagemaker.__version__}\")\n",
    "print(f\"Boto3 Version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install [rich](https://github.com/Textualize/rich) library for progress bars and for rich print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install `rich` library and load rich extension\n",
    "!pip install -U rich --quiet\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import packages, establish session and unique ID for job name suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from time import gmtime, strftime, sleep\n",
    "from uuid import uuid4\n",
    "\n",
    "# import print and progress bar components from rich\n",
    "from rich import print\n",
    "from rich.progress import (\n",
    "    Progress,\n",
    "    TextColumn,\n",
    "    SpinnerColumn,\n",
    "    BarColumn,\n",
    "    TimeElapsedColumn,\n",
    ")\n",
    "from rich.progress import track\n",
    "from rich.live import Live\n",
    "from rich.pretty import Pretty\n",
    "from rich.panel import Panel\n",
    "\n",
    "\n",
    "# Define region, bucket\n",
    "session = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "bucket = session.default_bucket()\n",
    "# use the below for default SageMaker execution role else replace with your own IAM Role ARN\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "prefix = \"autopilot/bankadditional\"\n",
    "\n",
    "today = datetime.now().strftime(\"%d%b%Y\")\n",
    "timestamp_suffix = f\"{str(uuid4())[:6]}-{today}\"\n",
    "\n",
    "# Define sagemaker client object to invoke Sagemaker services\n",
    "sm_client = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "\n",
    "# Set prefix for AutoML jobnames. Let's keep the prefix short as we'll be adding suffixes to distinguish job names.\n",
    "automl_job_prefix = \"bankmrkt\"  # 6-8 chars max\n",
    "model_prefix = automl_job_prefix\n",
    "\n",
    "print(f\"Bucket: [b green]s3://{bucket}/{prefix}[/b green]\")\n",
    "print(f\"Region: [b yellow]{region}[/b yellow]\")\n",
    "print(f\"Role: [b green]{role}[/b green]\")\n",
    "print(f\"Job and model prefix string: [b cyan]{automl_job_prefix}[/b cyan]\")\n",
    "print(f\"suffix string: [b yellow]{timestamp_suffix}[/b yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "This example uses [direct marketing dataset](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip):\n",
    "\n",
    "[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n",
    "\n",
    "Download and unzip the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "if not os.path.exists('data/bank-additional/bank-additional-full.csv'):\n",
    "    !wget -P data/ -N https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n",
    "    with zipfile.ZipFile(\"data/bank-additional.zip\", \"r\") as z:\n",
    "        print(\"Unzipping dataset to data folder...\")\n",
    "        z.extractall(\"data\")\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Skipping download..dataset exists at ./data/bank-additional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize dataset\n",
    "The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).\n",
    "\n",
    "Problem Type: **Binary Classification**\n",
    "\n",
    "Ref: <https://archive.ics.uci.edu/ml/datasets/bank+marketing>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"./data/bank-additional/bank-additional-full.csv\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 10)  # View all of the columns\n",
    "df_data  # show first 5 and last 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset to S3\n",
    "We'll upload the `bank-additional-full.csv` from the extracted Zip file to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this flag to False for subsequent runs of this notebook\n",
    "upload_dataset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"data/bank-additional/bank-additional-full.csv\"\n",
    "\n",
    "if upload_dataset:\n",
    "    print(f\"Uploading data to s3...\")\n",
    "    dataset_s3uri = session.upload_data(DATA_FILE, key_prefix=f\"{prefix}/raw\")\n",
    "    print(f\"Data uploaded to : \\n [b yellow2]{dataset_s3uri}[/b yellow2]\")\n",
    "else:\n",
    "    dataset_s3uri = f\"s3://{bucket}/{prefix}/raw/bank-additional-full.csv\"\n",
    "    print(f\"Skipping upload .. dataset is under: {dataset_s3uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Autopilot jobs in `ENSEMBLING` and `HPO` modes\n",
    "\n",
    "\n",
    "First we'll specify the AutoML job config constants\n",
    "- `TargetAttributeName` (Target column `y` for your dataset)\n",
    "- `Training Mode` - `Valid values: AUTO | ENSEMBLING | HYPERPARAMETER_TUNING`\n",
    "- `ProblemType` (optional) `Valid values: BinaryClassification | MulticlassClassification | Regression`\n",
    "- `ObjectiveMetric` (Optional) Valid Values: `Accuracy | F1 | MSE` [`AutoMLJobObjective`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AutoMLJobObjective.html)\n",
    "- `Max_Candidates` (Optional) (set only for HPO Jobs)\n",
    "- `OutputDataConfig` (Optional, set if you need to specify output location for artifacts generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autopilot job params\n",
    "target_column = \"y\"\n",
    "training_mode = \"ENSEMBLING\"\n",
    "\n",
    "# Optional Parameters\n",
    "problem_type = \"BinaryClassification\"\n",
    "objective_metric = \"F1\"\n",
    "max_job_runtime_seconds = 3600\n",
    "max_runtime_per_job_seconds = 1200\n",
    "max_candidates = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the Autopilot job config values\n",
    "- [`AutoMLJobConfig`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AutoMLJobConfig.html) (`Mode` = `AUTO | ENSEMBLING | HYPERPARAMETER_TUNING`)\n",
    "- [`InputDataConfig`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateAutoMLJob.html#sagemaker-CreateAutoMLJob-request-InputDataConfig)\n",
    "- [`AutoMLJobObjective`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AutoMLJobObjective.html) (Optional. `Accuracy | MSE | F1 | F1macro | AUC`)\n",
    "- [`OutputDataConfig`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AutoMLOutputDataConfig.html) (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Autopilot job config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_job_config = {\n",
    "    \"CompletionCriteria\": {\n",
    "        \"MaxRuntimePerTrainingJobInSeconds\": max_runtime_per_job_seconds,\n",
    "        \"MaxAutoMLJobRuntimeInSeconds\": max_job_runtime_seconds,\n",
    "    },\n",
    "    \"Mode\": training_mode,\n",
    "}\n",
    "\n",
    "automl_job_objective = {\"MetricName\": objective_metric}\n",
    "\n",
    "input_data_config = [\n",
    "    {\n",
    "        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": dataset_s3uri}},\n",
    "        \"TargetAttributeName\": target_column,\n",
    "    }\n",
    "]\n",
    "\n",
    "output_data_config = {\"S3OutputPath\": f\"s3://{bucket}/{prefix}/output\"}\n",
    "\n",
    "# Optional: Define a Tag\n",
    "tags_config = [{\"Key\": \"Project\", \"Value\": \"Autopilot-serverless\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Autopilot job with training mode set to `ENSEMBLING`\n",
    "\n",
    "Construct AutoML job name with prefix and suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ens_automl_job_name = f\"{model_prefix}-ENS-{timestamp_suffix}\"\n",
    "    print(\n",
    "        f\"Launching AutoMLJob → [magenta2]{ens_automl_job_name}[/magenta2] with mode set to [b]{training_mode}[/b]\"\n",
    "    )\n",
    "    response = sm_client.create_auto_ml_job(\n",
    "        AutoMLJobName=ens_automl_job_name,\n",
    "        InputDataConfig=input_data_config,\n",
    "        OutputDataConfig=output_data_config,\n",
    "        AutoMLJobConfig=automl_job_config,\n",
    "        ProblemType=problem_type,\n",
    "        AutoMLJobObjective=automl_job_objective,\n",
    "        RoleArn=role,\n",
    "        Tags=tags_config,\n",
    "    )\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error launching Autopilot Job: {ens_automl_job_name}\")\n",
    "    print(f\"Exception:\\n{e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Autopilot job with training mode set to `HYPERPARAMETER_TUNING` mode\n",
    "\n",
    "We'll update the `automl_job_config` dict to update `training_mode` to `HYPERPARAMETER_TUNING` and set the `MaxCandidates` to 15.\n",
    "\n",
    ">NOTE: In `HPO` mode the best model is derived by tuning various hyperparameters, default setting for `max_candidates` is 250 but for demonstration purposes we'll set the `max_candidates` to 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the defined job prefix to construct model name(s) and later to construct endpoint config and endpoint names.\n",
    "try:\n",
    "    training_mode = \"HYPERPARAMETER_TUNING\"\n",
    "    automl_job_config[\"Mode\"] = training_mode\n",
    "    automl_job_config[\"CompletionCriteria\"][\"MaxCandidates\"] = 15\n",
    "    hpo_automl_job_name = f\"{model_prefix}-HPO-{timestamp_suffix}\"\n",
    "    print(\n",
    "        f\"Launching AutoMLJob → [magenta2]{hpo_automl_job_name}[/magenta2] with mode set to [bold]{training_mode}[/bold]\"\n",
    "    )\n",
    "    response = sm_client.create_auto_ml_job(\n",
    "        AutoMLJobName=hpo_automl_job_name,\n",
    "        InputDataConfig=input_data_config,\n",
    "        OutputDataConfig=output_data_config,\n",
    "        AutoMLJobConfig=automl_job_config,\n",
    "        ProblemType=problem_type,\n",
    "        AutoMLJobObjective=automl_job_objective,\n",
    "        RoleArn=role,\n",
    "        Tags=tags_config,\n",
    "    )\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error launching Autopilot Job: {ens_automl_job_name}\")\n",
    "    print(f\"[bright_red]{e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor AutoML job completion status\n",
    "\n",
    "Let's use `Progress()` from rich library to display a progress bar:\n",
    "- Create Progress bar with columns in this order: `TextColumn`, `Spinner`, `ProgressBar` and `TimeElapsed`\n",
    "- Add tasks to progress bar; add custom field \"Status\" to override task description with AutoML Jobs Primary and Secondary Status\n",
    "- while progress is not finished, call function `get_job_status` every 15 secs to get the current status and update progress bar task status accordingly\n",
    "\n",
    "Helper function to query Autopilot job status and update progress bar task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(job_name, taskid=1000):\n",
    "    resp = sm_client.describe_auto_ml_job(AutoMLJobName=job_name)\n",
    "    p_status = resp[\"AutoMLJobStatus\"]\n",
    "    s_status = resp[\"AutoMLJobSecondaryStatus\"]\n",
    "    desc = f\"[b]{job_name}[/b]: [b green]{p_status} | [b magenta2]{s_status}[/b magenta2] ...\"\n",
    "    if taskid == 1000:\n",
    "        return (p_status, desc)\n",
    "    else:\n",
    "        progress.start_task(taskid)\n",
    "    if not p_status in (\"Completed\", \"Failed\"):\n",
    "        progress.update(taskid, status=desc, advance=0.85)\n",
    "    else:\n",
    "        progress.update(taskid, status=desc, advance=100)\n",
    "        progress.stop_task(taskid)\n",
    "    return (p_status, desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll utilize [rich](https://github.com/Textualize/rich) library's [Progress bars](https://rich.readthedocs.io/en/latest/progress.html) to monitor job completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_job_names = [hpo_automl_job_name, ens_automl_job_name]\n",
    "\n",
    "progress = Progress(\n",
    "    TextColumn(\"{task.fields[status]}\"),\n",
    "    SpinnerColumn(),\n",
    "    BarColumn(),\n",
    "    TimeElapsedColumn(),\n",
    ")\n",
    "\n",
    "tasks = list()\n",
    "for jobname in automl_job_names:\n",
    "    (p_status, desc) = get_job_status(jobname)\n",
    "    task = progress.add_task(desc, total=100, status=desc)\n",
    "    tasks.append(task)\n",
    "\n",
    "with Live(progress, refresh_per_second=4):\n",
    "    while not progress.finished:\n",
    "        statuses = [get_job_status(jobname, _) for _, jobname in enumerate(automl_job_names)]\n",
    "        while statuses[0][0] not in (\"Completed\", \"Failed\") or statuses[1][0] not in (\n",
    "            \"Completed\",\n",
    "            \"Failed\",\n",
    "        ):\n",
    "            statuses = [get_job_status(jobname, _) for _, jobname in enumerate(automl_job_names)]\n",
    "            sleep(15)\n",
    "        progress.completed = True\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the above jobs complete.\n",
    ">**NOTE:** Jobs with `ENSEMBLING` mode finishes faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model(s) from the Best Candidate generated by Autopilot\n",
    "- In `Ensemble` training mode Autopilot generates a single Inference container.\n",
    "\n",
    "- In `HPO` training mode, for Classification Problem types, Autopilot generates 3 containers which consist of data and inference model containers.\n",
    "\n",
    "![](./images/ap-jobprofile-hpo-04Oct2022.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naming convention\n",
    "\n",
    "**Model** names we append model type i.e. `data-model` or `inference` and the index of the container.\n",
    "\n",
    "_e.g._ `bankmrkt-hpo-d3129a-22Nov2022-datamodel-0` OR `bankmrkt-hpo-d3129a-22Nov2022-inf-1`\n",
    "\n",
    "for **Endpoint config** names we'll prefix the Model name (see above) with `epc-`\n",
    "\n",
    "_e.g._ `epc-bankmrkt-hpo-d3129a-22Nov2022-datamodel-0` OR `epc-bankmrkt-hpo-d3129a-22Nov2022-inf-1`\n",
    "\n",
    "for **Endpoint** names we'll prefix the Model name (see above) with `ep-`\n",
    "\n",
    "_e.g._ `ep-bankmrkt-hpo-d3129a-22Nov2022-datamodel-0` OR `ep-bankmrkt-hpo-d3129a-22Nov2022-inf-1`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions to create model(s), serverless endpoint config and endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autopilot_model(sm_client, model_name, role, model_container, index):\n",
    "    try:\n",
    "        transform_mode = model_container[\"Environment\"][\"AUTOML_TRANSFORM_MODE\"]\n",
    "        if transform_mode:\n",
    "            model_name = f\"{model_name}-datamodel-{index}\"\n",
    "    except:\n",
    "        model_name = f\"{model_name}-Inf-{index}\"\n",
    "\n",
    "    if len(model_name) <= 63:\n",
    "        print(f\"Creating Model {index}: {model_name} ...\")\n",
    "        model_response = sm_client.create_model(\n",
    "            ModelName=model_name, ExecutionRoleArn=role, Containers=[model_container]\n",
    "        )\n",
    "        status_code = model_response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "        model_arn = model_response[\"ModelArn\"]\n",
    "        return (status_code, model_arn)\n",
    "    else:\n",
    "        print(f\"Model Name: {model_name} length exceeds max. allowed chars : 63\")\n",
    "        raise ValueError(\"Model name cannot exceed 63 chars.\")\n",
    "\n",
    "\n",
    "def create_serverless_endpoint_config(\n",
    "    sm_client, endpoint_config_name, model_name, memory: int = 2048, max_concurrency: int = 20\n",
    "):\n",
    "    if len(endpoint_config_name) <= 63:\n",
    "        print(f\"Creating Endpoint Config: {endpoint_config_name} ...\")\n",
    "        try:\n",
    "            epc_response = sm_client.create_endpoint_config(\n",
    "                EndpointConfigName=endpoint_config_name,\n",
    "                ProductionVariants=[\n",
    "                    {\n",
    "                        \"ModelName\": model_name,\n",
    "                        \"VariantName\": \"AllTraffic\",\n",
    "                        \"ServerlessConfig\": {\n",
    "                            \"MemorySizeInMB\": memory,\n",
    "                            \"MaxConcurrency\": max_concurrency,\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            status_code = epc_response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "            epc_arn = epc_response[\"EndpointConfigArn\"]\n",
    "            return (status_code, epc_arn)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating EndpointConfig: {endpoint_config_name}\")\n",
    "            print(f\"[bright_red]{e}\")\n",
    "    else:\n",
    "        print(f\"EndpointConfig name exceeds allowed 63 char limit\")\n",
    "        raise ValueError(\"EndpointConfig name cannot exceed 63 chars.\")\n",
    "\n",
    "\n",
    "def create_serverless_endpoint(sm_client, endpoint_name, endpoint_config_name):\n",
    "    if len(endpoint_config_name) <= 63:\n",
    "        print(f\"Creating Serverless Endpoint: {endpoint_name} ...\")\n",
    "        try:\n",
    "            ep_response = sm_client.create_endpoint(\n",
    "                EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "            )\n",
    "            status_code = ep_response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "            return status_code\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Endpoint: {endpoint_name}\")\n",
    "            print(f\"[bright_red]{e}\")\n",
    "    else:\n",
    "        print(f\"Endpoint name exceeds allowed 63 char limit\")\n",
    "        raise ValueError(\"Endpoint name cannot exceed 63 chars.\")\n",
    "\n",
    "\n",
    "def get_s3_objsize_in_MB(bucket, key):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    resp = s3.head_object(Bucket=bucket, Key=key)[\"ContentLength\"]\n",
    "    size = round(resp / (1024 * 1024))\n",
    "    if size < 1:\n",
    "        print(f\"Model Size: ~ {round(resp / 1024)} KB\")\n",
    "    else:\n",
    "        print(f\"Model Size: ~ {size} MB\")\n",
    "\n",
    "    return size\n",
    "\n",
    "\n",
    "def set_serverless_endpoint_memory(model_size: int):\n",
    "    if model_size <= 1024:\n",
    "        return 1024\n",
    "    elif model_size > 1024 and model_size <= 2048:\n",
    "        return 2048\n",
    "    elif model_size > 2048 and model_size <= 3072:\n",
    "        return 3072\n",
    "    elif model_size > 3072 and model_size <= 4096:\n",
    "        return 4096\n",
    "    elif model_size > 4096 and model_size <= 5120:\n",
    "        return 5120\n",
    "    elif model_size > 5120 and model_size <= 6144:\n",
    "        return 6144\n",
    "    elif model_size > 6144:\n",
    "        raise ValueError(\"Model size is greater than 6GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify model size and create Serverless Endpoint config accordingly\n",
    "\n",
    ">Serverless Inference auto-assigns compute resources proportional to the memory you select. \n",
    "If you choose a larger memory size, your container has access to more `vCPUs`. Choose your endpoint’s memory size according to your model size. \n",
    "Generally, the memory size should be at least as large as your model size. \n",
    "\n",
    "Ref: <https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-create.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "\n",
    "response = sm_client.describe_auto_ml_job(AutoMLJobName=ens_automl_job_name)\n",
    "best_candidate = response[\"BestCandidate\"]\n",
    "inference_container = response[\"BestCandidate\"][\"InferenceContainers\"][0]\n",
    "print(f\"Inference Container for AutoML job: [b magenta2]{ens_automl_job_name}[/b magenta2]\")\n",
    "print(inference_container)\n",
    "\n",
    "# Verify generated model size before creating endpoint config.\n",
    "# print(f\"calculating generated model size ...\")\n",
    "# Extract s3 Key from ModelDataUrl\n",
    "model_dataurl_key = inference_container[\"ModelDataUrl\"].split(f\"{bucket}\")[1][1:]\n",
    "ens_model_size = get_s3_objsize_in_MB(bucket, model_dataurl_key)\n",
    "print(f\"Ensemble Model Size: ~ [b]{ens_model_size}MB[b/]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set serverless endpoint config `MemorySize` and `MaxConcurrency`.\n",
    "\n",
    "Generally, the memory size should be **at least** as large as your model size. \n",
    "\n",
    "We'll choose `4096` (4 GB) for endpoint memory size and set `MaxConcurrency` to 10.\n",
    "\n",
    "Your serverless endpoint has a minimum RAM size of **1024 MB (1 GB)**, and the maximum RAM size you can choose is **6144 MB (6 GB)**\n",
    "\n",
    "If you don't specify any Memory `2048` (2 GB) is chosen as default.\n",
    "\n",
    "The memory sizes you can choose are 1024 MB, 2048 MB, 3072 MB, 4096 MB, 5120 MB, or 6144 MB.\n",
    "\n",
    "Ref: <https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html#serverless-endpoints-how-it-works-memory>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list()\n",
    "# create model\n",
    "(status, model_arn) = create_autopilot_model(\n",
    "    sm_client, ens_automl_job_name, role, inference_container, 0\n",
    ")\n",
    "model_name = model_arn.split(\"/\")[1]\n",
    "models.append(model_name)\n",
    "\n",
    "endpoint_configs = list()\n",
    "endpoint_config_name = f\"epc-{model_name}\"\n",
    "memory = 4096\n",
    "# create endpoint config\n",
    "(status, epc_arn) = create_serverless_endpoint_config(\n",
    "    sm_client, endpoint_config_name, model_name, memory=memory, max_concurrency=10\n",
    ")\n",
    "endpoint_configs.append(endpoint_config_name)\n",
    "\n",
    "endpoints = list()\n",
    "endpoint_name = endpoint_config_name.replace(\"epc-\", \"ep-\")\n",
    "# create serverless endpoint\n",
    "create_serverless_endpoint(sm_client, endpoint_name, endpoint_config_name)\n",
    "endpoints.append(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor endpoint creation status\n",
    "\n",
    "Helper function to get endpoint status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_endpoint_status(endpoint_name, taskid=1000):\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    desc = f\"[b]Endpoint {endpoint_name}[/b] | [i magenta2] {status} ...\"\n",
    "    if taskid == 1000:\n",
    "        return (status, desc)\n",
    "    else:\n",
    "        progress.start_task(taskid)\n",
    "    if not status in (\"InService\", \"Failed\"):\n",
    "        progress.update(taskid, description=desc, advance=1.25)\n",
    "    else:\n",
    "        progress.update(taskid, description=desc, advance=100)\n",
    "        progress.stop_task(taskid)\n",
    "    return (status, desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for endpoint status to be `InService`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = Progress(\n",
    "    TextColumn(\"{task.description}\"),\n",
    "    SpinnerColumn(),\n",
    "    BarColumn(),\n",
    "    TimeElapsedColumn(),\n",
    ")\n",
    "\n",
    "tasks = list()\n",
    "for ep in endpoints:\n",
    "    (status, desc) = get_endpoint_status(ep)\n",
    "    task = progress.add_task(description=desc, total=100)\n",
    "    tasks.append(task)\n",
    "\n",
    "# rich progress bar to monitor endpoint creation\n",
    "with Live(progress, refresh_per_second=4):\n",
    "    while not progress.finished:\n",
    "        statuses = [get_endpoint_status(ep, _) for _, ep in enumerate(endpoints)]\n",
    "        while statuses[0][0] not in (\"InService\", \"Failed\"):\n",
    "            statuses = [get_endpoint_status(ep, _) for _, ep in enumerate(endpoints)]\n",
    "            sleep(15)\n",
    "        progress.completed = True\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Inference request to serverless endpoint with ENSEMBLE model\n",
    "\n",
    ">**NOTE:** Serverless endpoints, being fully-managed, provision compute resources on demand, as a result your endpoint may experience cold starts. Typically, you'll experience a cold start during the first inference request and after a brief period of inactivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "endpoint = endpoints[0]\n",
    "\n",
    "payload = \"51,technician,married,professional.course,no,yes,no,cellular,apr,thu,687,1,0,1,success,-1.8,93.075,-47.1,1.365,5099.1\"\n",
    "# payload = \"42,services,married,professional.course,no,yes,no,telephone,may,thu,813,1,999,0,nonexistent,1.1,93.994,-36.4,4.855,5191.0\"\n",
    "# payload = \"37,services,married,high.school,no,yes,no,telephone,may,mon,226,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\"\n",
    "# payload = \"55,admin.,married,high.school,no,no,no,telephone,may,thu,94,1,999,0,nonexistent,1.1,93.994,-36.4,4.855,5191.0\"\n",
    "# payload = \"34,blue-collar,married,basic.4y,no,no,no,telephone,may,tue,800,4,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\"\n",
    "\n",
    "try:\n",
    "    print(f\"Invoking endpoint: [magenta2]{endpoint}[/magenta2] with payload .. \\n\")\n",
    "    pretty = Pretty(payload, no_wrap=True)\n",
    "    panel = Panel(pretty, title=\"payload\")\n",
    "    print(panel)\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint,\n",
    "        sagmaker_session=session,\n",
    "        serializer=CSVSerializer(),\n",
    "    )\n",
    "    prediction = predictor.predict(payload)\n",
    "    print(f\"Predicted Label: [b]{prediction.decode('utf-8')}[/b]\")\n",
    "except Exception as e:\n",
    "    print(f\"Error invoking Endpoint: [b yellow]{endpoint}[/b yellow]\")\n",
    "    print(f\"[bright_red]{e}[/bright_red]\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup (ensemble endpoint)\n",
    "Delete endpoint, endpoint config and model in that order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epc_name = endpoint.replace(\"ep-\", \"epc-\")\n",
    "model_name = endpoint.replace(\"ep-\", \"\")\n",
    "\n",
    "print(f\"Deleting endpoint : [magenta2]{endpoint}\")\n",
    "try:\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint)\n",
    "except Exception as e:\n",
    "    print(f\"[bright_red]{e}\")\n",
    "    pass\n",
    "\n",
    "print(f\"Deleting EndpointConfig : [magenta2]{epc_name}\")\n",
    "try:\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=epc_name)\n",
    "except Exception as e:\n",
    "    print(f\"[bright_red]{e}\")\n",
    "    pass\n",
    "\n",
    "print(f\"Deleting Model : [magenta2]{model_name}\")\n",
    "try:\n",
    "    sm_client.delete_model(ModelName=model_name)\n",
    "except Exception as e:\n",
    "    print(f\"[bright_red]{e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy HPO models to serverless endpoints\n",
    "\n",
    "Autopilot in HYPERPARAMETER_TUNING mode generates 3 inference containers for binary classification problem types.\n",
    "\n",
    "Ref: <https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development-container-output.html#autopilot-problem-type-container-output>\n",
    "\n",
    "We'll enumerate through `InferenceContainers` list from `BestCandidate` HPO Model and create endpoints accordingly\n",
    "\n",
    "- Step 1: Create Model\n",
    "- Step 2: Create Endpoint Config with Model Name\n",
    "- Step 3: Create Endpoint with Endpoint Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at `InferenceContainers` from the generated `BestCandidate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "\n",
    "job_response = sm_client.describe_auto_ml_job(AutoMLJobName=hpo_automl_job_name)\n",
    "best_candidate = job_response[\"BestCandidate\"]\n",
    "inference_containers = job_response[\"BestCandidate\"][\"InferenceContainers\"]\n",
    "print(f\"Inference Containers for AutoML job: [b magenta2]{hpo_automl_job_name}\")\n",
    "print(inference_containers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get model sizes of generated inference containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, container in enumerate(inference_containers):\n",
    "    print(f\"calculating generated model_{idx} size\")\n",
    "    # Extract s3 Key from ModelDataUrl\n",
    "    model_dataurl_key = container[\"ModelDataUrl\"].split(f\"{bucket}\")[1][1:]\n",
    "    # print(model_dataurl_key)\n",
    "    model_size = get_s3_objsize_in_MB(bucket, model_dataurl_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All generated models are less than 1 MB. \n",
    "Let's set `MemorySize` to **2048 MB** and `MaxConcurrency` to **10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list()\n",
    "endpoint_configs = list()\n",
    "endpoints = list()\n",
    "\n",
    "memory = 2048\n",
    "max_concurreny = 10\n",
    "\n",
    "# Create model, endpoint_config, endpoint and store them in lists for easier access\n",
    "for idx, container in enumerate(inference_containers):\n",
    "    (status, model_arn) = create_autopilot_model(\n",
    "        sm_client, hpo_automl_job_name, role, container, idx\n",
    "    )\n",
    "    model_name = model_arn.split(\"/\")[1]\n",
    "    print(f\"\\tcreated model: {model_name}...\")\n",
    "    models.append(model_name)\n",
    "\n",
    "    endpoint_config_name = f\"epc-{model_name}\"\n",
    "    endpoint_name = f\"ep-{model_name}\"\n",
    "\n",
    "    (status, epc_arn) = create_serverless_endpoint_config(\n",
    "        sm_client, endpoint_config_name, model_name, memory=memory, max_concurrency=max_concurreny\n",
    "    )\n",
    "    print(f\"\\tcreated epc: {endpoint_config_name}\")\n",
    "    endpoint_configs.append(endpoint_config_name)\n",
    "\n",
    "    res = create_serverless_endpoint(sm_client, endpoint_name, endpoint_config_name)\n",
    "    print(f\"\\tcreated ep: [magenta2]{endpoint_name}\")\n",
    "    endpoints.append(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor Endpoints Status Creation : wait till Endpoints are `InService` state\n",
    "Wait till all Endpoints are in `InService` status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = Progress(\n",
    "    TextColumn(\"{task.description}\"),\n",
    "    SpinnerColumn(),\n",
    "    BarColumn(),\n",
    "    TimeElapsedColumn(),\n",
    ")\n",
    "\n",
    "tasks = list()\n",
    "for ep in endpoints:\n",
    "    (status, desc) = get_endpoint_status(ep)\n",
    "    task = progress.add_task(description=desc, total=100)\n",
    "    tasks.append(task)\n",
    "\n",
    "with Live(progress, refresh_per_second=4):\n",
    "    while not progress.finished:\n",
    "        statuses = [get_endpoint_status(ep, _) for _, ep in enumerate(endpoints)]\n",
    "        while statuses[0][0] not in (\"InService\", \"Failed\") or statuses[1][0] not in (\n",
    "            \"InService\",\n",
    "            \"Failed\",\n",
    "        ):\n",
    "            statuses = [get_endpoint_status(ep, _) for _, ep in enumerate(endpoints)]\n",
    "            sleep(10)\n",
    "        progress.completed = True\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send inference request to get predictions from each endpoint\n",
    "\n",
    "Inference request flow:\n",
    "\n",
    "![](./images/ap-hpo-serverless-payloadflow.png)\n",
    "\n",
    ">NOTE: Serverless endpoints, being fully-managed, provision compute resources on demand, as a result the endpoint may experience cold starts. Typically, you'll experience a cold start during the first inference request and after a brief period of inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "payload = \"51,technician,married,professional.course,no,yes,no,cellular,apr,thu,687,1,0,1,success,-1.8,93.075,-47.1,1.365,5099.1\"\n",
    "# payload = \"42,services,married,professional.course,no,yes,no,telephone,may,thu,813,1,999,0,nonexistent,1.1,93.994,-36.4,4.855,5191.0\"\n",
    "# payload = \"37,services,married,high.school,no,yes,no,telephone,may,mon,226,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\"\n",
    "# payload = \"55,admin,married,high.school,no,no,no,telephone,may,thu,94,1,999,0,nonexistent,1.1,93.994,-36.4,4.855,5191.0\"\n",
    "# payload = \"34,blue-collar,married,basic.4y,no,no,no,telephone,may,tue,800,4,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\"\n",
    "# payload = \"100,services,married,high.school,no,yes,no,cellular,apr,thu,483,2,999,0,nonexistent,-1.8,93.075,-47.1,1.41,5099.1\"\n",
    "\n",
    "for _, ep in enumerate(endpoints):\n",
    "    try:\n",
    "        print(f\"Invoking Endpoint {_}: [b magenta2] {ep}\")\n",
    "        pretty = Pretty(payload, no_wrap=True)\n",
    "        panel = Panel(pretty, title=\"payload\")\n",
    "        print(panel)\n",
    "        predictor = Predictor(\n",
    "            endpoint_name=ep,\n",
    "            sagemaker_session=session,\n",
    "            serializer=CSVSerializer(),\n",
    "        )\n",
    "        prediction = predictor.predict(payload)\n",
    "        payload = prediction\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Endpoint; {ep} \\n {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"Final Prediction: [b yellow2]{payload.decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (HPO endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "\n",
    "print(\"Deleting endpoints...\")\n",
    "for _, ep in enumerate(endpoints):\n",
    "    try:\n",
    "        print(f\"\\tDeleting [b]{ep}[/b]...\")\n",
    "        sm_client.delete_endpoint(EndpointName=ep)\n",
    "    except Exception as e:\n",
    "        print(f\"[bright_red]{e}\")\n",
    "        continue\n",
    "print(\"--\" * 15)\n",
    "print(\"Deleting endpoint configs...\")\n",
    "for (_, epc) in enumerate(endpoint_configs):\n",
    "    try:\n",
    "        print(f\"\\tDeleting [b]{epc}[/b]...\")\n",
    "        sm_client.delete_endpoint_config(EndpointConfigName=epc)\n",
    "    except Exception as e:\n",
    "        print(f\"[bright_red]{e}\")\n",
    "        continue\n",
    "print(\"--\" * 15)\n",
    "print(\"Deleting models...\")\n",
    "for (_, mdl) in enumerate(models):\n",
    "    try:\n",
    "        print(f\"\\tDeleting [b]{mdl}[/b]...\")\n",
    "        sm_client.delete_model(ModelName=mdl)\n",
    "    except Exception as e:\n",
    "        print(f\"[bright_red]{e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Done\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c4c57eb149902836539fe532ea353cbda55dc8653105f24c3221170071603b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
