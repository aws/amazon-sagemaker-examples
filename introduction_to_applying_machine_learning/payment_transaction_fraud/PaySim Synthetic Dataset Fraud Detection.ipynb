{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile money transactions Fraud Detection\n",
    "**Acknowledgements: This dataset is from Kaggle.\n",
    "For details, see https://www.kaggle.com/ntnu-testimon/paysim1/home**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [About this dataset](#About-this-dataset)\n",
    "- [Before you start](#Before-you-start)\n",
    "- [Setup](#Setup)\n",
    "- [Data Processing](#Data-Processing)\n",
    "  * [Load](#Load-data)\n",
    "  * [Explore](#Explore-the-dataset)\n",
    "  * [Feature Engineering](#Feature-Engineering)\n",
    "  * [Prepare dataset for SageMaker XGBoost](#Prepare-dataset-for-SageMaker-XGBoost)\n",
    "- [Training the XGBoost model](#Training-the-XGBoost-model)\n",
    "- [Predict Using Batch transform](#Predict-Using-Batch-transform)\n",
    "- [Predict using API inference endpoint](#Predict-using-API-inference-endpoint)\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataset\n",
    "\n",
    "\n",
    "There is a lack of public available datasets on financial services and specially in the emerging mobile money transactions domain. Financial datasets are important to many researchers and in particular to us performing research in the domain of fraud detection. Part of the problem is the intrinsically private nature of financial transactions, that leads to no publicly available datasets.\n",
    "\n",
    "We present a synthetic dataset generated using the simulator called PaySim as an approach to such a problem. PaySim uses aggregated data from the private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behaviour to later evaluate the performance of fraud detection methods.\n",
    "Content\n",
    "\n",
    "PaySim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world.\n",
    "\n",
    "This synthetic dataset is scaled down 1/4 of the original dataset and it is created just for Kaggle.\n",
    "\n",
    "### Headers\n",
    "\n",
    "This is a sample of 1 row with headers explanation:\n",
    "\n",
    "1,PAYMENT,1060.31,C429214117,1089.0,28.69,M1591654462,0.0,0.0,0,0\n",
    "\n",
    "step - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).\n",
    "\n",
    "type - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\n",
    "\n",
    "amount - amount of the transaction in local currency.\n",
    "\n",
    "nameOrig - customer who started the transaction\n",
    "\n",
    "oldbalanceOrg - initial balance before the transaction\n",
    "\n",
    "newbalanceOrig - new balance after the transaction\n",
    "\n",
    "nameDest - customer who is the recipient of the transaction\n",
    "\n",
    "oldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).\n",
    "\n",
    "newbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).\n",
    "\n",
    "isFraud - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.\n",
    "\n",
    "isFlaggedFraud - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction.\n",
    "Past Research\n",
    "\n",
    "There are 5 similar files that contain the run of 5 different scenarios. These files are better explained at my PhD thesis chapter 7 (PhD Thesis Available here http://urn.kb.se/resolve?urn=urn:nbn:se:bth-12932).\n",
    "\n",
    "We ran PaySim several times using random seeds for 744 steps, representing each hour of one month of real time, which matches the original logs. Each run took around 45 minutes on an i7 intel processor with 16GB of RAM. The final result of a run contains approximately 24 million of financial records divided into the 5 types of categories: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "This work is part of the research project ”Scalable resource-efficient systems for big data analytics” funded by the Knowledge Foundation (grant: 20140032) in Sweden.\n",
    "\n",
    "Please refer to this dataset using the following citations:\n",
    "\n",
    "PaySim first paper of the simulator:\n",
    "\n",
    "E. A. Lopez-Rojas , A. Elmir, and S. Axelsson. \"PaySim: A financial mobile money simulator for fraud detection\". In: The 28th European Modeling and Simulation Symposium-EMSS, Larnaca, Cyprus. 2016\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "1. Manually Download the dataset from Kaggle https://www.kaggle.com/ntnu-testimon/paysim1/downloads/PS_20174392719_1491204439457_log.csv/2 and upload to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the location of the input dataset uploaded from kaggle\n",
    "## Say. s3://mybucket/PaySimFraudDetection>/PS_20174392719_1491204439457_log.csv.zip\n",
    "s3_source_data=\"s3://<s3 path>\"\n",
    "\n",
    "##Results\n",
    "bucket=\"<enter your bucket to hold results>\"\n",
    "prefix=\"DemoPaySimFraudDetection\"\n",
    "bucket_prefix=\"{}/{}\".format(bucket,prefix)\n",
    "tmpdir=\"./tmpDemoPaySimFraudDetection\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is to ensure that the right libraries  are installed...\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $tmpdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "local_input_zip=os.path.join(tmpdir, \"paysim.zip\")\n",
    "\n",
    "!aws s3 cp $s3_source_data $local_input_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o $local_input_zip -d $tmpdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Load the csv file into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(tmpdir,'PS_20174392719_1491204439457_log.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist (bins=50, figsize=(15,15), color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of transactions wrt to source account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nameOrig'].value_counts().hist (bins=500, figsize=(15,5), color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nameOrig'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of transactions wrt to dest account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nameDest'].value_counts().hist (bins=500, figsize=(15,5), color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=['step', 'nameOrig']).head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isFraud.value_counts().plot.pie(autopct='%.2f',figsize=(5, 5), colors=[\"green\",\"cyan\"], explode=[0,.1])\n",
    "plt.title('Class Distribution')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highly imbalanced dataset\n",
    "Since this is a highly imbalanced dataset, use AUCPR instead of AUC under ROC as the eval metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( 1,2, figsize=(15,5))\n",
    "\n",
    "ax[0].set_title(\"Fraudent Records correlation\")\n",
    "sns.heatmap(data.query('isFraud == 1').drop(['isFraud', 'isFlaggedFraud'],1).corr(),  cmap=\"OrRd\", ax=ax[0])\n",
    "\n",
    "ax[1].set_title(\"Non-fraudent Records correlation\")\n",
    "sns.heatmap(data.query('isFraud == 0').drop(['isFraud', 'isFlaggedFraud'],1).corr(),  cmap=\"OrRd\", ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Amount and destination difference in balance dont match\n",
    "When the record has the isFlaggedFraud = 1, this means that the transaction was detected and stopped from being processed, that is the reason why it didn't affect the account destination/origin (previous value). \n",
    "\n",
    "**Note:** there is not record of balance from clients that start with M (Merchants).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data\n",
    "data_clean = data_clean.drop([\"newbalanceOrig\", \"newbalanceDest\", \"isFlaggedFraud\" ],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** If you dont remove the newbalanceDest you will get better results > 90% AUCPR. But I would think that is not entirely fair because in the dataset the source amount transfered doesnt add up to the destination increase in balance if the transfer is stopped. This happens whena the banks modelling system which detects a potential fraud transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"isMerchantTransOrig\"] = data_clean[\"nameOrig\"].str.startswith('M').astype(int) \n",
    "data_clean[\"isMerchantTransDest\"] = data_clean[\"nameDest\"].str.startswith('M').astype(int) \n",
    "\n",
    "data_clean[\"isMerchantTrans\"] = data_clean[\"isMerchantTransOrig\"] |  data_clean[\"isMerchantTransDest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( 1,2, figsize=(15,5))\n",
    "\n",
    "\n",
    "ax[0].set_title(\"Fraudent Records correlation after clean\")\n",
    "sns.heatmap(data_clean.query('isFraud == 1').drop(['isFraud'],1).corr(),  cmap=\"OrRd\", ax=ax[0])\n",
    "\n",
    "ax[1].set_title(\"Non-fraudent Records correlation after clean\")\n",
    "sns.heatmap(data_clean.query('isFraud == 0').drop(['isFraud'],1).corr(),  cmap=\"OrRd\", ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset for SageMaker XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column order - Labels in first column\n",
    "Recorder columns such that the label is the first column. This is because of the format expected by XGBoost SageMaker implementation, for more details see https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data_clean.drop([\"isFraud\"],1).columns.tolist()\n",
    "cols.insert(0, \"isFraud\")\n",
    "data_clean = data_clean[cols]\n",
    "data_clean.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unimportant non numerical column drop\n",
    "XGBoost only works with numerical values, drop non-numerical columns source/des accounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean.drop([ 'nameOrig', 'nameDest'],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onehot encode categorical columns \n",
    "XG boost only works with numerical values, so translate categorical columns into one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean =  pd.get_dummies(data_clean,prefix=['transaction_type'],  columns=['type']) \n",
    "data_clean.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test set split\n",
    "Split the dataset into train, test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "\n",
    "train_val, test = train_test_split(data_clean, test_size = 0.2, random_state = 777)\n",
    "train, validation = train_test_split(train_val, test_size = 0.2, random_state = 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( 1,3, figsize=(15,5))\n",
    "\n",
    "train.isFraud.value_counts().plot.pie(autopct='%.2f', ax = ax[0], colors=[\"green\",\"cyan\"], explode=[0,.1])\n",
    "ax[0].set_title('Train fraud distribution ({} records)'.format(train.shape[0]))\n",
    "\n",
    "test.isFraud.value_counts().plot.pie(autopct='%.2f', ax = ax[1], colors=[\"green\",\"cyan\"], explode=[0,.1])\n",
    "ax[1].set_title('Test fraud distribution ({} records)'.format(test.shape[0]))\n",
    "\n",
    "validation.isFraud.value_counts().plot.pie(autopct='%.2f', ax = ax[2], colors=[\"green\",\"cyan\"], explode=[0,.1])\n",
    "ax[2].set_title('Validation fraud distribution ({} records)'.format(validation.shape[0]))\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "trainfile=os.path.join(tmpdir, \"train_paysim.csv\")\n",
    "testfile=os.path.join(tmpdir,\"test_paysim.csv\")\n",
    "validationfile=os.path.join(tmpdir,\"validation_paysim.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the records to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(path_or_buf=trainfile, sep=',', na_rep='', header=False, index=False,  mode='w', encoding='UTF-8', quotechar='\"', line_terminator='\\n', decimal='.')\n",
    "test.to_csv(path_or_buf=testfile, sep=',', na_rep='', header=False, index=False,  mode='w', encoding='UTF-8', quotechar='\"', line_terminator='\\n', decimal='.')\n",
    "validation.to_csv(path_or_buf=validationfile, sep=',', na_rep='', header=False, index=False,  mode='w', encoding='UTF-8', quotechar='\"', line_terminator='\\n', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $trainfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the data to s3 into train and test channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3train=\"s3://{}/train/{}\".format(bucket_prefix, \"train.txt\")\n",
    "s3validation=\"s3://{}/validation/{}\".format(bucket_prefix, \"validation.txt\")\n",
    "\n",
    "print(trainfile)\n",
    "!aws s3 cp $trainfile $s3train\n",
    "!aws s3 cp $validationfile $s3validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training the XGBoost model\n",
    "\n",
    "After setting training parameters, we kick off training, and poll for status until training is completed, which in this example, takes between 5 and 6 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'Fraud-xgboost-classification-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "#Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/single-xgboost\".format(bucket_prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m4.4xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\":\"reg:logistic\",\n",
    "        \"num_round\":\"50\",\n",
    "        \"eval_metric\":\"auc\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 3600\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3train,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3validation,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "client.create_training_job(**create_training_params)\n",
    "\n",
    "import time\n",
    "\n",
    "status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "while status !='Completed' and status!='Failed':\n",
    "    time.sleep(60)\n",
    "    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name=job_name + '-model'\n",
    "print(model_name)\n",
    "\n",
    "info = client.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Using Batch transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good use of batch transform where you simply evalute the model  before deloying it as an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "batchfileinput=os.path.join(tmpdir, \"batchvalidation.csv\")\n",
    "batchfileresults=os.path.join(tmpdir, \"batchvalidation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "\n",
    "file_name = testfile \n",
    "with open(file_name, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "input_records = [\",\".join(l.strip().split(\",\")[1:]) for l in lines]\n",
    "labels = [int(l.split(\",\")[0]) for l in lines]\n",
    "\n",
    "\n",
    "with open(batchfileinput , \"w\") as f:\n",
    "    f.writelines([\"{}\\n\".format(item) for item in input_records])\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "\n",
    "fmttime= strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "input_key_file=\"batchvalidation.csv\"\n",
    "input_batch_key=\"{}/batchTransform/{}_input/{}\".format(prefix, fmttime, input_key_file)\n",
    "input_location = 's3://{}/{}'.format(bucket, input_batch_key)\n",
    "output_batch_key = \"{}/batchTransform/{}_output\".format(prefix,fmttime)\n",
    "output_location = 's3://{}/{}'.format(bucket, output_batch_key)\n",
    "\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(batchfileinput, bucket, input_batch_key)\n",
    "\n",
    "# Initialize the transformer object\n",
    "transformer =sagemaker.transformer.Transformer(\n",
    "    base_transform_job_name='Batch-Transform',\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.xlarge',\n",
    "    output_path=output_location\n",
    "    )\n",
    "# To start a transform job:\n",
    "transformer.transform(input_location, content_type='text/csv', split_type='Line')\n",
    "# Then wait until transform job is completed\n",
    "transformer.wait()\n",
    "\n",
    "# To fetch validation result \n",
    "outputkey ='{}/{}.out'.format(output_batch_key, input_key_file)\n",
    "print(outputkey)\n",
    "s3_client.download_file(bucket, outputkey, batchfileresults)\n",
    "with open(batchfileresults) as f:\n",
    "    results = f.readlines()   \n",
    "    predicted = [float(r) for r in results]\n",
    "print(\"Sample transform result: {}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement using - AUCPR\n",
    "Because the postive samples are underrepresented, measures such as AUC under ROC or accuracy inflate the numbers. So, use the AUC under the Precision Recall curve instead as it doesnt take into account True Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "micro_score = sklearn.metrics.average_precision_score(labels, predicted, average='micro',  sample_weight=None)\n",
    "print(\"AUC under precision recall curve is {}\".format(micro_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using API inference endpoint\n",
    "Now you are ready to deploy your model as an API.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'DEMO-XGBoostEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'DEMO-XGBoostEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke your api to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "\n",
    "file_name = testfile \n",
    "with open(file_name, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "input_records = [\",\".join(l.strip().split(\",\")[1:]) for l in lines]\n",
    "labels = [int(l.split(\",\")[0]) for l in lines]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predicted = []\n",
    "for record_chunks in chunks(input_records, 10000):\n",
    "    formatted = \"\\n\".join(record_chunks)\n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=formatted.encode('utf-8'))\n",
    "    result = response['Body'].read()\n",
    "    result = result.decode(\"utf-8\")\n",
    "    predicted.extend([float(r) for r in result.split(',')])\n",
    "    \n",
    "    print(\"Predicted {} out of {} so far ..\".format(len(predicted), len(input_records)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "    \n",
    "macro_score = sklearn.metrics.average_precision_score(labels, predicted, average='macro',  sample_weight=None)\n",
    "\n",
    "print(\"The AUC under precision recall curve is {}\".format(micro_score, macro_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold=.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(labels, pd.DataFrame(predicted) > confidence_threshold, labels=[1,0], sample_weight=None)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=[\"Fraud\", \"Non-Fraud\"], columns=[\"Fraud\", \"Non-Fraud\"], \n",
    ")\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt=\"d\", cmap=\"tab10\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(labels, pd.DataFrame(predicted) > confidence_threshold, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete endpoint as this is just a demo.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up local tmp directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $tmpdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "This XGBoost model doesnt take into account the time series ( the step sequence). So inorder to improve the model, we will look at using Time Series Classification Techniques. \n",
    "\n",
    "**Coming soon.....**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
