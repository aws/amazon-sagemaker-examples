{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engine for E-Commerce Sales\n",
    "\n",
    "This notebook gives an overview of techniques and services offer by SageMaker to build and deploy a personalized recommendation engine.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset for this demo comes from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail). It contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. The following attributes are included in our dataset:\n",
    "+ InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "+ StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "+ Description: Product (item) name. Nominal.\n",
    "+ Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "+ InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "+ UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n",
    "+ CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "+ Country: Country name. Nominal, the name of the country where each customer resides. \n",
    "\n",
    "Citation: Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197â€“208, 2012 (Published online before print: 27 August 2012. doi: 10.1057/dbm.2012.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation\n",
    "----\n",
    "The first of the notebook will focus on preparing the data for training.\n",
    "\n",
    "### Solution Architecture\n",
    "![Architecture](./images/retail_rec_dataprep.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "import boto3\n",
    "\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack, save_npz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sagemaker.__version__ >= \"2.21.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"using bucket{bucket} in region {region} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Online Retail.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "First, we check for any null (i.e. missing) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any records with a missing CustomerID. If we do not know who the customer is, then it is not helpful to us when we make recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"CustomerID\"], inplace=True)\n",
    "df[\"Description\"] = df[\"Description\"].apply(lambda x: x.strip())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df[\"Quantity\"], kde=True)\n",
    "plt.title(\"Distribution of Quantity\")\n",
    "plt.xlabel(\"Quantity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our quantities are realteively small (positive) numbers, but there are also some negative quantities as well as extreme outliers (both postiive and negative outliers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df[\"UnitPrice\"], kde=True)\n",
    "plt.title(\"Distribution of Unit Prices\")\n",
    "plt.xlabel(\"Price\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no negative prices, which is good, but we can see some extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby([\"StockCode\", \"Description\", \"CustomerID\", \"Country\", \"UnitPrice\"])[\n",
    "    \"Quantity\"\n",
    "].sum()\n",
    "df = df.loc[df > 0].reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(dataframe):\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    onehot_cols = [\"StockCode\", \"CustomerID\", \"Country\"]\n",
    "    ohe_output = enc.fit_transform(dataframe[onehot_cols])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=2)\n",
    "    unique_descriptions = dataframe[\"Description\"].unique()\n",
    "    vectorizer.fit(unique_descriptions)\n",
    "    tfidf_output = vectorizer.transform(dataframe[\"Description\"])\n",
    "\n",
    "    row = range(len(dataframe))\n",
    "    col = [0] * len(dataframe)\n",
    "    unit_price = csr_matrix((dataframe[\"UnitPrice\"].values, (row, col)), dtype=\"float32\")\n",
    "\n",
    "    X = hstack([ohe_output, tfidf_output, unit_price], format=\"csr\", dtype=\"float32\")\n",
    "\n",
    "    y = dataframe[\"Quantity\"].values.astype(\"float32\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = loadDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display sparsity\n",
    "total_cells = X.shape[0] * X.shape[1]\n",
    "(total_cells - X.nnz) / total_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is over 99.9% sparse. Because of this high sparsity, the sparse matrix data type allows us to represent our data using only a small fraction of the memory that a dense matrix would require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data For Modeling\n",
    "\n",
    "+ Split the data into training and testing sets\n",
    "+ Write the data to protobuf recordIO format for Pipe mode. [Read more](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html) about protobuf recordIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save numpy arrays to local storage in /data folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/online_retail_preprocessed.csv\", index=False)\n",
    "save_npz(\"data/X_train.npz\", X_train)\n",
    "save_npz(\"data/X_test.npz\", X_test)\n",
    "np.savez(\"data/y_train.npz\", y_train)\n",
    "np.savez(\"data/y_test.npz\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"personalization\"\n",
    "\n",
    "train_key = \"train.protobuf\"\n",
    "train_prefix = f\"{prefix}/train\"\n",
    "\n",
    "test_key = \"test.protobuf\"\n",
    "test_prefix = f\"{prefix}/test\"\n",
    "\n",
    "output_prefix = f\"s3://{bucket}/{prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeDatasetToProtobuf(X, y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, y)\n",
    "    buf.seek(0)\n",
    "    obj = \"{}/{}\".format(prefix, key)\n",
    "    boto3.resource(\"s3\").Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return \"s3://{}/{}\".format(bucket, obj)\n",
    "\n",
    "\n",
    "train_data_location = writeDatasetToProtobuf(X_train, y_train, bucket, train_prefix, train_key)\n",
    "test_data_location = writeDatasetToProtobuf(X_test, y_test, bucket, test_prefix, test_key)\n",
    "\n",
    "print(train_data_location)\n",
    "print(test_data_location)\n",
    "print(\"Output: {}\".format(output_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train, Tune, and Deploy Model\n",
    "----\n",
    "This second part will focus on training, tuning, and deploying a model trained on the data prepared in part 1.\n",
    "\n",
    "### Solution Architecture\n",
    "![Architecture](./images/retail_rec_train_reg_deploy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.lineage import context, artifact, association, action\n",
    "import boto3\n",
    "\n",
    "from model_package_src.inference_specification import InferenceSpecification\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from scipy.sparse import csr_matrix, hstack, load_npz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = \"personalization\"\n",
    "\n",
    "output_prefix = f\"s3://{bucket}/{prefix}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data For Modeling\n",
    "\n",
    "+ Split the data into training and testing sets\n",
    "+ Write the data to protobuf recordIO format for Pipe mode. [Read more](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html) about protobuf recordIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array\n",
    "X_train = load_npz(\"./data/X_train.npz\")\n",
    "X_test = load_npz(\"./data/X_test.npz\")\n",
    "y_train_npzfile = np.load(\"./data/y_train.npz\")\n",
    "y_test_npzfile = np.load(\"./data/y_test.npz\")\n",
    "y_train = y_train_npzfile.f.arr_0\n",
    "y_test = y_test_npzfile.f.arr_0\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"factorization-machines\", region=boto_session.region_name)\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    output_path=output_prefix,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=input_dims,\n",
    "    predictor_type=\"regressor\",\n",
    "    mini_batch_size=1000,\n",
    "    num_factors=64,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"training_job_name\" not in locals():\n",
    "\n",
    "    fm.fit({\"train\": train_data_location, \"test\": test_data_location})\n",
    "    training_job_name = fm.latest_training_job.job_name\n",
    "\n",
    "else:\n",
    "    print(f\"Using previous training job: {training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint. This will allow out to make predictions (or inference) from the model dyanamically.\n",
    "\n",
    "Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, Amazon Redshift, Amazon Athena, or other deployment target.\n",
    "\n",
    "Here we will take the top customer, the customer who spent the most money, and try to find which items to recommend to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMSerializer(JSONSerializer):\n",
    "    def serialize(self, data):\n",
    "        js = {\"instances\": []}\n",
    "        for row in data:\n",
    "            js[\"instances\"].append({\"features\": row.tolist()})\n",
    "        return json.dumps(js)\n",
    "\n",
    "\n",
    "fm_predictor = fm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    serializer=FMSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find customer who spent the most money\n",
    "df = pd.read_csv(\"data/online_retail_preprocessed.csv\")\n",
    "\n",
    "df[\"invoice_amount\"] = df[\"Quantity\"] * df[\"UnitPrice\"]\n",
    "top_customer = (\n",
    "    df.groupby(\"CustomerID\").sum()[\"invoice_amount\"].sort_values(ascending=False).index[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(df, customer_id, n_recommendations, n_ranks=100):\n",
    "    popular_items = (\n",
    "        df.groupby([\"StockCode\", \"UnitPrice\"])\n",
    "        .nunique()[\"CustomerID\"]\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    top_n_items = popular_items[\"StockCode\"].iloc[:n_ranks].values\n",
    "    top_n_prices = popular_items[\"UnitPrice\"].iloc[:n_ranks].values\n",
    "\n",
    "    # stock codes can have multiple descriptions, so we will choose whichever description is most common\n",
    "    item_map = df.groupby(\"StockCode\").agg(lambda x: x.value_counts().index[0])[\"Description\"]\n",
    "\n",
    "    # find customer's country\n",
    "    df_subset = df.loc[df[\"CustomerID\"] == customer_id]\n",
    "    country = df_subset[\"Country\"].value_counts().index[0]\n",
    "\n",
    "    data = []\n",
    "    flattened_item_map = [item_map[i] for i in top_n_items]\n",
    "    for idx in range(len(top_n_items)):\n",
    "        data.append(\n",
    "            {\n",
    "                \"StockCode\": top_n_items[idx],\n",
    "                \"Description\": flattened_item_map[idx],\n",
    "                \"CustomerID\": customer_id,\n",
    "                \"Country\": country,\n",
    "                \"UnitPrice\": top_n_prices[idx],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_inference = pd.DataFrame(data)\n",
    "\n",
    "    # we need to build the data set similar to how we built it for training\n",
    "    # it should have the same number of features as the training data\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    onehot_cols = [\"StockCode\", \"CustomerID\", \"Country\"]\n",
    "    enc.fit(df[onehot_cols])\n",
    "    onehot_output = enc.transform(df_inference[onehot_cols])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=2)\n",
    "    unique_descriptions = df[\"Description\"].unique()\n",
    "    vectorizer.fit(unique_descriptions)\n",
    "    tfidf_output = vectorizer.transform(df_inference[\"Description\"])\n",
    "\n",
    "    row = range(len(df_inference))\n",
    "    col = [0] * len(df_inference)\n",
    "    unit_price = csr_matrix((df_inference[\"UnitPrice\"].values, (row, col)), dtype=\"float32\")\n",
    "\n",
    "    X_inference = hstack([onehot_output, tfidf_output, unit_price], format=\"csr\")\n",
    "\n",
    "    result = fm_predictor.predict(X_inference.toarray())\n",
    "    preds = [i[\"score\"] for i in result[\"predictions\"]]\n",
    "    index_array = np.array(preds).argsort()\n",
    "    items = enc.inverse_transform(onehot_output)[:, 0]\n",
    "    top_recs = np.take_along_axis(items, index_array, axis=0)[: -n_recommendations - 1 : -1]\n",
    "    recommendations = [[i, item_map[i]] for i in top_recs]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 5 recommended products:\")\n",
    "get_recommendations(df, top_customer, n_recommendations=5, n_ranks=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done with the endpoint, you should delete the endpoint to save cost and free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.delete_model()\n",
    "fm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Part: Registering the Model in SageMaker Model Registry\n",
    "\n",
    "Once a useful model has been trained, you have the option to register the model for future reference and possible deployment. To do so, we must first properly associate the artifacts of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_info = sagemaker_boto_client.describe_training_job(TrainingJobName=training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_s3_uri = training_job_info[\"InputDataConfig\"][0][\"DataSource\"][\"S3DataSource\"][\n",
    "    \"S3Uri\"\n",
    "]\n",
    "\n",
    "matching_artifacts = list(\n",
    "    artifact.Artifact.list(source_uri=training_data_s3_uri, sagemaker_session=sagemaker_session)\n",
    ")\n",
    "\n",
    "if matching_artifacts:\n",
    "    training_data_artifact = matching_artifacts[0]\n",
    "    print(f\"Using existing artifact: {training_data_artifact.artifact_arn}\")\n",
    "else:\n",
    "    training_data_artifact = artifact.Artifact.create(\n",
    "        artifact_name=\"TrainingData\",\n",
    "        source_uri=training_data_s3_uri,\n",
    "        artifact_type=\"Dataset\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    print(f\"Create artifact {training_data_artifact.artifact_arn}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Artifact\n",
    "\n",
    "We do not need a code artifact because we are using a built-in SageMaker Algorithm called Factorization Machines. The Factorization Machines container contains all of the code and, by default, our model training stores the Factorization Machines image for tracking purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_s3_uri = training_job_info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "matching_artifacts = list(\n",
    "    artifact.Artifact.list(source_uri=trained_model_s3_uri, sagemaker_session=sagemaker_session)\n",
    ")\n",
    "\n",
    "if matching_artifacts:\n",
    "    model_artifact = matching_artifacts[0]\n",
    "    print(f\"Using existing artifact: {model_artifact.artifact_arn}\")\n",
    "else:\n",
    "    model_artifact = artifact.Artifact.create(\n",
    "        artifact_name=\"TrainedModel\",\n",
    "        source_uri=trained_model_s3_uri,\n",
    "        artifact_type=\"Model\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    print(f\"Create artifact {model_artifact.artifact_arn}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set artifact associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component = sagemaker_boto_client.describe_trial_component(\n",
    "    TrialComponentName=training_job_name + \"-aws-training-job\"\n",
    ")\n",
    "trial_component_arn = trial_component[\"TrialComponentArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_list = [[training_data_artifact, \"ContributedTo\"], [model_artifact, \"Produced\"]]\n",
    "\n",
    "for art, assoc in artifact_list:\n",
    "    try:\n",
    "        association.Association.create(\n",
    "            source_arn=art.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type=assoc,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "        )\n",
    "        print(f\"Association with {art.artifact_type}: SUCCEESFUL\")\n",
    "    except:\n",
    "        print(f\"Association already exists with {art.artifact_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"retail-recommendations\"\n",
    "model_matches = sagemaker_boto_client.list_models(NameContains=model_name)[\"Models\"]\n",
    "\n",
    "if not model_matches:\n",
    "    print(f\"Creating model {model_name}\")\n",
    "    model = sagemaker_session.create_model_from_job(\n",
    "        name=model_name,\n",
    "        training_job_name=training_job_info[\"TrainingJobName\"],\n",
    "        role=sagemaker_role,\n",
    "        image_uri=training_job_info[\"AlgorithmSpecification\"][\"TrainingImage\"],\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model {model_name} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Package Group\n",
    "\n",
    "After associating all the relevant artifacts, the Model Package Group can now be created. A Model Package Groups holds multiple versions or iterations of a model. Though it is not required to create them for every model in the registry, they help organize various models which all have the same purpose and provide autiomatic versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mpg_name\" not in locals():\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    mpg_name = f\"retail-recommendation-{timestamp}\"\n",
    "\n",
    "print(f\"Model Package Group name: {mpg_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_input_dict = {\n",
    "    \"ModelPackageGroupName\": mpg_name,\n",
    "    \"ModelPackageGroupDescription\": \"Recommendation for Online Retail Sales\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_mpg = sagemaker_boto_client.list_model_package_groups(NameContains=mpg_name)[\n",
    "    \"ModelPackageGroupSummaryList\"\n",
    "]\n",
    "\n",
    "if matching_mpg:\n",
    "    print(f\"Using existing Model Package Group: {mpg_name}\")\n",
    "else:\n",
    "    mpg_response = sagemaker_boto_client.create_model_package_group(**mpg_input_dict)\n",
    "    print(f\"Create Model Package Group {mpg_name}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_report = {\"regression_metrics\": {}}\n",
    "\n",
    "for metric in training_job_info[\"FinalMetricDataList\"]:\n",
    "    stat = {metric[\"MetricName\"]: {\"value\": metric[\"Value\"]}}\n",
    "    model_metrics_report[\"regression_metrics\"].update(stat)\n",
    "\n",
    "with open(\"training_metrics.json\", \"w\") as f:\n",
    "    json.dump(model_metrics_report, f)\n",
    "\n",
    "metrics_s3_key = f\"training_jobs/{training_job_info['TrainingJobName']}/training_metrics.json\"\n",
    "s3_client.upload_file(Filename=\"training_metrics.json\", Bucket=bucket, Key=metrics_s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the inference spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_inference_spec = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=training_job_info[\"AlgorithmSpecification\"][\"TrainingImage\"],\n",
    "    supports_gpu=False,\n",
    "    supported_content_types=[\"application/x-recordio-protobuf\", \"application/json\"],\n",
    "    supported_mime_types=[\"text/csv\"],\n",
    ")\n",
    "\n",
    "mp_inference_spec[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"] = training_job_info[\n",
    "    \"ModelArtifacts\"\n",
    "][\"S3ModelArtifacts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model metrics\n",
    "\n",
    "Metrics other than model quality can be defined. See the Boto3 documentation for [creating a model package](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model_package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {\n",
    "    \"ModelQuality\": {\n",
    "        \"Statistics\": {\n",
    "            \"ContentType\": \"application/json\",\n",
    "            \"S3Uri\": f\"s3://{bucket}/{metrics_s3_key}\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_input_dict = {\n",
    "    \"ModelPackageGroupName\": mpg_name,\n",
    "    \"ModelPackageDescription\": \"Factorization Machine Model to create personalized retail recommendations\",\n",
    "    \"ModelApprovalStatus\": \"PendingManualApproval\",\n",
    "    \"ModelMetrics\": model_metrics,\n",
    "}\n",
    "\n",
    "mp_input_dict.update(mp_inference_spec)\n",
    "mp_response = sagemaker_boto_client.create_model_package(**mp_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait until model package is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_info = sagemaker_boto_client.describe_model_package(\n",
    "    ModelPackageName=mp_response[\"ModelPackageArn\"]\n",
    ")\n",
    "mp_status = mp_info[\"ModelPackageStatus\"]\n",
    "\n",
    "while mp_status not in [\"Completed\", \"Failed\"]:\n",
    "    time.sleep(5)\n",
    "    mp_info = sagemaker_boto_client.describe_model_package(\n",
    "        ModelPackageName=mp_response[\"ModelPackageArn\"]\n",
    "    )\n",
    "    mp_status = mp_info[\"ModelPackageStatus\"]\n",
    "    print(f\"model package status: {mp_status}\")\n",
    "print(f\"model package status: {mp_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package = sagemaker_boto_client.list_model_packages(ModelPackageGroupName=mpg_name)[\n",
    "    \"ModelPackageSummaryList\"\n",
    "][0]\n",
    "model_package_update = {\n",
    "    \"ModelPackageArn\": model_package[\"ModelPackageArn\"],\n",
    "    \"ModelApprovalStatus\": \"Approved\",\n",
    "}\n",
    "\n",
    "update_response = sagemaker_boto_client.update_model_package(**model_package_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker_session)\n",
    "display(viz.show(training_job_name=training_job_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
