{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creative writing using GPT-2 Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creative writing can be very fun, yet challenging, especially when you hit that writer's block. In this notebook you will learn how to use AWS Marketplace GPT-2-XL pre-trained model on Amazon SageMaker to generate text based on your prompt to help author prose and poetry.\n",
    "\n",
    "GPT2 ([Generative Pre-trained Transformer 2](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)) algorithm is an unsupervised transformer language model. Transformer language models take advantage of transformer blocks. These blocks make it possible to process intra-sequence dependencies for all tokens in a sequence at the same time. GPT2 has been developed by [OpenAI](https://openai.com/) and is a powerful generative NLP model that excels in processing long-range dependencies and it is pre-trained on a diverse corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview:\n",
    "In [step 1](#Step-1-Determine-input-prompt-and-visualize-word-dependencies) of this notebook, you will determine an input prompt that will be used to condition the GPT-2 model for text generation. You will also [visualize attention](#Step-1.1-Introduction-to-attention) mechanism of GPT-2 model. In [step 2](#Step-2-Use-an-ML-model-to-generate-text-based-on-prompt), you will create the model from an AWS Marketplace subscription, and deploy to an Amazon SageMaker endpoint. In [step 3](#Step-3-Explore-use-cases-and-model-parameters), you will explore text generation use cases with various model parameter settings. In [step 4](#Step-4-Use-Amazon-SageMaker-batch-transform), you will perform inference asynchronously using SageMaker batch transform instead of the endpoint. In [Step 5](#Step-5-Next-steps) you will find additional models to explore and experiment with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents:\n",
    "* [Pre-requisites](#Pre-requisites)\n",
    "* [Step 1 Determine input prompt and visualize word dependencies](#Step-1-Determine-input-prompt-and-visualize-word-dependencies)\n",
    "    * [Step 1.1 Introduction to attention](#Step-1.1-Introduction-to-attention)\n",
    "    * [Step 1.2 Specify input prompt and visualize attention mechanism](#Step-1.2-Specify-input-prompt-and-visualize-attention-mechanism)\n",
    "* [Step 2 Use an ML model to generate text based on prompt](#Step-2-Use-an-ML-model-to-generate-text-based-on-prompt)\n",
    "    * [Step 2.1 Specify model arn from AWS Marketplace subscription](#Step-2.1-Specify-model-arn-from-AWS-Marketplace-subscription)\n",
    "    * [Step 2.2 Create model from model package and deploy to endpoint](#Step-2.2-Create-model-from-model-package-and-deploy-to-endpoint)\n",
    "* [Step 3 Explore use cases and model parameters](#Step-3-Explore-use-cases-and-model-parameters)\n",
    "    * [Step 3.1 Use case 1: Assist writing of prose](#Step-3.1-Use-case-1:-Assisted-writing-of-prose)\n",
    "    * [Step 3.2 Use case 2: Autonomous authoring of poem](#Step-3.2-Use-case-2:-Autonomous-authoring-of-poem)\n",
    "    * [Step 3.3 Delete Amazon SageMaker endpoint](#Step-3.3-Delete-Amazon-SageMaker-endpoint)\n",
    "* [Step 4 Use Amazon SageMaker batch transform](#Step-4-Use-Amazon-SageMaker-batch-transform)\n",
    "    * [Step 4.1 Create input file for batch transform job](#Step-4.1-Create-input-file-for-batch-transform-job)\n",
    "    * [Step 4.2 Upload file to S3](#Step-4.2-Upload-file-to-S3)\n",
    "    * [Step 4.3 Execute the batch transform job](#Step-4.3-Execute-the-batch-transform-job)\n",
    "    * [Step 4.4 Visualize output](#Step-4.4-Visualize-output)\n",
    "    * [Step 4.5 Delete the model](#Step-4.5-Delete-the-model)\n",
    "* [Step 5 Next steps](#Step-5-Next-steps)\n",
    "    * [Step 5.1 Additional resources](#Step-5.1-Additional-resources)\n",
    "    * [Step 5.2 Cancel AWS Marketplace subscription](#Step-5.2-Cancel-AWS-Marketplace-subscription)\n",
    "\n",
    "#### Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-requisites**\n",
    "\n",
    "This sample notebook requires a subscription to **[GPT-2 XL - Text generation](https://aws.amazon.com/marketplace/pp/prodview-cdujckyfypprg)**, a pre-trained machine learning model package from AWS Marketplace. \n",
    "If your AWS account has not been subscribed to this listing, here is the process you can follow: \n",
    "1. Open the [listing](https://aws.amazon.com/marketplace/pp/prodview-cdujckyfypprg) from AWS Marketplace\n",
    "1. Read the **Highlights** section and then **product overview** section of the listing.\n",
    "1. View **usage information** and then **additional resources.**\n",
    "1. Note the supported instance types.\n",
    "1. Next, click on **Continue to subscribe.**\n",
    "1. Review **End-user license agreement, support terms**, as well as **pricing information.**\n",
    "1. **\"Accept Offer\"** button needs to be clicked if your organization agrees with EULA, pricing information as well as support terms.  If **Continue to configuration** button is active, it means your account already has a subscription to this listing. Once you click on **Continue to configuration** button and then choose region, you will see that a Product Arn will appear. This is the model package ARN that you need to specify while creating a deployable model. However, for this notebook, the Model Package ARN has been specified in **src/model_package_arns.py** file and you do not need to specify the same explicitly.\n",
    "\n",
    "2. This notebook requires the IAM role associated with this notebook to have *AmazonSageMakerFullAccess* IAM permission.\n",
    "\n",
    "3. Note: If you want to run this notebook on AWS SageMaker Studio - please use Classic Jupyter mode to be able correctly render visualization. Pick instance type **'ml.t3.large'** or larger. Set kernel to **'conda_python3'**.\n",
    "\n",
    "    <img style=\"float: left;\" src=\"images/classicjupyter.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing Dependencies**\n",
    "\n",
    "In Step 1 of this notebook, you will use BertViz to visualize Transformer Attention.\n",
    "\n",
    "[BertViz](https://github.com/jessevig/bertviz) is an opensource package for visualizing attention in the Transformer models, including GPT-2. It provides the ability to visualize attention scores and contextual dependencies between tokens. You will take a closer look at the attention mechanism in the next section of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Installing BertViz for attention visualization\n",
    "import sys\n",
    "!rm -rf bertviz\n",
    "!git clone https://github.com/jessevig/bertviz.git\n",
    "!cd bertviz && {sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import base64\n",
    "import json \n",
    "import uuid\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from urllib.parse import urlparse\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from PIL import Image as ImageEdit\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import boto3\n",
    "from pprint import pprint\n",
    "from src.model_package_arns import ModelPackageArnProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing BertViz packages\n",
    "from bertviz.bertviz.transformers_neuron_view import GPT2Model, GPT2Tokenizer\n",
    "from bertviz.bertviz.neuron_view import show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize JavaScript dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
    "    jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Determine input prompt and visualize word dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 uses input text to set the initial context for further text generation. The length of an input text can range from few words to a maximum sequence length of 1024 tokens. The longer an initial input, the more subject context is provided to a model. Generally, longer inputs produce a more coherent text output.\n",
    "\n",
    "In this example, let's start with a simple sentence: **\"Machine learning is great for humanity. It helps\"**. There is only part of the second sentence provided intentionally to explore what options of continuation GPT-2 can generate for us and understand how a GPT-2 model can expand on this context while generating text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1 Introduction to attention\n",
    "\n",
    "[Self-Attention mechanism](https://arxiv.org/abs/1706.03762) is one of the key components for Transformers architectures, including GPT-2. It helps to relate different positions of a specific sequence of tokens in order to compute contextual representation of the sequence.\n",
    "\n",
    "The original paper ([Attention is all you need](https://arxiv.org/abs/1706.03762)) introduces Self-Attention mechanism as follows:\n",
    ">We call our particular attention \"Scaled Dot-Product Attention\". The input consists of queries and keys of dimension 𝑑𝑘, and values of dimension 𝑑𝑣. We compute the dot products of the query with all keys, divide each by 𝑑𝑘⎯⎯⎯⎯√, and apply a softmax function to obtain the weights on the values.\n",
    "\n",
    "$$                                                                         \n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V               \n",
    "$$   \n",
    "\n",
    "This notebook uses [BertViz](https://github.com/jessevig/bertviz) tool to quickly gain better understanding of practical meaning in addition to the mathematical definition of attention.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2 Specify input prompt and visualize attention mechanism\n",
    "\n",
    "You can experiment with different prompts and see what contextual dependencies exist in your own examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"input\": \"Machine learning is great for humanity. It helps\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this is how it will look for our initial input:\n",
    "\n",
    "<img style=\"float: left;margin:0px 10px\" src=\"images/example-input.png\">\n",
    "\n",
    "\n",
    "BertViz package uses the attention patterns produced by one or more attention heads in a given transformer layer to provide visualization of words (tokens) in a given text sequence.\n",
    "\n",
    "This example specifically uses a neuron view mode which visualizes the individual neurons in the query and key vectors and shows how they are used to compute attention.\n",
    "\n",
    "The darker blue color reflects a higher score and \"stronger\" attention connection between the selected token and other tokens in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that word \"helps\" is contextually related to \"Machine learning\" which is at the beginning of the previous sentence. It is also noticeable there is a connection to the word \"humanity\" despite it being in a separate sentence. \n",
    "This is just one representation of Layer #8 and Head #8 of a model. Other combinations of layers and heads can surface non-obvious relations between positions in a given sequence of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will render neuron view visualization. You will be able to point your mouse at the word (token) in your input on the left and it will show you dependencies with other words on the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = payload['input']\n",
    "model_type = 'gpt2'\n",
    "model_version = 'gpt2'\n",
    "model = GPT2Model.from_pretrained(model_version)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_version)\n",
    "show(model, model_type, tokenizer, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to change layer and head numbers on the widget and hover over to see different dependencies between tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention is one of the most important components of generative NLP models such as GPT, GPT2, GPT3. It makes it possible to achieve  State of The Art performance results in predicting the next word by the given context. \n",
    "\n",
    "Because of the auto-regressive nature of the text generation process, it is possible to generate long stretches of contextually coherent and (many times) close to \"written by human quality\" paragraphs of text.\n",
    "\n",
    "Note that \"words\" and \"tokens\" here are used interchangeably for simplicity. However, one word can consist of several tokens. In this case, attention will be rendered for each part of the word separately. [Byte_Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) improves the tokenization process in GPT2. It facilitates proper tokenization based on training dataset specifics.\n",
    "\n",
    "There are multiple layers and multiple attention heads in GPT-2 model architecture. Since the model has been already pre-trained on a large text corpus, different layers and different heads produce attention scores that reflect multiple semantic levels of connections between tokens.\n",
    "\n",
    "**Optional reading**\n",
    "\n",
    "To learn more about GPT architecture, attention, and how these types of visualizations work, please refer to these papers: \n",
    "- [Visualizing Attention in Transformer-Based Language Representation Models](https://arxiv.org/pdf/1904.02679.pdf)\n",
    "\n",
    "- [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - Original introduction of GPT architecture paper\n",
    "\n",
    "- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) - GPT2 architecture paper\n",
    "\n",
    "- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf) - GPT3 architecture paper\n",
    "\n",
    "- [Better Language Models\n",
    "and Their Implications](https://openai.com/blog/better-language-models) - [OpenAI](https://openai.com)'s original blog post about GPT2\n",
    "\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer) - More details about Transformers architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Use an ML model to generate text based on prompt\n",
    "\n",
    "Because you utilize [GPT-2 XL - Text generation](https://aws.amazon.com/marketplace/pp/prodview-cdujckyfypprg) algorithm from AWS Marketplace - all you need to do to start using it - is to deploy it as an inference endpoint in your account. Alternatively, we can use SageMaker Batch Transformation to run inference on batch payloads. \n",
    "\n",
    "To do that, let's set our payload variable as well as endpoint_name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sage.Session()\n",
    "bucket=sagemaker_session.default_bucket()\n",
    "endpoint_name='demo-gpt2-endpoint'\n",
    "client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to specify instance types for real-time inference endpoint as well as batch transformation.\n",
    "You can refer to compatible instance types for this AWS Marketplace offering [here](https://aws.amazon.com/marketplace/pp/prodview-cdujckyfypprg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_time_inference_instance_type='ml.m4.2xlarge'\n",
    "batch_transform_inference_instance_type='ml.m4.2xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_type='application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Specify model arn from AWS Marketplace subscription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use **ModelPackageArnProvider** class to make sure you get the correct ARN in every supported region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_arn = ModelPackageArnProvider.get_gpt2_model_package_arn(sagemaker_session.boto_region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2 Create model from model package and deploy to endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you will invoke **model.deploy** API call to create a real-time inference endpoint with GPT2-XL model.\n",
    "\n",
    "This step might take several minutes to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelPackage(role=role, model_package_arn=model_package_arn)\n",
    "predictor = model.deploy(1, real_time_inference_instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Explore use cases and model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 can be used by AI researchers and practitioners to better understand the behaviors, capabilities, biases, and constraints of large-scale generative language models.\n",
    "\n",
    "Also, it can be used in some non-research settings such as:\n",
    "* Writing assistance: Grammar assistance, autocompletion (for normal prose or code)\n",
    "* Creative writing and art: exploring the generation of creative, fictional texts; aiding the creation of poetry and other literary art.\n",
    "* Entertainment: Creation of games, chatbots, and amusing generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT-2 model package parameters:**\n",
    "\n",
    "You can tweak the following model parameters to influence the serving behavior of the model.\n",
    "\n",
    "* **input**: str (required): Input text\n",
    "* **length**: int = 50: The number of words to generate\n",
    "* **stop_token**: str: Stop text generation if it finds this word (token)\n",
    "* **num_return_sequences**: int = 1: Number of different sequences to generate. All sequences will start from the same input\n",
    "* **temperature**: float = 1.0: temperature of softmax - higher values increase creativity and decrease output coherence\n",
    "* **k**: int = 50: top-k sampling - model will choose from top k most probable words. Lower values eliminate less coherent words\n",
    "* **p**: float = 1.0: top-p nucleus sampling. Should be between 0 and 1 to activate. The alternative to top-k to select a minimum number of candidate words in which cumulative probability exceeds p. Values closer to 1.0 generally provide more coherent outputs.\n",
    "* **repetition_penalty**: float = 1.0: Higher value discourages model from repeating the same token\n",
    "* **seed**: int = None: Set this random seed before making a prediction\n",
    "* **test**: bool = False: If true it will return a sample test response no predictions will be made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1 Use case 1: Assisted writing of prose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first use case, you will see how GPT-2 can assist an author generate ideas for fiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for fiction\n",
    "payload = '{\"input\": \"Machine learning is great for humanity. It helps\", \"length\": 50,\\\n",
    "\"repetition_penalty\": 10,\"num_return_sequences\": 1}'\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=content_type,\n",
    "    Body=payload\n",
    "    )\n",
    "\n",
    "output=response[\"Body\"].read()\n",
    "\n",
    "pprint(json.loads(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output above, you will notice that one sequence is generated based on 'input' prompt since 'num_return_sequences' was set to '1'. The length of the sequence is roughly 50 words.\n",
    "\n",
    "Now, let's add an additional parameter, 'stop_token' to the request payload and see its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for fiction\n",
    "payload = '{\"input\": \"Machine learning is great for humanity. It helps build robots.\", \"length\": 50, \"stop_token\": \"robots\",\\\n",
    "\"repetition_penalty\": 10,\"num_return_sequences\": 1}'\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=content_type,\n",
    "    Body=payload\n",
    "    )\n",
    "\n",
    "output=response[\"Body\"].read()\n",
    "\n",
    "pprint(json.loads(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that no 'output' was generated because the 'stop_token' word 'robots' was included in the 'input'.\n",
    "\n",
    "Now, change the 'input' to not include the 'stop_token' and change the value of 'length' to 100. 'Repetition_penalty' is increased to 10 from default value of 1 to discourage model from generating repetitive text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for fiction\n",
    "payload = '{\"input\": \"Machine learning is great for humanity. It helps\", \"length\": 100, \"stop_token\": \"robots\",\\\n",
    "\"repetition_penalty\": 10,\"num_return_sequences\": 1}'\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=content_type,\n",
    "    Body=payload\n",
    "    )\n",
    "\n",
    "output=response[\"Body\"].read()\n",
    "\n",
    "pprint(json.loads(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'stop_token' parameter has no effect on the output above, since it is not included in the 'input'. The output is more verbose, because we changed the 'length' to 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2 Use case 2: Autonomous authoring of poem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second use case, you will see how GPT-2 can author a Shakespear-inspired poem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for poetry\n",
    "# Sample from Shakespeare's Macbeth\n",
    "payload = '{\"input\": \"The merciless Macdonwald—Worthy to be a rebel, for to that\\\n",
    "The multiplying villanies of nature\\\n",
    "Do swarm upon him—from the western isles\\\n",
    "Of kerns and gallowglasses is supplied;\\\n",
    "And fortune, on his damned quarrel smiling\", \"length\": 100, \"repetition_penalty\": 1,\\\n",
    "\"num_return_sequences\": 2, \"temperature\": 1, \"k\": 50}'\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=content_type,\n",
    "    Body=payload\n",
    "    )\n",
    "\n",
    "output=response[\"Body\"].read()\n",
    "\n",
    "pprint(json.loads(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'output' depicts a completely different style of writing, conditioned by the 'input'. This shows how GPT-2 adapts to 'input'. The 'output' now comprises of two sequences, since \"num_return_sequences\" was set to 2.\n",
    "\n",
    "Increase the values of 'temperature' to 10, and 'k' to 90. The effect should be more creative use of words and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for poetry\n",
    "# Sample from Shakespeare's Macbeth\n",
    "payload = '{\"input\": \"The merciless Macdonwald—Worthy to be a rebel, for to that\\\n",
    "The multiplying villanies of nature\\\n",
    "Do swarm upon him—from the western isles\\\n",
    "Of kerns and gallowglasses is supplied;\\\n",
    "And fortune, on his damned quarrel smiling\", \"length\": 100, \"repetition_penalty\": 1,\\\n",
    "\"num_return_sequences\": 2, \"temperature\": 10, \"k\": 90}'\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=content_type,\n",
    "    Body=payload\n",
    "    )\n",
    "\n",
    "output=response[\"Body\"].read()\n",
    "\n",
    "pprint(json.loads(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few things you can do to continue experimenting with this model\n",
    "1. Try different prompts representing other genres of writing such as science fiction, non-fiction, essay, novel, etc.\n",
    "2. Play with the parameter settings to see which generates novel, coherent ideas for writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3 Delete Amazon SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the endpoint when you no longer need it, to avoid getting charged for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name)\n",
    "sagemaker_session.delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Use Amazon SageMaker batch transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use batch transform to run inference when you don't need a persistent endpoint. You would need to specify the following arguments in the job:\n",
    "* Hardware specification (instance count and type).\n",
    "* strategy: determines how records should be batched into each prediction request within the batch transform job. For this job specify 'SingleRecord'. 'MultiRecord' may be used for batching inputs.\n",
    "* assemble_with: Which controls how predictions are output. 'None' does not perform any special processing, 'Line' places each prediction on its own line.\n",
    "* output_path: The S3 location for batch transform to be output. Note, file(s) will be named with '.out' suffixed to the input file(s) names. In this case, the output file will be 'input.json.out'. Note that in this case, multiple batch transform runs will overwrite existing files in the S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1 Create input file for batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'input.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $file_name\n",
    "\n",
    "{\"input\": \"The merciless Macdonwald—Worthy to be a rebel, for to that The multiplying villanies of nature Do swarm upon him—from the western isles Of kerns and gallowglasses is supplied; And fortune, on his damned quarrel smiling\", \"length\": 50, \"repetition_penalty\": 1,\"num_return_sequences\": 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2 Upload file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the file to S3\n",
    "transform_input = sagemaker_session.upload_data(file_name, key_prefix=endpoint_name) \n",
    "print(\"Transform input uploaded to \" + transform_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3 Execute the batch transform job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from the batch transform job will be saved to S3 bucket in the 'output_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run a batch-transform job\n",
    "transformer = model.transformer(instance_count=1,\n",
    "                                instance_type='ml.m4.2xlarge',\n",
    "                                strategy='SingleRecord',\n",
    "                                assemble_with='Line',\n",
    "                                output_path='s3://{}/{}/output'.format(bucket,endpoint_name))\n",
    "transformer.transform(transform_input, content_type=content_type)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output is available on following path\n",
    "transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.4 Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "parsed_url = urlparse(transformer.output_path)\n",
    "bucket_name = parsed_url.netloc\n",
    "file_key = '{}/{}.out'.format(parsed_url.path[1:], file_name.split(\"/\")[-1])\n",
    "print(file_key)\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "response = s3_client.get_object(Bucket = sagemaker_session.default_bucket(), Key = file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=response[\"Body\"].read()\n",
    "\n",
    "pprint(json.loads(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.5 Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1 Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are few other models from AWS Marketplace you can explore to see what generative AI/ML models can do.\n",
    "\n",
    "1. [GluonNLP Sentence Generator](https://aws.amazon.com/marketplace/pp/prodview-vajwpk2u7y5om). Pre-trained sequence sampler for sentence generation, powered by GluonNLP.\n",
    "2. [Mphasis Natural Language Sentence Generator](https://aws.amazon.com/marketplace/pp/prodview-yzp2pki67ipa4). The solution generates new text data from existing data using data augmentation at the sentence level.\n",
    "3. [Mphasis DeepInsights Text Paraphraser](https://aws.amazon.com/marketplace/pp/prodview-jtw7qhzepbjp2). The solution paraphrases textual data using Natural Language Processing techniques.\n",
    "4. [appen Lyrics Generator (CPU)](https://aws.amazon.com/marketplace/pp/prodview-qqzh5iao6si4c). This model will generate lyrics for your next Billboard-topping single."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2 Cancel AWS Marketplace subscription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you subscribed to AWS Marketplace model for an experiment and would like to unsubscribe, you can follow the steps below. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model.\n",
    "\n",
    "**Steps to unsubscribe from the product on AWS Marketplace:**\n",
    "\n",
    "Navigate to Machine Learning tab on Your [Software subscriptions page](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=lbr_tab_ml).\n",
    "Locate the listing that you would need to cancel, and click Cancel Subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
