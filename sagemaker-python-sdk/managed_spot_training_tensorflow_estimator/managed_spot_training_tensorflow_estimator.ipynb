{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using SageMaker Estimators on SageMaker Managed Spot Training\n",
    "\n",
    "The example here is almost the same as [Creating, training, and serving using SageMaker Estimators](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators/tensorflow_iris_dnn_classifier_using_estimators.ipynb).\n",
    "\n",
    "This notebook tackles the exact same problem with the same solution, but it has been modified to be able to run using SageMaker Managed Spot infrastructure. SageMaker Managed Spot uses [EC2 Spot Instances](https://aws.amazon.com/ec2/spot/) to run Training at a lower cost.\n",
    "\n",
    "Please read the original notebook and try it out to gain an understanding of the ML use-case and how it is being solved. We will not delve into that here in this notebook.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "This notebook uses the Iris dataset from the UCI Machine Learning Repository.\n",
    "\n",
    "> Iris Data Set [https://archive.ics.uci.edu/ml/datasets/iris].\n",
    "\n",
    "\n",
    "> Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "## Set up variables and define functions\n",
    "\n",
    "Again, we won't go into detail explaining the code below, it has been lifted verbatim from [Creating, training, and serving using SageMaker Estimators](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators/tensorflow_iris_dnn_classifier_using_estimators.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = Session().default_bucket()\n",
    "\n",
    "# Location to save your custom code in tar.gz format.\n",
    "custom_code_upload_location = \"s3://{}/customcode/tensorflow_iris\".format(bucket)\n",
    "\n",
    "# Location where results of model training are saved.\n",
    "model_artifacts_location = \"s3://{}/artifacts\".format(bucket)\n",
    "\n",
    "# IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "role = get_execution_role()\n",
    "\n",
    "\n",
    "def estimator(model_path, hyperparameters):\n",
    "    feature_columns = [tf.feature_column.numeric_column(INPUT_TENSOR_NAME, shape=[4])]\n",
    "    return tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[10, 20, 10],\n",
    "        n_classes=3,\n",
    "        model_dir=model_path,\n",
    "    )\n",
    "\n",
    "\n",
    "def estimator(model_path, hyperparameters):\n",
    "    feature_columns = [tf.feature_column.numeric_column(INPUT_TENSOR_NAME, shape=[4])]\n",
    "    return tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[10, 20, 10],\n",
    "        n_classes=3,\n",
    "        model_dir=model_path,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_input_fn(training_dir, hyperparameters):\n",
    "    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "        filename=os.path.join(training_dir, \"iris_training.csv\"),\n",
    "        target_dtype=np.int,\n",
    "        features_dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x={INPUT_TENSOR_NAME: np.array(training_set.data)},\n",
    "        y=np.array(training_set.target),\n",
    "        num_epochs=None,\n",
    "        shuffle=True,\n",
    "    )()\n",
    "\n",
    "\n",
    "def serving_input_fn(hyperparameters):\n",
    "    feature_spec = {INPUT_TENSOR_NAME: tf.FixedLenFeature(dtype=tf.float32, shape=[4])}\n",
    "    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\n",
    "\n",
    "\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managed Spot Training with a TensorFlow Estimator\n",
    "\n",
    "For Managed Spot Training using a TensorFlow Estimator we need to configure two things:\n",
    "1. Enable the `use_spot_instances` constructor arg - a simple self-explanatory boolean.\n",
    "2. Set the `max_wait` constructor arg - this is an int arg representing the amount of time you are willing to wait for Spot infrastructure to become available. Some instance types are harder to get at Spot prices and you may have to wait longer. You are not charged for time spent waiting for Spot infrastructure to become available, you're only charged for actual compute time spent once Spot instances have been successfully procured.\n",
    "\n",
    "Normally, a third requirement would also be necessary here - modifying your code to ensure a regular checkpointing cadence - however, TensorFlow Estimators already do this, so no changes are necessary here. Checkpointing is highly recommended for Manage Spot Training jobs due to the fact that Spot instances can be interrupted with short notice and using checkpoints to resume from the last interruption ensures you don't lose any progress made before the interruption.\n",
    "\n",
    "Feel free to toggle the `use_spot_instances` variable to see the effect of running the same job using regular (a.k.a. \"On Demand\") infrastructure.\n",
    "\n",
    "Note that `max_wait` can be set if and only if `use_spot_instances` is enabled and **must** be greater than or equal to `max_run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_spot_instances = True\n",
    "max_run = 3600\n",
    "max_wait = 7200 if use_spot_instances else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "iris_estimator = TensorFlow(\n",
    "    entry_point=\"iris_dnn_classifier.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.15.4\",\n",
    "    py_version=\"py3\",\n",
    "    output_path=model_artifacts_location,\n",
    "    code_location=custom_code_upload_location,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    training_steps=1000,\n",
    "    evaluation_steps=100,\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_run=max_run,\n",
    "    max_wait=max_wait,\n",
    ")\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\"sagemaker-sample-files\", \"datasets/tabular/iris/iris_train.csv\", \"iris_train.csv\")\n",
    "s3.download_file(\"sagemaker-sample-files\", \"datasets/tabular/iris/iris_test.csv\", \"iris_test.csv\")\n",
    "s3.upload_file(\"iris_train.csv\", bucket, \"DEMO-tensorflow-iris/iris_train.csv\")\n",
    "s3.upload_file(\"iris_test.csv\", bucket, \"DEMO-tensorflow-iris/iris_test.csv\")\n",
    "train_data_location = \"s3://{}/DEMO-tensorflow-iris/\".format(bucket)\n",
    "iris_estimator.fit(train_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Savings\n",
    "Towards the end of the job you should see two lines of output printed:\n",
    "\n",
    "- `Training seconds: X` : This is the actual compute-time your training job spent\n",
    "- `Billable seconds: Y` : This is the time you will be billed for after Spot discounting is applied.\n",
    "\n",
    "If you enabled the `use_spot_instances` var then you should see a notable difference between `X` and `Y` signifying the cost savings you will get for having chosen Managed Spot Training. This should be reflected in an additional line:\n",
    "- `Managed Spot Training savings: (1-Y/X)*100 %`\n",
    "\n",
    "For instance:\n",
    "\n",
    "> Training seconds: 42<br/>Billable seconds: 8<br/>Managed Spot Training savings: 81.0%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
