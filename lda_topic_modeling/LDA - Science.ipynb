{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Latent Dirichlet Allocation - Scientific Deep Dive\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Data Exploration](#DataExploration)\n",
    "1. [Training](#Training)\n",
    "1. [Inference](#Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "\n",
    "Amazon SageMaker LDA is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. Latent Dirichlet Allocation (LDA) is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LDA Model\n",
    "\n",
    "As mentioned above, LDA is a model for discovering latent topics describing a collection of documents. In this section we will give a brief introduction to the model. Let,\n",
    "\n",
    "* $M$ = the number of *documents* in a corpus\n",
    "* $N$ = the average *length* of a document.\n",
    "* $V$ = the size of the *vocabulary*; the total number of unique words\n",
    "\n",
    "We denote a *document* by a vector $w \\in \\mathbb{R}^V$ where $w_i$ equals the number of times the $i$th word in the vocabulary occurs within the document. This is called the \"bag-of-words\" format of representing a document. The *length* of a document is equal to the total number of words in the document: $N_w = \\sum_{i=1}^V w_i$.\n",
    "\n",
    "An LDA model is defined by two parameters: a topic-word distribution matrix $\\beta \\in \\mathbb{R}^{K \\times V}$ and a  Dirichlet topic prior $\\alpha \\in \\mathbb{R}^K$. In particular, let,\n",
    "\n",
    "$$\\beta = \\left[ \\beta_1, \\ldots, \\beta_K \\right]$$\n",
    "\n",
    "be a collection of $K$ *topics* where each topic $\\beta_k \\in \\mathbb{R}^V$ is represented as probability distribution over the vocabulary. One of the utilities of the LDA model is that a given word is allowed to appear in multiple topics with positive probability. The Dirichlet topic prior is a vector $\\alpha \\in \\mathbb{R}^K$ such that $\\alpha_k > 0$ for all $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "---\n",
    "\n",
    "## An Example Dataset\n",
    "\n",
    "Before explaining further let's get our hands dirty with an example dataset. The following synthetic data comes from [1] and comes with a very useful visual interpretation.\n",
    "\n",
    "> [1] Thomas Griffiths and Mark Steyvers. *Finding Scientific Topics.* Proceedings of the National Academy of Science, 101(suppl 1):5228-5235, 2004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from generate_example_data import generate_griffiths_data\n",
    "\n",
    "num_documents = 10000\n",
    "known_alpha, known_beta, documents, topic_mixtures = generate_griffiths_data(\n",
    "    num_documents=num_documents, num_topics=10)\n",
    "num_topics, vocabulary_size = known_beta.shape\n",
    "\n",
    "\n",
    "# reserve a holdout set of documents and topic mixtures for testing purposes\n",
    "num_training = int(0.8*num_documents)\n",
    "documents_test = documents[num_training:]\n",
    "documents = documents[:num_training]\n",
    "\n",
    "topic_mixtures_test = topic_mixtures[num_training:]\n",
    "topix_mixtures = topic_mixtures[:num_training]\n",
    "\n",
    "num_documents_test = len(documents_test)\n",
    "num_documents = len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect these data. Starting with the documents, note that the vocabulary size is equal to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('first document = {}'.format(documents[0]))\n",
    "print('\\nlength of first document = {}'.format(documents[0].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigate the topic-word probability matrix, $\\beta$. Let's look at the first topic and verify that it is a probability distribution on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('first topic = {}'.format(known_beta[0]))\n",
    "\n",
    "print('\\nbeta shape: (num_topics, vocabulary_size) = {}'.format(known_beta.shape))\n",
    "print('\\nsum of elements of first topic = {}'.format(known_beta[0].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human beings are visual creatures. Lucky for us, this example LDA dataset has a natural visualization. We reshape each topic-word distribution to a 5x5 pixel image. Each pixel represents a word from the 25-word-long vocabulary and the color represents the frequency of occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_example_data import plot_lda\n",
    "\n",
    "fig = plot_lda(known_beta, nrows=1, ncols=10)\n",
    "fig.suptitle(r'Known $\\beta$ - Topic-Word Probability Distributions')\n",
    "fig.set_dpi(160)\n",
    "fig.set_figheight(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's inspect some documents in this visual format. In these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_lda(documents[:12], nrows=3, ncols=4, cmap='gray_r')\n",
    "fig.suptitle(r'$w$ - Sample Document Word Counts')\n",
    "fig.set_dpi(160)\n",
    "fig.set_figheight(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Documents\n",
    "\n",
    "LDA is a generative model, meaning that the LDA parameters $(\\alpha, \\beta)$ can be used to construct documents word-by-word by drawing from the topics. In fact, looking closely at the example documents above you can see that some documents sample more words from some topics than from others.\n",
    "\n",
    "LDA works as follows: given $M$ documents $w^{(1)}, w^{(2)}, \\ldots, w^{(M)}$, an average document length of $N$, and an LDA model $(\\alpha, \\beta)$,\n",
    "\n",
    "**For** each document $m$:\n",
    "* sample a topic mixture: $\\theta^{(m)} \\sim \\text{Dirichlet}(\\alpha)$\n",
    "* **For** each word $n$ in the document:\n",
    "  * Sample a topic $z_n^{(m)} \\sim \\text{Multinomial}\\big( \\theta^{(m)} \\big)$\n",
    "  * Sample a word from this topic, $w_n^{(m)} \\sim \\text{Multinomial}\\big( \\beta_{z_n^{(m)}} \\; \\big)$\n",
    "  * Add to document\n",
    "\n",
    "The [plate notation](https://en.wikipedia.org/wiki/Plate_notation) for the LDA model, introduced in [2], encapsulates this process pictorially.\n",
    "\n",
    "![](http://scikit-learn.org/stable/_images/lda_model_graph.png)\n",
    "\n",
    "> [2] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3(Jan):993â€“1022, 2003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Mixtures\n",
    "\n",
    "For the documents we generated above lets look at their corresponding topic mixtures, $\\theta \\in \\mathbb{R}^K$. The topic mixtures represent the probablility that a given word of the document is sampled from a particular topic. The objective of inference, also known as scoring, is to determine the most likely topic mixture of a given input document.\n",
    "\n",
    "Since we generated these example documents using the LDA model we know the topic mixture generating them. Let's examine these topic mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('first document =\\n{}'.format(documents[0]))\n",
    "print('\\nlength of first document = {}'.format(documents[0].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('first document topic mixture =\\n{}'.format(topic_mixtures[0]))\n",
    "print('\\nsum(theta) = {}'.format(topic_mixtures[0].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(documents[0].reshape(5,5), cmap='gray_r')\n",
    "plt.title(r'$w$ - Sample Document', fontsize=20)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "\n",
    "plt.matshow(topic_mixtures[0].reshape(1,-1), cmap='Reds', vmin=0, vmax=1)\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.title(r'$\\theta$ - Sample Topic Mixture', fontsize=20)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "fig = plot_lda(known_beta, nrows=1, ncols=10)\n",
    "fig.suptitle(r'Known $\\beta$ - Topic-Word Probability Distributions')\n",
    "fig.set_dpi(160)\n",
    "fig.set_figheight(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that the ***first*** and ***third*** topics are most represented in this document. These correspond to the first and third \"column topics\". Looking at the document, itself, it seems to be the case as the word count is noticably larger in these two columns.\n",
    "\n",
    "We plot the first few sample documents $w \\in \\mathbb{R}^V$ along with their corresponding topic mixtures $\\theta \\in \\mathbb{R}^K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
    "\n",
    "def plot_document_with_topic(fig, gsi, index, topic_mixtures=None,\n",
    "                             vmin=0, vmax=32):\n",
    "    ax_doc = fig.add_subplot(gsi[:5,:])\n",
    "    ax_doc.matshow(documents[index].reshape(5,5), cmap='gray_r',\n",
    "                   vmin=vmin, vmax=vmax)\n",
    "    ax_doc.set_xticks([])\n",
    "    ax_doc.set_yticks([])\n",
    "\n",
    "    if topic_mixtures is not None:\n",
    "        ax_topic = plt.subplot(gsi[-1,:])\n",
    "        ax_topic.matshow(topic_mixtures[index].reshape(1,-1), cmap='Reds',\n",
    "                         vmin=0, vmax=1)\n",
    "        ax_topic.set_xticks([])\n",
    "        ax_topic.set_yticks([])\n",
    "\n",
    "def plot_lda_topics(documents, nrows, ncols, with_colorbar=True,\n",
    "                    topic_mixtures=None, cmap='Viridis', dpi=160):\n",
    "    fig = plt.figure()\n",
    "    gs = GridSpec(nrows, ncols)\n",
    "    \n",
    "    vmin, vmax = (0, documents.max())\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            index = i*ncols + j\n",
    "            gsi = GridSpecFromSubplotSpec(6, 5, subplot_spec=gs[i,j])\n",
    "            plot_document_with_topic(fig, gsi, index, topic_mixtures=topic_mixtures,\n",
    "                                     vmin=vmin, vmax=vmax)\n",
    "            \n",
    "    return fig\n",
    "        \n",
    "    \n",
    "# plot the documents with their topic mixtures\n",
    "fig = plot_lda_topics(documents, 3, 4, topic_mixtures=topic_mixtures)\n",
    "fig.suptitle(r'$(w,\\theta)$ - Sample Document Word Counts and Topic Mixtures')\n",
    "fig.set_dpi(160)\n",
    "\n",
    "# plot the known probability distributions (for reference)\n",
    "fig = plot_lda(known_beta, nrows=1, ncols=10)\n",
    "fig.suptitle(r'Known $\\beta$ - Topic-Word Probability Distributions')\n",
    "fig.set_dpi(160)\n",
    "fig.set_figheight(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "---\n",
    "\n",
    "In this section we will give some insight into how AWS SageMaker LDA fits an LDA model to a corpus, create an run a SageMaker LDA training job, and examine the output trained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Estimation using Tensor Decompositions\n",
    "\n",
    "Given a document corpus, Amazon SageMaker LDA uses a spectral tensor decomposition technique to determine the LDA model $(\\alpha, \\beta)$ which most likely describes the corpus. See [1] for a primary reference of the theory behind the algorithm. The spectral decomposition, itself, is computed using the CPDecomp algorithm described in [2].\n",
    "\n",
    "The overall idea is the following: given a corpus of documents $\\mathcal{W} = \\{w^{(1)}, \\ldots, w^{(M)}\\}, \\; w^{(m)} \\in \\mathbb{R}^V,$ we construct a statistic tensor,\n",
    "\n",
    "$$T \\in \\bigotimes^3 \\mathbb{R}^V$$\n",
    "\n",
    "such that the spectral decomposition of the tensor is approximately the LDA parameters $\\alpha \\in \\mathbb{R}^K$ and $\\beta \\in \\mathbb{R}^{K \\times V}$ which maximize the likelihood of observing the corpus for a given number of topics, $K$,\n",
    "\n",
    "$$T \\approx \\sum_{k=1}^K \\alpha_k \\; (\\beta_k \\otimes \\beta_k \\otimes \\beta_k)$$\n",
    "\n",
    "This statistic tensor encapsulates information from the corpus such as the document mean, cross correlation, and higher order statistics. For details, see [1].\n",
    "\n",
    "\n",
    "> [1] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham Kakade, and Matus Telgarsky. *\"Tensor Decompositions for Learning Latent Variable Models\"*, Journal of Machine Learning Research, 15:2773â€“2832, 2014.\n",
    ">\n",
    "> [2] Tamara Kolda and Brett Bader. *\"Tensor Decompositions and Applications\"*. SIAM Review, 51(3):455â€“500, 2009.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Running a Training Job\n",
    "\n",
    "To run this job all you need to provide is an AWS S3 bucket to store the training input and output along with an access role for SageMaker to access this bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_role = 'arn:aws:iam: <<<PROVIDE ACCESS ROLE>>>'\n",
    "bucket = '<<<PROVIDE BUCKET>>>'\n",
    "\n",
    "\n",
    "s3_access_role = 'arn:aws:iam::874786414999:role/ease-access-role'\n",
    "bucket = 'lda-notebook-example'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to RecordIO Protobuf Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from mxnet.recordio import MXRecordIO\n",
    "from record_pb2 import Record\n",
    "\n",
    "def save_documents(fname, documents):\n",
    "    \"\"\"Saves a Numpy array of documents to RecordIO Protobuf format.\"\"\"\n",
    "    feature_size = documents.shape[1]\n",
    "    \n",
    "    # convert to protobuf\n",
    "    protobuf = [\n",
    "        list_to_record_bytes(\n",
    "            document.astype(np.float32).tolist(),\n",
    "            feature_size=feature_size)\n",
    "        for document in documents\n",
    "    ]\n",
    "\n",
    "    # write to recordio\n",
    "    recordio = MXRecordIO(fname, \"w\")\n",
    "    for datum in protobuf:\n",
    "        recordio.write(datum)\n",
    "    recordio.close()\n",
    "    \n",
    "\n",
    "def list_to_record_bytes(values, keys=None, label=None, feature_size=None):\n",
    "    \"\"\"Takes a list and returns a serialized bytestring (using the vector/record representation)\"\"\"\n",
    "    record = Record()\n",
    "    record.features['values'].float32_tensor.values.extend(values)\n",
    " \n",
    "    if keys is not None:\n",
    "        if feature_size is None:\n",
    "            raise ValueError(\"For sparse tensors the feature size must be specified.\")\n",
    "        record.features['values'].float32_tensor.keys.extend(keys)\n",
    "\n",
    "    if feature_size is not None:\n",
    "        record.features['values'].float32_tensor.shape.extend([feature_size])\n",
    " \n",
    "    if label is not None:\n",
    "        record.label['values'].float32_tensor.values.extend([label])\n",
    "        \n",
    "    return record.SerializeToString()\n",
    "\n",
    "    \n",
    "def libsvm_record_converter(label, keys, values, feature_size=None):\n",
    "    record = Record()\n",
    "    record.features['values'].float32_tensor.values.extend(values)\n",
    " \n",
    "    if keys is not None:\n",
    "        if feature_size is None:\n",
    "            raise ValueError(\"For sparse tensors the feature size must be specified.\")\n",
    "        record.features['values'].float32_tensor.keys.extend(keys)\n",
    "\n",
    "    if feature_size is not None:\n",
    "        record.features['values'].float32_tensor.shape.extend([feature_size])\n",
    " \n",
    "    if label is not None:\n",
    "        record.label['values'].float32_tensor.values.extend([label])\n",
    " \n",
    "    return record\n",
    "\n",
    "\n",
    "fname = 'data.pbr'\n",
    "save_documents(fname, documents)\n",
    "key = 'lda-science-notebook/training/' + fname\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_file(fname)\n",
    "print('Uploaded document data \"{}\" to \"{}/{}\"'.format(fname, bucket, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters\n",
    "\n",
    "* `num_topics = 10` - In this example we know a priori that the training corpus was generated by ten topics. Let's see if we can recover these topics.\n",
    "* `alpha0 = 1.0` - Some documents have \"decent\" mixing whereas others are singularly represented by a topic. Setting `alpha0` to 1 tells Amazon SageMaker LDA to not favor sparse or dense topic mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "job_name_prefix = 'lda-science-notebook'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "training_image = '462891221994.dkr.ecr.us-west-2.amazonaws.com/lda:1'\n",
    "\n",
    "training_params = {\n",
    "    'AlgorithmSpecification': {\n",
    "        'TrainingImage': training_image,\n",
    "        'TrainingInputMode': 'File',\n",
    "    },\n",
    "    'HyperParameters': {\n",
    "        'num_topics': str(10),\n",
    "        'feature_dim': str(25),\n",
    "        'mini_batch_size': str(len(documents)),\n",
    "        'alpha0': str(1.0),\n",
    "    },\n",
    "    'InputDataConfig': [\n",
    "        {\n",
    "            'ChannelName': 'train',\n",
    "            'CompressionType': 'None',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': 's3://{}/{}/training/'.format(bucket, job_name_prefix),\n",
    "                    'S3DataDistributionType': 'FullyReplicated',\n",
    "                }\n",
    "            },\n",
    "            'RecordWrapperType': 'None',\n",
    "        }\n",
    "    ],\n",
    "    'OutputDataConfig': {\n",
    "        'S3OutputPath': 's3://{}/{}/output'.format(bucket, job_name_prefix),\n",
    "    },\n",
    "    'ResourceConfig': {\n",
    "        'InstanceCount': 1,\n",
    "        'InstanceType': 'ml.c4.2xlarge',\n",
    "        'VolumeSizeInGB': 50,\n",
    "    },\n",
    "    'RoleArn': s3_access_role,\n",
    "    'StoppingCondition': {\n",
    "        'MaxRuntimeInSeconds': 60*60,\n",
    "    },\n",
    "    'TrainingJobName': job_name,\n",
    "}\n",
    "\n",
    "\n",
    "print('Training job name: {}'.format(job_name))\n",
    "print('\\nInput Data Location: {}'.format(training_params['InputDataConfig'][0]['DataSource']['S3DataSource']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training job\n",
    "sagemaker = boto3.client(service_name='sagemaker')\n",
    "sagemaker.create_training_job(**training_params)\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n",
    "\n",
    "\n",
    "# wait for the job to finish and report the ending status\n",
    "sagemaker.get_waiter('TrainingJob_Created').wait(TrainingJobName=job_name)\n",
    "training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "status = training_info['TrainingJobStatus']\n",
    "print(\"Training job ended with status: \" + status)\n",
    "if status == 'Failed':\n",
    "    message = sagemaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))\n",
    "    raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Trained Model\n",
    "\n",
    "Let's compare the trained model to the known one used to generate the document corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path, tarfile\n",
    "import mxnet as mx\n",
    "\n",
    "# download and extract the model file from S3\n",
    "#\n",
    "model_fname = 'model.tar.gz'\n",
    "model_key = os.path.join('lda-science-notebook', 'output', job_name, 'output', model_fname)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(model_key).download_file(fname)\n",
    "print('Downloaded model tarball {}'.format(model_key))\n",
    "\n",
    "with tarfile.open(fname) as tar:\n",
    "    tar.extractall()\n",
    "print('Extracted model tarball')\n",
    "\n",
    "model_list = [\n",
    "    fname for fname in os.listdir('.')\n",
    "    if fname.startswith('model_')\n",
    "]\n",
    "model_fname = model_list[0]\n",
    "print('Found model file: {}'.format(model_fname))\n",
    "\n",
    "\n",
    "# get the model from the model file and store in Numpy arrays\n",
    "#\n",
    "alpha, beta = mx.ndarray.load(model_fname)\n",
    "found_alpha_permuted = alpha.asnumpy()\n",
    "found_beta_permuted = beta.asnumpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presumably, SageMaker LDA has found the topics most likely used to generate the training corpus. However, even if this is case the topics would not be returned in any particular order. Therefore, we match the found topics to the known topics closest in L1-norm in order to find the topic permutation.\n",
    "\n",
    "Note that we will use the `permutation` later during inference to match known topic mixtures to found topic mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_example_data import match_estimated_topics\n",
    "\n",
    "permutation, found_beta = match_estimated_topics(known_beta, found_beta_permuted)\n",
    "found_alpha = found_alpha_permuted[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the known topic-word probability distribution, $\\beta \\in \\mathbb{R}^{K \\times V}$ next to the distribution found by the SageMaker LDA as well as the L1-norm errors between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_lda(np.vstack([known_beta, found_beta]), 2, 10)\n",
    "fig.set_dpi(160)\n",
    "fig.suptitle('Known vs. Found Topic-Word Probability Distributions')\n",
    "fig.set_figheight(3)\n",
    "\n",
    "beta_error = np.linalg.norm(known_beta - found_beta, 1)\n",
    "alpha_error = np.linalg.norm(known_alpha - found_alpha, 1)\n",
    "print('L1-error (beta) = {}'.format(beta_error))\n",
    "print('L1-error (alpha) = {}'.format(alpha_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "With a trained model in hand we will now perform inference on the input training document to recover their topic mixtures. We'll compare the inferred mixtures to those known from generating the example data.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We first create a SageMaker Model, SageMaker Endpoint Configuration, and SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, time\n",
    "\n",
    "# get the location of the model generated by the above training job\n",
    "model_name = job_name\n",
    "model_data = training_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "model_params = {\n",
    "    'ExecutionRoleArn': s3_access_role,\n",
    "    'ModelName': model_name,\n",
    "    'PrimaryContainer': {\n",
    "        'Image': training_image,\n",
    "        'ModelDataUrl': model_data,\n",
    "    },\n",
    "}\n",
    "\n",
    "model_response = sagemaker.create_model(**model_params)\n",
    "print('Model name: {}'.format(model_name))\n",
    "print('ModelArn:   {}\\n'.format(model_response['ModelArn']))\n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = job_name_prefix + '-endpoint-config' + timestamp\n",
    "endpoint_config_params = {\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "    'ProductionVariants': [\n",
    "        {\n",
    "            'InstanceType': 'ml.c4.xlarge',\n",
    "            'InitialInstanceCount': 1,\n",
    "            'ModelName': model_name,\n",
    "            'VariantName': 'AllTraffic'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "endpoint_config_response = sagemaker.create_endpoint_config(**endpoint_config_params)\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = job_name_prefix + '-endpoint' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "\n",
    "endpoint_response = sagemaker.create_endpoint(**endpoint_params)\n",
    "response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "\n",
    "sagemaker.get_waiter('Endpoint_Created').wait(EndpointName=endpoint_name)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))\n",
    "\n",
    "# print the final status of the endpoint\n",
    "endpoint_response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke Inference on Training Data\n",
    "\n",
    "We infer the topic mixture $\\theta \\in \\mathbb{R}^K$ from a set of input documents $w \\in \\mathbb{R}^V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, json\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "lda_runtime = boto3.client('sagemaker-runtime')\n",
    "def np2csv(arr):\n",
    "    csv = io.BytesIO()\n",
    "    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n",
    "    return csv.getvalue().decode().rstrip()\n",
    "\n",
    "payload = np2csv(documents[:12])\n",
    "invoke_endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'ContentType': 'text/csv',\n",
    "    'Body': payload,\n",
    "}\n",
    "response = lda_runtime.invoke_endpoint(**invoke_endpoint_params)\n",
    "results = json.loads(response['Body'].read().decode())\n",
    "\n",
    "inferred_topic_mixtures_permuted = np.array([prediction['topic_mixture'] for prediction in results['predictions']])\n",
    "print('Computed inferred topic mixtures (permuted)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we knew the topics a priori we were able to match known topics to found topics. To more easily compare known topic mixtures to found topic mixtures we apply the same permutation to the results, here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_topic_mixtures = inferred_topic_mixtures_permuted[:,permutation]\n",
    "\n",
    "print('Known topic mixture:\\n{}'.format(topic_mixtures[0]))\n",
    "print('Found topic mixture:\\n{}'.format(inferred_topic_mixtures[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 0.4\n",
    "x = np.arange(10)\n",
    "\n",
    "nrows, ncols = 3, 4\n",
    "fig, ax = plt.subplots(nrows, ncols)\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        index = i*ncols + j\n",
    "        ax[i,j].bar(x, topic_mixtures[index], width, color='C0')\n",
    "        ax[i,j].bar(x+width, inferred_topic_mixtures[index], width, color='C1')\n",
    "        ax[i,j].set_xticks(range(num_topics))\n",
    "        ax[i,j].set_xticklabels([])\n",
    "        ax[i,j].set_yticks(np.linspace(0,1,10))\n",
    "        ax[i,j].set_yticklabels([])\n",
    "        \n",
    "fig.suptitle('Known vs. Inferred Topic Mixtures (Training Data)')\n",
    "fig.set_dpi(160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benefits of the LDA model as opposed to, say, [Probabilistic latent semantic indexing (pLSI)](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis) is that we can infer topic mixtures of documents outside the training corpus. Let's also apply inference to the first few test documents and compare with their known topic mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's measure the inference error across the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop / Close the Endpoint\n",
    "\n",
    "Finally, we should delete the endpoint before we close the notebook.\n",
    "\n",
    "To restart the endpoint you can follow the code above using the same `endpoint_name` we created or you can navigate to the \"Endpoints\" tab in the SageMaker console, select the endpoint with the name stored in the variable `endpoint_name`, and select \"Delete\" from the \"Actions\" dropdown menu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
