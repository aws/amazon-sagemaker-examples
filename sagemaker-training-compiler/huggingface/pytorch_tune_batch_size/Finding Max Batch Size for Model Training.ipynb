{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd15c8e",
   "metadata": {},
   "source": [
    "# SageMaker Training Compiler - Finding Max Batch Size for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1cb84",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "3. [Finding max batch size](#Finding-max-batch-size)\n",
    "    1. [Model and instance type specifications](#Model-and-instance-type-specifications)  \n",
    "    2. [Finding max batch size for SageMaker Training Compiler with Hugging Face and PyTorch](#Finding-max-batch-size-for-SageMaker-Training-Compiler-with-Hugging-Face-and-PyTorch)\n",
    "    3. [Wait for find max batch job to complete](#Wait-for-find-max-batch-job-to-complete)\n",
    "4. [Results](#Results)  \n",
    "    1. [Load logs for find max batch job](#Load-logs-for-find-max-batch-job)  \n",
    "5. [Clean up](#Clean-up) \n",
    "6. [Conclusion](#Conclusion) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c125ff",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The SageMaker Training Compiler allows AWS customers to train deep learning models faster on scalable GPU instances managed by SageMaker. The memory optimizations made by SageMaker Training Compiler typically allow for your training job to fit more data into GPU memory. By increasing the batch size as much as possible in your training job, you can speed up your training jobs even further.\n",
    "\n",
    "For example, for a PyTorch fine-tuning job (Sequence_Len=512, Automatic Mixed Precision (AMP)) with a GPT-2 model from Hugging Face, the maximum batch size that can fit on an ml.p3.2xlarge instance increased from 6 to 20 with the Training Compiler enabled. A list of model examples and maximum batch sizes is available in the SageMaker Training Compiler documentation under \"Tested Models\": https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html\n",
    "\n",
    "The goal of this Notebook is to give you an example of how you can find the max batch size for a particular model and instance type. We show you how to find the max batch size for a gpt2 model below running on an `ml.p3.8xlarge` instance. You can customize this Notebook to fit your use case, and use the resulting max batch size as the value of your batch size parameter in your full training job.\n",
    "\n",
    "The Notebook uses the HuggingFace training scripts (`run_mlm.py` and `run_clm.py`) and a hands-on script (`find_max_batch_size.py`) to iteratively search for the maximum batch for a given GPU instance. \n",
    "\n",
    "This Notebook runs the `run_clm.py` by default, as will be shown in the following sections. If you want to test with your own training script, you need to update the following:\n",
    "- The `find_max_batch_size.py` script - In line 23 to 28 of the script, specify the right directory path and the file name of your training script.\n",
    "- `hyperparameters` - In the following Tune a Native PyTorch Training Job section, modify the hyperparameters that your training script requires accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e33ba",
   "metadata": {},
   "source": [
    "## Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4801c",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "This example notebook requires the **SageMaker Python SDK v2.70.0** and **transformers v4.11.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465feea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==2.70.0\n",
      "  Using cached sagemaker-2.70.0-py2.py3-none-any.whl\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting protobuf>=3.1\n",
      "  Using cached protobuf-3.19.4-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting importlib-metadata>=1.4.0\n",
      "  Using cached importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Collecting pathos\n",
      "  Using cached pathos-0.2.8-py2.py3-none-any.whl (81 kB)\n",
      "Collecting attrs\n",
      "  Using cached attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "Collecting protobuf3-to-dict>=0.1.5\n",
      "  Using cached protobuf3_to_dict-0.1.5-py3-none-any.whl\n",
      "Collecting numpy>=1.9.0\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting boto3>=1.20.18\n",
      "  Downloading boto3-1.21.7-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smdebug-rulesconfig==1.0.1\n",
      "  Using cached smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Collecting google-pasta\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 8.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.25.0,>=1.24.7\n",
      "  Downloading botocore-1.24.7-py3-none-any.whl (8.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.6 MB 58.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Using cached urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting typing-extensions>=3.6.4\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Using cached pyparsing-3.0.7-py3-none-any.whl (98 kB)\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Collecting multiprocess>=0.70.12\n",
      "  Using cached multiprocess-0.70.12.2-py36-none-any.whl (106 kB)\n",
      "Collecting pox>=0.3.0\n",
      "  Using cached pox-0.3.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting dill>=0.3.4\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting ppft>=1.6.6.4\n",
      "  Using cached ppft-1.6.6.4-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: six, urllib3, python-dateutil, jmespath, dill, botocore, zipp, typing-extensions, s3transfer, pytz, pyparsing, protobuf, ppft, pox, numpy, multiprocess, smdebug-rulesconfig, protobuf3-to-dict, pathos, pandas, packaging, importlib-metadata, google-pasta, boto3, attrs, sagemaker\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.7\n",
      "    Uninstalling urllib3-1.26.7:\n",
      "      Successfully uninstalled urllib3-1.26.7\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 0.10.0\n",
      "    Uninstalling jmespath-0.10.0:\n",
      "      Successfully uninstalled jmespath-0.10.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.23.21\n",
      "    Uninstalling botocore-1.23.21:\n",
      "      Successfully uninstalled botocore-1.23.21\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.6.0\n",
      "    Uninstalling zipp-3.6.0:\n",
      "      Successfully uninstalled zipp-3.6.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.0.1\n",
      "    Uninstalling typing-extensions-4.0.1:\n",
      "      Successfully uninstalled typing-extensions-4.0.1\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.5.0\n",
      "    Uninstalling s3transfer-0.5.0:\n",
      "      Successfully uninstalled s3transfer-0.5.0\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2021.3\n",
      "    Uninstalling pytz-2021.3:\n",
      "      Successfully uninstalled pytz-2021.3\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.6\n",
      "    Uninstalling pyparsing-3.0.6:\n",
      "      Successfully uninstalled pyparsing-3.0.6\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "  Attempting uninstall: ppft\n",
      "    Found existing installation: ppft 1.6.6.4\n",
      "    Uninstalling ppft-1.6.6.4:\n",
      "      Successfully uninstalled ppft-1.6.6.4\n",
      "  Attempting uninstall: pox\n",
      "    Found existing installation: pox 0.3.0\n",
      "    Uninstalling pox-0.3.0:\n",
      "      Successfully uninstalled pox-0.3.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.12.2\n",
      "    Uninstalling multiprocess-0.70.12.2:\n",
      "      Successfully uninstalled multiprocess-0.70.12.2\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "  Attempting uninstall: protobuf3-to-dict\n",
      "    Found existing installation: protobuf3-to-dict 0.1.5\n",
      "    Uninstalling protobuf3-to-dict-0.1.5:\n",
      "      Successfully uninstalled protobuf3-to-dict-0.1.5\n",
      "  Attempting uninstall: pathos\n",
      "    Found existing installation: pathos 0.2.8\n",
      "    Uninstalling pathos-0.2.8:\n",
      "      Successfully uninstalled pathos-0.2.8\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.5\n",
      "    Uninstalling pandas-1.1.5:\n",
      "      Successfully uninstalled pandas-1.1.5\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.8.2\n",
      "    Uninstalling importlib-metadata-4.8.2:\n",
      "      Successfully uninstalled importlib-metadata-4.8.2\n",
      "  Attempting uninstall: google-pasta\n",
      "    Found existing installation: google-pasta 0.2.0\n",
      "    Uninstalling google-pasta-0.2.0:\n",
      "      Successfully uninstalled google-pasta-0.2.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.20.21\n",
      "    Uninstalling boto3-1.20.21:\n",
      "      Successfully uninstalled boto3-1.20.21\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.2.0\n",
      "    Uninstalling attrs-21.2.0:\n",
      "      Successfully uninstalled attrs-21.2.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.70.0\n",
      "    Uninstalling sagemaker-2.70.0:\n",
      "      Successfully uninstalled sagemaker-2.70.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\n",
      "thinc 8.0.10 requires typing-extensions<4.0.0.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.1.1 which is incompatible.\n",
      "spacy 3.0.6 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.8.2 which is incompatible.\n",
      "spacy 3.0.6 requires typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.1.1 which is incompatible.\n",
      "awscli 1.22.21 requires botocore==1.23.21, but you have botocore 1.24.7 which is incompatible.\n",
      "aiobotocore 1.3.0 requires botocore<1.20.50,>=1.20.49, but you have botocore 1.24.7 which is incompatible.\u001b[0m\n",
      "Successfully installed attrs-21.4.0 boto3-1.21.7 botocore-1.24.7 dill-0.3.4 google-pasta-0.2.0 importlib-metadata-4.8.3 jmespath-0.10.0 multiprocess-0.70.12.2 numpy-1.19.5 packaging-21.3 pandas-1.1.5 pathos-0.2.8 pox-0.3.0 ppft-1.6.6.4 protobuf-3.19.4 protobuf3-to-dict-0.1.5 pyparsing-3.0.7 python-dateutil-2.8.2 pytz-2021.3 s3transfer-0.5.2 sagemaker-2.70.0 six-1.16.0 smdebug-rulesconfig-1.0.1 typing-extensions-4.1.1 urllib3-1.26.8 zipp-3.6.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall sagemaker==2.70.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c266b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.11.0)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (0.0.46)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (2021.4.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (4.8.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (0.2.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (2.26.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (0.10.3)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (0.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.11.0) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.0) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers==4.11.0) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.11.0) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.11.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.11.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.11.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.11.0) (1.26.8)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.11.0) (8.0.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.11.0) (1.0.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.11.0) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3651b473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker: 2.70.0\n",
      "transformers: 4.11.0\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import sagemaker\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"sagemaker: {sagemaker.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7598fb",
   "metadata": {},
   "source": [
    "### Development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5efa7ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::470831022120:role/service-role/AWSDeepRacerSageMakerAccessRole\n",
      "sagemaker bucket: sagemaker-us-west-2-470831022120\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# SageMaker session bucket -> used for uploading data, models and logs\n",
    "# SageMaker will automatically create this bucket if it does not exist\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa609c",
   "metadata": {},
   "source": [
    "## Finding max batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326fa565",
   "metadata": {},
   "source": [
    "### Model and instance type specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d91c1",
   "metadata": {},
   "source": [
    "This notebook uses HF training script to demonstrate how to find the max batch size that can fit in memory, if you're using a customized training script, please update `find_max_batch_size.py` script and `hyperparameters` accordingly. Below, we specify the model we would like to find the max batch size for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0c5e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_MODELING_LOSS = \"clm\"  \n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "TOKENIZER_NAME = \"gpt2\"\n",
    "MODEL_CONFIG = \"model_name_or_path\"\n",
    "\n",
    "INSTANCE_TYPE = \"ml.p3.8xlarge\"\n",
    "\n",
    "# hyperparameters are passed to the training entrypoint as arguments\n",
    "hyperparameters = {\n",
    "    \"training_script\": f\"run_{LANGUAGE_MODELING_LOSS}.py\",\n",
    "    MODEL_CONFIG: MODEL_NAME,\n",
    "    \"tokenizer_name\": TOKENIZER_NAME,\n",
    "    \"fp16\": True,\n",
    "    \"sequence_len\": 512,\n",
    "    \"per_device_train_batch_size_min\" : 1,\n",
    "    \"per_device_train_batch_size_max\" : 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd53820",
   "metadata": {},
   "source": [
    "### Finding max batch size for SageMaker Training Compiler with Hugging Face and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73c7944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\n",
      "import os, subprocess, time\n",
      "import torch\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    os.environ[\"GPU_NUM_DEVICES\"] = str(torch.cuda.device_count())\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "    # please update parameters if using a customized training script\n",
      "    # model configs\n",
      "    parser.add_argument(\n",
      "        \"--language_modeling_loss\",\n",
      "        type=str,\n",
      "        default=\"clm\",\n",
      "        help=\"select either use training script run_mlm or run_clm\",\n",
      "    )\n",
      "    parser.add_argument(\"--model_name_or_path\", type=str, default=\"gpt2\", help=\"HF model name\")\n",
      "    parser.add_argument(\"--tokenizer_name\", type=str, default=\"gpt2\", help=\"tokenizer name\")\n",
      "    parser.add_argument(\"--sequence_len\", type=str, default=\"512\", help=\"sequence length\")\n",
      "    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"whether to train in amp mode\")\n",
      "\n",
      "    # batch size config\n",
      "    parser.add_argument(\n",
      "        \"--per_device_train_batch_size_min\", type=int, default=1, help=\"minimum batch size to try\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--per_device_train_batch_size_max\", type=int, default=256, help=\"maximum batch size to try\"\n",
      "    )\n",
      "    args, rem_args = parser.parse_known_args()\n",
      "\n",
      "    # Causal Language Modeling (run_clm) or Masked Language Modeling (run_mlm)\n",
      "    training_command = \"python \"\n",
      "    if args.language_modeling_loss == \"mlm\":\n",
      "        training_command += \"./run_mlm.py \"\n",
      "    else:\n",
      "        training_command += \"./run_clm.py \"\n",
      "    training_command += f\"--model_name_or_path {args.model_name_or_path} \"\n",
      "    training_command += (\n",
      "        f\"--max_seq_length {args.sequence_len} \"\n",
      "        if args.language_modeling_loss == \"mlm\"\n",
      "        else (f\"--block_size {args.sequence_len} \")\n",
      "    )\n",
      "    training_command += \"--dataset_name glue \"\n",
      "    training_command += \"--dataset_config_name sst2 \"\n",
      "    training_command += \"--do_train \"\n",
      "    training_command += \"--num_train_epochs 1 \"\n",
      "    training_command += \"--max_steps 10 \"\n",
      "    training_command += \"--save_strategy no \"\n",
      "    training_command += \"--logging_strategy no \"\n",
      "    training_command += \"--output_dir /tmp/test \"\n",
      "    training_command += \"--overwrite_output_dir \"\n",
      "    if args.fp16:\n",
      "        training_command += \"--fp16 \"\n",
      "    # find max batch size between per_device_train_batch_size_min and per_device_train_batch_size_max\n",
      "    print(\"Tuning Command: \", training_command)\n",
      "    assert (\n",
      "        args.per_device_train_batch_size_min >= 1\n",
      "        and args.per_device_train_batch_size_min <= args.per_device_train_batch_size_max\n",
      "    )\n",
      "    batch_result = 0\n",
      "    low, high = args.per_device_train_batch_size_min, args.per_device_train_batch_size_max\n",
      "    tic, i = time.perf_counter(), 0\n",
      "    while low <= high:\n",
      "        batch_to_try, i = (low + high) // 2, i + 1\n",
      "        log_info = f\"model: {args.model_name_or_path} trying batch_size: {batch_to_try} \"\n",
      "        try:\n",
      "            # please update batch_size parameter naming if using a customized training script\n",
      "            training_command_batch = (\n",
      "                training_command + f\"--per_device_train_batch_size {batch_to_try}\"\n",
      "            )\n",
      "            subprocess.check_output(training_command_batch, shell=True)\n",
      "            batch_result, low = batch_to_try, batch_to_try + 1\n",
      "            log_info += \"succeed\"\n",
      "        except subprocess.CalledProcessError as exc:\n",
      "            high = batch_to_try - 1\n",
      "            log_info += \"failed\"\n",
      "        print(log_info)\n",
      "    toc = time.perf_counter()\n",
      "\n",
      "    print(f\"Total max batch found in {toc - tic:0.4f} seconds, {i} iterations\")\n",
      "    print(\n",
      "        f\"[result]: model: {args.model_name_or_path}, max_batch_size between {args.per_device_train_batch_size_min} and {args.per_device_train_batch_size_max} is {batch_result}\"\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "# This prints the training script for reference\n",
    "# The script iteratively tests different batch sizes\n",
    "!pygmentize ./scripts/find_max_batch_size.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98587d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch-trcomp-training-2022-02-24-23-17-47-393'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n",
    "\n",
    "# configure the training job\n",
    "optimized_estimator = HuggingFace(\n",
    "    entry_point=\"find_max_batch_size.py\",  # Wrapper around training script that finds the maximum batch size\n",
    "    compiler_config=TrainingCompilerConfig(),  # We are enabling SageMaker Training Compiler here !\n",
    "    source_dir=\"./scripts\",\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    volume_size=100,\n",
    "    py_version=\"py38\",\n",
    "    transformers_version=\"4.11.0\",\n",
    "    pytorch_version=\"1.9.0\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_profiler=True,  # Disabling SageMaker Profiler to avoid overheads during benchmarking\n",
    "    debugger_hook_config=False,  # Disabling SageMaker Debugger to avoid overheads during benchmarking\n",
    ")\n",
    "\n",
    "# start the training job\n",
    "optimized_estimator.fit(wait=False)\n",
    "optimized_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbecffb3",
   "metadata": {},
   "source": [
    "### Wait for for the training jobs to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd5f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = optimized_estimator.sagemaker_session.sagemaker_client.get_waiter(\n",
    "    \"training_job_completed_or_stopped\"\n",
    ")\n",
    "waiter.wait(TrainingJobName=optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa2ff6",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca81ba",
   "metadata": {},
   "source": [
    "### Load logs for training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8454da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture optimized\n",
    "\n",
    "# access the logs of the optimized training job\n",
    "optimized_estimator.sagemaker_session.logs_for_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ffc316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTotal max batch found in 1354.5406 seconds, 7 iterations\u001b[0m\n",
      "\u001b[34m[result]: model: gpt2, max_batch_size between 1 and 128 is 23\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Print the max batch size below\n",
    "\n",
    "for line in optimized.stdout.split(\"\\n\"):\n",
    "    if 'result' in line and 'max_batch_size' in line or 'Total max batch' in line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ea509",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Stop all training jobs launched if the jobs are still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "def stop_training_job(name):\n",
    "    status = sm.describe_training_job(TrainingJobName=name)[\"TrainingJobStatus\"]\n",
    "    if status == \"InProgress\":\n",
    "        sm.stop_training_job(TrainingJobName=name)\n",
    "\n",
    "\n",
    "stop_training_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e33f76",
   "metadata": {},
   "source": [
    "Also, to find instructions on cleaning up resources, see [Clean Up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html) in the *Amazon SageMaker Developer Guide*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c5820",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "SageMaker Training Compiler improves the efficiency of your training job by typically decreasing the memory footpeing of the job. In this notebook, you found the largest `batch_size` that can fit in memory with Training Compiler's optimizations. Increasing the `batch_size` can decrease the time needed to train a model, reducing cost and enabling faster iteration.\n",
    "\n",
    "Remeber that learning rate should be adjusted when `batch_size` is changed to minimize difference in convergence behavior during training. For more information, see https://arxiv.org/abs/1706.02677"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
