{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a Trained Model\n",
    "\n",
    "In this notebook, we will walk through the process of deploying a trained model to a SageMaker Endpoint. If you recently ran [the notebook for traingin](get_started_mnist_deploy.ipynb) with %store% magic, the `model_data` can be restored. Otherwise, we will retrieve the \n",
    "model artifact from a public S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setups\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Get global config\n",
    "with open('code/config.json', 'r') as f:\n",
    "    CONFIG=json.load(f)\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "%store -r model_data\n",
    "del model_data\n",
    "try: \n",
    "    model_data\n",
    "except NameError:\n",
    "    import json\n",
    "    model_data = 's3://' + CONFIG['public_bucket'] + '/datasets/image/MNIST/model/pytorch-training-2020-11-21-22-02-56-203/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Model Object\n",
    "\n",
    "The `PyTorchModel` class allows you to define an environment for making inference using your\n",
    "model artifact. Like `PyTorch` class we discussed \n",
    "[in this notebook for training an PyTorch model](\n",
    "get_started_mnist_train.ipynb), it is high level API used to set up a docker image for your model hosting service.\n",
    "\n",
    "Once it is properly configured, it can be used to create a SageMaker\n",
    "Endpoint on an EC2 instance. The SageMaker Endpoint is a containerized environment that uses your trained model \n",
    "to make inference on incoming data via RESTful API calls. \n",
    "\n",
    "Some common parameters used to initiate the `PyTorchModel` class are:\n",
    "- entry_point: A user defined python file to be used by the inference image as handlers of incoming requests\n",
    "- source_dir: The directory of the `entry_point`\n",
    "- role: An IAM role to make AWS service requests\n",
    "- model_data: the S3 location of the compressed model artifact. It can be a path to a local file if the endpoint \n",
    "is to be deployed on the SageMaker instance you are using to run this notebook (local mode)\n",
    "- framework_version: version of the PyTorch package to be used\n",
    "- py_version: python version to be used\n",
    "\n",
    "We will elaborate on the `entry_point` below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(\n",
    "    entry_point='inference.py',\n",
    "    source_dir='code',\n",
    "    role=role,\n",
    "    model_data=model_data,\n",
    "    framework_version='1.5.0',\n",
    "    py_version='py3'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry Point for the Inference Image\n",
    "\n",
    "Your model artifacts pointed by `model_data` is pulled by the `PyTorchModel` and it is decompressed and saved in\n",
    "in the docker image it defines. They become regular model checkpoint files that you would produce outside SageMaker. This means in order to use your trained model for serving, \n",
    "you need to tell `PyTorchModel` class how to a recover a PyTorch model from the static checkpoint.\n",
    "\n",
    "Also, the deployed endpoint interacts with RESTful API calls, you need to tell it how to parse an incoming \n",
    "request to your model. \n",
    "\n",
    "These two instructions needs to be defined as two functions in the python file pointed by `entry_point`.\n",
    "\n",
    "By convention, we name this entry point file `inference.py` and we put it in the `code` directory.\n",
    "\n",
    "To tell the inference image how to load the model checkpoint, you need to implement a function called \n",
    "`model_fn`. This function takes one positional argument \n",
    "\n",
    "- `model_dir`: the directory of the static model checkpoints in the inference image.\n",
    "\n",
    "The return of `model_fn` is an PyTorch model. In this example, the `model_fn`\n",
    "looks like:\n",
    "\n",
    "```python\n",
    "def model_fn(model_dir):\n",
    "    model = Net().to(device)\n",
    "    model.eval() \n",
    "    return model\n",
    "```\n",
    "\n",
    "Next, you need to tell the hosting service how to handle the incoming data. This includes:\n",
    "\n",
    "* How to parse the incoming request\n",
    "* How to use the trained model to make inference\n",
    "* How to return the prediction to the caller of the service\n",
    "\n",
    "\n",
    "You do it by implementing 3 functions:\n",
    "\n",
    "#### `input_fn` function\n",
    "\n",
    "The SageMaker PyTorch model server will invoke an `input_fn` function in your inferece\n",
    "entry point. This function handles data decoding. The `input_fn` have the following signature:\n",
    "```python\n",
    "def input_fn(request_body, request_content_type)\n",
    "```\n",
    "The two positional arguments are:\n",
    "- `request_body`: the payload of the incoming request\n",
    "- `request_content_type`: the content type of the incoming request\n",
    "\n",
    "The return of `input_fn` is an object that can be passed to `predict_fn`\n",
    "\n",
    "In this example, the `input_fn` looks like:\n",
    "```python\n",
    "def input_fn(request_body, request_content_type):\n",
    "    assert request_content_type=='application/json'\n",
    "    data = json.loads(request_body)['inputs']\n",
    "    data = torch.tensor(data, dtype=torch.float32, device=device)\n",
    "    return data\n",
    "```\n",
    "It requires the request payload is encoded as a json string and\n",
    "it assumes the decoded payload contains a key `inputs`\n",
    "that maps to the input data to be consumed by the model.\n",
    "\n",
    "\n",
    "\n",
    "#### `predict_fn` \n",
    "After the inference request has been deserialzed by `input_fn`, the SageMaker PyTorch\n",
    "model server invokes `predict_fn` on the return value of `input_fn`.\n",
    "\n",
    "The `predict_fn` function has the following signature:\n",
    "```python\n",
    "def predict_fn(input_object, model)\n",
    "```\n",
    "The two positional arguments are:\n",
    "- `input_object`: the return value from `input_fn`\n",
    "- `model`: the return value from `model_fn`\n",
    "\n",
    "The return of `predict_fn` is the first argument to be passed to `output_fn`\n",
    "\n",
    "In this example, the `predict_fn` function looks like\n",
    "\n",
    "```python\n",
    "def predict_fn(input_object, model):\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_object)\n",
    "    return prediction\n",
    "```\n",
    "\n",
    "Note that we directly feed the return of `input_fn` to `predict_fn`.\n",
    "This means you should invoke the SageMaker PyTorch model server with data that\n",
    "can be readily consumed by the model, i.e. normalized and has batch and channel dimension. \n",
    "\n",
    "\n",
    "#### `output_fn` \n",
    "After invoking `predict_fn`, the model server invokes `output_fn` for data post-process.\n",
    "The `output_fn` has the following signature:\n",
    "\n",
    "```python\n",
    "def output_fn(prediction, content_type)\n",
    "```\n",
    "\n",
    "The two positional arguments are:\n",
    "- `prediction`: the return value from `predict_fn`\n",
    "- `content_type`: the content type of the response\n",
    "\n",
    "The return of `output_fn` should be a byte array of data serialized to `content_type`.\n",
    "\n",
    "In this exampe, the `output_fn` function looks like\n",
    "\n",
    "```python\n",
    "def output_fn(predictions, content_type):\n",
    "    assert content_type == 'application/json'\n",
    "    res = predictions.cpu().numpy().tolist()\n",
    "    return json.dumps(res)\n",
    "```\n",
    "\n",
    "After the inference, the function uses `content_type` to encode the \n",
    "prediction into the content type of the response. In this example,\n",
    "the function requires the caller of the service to accept json string. \n",
    "\n",
    "For more info on handler functions, check the [SageMaker Python SDK document](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#process-model-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the inference container\n",
    "Once the `PyTorchModel` class is initiated, we can call its `deploy` method to run the container for the hosting\n",
    "service. Some common parameters needed to call `deploy` methods are:\n",
    "\n",
    "- initial_instance_count: the number of SageMaker instances to be used to run the hosting service.\n",
    "- instance_type: the type of SageMaker instance to run the hosting service. Set it to `local` if you want run the hosting service on the local SageMaker instance. Local mode are typically used for debugging. \n",
    "- serializer: A python callable used to serialize (encode) the request data.\n",
    "- deserializer: A python callable used to deserialize (decode) the response data.\n",
    "\n",
    "Commonly used serializers and deserialzers are implemented in `sagemaker.serializers` and `sagemaker.deserializer`\n",
    "submodules of the SageMaker Python SDK. \n",
    "\n",
    "Since in the `transform_fn` we declared that the incoming requests are json-encoded, we need use a json serializer,\n",
    "to encode the incoming data into a json string. Also, we declared the return content type to be json string, we\n",
    "need to use a json deserializer to parse the response into a an (in this case, an \n",
    "integer represeting the predicted hand-written digit). \n",
    "\n",
    "<span style=\"color:red\"> Note: local mode is not supported in SageMaker Studio </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# set local_mode to False if you want to deploy on a remote\n",
    "# SageMaker instance\n",
    "\n",
    "local_mode=True\n",
    "\n",
    "if local_mode:\n",
    "    instance_type='local'\n",
    "else:\n",
    "    instance_type='ml.c4.xlarge'\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predictor` we get above can be used to make prediction requests agaist a SageMaker endpoint. For more\n",
    "information, check [the api reference for SageMaker Predictor](\n",
    "https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html#sagemaker.predictor.Predictor)\n",
    "\n",
    "Now, let's test the endpoint with some dummy data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "dummy_data = {\n",
    "    'inputs': np.random.rand(16, 1, 28, 28).tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `transform_fn`, we declared that the parsed data is a python dictionary with a key `inputs` and its value should \n",
    "be a 1D array of length 784. Hence, the definition of `dummy_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predictor.predict(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input data does not look exactly like `dummy_data`, the endpoint will raise an exception. This is because \n",
    "of the stringent way we defined the `transform_fn`. Let's test the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = [random.random() for _ in range(784)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `dummy_data` is parsed in `transform_fn`, it does not have an `inputs` field, so `transform_fn` will crush. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to make inference on incorrectly formated input data\n",
    "# res = predictor.predict(dummy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use real MNIST test to test the endpoint. We will use helper functions defined in `code.utils` to \n",
    "download MNIST data set and normalize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mnist import mnist_to_numpy, normalize\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_dir = '/tmp/data'\n",
    "X, _ = mnist_to_numpy(data_dir, train=False)\n",
    "\n",
    "# randomly sample 16 images to inspect\n",
    "mask = random.sample(range(X.shape[0]), 16)\n",
    "samples = X[mask]\n",
    "\n",
    "# plot the images \n",
    "fig, axs = plt.subplots(nrows=1, ncols=16, figsize=(16, 1))\n",
    "\n",
    "for i, splt in enumerate(axs):\n",
    "    splt.imshow(samples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.shape, samples.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we invoke the SageMaker PyTorch model server with `samples`, we will need to do\n",
    "some pre-processing\n",
    "- convert its data type to 32 bit floating point\n",
    "- normalize each channel (only one channel for MNIST)\n",
    "- add a channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = normalize(samples.astype(np.float32), axis=(1, 2))\n",
    "\n",
    "res = predictor.predict(\n",
    "    {\n",
    "        'inputs': np.expand_dims(samples, axis=1).tolist()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is a list of probablity vector of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(np.array(res, dtype=np.float32), axis=1).tolist()\n",
    "print(\"Predicted digits: \", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and debug the entry point before deployment\n",
    "\n",
    "When deploying a model to a SageMaker Endpoint, it is a good practice to test the entry \n",
    "point. The following snippet shows you how you can test and debug the `model_fn` and \n",
    "`transform_fn` you implemented in the entry point for the inference image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize code/test_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `test` function simulates how the inference container works. It pulls the model\n",
    "artifact and loads the model into \n",
    "memory by calling `model_fn` and parse `model_dir` to it. \n",
    "When it receives a request,\n",
    "it calls `input_fn`, `predict_fn` and `output_fn` consecutively. \n",
    "\n",
    "Implementing such a test function helps you debugging the entry point before put it into\n",
    "the production. If `test` runs correctly, then you can be certain that if the incoming\n",
    "data and its content type are what they suppose to be, then the endpoint point is going\n",
    "to work as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up (Optional)\n",
    "\n",
    "If you do not plan to use the endpoint, you should delete it to free up some computation \n",
    "resource. If you use local, you will need to manually delete the docker container bounded\n",
    "at port 8080 (the port that listens to the incoming request).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not local_mode:\n",
    "    predictor.delete_endpoint()\n",
    "else:\n",
    "    os.system(\"docker container ls | grep 8080 | awk '{print $1}' | xargs docker container rm -f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
