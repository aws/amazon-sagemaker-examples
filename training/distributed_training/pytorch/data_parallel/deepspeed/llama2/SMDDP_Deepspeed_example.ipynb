{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Llama2 model using SageMaker Distributed Data Parallel Library (SMDDP) and DeepSpeed\n",
    "\n",
    "In this tutorial, we will show how to train or fine-tune the new [LLama2-7B](https://huggingface.co/meta-llama/Llama-2-7b) model.  We will use DeepSpeed ZeRO stage 3, a sharded data parallelism technique that alleviates the memory bottleneck when training large models.  \n",
    "\n",
    "In addition, we will utilize a the **SMDDP library**, a handy SageMaker feature which accelerates training by speeding up GPU communication between nodes.  We will use 2 p4d.24xlarge instances, which come with 8x NVIDIA A100 40GB GPUs. \n",
    "\n",
    "*Note: For the purpose of this example, we will use a dummy synthetic dataset to avoid dealing with an access token required to initialize a Llama2 tokenizer.  This example can be easily modified to supply your own dataset if you own a Llama2 access token*\n",
    "\n",
    "Within `code/train.py` is the entry point for the training script where we initialize the SMDDP library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you are going to use Sagemaker in a local environment, you need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sagemaker_session.boto_region_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that SageMaker by default uses the latest [AWS Deep Learning Container (DLC)](https://aws.amazon.com/machine-learning/containers/), so you can comment out the `ecr_image` variable if you don't want to use your own custom image built from a DLC. Also note that if using FSx when launching the SageMaker notebook instance, you will need to use the same `subnet` and `security_group_config`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ecr_image = \"<ECR_IMAGE_URI>\"\n",
    "subnet_config = [\"<SUBNET_CONFIG_ID>\"]\n",
    "security_group_config = [\"<SECURITY_GROUP_CONFIG>\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Training Job\n",
    "\n",
    "We will now set the hyperparameters and define the estimator object for our training job.  Since we are using DeepSpeed, we must provide a DeepSpeed config JSON file, which is located in the `code` folder.  We will  use the `PyTorch` estimator class and configure it to use the `torch_distributed` distribution, which will launch the training job using `torchrun`.  This launcher kicks off the training script as a distributed training job on SageMaker and is the recommended launcher for sharded data parallel training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "  'model_id': 'meta-llama/Llama-2-7b-chat-hf',\n",
    "  'gradient_checkpointing': True,\n",
    "  'bf16': True,\n",
    "  'optimizer': \"adamw_torch\",\n",
    "  'per_device_train_batch_size': 1,\n",
    "  'epochs': 1,\n",
    "  'max_steps':50,\n",
    "  'deepspeed_config':'dsconfig.json'\n",
    "}\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "estimator = PyTorch(\n",
    "  entry_point=\"train.py\",\n",
    "  base_job_name=\"llama2-training-smddp\",\n",
    "  role=role,\n",
    "  image_uri=ecr_image,\n",
    "  source_dir=\"code\",\n",
    "  instance_count=2,\n",
    "  instance_type=\"ml.p4d.24xlarge\",\n",
    "  sagemaker_session=sagemaker_session,\n",
    "  subnets=subnet_config,\n",
    "  hyperparameters=hyperparameters,\n",
    "  security_group_ids=security_group_config,\n",
    "  keep_alive_period_in_seconds=600,\n",
    "  distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "  debugger_hook_config=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the traning job \n",
    "We can now start our training job, with the `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "estimator.fit(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate the warm pool cluster if no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.update_training_job(estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\":0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
