{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fleet Predictive Maintenance: Part 2. Data Preparation with Data Wrangler\n",
    "\n",
    "1. [Architecure](0_usecase_and_architecture_predmaint.ipynb#0_Architecture)\n",
    "1. [Data Prep: Processing Job from Data Wrangler Output](./1_dataprep_dw_job_predmaint.ipynb)\n",
    "1. [Data Prep: Featurization](./2_dataprep_predmaint.ipynb)\n",
    "1. [Train, Tune and Predict using Batch Transform](./3_train_tune_predict_predmaint.ipynb.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Data Wrangler Job Notebook\n",
    "\n",
    "This notebook uses the Data Wrangler .flow file to submit a SageMaker Data Wrangler Job\n",
    "with the following steps:\n",
    "\n",
    "* Push Data Wrangler .flow file to S3\n",
    "* Parse the .flow file inputs, and create the argument dictionary to submit to a boto client\n",
    "* Submit the ProcessingJob arguments and wait for Job completion\n",
    "\n",
    "Optionally, the notebook also gives an example of starting a SageMaker XGBoost TrainingJob using\n",
    "the newly processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Python SDK version 2.x is required\n",
    "import pkg_resources\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "original_version = pkg_resources.get_distribution(\"sagemaker\").version\n",
    "_ = subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.20.0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "The following lists configurable parameters that are used throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket for saving processing job outputs\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"data_wrangler_flows\"\n",
    "flow_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_name = f\"flow-{flow_id}\"\n",
    "flow_uri = f\"s3://{bucket}/{prefix}/{flow_name}.flow\"\n",
    "\n",
    "flow_file_name = \"dw_flow/prm.flow\"\n",
    "\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "container_uri = (\n",
    "    \"415577184552.dkr.ecr.us-east-2.amazonaws.com/sagemaker-data-wrangler-container:1.2.1\"\n",
    ")\n",
    "\n",
    "# Processing Job Resources Configurations\n",
    "# Data wrangler processing job only supports 1 instance.\n",
    "instance_count = 1\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Processing Job Path URI Information\n",
    "output_prefix = f\"export-{flow_name}/output\"\n",
    "output_path = f\"s3://{bucket}/{output_prefix}\"\n",
    "output_name = \"ff586e7b-a02d-472b-91d4-da3dd05d7a30.default\"\n",
    "\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_id}\"\n",
    "\n",
    "processing_dir = \"/opt/ml/processing\"\n",
    "\n",
    "# Modify the variable below to specify the content type to be used for writing each output\n",
    "# Currently supported options are 'CSV' or 'PARQUET', and default to 'CSV'\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# URL to use for sagemaker client.\n",
    "# If this is None, boto will automatically construct the appropriate URL to use\n",
    "# when communicating with sagemaker.\n",
    "sagemaker_endpoint_url = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For this demo, the following cell has been added to the generated code from the Data Wrangler export. The changes are needed to update the S3 bucket in the .flow file to match your S3 location as well as make sure we have the right container URI depending on your region.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_helpers import update_dw_s3uri, get_dw_container_for_region\n",
    "\n",
    "# update the flow file to change the s3 location to our bucket\n",
    "update_dw_s3uri(flow_file_name)\n",
    "\n",
    "# get the Data Wrangler container associated with our region\n",
    "region = boto3.Session().region_name\n",
    "container_uri = get_dw_container_for_region(region)\n",
    "\n",
    "dw_output_path_prm = output_path\n",
    "print(\n",
    "    f\"Storing dw_output_path_prm = {dw_output_path_prm} for use in next notebook 2_fleet_predmaint.ipynb\"\n",
    ")\n",
    "%store dw_output_path_prm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Flow to S3\n",
    "\n",
    "Use the following cell to upload the Data Wrangler .flow file to Amazon S3 so that\n",
    "it can be used as an input to the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .flow file\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Upload to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"{prefix}/{flow_name}.flow\")\n",
    "\n",
    "print(f\"Data Wrangler Flow notebook uploaded to {flow_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processing Job arguments\n",
    "\n",
    "This notebook submits a Processing Job using the Sagmaker Python SDK. Below, utility methods are \n",
    "defined for creating Processing Job Inputs for the following sources: S3, Athena, and Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import (\n",
    "    AthenaDatasetDefinition,\n",
    "    DatasetDefinition,\n",
    "    RedshiftDatasetDefinition,\n",
    ")\n",
    "\n",
    "\n",
    "def create_flow_notebook_processing_input(base_dir, flow_s3_uri):\n",
    "    return ProcessingInput(\n",
    "        source=flow_s3_uri,\n",
    "        destination=f\"{base_dir}/flow\",\n",
    "        input_name=\"flow\",\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_s3_processing_input(s3_dataset_definition, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        source=s3_dataset_definition[\"s3ExecutionContext\"][\"s3Uri\"],\n",
    "        destination=f\"{base_dir}/{name}\",\n",
    "        input_name=name,\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_athena_processing_input(athena_dataset_defintion, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        input_name=name,\n",
    "        dataset_definition=DatasetDefinition(\n",
    "            local_path=f\"{base_dir}/{name}\",\n",
    "            athena_dataset_definition=AthenaDatasetDefinition(\n",
    "                catalog=athena_dataset_defintion[\"catalogName\"],\n",
    "                database=athena_dataset_defintion[\"databaseName\"],\n",
    "                query_string=athena_dataset_defintion[\"queryString\"],\n",
    "                output_s3_uri=athena_dataset_defintion[\"s3OutputLocation\"] + f\"{name}/\",\n",
    "                output_format=athena_dataset_defintion[\"outputFormat\"].upper(),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_redshift_processing_input(redshift_dataset_defintion, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        input_name=name,\n",
    "        dataset_definition=DatasetDefinition(\n",
    "            local_path=f\"{base_dir}/{name}\",\n",
    "            redshift_dataset_definition=RedshiftDatasetDefinition(\n",
    "                cluster_id=redshift_dataset_defintion[\"clusterIdentifier\"],\n",
    "                database=redshift_dataset_defintion[\"database\"],\n",
    "                db_user=redshift_dataset_defintion[\"dbUser\"],\n",
    "                query_string=redshift_dataset_defintion[\"queryString\"],\n",
    "                cluster_role_arn=redshift_dataset_defintion[\"unloadIamRole\"],\n",
    "                output_s3_uri=redshift_dataset_defintion[\"s3OutputLocation\"] + f\"{name}/\",\n",
    "                output_format=redshift_dataset_defintion[\"outputFormat\"].upper(),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_processing_inputs(processing_dir, flow, flow_uri):\n",
    "    \"\"\"Helper function for creating processing inputs\n",
    "    :param flow: loaded data wrangler flow notebook\n",
    "    :param flow_uri: S3 URI of the data wrangler flow notebook\n",
    "    \"\"\"\n",
    "    processing_inputs = []\n",
    "    flow_processing_input = create_flow_notebook_processing_input(processing_dir, flow_uri)\n",
    "    processing_inputs.append(flow_processing_input)\n",
    "\n",
    "    for node in flow[\"nodes\"]:\n",
    "        if \"dataset_definition\" in node[\"parameters\"]:\n",
    "            data_def = node[\"parameters\"][\"dataset_definition\"]\n",
    "            name = data_def[\"name\"]\n",
    "            source_type = data_def[\"datasetSourceType\"]\n",
    "\n",
    "            if source_type == \"S3\":\n",
    "                processing_inputs.append(create_s3_processing_input(data_def, name, processing_dir))\n",
    "            elif source_type == \"Athena\":\n",
    "                processing_inputs.append(\n",
    "                    create_athena_processing_input(data_def, name, processing_dir)\n",
    "                )\n",
    "            elif source_type == \"Redshift\":\n",
    "                processing_inputs.append(\n",
    "                    create_redshift_processing_input(data_def, name, processing_dir)\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"{source_type} is not supported for Data Wrangler Processing.\")\n",
    "\n",
    "    return processing_inputs\n",
    "\n",
    "\n",
    "def create_processing_output(output_name, output_path, processing_dir):\n",
    "    return ProcessingOutput(\n",
    "        output_name=output_name,\n",
    "        source=os.path.join(processing_dir, \"output\"),\n",
    "        destination=output_path,\n",
    "        s3_upload_mode=\"EndOfJob\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_container_arguments(output_name, output_content_type):\n",
    "    output_config = {output_name: {\"content_type\": output_content_type}}\n",
    "    return [f\"--output-config '{json.dumps(output_config)}'\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start ProcessingJob\n",
    "\n",
    "Now, the Processing Job is submitted using the Processor from the Sagemaker SDK.\n",
    "Logs are turned off, but can be turned on for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.processing import Processor\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "processor.run(\n",
    "    inputs=create_processing_inputs(processing_dir, flow, flow_uri),\n",
    "    outputs=[create_processing_output(output_name, output_path, processing_dir)],\n",
    "    arguments=create_container_arguments(output_name, output_content_type),\n",
    "    wait=True,\n",
    "    logs=False,\n",
    "    job_name=processing_job_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kick off SageMaker Training Job (Optional)\n",
    "\n",
    "Data Wrangler is a SageMaker tool for processing data to be used for Machine Learning. Now that\n",
    "the data has been processed, users will want to train a model using the data. The following shows\n",
    "an example of doing so using a popular algorithm XGBoost.\n",
    "\n",
    "It is important to note that the following XGBoost objective ['binary', 'regression',\n",
    "'multiclass'], hyperparameters, or content_type may not be suitable for the output data, and will\n",
    "require changes to train a proper model. Furthermore, for CSV training, the algorithm assumes that\n",
    "the target variable is in the first column. For more information on SageMaker XGBoost, please see\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html.\n",
    "\n",
    "### Find Training Data path\n",
    "\n",
    "The below demonstrates how to recursively search the output directory to find the data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "list_response = s3_client.list_objects_v2(Bucket=bucket, Prefix=output_prefix)\n",
    "\n",
    "training_path = None\n",
    "\n",
    "for content in list_response[\"Contents\"]:\n",
    "    if \"_SUCCESS\" not in content[\"Key\"]:\n",
    "        training_path = content[\"Key\"]\n",
    "\n",
    "print(training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the Training Job hyperparameters are set. For more information on XGBoost Hyperparameters,\n",
    "see https://xgboost.readthedocs.io/en/latest/parameter.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "hyperparameters = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"10\",\n",
    "}\n",
    "train_content_type = (\n",
    "    \"application/x-parquet\" if output_content_type.upper() == \"PARQUET\" else \"text/csv\"\n",
    ")\n",
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f\"s3://{bucket}/{training_path}\",\n",
    "    content_type=train_content_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TrainingJob configurations are set using the SageMaker Python SDK Estimator, and which is fit\n",
    "using the training data from the ProcessingJob that was run earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    iam_role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    ")\n",
    "estimator.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Uncomment the following code cell to revert the SageMaker Python SDK to the original version used\n",
    "before running this notebook. This notebook upgrades the SageMaker Python SDK to 2.x, which may\n",
    "cause other example notebooks to break. To learn more about the changes introduced in the\n",
    "SageMaker Python SDK 2.x update, see\n",
    "[Use Version 2.x of the SageMaker Python SDK.](https://sagemaker.readthedocs.io/en/stable/v2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = subprocess.check_call(\n",
    "#         [sys.executable, \"-m\", \"pip\", \"install\", f\"sagemaker=={original_version}\"]\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
