{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U git+https://github.com/aws/sagemaker-python-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling SKLearn with Amazon SageMaker Pipelines\n",
    "\n",
    "In this notebook, we will use Amazon SageMaker Pipelines to create two workflows with Scikit-Learn. We will create a pipeline that preprocess data and trains a model (we will use scikit-learn Pipeline), then we will schedule inference with SageMaker Batch Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "For this example, we will create an artificial dataset with the `sklearn.datasets.make_classification()` function from the scikit-learn library. Once that's created, we will store it locally and then in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-west-1-859755744029/sklearn-pipeline/data/source/data.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sagemaker import Session\n",
    "\n",
    "session = Session()\n",
    "bucket = (\n",
    "    session.default_bucket()\n",
    ")  # Change to another bucket if running outside of SageMaker\n",
    "prefix = \"sklearn-pipeline/data\"  # Choose your preferred prefix, but keep it consistent\n",
    "\n",
    "# Create a random dataset for classification\n",
    "X, y = make_classification(random_state=42)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=[\"y\"])], axis=1)\n",
    "data.to_csv(\"/tmp/data.csv\", index=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "pd.DataFrame(X_test).to_csv(\"/tmp/x_test.csv\", index=False, header=False)\n",
    "pd.DataFrame(y_test).to_csv(\"/tmp/y_test.csv\", index=False, header=False)\n",
    "# Upload to S3\n",
    "data_path = session.upload_data(\n",
    "    path=\"/tmp/data.csv\", bucket=bucket, key_prefix=f\"{prefix}/source\"\n",
    ")\n",
    "x_test_path = session.upload_data(\n",
    "    path=\"/tmp/x_test.csv\", bucket=bucket, key_prefix=f\"{prefix}/test\"\n",
    ")\n",
    "y_test_path = session.upload_data(\n",
    "    path=\"/tmp/y_test.csv\", bucket=bucket, key_prefix=f\"{prefix}/test\"\n",
    ")\n",
    "\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker helper variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pipeline\n",
    "\n",
    "Let's start by creating the preprocessing and training pipeline. In this case, we will use two pre-existing scripts, `preprocessing.py` and `training.py`, to preprocess our input data and train our model. Both scripts use the `sklearn.pipeline` library, and are expected to output a `joblib` compressed file to be re-used during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create the parameters of the pipeline\n",
    "\n",
    "Before we begin to create the pipeline itself, we should think about how to parameterize it. For example, we may use different instance types for different purposes, such as CPU-based types for data processing and GPU-based or more powerful types for model training. These are all \"knobs\" of the pipeline that we can parameterize. Parameterizing enables custom pipeline executions and schedules without having to modify the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "# raw input data\n",
    "input_data = ParameterString(name=\"InputData\", default_value=data_path)\n",
    "\n",
    "# processing step parameters\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\", default_value=1\n",
    ")\n",
    "\n",
    "# training step parameters\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\", default_value=\"ml.c5.2xlarge\"\n",
    ")\n",
    "training_instance_count = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\", default_value=1\n",
    ")\n",
    "\n",
    "# batch inference step parameters\n",
    "batch_instance_type = ParameterString(\n",
    "    name=\"BatchInstanceType\", default_value=\"ml.c5.xlarge\"\n",
    ")\n",
    "batch_instance_count = ParameterInteger(name=\"BatchInstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create the `SKLearnProcessor` and the `ProcessorStep`\n",
    "\n",
    "The first step in the pipeline will preprocess the data to prepare it for training. We create a `SKLearnProcessor` object similar to the one above, but now parameterized, so we can separately track and change the job configuration as needed, for example to increase the instance type size and count to accommodate a growing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"scheduled-pipelines-sklearn\",\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"scheduled-pipeline-sklearn-process\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=input_data,\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/output/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/output/test\"),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"pipeline\", source=\"/opt/ml/processing/output/pipeline\"\n",
    "        ),\n",
    "    ],\n",
    "    code=\"./preprocessing.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Create the `SKLearnEstimator` and its `TrainingStep`\n",
    "\n",
    "Next, we specify a `Estimator` object, and define a `TrainingStep` to insert the training job in the pipeline with inputs from the previous SageMaker Processing step. Notice that we have used the hyperparameters from the best estimator in the tuning job we ran before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "# Define the Estimator from SageMaker (Script Mode)\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"training.py\",\n",
    "    role=role,\n",
    "    instance_count=training_instance_count,\n",
    "    instance_type=training_instance_type,\n",
    "    framework_version=framework_version,\n",
    "    base_job_name=\"sklearn-training\",\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"model_accuracy\", \"Regex\": \"Model Accuracy: ([0-9.]+).*$\"}\n",
    "    ],\n",
    "    hyperparameters={\"n-estimators\": 100, \"min-samples-leaf\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"scheduled-pipeline-sklearn-training\",\n",
    "    estimator=sklearn_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "        \"pipeline\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"pipeline\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Create the `SKLearnModel` abstraction\n",
    "\n",
    "As another step, we create a SageMaker `SKLearnModel` object to wrap the model artifact, and associate it with a separate SageMaker prebuilt SKLearn inference container to potentially use later for validation and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearnModel\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "model = SKLearnModel(\n",
    "    entry_point=\"training.py\",\n",
    "    framework_version=framework_version,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "inputs_model = CreateModelInput(instance_type=batch_instance_type)\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"scheduled-pipeline-sklearn-createmodel\",\n",
    "    model=model,\n",
    "    inputs=inputs_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and execute the Pipeline\n",
    "\n",
    "With all the pipeline steps now defined, we can define the pipeline itself as a `Pipeline` object comprising a series of those steps. Parallelized and conditional steps also are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = \"scheduled-pipeline-sklearn\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        input_data,\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        training_instance_count,\n",
    "        batch_instance_type,\n",
    "        batch_instance_count,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_create_model],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline\n",
    "\n",
    "Once our model has been trained and the model abstraction created, we can now define the pipeline that we will use to schedule our inferences. This is a very simple pipeline, only composed of one step, the `TransformStep`. In fact, this pipeline is used specifically to wrap around Batch Transform in order to run it on a schedule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "\n",
    "# model name\n",
    "model_name = ParameterString(name=\"ModelName\")\n",
    "\n",
    "# test data\n",
    "test_data = ParameterString(name=\"TestData\")\n",
    "output_path = ParameterString(\n",
    "    name=\"OutputPath\", default_value=f\"s3://{bucket}/{prefix}/output/\"\n",
    ")\n",
    "\n",
    "# batch inference step parameters\n",
    "batch_instance_type = ParameterString(\n",
    "    name=\"BatchInstanceType\", default_value=\"ml.c5.xlarge\"\n",
    ")\n",
    "batch_instance_count = ParameterInteger(name=\"BatchInstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearnModel\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_count=batch_instance_count,\n",
    "    instance_type=batch_instance_type,\n",
    "    base_transform_job_name=\"sklearn-transformer\",\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.inputs import TransformInput\n",
    "\n",
    "transformer_step = TransformStep(\n",
    "    name=\"scheduled-pipeline-sklearn-transformer\",\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(data=test_data, content_type=\"text/csv\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = \"sklearn-scheduled-inference\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        model_name,\n",
    "        test_data,\n",
    "        output_path,\n",
    "        batch_instance_type,\n",
    "        batch_instance_count,\n",
    "    ],\n",
    "    steps=[transformer_step],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import JSON\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "JSON(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(\n",
    "    role_arn=role,\n",
    "    description=\"A SM Pipeline to have scheduled inference with SM Batch Transform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"ModelName\": \"sklearn-pipeline-training-2021-06-17-13-53-49-328\",\n",
    "        \"TestData\": x_test_path,\n",
    "    }\n",
    ")\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://$bucket/$prefix/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now schedule it at specific moments of the day with [EventBridge Scheduled rule](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html) or [CloudWatch Rule](https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Scheduled-Rule.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
