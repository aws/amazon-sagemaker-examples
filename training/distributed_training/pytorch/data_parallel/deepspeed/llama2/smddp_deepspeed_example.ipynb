{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Llama2 model using SageMaker Distributed Data Parallel Library (SMDDP) and DeepSpeed\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, we will show how to train or fine-tune the new [Llama2-7B](https://huggingface.co/meta-llama/Llama-2-7b) model.  We will use [DeepSpeed](https://www.deepspeed.ai/training/) ZeRO stage 3, a sharded data parallelism technique.  Using DeepSpeed will allow us to reap the benefits of data parallelism and efficiently train over a vast datest, while dealing with limited GPU memory.  \n",
    "\n",
    "In addition, we will utilize the **SMDDP library**, a handy SageMaker feature which accelerates training by speeding up GPU communication between p4d/p4de instance types.  The SMDDP Library is compatible with ml.p4d.24xlarge and ml.p4de.24xlarge instances.  For this example, we will use 2 ml.p4d.24xlarge instances, which come with 8 NVIDIA A100 40GB GPUs. \n",
    "\n",
    "*Note 1: For the purpose of this example, we will use a dummy synthetic dataset to avoid dealing with an access token required to initialize a Llama2 tokenizer.  This example can be easily modified to supply your own dataset if you own a Llama2 access token.*\n",
    "\n",
    "*Note 2: The SMDDP library for accelerated sharded data parallel training is compatible with deep learning containers from PyTorch 2.0 onwards.  Ensure you are using PyTorch >=2.0 for this example.*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files ##\n",
    "\n",
    "All training and helper scripts are stored in the `code/` folder:\n",
    "* `dsconfig.json` - DeepSpeed config file \n",
    "* `requirements.txt` - Dependencies for this example that will be installed on container when training job is launched.\n",
    "* `train.py` - Entry point training script\n",
    "* `utils.py` - Defines dummy dataset and constructs dataloaders for the training job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How optimized GPU communication is enabled with SMDDP in DeepSpeeed\n",
    "Enabling the SMDDP library in an existing DeepSpeed training script is seamless.  As shown in `train.py`, the only code modifications required are:\n",
    "* Importing the library: `import smdistributed.dataparallel.torch.torch_smddp`\n",
    "* Creating the process group with `\"smddp\"` backend: `deepspeed.init_distributed(dist_backend=\"smddp\")`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you are going to use Sagemaker in a local environment, you need access to an IAM Role with the required permissions for Sagemaker. You can find more about it [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sagemaker_session.boto_region_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that SageMaker by default uses the latest [AWS Deep Learning Container (DLC)](https://aws.amazon.com/machine-learning/containers/), but if you want to use your own DLC, you can set the `use_ecr_image` flag to `True` and set the `ecr_image` variable. Also note that if using FSx when launching the SageMaker notebook instance, you will need to use the same `subnet` and `security_group_config`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ecr_image = False\n",
    "use_fsx = False\n",
    "kwargs = {}\n",
    "\n",
    "if use_ecr_image:\n",
    "    ecr_image = \"<ECR_IMAGE_URI>\"\n",
    "    kwargs[\"image_uri\"] = ecr_image\n",
    "\n",
    "if use_fsx:\n",
    "    subnet_config = [\"<SUBNET_CONFIG_ID>\"]\n",
    "    security_group_config = [\"<SECURITY_GROUP_CONFIG>\"]\n",
    "    kwargs[\"subnets\"] = subnet_config\n",
    "    kwargs[\"security_group_ids\"] = security_group_config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Training Job\n",
    "\n",
    "We will now set the hyperparameters and define the estimator object for our training job.  Since we are using DeepSpeed, we must provide a DeepSpeed config JSON file, which is located in the `code/` folder. \n",
    "\n",
    " We will  use the `PyTorch` estimator class and configure it to use the `torch_distributed` distribution, which will launch a the training job using `torchrun`.  This is a popular launcher for PyTorch-based distributed training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"model_id\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"bf16\": True,\n",
    "    \"optimizer\": \"adamw_torch\",\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"epochs\": 1,\n",
    "    \"max_steps\": 50,\n",
    "    \"deepspeed_config\": \"dsconfig.json\",\n",
    "}\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    max_run=1800,\n",
    "    job_name=\"llama2-training-smddp\",\n",
    "    role=role,\n",
    "    source_dir=\"./code\",\n",
    "    framework_version=\"2.0.1\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    disable_output_compression=True,\n",
    "    hyperparameters=hyperparameters,\n",
    "    keep_alive_period_in_seconds=600,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    debugger_hook_config=False,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the traning job \n",
    "We can now start our training job, with the `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "estimator.fit(wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Output\n",
    "You should see output similar to the following in the SageMaker job logs after initialization and once training begins:\n",
    "\n",
    "```Processing training batch 0\n",
    "Processing training batch 1\n",
    "******epoch=0: train_ppl=tensor(71973.6484, device='cuda:0') train_loss=tensor(11.1841, device='cuda:0')******\n",
    "Performing validation on training batch 1\n",
    "Performing validation on training batch 1\n",
    "*******epoch=0: eval_ppl=tensor(70934.4062, device='cuda:0') eval_loss=tensor(11.1695, device='cuda:0')*******\n",
    "Training done!`\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If DeepSpeed 0.9.2 pip installation fails, you may need to first install `Pydantic==1.10.13` in your docker image*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate the warm pool cluster if no longer needed\n",
    "\n",
    "Once finished experimenting, you can terminate the warm pool cluster to reduce billed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.update_training_job(\n",
    "    estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\": 0}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/training|distributed_training|pytorch|data_parallel|deepspeed|llama2|smddp_deepspeed_example.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
