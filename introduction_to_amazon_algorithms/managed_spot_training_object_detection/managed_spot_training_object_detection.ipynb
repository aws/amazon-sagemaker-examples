{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection using managed spot training\n",
    "\n",
    "This notebook shows how to use [Amazon SageMaker managed spot training](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html) to run training jobs at potentially lower cost. Managed spot training uses [Amazon EC2 Spot instances](https://aws.amazon.com/ec2/spot/) and manages the Spot interruptions on your behalf.\n",
    "\n",
    "To highlight the differences between on-demand and Spot instances, this notebook is the same as [Amazon SageMaker Object Detection using the RecordIO format](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb), but has been updated to use managed spot training. For a full description of the ML use case and how it is being solved, see the original notebook.\n",
    "\n",
    "## Setup\n",
    "\n",
    "See [Amazon SageMaker Object Detection using the RecordIO format](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb) for a description of the code.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This notebook has been tested with:\n",
    "* SageMaker Python SDK 1.72.1\n",
    "* Python 3.6\n",
    "* Kernel: conda_mxnet_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"DEMO-ObjectDetection\"\n",
    "training_image = sagemaker.image_uris.retrieve(\n",
    "    region=sess.boto_region_name, framework=\"object-detection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and prepare data\n",
    "This notebook downloads and uses the Pascal VOC dataset, which has the following database usage rights:\n",
    "> The VOC data includes images obtained from the Flickr website. Use of these images must respect the corresponding terms of use: \n",
    "> * Flickr terms of use (https://www.flickr.com/help/terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!wget -P /tmp http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "!wget -P /tmp http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "!wget -P /tmp http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "\n",
    "# Extract the data.\n",
    "!tar -xf /tmp/VOCtrainval_11-May-2012.tar && rm /tmp/VOCtrainval_11-May-2012.tar\n",
    "!tar -xf /tmp/VOCtrainval_06-Nov-2007.tar && rm /tmp/VOCtrainval_06-Nov-2007.tar\n",
    "!tar -xf /tmp/VOCtest_06-Nov-2007.tar && rm /tmp/VOCtest_06-Nov-2007.tar\n",
    "\n",
    "# Convert data into RecordIO\n",
    "!python tools/prepare_dataset.py --dataset pascal --year 2007,2012 --set trainval --target VOCdevkit/train.lst\n",
    "!rm -rf VOCdevkit/VOC2012\n",
    "!python tools/prepare_dataset.py --dataset pascal --year 2007 --set test --target VOCdevkit/val.lst --no-shuffle\n",
    "!rm -rf VOCdevkit/VOC2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to Amazon Simple Storage Service (Amazon S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the RecordIO files to train and validation channels\n",
    "train_channel = prefix + \"/train\"\n",
    "validation_channel = prefix + \"/validation\"\n",
    "\n",
    "sess.upload_data(path=\"VOCdevkit/train.rec\", bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path=\"VOCdevkit/val.rec\", bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = \"s3://{}/{}\".format(bucket, train_channel)\n",
    "s3_validation_data = \"s3://{}/{}\".format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managed spot training\n",
    "\n",
    "Managed spot training is controlled by two arguments to the `sagemaker.estimator.Estimator` constructor:\n",
    "\n",
    "* `train_use_spot_instances`: Set to `True` to use Spot instances for training jobs.\n",
    "* `train_max_wait`: Represents the amount of time to wait for a Spot instance to become available. Be aware that some Spot instance types take longer to get. You are charged only for actual compute time spent once Spot instances have been acquired, and not for time spent waiting for Spot instances to become available.\n",
    "\n",
    "Note that `train_max_wait` can be set only if `train_use_spot_instances` is `True` and **must** be greater than or equal to `train_max_run`.\n",
    "\n",
    "Toggle `train_use_spot_instances` in the following code to see the effect of running the same job using on-demand instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use_spot_instances = True\n",
    "train_max_run = 3600\n",
    "train_max_wait = 3600 if train_use_spot_instances else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Train the object detector by creating a `sagemaker.estimator.Estimator` object and launching the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "od_model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    volume_size=50,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    "    use_spot_instances=train_use_spot_instances,\n",
    "    max_run=train_max_run,\n",
    "    max_wait=train_max_wait,\n",
    ")\n",
    "\n",
    "od_model.set_hyperparameters(\n",
    "    base_network=\"resnet-50\",\n",
    "    use_pretrained_model=1,\n",
    "    num_classes=20,\n",
    "    mini_batch_size=32,\n",
    "    epochs=1,\n",
    "    learning_rate=0.001,\n",
    "    lr_scheduler_step=\"3,6\",\n",
    "    lr_scheduler_factor=0.1,\n",
    "    optimizer=\"sgd\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005,\n",
    "    overlap_threshold=0.5,\n",
    "    nms_threshold=0.45,\n",
    "    image_shape=300,\n",
    "    label_width=350,\n",
    "    num_training_samples=16551,\n",
    ")\n",
    "\n",
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}\n",
    "\n",
    "od_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Savings\n",
    "At the end of the job output, two lines are printed:\n",
    "\n",
    "* `Training seconds: X` : The actual compute time spent on the training job.\n",
    "* `Billable seconds: Y` : The time you will be billed for after Spot discounting is applied.\n",
    "\n",
    "When `train_use_spot_instances` is `True`, you should see a notable difference between training and billable seconds. This shows the cost savings when managed spot training is used, and is summarized in the final output:\n",
    "\n",
    "* `Managed Spot Training savings: (1-Y/X)*100 %`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
