{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872c9a95-4bed-40b5-a76a-2cf5982c1153",
   "metadata": {},
   "source": [
    "# Serve OpenAssistant Open-Assistant SFT-1 12B Model on Amazon SageMaker using LMI (Large Model Inference) DJL-based container\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "This notebook focuses on deploying the [`OpenAssistant/oasst-sft-1-pythia-12b`](https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b) HuggingFace model to a SageMaker Endpoint for a text generation task. In this example, you will use the SageMaker-managed [LMI (Large Model Inference)](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html) Docker image as inference image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/).\n",
    "\n",
    "You will successively deploy the `OpenAssistant/oasst-sft-1-pythia-12b` model twice on a `ml.g5.12xlarge` GPU instance (4 devices), once using the DeepSpeed inference handler, once using the HuggingFace Accelerate inference handler. This will allow you to compare the latency and the quality of the text generated by these two solutions.\n",
    "\n",
    "**Notices:**\n",
    "* Make sure that the `ml.g5.12xlarge` instance type is available in your AWS Region.\n",
    "* Make sure that the value of your \"ml.g5.12xlarge for endpoint usage\" Amazon SageMaker service quota allows you to deploy one Endpoint using this instance type.\n",
    "\n",
    "This notebook leverages the [`sagemaker` Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html) to abstract away the management of as many resources and configuration as we can, hence demonstrating that the deployment of LLMs to SageMaker can be performed with great simplicity and minimal amount of code.\n",
    "\n",
    "### License agreement\n",
    "* This model and the dataset it has been trained on are both under the [Apache 2.0](https://huggingface.co/models?license=license:apache-2.0) license.\n",
    "* This notebook is a sample notebook and not intended for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa8856-71c9-4333-b734-bd28b0071e08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html#)\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html), DJL support requires versions greater than 2.136.0 \n",
    "* HuggingFace [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index)\n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869f0d19-e197-43ff-9b6a-9fb31d84816a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.12.0 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.0a6 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.12.0 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.1.0 which is incompatible.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0 which is incompatible.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.2 which is incompatible.\n",
      "awscli 1.27.111 requires botocore==1.29.111, but you have botocore 1.29.142 which is incompatible.\n",
      "awscli 1.27.111 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker huggingface_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ddf083-868f-4775-b2cb-2ff34dd5f8ca",
   "metadata": {},
   "source": [
    "### Imports & global variables assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1697d3bb-040b-452e-a79e-9b17c717cca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import boto3\n",
    "import huggingface_hub\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9db6a2-2510-4844-b73f-fe050f52436b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SM_DEFAULT_EXECUTION_ROLE_ARN = sagemaker.get_execution_role()\n",
    "SM_SESSION = sagemaker.session.Session()\n",
    "SM_ARTIFACT_BUCKET_NAME = SM_SESSION.default_bucket()\n",
    "DEFAULT_BUCKET_PREFIX = SM_SESSION.default_bucket_prefix\n",
    "\n",
    "REGION_NAME = SM_SESSION._region_name\n",
    "S3_CLIENT = boto3.client(\"s3\", region_name=REGION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b34d1c-cf7a-4f7e-8091-215c52c3aba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HOME_DIR = os.environ[\"HOME\"]\n",
    "\n",
    "# HuggingFace local model storage\n",
    "HF_LOCAL_CACHE_DIR = Path(HOME_DIR) / \".cache\" / \"huggingface\" / \"hub\"\n",
    "HF_LOCAL_DOWNLOAD_DIR = Path.cwd() / \"model_repo\"\n",
    "HF_LOCAL_DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Inference code local storage\n",
    "SOURCE_DIR = Path.cwd() / \"code\"\n",
    "SOURCE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Selected HuggingFace model\n",
    "HF_HUB_MODEL_NAME = \"OpenAssistant/oasst-sft-1-pythia-12b\"\n",
    "\n",
    "# HuggingFace remote model storage (Amazon S3)\n",
    "HF_MODEL_KEY_PREFIX = f\"hf-large-model-djl/{HF_HUB_MODEL_NAME}\"\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if DEFAULT_BUCKET_PREFIX:\n",
    "    HF_MODEL_KEY_PREFIX = f\"{DEFAULT_BUCKET_PREFIX}/{HF_MODEL_KEY_PREFIX}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7ba34-d9e0-4919-a6de-771f7d04d33b",
   "metadata": {},
   "source": [
    "### Storage utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4ad83d-62b5-4dfc-b800-cac6c61368ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_s3_objects(bucket: str, key_prefix: str) -> List[Dict[str, Any]]:\n",
    "    paginator = S3_CLIENT.get_paginator(\"list_objects\")\n",
    "    operation_parameters = {\"Bucket\": bucket, \"Prefix\": key_prefix}\n",
    "    page_iterator = paginator.paginate(**operation_parameters)\n",
    "    return [obj for page in page_iterator for obj in page[\"Contents\"]]\n",
    "\n",
    "\n",
    "def delete_s3_objects(bucket: str, keys: str) -> None:\n",
    "    S3_CLIENT.delete_objects(Bucket=bucket, Delete={\"Objects\": [{\"Key\": key} for key in keys]})\n",
    "\n",
    "\n",
    "def get_local_model_cache_dir(hf_model_name: str) -> str:\n",
    "    for dir_name in os.listdir(HF_LOCAL_CACHE_DIR):\n",
    "        if dir_name.endswith(hf_model_name.replace(\"/\", \"--\")):\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"Could not find HF local cache directory for model {hf_model_name}\")\n",
    "    return HF_LOCAL_CACHE_DIR / dir_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbea82-c7b8-4ebd-bb9a-bc443c009f0e",
   "metadata": {},
   "source": [
    "### Inference utility functions\n",
    "Prompting the model requires marking the beginning and the end of the prompt with [special and model-specific tokens](https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5#prompting). The following inference helper functions are used for all deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538f2c6c-f90b-45fb-a4fb-8ea31f7914a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model-specific tokens\n",
    "PROMPT_BOS_TOKEN = \"<|prompter|>\"\n",
    "PROMPT_EOS_TOKEN = \"<|endoftext|><|assistant|>\"\n",
    "\n",
    "\n",
    "def decorate_prompt(prompt: str) -> str:\n",
    "    return f\"{PROMPT_BOS_TOKEN}{prompt}{PROMPT_EOS_TOKEN}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66276d81-05b6-4c06-9570-1e1047a68930",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Model upload to Amazon S3\n",
    "Models served by a LMI container can be downloaded to the container in different ways:\n",
    "* Like all the SageMaker Inference containers, having the container to download the model from Amazon S3 as a single `model.tar.gz` file. In the case of LLMs, this approach is discouraged since downloading and decompression times can become unreasonably high.\n",
    "* Having the container to download the model directly from the HuggingFace Hub for you. This option may involve high download times too.\n",
    "* Having the container to download the uncompressed model from Amazon S3 with maximal throughput by using the [`s5cmd`](https://github.com/peak/s5cmd) utility. This option is specific to LMI containers and is the recommended one. It requires however, that the model has been previously uploaded to a S3 Bucket. \n",
    "\n",
    "In this section, you will:\n",
    "1. Download the model from the HuggingFace Hub to your local host,\n",
    "2. Upload the downloaded model to a S3 Bucket. This notebook uses the SageMaker's default regional Bucket. Feel free to upload the model to the Bucket of your choice by modifying the `SM_ARTIFACT_BUCKET_NAME` global variable accordingly.\n",
    "\n",
    "Each operation takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b12738-ce43-4a02-bbf8-a3c4e591a762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5643edc79f7740bb838f557d13986222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b0b14b647a432fb38add7e48786d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/47.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e32b76417c472087024777b25fc4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/303 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17c26327a624a4d9160fd9800e623c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437cab6efb894086979b7cc4eff60f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)452a1/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec422f03a5844c1bed7ab9e41168535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7de452a1/config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e29bd7b1e944cab5c8f45cdef585b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd222f197df45859a6ab41eff17c947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/521 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9c0939daf54f8087d6e9d544b0711d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e8f4ad462c4b66909c955fa0d20ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/4.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=HF_HUB_MODEL_NAME,\n",
    "    revision=\"main\",\n",
    "    local_dir=HF_LOCAL_DOWNLOAD_DIR,\n",
    "    local_dir_use_symlinks=\"auto\",  # Files larger than 5MB are actually symlinked to the local HF cache\n",
    "    allow_patterns=[\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\"],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da993988-8fa4-4b10-89c3-fe7688836822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifacts have been successfully uploaded to: s3://sagemaker-eu-west-1-893516550009/hf-large-model-djl/OpenAssistant/oasst-sft-1-pythia-12b\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = SM_SESSION.upload_data(\n",
    "    path=HF_LOCAL_DOWNLOAD_DIR.as_posix(),\n",
    "    bucket=SM_ARTIFACT_BUCKET_NAME,\n",
    "    key_prefix=HF_MODEL_KEY_PREFIX,\n",
    ")\n",
    "print(f\"Model artifacts have been successfully uploaded to: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c389f-c15a-4036-a511-4791db9ef9bf",
   "metadata": {},
   "source": [
    "The `huggingface_hub.snapshot_download` function downloaded the model repository to a cache located in your home directory. Downloaded files were duplicated in the target local download directory. Large files (larger than 5 MB) were not duplicated however but simply symlinked. Still, uncompressed LLM artifacts consume disk space. The two following cells removes the downloaded files from your local host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc8be488-d83d-40af-9d04-de4f016ae667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HF model artifacts from the local download directory\n",
    "shutil.rmtree(HF_LOCAL_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194217d0-5712-466c-992c-4e698f8f8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HF model artifacts from the local HF cache directory\n",
    "hf_local_cache_dir = get_local_model_cache_dir(hf_model_name=HF_HUB_MODEL_NAME)\n",
    "shutil.rmtree(hf_local_cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809d624-aa33-48a7-89dc-ffa73c79f0a9",
   "metadata": {},
   "source": [
    "## 2. Deployment to a SageMaker Endpoint using a SageMaker LMI Docker image\n",
    "Start up of LLM inference containers can last for longer than smaller model mainly because of longer model downloading and loading times. Timeout values need to be increased accordingly from their default values. Each endpoint deployment takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36cda14-a3d9-43c8-b270-20360e9f0052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTAINER_STARTUP_CONFIGURATION = {\n",
    "    \"model_data_download_timeout\": 5 * 60,  # in seconds\n",
    "    \"container_startup_health_check_timeout\": 7 * 60,  # in seconds\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babfb853-39a3-48ed-8955-5f6a5498a101",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1. Inference using the DeepSpeed handler\n",
    "In this section, you will deploy the `OpenAssistant/oasst-sft-1-pythia-12b` model to a SageMaker Endpoint consisting of a single `ml.g5.12xlarge` instance. The inference engine used by the DJL Serving stack is DeepSpeed. The model will be partitioned over all (i.e. 4) the available GPUs. Chosen precision is FP16. See [this section](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html) of the documentation for more information about LMI-specific configuration options. To each option corresponds a dedicated argument in the constructor of the DJL `Model` classes.\n",
    "\n",
    "The model server configuration is generated by the `sagemaker.djl_inference.DeepSpeedModel` class from the argument we pass to its constructor. The DJL classes from the `sagemaker` SDK still allow customization (custom Python handler, Python dependencies and model server configuration), see the [dedicated section](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/using_djl.html#inference-code-and-model-server-properties) of the documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8004fb-4e20-4808-8dde-d564a683f2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.djl_inference import DeepSpeedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64a6d320-b879-4d51-a87c-3742c261934c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deepspeed_model = DeepSpeedModel(\n",
    "    model_id=MODEL_ID,\n",
    "    djl_version=\"0.22.1\",\n",
    "    role=SM_DEFAULT_EXECUTION_ROLE_ARN,\n",
    "    tensor_parallel_degree=4,\n",
    "    low_cpu_mem_usage=True,\n",
    "    task=\"text-generation\",\n",
    "    dtype=\"fp16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d253f3f0-1221-4d9b-83fd-77ee02fac813",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "deepspeed_predictor = deepspeed_model.deploy(\n",
    "    instance_type=\"ml.g5.12xlarge\", initial_instance_count=1, **CONTAINER_STARTUP_CONFIGURATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09844ca0-24e4-4200-9ca1-4df9be701715",
   "metadata": {},
   "source": [
    "**Notice:** The structure of the `data` dictionary is set by the handler script. Expected structure is:\n",
    "```python\n",
    "{\n",
    "\"inputs\": [\"prompt1\", \"prompt2\", ...],\n",
    "\"parameters\": {\"param1\": \"value1\", ...}\n",
    "}\n",
    "```\n",
    "Notice that the handler can handle batches of prompts. In this notebook's examples, you only send one prompt at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19f2cc47-3917-4d09-9ceb-bc19aa257f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.76 ms, sys: 6.72 ms, total: 14.5 ms\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompts = [\"I am currently in Paris. What are my travel options to Berlin?\"]\n",
    "inference_parameters = {\"max_new_tokens\": 128, \"do_sample\": False}\n",
    "\n",
    "response = deepspeed_predictor.predict(\n",
    "    data={\n",
    "        \"inputs\": [decorate_prompt(prompt) for prompt in prompts],\n",
    "        \"parameters\": inference_parameters,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d45869b-a365-4b3a-b39b-f4ae174813db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "PROMPT:\n",
      "I am currently in Paris. What are my travel options to Berlin?\n",
      "GENERATED TEXT:\n",
      "If you are in Paris, you can take a train to Berlin. The fastest way is to take the high speed train, the Eurostar. It will take around 4 hours and costs around €100. Or else you can take a bus.\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "for prompt, generated_text in zip(prompts, response):\n",
    "    print(\"#\" * 100)\n",
    "    text = generated_text[\"generated_text\"][len(prompt) :]\n",
    "    print(f\"PROMPT:\\n{prompt}\\nGENERATED TEXT:\\n{text}\")\n",
    "else:\n",
    "    print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ced398-6387-41d2-b808-7328d1e5b689",
   "metadata": {},
   "source": [
    "The following cells destroy the Endpoint and all associated resources (i.e. the EndpointConfig and the Model objects) created by the call to `Model.deploy`.\n",
    "You may delay this step until you are done comparing the two inference engines. Given the relatively high cost of the `ml.g5.12xlarge` instances, we strongly advocate for a cautious management of the Endpoints they power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fb2a0f8-5413-40e3-950e-54d3be88efce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean-up\n",
    "deepspeed_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "deepspeed_model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68712ce8-9940-40b0-8dec-c5faac2df72a",
   "metadata": {},
   "source": [
    "### 2.2. Inference using the HuggingFace Accelerate handler\n",
    "Like in the preceding section, you deploy the `OpenAssistant/oasst-sft-1-pythia-12b` model to a SageMaker Endpoint consisting of a single `ml.g5.12xlarge` instance. The model is partitioned over all (i.e. 4) the available GPUs and chosen precision is FP16. The only difference is that you use the HuggingFace Accelerate handler as inference engine (referred as the `Python` engine in the [DJL Serving general settings](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html)).\n",
    "\n",
    "Like in the preceding section, we use the dedicated `Model` class from the `sagemaker` Python SDK, namely the `HuggingFaceAccelerateModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9735dcd6-871c-49ff-b450-ea19a517d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.djl_inference import HuggingFaceAccelerateModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81b497f2-d15b-407a-8609-bd9a9a0c3bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_accelerate_model = HuggingFaceAccelerateModel(\n",
    "    model_id=MODEL_ID,\n",
    "    djl_version=\"0.22.1\",\n",
    "    role=SM_DEFAULT_EXECUTION_ROLE_ARN,\n",
    "    number_of_partitions=4,\n",
    "    device_map=\"balanced_low_0\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    task=\"text-generation\",\n",
    "    dtype=\"fp16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba19af85-3658-43c8-831a-ec662db9794f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "hf_accelerate_predictor = hf_accelerate_model.deploy(\n",
    "    instance_type=\"ml.g5.12xlarge\", initial_instance_count=1, **CONTAINER_STARTUP_CONFIGURATION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "241be687-a8b1-41cd-98f0-5c50ce90e5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 ms, sys: 0 ns, total: 18.2 ms\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompts = [\"I am currently in Paris. What are my travel options to Berlin?\"]\n",
    "inference_parameters = {\"max_new_tokens\": 128, \"do_sample\": False}\n",
    "\n",
    "response = hf_accelerate_predictor.predict(\n",
    "    data={\n",
    "        \"inputs\": [decorate_prompt(prompt) for prompt in prompts],\n",
    "        \"parameters\": inference_parameters,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d816072-002c-405f-b99c-b0d92cea1b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "PROMPT:\n",
      "I am currently in Paris. What are my travel options to Berlin?\n",
      "GENERATED TEXT:\n",
      "If you are in Paris, you have several options to travel to Berlin. The most common way to travel from Paris to Berlin is by plane. You can fly from Paris to Berlin with a number of airlines, such as Air France, Lufthansa, or Germanwings. Alternatively, you can take a train from Paris to Berlin. The train ride can be long, but it is a convenient way to travel. You can also take a bus from Paris to Berlin. This option may be more time-consuming, but it can be a good way to get a feel for the country. Finally, you can drive from Paris\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "for prompt, generated_text in zip(prompts, response):\n",
    "    print(\"#\" * 100)\n",
    "    text = generated_text[\"generated_text\"][len(prompt) :]\n",
    "    print(f\"PROMPT:\\n{prompt}\\nGENERATED TEXT:\\n{text}\")\n",
    "else:\n",
    "    print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8911718-072a-47f4-9ea4-8976c69b5d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean-up\n",
    "hf_accelerate_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "hf_accelerate_model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e74f0-c489-4f2b-b468-f94caa2f1305",
   "metadata": {},
   "source": [
    "## 3. Clean-up\n",
    "At this stage:\n",
    "* All your Endpoint are supposed to be deleted, along with the EndpointConfig and Model resources they were associated with,\n",
    "* You have freed the disk space of your local host from the large model artifacts downloaded from the HuggingFace Hub.\n",
    "\n",
    "The only remaining cleanup task consist of removing the model artifacts from Amazon S3. This is what performs the next and last cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e601e69-6702-4514-b11e-48b9bd5ec55f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove HF model artifacts from S3\n",
    "hf_s3_objects = list_s3_objects(bucket=SM_ARTIFACT_BUCKET_NAME, key_prefix=HF_MODEL_KEY_PREFIX)\n",
    "hf_s3_objects_keys = [obj[\"Key\"] for obj in hf_s3_objects]\n",
    "delete_s3_objects(bucket=SM_ARTIFACT_BUCKET_NAME, keys=hf_s3_objects_keys)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
