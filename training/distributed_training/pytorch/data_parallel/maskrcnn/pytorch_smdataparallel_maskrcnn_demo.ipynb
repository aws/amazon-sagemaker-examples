{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed data parallel MaskRCNN training with PyTorch and SMDataParallel\n",
    "\n",
    "SMDataParallel is a new capability in Amazon SageMaker to train deep learning models faster and cheaper. SMDataParallel is a distributed data parallel training framework for PyTorch, TensorFlow, and MXNet.\n",
    "\n",
    "This notebook example shows how to use SMDataParallel with PyTorch(version 1.8.0) on [Amazon SageMaker](https://aws.amazon.com/sagemaker/) to train a MaskRCNN model on [COCO 2017 dataset](https://cocodataset.org/#home) using [Amazon FSx for Lustre file-system](https://aws.amazon.com/fsx/lustre/) as data source.\n",
    "\n",
    "The outline of steps is as follows:\n",
    "\n",
    "1. Stage COCO 2017 dataset in [Amazon S3](https://aws.amazon.com/s3/)\n",
    "2. Create Amazon FSx Lustre file system and import data into the file system from S3\n",
    "3. Build Docker training image and push it to [Amazon ECR](https://aws.amazon.com/ecr/)\n",
    "4. Configure data input channels for SageMaker\n",
    "5. Configure hyper-parameters\n",
    "6. Define training metrics\n",
    "7. Define training job, set distribution strategy to SMDataParallel and start training\n",
    "\n",
    "**NOTE:**  With large training dataset, we recommend using [Amazon FSx](https://aws.amazon.com/fsx/) as the input file system for the SageMaker training job. FSx file input to SageMaker significantly cuts down training start up time on SageMaker because it avoids downloading the training data each time you start the training job (as done with S3 input for SageMaker training job) and provides good data read throughput.\n",
    "\n",
    "\n",
    "**NOTE:** This example requires SageMaker Python SDK v2.X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Initialize the notebook instance. Get the aws region, sagemaker execution role.\n",
    "\n",
    "The IAM role arn is used to give training and hosting access to your data. See the [Amazon SageMaker Roles](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with the appropriate full IAM role arn string(s). As described above, since we will be using FSx, please make sure to attach `FSx Access` permission to this IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sys\n",
    "\n",
    "! {sys.executable} -m pip install --upgrade sagemaker\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "fsx_client = boto3.client(\"fsx\")\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")\n",
    "\n",
    "subnets = [\n",
    "    \"<SUBNET_ID>\"\n",
    "]  # this will be used for FSx and will be where the training job runs. Example: subnet-0f9XXXX\n",
    "security_group_ids = [\"<SECURITY_GROUP_ID>\"]  # Example: sg-03ZZZZZZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage COCO\n",
    "\n",
    "The following bash script will grab data from COCO, decompressing the data, and then send it to s3. This notebook uses FSx as a file source so you are going to put the data in a folder that you will setup to sync directly to FSx. This script will take ~20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!bash ./upload_coco2017_to_s3.sh {bucket} fsx_sync/train-coco/coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare SageMaker Training Images\n",
    "\n",
    "1. SageMaker by default uses the latest [Amazon Deep Learning Container Images (DLC)](https://github.com/aws/deep-learning-containers/blob/master/available_images.md) PyTorch training image. In this step, we use it as a base image and install additional dependencies required for training MaskRCNN model.\n",
    "2. In the Github repository https://github.com/HerringForks/DeepLearningExamples.git we have made PyTorch-SMDataParallel MaskRCNN training script available for your use. We will be installing the same on the training image.\n",
    "\n",
    "### Build and Push Docker Image to ECR\n",
    "\n",
    "Run the below command build the docker image and push it to ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"<ADD NAME OF REPO>\"  # Example: mask-rcnn-smdataparallel-sagemaker\n",
    "tag = \"<ADD TAG FOR IMAGE>\"  # Example: pt1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! chmod +x build_and_push.sh; bash build_and_push.sh {region} {image} {tag}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing FSx Input for SageMaker\n",
    "\n",
    "1. Download and prepare your training dataset on S3.\n",
    "2. Follow the steps listed here to create a FSx linked with your S3 bucket with training data - https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-fs-linked-data-repo.html. Make sure to add an endpoint to your VPC allowing S3 access.\n",
    "3. Follow the steps listed here to configure your SageMaker training job to use FSx https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/\n",
    "\n",
    "### Important Caveats\n",
    "\n",
    "1. You need use the same `subnet` and `vpc` and `security group` used with FSx when launching the SageMaker notebook instance. The same configurations will be used by your SageMaker training job.\n",
    "2. Make sure you set appropriate inbound/output rules in the `security group`. Specifically, opening up these ports is necessary for SageMaker to access the FSx file system in the training job. https://docs.aws.amazon.com/fsx/latest/LustreGuide/limit-access-security-groups.html\n",
    "3. Make sure `SageMaker IAM Role` used to launch this SageMaker training job has access to `AmazonFSx`.\n",
    "\n",
    "You also can automatically create a FSx file system with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use boto3 to create FSx\n",
    "\n",
    "fsx_response = fsx_client.create_file_system(\n",
    "    FileSystemType=\"LUSTRE\",\n",
    "    StorageCapacity=1200,\n",
    "    StorageType=\"SSD\",\n",
    "    SubnetIds=subnets,\n",
    "    SecurityGroupIds=security_group_ids,\n",
    "    Tags=[\n",
    "        {\"Key\": \"Name\", \"Value\": \"COCO-storage\"},\n",
    "    ],\n",
    "    LustreConfiguration={\n",
    "        \"WeeklyMaintenanceStartTime\": \"7:03:00\",\n",
    "        \"ImportPath\": f\"s3://{bucket}/fsx_sync/\",  # where FSx will import data from in s3, can do entire bucket or a specific folder\n",
    "        \"ImportedFileChunkSize\": 1024,\n",
    "        \"DeploymentType\": \"PERSISTENT_1\",  # |'SCRATCH_1' |'SCRATCH_2' # PERSISTENT means the storage in FSx will be persistent, SCRATCH indicates the storage is temporary\n",
    "        \"AutoImportPolicy\": \"NEW\",  # 'NONE'| |'NEW_CHANGED' # this policy is how often data will be imported to FSx from S3\n",
    "        \"PerUnitStorageThroughput\": 200,  # this is specific to PERSISTENT storage, not required for temporary\n",
    "    },\n",
    ")\n",
    "\n",
    "fsx_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "If we are running our model in a VPC and restrict outside internet access, we'll need to download our base model ahead of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/detectron/ImageNetPretrained/MSRA/R-50.pkl\n",
    "!aws s3 cp R-50.pkl s3://{bucket}/pretrained_weights/R-50.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training metrics\n",
    "\n",
    "To get more information on your training job, you define some algorithm metrics. SageMaker will scrape the logs from the training job and render them in the training job console (and store them in SageMaker Experiments). The metrics defined are pretty standard, you just need to define the regex to find them, feel free to define your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {\"Name\": \"loss\", \"Regex\": \".*loss:\\s([0-9\\\\.]+)\\s*\"},\n",
    "    {\"Name\": \"loss_classifier\", \"Regex\": \".*loss_cls:\\s([0-9\\\\.]+)\\s*\"},\n",
    "    {\"Name\": \"loss_box_reg\", \"Regex\": \".*loss_box_reg:\\s([0-9\\\\.]+)\\s*\"},\n",
    "    {\"Name\": \"loss_mask\", \"Regex\": \".*loss_mask:\\s([0-9\\\\.]+)\\s*\"},\n",
    "    {\"Name\": \"loss_objectness\", \"Regex\": \".*loss_objectness:\\s([0-9\\\\.]+)\\s*\"},\n",
    "    {\"Name\": \"loss_rpn_box_reg\", \"Regex\": \".*loss_rpn_box_reg:\\s([0-9\\\\.]+)\\s*\"},\n",
    "    {\"Name\": \"overall_training_speed\", \"Regex\": \".*Overall training speed:\\s([0-9\\\\.]+)\\s*\"},\n",
    "    {\"Name\": \"lr\", \"Regex\": \".*lr:\\s([0-9\\\\.]+)\\s*\"},\n",
    "    {\"Name\": \"iter\", \"Regex\": \".*iter:\\s([0-9\\\\.]+)\\s*\"},\n",
    "    {\"Name\": \"avg iter/s\", \"Regex\": \".*avg iter/s:\\s([0-9\\\\.]+)\\s*\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker PyTorch Estimator function options\n",
    "\n",
    "In the following code block, you can update the estimator function to use a different instance type, instance count, and distribution strategy. You're also passing in the training script you reviewed in the previous cell.\n",
    "\n",
    "**Instance types**\n",
    "\n",
    "SMDataParallel supports model training on SageMaker with the following instance types only:\n",
    "1. ml.p3.16xlarge\n",
    "1. ml.p3dn.24xlarge [Recommended]\n",
    "1. ml.p4d.24xlarge [Recommended]\n",
    "\n",
    "**Instance count**\n",
    "\n",
    "To get the best performance and the most out of SMDataParallel, you should use at least 2 instances, but you can also use 1 for testing this example.\n",
    "\n",
    "**Distribution strategy**\n",
    "\n",
    "Note that to use DDP mode, you update the `distribution` strategy, and set it to use `smdistributed dataparallel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p3dn.24xlarge\"  # Other supported instance type: ml.p3.16xlarge, ml.p4d.24xlarge\n",
    "instance_count = 2  # You can use 2, 4, 8 etc.\n",
    "docker_image = f\"{account}.dkr.ecr.{region}.amazonaws.com/{image}:{tag}\"  # YOUR_ECR_IMAGE_BUILT_WITH_ABOVE_DOCKER_FILE\n",
    "username = \"AWS\"\n",
    "job_name = f\"pytorch-smdataparallel-mrcnn-fsx-{int(time.time())}\"  # This job name is used as prefix to the sagemaker training job. Makes it easy for your look for your training job in SageMaker Training job console.\n",
    "file_system_id = fsx_response[\"FileSystem\"][\n",
    "    \"FileSystemId\"\n",
    "]  # FSx file system ID with your training dataset. Example: 'fs-0bYYYYYY'\n",
    "config_file = \"e2e_mask_rcnn_R_50_FPN_1x_16GPU_4bs.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"config-file\": config_file,\n",
    "    \"skip-test\": \"\",\n",
    "    \"seed\": 987,\n",
    "    \"dtype\": \"float16\",\n",
    "    \"spot_ckpt\": f\"s3://{bucket}/pretrained_weights/R-50.pkl\",  # this is where our script will look for existing weights to initialize the model backbone from.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"train_pytorch_smdataparallel_maskrcnn.py\",\n",
    "    role=role,\n",
    "    image_uri=docker_image,\n",
    "    source_dir=\".\",\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    framework_version=\"1.8.0\",\n",
    "    py_version=\"py36\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    subnets=subnets,\n",
    "    security_group_ids=security_group_ids,\n",
    "    debugger_hook_config=False,\n",
    "    # Training using SMDataParallel Distributed Training Framework\n",
    "    distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FSx Input for your SageMaker Training job\n",
    "\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "file_system_directory_path = \"YOUR_MOUNT_PATH_FOR_TRAINING_DATA\"  # NOTE: '/fsx/' will be the root mount path. Example: '/fsx/mask_rcnn/PyTorch'\n",
    "file_system_access_mode = \"ro\"\n",
    "file_system_type = \"FSxLustre\"\n",
    "train_fs = FileSystemInput(\n",
    "    file_system_id=file_system_id,\n",
    "    file_system_type=file_system_type,\n",
    "    directory_path=file_system_directory_path,\n",
    "    file_system_access_mode=file_system_access_mode,\n",
    ")\n",
    "data_channels = {\"train\": train_fs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit SageMaker training job\n",
    "estimator.fit(inputs=data_channels, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete FSx\n",
    "fsx_client.delete_file_system(FileSystemId=fsx_response[\"FileSystem\"][\"FileSystemId\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
