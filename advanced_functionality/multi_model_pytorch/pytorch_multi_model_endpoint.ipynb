{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a93ca6",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using PyTorch\n",
    "\n",
    "> *This notebook works well with SageMaker Studio kernel `Python 3 (Data Science)`, or SageMaker Notebook Instance kernel `conda_python3`*\n",
    "\n",
    "With [Amazon SageMaker multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html), customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of many models, which can be served from a common inference container, needs to be callable on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, a traditional endpoint is still the best choice.\n",
    "\n",
    "In some cases where the variable latency is tolerable, and cost optimization is more important, customers may also decide to use MMEs for A/B/n testing, in place of the more typical [production variant based strategy discussed here](https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/).\n",
    "\n",
    "To demonstrate how multi-model endpoints can be created and used, this notebook provides an example using models trained with the [SageMaker PyTorch framework container](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html). We'll take an A/B scenario for simplicity, training and deploying just two models to our endpoint.\n",
    "\n",
    "For other MME use cases, you can also refer to:\n",
    "\n",
    "- Segmented home value modelling examples with the [Scikit-Learn framework](https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/multi_model_sklearn_home_value), the [XGBoost pre-built algorithm](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_xgboost_home_value), and the [Linear Learner algorithm](https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/multi_model_linear_learner_home_value).\n",
    "- An [example with MXNet](https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/multi_model_bring_your_own) and corresponding [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html) on how to use MME with your own custom containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446701de",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [The example use case: MNIST](#The-example-use-case:-MNIST)\n",
    "1. [Train multiple models](#Train-multiple-models)\n",
    "1. [Check single-model deployment](#Check-single-model-deployment)\n",
    "1. [Create the Multi-Model Endpoint with the SageMaker SDK](#Create-the-Multi-Model-Endpoint-with-the-SageMaker-SDK)\n",
    "  1. [Deploy the Multi-Model Endpoint](#Deploy-the-Multi-Model-Endpoint)\n",
    "  1. [Dynamically deploying models to the endpoint](#Dynamically-deploying-models-to-the-endpoint)\n",
    "1. [Get predictions from the endpoint](#Get-predictions-from-the-endpoint)\n",
    "1. [Updating a model](#Updating-a-model)\n",
    "1. [Clean up](#Clean-up)\n",
    "\n",
    "Before these sections though, we'll load the libraries needed for this notebook and define some configurations you can edit - for where the data will be saved in [Amazon S3](https://aws.amazon.com/s3/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from tempfile import TemporaryFile\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from sagemaker.pytorch import PyTorch as PyTorchEstimator, PyTorchModel\n",
    "\n",
    "smsess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Configuration:\n",
    "bucket_name = smsess.default_bucket()\n",
    "prefix = \"mnist/\"\n",
    "output_path = f\"s3://{bucket_name}/{prefix[:-1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1b8ac",
   "metadata": {},
   "source": [
    "## The example use case: MNIST\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images.\n",
    "\n",
    "In this example, we download the MNIST data from a public S3 bucket and upload it to your default SageMaker bucket as selected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sample_data(\n",
    "    to_bucket: str,\n",
    "    to_prefix: str,\n",
    "    from_bucket: str = \"sagemaker-sample-files\",\n",
    "    from_prefix: str = \"datasets/image/MNIST\",\n",
    "    dataset: str = \"mnist-train\",\n",
    "):\n",
    "    DATASETS = {\n",
    "        \"mnist-train\": [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\"],\n",
    "        \"mnist-test\": [\"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"],\n",
    "    }\n",
    "\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"dataset '{dataset}' not in known set: {set(DATASETS.keys())}\")\n",
    "\n",
    "    if len(from_prefix) and not from_prefix.endswith(\"/\"):\n",
    "        from_prefix += \"/\"\n",
    "    if len(to_prefix) and not to_prefix.endswith(\"/\"):\n",
    "        to_prefix += \"/\"\n",
    "\n",
    "    s3client = boto3.client(\"s3\")\n",
    "    for key in DATASETS[dataset]:\n",
    "        # If you're in the same region as the source bucket, could consider copy_object() instead:\n",
    "        with TemporaryFile() as ftmp:\n",
    "            s3client.download_fileobj(from_bucket, f\"{from_prefix}{key}\", ftmp)\n",
    "            ftmp.seek(0)\n",
    "            s3client.upload_fileobj(ftmp, to_bucket, f\"{to_prefix}{key}\")\n",
    "\n",
    "\n",
    "train_prefix = f\"{prefix}data/train\"\n",
    "fetch_sample_data(to_bucket=bucket_name, to_prefix=train_prefix, dataset=\"mnist-train\")\n",
    "train_s3uri = f\"s3://{bucket_name}/{train_prefix}\"\n",
    "print(f\"Uploaded training data to {train_s3uri}\")\n",
    "\n",
    "test_prefix = f\"{prefix}data/test\"\n",
    "fetch_sample_data(to_bucket=bucket_name, to_prefix=test_prefix, dataset=\"mnist-test\")\n",
    "test_s3uri = f\"s3://{bucket_name}/{test_prefix}\"\n",
    "print(f\"Uploaded training data to {test_s3uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data:\")\n",
    "!aws s3 ls --recursive $train_s3uri\n",
    "print(\"Test data:\")\n",
    "!aws s3 ls --recursive $test_s3uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1caf03",
   "metadata": {},
   "source": [
    "## Train multiple models\n",
    "\n",
    "In this following section, we'll train multiple models on the same dataset, using the SageMaker PyTorch Framework Container.\n",
    "\n",
    "For a simple example, we'll just create two models `A` and `B`, using the same code but some slightly different hyperparameters between each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f74a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(base_job_name, hyperparam_overrides={}):\n",
    "    hyperparameters = {\n",
    "        \"batch-size\": 128,\n",
    "        \"epochs\": 20,\n",
    "        \"learning-rate\": 1e-3,\n",
    "        \"log-interval\": 100,\n",
    "    }\n",
    "    for k, v in hyperparam_overrides.items():\n",
    "        hyperparameters[k] = v\n",
    "\n",
    "    return PyTorchEstimator(\n",
    "        base_job_name=base_job_name,\n",
    "        entry_point=\"train.py\",\n",
    "        source_dir=\"code\",  # directory of your training script\n",
    "        role=role,\n",
    "        # At the time of writing, this example gives a deployment error in container v1.8.1 with\n",
    "        # upgraded TorchServe: so specifically setting \"1.8.0\". But \"1.7\" and \"1.6\" should be fine.\n",
    "        framework_version=\"1.8.0\",\n",
    "        py_version=\"py3\",\n",
    "        instance_type=\"ml.c4.xlarge\",\n",
    "        instance_count=1,\n",
    "        output_path=output_path,\n",
    "        hyperparameters=hyperparameters,\n",
    "    )\n",
    "\n",
    "\n",
    "estimatorA = get_estimator(base_job_name=\"mnist-a\", hyperparam_overrides={\"weight-decay\": 1e-4})\n",
    "estimatorB = get_estimator(base_job_name=\"mnist-b\", hyperparam_overrides={\"weight-decay\": 1e-2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b6817",
   "metadata": {},
   "source": [
    "By default, calling the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/)'s [Estimator.fit()](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase.fit) method waits for the training job to complete, streaming progress information and logs to the notebook.\n",
    "\n",
    "This is not the only supported configuration though: For example we can also start jobs asynchronously by setting `wait=False`, or retrospectively `wait()` on previously started jobs (optionally pulling through the logs).\n",
    "\n",
    "The below section will kick off both training jobs in parallel, stream the logs from `B` as it runs, and then wait for `A` to complete if it hasn't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48aad81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimatorA.fit({\"training\": train_s3uri, \"testing\": test_s3uri}, wait=False)\n",
    "print(\"Started estimator A training in background (logs will not show)\")\n",
    "\n",
    "print(\"Training estimator B with logs:\")\n",
    "estimatorB.fit({\"training\": train_s3uri, \"testing\": test_s3uri})\n",
    "\n",
    "print(\"\\nWaiting for estimator A to complete:\")\n",
    "estimatorA.latest_training_job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30112fc5",
   "metadata": {},
   "source": [
    "## Check single-model deployment\n",
    "\n",
    "Before trying to set up a multi-model deployment, it may be helpful to quickly check a single model can be deployed and invoked as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcead338",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = estimatorA.create_model(role=role, source_dir=\"code\", entry_point=\"inference.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f58085",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorA = modelA.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    ")\n",
    "predictorA.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictorA.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_request():\n",
    "    \"\"\"Create a dummy predictor.predict example data (16 images of random pixels)\"\"\"\n",
    "    return {\"inputs\": np.random.rand(16, 1, 28, 28).tolist()}\n",
    "\n",
    "\n",
    "dummy_data = get_dummy_request()\n",
    "\n",
    "start_time = time.time()\n",
    "predicted_value = predictorA.predict(dummy_data)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"Model took {int(duration * 1000):,d} ms\")\n",
    "np.array(predicted_value)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84737f76",
   "metadata": {},
   "source": [
    "Assuming the test worked, this endpoint is no longer needed so can be disposed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorA.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd33e1",
   "metadata": {},
   "source": [
    "## Create the Multi-Model Endpoint with the SageMaker SDK\n",
    "\n",
    "### Create a SageMaker Model from one of the Estimators\n",
    "\n",
    "Multi-Model Endpoints load models on demand in a *shared container*, so we'll first create a Model from any of our estimators to define this runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = estimatorA.create_model(role=role, source_dir=\"code\", entry_point=\"inference.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c9f75",
   "metadata": {},
   "source": [
    "### Create the Amazon SageMaker MultiDataModel entity\n",
    "\n",
    "We create the multi-model endpoint using the [```MultiDataModel```](https://sagemaker.readthedocs.io/en/stable/api/inference/multi_data_model.html) class.\n",
    "\n",
    "You can create a MultiDataModel by directly passing in a `sagemaker.model.Model` object - in which case, the Endpoint will inherit information about the image to use, as well as any environmental variables, network isolation, etc., once the MultiDataModel is deployed.\n",
    "\n",
    "In addition, a MultiDataModel can also be created without explicitly passing a `sagemaker.model.Model` object. Please refer to the documentation for additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where our MME will read models from on S3.\n",
    "multi_model_prefix = f\"{prefix}multi-model/\"\n",
    "multi_model_s3uri = f\"s3://{bucket_name}/{multi_model_prefix}\"\n",
    "print(multi_model_s3uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50efe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mme = MultiDataModel(\n",
    "    name=\"mnist-multi-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n",
    "    model_data_prefix=multi_model_s3uri,\n",
    "    model=model,  # passing our model\n",
    "    sagemaker_session=smsess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abb202",
   "metadata": {},
   "source": [
    "### Deploy the Multi-Model Endpoint\n",
    "\n",
    "You need to consider the appropriate instance type and number of instances for the projected prediction workload across all the models you plan to host behind your multi-model endpoint. The number and size of the individual models will also drive memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "    print(\"Deleting previous endpoint...\")\n",
    "    time.sleep(10)\n",
    "except (NameError, ClientError):\n",
    "    pass\n",
    "\n",
    "predictor = mme.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    ")\n",
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8e6dc",
   "metadata": {},
   "source": [
    "### Our endpoint has launched! Let's look at what models are available to the endpoint!\n",
    "\n",
    "By 'available', what we mean is, what model artifacts are currently stored under the S3 prefix we defined when setting up the `MultiDataModel` above i.e. `model_data_prefix`.\n",
    "\n",
    "Currently, since we have no artifacts (i.e. `tar.gz` files) stored under our defined S3 prefix, our endpoint, will have no models 'available' to serve inference requests.\n",
    "\n",
    "We will demonstrate how to make models 'available' to our endpoint below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a174381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No models visible!\n",
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ae3e9",
   "metadata": {},
   "source": [
    "### Dynamically deploying models to the endpoint\n",
    "\n",
    "The `.add_model()` method of the `MultiDataModel` will copy over our model artifacts from where they were initially stored, by training, to where our endpoint will source model artifacts for inference requests.\n",
    "\n",
    "Note that we can continue using this method, as shown below, to dynamically deploy more models to our live endpoint as required!\n",
    "\n",
    "`model_data_source` refers to the location of our model artifact (i.e. where it was deposited on S3 after training completed)\n",
    "\n",
    "`model_data_path` is the **relative** path to the S3 prefix we specified above (i.e. `model_data_prefix`) where our endpoint will source models for inference requests. Since this is a **relative** path, we can simply pass the name of what we wish to call the model artifact at inference time.\n",
    "\n",
    "> **Note:** To directly use training job `model.tar.gz` outputs as we do here, you'll need to make sure your training job produces results that:\n",
    ">\n",
    "> - Already include any required inference code in a `code/` subfolder, and\n",
    "> - (If you're using SageMaker PyTorch containers v1.6+) have been packaged to be compatible with TorchServe.\n",
    ">\n",
    "> See the `enable_sm_oneclick_deploy()` and `enable_torchserve_multi_model()` functions in [src/train.py](src/train.py) for notes on this. Alternatively, you can perform the same steps after the fact - to produce a new, serving-ready `model.tar.gz` from your raw training job result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387900c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, est in {\"ModelA\": estimatorA, \"ModelB\": estimatorB}.items():\n",
    "    artifact_path = est.latest_training_job.describe()[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "    # This is copying over the model artifact to the S3 location for the MME.\n",
    "    mme.add_model(model_data_source=artifact_path, model_data_path=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1470f7",
   "metadata": {},
   "source": [
    "### Our models are ready to invoke!\n",
    "\n",
    "We can see that the S3 prefix we specified when setting up `MultiDataModel` now has model artifacts listed. As such, the endpoint can now serve up inference requests for these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26261ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d28e7",
   "metadata": {},
   "source": [
    "## Get predictions from the endpoint\n",
    "\n",
    "Recall that `mme.deploy()` returns a [RealTimePredictor](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/predictor.py#L35) that we saved in a variable called `predictor`.\n",
    "\n",
    "That `predictor` can now be used as usual to request inference - but specifying which model to call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = get_dummy_request()\n",
    "\n",
    "start_time = time.time()\n",
    "predicted_value = predictor.predict(dummy_data, target_model=\"ModelA\")\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"Model took {int(duration * 1000):,d} ms\")\n",
    "np.array(predicted_value)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880e078",
   "metadata": {},
   "source": [
    "## Updating a model\n",
    "\n",
    "To update a model, you would follow the same approach as above and add it as a new model. For example, `ModelA-2`.\n",
    "\n",
    "You should avoid overwriting model artifacts in Amazon S3, because the old version of the model might still be loaded in the endpoint's running container(s) or on the storage volume of instances on the endpoint: This would lead invocations to still use the old version of the model.\n",
    "\n",
    "Alternatively, you could stop the endpoint and re-deploy a fresh set of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb05ed2",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Endpoints should be deleted when no longer in use, since (per the [SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/)) they're billed by time deployed. Here we'll also delete the endpoint configuration - to keep things tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe61045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
