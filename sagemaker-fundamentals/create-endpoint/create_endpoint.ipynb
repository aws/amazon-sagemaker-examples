{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# build the image\n",
    "cd container/\n",
    "\n",
    "# tag it as example-image:latest\n",
    "docker build -t example-serve:latest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen\n",
    "import subprocess\n",
    "\n",
    "cmd = \"docker run -p 8080:8080 --rm example-serve:latest serve\"\n",
    "\n",
    "\n",
    "process = subprocess.Popen(cmd, shell=True,\n",
    "                           stdout=subprocess.PIPE, \n",
    "                           stderr=subprocess.PIPE,\n",
    "                           start_new_session=True\n",
    "                          )\n",
    "\n",
    "\n",
    "process.wait()\n",
    "\n",
    "# wait for the process to terminate\n",
    "out, err = process.communicate()\n",
    "errcode = process.returncode\n",
    "\n",
    "print(out.decode('utf-8'))\n",
    "print(process.pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system('ls .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "proc = await asyncio.create_subprocess_exec(\n",
    "    'ls','.',\n",
    "    stdout=asyncio.subprocess.PIPE,\n",
    "    stderr=asyncio.subprocess.PIPE)\n",
    "\n",
    "\n",
    "# if proc takes very long to complete, the CPUs are free to use cycles for \n",
    "# other processes\n",
    "#stdout, stderr = await proc.communicate()\n",
    "stdout, stderr = proc.communicate()\n",
    "proc.returncode\n",
    "# 0\n",
    "\n",
    "# must call decode because stdout is a bytes object\n",
    "stdout.decode()\n",
    "# total 24K\n",
    "# drwxrwxr-x  3 felipe felipe 4,0K Nov  4 17:52 .\n",
    "# drwxrwxr-x 39 felipe felipe 4,0K Nov  3 18:31 ..\n",
    "# drwxrwxr-x  2 felipe felipe 4,0K Nov  3 19:32 .ipynb_checkpoints\n",
    "# -rw-rw-r--  1 felipe felipe  11K Nov  4 17:52 main.ipynb\n",
    "\n",
    "stderr.decode()\n",
    "# ''  empty string   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(Popen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# run the inference container\n",
    "docker run -p 8080:8080 --rm example-serve:latest serve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -H \"Content-Type: text/csv\" -v http://localhost:8080/ping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl --data-binary @container/local_test/payload.csv -H \"Content-Type: text/csv\" -v http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import pprint\n",
    "import os\n",
    "import time\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "iam = boto3.client('iam')\n",
    "ecr = boto3.client('ecr')\n",
    "\n",
    "image_name=\"example-inference\"\n",
    "\n",
    "try:\n",
    "    # The repository might already exist\n",
    "    # in your ECR\n",
    "    cr_res = ecr.create_repository(\n",
    "        repositoryName=image_name)\n",
    "    pp.pprint(cr_res)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "account=$(aws sts get-caller-identity --query Account | sed -e 's/^\"//' -e 's/\"$//')\n",
    "region=$(aws configure get region)\n",
    "ecr_account=${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Give docker your ECR login password\n",
    "aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $ecr_account\n",
    "\n",
    "# Fullname of the repo\n",
    "fullname=$ecr_account/example-inference:latest\n",
    "\n",
    "#echo $fullname\n",
    "# Tag the image with the fullname\n",
    "docker tag example-inference:latest $fullname\n",
    "\n",
    "# Push to ECR\n",
    "docker push $fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the ECR repository\n",
    "repo_res = ecr.describe_images(\n",
    "    repositoryName='example-inference')\n",
    "pp.pprint(repo_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri=\"688520471316.dkr.ecr.us-west-2.amazonaws.com/example-inference:latest\"\n",
    "role_arn = 'arn:aws:iam::688520471316:role/sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_boto3 = boto3.client('sagemaker')\n",
    "\n",
    "cm_res = sm_boto3.create_model(\n",
    "    ModelName='example-inference',\n",
    "    Containers=[\n",
    "        {\n",
    "            'Image': image_uri,\n",
    "   \n",
    "        },\n",
    "    ],\n",
    "    ExecutionRoleArn=role_arn,\n",
    "    EnableNetworkIsolation=False\n",
    ")\n",
    "\n",
    "pp.pprint(cm_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create endpoint config\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint\n",
    "model_name='example-inference'\n",
    "initial_instance_count=1\n",
    "instance_type='ml.t2.medium'\n",
    "variant_name = \"AMeaningfulProdVarName\" #^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n",
    "\n",
    "# why do we need it\n",
    "# think about a use case, where we need many variants\n",
    "production_variants = [\n",
    "    {\n",
    "        \"VariantName\": variant_name,\n",
    "        \"ModelName\": model_name,\n",
    "        \"InitialInstanceCount\": initial_instance_count,\n",
    "        \"InstanceType\": instance_type\n",
    "    }\n",
    "]\n",
    "\n",
    "endpoint_config_name = \"ExampleInferenceConfig\" #^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n",
    "\n",
    "endpoint_config = {\n",
    "    \"EndpointConfigName\": endpoint_config_name,\n",
    "    \"ProductionVariants\": production_variants,\n",
    "}\n",
    "\n",
    "ep_conf_res = sm_boto3.create_endpoint_config(**endpoint_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(ep_conf_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create endpoint\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint\n",
    "\n",
    "\n",
    "endpoint_name='exmaple-endpoint'\n",
    "ep_res = sm_boto3.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(ep_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe endpoint\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.describe_endpoint\n",
    "\n",
    "ep_des_res = sm_boto3.describe_endpoint(\n",
    "    EndpointName=endpoint_name\n",
    ")\n",
    "\n",
    "pp.pprint(ep_des_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke endpoint \n",
    "\n",
    "```sh\n",
    "grep -rwn .  -e predict\n",
    "```\n",
    "\n",
    "```python\n",
    "def predict(\n",
    "        self, data, initial_args=None, target_model=None, target_variant=None, inference_id=None\n",
    "    ):\n",
    "        \"\"\"Return the inference from the specified endpoint.\n",
    "        Args:\n",
    "            data (object): Input data for which you want the model to provide\n",
    "                inference. If a serializer was specified when creating the\n",
    "                Predictor, the result of the serializer is sent as input\n",
    "                data. Otherwise the data must be sequence of bytes, and the\n",
    "                predict method then sends the bytes in the request body as is.\n",
    "            initial_args (dict[str,str]): Optional. Default arguments for boto3\n",
    "                ``invoke_endpoint`` call. Default is None (no default\n",
    "                arguments).\n",
    "            target_model (str): S3 model artifact path to run an inference request on,\n",
    "                in case of a multi model endpoint. Does not apply to endpoints hosting\n",
    "                single model (Default: None)\n",
    "            target_variant (str): The name of the production variant to run an inference\n",
    "                request on (Default: None). Note that the ProductionVariant identifies the\n",
    "                model you want to host and the resources you want to deploy for hosting it.\n",
    "            inference_id (str): If you provide a value, it is added to the captured data\n",
    "                when you enable data capture on the endpoint (Default: None).\n",
    "        Returns:\n",
    "            object: Inference for the given input. If a deserializer was specified when creating\n",
    "                the Predictor, the result of the deserializer is\n",
    "                returned. Otherwise the response returns the sequence of bytes\n",
    "                as is.\n",
    "        \"\"\"\n",
    "\n",
    "        request_args = self._create_request_args(\n",
    "            data, initial_args, target_model, target_variant, inference_id\n",
    "        )\n",
    "        response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n",
    "        return self._handle_response(response)\n",
    "```\n",
    "\n",
    "so `sagemaker_session.sagemaker_runtime_client` invokes the endpint\n",
    "\n",
    "I searched `sagemaker_runtime` in session.py\n",
    "\n",
    "```python\n",
    "def _initialize(\n",
    "        self,\n",
    "        boto_session,\n",
    "        sagemaker_client,\n",
    "        sagemaker_runtime_client,\n",
    "        sagemaker_featurestore_runtime_client,\n",
    "    ):\n",
    "        \"\"\"Initialize this SageMaker Session.\n",
    "        Creates or uses a boto_session, sagemaker_client and sagemaker_runtime_client.\n",
    "        Sets the region_name.\n",
    "        \"\"\"\n",
    "        self.boto_session = boto_session or boto3.DEFAULT_SESSION or boto3.Session()\n",
    "\n",
    "        self._region_name = self.boto_session.region_name\n",
    "        if self._region_name is None:\n",
    "            raise ValueError(\n",
    "                \"Must setup local AWS configuration with a region supported by SageMaker.\"\n",
    "            )\n",
    "\n",
    "        self.sagemaker_client = sagemaker_client or self.boto_session.client(\"sagemaker\")\n",
    "        prepend_user_agent(self.sagemaker_client)\n",
    "\n",
    "        if sagemaker_runtime_client is not None:\n",
    "            self.sagemaker_runtime_client = sagemaker_runtime_client\n",
    "        else:\n",
    "            config = botocore.config.Config(read_timeout=80)\n",
    "            self.sagemaker_runtime_client = self.boto_session.client(\n",
    "                \"runtime.sagemaker\", config=config\n",
    "            )\n",
    "\n",
    "        prepend_user_agent(self.sagemaker_runtime_client)\n",
    "\n",
    "        if sagemaker_featurestore_runtime_client:\n",
    "            self.sagemaker_featurestore_runtime_client = sagemaker_featurestore_runtime_client\n",
    "        else:\n",
    "            self.sagemaker_featurestore_runtime_client = self.boto_session.client(\n",
    "                \"sagemaker-featurestore-runtime\"\n",
    "            )\n",
    "\n",
    "        self.local_mode = False\n",
    " \n",
    "```\n",
    "\n",
    "This led to investigate [`SageMakerRuntime` client in boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html)\n",
    "\n",
    "\n",
    "```\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName='string',\n",
    "    Body=b'bytes'|file,\n",
    "    ContentType='string',\n",
    "    Accept='string',\n",
    "    CustomAttributes='string',\n",
    "    TargetModel='string',\n",
    "    TargetVariant='string',\n",
    "    TargetContainerHostname='string',\n",
    "    InferenceId='string'\n",
    ")\n",
    "```\n",
    "\n",
    "To figure out how content type looks like, I looked up [`serializers.py`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/serializers.py) and [`deserializers.py`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/deserializers.py)\n",
    "\n",
    "\n",
    "Why is it called SageMaker runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to serialize csv\n",
    "\n",
    "import csv\n",
    "import io\n",
    "\n",
    "def _serialize_row(data):\n",
    "    \"\"\"Serialize data as a CSV-formatted row.\n",
    "    Args:\n",
    "        data (sting): Data to be serialized in a row.\n",
    "    Returns:\n",
    "        str: The data serialized as a CSV-formatted row.\n",
    "    \"\"\"\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = np.ndarray.flatten(data)\n",
    "\n",
    "    if hasattr(data, \"__len__\"):\n",
    "        if len(data) == 0:\n",
    "            raise ValueError(\"Cannot serialize empty array\")\n",
    "        csv_buffer = io.StringIO()\n",
    "        csv_writer = csv.writer(csv_buffer, delimiter=\",\")\n",
    "        csv_writer.writerow(data)\n",
    "        return csv_buffer.getvalue().rstrip(\"\\r\\n\")\n",
    "        \n",
    "csv_buffer = io.StringIO()\n",
    "csv_writer = csv.writer(csv_buffer, delimiter=',')\n",
    "csv_writer.writerow('xyze')\n",
    "v = csv_buffer.getvalue().rstrip('\\r\\n')\n",
    "\n",
    "print(v.rstrip(\"\\r\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke endpoint\n",
    "import json\n",
    "\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "body=json.dumps('a json string')\n",
    "\n",
    "# the model only supports csv data (look at how the model is defined in container/predictor.py)\n",
    "content_type='text/csv'\n",
    "# see the cell below for serializing a string into csv format\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "csv_writer = csv.writer(csv_buffer, delimiter=',')\n",
    "csv_writer.writerow('xyze')\n",
    "body = csv_buffer.getvalue().rstrip('\\r\\n')\n",
    "\n",
    "# respnse type is also text/csv (look at decision_tree/predictor.py)\n",
    "accept='text/csv'\n",
    "\n",
    "res=sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=body,                # encoded input data\n",
    "    ContentType=content_type, # I told the endpoint what's the encode\n",
    "    Accept=accept             # I told the endpoint how I wish to decode its response\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to decode json string\n",
    "\n",
    "import codecs\n",
    "\n",
    "class SimpleBaseDeserializer:\n",
    "    pass\n",
    "\n",
    "class JSONDeserializer(SimpleBaseDeserializer):\n",
    "    \"\"\"Deserialize JSON data from an inference endpoint into a Python object.\"\"\"\n",
    "\n",
    "    def __init__(self, accept=\"application/json\"):\n",
    "        \"\"\"Initialize a ``JSONDeserializer`` instance.\n",
    "        Args:\n",
    "            accept (union[str, tuple[str]]): The MIME type (or tuple of allowable MIME types) that\n",
    "                is expected from the inference endpoint (default: \"application/json\").\n",
    "        \"\"\"\n",
    "        super(JSONDeserializer, self).__init__(accept=accept)\n",
    "\n",
    "    def deserialize(self, stream, content_type):\n",
    "        \"\"\"Deserialize JSON data from an inference endpoint into a Python object.\n",
    "        Args:\n",
    "            stream (botocore.response.StreamingBody): Data to be deserialized.\n",
    "            content_type (str): The MIME type of the data.\n",
    "        Returns:\n",
    "            object: The JSON-formatted data deserialized into a Python object.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json.load(codecs.getreader(\"utf-8\")(stream))\n",
    "        finally:\n",
    "            stream.close()\n",
    "            \n",
    "            \n",
    "\n",
    "class CSVDeserializer(SimpleBaseDeserializer):\n",
    "    \"\"\"Deserialize a stream of bytes into a list of lists.\n",
    "    Consider using :class:~`sagemaker.deserializers.NumpyDeserializer` or\n",
    "    :class:~`sagemaker.deserializers.PandasDeserializer` instead, if you'd like to convert text/csv\n",
    "    responses directly into other data types.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding=\"utf-8\", accept=\"text/csv\"):\n",
    "        \"\"\"Initialize a ``CSVDeserializer`` instance.\n",
    "        Args:\n",
    "            encoding (str): The string encoding to use (default: \"utf-8\").\n",
    "            accept (union[str, tuple[str]]): The MIME type (or tuple of allowable MIME types) that\n",
    "                is expected from the inference endpoint (default: \"text/csv\").\n",
    "        \"\"\"\n",
    "        super(CSVDeserializer, self).__init__(accept=accept)\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def deserialize(self, stream, content_type):\n",
    "        \"\"\"Deserialize data from an inference endpoint into a list of lists.\n",
    "        Args:\n",
    "            stream (botocore.response.StreamingBody): Data to be deserialized.\n",
    "            content_type (str): The MIME type of the data.\n",
    "        Returns:\n",
    "            list: The data deserialized into a list of lists representing the\n",
    "                contents of a CSV file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            decoded_string = stream.read().decode(self.encoding)\n",
    "            return list(csv.reader(decoded_string.splitlines()))\n",
    "        finally:\n",
    "            stream.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode response\n",
    "res_body = res['Body']\n",
    "res_body.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_res = sm_boto3.delete_endpoint(\n",
    "    EndpointName=endpoint_name)\n",
    "pp.pprint(del_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Endpoint \n",
    "\n",
    "In this notebook, you will learn basics about hosting your trained model on Amazon SageMaker for inference. There are two ways you can use Amazon SageMaker for inference:\n",
    "1. Set up persistent endpoint for real-time online inference\n",
    "2. Gather data to be predicted in batch and use SageMaker batch transform for offline inference. \n",
    "\n",
    "In this notebook, we focus on the first option and we will discuss batch transform in another notebook. \n",
    "\n",
    "You are highly recommeneded to go through [the section on model deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html) in the official docs before moving on.\n",
    "\n",
    "\n",
    "The pricing for setting up an endpoint can be found [here](https://aws.amazon.com/sagemaker/pricing/)\n",
    "\n",
    "Like a [CreateTrainingJob](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create-training-job.ipynb), Amazon SageMaker interacts with your inference logic via a containerized enviornment. \n",
    "\n",
    "The following APIs are relavent:\n",
    "* [`CreateModel`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model)\n",
    "* [`CreateEndpointConfig`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config)\n",
    "* [`CreateEndpoint`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint)\n",
    "\n",
    "You are highly recommended to go through them. It's okay if you don't understand everything, we will go through them in detail in this notebook. \n",
    "\n",
    "The outline of this notebook is:\n",
    "* Create an IAM role for SageMaker\n",
    "* Build an inference image\n",
    "* Test the inference image / container locally and push it to ECR\n",
    "* Use the ECR address of the inference container to define a model by calling `CreateModel`\n",
    "* Specify configuration of an endpoint by calling `CreateEndpointConfig`\n",
    "* Use model definition from 3 and endpoint configuration from 4 to create an endpoint by calling `CreateEndpoint`\n",
    "* Invoke the endpoint by using SageMaker runtime client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setups\n",
    "import boto3\n",
    "import datetime\n",
    "import pprint\n",
    "import os\n",
    "import time\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an IAM service role\n",
    "\n",
    "Review [notebook on execution role](https://github.com/hsl89/amazon-sagemaker-examples/blob/execution-role/sagemaker-fundamentals/execution-role/execution-role.ipynb) for step-by-step instructions on how to create an IAM Role.\n",
    "\n",
    "The service role is intended to be assumed by the SageMaker service. For simplicity, we will give it `AmazonSageMakerFullAccess` permission. However, in order to do what we need in this notebook, we do not need such a comprehensive permission. You are highly encouraged to play with the helper functions we provide in `iam_helpers.py` to figure out what are the minimum permissions needed to run this notebook. \n",
    "\n",
    "First get some useful functions we created there to help us creating an execution role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "file=$(ls . | grep iam_helpers.py)\n",
    "\n",
    "if [ -f \"$file\" ]\n",
    "then\n",
    "    rm $file\n",
    "fi\n",
    "\n",
    "wget https://raw.githubusercontent.com/hsl89/amazon-sagemaker-examples/sagemaker-fundamentals/sagemaker-fundamentals/execution-role/iam_helpers.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an role\n",
    "from iam_helpers import create_execution_role, attach_permission\n",
    "\n",
    "role_name='sm' \n",
    "role = create_execution_role(role_name=role_name)['Role']\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach AmazonSageMakerFullAccess\n",
    "iam = boto3.client('iam')\n",
    "res = iam.attach_role_policy(\n",
    "    RoleName=role['RoleName'],\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    ")\n",
    "\n",
    "pp.pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an inference image\n",
    "\n",
    "You inference image must be a self-contained web server. When you run your inference container locally, it should listen on port 8080 and accept POST requests to the `/invocations` endpoint. The payload of the POST requests is the content of the data that you want your model to predict. Since the inference container is essentially a web server, you should expect it to look differently from the container we used for [`CreateTrainingJob`](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create-training-job.ipynb). \n",
    "\n",
    "In this notebook, we use a minimal python stack to build our web server:\n",
    "![Request serving stack](stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further readings on the serving stack\n",
    "\n",
    "* [Overview of the stack](https://flask.palletsprojects.com/en/1.1.x/deploying/uwsgi/)\n",
    "* [Ngnix homepage](https://www.nginx.com/resources/wiki/start/) \n",
    "* [WSGI homepage](https://gunicorn.org/)\n",
    "* [Flask homepage](https://flask.palletsprojects.com/en/1.1.x/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How SageMaker runs your container\n",
    "\n",
    "SageMaker runs your container like\n",
    "\n",
    "```sh\n",
    "docker run <image> serve\n",
    "```\n",
    "\n",
    "This means you need to have an executable called `serve` in the `PATH`. In this notebook, we will create a python script as an **executable** and put it in the working directory of the docker image. \n",
    "        \n",
    "The folder `container/src` contains the configs and entry point of the web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls  container/src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrypoint for Ngnixs server\n",
    "\n",
    "`serve` is a python executable that is intended to be used as the entrypoint for the inference image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat container/src/serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config file for Ngnix server\n",
    "`nginx.conf` is the config file for the nginx server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat container/src/nginx.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WSGI config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat container/src/wsgi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference logic\n",
    "\n",
    "The most important file in `container/src` is `predictor.py`. It contains the inference logic. Other files in the `container/src` can be used **as it**. But you will need to customize `predictor.py` to implement your own inference logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize container/src/predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the container\n",
    "\n",
    "We build the container from `container/Dockderfile`. And let's call this image `example-serve`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# build the image\n",
    "cd container/\n",
    "\n",
    "# tag it as example-image:latest\n",
    "docker build -t example-serve:latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your image\n",
    "\n",
    "Like in the [notebook for CreateTrainingJob](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create-training-job.ipynb), we replicate the Amazon SageMaker hosting environment and test your image locally before serving in production. You are encouraged to read through the section on [Use Your Own Inference Code with Hosting Services](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html) and think about how would you replicate SageMaker hosting environment before moving on. \n",
    "\n",
    "Like for `CreateTrainingJob`, SageMaker reserves `/opt/ml` directory in your image to inject ML-related info for `CreateEndpoint`. In particular, it downloads your trained model artifact and inject it in the directory `/opt/ml/model`. When calling `CreateEndpoint` you will need to tell SageMaker the S3 URI of your model artifact. SageMaker will use then pull the artifact and inject it into `/opt/ml/model`. This means when defining your own inference logic, you should load your trained model from `/opt/ml/model`. \n",
    "\n",
    "We will use docker python client to run your image and we will mount `container/local_test/ml` to `/opt/ml` as docker volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what's inside `container/ml`\n",
    "!ls container/local_test/ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference logic we implemented in `container/src/predictor.py` under `def inference():` does not require a real ML model. Therefore we do not need to inject anything for the purpose of local test. We will discuss how to load a real model in a more advanced notebook. \n",
    "\n",
    "<span style=\"color:red\"> TODO for Dev:  add link to the advanced notebook when it is ready</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the container\n",
    "\n",
    "To run the container `example-serve`, open a terminal in the current directory and go to `container/local_test`\n",
    "\n",
    "```sh\n",
    "cd container/local_test\n",
    "```\n",
    "\n",
    "Then run the following command\n",
    "\n",
    "```sh\n",
    "docker run -v ml:/opt/ml -p 8080:8080 --rm example-serve:latest serve \n",
    "```\n",
    "\n",
    "`-v ml:/opt/ml` binds the directory `ml` (in `container/local_test`) to `/opt/ml` in the image as a docker volume.\n",
    "\n",
    "`-p 8080:8080` exposes port 8080 inside container as port 8080 on the hos\n",
    "\n",
    "`--rm` removes the container from daemon when it is stopped. \n",
    "\n",
    "We suggest you to run the image from the shell instead of within the notebook because when you are debugging your own container, you can more easily stdout from the container when you have a shell process running it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ping your container\n",
    "Once your container is up, you can ping it at `http://localhost:8080`. \n",
    "\n",
    "To trigger the logic under `def ping():` in `container/src/predictor.py`, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "curl localhost:8080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To trigger the logic under `def inference():` in `container/src/predictor.py` with a json string, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "curl --header \"Content-Type: application/json\" \\\n",
    "  --request POST \\\n",
    "  --data '{\"key\":\"value\"}' \\\n",
    "  http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "curl --header \"Content-Type: text/csv\" \\\n",
    "  --request POST \\\n",
    "  http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the container, go to the terminal that runs your container and press `Control + C`. Alternatively, you can find out it container id by grepping for a docker process that binds port 8080 on the host and manually remove it.\n",
    "\n",
    "```sh\n",
    "docker rm -f $(docker ps | grep -e \"0.0.0.0:8080->8080/tcp\" | awk '{print $1}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push the image to ECR\n",
    "Now you have tested your image, the next thing to do is to push it to your ECR so that SageMaker can download it. We have discussed this in the [previous notebook on `CreateTrainingJob`](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create-training-job.ipynb) in the section where we push the training image to ECR. \n",
    "\n",
    "In the notebook for `CreateTrainingJob`, we created the ECR repo and pushed the training image there via the IAM user (you). In this notebook, let's do something different: we will create an ECR repo and push the image there using the IAM role `sm` you created at the beginining. For this purpose, you will need to make sure the IAM user you are assuming now has the permission to assume role, i.e. (`sts: AssumeRole`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that you can assume role\n",
    "user_arn = boto3.client('sts').get_caller_identity()['Arn'] # you\n",
    "\n",
    "user_prp = iam.simulate_principal_policy(\n",
    "    PolicySourceArn=user_arn,\n",
    "    ActionNames=['sts:AssumeRole']\n",
    ")\n",
    "print(\"== User's Permission to Assume Role ==\")\n",
    "pp.pprint(user_prp['EvaluationResults'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boto session with the role\n",
    "\n",
    "now = str(time.time()).split('.')[0]\n",
    "\n",
    "obj = boto3.client('sts').assume_role(\n",
    "    RoleArn=role['Arn'],\n",
    "    RoleSessionName=now\n",
    ")\n",
    "\n",
    "cred=obj['Credentials']\n",
    "\n",
    "sess = boto3.session.Session(\n",
    "    aws_access_key_id=cred['AccessKeyId'],\n",
    "    aws_secret_access_key=cred['SecretAccessKey'],\n",
    "    aws_session_token=cred['SessionToken']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the profile of the session\n",
    "assumed_role=sess.client('sts').get_caller_identity()\n",
    "pp.pprint(assumed_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the assumed role has the previlege to create ECR repo\n",
    "role_prp = iam.simulate_principal_policy(\n",
    "    PolicySourceArn=role['Arn'],\n",
    "    ActionNames=['ecr:GetAuthorizationToken', 'ecr:CreateRepository']\n",
    ")\n",
    "\n",
    "pp.pprint(role_prp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr = sess.client('ecr')\n",
    "\n",
    "try:\n",
    "    # The repository might already exist\n",
    "    # in your ECR\n",
    "    cr_res = ecr.create_repository(\n",
    "        repositoryName='example-serve')\n",
    "    pp.pprint(cr_res)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "account=$(aws sts get-caller-identity --query Account | sed -e 's/^\"//' -e 's/\"$//')\n",
    "region=$(aws configure get region)\n",
    "ecr_account=${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Give docker your ECR login password\n",
    "aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $ecr_account\n",
    "\n",
    "# Fullname of the repo\n",
    "fullname=$ecr_account/example-serve:latest\n",
    "\n",
    "#echo $fullname\n",
    "# Tag the image with the fullname\n",
    "docker tag example-serve:latest $fullname\n",
    "\n",
    "# Push to ECR\n",
    "docker push $fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
