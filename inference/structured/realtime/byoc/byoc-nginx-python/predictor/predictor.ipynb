{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a485b55-4b06-4fa9-8148-0bb37603eb6a",
   "metadata": {},
   "source": [
    "# Build Predictor (XGBoost) Model\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/inference|structured|realtime|byoc|byoc-nginx-python|featurizer|predictor.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "We demonstrate building a ML inference application to predict the rings of Abalone.\n",
    "\n",
    "After the model is hosted for inference, the payload will be sent as a raw (untransformed) csv string to a real-time endpoint.\n",
    "The raw payload is first received by the preprocessor container. The raw payload is then transformed (feature-engineering) by the preprocessor, and the transformed record (float values) are returned as a csv string by the preprocessor container.\n",
    "\n",
    "The transformed record is then passed to the predictor container (XGBoost model). The predictor then converts the transformed record into [`XGBoost DMatrix`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix) format, loads the model, calls `booster.predict(input_data)` and returns the predictions (Rings) in a JSON format.\n",
    "\n",
    "![Abalone Predictor](../images/byoc-predictor.png)\n",
    "\n",
    "We use [nginx](https://nginx.org/) as the reverse proxy, [gunicorn](https://gunicorn.org/#deployment) as the web server gateway interface and the inference code as python [\"Flask\"](https://flask.palletsprojects.com/en/2.3.x/tutorial/factory/) app.\n",
    "\n",
    "## Dataset and model\n",
    "\n",
    "For this example, we use a pre-trained [XGBoost](https://xgboost.readthedocs.io) model on [UCI Abalone dataset](https://archive.ics.uci.edu/ml/datasets/abalone).\n",
    "\n",
    "The pre-trained model accepts input in `text/csv` format and returns prediction results in `application/json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d5400-7949-4a1b-b228-bbc772c75191",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Upgrade the below packages to the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b997c6-f0ea-4be5-b2a0-d3bd1b726f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U awscli boto3 sagemaker watermark scikit-learn tqdm --quiet\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -p awscli,boto3,sagemaker,scikit-learn,tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ecbf3-e4a5-494b-92da-cc3ded85f973",
   "metadata": {},
   "source": [
    "### Inference script for a predictor (XGBoost) model\n",
    "\n",
    "- In this example, we download a pre-trained XGBoost model on the UCI abalone dataset from s3.\n",
    "- The inference code is implemented in [`code/inference.py`](./code/inference.py). The [Flask](https://flask.palletsprojects.com/) app implementation is as follows:\n",
    "  - Implement routes for `/ping` and `/invocations`\n",
    "  - Implement functions to handle preprocessing, model loading and prediction\n",
    "  - Predictions will be returned from `/invocations` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b04af-6aa0-4b53-b37c-68ace4c870ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "from sagemaker import get_execution_role, session\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader, s3_path_join\n",
    "from pathlib import Path\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "sm_session = session.Session()\n",
    "region = sm_session._region_name\n",
    "role = get_execution_role()\n",
    "bucket = sm_session.default_bucket()\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "prefix = \"sagemaker/abalone/models/byoc\"\n",
    "\n",
    "predictor_image_name = \"abalone/predictor\"\n",
    "\n",
    "pretrained_xgboost_model_s3uri = (\n",
    "    f\"s3://sagemaker-example-files-prod-{region}/models/xgb-abalone/xgboost-model\"\n",
    ")\n",
    "\n",
    "base_dir = Path(\"../data\").resolve()\n",
    "predictor_model_dir = Path(\"./models\").absolute()\n",
    "\n",
    "if not predictor_model_dir.exists():\n",
    "    predictor_model_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d9c2d1-c7f7-4523-abeb-51efc1b1c85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download pretrained xgboost model\n",
    "!aws s3 cp $pretrained_xgboost_model_s3uri $predictor_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137679d4-58bb-464d-9acd-e3df30405266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize ./code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838fec2-f74a-4186-95b6-6b486afe1a7e",
   "metadata": {},
   "source": [
    "### Build and test custom inference image locally\n",
    "\n",
    " - [Dockerfile](./Dockerfile) implementation\n",
    "   - Set required LABEL `LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true`\n",
    "   - Installs required software and python packages\n",
    "   - Copies all files under code directory to `/opt/program`\n",
    "   - Sets `ENTRYPOINT` to `[\"python\"]`\n",
    "   - Sets `CMD` to `[\"serve\"]` (python script that launches nginx, gunicorn in the background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38730055-fd66-40ce-a520-dd44bcb16ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba8ab1-4e0e-41f5-ad4e-e647b2a13efc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build image locally\n",
    "!docker build -t $predictor_image_name ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd93d6-e00a-44ce-a742-79e1bc5b4076",
   "metadata": {},
   "source": [
    "### Launch and test custom Inference container locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef0220-ae37-4d72-b230-a292498c73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a terminal and cd into the predictor directory (the location where predictor.ipynb is located) run the following command\n",
    "# run this command to launch container locally\n",
    "# mounts the [models](./models) directory to `/opt/ml/model` directory inside the container and maps container port `8080` to host port `8080`\n",
    "# docker run --rm -v $(pwd)/models:/opt/ml/model -p 8080:8080 abalone/predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c4680-48ab-4ce2-aaaf-168da80bc0b0",
   "metadata": {},
   "source": [
    "#### Check container health by invoking `/ping`\n",
    "\n",
    "If the `/ping` was successful you should see a response similar to `\"GET /ping HTTP/1.1\" 200 1` in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd99bed-c9a6-4194-b41f-54501aba6816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ping local inference endpoint\n",
    "# !curl http://localhost:8080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9893e277-20ea-40e5-86a9-e5b5809ac892",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Verify container logs locally (using docker logs)\n",
    "\n",
    "- To inspect a running container to view container config values or IP address we use `docker inspect <CONTAINER_ID_OR_NAME>`\n",
    "- To view and tail logs generated in the container we use `docker logs --follow <NUM_OF_LINES> <CONTAINER_ID_OR_NAME>`\n",
    "- SageMaker publishes container logs to CloudWatch. CloudWatch logs for a given endpoint are published to the following log stream path\n",
    "`/aws/sagemaker/Endpoints/ENDPOINT_NAME/VARIANT_NAME/CONTAINER_NAME`\n",
    "\n",
    "**NOTE:** \n",
    "1. Run this command in a terminal as running this inside a cell would hang execution.\n",
    "1. the below command assumes there is only one running container. If you have more, then use command with container name `docker inspect <CONTAINER_ID_OR_NAME>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292186ed-22a4-4f4f-a23c-acfb138e2ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RUN THE BELOW IN A SEPARATE NEW TERMINAL\n",
    "# docker ps --format \"{{.Names}}\" | xargs -n1 -I{} docker logs --follow --tail 50 {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79be964-d4dd-4cff-ac62-ef57c6dd0627",
   "metadata": {},
   "source": [
    "### Troubleshooting container locally (using logs)\n",
    "\n",
    "`!docker logs abalone/featurizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902de0e4-03da-4592-82c0-461d1ee17b51",
   "metadata": {},
   "source": [
    "#### Test records for inference\n",
    "\n",
    "Grab a test record from [abalone_test_predictor.csv](../featurizer/data/abalone_test_predictor.csv), generated by [`featurizer.ipynb`](../featurizer/featurizer.ipynb), format it as a CSV record, and send it as raw data to the endpoint `http://localhost:8080/invocations` path\n",
    "\n",
    "Uncomment below cells to test inference on local container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365d9de-51fe-46dc-9e5e-a82b05685bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send test records to /invocations on the endpoint\n",
    "# !curl --data-raw '-1.3317586042173168,-1.1425409076053987,-1.0579488602777858,-1.177706547272754,-1.130662184748842,-1.1493955859050584,-1.139968767909096,0.0,1.0,0.0' \\\n",
    "# -H 'Content-Type: text/csv; charset=utf-8' \\\n",
    "# -v http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86286333-a056-4619-be76-2b65d492cd08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Send test records to /invocations on the endpoint\n",
    "# !curl --data-raw '0.7995425613971686,0.877965470587042,1.326659055767273,1.398563012556441,0.9896192483949702,1.509166873607132,2.01650402614155,0.0,0.0,1.0' \\\n",
    "# -H 'Content-Type: text/csv; charset=utf-8' \\\n",
    "# -v http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad32979-3897-42e7-94f4-8537e12c720a",
   "metadata": {},
   "source": [
    "### Tag and push the local image to private ECR\n",
    "\n",
    "- Tag the `abalone/predictor` local image to `{account_id}.dkr.ecr.{region}.amazonaws.com/{imagename}:{tag}` format\n",
    "- Run [./build_n_push.sh](./build_n_push.sh) shell script with image name `nginx` as parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863efc3-4dfd-4ecc-8206-39f7b51ea690",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod +x ./build_n_push.sh\n",
    "\n",
    "!./build_n_push.sh abalone/predictor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
