{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0fcb5e",
   "metadata": {},
   "source": [
    "# Run deep learning models on GPUs with Amazon SageMaker endpoints (SME) with ONNX backend on Triton Server\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) provides a scalable and cost-effective way to deploy large number of deep learning models. \n",
    "\n",
    "<div class=\"alert alert-info\"> <strong> Note </strong>\n",
    "This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `g4dn.xlarge`. It is a modified version of the original version of this sample notebook <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/multi-model-endpoints/SME-on-gpu/cv/resnet50_mme_with_gpu.ipynb\">Here</a> by <a href=\"https://github.com/vikramelango\">Vikram Elango</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c743c",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Model endpoints with GPU Support\n",
    "\n",
    "Amazon SageMaker endpoints with GPU works using [NVIDIA Triton Inference Server](https://github.com/triton-inference-server/server/). NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance. Triton supports all major training and inference frameworks, such as TensorFlow, NVIDIA TensorRT, PyTorch, MXNet, Python, ONNX, XGBoost, scikit-learn, RandomForest, OpenVINO, custom C++, and more. It offers dynamic batching, concurrent execution, post-training quantization, optimal model configuration to achieve high performance inference.\n",
    "When SageMaker receives an invocation request for a particular model, it does the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bbc9b0",
   "metadata": {},
   "source": [
    "## How it works?\n",
    "\n",
    "1. SageMaker routes traffic to the right instance behind the endpoint where the target model is loaded. SageMaker takes care of model management behind the endpoint, loads model to the container's memory and unloads the model based on the endpoint's traffic pattern.\n",
    "3. SageMaker loads the model to NVIDIA Triton containerâ€™s memory on GPU accelerated instance and serve the inference request. If the model is already loaded in the container memory, the subsequent requests are served faster as SageMaker does not need to download and load it again.\n",
    "4. SageMaker takes care of traffic shaping to the SME endpoint, SageMaker continues to route traffic to the instance where the model is loaded. If the instance resources reach capacity due to high utilization, SageMaker unloads least used models from the container to free up resource to load more frequently used models.\n",
    "5. SageMaker SME can horizontally scale using auto-scaling policy, provision additional GPU compute instances based on metrics such as GPU utilization, memory utilization etc. to serve spiky traffic to SME endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6792f1",
   "metadata": {},
   "source": [
    "In this notebook, we will show you how to use the new features Amazon SageMaker SME with GPU with a computer vision use case. For demonstration purpose, we will use a ResNet-50 convolutional neural network pre-trained model that can classify images into 1000 categories. We will -\n",
    "\n",
    "*  Show how to use NVIDIA Triton inference container on SageMaker SME, leverage different model frameworks such and ONNXRuntime, PyTorch and TensorRT backends. \n",
    "*  Walk you through steps to convert ResNet-50 models to optimized ONNXRuntime backend engine format along with TensorRT engine format and deploy it with SageMaker SME. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a906b5",
   "metadata": {},
   "source": [
    "### Installs\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server. Update SageMaker, boto3, awscli etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed525854",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker\n",
    "!pip install nvidia-pyindex --quiet\n",
    "!pip install tritonclient[http] --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc0567",
   "metadata": {},
   "source": [
    "### Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad513cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "# variables\n",
    "s3_client = boto3.client(\"s3\")\n",
    "auto_scaling_client = boto3.client(\"application-autoscaling\")\n",
    "sample_image_name = \"shiba_inu_dog.jpg\"\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# sagemaker variables\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "default_bucket_prefix = sagemaker_session.default_bucket_prefix\n",
    "prefix = \"resnet50-sme-gpu-onnx\"\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if default_bucket_prefix:\n",
    "    prefix = f\"{default_bucket_prefix}/{prefix}\"\n",
    "\n",
    "# endpoint variables\n",
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "\n",
    "# account mapping for SageMaker Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "sme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.12-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1893d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898116a8",
   "metadata": {},
   "source": [
    "### Creating Model Artifacts\n",
    "\n",
    "This section presents overview of steps to prepare ResNet-50 pre-trained model to be deployed on SageMaker SME using Triton Inference server model configurations. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"><strong> Note </strong>\n",
    "We are demonstrating deployment with 2 models. However, customers can prepare and 100s of models. The models may or may not share the same framework.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9203d89",
   "metadata": {},
   "source": [
    "#### Prepare PyTorch Model \n",
    "\n",
    "`generate_model_pytorch.sh` file in the `workspace` directory contains scripts to generate a PyTorch model. First, we load a pre-trained ResNet50 model using torchvision models package. We save the model as model.pt file in TorchScript optimized and serialized format. TorchScript needs an example inputs to do a model forward pass, so we pass one instance of an RGB image with 3 color channels of dimension 224X224. The script for exporting this model can be found [here](./workspace/generate_model_pytorch.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fa0bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it \\\n",
    "            -v `pwd`/workspace:/workspace nvcr.io/nvidia/pytorch:22.12-py3 \\\n",
    "            /bin/bash generate_model_pytorch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1a8c9",
   "metadata": {},
   "source": [
    "#### PyTorch Model Repository\n",
    "\n",
    "The model repository contains model to serve, in our case it will be the `model.pt` and configuration file with input/output specifications and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479af49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "display.Image(\"images/pyt-model-repo.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26dcfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054051c",
   "metadata": {},
   "source": [
    "#### PyTorch Model configuration\n",
    "\n",
    "Model configuration file `config.pbtxt` must specify name of the model(`resnet`), the platform and backend properties (`pytorch_libtorch`), max_batch_size(128) and the input and output tensors along with the data type(TYPE_FP32) information. Additionally, you can specify `instance_group` and `dynamic_batching` properties to achieve high performance inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e98760",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-pt/resnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f97e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile triton-serve-pt/resnet/config.pbtxt\n",
    "name: \"resnet\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 128\n",
    "input {\n",
    "  name: \"INPUT__0\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: 3\n",
    "  dims: 224\n",
    "  dims: 224\n",
    "}\n",
    "output {\n",
    "  name: \"OUTPUT__0\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c9c00",
   "metadata": {},
   "source": [
    "#### Prepare TensorRT Model\n",
    "\n",
    "1. We export the pre-trained ResNet50 model into an ONNX file, which runs the model once to trace its execution and then export the traced model to the specified file. It is one of the better options in terms model conversion and deployment when converting using ONNX.\n",
    "\n",
    "2. We use `trtexec` to automatically convert ONNX model to TensorRT plan. As ONNX is framework agnostic it works with models in TF, PyTorch and more. You will export the weights of your model from the framework and load them into your TensorRT network.\n",
    "\n",
    "\n",
    "In this step, we load pre-trained ResNet50 model from torch and convert to onnx representation using torch onnx exporter. Once onnx model is created, we use TensorRT trtexec command to create the model plan to be hosted with Triton. The script for exporting this model can be found [here](./workspace/generate_model_trt.sh). This is run as part of the `generate_model_trt.sh` script from the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it \\\n",
    "            -v `pwd`/workspace:/workspace nvcr.io/nvidia/pytorch:22.12-py3 \\\n",
    "            /bin/bash generate_model_trt.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431fbae",
   "metadata": {},
   "source": [
    "#### TensorRT Model Repository\n",
    "\n",
    "The model repository contains model to serve, for TensorRT model it will be the model.plan(created in above steps) and configuration file with input/output specifications and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fd279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display.Image(\"images/trt-model-repo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14c407",
   "metadata": {},
   "source": [
    "#### TensorRT Model configuration\n",
    "\n",
    " For the TensorRT model, we specify `tensorrt_plan` as platform, input tensor specification of the image of dimension 224X224 which has 3 color channels. Output tensor with 1000 dimensions of type TYPE_FP32 corresponding the different object categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccc076",
   "metadata": {},
   "source": [
    "#### ONNX Model configuration\n",
    "\n",
    " For the ONNX model, we specify `tensorrt_plan` as platform, input tensor specification of the image of dimension 224X224 which has 3 color channels. Output tensor with 1000 dimensions of type TYPE_FP32 corresponding the different object categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-onnx/resnet/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37511225",
   "metadata": {},
   "source": [
    "from https://github.com/triton-inference-server/server/blob/main/docs/examples/model_repository/densenet_onnx/config.pbtxt\n",
    "\n",
    "Note that the ONNX model is already generated by the script \"generate_model_trt.sh\" as a substep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0afc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile triton-serve-onnx/resnet/config.pbtxt\n",
    "name: \"resnet\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size : 16\n",
    "    \n",
    "input [\n",
    "  {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "optimization { execution_accelerators {\n",
    "  gpu_execution_accelerator : [ {\n",
    "    name : \"tensorrt\"\n",
    "    parameters { key: \"precision_mode\" value: \"FP32\" }\n",
    "    parameters { key: \"trt_engine_cache_enable\" value: \"true\" }\n",
    "    parameters { key: \"trt_engine_cache_path\" value: \"/tmp\" }      \n",
    "    parameters { key: \"max_workspace_size_bytes\" value: \"1073741824\" }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "dynamic_batching {\n",
    "    preferred_batch_size: [1, 8, 16]\n",
    "    max_queue_delay_microseconds: 250000\n",
    "}\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "    }\n",
    "]\n",
    "\n",
    "model_warmup[\n",
    "    {\n",
    "        name: \"resnet\"\n",
    "        batch_size: 1\n",
    "        inputs {\n",
    "            key: \"input\"\n",
    "            value: {\n",
    "                data_type: TYPE_FP32\n",
    "                dims: [ 3, 224, 224 ]\n",
    "                random_data: true\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ae580",
   "metadata": {},
   "source": [
    "#### 3. Export model artifacts to S3\n",
    "\n",
    "SageMaker expects the model artifacts in below format, it should also satisfy Triton container requirements such as model name, version, config.pbtxt files etc. `tar` the folder containing the model file as `model.tar.gz` and upload it to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-onnx/resnet/1/\n",
    "!cp -f workspace/model.onnx triton-serve-onnx/resnet/1/\n",
    "!tar -C triton-serve-onnx/ -czf resnet_onnx_v1.tar.gz resnet\n",
    "model_uri_onnx = sagemaker_session.upload_data(path=\"resnet_onnx_v1.tar.gz\", key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfc72d9",
   "metadata": {},
   "source": [
    "Now that we have uploaded the model artifacts to S3, we can create a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ad0d0",
   "metadata": {},
   "source": [
    "#### Deploy Models with SME\n",
    "\n",
    "We will now deploy ResNet-50 model with ONNX framework backends.\n",
    "\n",
    "\n",
    "We will use AWS SDK for Python (Boto) APIs [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model), [create_endpoint_config](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config) and [create_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint) to create a mulit-model endpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf933646",
   "metadata": {},
   "source": [
    "#### Create a  model object\n",
    "\n",
    "Using the SageMaker boto3 client, create the model using [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model) API. We will pass the container definition to the create model API along with ModelName and ExecutionRoleArn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a244f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[{\"Image\": sme_triton_image_uri, \"ModelDataUrl\": model_uri_onnx}],\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc0bb1",
   "metadata": {},
   "source": [
    "#### Define configuration for the endpoint\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> <strong> Note </strong>\n",
    "Based on our findings, customers get price performance on ML optimized instances with single GPU core. Hence, this feature is only enabled for single GPU core instances. For full list of instances supported see this (https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html#multi-model-support to Docs page where we capture list of instances.)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b931d116",
   "metadata": {},
   "source": [
    "#### Create Single Model Endpoint\n",
    "\n",
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b302e83",
   "metadata": {},
   "source": [
    "#### Setup Autoscaling policies for GPU endpoint\n",
    "\n",
    "Amazon SageMaker inference endpoints supports automatic scaling (auto scaling) for your hosted models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, auto scaling brings more instances online. When the workload decreases, auto scaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.\n",
    "\n",
    "In the below scaling policy, use a custom metric GPUUtilization in TargetTrackingScalingPolicyConfiguration configuration and set a TargetValue of 60.0 for the target value of that metric. This autoscaling policy will provision additional instances upto MaxCapacity when GPU Utilization is more than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07793e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform auto-scaling of the endpoint based on GPU memory utilization\n",
    "\n",
    "# This is the format in which application autoscaling references the endpoint\n",
    "resource_id = \"endpoint/\" + endpoint_name + \"/variant/\" + \"AllTraffic\"\n",
    "response = auto_scaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=5,\n",
    ")\n",
    "\n",
    "\n",
    "# GPUMemoryUtilization metric\n",
    "response = auto_scaling_client.put_scaling_policy(\n",
    "    PolicyName=\"GPUUtil-ScalingPolicy\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n",
    "    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        # Scale out when GPU utilization hits GPUUtilization target value.\n",
    "        \"TargetValue\": 60.0,\n",
    "        \"CustomizedMetricSpecification\": {\n",
    "            \"MetricName\": \"GPUUtilization\",\n",
    "            \"Namespace\": \"/aws/sagemaker/Endpoints\",\n",
    "            \"Dimensions\": [\n",
    "                {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                {\"Name\": \"VariantName\", \"Value\": \"AllTraffic\"},\n",
    "            ],\n",
    "            \"Statistic\": \"Average\",  # Possible - 'Statistic': 'Average'|'Minimum'|'Maximum'|'SampleCount'|'Sum'\n",
    "            \"Unit\": \"Percent\",\n",
    "        },\n",
    "        \"ScaleInCooldown\": 600,\n",
    "        \"ScaleOutCooldown\": 200,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96f53d",
   "metadata": {},
   "source": [
    "#### Prepare Input Payload for PyTorch and TensorRT model\n",
    "\n",
    "The following method transforms a sample image we will be using for inference into the payload that can be sent for inference to the Triton server.\n",
    "\n",
    "The `tritonclient` package provides utility methods to generate the payload without having to know the details of the specification. We'll use the following methods to convert our inference request into a binary format which provides lower latencies for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.download_file(\n",
    "    f\"sagemaker-example-files-prod-{region}\",\n",
    "    \"datasets/image/pets/shiba_inu_dog.jpg\",\n",
    "    \"shiba_inu_dog.jpg\",\n",
    ")\n",
    "\n",
    "\n",
    "def get_sample_image():\n",
    "    image_path = \"./shiba_inu_dog.jpg\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = img.resize((224, 224))\n",
    "    img = (np.array(img).astype(np.float32) / 255) - np.array(\n",
    "        [0.485, 0.456, 0.406], dtype=np.float32\n",
    "    ).reshape(1, 1, 3)\n",
    "    img = img / np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 1, 3)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return img.tolist()\n",
    "\n",
    "\n",
    "def _get_sample_image_binary(input_name, output_name):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    inputs.append(httpclient.InferInput(input_name, [1, 3, 224, 224], \"FP32\"))\n",
    "    input_data = np.array(get_sample_image(), dtype=np.float32)\n",
    "    input_data = np.expand_dims(input_data, axis=0)\n",
    "    inputs[0].set_data_from_numpy(input_data, binary_data=True)\n",
    "    outputs.append(httpclient.InferRequestedOutput(output_name, binary_data=True))\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length\n",
    "\n",
    "\n",
    "def get_sample_image_binary_pt():\n",
    "    return _get_sample_image_binary(\"INPUT__0\", \"OUTPUT__0\")\n",
    "\n",
    "\n",
    "def get_sample_image_binary_trt():\n",
    "    return _get_sample_image_binary(\"input\", \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d73ac1",
   "metadata": {},
   "source": [
    "#### Invoke target model on Sagemaker Endpoint\n",
    "\n",
    "Once the endpoint is successfully created, we can send inference request to endpoint using invoke_endpoint API. We specify the TargetModel in the invocation call and pass in the payload for each model type. Sample invocation for PyTorch model and TensorRT model is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce4568",
   "metadata": {},
   "source": [
    "#### TensorRT model prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef84ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f2687",
   "metadata": {},
   "source": [
    "#### ONNX model prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX payload\n",
    "onnx_payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"input\",\n",
    "            \"shape\": [1, 3, 224, 224],\n",
    "            \"datatype\": \"FP32\",\n",
    "            \"data\": get_sample_image(),\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(onnx_payload),\n",
    ")\n",
    "\n",
    "response = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "output = response[\"outputs\"][0][\"data\"]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb3031",
   "metadata": {},
   "source": [
    "#### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm_client.delete_model(ModelName=sm_model_name)\n",
    "# sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "# sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f65b36",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|cv|realtime|Triton|single-model|resnet_onnx-backend|resnet_onnx_backend_SME_triton_v2.ipynb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
