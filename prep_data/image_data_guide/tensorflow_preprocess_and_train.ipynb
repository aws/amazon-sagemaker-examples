{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, Structure, and Preprocess Image Data for TensorFlow Models\n",
    "\n",
    "**Notes**: \n",
    "* This notebook should be used with the conda_pytorch_latest_p36 kernel\n",
    "* You can also explore image preprocessing with PyTorch and SageMaker Built-in Algorithms by running [Download, Structure, and Preprocess Image Data for PyTorch Models](pytorch_preprocess_and_train.ipynb) and [Download, Structure, and Preprocess Image Data for SageMaker Built-In Algorithms](builtin_preprocess_and_train.ipynb), respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of this notebook is to demonstrate how you can preprocess image data to train PyTorch Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Part 1: Download the Dataset](#Part-1:-Download-the-Dataset)\n",
    "1. [Part 2: Structure the Dataset](#Part-2:-Structure-the-Dataset)\n",
    "1. [Part 3: Preprocess Images for TensorFlow Models](#Part-3:-Preprocess-Images-for-TensorFlow-Models)\n",
    "1. [Part 4: Train the TensorFlow Model](#Part-4:-Train-the-TensorFlow-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Download the Dataset\n",
    "----\n",
    "----\n",
    "In this section, you will use a dataset manifest to download animal images from the COCO dataset for all ten animal classes. You will then download frog images from the CIFAR dataset and add them to your COCO animal images. In order to simulate coming to SageMaker with your own dataset, we will keep the data in an unstructured form until the next notebook where you will learn the best practices for structuring an image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import shutil\n",
    "import urllib\n",
    "import pathlib\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread, imwrite\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The COCO and CIFAR Datasets\n",
    "___\n",
    "For this series of notebooks we will be sampling images from the [COCO dataset](https://cocodataset.org) and [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) (before beginning the notebooks in this series, it's a good idea to browse each dataset website to familiaraize youreself with the data). Both are datasets of images, but come formatted very differently. The COCO dataset contains images from Flickr that represent a real-world dataset which isn't formatted or resized specifically for deep learning. This makes it a good dataset for this guide because we want it to be as comprehensive as possible. The CIFAR-10 images, on the other hand, are preprocessed specifically for deep learning as they come cropped, resized and vectorized (i.e. not in a readable image format). This notebooks will show you how to work with both types of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the annotations\n",
    "____\n",
    "The dataset annotation file contains info on each image in the dataset such as the class, superclass, file name and url to download the file. Just the annotations for the COCO dataset are about 242MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "urllib.request.urlretrieve(anno_url, \"coco-annotations.zip\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.unpack_archive(\"coco-annotations.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the annotations into Python\n",
    "The training and validation annotations come in separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"annotations/instances_train2017.json\", \"r\") as f:\n",
    "    train_metadata = json.load(f)\n",
    "\n",
    "with open(\"annotations/instances_val2017.json\", \"r\") as f:\n",
    "    val_metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract only the animal annotations\n",
    "___\n",
    "To limit the scope of the dataset for this guide we're only using the images of animals in the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = {\n",
    "    c[\"id\"]: c[\"name\"] for c in train_metadata[\"categories\"] if c[\"supercategory\"] == \"animal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract metadata and image filepaths\n",
    "For the train and validation sets, the data we need for the image labels and the filepaths are under different headings in the annotations. We have to extract each out and combine them into a single annotation in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annos = {}\n",
    "for a in train_metadata[\"annotations\"]:\n",
    "    if a[\"category_id\"] in category_labels:\n",
    "        train_annos[a[\"image_id\"]] = {\"category_id\": a[\"category_id\"]}\n",
    "\n",
    "train_images = {}\n",
    "for i in train_metadata[\"images\"]:\n",
    "    train_images[i[\"id\"]] = {\"coco_url\": i[\"coco_url\"], \"file_name\": i[\"file_name\"]}\n",
    "\n",
    "val_annos = {}\n",
    "for a in val_metadata[\"annotations\"]:\n",
    "    if a[\"category_id\"] in category_labels:\n",
    "        val_annos[a[\"image_id\"]] = {\"category_id\": a[\"category_id\"]}\n",
    "\n",
    "val_images = {}\n",
    "for i in val_metadata[\"images\"]:\n",
    "    val_images[i[\"id\"]] = {\"coco_url\": i[\"coco_url\"], \"file_name\": i[\"file_name\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine label and filepath info\n",
    "Later in this series of guides we'll make our own train, validation and test splits. For this reason we'll combine the training and validation datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, anno in train_annos.items():\n",
    "    anno.update(train_images[id])\n",
    "\n",
    "for id, anno in val_annos.items():\n",
    "    anno.update(val_images[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_annos = {}\n",
    "for k, v in train_annos.items():\n",
    "    all_annos.update({k: v})\n",
    "for k, v in val_annos.items():\n",
    "    all_annos.update({k: v})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the dataset\n",
    "___\n",
    "In order to make working with the data easier, we'll select 250 images from each class at random. To make sure you get the same set of cell images for each run of this we'll also set Numpy's random seed to 0. This is a small fraction of the dataset, but it demonstrates how using transfer learning can give you good results without needing very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_annos = {}\n",
    "\n",
    "for category_id in category_labels:\n",
    "    subset = [k for k, v in all_annos.items() if v[\"category_id\"] == category_id]\n",
    "    sample = np.random.choice(subset, size=250, replace=False)\n",
    "    for k in sample:\n",
    "        sample_annos[k] = all_annos[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a download function\n",
    "In order to parallelize downloading the images we must wrap the download and save process with a function for multi-threading with joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, path):\n",
    "    data = imread(url)\n",
    "    imwrite(path / url.split(\"/\")[-1], data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the sample of the dataset (2,500 images, ~5min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = pathlib.Path(\"data_sample_2500\")\n",
    "sample_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_backend(\"threading\", n_jobs=5):\n",
    "    Parallel(verbose=3)(\n",
    "        delayed(download_image)(a[\"coco_url\"], sample_dir) for a in sample_annos.values()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with CIFAR-10 frog data\n",
    "___\n",
    "The COCO dataset doesn't include any images of frogs, but let's say our model must also be able to label images of frogs. To fix this we can download another dataset of images which includes frogs, sample 250 frog images and add them to our existing image data. These images are much smaller (32x32) so they will appear pixelated and blurry when we increase the size of them to (244x244). We'll use the CIFAR-10 dataset to achieve this. As you'll see the CIFAR-10 dataset comes formatted in a very different manner from COCO dataset. We must process the CIFAR-10 data into individual image files so that it's congruent to our COCO images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and extract the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cs.toronto.edu/%7Ekriz/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tarfile.open(\"cifar-10-python.tar.gz\")\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open first batch of CIFAR-10 dataset\n",
    "The CIFAR-10 dataset comes in five training batches and one test batch. Each training batch has 10,000 randomly ordered images. Since we only need 250 frog images for our dataset, just pulling from the first batch will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./cifar-10-batches-py/data_batch_1\", \"rb\") as f:\n",
    "    batch_1 = pickle.load(f, encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = batch_1[b\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull 250 sample frog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frog_indices = np.array(batch_1[b\"labels\"]) == 6\n",
    "sample_frog_indices = np.random.choice(frog_indices.nonzero()[0], size=250, replace=False)\n",
    "sample_data = image_data[sample_frog_indices, :]\n",
    "frog_images = sample_data.reshape(len(sample_data), 3, 32, 32).transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View frog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4, figsize=(10, 7))\n",
    "indices = np.random.randint(low=0, high=249, size=12)\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(frog_images[indices[i]])\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write sample frog images to `data_sample_2500` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frog_filenames = np.array(batch_1[b\"filenames\"])[sample_frog_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, filename in enumerate(frog_filenames):\n",
    "    filename = filename.decode()\n",
    "    data = frog_images[idx]\n",
    "    if filename.endswith(\".png\"):\n",
    "        filename = filename.replace(\".png\", \".jpg\")\n",
    "    imwrite(sample_dir / filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir.rename(\"data_sample_2750\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add frog annotations to `sample_annos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels[26] = \"frog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_anno_idx = np.array(list(sample_annos.keys())).max() + 1\n",
    "\n",
    "frog_anno_ids = range(next_anno_idx, next_anno_idx + len(frog_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, frog_id in enumerate(frog_anno_ids):\n",
    "    sample_annos[frog_id] = {\n",
    "        \"category_id\": 26,\n",
    "        \"file_name\": frog_filenames[idx].decode().replace(\".png\", \".jpg\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Structure the Dataset\n",
    "----\n",
    "----\n",
    "\n",
    "In this section, you will properly structure your image files for ingestion by the model. Then, we will use Python to create the new folder structure and copy the files into the correct set and label folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper folder structure\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although most tools can accommodate data in any file structure with enough tinkering, it makes most sense to use the sensible defaults that frameworks like MXNet, TensorFlow and PyTorch all share to make data ingestion as smooth as possible. By default, most tools will look for image data in the file structure depicted below:\n",
    "```\n",
    "+-- train\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|\n",
    "+-- val\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|\n",
    "+-- test\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "```\n",
    "You will notice that the COCO dataset does not come structured like above so we must use the annotation data to help restructure the folders of the COCO dataset so they match the pattern above. Once the new directory structures are created you can use your desired framework's data loading tool to gracefully load and define transformation for your image data. Many datasets may already be in this structure in which case you can skip this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train, validation and test splits\n",
    "___\n",
    "We should divide our data into train, validation and test splits. A typical split ratio is 80/10/10. Our image classification algorithm will train on the first 80% (training) and evaluate its performance at each epoch with the next 10% (validation) and we'll give our model's final accuracy results using the last 10% (test). It's important that before we split the data we make sure to shuffle it randomly so that class distribution among splits is roughly proportional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "image_ids = sorted(list(sample_annos.keys()))\n",
    "np.random.shuffle(image_ids)\n",
    "first_80 = int(len(image_ids) * 0.8)\n",
    "next_10 = int(len(image_ids) * 0.9)\n",
    "train_ids, val_ids, test_ids = np.split(image_ids, [first_80, next_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new folder structure and copy image files\n",
    "___\n",
    "This new folder structure can then be read by data loaders for SageMaker's built-in algorithms, TensorFlow or PyTorch for easy loading of the image data into your framework of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstruct_dir = Path(\"data_sample_2750\")\n",
    "struct_dir = Path(\"data_structured\")\n",
    "struct_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for name, split in zip([\"train\", \"val\", \"test\"], [train_ids, val_ids, test_ids]):\n",
    "    split_dir = struct_dir / name\n",
    "    split_dir.mkdir(exist_ok=True)\n",
    "    for image_id in tqdm(split):\n",
    "        category_dir = split_dir / f'{category_labels[sample_annos[image_id][\"category_id\"]]}'\n",
    "        category_dir.mkdir(exist_ok=True)\n",
    "        source_path = (unstruct_dir / sample_annos[image_id][\"file_name\"]).as_posix()\n",
    "        target_path = (category_dir / sample_annos[image_id][\"file_name\"]).as_posix()\n",
    "        shutil.copy(source_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Preprocess Images for TensorFlow Models\n",
    "----\n",
    "----\n",
    "\n",
    "In this notebook, you will create resizing and data augmentation transforms for trainging with the TensorFlow framework. You will also convert your data to TensorFlow's TFRecord format for the most efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "___\n",
    "For this guide we'll use the SageMaker Python SDK version 2.9.2. By default, SageMaker Notebooks come with version 1.72.0. Other guides provided by Amazon may be set up to work with other versions of the Python SDK so you may wish to roll-back to 1.72.0. In addition to updating the SageMaker SDK we'll also update TensorFlow to 2.3.1 and install TensorFlow Datasets.\n",
    "\n",
    "We will also debug our code by training on the instance running this notebook (Local Mode). In order to run through one epoch of training in a reasonable amount of time I advise using a notebook backed by a p2.xlarge instance. Once youre script has completely run locally and all bugs have been ironed out, then you can switch back to a smaller instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update SageMaker Python SDK and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "original_sagemaker_version = !pip list | grep -E \"sagemaker\\s\" | awk '{print $2}'\n",
    "original_tensorflow_version = !pip list | grep -E \"tensorflow\\s\" | awk '{print $2}'\n",
    "!{sys.executable} -m pip install -q \"sagemaker==2.9.2\" \"tensorflow-serving-api==2.3.0\" \"tensorflow==2.3.1\" \"tensorflow-datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker  updated  {original_sagemaker_version[0]} -> {sagemaker.__version__}\")\n",
    "print(f\"tensorflow updated  {original_tensorflow_version[0]} -> {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data with TensorFlow Datasets\n",
    "___\n",
    "TensorFlow Datasets is a helpful module for getting your data ready for use with TensorFlow and Keras by generating wrapper for the dataset and each record in it. This wrapper has mathods which allow you to easily control sharding, batch size, and prefetching as well data transformations and augmentations. TensorFlow Datasets can also import many external datasets from the internet which come already structured and annoatated. However, for this guide we'll assume that your dataset isn't perfectly organized from the get-go.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ImageFolder builder\n",
    "\n",
    "`tfds.ImageFolder` is a pre-made builder for reading image data in the common folder structure we created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = tfds.ImageFolder(\"./data_structured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your image data is cataloged, you can generate a TensorFlow dataset for traing and validation. These datasets are very flexible can by be used for processing, augmentation and training with just TensorFlow or with Keras as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow resizing and augmentations\n",
    "___\n",
    "\n",
    "In this step we create separate datasets for training and validation then define the necessary transformations required before our algorithm can train on the data. We will also define image augmentations which allow us to get the most out of the data we have and improve training effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training and validation datasets\n",
    "The `.as_dataset()` method is a conveient way generating `(image, label)` tuples required by the training algorithm\n",
    "* split - designates the data split for this dataset\n",
    "* shuffle_files - mix the order of files\n",
    "* as_supervised - discards any metadata just keeping the (image, label) tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = image_folder.as_dataset(split=[\"train\"], shuffle_files=True, as_supervised=True)[0]\n",
    "\n",
    "# create a sample which is easy to iterate through for example purposes\n",
    "sample_ds = train_ds.take(100).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define resize transformation\n",
    "Before going to the GPU for training, all image data must have the same dimensions for length, width and channel. Typically, algorithms use a square format so the length and width are the same and many pre-made datasets areadly have the images nicely cropped into squares. However, most real-world datasets will begin with images in many different dimensions and ratios. In order to prep our dataset for training we need to resize and crop the images if they aren't already square. \n",
    "\n",
    "This transformation is deceptivley simple because if we want to keep the images from looking squished or stretched, we need to crop it to a square *and* we want to make sure the important object in the image doesn't get cropped out. Unfortunately, there is no easy way to make sure each crop is optimal so we typically choose a center crop which works well most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, label):\n",
    "    image = tf.image.resize(image, (400, 400), preserve_aspect_ratio=True)\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 244, 244)\n",
    "    return (image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the cell below to see the resize transform on different image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "image, label = next(sample_ds)\n",
    "image_resized = resize(image, label)[0]\n",
    "ax[0].imshow(image)\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_title(f\"Before - {image.shape}\")\n",
    "ax[1].imshow(image_resized / 255)\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set_title(f\"After - {image_resized.shape}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data augmentations\n",
    "An easy way to improve trainging is to randomly augment the images to help our training algorithm generalize better. Threre are many augmentations to choose from, but keep in mind that the more we add to our augment function, the more processing will be required before we can send the image to the GPU for training. Also, it's important to note that we don't need to augment the validation data because we want to generate a prediction on the image as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, 0.2)\n",
    "    image = tf.image.random_hue(image, 0.1)\n",
    "    return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "image, label = next(sample_ds)\n",
    "image_aug = augment(image, label)[0]\n",
    "ax[0].imshow(image)\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(image_aug)\n",
    "ax[1].axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply transformations to the datasets\n",
    "The training data set will get the resize and augment functions applied to it, but the validation dataset only gets resized because it's not directly used for training. When we call the `.map()` method to apply the transformation to each record. However, it doesn't actually transform the image yet. Rather, the transformation will be fully applied by the CPU right before it gets sent to the GPU for training. This is nice beause we can experiment quickly without having to wait for all the images to be transformed.\n",
    "\n",
    "You may be wondering why we're applying the transformations randomly. This is done because our training algorithm will cycle through the data in epochs. Each epoch it will get a chance to view the image again so instead of sending the same image through each time, we'll apply a random augmentation. Ideally, we'd let the algorithm see all versions of the image each epoch, but this would scale the size of the training dataset by the number of augmentations. Scaling the data storage and training time by that factor isn't worth the relatively minor changes introduced into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(resize).map(augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the transformations\n",
    "Just to make sure everything is working we can apply some transformations on a few images and view them to make sure the output looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4, figsize=(12, 7))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    sample = next(iter(train_ds))\n",
    "    ax.imshow(tf.cast(sample[0], dtype=tf.uint8))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the datasets to TFRecord format\n",
    "___\n",
    "TensorFlow has its own record format which makes moving and training on image data much easier. The format is called TFRecord and it basically converts your image data into one or more binary chunks that are much easier to read process than thousands of individual files. One downside to the TFRecord format is that the images it saves are uncompressed so if you have large jpeg images this can really add up to a large filesize. One solution is to use TFRecord's built-in compression, but you'll still have to uncompress the files during training which may slow training down. The solution we'll implement here is preform the resizing transform before converting to a TFRecord so the uncompressed image size is much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _image_as_bytes_feature(image):\n",
    "    \"\"\"Returns a bytes_list from an image tensor.\"\"\"\n",
    "\n",
    "    if image.dtype != tf.uint8:\n",
    "        # `tf.io.encode_jpeg``requires tf.unit8 input images, with values between\n",
    "        # 0 and 255. We do the conversion with the following function, if needed:\n",
    "        image = tf.image.convert_image_dtype(image, tf.uint8, saturate=True)\n",
    "\n",
    "    # We convert the image tensor back into a byte list...\n",
    "    image_string = tf.io.encode_jpeg(image, quality=90)\n",
    "\n",
    "    # ... and then into a Feature:\n",
    "    return _bytes_feature(image_string)\n",
    "\n",
    "\n",
    "def image_example(image_tensor, label):\n",
    "    image_shape = image_tensor.shape\n",
    "\n",
    "    feature = {\n",
    "        \"height\": _int64_feature(image_shape[0]),\n",
    "        \"width\": _int64_feature(image_shape[1]),\n",
    "        \"depth\": _int64_feature(image_shape[2]),\n",
    "        \"label\": _int64_feature(label),\n",
    "        \"image_raw\": _image_as_bytes_feature(image_tensor),\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define resize and rescale transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_rescale(image, label):\n",
    "    image = tf.image.resize(image, (400, 400), preserve_aspect_ratio=True)\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 244, 244)\n",
    "    image = image / 255.0\n",
    "    return (image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write data to TFRecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_dir = pathlib.Path(\"./data_tfrecord\")\n",
    "tfrecord_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = image_folder.as_dataset(split=[\"train\"], shuffle_files=True, as_supervised=True)[0]\n",
    "val_ds = image_folder.as_dataset(split=[\"val\"], shuffle_files=True, as_supervised=True)[0]\n",
    "test_ds = image_folder.as_dataset(split=[\"test\"], shuffle_files=True, as_supervised=True)[0]\n",
    "\n",
    "train_ds = train_ds.map(resize_rescale)\n",
    "val_ds = val_ds.map(resize_rescale)\n",
    "test_ds = test_ds.map(resize_rescale)\n",
    "\n",
    "for name, data_split in zip([\"train\", \"val\", \"test\"], [train_ds, val_ds, test_ds]):\n",
    "    record_file = f\"data_tfrecord/{name}.tfrecord\"\n",
    "    with tf.io.TFRecordWriter(record_file) as writer:\n",
    "        for image_tensor, label in tqdm(data_split):\n",
    "            tf_example = image_example(image_tensor, label)\n",
    "            writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload datasets to S3\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "prefix = \"DEMO-sm-preprocess-train-image-data-pytorch-algo\"\n",
    "s3 = boto3.resource(\"s3\")\n",
    "region = sagemaker.Session().boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload .rec files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uploader = sagemaker.s3.S3Uploader()\n",
    "\n",
    "for data_split in [\"train\", \"val\"]:\n",
    "    data_path = f\"data_tfrecord/{data_split}.tfrecord\"\n",
    "    data_s3_uri = s3_uploader.upload(\n",
    "        local_path=data_path, desired_s3_uri=f\"s3://{bucket_name}/{prefix}/data/{data_split}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the TensorFlow Model\n",
    "----\n",
    "----\n",
    "In this section, you will use the SageMaker SDK to create a TensorFlow Estimator and train it on a remote EC2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm hyperparameters\n",
    "___\n",
    "Hyperparamters represent the tuning knobs for our algorithm which we set before training begins. Typically they are pre-set to defaults so if we don't specify them we can still run the training algorithm, but they usually need tweaking to get optimal results. What these values should be depend entirely on the dataset. Unfortunately, there's no formula to tell us what the best settings are, we just have to try them ourselves and see what we get, but there are best practices and tips to help guide us in choosing them.\n",
    "\n",
    "* **Optimizer** - The optimizer refers to the optimization algorithm being used to choose the best weights. For deep learning on image data, SGD or ADAM is typically used.\n",
    "\n",
    "* **Learning Rate** - After each batch of training we update the model's weights to give us the best possible results for that batch. The learning rate controls by how much we should update the weights. Best practices dictate a value between 0.2 and .001, typically never going higher than 1. The higher the learning rate, the faster your training will converge to the optimal weights, but going too fast can lead you to overshoot the target. In this example, we're using the weights from a pre-trained model so we'd want to start with a lower learning rate because the weights have already been optimized and we don't want move too far away from them.\n",
    "\n",
    "*  **Epochs** - An epoch refers to one cycle through the training set and having more epochs to train means having more oppotunities to improve accracy. Suitable values range from 5 to 25 epochs depending on your time and budget constraints. Ideally, the right number of epochs is right before your validation accuracy plateaus.\n",
    "\n",
    "* **Batch Size** - Training on batches reduces the amount of data you need to hold in RAM and can speed up the training algorithm. For these reasons the training data is nearly always batched. The optimal batch size will depended on the dataset, how large the images are and how much RAM the training computer has. For a dataset like ours reasonable vaules would be bewteen 8 and 64 images per batch.\n",
    "\n",
    "* **Loss** - This is the type of loss function that will be used by the optimizer to update the model's weights during training. For training on a dataset with with more than two classes, the most common loss function is Cross-Entropy Loss. In TensorFlow, if your labels are a single number corresponding to a class (i.e. mutually excusive) then the type of loss is Sparse Categorical Crossentropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the training script\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "These helper functions define transformations needed to be done to our TFRecords datasets before training. For more in-depth info see the Pre-processing guide in this series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 7,23p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution safety\n",
    "For safety we wrap the training code in this standard if statement though it is not strictly required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 25p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse argument variables\n",
    "These argument variables are passed via the hyperparameter argument for the estimator config and the input argument to the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 27,34p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use autotune for configuring parallelization\n",
    "In order to speed up training, TensorFlow can spread certain tasks scross mutilple cores. It can be difficult to determine the optimal number of workers to spread the work across (too few and you underutilizing your GPU and too many will cause a lag due to the overhead of scheduling the work). Luckily, TensorFlow comes wih a method of determing the right amount based on the computer doing the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 36p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the datasets\n",
    "The training and validation datasets are loaded. Augmentation is applied to the training data, but not the validation data. We don't need to do any resizing or rescaling because we already applied this transformation when we converted the images to TDRecord files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 38,56p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine if GPU is available\n",
    "This will set the device of training as the GPU if a GPU is available, otherwise it'll use a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 58,63p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and modify the base model\n",
    "First the device context is set to ensure we're using the proper device (GPU or CPU). Then we use a ResNet50 architecture and initialize the weights to weights pre-trainged on the ImageNet dataset. Since the top layer of the pretained model is configured for the ImageNet images, we need to removbe the classification layer (`inlcude_top=False`) and replace it with a classifiaction layer for our 11 animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 65,73p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimizer and train the model\n",
    "For this example we'll use SGD to optimize the weights of the model. At the end of training the weights for the epoch with the best validation accuracy are saved so we can load the model later for predictions on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_tensorflow/tensorflow_train.py\" | sed -n 75,85p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator configuration\n",
    "___\n",
    "\n",
    "These define the the resources to use for training and how they are configured. Here are some important one to single out:\n",
    "\n",
    "* **entry_point (str)** – Path (absolute or relative) to the Python source file which should be executed as the entry point to training. If source_dir is specified, then entry_point must point to a file located at the root of source_dir.\n",
    "\n",
    "* **framework_version (str)** – PyTorch version you want to use for executing your model training code. Defaults to None. Required unless image_uri is provided. List of supported versions: https://github.com/aws/sagemaker-python-sdk#pytorch-sagemaker-estimators.\n",
    "\n",
    "* **py_version (str)** – Python version you want to use for executing your model training code. One of ‘py2’ or ‘py3’. Defaults to None. Required unless image_uri is provided.\n",
    "\n",
    "* **source_dir (str)** – Path (absolute, relative or an S3 URI) to a directory with any other training source code dependencies aside from the entry point file (default: None). If source_dir is an S3 URI, it must point to a tar.gz file. Structure within this directory are preserved when training on Amazon SageMaker.\n",
    "\n",
    "* **dependencies (list[str])** – A list of paths to directories (absolute or relative) with any additional libraries that will be exported to the container (default: []). The library folders will be copied to SageMaker in the same folder where the entrypoint is copied. If ‘git_config’ is provided, ‘dependencies’ should be a list of relative locations to directories with any additional libraries needed in the Git repo.\n",
    "\n",
    "* **git_config (dict[str, str])** – Git configurations used for cloning files, including repo, branch, commit, 2FA_enabled, username, password and token. The repo field is required. All other fields are optional. repo specifies the Git repository where your training script is stored. If you don’t provide branch, the default value ‘master’ is used. If you don’t provide commit, the latest commit in the specified branch is used.\n",
    "\n",
    "* **role (str)** – An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs that create Amazon SageMaker endpoints use this role to access training data and model artifacts. After the endpoint is created, the inference code might use the IAM role, if it needs to access an AWS resource.\n",
    "\n",
    "* **instance_count (int)** – Number of Amazon EC2 instances to use for training.\n",
    "\n",
    "* **instance_type (str)** – Type of EC2 instance to use for training, for example, ‘ml.c4.xlarge’.\n",
    "\n",
    "* **volume_size (int)** – Size in GB of the EBS volume to use for storing input data during training (default: 30). Must be large enough to store training data if File Mode is used (which is the default).\n",
    "\n",
    "* **model_uri (str)** – URI where a pre-trained model is stored, either locally or in S3 (default: None). If specified, the estimator will create a channel pointing to the model so the training job can download it. This model can be a ‘model.tar.gz’ from a previous training job, or other artifacts coming from a different source. In local mode, this should point to the path in which the model is located and not the file itself, as local Docker containers will try to mount the URI as a volume.\n",
    "\n",
    "* **output_path (str)** - S3 location for saving the training result (model artifacts and output files). If not specified, results are stored to a default bucket. If the bucket with the specific name does not exist, the estimator creates the bucket during the fit() method execution. file:// urls are used for local mode. For example: ‘file://model/’ will save to the model folder in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on an EC2 instance\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define hyperparameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 3,\n",
    "    \"batch-size\": 32,\n",
    "    \"learning-rate\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the estimator configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_config = {\n",
    "    \"entry_point\": \"tensorflow_train.py\",\n",
    "    \"source_dir\": \"training_tensorflow\",\n",
    "    \"framework_version\": \"2.3\",\n",
    "    \"py_version\": \"py37\",\n",
    "    \"instance_type\": \"ml.p3.2xlarge\",\n",
    "    \"instance_count\": 1,\n",
    "    \"role\": sagemaker.get_execution_role(),\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"output_path\": f\"s3://{bucket_name}/{prefix}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(**estimator_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the data channels for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_channels = {\n",
    "    \"training\": f\"s3://{bucket_name}/{prefix}/data/train/train.tfrecord\",\n",
    "    \"validation\": f\"s3://{bucket_name}/{prefix}/data/val/val.tfrecord\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit(s3_data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model and predict on test data\n",
    "___\n",
    "\n",
    "After training the model and saving it to S3, we can retrive it and load it back into TensorFlow to generate predicions. It's important that after training we evaluate the model on the test data. This data has never been seen by the model for trainging or for choosing the best epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the trained model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.s3.S3Downloader().download(tf_estimator.model_data, \"training_tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfile = tarfile.open(\"training_tensorflow/model.tar.gz\")\n",
    "tfile.extractall(\"training_tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"training_tensorflow/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load images from the test dataset for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = tfds.ImageFolder(\"./data_structured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord_parser(record):\n",
    "    features = {\n",
    "        \"height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"depth\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"image_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(record, features)\n",
    "    return tf.io.decode_jpeg(parsed_features[\"image_raw\"]), parsed_features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.TFRecordDataset(filenames=[\"data_tfrecord/test.tfrecord\"], num_parallel_reads=2)\n",
    "\n",
    "test_ds = test_ds.map(tfrecord_parser, num_parallel_calls=2).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link the model predictions (0 to 9) back to original class names (bear to zebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = {idx: name for idx, name in enumerate(sorted(category_labels.values()))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show validation images with model predictions\n",
    "Re-run cell to see more predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4, figsize=(10, 7))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    sample = next(iter(test_ds))\n",
    "    image = sample[0]\n",
    "    pred = model.predict(tf.expand_dims(image, axis=0))\n",
    "    pred_name = category_labels[np.argmax(pred)]\n",
    "    ax.imshow(image)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"prediction: {pred_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
