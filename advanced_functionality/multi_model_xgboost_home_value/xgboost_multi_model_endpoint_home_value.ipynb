{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using XGBoost\n",
    "With [Amazon SageMaker multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html), customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of a large number of models, which can be served from a common inference container to save inference costs, needs to be invokable on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, an endpoint deploying a single model is still the best choice.\n",
    "\n",
    "At a high level, Amazon SageMaker manages the loading and unloading of models for a multi-model endpoint, as they are needed. When an invocation request is made for a particular model, Amazon SageMaker routes the request to an instance assigned to that model, downloads the model artifacts from S3 onto that instance, and initiates loading of the model into the memory of the container. As soon as the loading is complete, Amazon SageMaker performs the requested invocation and returns the result. If the model is already loaded in memory on the selected instance, the downloading and loading steps are skipped and the invocation is performed immediately.\n",
    "\n",
    "To demonstrate how multi-model endpoints are created and used, this notebook provides an example using a set of XGBoost models that each predict housing prices for a single location. This domain is used as a simple example to easily experiment with multi-model endpoints.\n",
    "\n",
    "The Amazon SageMaker multi-model endpoint capability is designed to work across with Mxnet, PyTorch and Scikit-Learn machine learning frameworks (TensorFlow coming soon), SageMaker XGBoost, KNN, and Linear Learner algorithms.\n",
    "\n",
    "In addition, Amazon SageMaker multi-model endpoints are also designed to work with cases where you bring your own container that integrates with the multi-model server library. An example of this can be found [here](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/multi_model_bring_your_own) and documentation [here.](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. [Generate synthetic data for housing models](#Generate-synthetic-data-for-housing-models)\n",
    "1. [Train multiple house value prediction models](#Train-multiple-house-value-prediction-models)\n",
    "1. [Create the Amazon SageMaker MultiDataModel entity](#Create-the-Amazon-SageMaker-MultiDataModel-entity)\n",
    "  1. [Create the Multi-Model Endpoint](#Create-the-multi-model-endpoint)\n",
    "  1. [Deploy the Multi-Model Endpoint](#deploy-the-multi-model-endpoint)\n",
    "1. [Get Predictions from the endpoint](#Get-predictions-from-the-endpoint)\n",
    "1. [Additional Information](#Additional-information)\n",
    "1. [Clean up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data\n",
    "\n",
    "The code below contains helper functions to generate synthetic data in the form of a `1x7` numpy array representing the features of a house.\n",
    "\n",
    "The first entry in the array is the randomly generated price of a house. The remaining entries are the features (i.e. number of bedroom, square feet, number of bathrooms, etc.).\n",
    "\n",
    "These functions will be used to generate synthetic data for training, validation, and testing. It will also allow us to submit synthetic payloads for inference to test our multi-model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HOUSES_PER_LOCATION = 1000\n",
    "LOCATIONS  = ['NewYork_NY',    'LosAngeles_CA',   'Chicago_IL',    'Houston_TX',   'Dallas_TX',\n",
    "              'Phoenix_AZ',    'Philadelphia_PA', 'SanAntonio_TX', 'SanDiego_CA',  'SanFrancisco_CA']\n",
    "PARALLEL_TRAINING_JOBS = 4 # len(LOCATIONS) if your account limits can handle it\n",
    "MAX_YEAR = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_price(house):\n",
    "    _base_price = int(house['SQUARE_FEET'] * 150)\n",
    "    _price = int(_base_price + (10000 * house['NUM_BEDROOMS']) + \\\n",
    "                               (15000 * house['NUM_BATHROOMS']) + \\\n",
    "                               (15000 * house['LOT_ACRES']) + \\\n",
    "                               (15000 * house['GARAGE_SPACES']) - \\\n",
    "                               (5000 * (MAX_YEAR - house['YEAR_BUILT'])))\n",
    "    return _price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_house():\n",
    "    _house = {'SQUARE_FEET':   int(np.random.normal(3000, 750)),\n",
    "              'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "              'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "              'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "              'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "              'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10)))}\n",
    "    _price = gen_price(_house)\n",
    "    return [_price, _house['YEAR_BUILT'],   _house['SQUARE_FEET'], \n",
    "                    _house['NUM_BEDROOMS'], _house['NUM_BATHROOMS'], \n",
    "                    _house['LOT_ACRES'],    _house['GARAGE_SPACES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_houses(num_houses):\n",
    "    _house_list = []\n",
    "    for i in range(num_houses):\n",
    "        _house_list.append(gen_random_house())\n",
    "    _df = pd.DataFrame(_house_list, \n",
    "                       columns=['PRICE',        'YEAR_BUILT',    'SQUARE_FEET',  'NUM_BEDROOMS',\n",
    "                                'NUM_BATHROOMS','LOT_ACRES',     'GARAGE_SPACES'])\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train multiple house value prediction models\n",
    "\n",
    "In the follow section, we are setting up the code to train a house price prediction model for each of 4 different cities.\n",
    "\n",
    "As such, we will launch multiple training jobs asynchronously, using the XGBoost algorithm.\n",
    "\n",
    "In this notebook, we will be using the AWS Managed XGBoost Image for both training and inference - this image provides native support for launching multi-model endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import image_uris\n",
    "import boto3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "BUCKET = sagemaker_session.default_bucket()\n",
    "\n",
    "# This is references the AWS managed XGBoost container\n",
    "XGBOOST_IMAGE = image_uris.retrieve(region=boto3.Session().region_name, framework='xgboost', version='1.0-1')\n",
    "\n",
    "DATA_PREFIX = 'XGBOOST_BOSTON_HOUSING'\n",
    "MULTI_MODEL_ARTIFACTS = 'multi_model_artifacts'\n",
    "\n",
    "TRAIN_INSTANCE_TYPE = 'ml.m4.xlarge'\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.m4.xlarge'\n",
    "\n",
    "ENDPOINT_NAME = 'mme-xgboost-housing'\n",
    "\n",
    "MODEL_NAME = ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split a given dataset into train, validation, and test\n",
    "\n",
    "The code below will generate 3 sets of data. 1 set to train, 1 set for validation and 1 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 7\n",
    "SPLIT_RATIOS = [0.6, 0.3, 0.1]\n",
    "\n",
    "def split_data(df):\n",
    "    # split data into train and test sets\n",
    "    seed      = SEED\n",
    "    val_size  = SPLIT_RATIOS[1]\n",
    "    test_size = SPLIT_RATIOS[2]\n",
    "    \n",
    "    num_samples = df.shape[0]\n",
    "    X1 = df.values[:num_samples, 1:] # keep only the features, skip the target, all rows\n",
    "    Y1 = df.values[:num_samples, :1] # keep only the target, all rows\n",
    "\n",
    "    # Use split ratios to divide up into train/val/test\n",
    "    X_train, X_val, y_train, y_val = \\\n",
    "        train_test_split(X1, Y1, test_size=(test_size + val_size), random_state=seed)\n",
    "    # Of the remaining non-training samples, give proper ratio to validation and to test\n",
    "    X_test, X_test, y_test, y_test = \\\n",
    "        train_test_split(X_val, y_val, test_size=(test_size / (test_size + val_size)), \n",
    "                         random_state=seed)\n",
    "    # reassemble the datasets with target in first column and features after that\n",
    "    _train = np.concatenate([y_train, X_train], axis=1)\n",
    "    _val   = np.concatenate([y_val,   X_val],   axis=1)\n",
    "    _test  = np.concatenate([y_test,  X_test],  axis=1)\n",
    "\n",
    "    return _train, _val, _test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch a single training job for a given housing location\n",
    "There is nothing specific to multi-model endpoints in terms of the models it will host. They are trained in the same way as all other SageMaker models. Here we are using the XGBoost estimator and not waiting for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_training_job(location):\n",
    "    # clear out old versions of the data\n",
    "    s3_bucket = s3.Bucket(BUCKET)\n",
    "    full_input_prefix = f'{DATA_PREFIX}/model_prep/{location}'\n",
    "    s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()\n",
    "\n",
    "    # upload the entire set of data for all three channels\n",
    "    local_folder = f'data/{location}'\n",
    "    inputs = sagemaker_session.upload_data(path=local_folder, key_prefix=full_input_prefix)\n",
    "    print(f'Training data uploaded: {inputs}')\n",
    "    \n",
    "    _job = 'xgb-{}'.format(location.replace('_', '-'))\n",
    "    full_output_prefix = f'{DATA_PREFIX}/model_artifacts/{location}'\n",
    "    s3_output_path = f's3://{BUCKET}/{full_output_prefix}'\n",
    "\n",
    "    \n",
    "    xgb = sagemaker.estimator.Estimator(XGBOOST_IMAGE, role, \n",
    "                                        instance_count=1, instance_type=TRAIN_INSTANCE_TYPE,\n",
    "                                        output_path=s3_output_path, base_job_name=_job,\n",
    "                                        sagemaker_session=sagemaker_session)\n",
    "    \n",
    "    xgb.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6, subsample=0.8, silent=0, \n",
    "                            early_stopping_rounds=5, objective='reg:linear', num_round=25) \n",
    "    \n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "    train_input = sagemaker.inputs.TrainingInput(s3_data=inputs+'/train', \n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    \n",
    "    val_input   = sagemaker.inputs.TrainingInput(s3_data=inputs+'/val', \n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    \n",
    "    remote_inputs = {'train': train_input, 'validation': val_input}\n",
    "\n",
    "    xgb.fit(remote_inputs, wait=False)\n",
    "    \n",
    "    # Return the estimator object\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kick off a model training job for each housing location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_locally(location, train, val, test):\n",
    "    os.makedirs(f'data/{location}/train')\n",
    "    np.savetxt( f'data/{location}/train/{location}_train.csv', train, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs(f'data/{location}/val')\n",
    "    np.savetxt(f'data/{location}/val/{location}_val.csv', val, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs(f'data/{location}/test')\n",
    "    np.savetxt(f'data/{location}/test/{location}_test.csv', test, delimiter=',', fmt='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded: s3://sagemaker-eu-west-1-872120163790/XGBOOST_BOSTON_HOUSING/model_prep/NewYork_NY\n",
      "Training data uploaded: s3://sagemaker-eu-west-1-872120163790/XGBOOST_BOSTON_HOUSING/model_prep/LosAngeles_CA\n",
      "Training data uploaded: s3://sagemaker-eu-west-1-872120163790/XGBOOST_BOSTON_HOUSING/model_prep/Chicago_IL\n",
      "Training data uploaded: s3://sagemaker-eu-west-1-872120163790/XGBOOST_BOSTON_HOUSING/model_prep/Houston_TX\n",
      "\n",
      "4 training jobs launched: ['xgb-NewYork-NY-2020-11-23-10-50-10-689', 'xgb-LosAngeles-CA-2020-11-23-10-50-11-151', 'xgb-Chicago-IL-2020-11-23-10-50-14-171', 'xgb-Houston-TX-2020-11-23-10-50-15-164']\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "estimators = []\n",
    "\n",
    "shutil.rmtree('data', ignore_errors=True)\n",
    "\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    _houses = gen_houses(NUM_HOUSES_PER_LOCATION)\n",
    "    _train, _val, _test = split_data(_houses)\n",
    "    save_data_locally(loc, _train, _val, _test)\n",
    "    estimator = launch_training_job(loc)\n",
    "    estimators.append(estimator)\n",
    "\n",
    "print()\n",
    "print(f'{len(estimators)} training jobs launched: {[x.latest_training_job.job_name for x in estimators]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for all model training to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_training_job_to_complete(estimator):\n",
    "    job = estimator.latest_training_job.job_name\n",
    "    print(f'Waiting for job: {job}')\n",
    "    status = estimator.latest_training_job.describe()['TrainingJobStatus']\n",
    "    while status == 'InProgress':\n",
    "        time.sleep(45)\n",
    "        status = estimator.latest_training_job.describe()['TrainingJobStatus']\n",
    "        if status == 'InProgress':\n",
    "            print(f'{job} job status: {status}')\n",
    "    print(f'DONE. Status for {job} is {status}\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job: xgb-NewYork-NY-2020-11-23-10-50-10-689\n",
      "xgb-NewYork-NY-2020-11-23-10-50-10-689 job status: InProgress\n",
      "xgb-NewYork-NY-2020-11-23-10-50-10-689 job status: InProgress\n",
      "DONE. Status for xgb-NewYork-NY-2020-11-23-10-50-10-689 is Completed\n",
      "\n",
      "Waiting for job: xgb-LosAngeles-CA-2020-11-23-10-50-11-151\n",
      "DONE. Status for xgb-LosAngeles-CA-2020-11-23-10-50-11-151 is Completed\n",
      "\n",
      "Waiting for job: xgb-Chicago-IL-2020-11-23-10-50-14-171\n",
      "DONE. Status for xgb-Chicago-IL-2020-11-23-10-50-14-171 is Completed\n",
      "\n",
      "Waiting for job: xgb-Houston-TX-2020-11-23-10-50-15-164\n",
      "DONE. Status for xgb-Houston-TX-2020-11-23-10-50-15-164 is Completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for est in estimators:\n",
    "    wait_for_training_job_to_complete(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the multi-model endpoint with the SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker Model from one of the Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = estimators[0]\n",
    "model = estimator.create_model(role=role, image_uri=XGBOOST_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Amazon SageMaker MultiDataModel entity\n",
    "\n",
    "We create the multi-model endpoint using the [```MultiDataModel```](https://sagemaker.readthedocs.io/en/stable/api/inference/multi_data_model.html) class.\n",
    "\n",
    "You can create a MultiDataModel by directly passing in a `sagemaker.model.Model` object - in which case, the Endpoint will inherit information about the image to use, as well as any environmental variables, network isolation, etc., once the MultiDataModel is deployed.\n",
    "\n",
    "In addition, a MultiDataModel can also be created without explictly passing a `sagemaker.model.Model` object. Please refer to the documentation for additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.multidatamodel import MultiDataModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where our MME will read models from on S3.\n",
    "model_data_prefix = f's3://{BUCKET}/{DATA_PREFIX}/{MULTI_MODEL_ARTIFACTS}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mme = MultiDataModel(name=MODEL_NAME,\n",
    "                     model_data_prefix=model_data_prefix,\n",
    "                     model=model,# passing our model - passes container image needed for the endpoint\n",
    "                     sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Multi Model Endpoint\n",
    "\n",
    "You need to consider the appropriate instance type and number of instances for the projected prediction workload across all the models you plan to host behind your multi-model endpoint. The number and size of the individual models will also drive memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = mme.deploy(initial_instance_count=1,\n",
    "                       instance_type=ENDPOINT_INSTANCE_TYPE,\n",
    "                       endpoint_name=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our endpoint has launched! Let's look at what models are available to the endpoint!\n",
    "\n",
    "By 'available', what we mean is, what model artfiacts are currently stored under the S3 prefix we defined when setting up the `MultiDataModel` above i.e. `model_data_prefix`.\n",
    "\n",
    "Currently, since we have no artifacts (i.e. `tar.gz` files) stored under  our defined S3 prefix, our endpoint, will have no models 'available' to serve inference requests.\n",
    "\n",
    "We will demonstrate how to make models 'available' to our endpoint below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No models visible!\n",
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets deploy model artifacts to be found by the endpoint\n",
    "\n",
    "We are now using the `.add_model()` method of the `MultiDataModel` to copy over our model artifacts from where they were initially stored, during training, to where our endpoint will source model artifacts for inference requests.\n",
    "\n",
    "`model_data_source` refers to the location of our model artifact (i.e. where it was deposited on S3 after training completed)\n",
    "\n",
    "`model_data_path` is the **relative** path to the S3 prefix we specified above (i.e. `model_data_prefix`) where our endpoint will source models for inference requests.\n",
    "\n",
    "Since this is a **relative** path, we can simply pass the name of what we wish to call the model artifact at inference time (i.e. `Chicago_IL.tar.gz`)\n",
    "\n",
    "### Dynamically deploying additional models\n",
    "\n",
    "It is also important to note, that we can always use the `.add_model()` method, as shown below, to dynamically deploy more models to the endpoint, to serve up inference requests as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for est in estimators:\n",
    "    artifact_path = est.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']\n",
    "    model_name = artifact_path.split('/')[-4]+'.tar.gz'\n",
    "    # This is copying over the model artifact to the S3 location for the MME.\n",
    "    mme.add_model(model_data_source=artifact_path, model_data_path=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have added the 4 model artifacts from our training jobs!\n",
    "\n",
    "We can see that the S3 prefix we specified when setting up `MultiDataModel` now has 4 model artifacts. As such, the endpoint can now serve up inference requests for these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chicago_IL.tar.gz',\n",
       " 'Houston_TX.tar.gz',\n",
       " 'LosAngeles_CA.tar.gz',\n",
       " 'NewYork_NY.tar.gz']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions from the endpoint\n",
    "\n",
    "Recall that ```mme.deploy()``` returns a [RealTimePredictor](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/predictor.py#L35) that we saved in a variable called ```predictor```.\n",
    "\n",
    "We will use ```predictor``` to submit requests to the endpoint.\n",
    "\n",
    "XGBoost supports ```text/csv``` for the content type and accept type. For more information on XGBoost Input/Output Interface, please see [here.](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#InputOutput-XGBoost)\n",
    "\n",
    "Since the default RealTimePredictor does not have a serializer or deserializer set for requests, we will also set these.\n",
    "\n",
    "This will allow us to submit a python list for inference, and get back a float response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = CSVSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "#predictor.content_type =predictor.content_type , removed as mentioned https://github.com/aws/sagemaker-python-sdk/blob/e8d16f8bc4c570f763f1129afc46ba3e0b98cdad/src/sagemaker/predictor.py#L82\n",
    "#predictor.accept = \"text/csv\" # removed also : https://github.com/aws/sagemaker-python-sdk/blob/e8d16f8bc4c570f763f1129afc46ba3e0b98cdad/src/sagemaker/predictor.py#L83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking models on a multi-model endpoint\n",
    "Notice the higher latencies on the first invocation of any given model. This is due to the time it takes SageMaker to download the model to the Endpoint instance and then load the model into the inference container. Subsequent invocations of the same model take advantage of the model already being loaded into the inference container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$548,638.31, took 16 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "predicted_value = predictor.predict(data=gen_random_house()[1:], target_model='Chicago_IL.tar.gz')\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print('${:,.2f}, took {:,d} ms\\n'.format(predicted_value[0], int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$517,095.44, took 19 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Invoke endpoint\n",
    "predicted_value = predictor.predict(data=gen_random_house()[1:], target_model='Chicago_IL.tar.gz')\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print('${:,.2f}, took {:,d} ms\\n'.format(predicted_value[0], int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$313,395.22, took 1,013 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Invoke endpoint\n",
    "predicted_value = predictor.predict(data=gen_random_house()[1:], target_model='Houston_TX.tar.gz')\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print('${:,.2f}, took {:,d} ms\\n'.format(predicted_value[0], int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$543,326.38, took 16 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Invoke endpoint\n",
    "predicted_value = predictor.predict(data=gen_random_house()[1:], target_model='Houston_TX.tar.gz')\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print('${:,.2f}, took {:,d} ms\\n'.format(predicted_value[0], int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating a model\n",
    "To update a model, you would follow the same approach as above and add it as a new model. For example, if you have retrained the `NewYork_NY.tar.gz` model and wanted to start invoking it, you would upload the updated model artifacts behind the S3 prefix with a new name such as `NewYork_NY_v2.tar.gz`, and then change the `target_model` field to invoke `NewYork_NY_v2.tar.gz` instead of `NewYork_NY.tar.gz`. You do not want to overwrite the model artifacts in Amazon S3, because the old version of the model might still be loaded in the containers or on the storage volume of the instances on the endpoint. Invocations to the new model could then invoke the old version of the model.\n",
    "\n",
    "Alternatively, you could stop the endpoint and re-deploy a fresh set of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Boto APIs to invoke the endpoint\n",
    "\n",
    "While developing interactively within a Jupyter notebook, since `.deploy()` returns a `RealTimePredictor` it is a more seamless experience to start invoking your endpoint using the SageMaker SDK. You have more fine grained control over the serialization and deserialization protocols to shape your request and response payloads to/from the endpoint.\n",
    "\n",
    "This is great for iterative experimentation within a notebook. Furthermore, should you have an application that has access to the SageMaker SDK, you can always import `RealTimePredictor` and attach it to an existing endpoint - this allows you to stick to using the high level SDK if preferable.\n",
    "\n",
    "Additional documentation on `RealTimePredictor` can be found [here.](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html?highlight=RealTimePredictor#sagemaker.predictor.RealTimePredictor)\n",
    "\n",
    "The lower level Boto3 SDK may be preferable if you are attempting to invoke the endpoint as a part of a broader architecture.\n",
    "\n",
    "Imagine an API gateway frontend that uses a Lambda Proxy in order to transform request payloads before hitting a SageMaker Endpoint - in this example, Lambda does not have access to the SageMaker Python SDK, and as such, Boto3 can still allow you to interact with your endpoint and serve inference requests.\n",
    "\n",
    "Boto3 allows for quick injection of ML intelligence via SageMaker Endpoints into existing applications with minimal/no refactoring to existing code.\n",
    "\n",
    "Boto3 will submit your requests as a binary payload, while still allowing you to supply your desired `Content-Type` and `Accept` headers with serialization being handled by the inference container in the SageMaker Endpoint.\n",
    "\n",
    "Additional documentation on `.invoke_endpoint()` can be found [here.](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "def predict_one_house_value(features, model_name):\n",
    "    print(f'Using model {model_name} to predict price of this house: {features}')\n",
    "    \n",
    "    # Notice how we alter the list into a string as the payload\n",
    "    body = ','.join(map(str, features)) + '\\n'\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "                        EndpointName=ENDPOINT_NAME,\n",
    "                        ContentType='text/csv',\n",
    "                        TargetModel=model_name,\n",
    "                        Body=body)\n",
    "    \n",
    "    predicted_value = json.loads(response['Body'].read())[0]\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print('${:,.2f}, took {:,d} ms\\n'.format(predicted_value, int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Chicago_IL.tar.gz to predict price of this house: [1990, 2269, 3, 1.0, 0.95, 3]\n",
      "$309,167.28, took 57 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_one_house_value(gen_random_house()[1:], 'Chicago_IL.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Here, to be sure we are not billed for endpoints we are no longer using, we clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
