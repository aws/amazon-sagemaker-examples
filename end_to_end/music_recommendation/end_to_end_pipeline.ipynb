{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Deploy, and Monitor the Music Recommender Model using SageMaker Pipelines\n",
    "\n",
    "----\n",
    "## Background\n",
    "\n",
    "In this notebook, we'll build an end-to-end pipeline to create a music recommender using [SageMaker Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html), which will automate  the entire modeling process from the beginning of data ingestion to monitoring the model. SageMaker Pipelines is a tool for building machine learning pipelines that take advantage of direct SageMaker integration. Because of this integration, you can create a pipeline and set up SageMaker Projects for orchestration using a tool that handles much of the step creation and management for you.\n",
    "\n",
    "----\n",
    "### Contents\n",
    "1. [Architecture: Create a SageMaker Pipeline to Automate All the Steps from Data Prep to Model Deployment](#Architecture:-Create-a-SageMaker-Pipeline-to-Automate-All-the-Steps-from-Data-Prep-to-Model-Deployment)\n",
    "1. [SageMaker Pipeline Overview](#SageMaker-Pipeline-Overview)\n",
    "1. [Clean Up](#Clean-Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required and/or update third-party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -Uq pip\n",
    "!python -m pip install -q sagemaker==2.45.0 imbalanced-learn awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pathlib\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set region and boto3 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pprint\n",
    "sys.path.insert(1, './code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dir = \"/opt/ml/processing\"\n",
    "\n",
    "# Output name is auto-generated from the select node's ID + output name from the flow file. \n",
    "# You can change to a different node ID to export a different step in the flow file\n",
    "output_name_tracks = \"19ad8e80-2002-4ee9-9753-fe9a384b1166.default\" # tracks node in flow file\n",
    "output_name_user_preferences = \"7a6dad19-2c80-43e3-b03d-ec23c3842ae9.default\" # joined node in flow file\"\n",
    "output_name_ratings = \"9a283380-91ca-478e-be99-6ba3bf57c680.default\" # ratings node in flow file\n",
    "\n",
    "#======> variables used for parameterizing the notebook run\n",
    "flow_instance_count = 1\n",
    "flow_instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "deploy_model_instance_type = \"ml.m4.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture: Create a SageMaker Pipeline to Automate All the Steps from Data Prep to Model Deployment\n",
    "\n",
    "----\n",
    "\n",
    "![arch diagram](./images/music-rec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Pipeline Overview\n",
    "\n",
    "---- \n",
    "\n",
    "### List of Steps\n",
    "\n",
    "1. [Step 1: Data Wrangler Preprocessing Step](#Step-1:-Data-Wrangler-Preprocessing-Step)\n",
    "1. [Step 2: Create Dataset and Train/Test Split](#Step-2:-Create-Dataset-and-Train/Test-Split)\n",
    "1. [Step 3: Train XGBoost Model](#Step-3:-Train-XGBoost-Model)\n",
    "1. [Step 4: Model Pre-Deployment Step](#Step-4:-Model-Pre-Deployment-Step)\n",
    "1. [Step 5: Register Model](#Step-5:-Register-Model)\n",
    "1. [Step 6: Deploy Model](#Step-6:-Deploy-Model)\n",
    "1. [Step 7: Monitor Model Deployed to SageMaker Hosted Endpoint](#Step-7:-Monitor-Model-Deployed-to-SageMaker-Hosted-Endpoint)\n",
    "1. [Combine Steps and Run Pipeline](#Combine-Steps-and-Run-Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that you've manually done each step in our machine learning workflow, you can certain steps to allow for faster model experimentation without sacrificing transparncy and model tracking. In this section you will create a pipeline which trains a new model, persists the model in SageMaker and then adds the model to the registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline parameters\n",
    "An important feature of SageMaker Pipelines is the ability to define the steps ahead of time, but be able to change the parameters to those steps at execution without having to re-define the pipeline. This can be achieved by using ParameterInteger, ParameterFloat or ParameterString to define a value upfront which can be modified when you call `pipeline.start(parameters=parameters)` later. Only certain parameters can be defined this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_param = ParameterString(\n",
    "    name=\"TrainingInstance\",\n",
    "    default_value=\"ml.m4.xlarge\",\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Wrangler Preprocessing Step\n",
    "\n",
    "#### Upload flow to S3\n",
    "This will become an input to the first step and, as such, needs to be in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"01_music_dataprep.flow\"\n",
    "\n",
    "s3_client.upload_file(Filename=flow_file_name, Bucket=bucket, Key=f'{prefix}/dataprep-notebooks/music_dataprep.flow')\n",
    "flow_s3_uri = f's3://{bucket}/{prefix}/dataprep-notebooks/music_dataprep.flow'\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Data Wrangler step's inputs\n",
    "In this step, new data from `source=` will be transformed according to the SageMaker Data Wrangler `.flow` file and later added to the existing feature groups we created in the `02` notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = []\n",
    "\n",
    "## Input - S3 Source: tracks.csv\n",
    "data_sources.append(ProcessingInput(\n",
    "    source=f\"s3://{bucket}/{prefix}/data/tracks_new.csv\", # You can override this to point to another dataset on S3\n",
    "    destination=f\"{processing_dir}/data/tracks_new.csv\",\n",
    "    input_name=\"tracks_new.csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))\n",
    "\n",
    "## Input - S3 Source: ratings.csv\n",
    "data_sources.append(ProcessingInput(\n",
    "    source=f\"s3://{bucket}/{prefix}/data/ratings_new.csv\", # You can override this to point to another dataset on S3\n",
    "    destination=f\"{processing_dir}/data/ratings_new.csv\",\n",
    "    input_name=\"ratings_new.csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))\n",
    "\n",
    "## Input - Flow: 01_music_dataprep.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=f\"{processing_dir}/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define outputs for the Data Wranger step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature group names we previously created in notebooks 02a-c\n",
    "fg_name_tracks = 'track-features-music-rec'\n",
    "fg_name_ratings = 'ratings-features-music-rec'\n",
    "fg_name_user_preferences = 'user-5star-track-features-music-rec'\n",
    "dw_ecrlist = {\n",
    "    'region':{'us-west-2':'174368400705',\n",
    "              'us-east-2':'415577184552',\n",
    "              'us-west-1':'926135532090',\n",
    "              'us-east-1':'663277389841'\n",
    "             }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_output_tracks = sagemaker.processing.ProcessingOutput(\n",
    "    output_name=output_name_tracks,\n",
    "    app_managed=True,\n",
    "    feature_store_output=sagemaker.processing.FeatureStoreOutput(\n",
    "        feature_group_name=fg_name_tracks)\n",
    "    )\n",
    "\n",
    "flow_output_user_preferences = sagemaker.processing.ProcessingOutput(\n",
    "    output_name=output_name_user_preferences,\n",
    "    app_managed=True,\n",
    "    feature_store_output=sagemaker.processing.FeatureStoreOutput(\n",
    "        feature_group_name=fg_name_user_preferences)\n",
    "    )\n",
    "\n",
    "flow_output_ratings = sagemaker.processing.ProcessingOutput(\n",
    "    output_name=output_name_ratings,\n",
    "    app_managed=True,\n",
    "    feature_store_output=sagemaker.processing.FeatureStoreOutput(\n",
    "        feature_group_name=fg_name_ratings)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output configuration used as processing job container arguments \n",
    "output_config_tracks = {\n",
    "    output_name_tracks: {\n",
    "        \"content_type\": \"CSV\"\n",
    "    }\n",
    "}\n",
    "\n",
    "output_config_user_preferences = {\n",
    "    output_name_user_preferences: {\n",
    "        \"content_type\": \"CSV\"\n",
    "    }\n",
    "}\n",
    "\n",
    "output_config_ratings = {\n",
    "    output_name_ratings: {\n",
    "        \"content_type\": \"CSV\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define processor and processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.network import NetworkConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangler Container URL\n",
    "# You can also find the proper container uri by exporting your Data Wrangler flow to a pipeline notebook\n",
    "\n",
    "container_uri = f\"{dw_ecrlist['region'][region]}.dkr.ecr.{region}.amazonaws.com/sagemaker-data-wrangler-container:1.x\"\n",
    "\n",
    "\n",
    "flow_processor = sagemaker.processing.Processor(\n",
    "    role=sagemaker_role, \n",
    "    image_uri=container_uri, \n",
    "    instance_count=flow_instance_count, \n",
    "    instance_type=flow_instance_type, \n",
    "    volume_size_in_gb=30,\n",
    "    network_config=NetworkConfig(enable_network_isolation=False),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "flow_step_tracks = ProcessingStep(\n",
    "    name='DataWranglerStepTracks', \n",
    "    processor=flow_processor, \n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[flow_output_tracks],\n",
    "    job_arguments=[f\"--output-config '{json.dumps(output_config_tracks)}'\"],\n",
    ")\n",
    "\n",
    "flow_step_ratings = ProcessingStep(\n",
    "    name='DataWranglerStepRatings', \n",
    "    processor=flow_processor, \n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[flow_output_ratings],\n",
    "    job_arguments=[f\"--output-config '{json.dumps(output_config_ratings)}'\"]\n",
    ")\n",
    "\n",
    "flow_step_user_preferences = ProcessingStep(\n",
    "    name='DataWranglerStepUserPref', \n",
    "    processor=flow_processor, \n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[flow_output_user_preferences],\n",
    "    job_arguments=[f\"--output-config '{json.dumps(output_config_user_preferences)}'\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Dataset and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='./code/create_datasets.py', Bucket=bucket, Key=f'{prefix}/code/create_datasets.py')\n",
    "create_dataset_script_uri = f's3://{bucket}/{prefix}/code/create_datasets.py'\n",
    "\n",
    "create_dataset_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    instance_count=2,\n",
    "    volume_size_in_gb=100,\n",
    "    base_job_name='music-recommendation-split-data',\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "create_dataset_step = ProcessingStep(\n",
    "    name='SplitData',\n",
    "    processor=create_dataset_processor,\n",
    "    outputs = [\n",
    "        sagemaker.processing.ProcessingOutput(output_name='train_data', source=f'{processing_dir}/output/train'),\n",
    "        sagemaker.processing.ProcessingOutput(output_name='test_data',  source=f'{processing_dir}/output/test')\n",
    "    ],\n",
    "    job_arguments=[\"--feature-group-name-tracks\", fg_name_tracks,\n",
    "                   \"--feature-group-name-ratings\", fg_name_ratings,\n",
    "                   \"--feature-group-name-user-preferences\", fg_name_user_preferences,\n",
    "                   \"--bucket-name\", bucket,\n",
    "                   \"--bucket-prefix\", prefix,\n",
    "                   \"--region\", region\n",
    "                  ],\n",
    "    code=create_dataset_script_uri,\n",
    "    depends_on=[flow_step_tracks.name, flow_step_ratings.name, flow_step_user_preferences.name]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train XGBoost Model\n",
    "In this step we use the ParameterString `train_instance_param` defined at the beginning of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"4\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"100\"\n",
    "}\n",
    "\n",
    "save_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_estimator = Estimator(\n",
    "    role=sagemaker_role,\n",
    "    instance_count=2,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    volume_size=60,\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", region, \"0.90-2\"),\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=f's3://{bucket}/{prefix}/training_jobs',\n",
    "    base_job_name='xgb-music-rec-model-pipeline',\n",
    "    max_run=1800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = TrainingStep(\n",
    "    name='TrainStep',\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        'train': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=create_dataset_step.properties.ProcessingOutputConfig.Outputs['train_data'].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        'validation': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=create_dataset_step.properties.ProcessingOutputConfig.Outputs['test_data'].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TuningStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model Pre-Deployment Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.model.Model(\n",
    "    name='music-recommender-xgboost-model',\n",
    "    image_uri=train_step.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sagemaker_role\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(\n",
    "    instance_type=\"ml.m4.xlarge\"\n",
    ")\n",
    "\n",
    "create_model_step = CreateModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Register Model\n",
    "In this step you will use the ParameterString `model_approval_status` defined at the outset of the pipeline code.\n",
    "\n",
    "\n",
    "[Pipeline Overview](#pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_step = RegisterModel(\n",
    "    name=\"XgboostRegisterModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=prefix,\n",
    "    approval_status=model_approval_status,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='./code/deploy_model.py', Bucket=bucket, Key=f'{prefix}/code/deploy_model.py')\n",
    "deploy_model_script_uri = f's3://{bucket}/{prefix}/code/deploy_model.py'\n",
    "pipeline_endpoint_name = 'music-rec-model-endpoint'\n",
    "\n",
    "deploy_model_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=sagemaker_role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    volume_size_in_gb=60,\n",
    "    base_job_name='music-recommender-deploy-model',\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "deploy_step = ProcessingStep(\n",
    "    name='DeployModel',\n",
    "    processor=deploy_model_processor,\n",
    "    job_arguments=[\n",
    "        \"--model-name\", create_model_step.properties.ModelName, \n",
    "        \"--region\", region,\n",
    "        \"--endpoint-instance-type\", deploy_model_instance_type,\n",
    "        \"--endpoint-name\", pipeline_endpoint_name\n",
    "    ],\n",
    "    code=deploy_model_script_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Monitor Model Deployed to SageMaker Hosted Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='./code/model_monitor.py', Bucket=bucket, Key=f'{prefix}/code/model_monitor.py')\n",
    "model_monitor_script_uri = f's3://{bucket}/{prefix}/code/model_monitor.py'\n",
    "mon_schedule_name_base = 'music-recommender-daily-monitor'\n",
    "\n",
    "\n",
    "model_monitor_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=sagemaker_role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    volume_size_in_gb=60,\n",
    "    base_job_name='music-recommendation-model-monitor',\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "monitor_model_step = ProcessingStep(\n",
    "    name='ModelMonitor',\n",
    "    processor=model_monitor_processor,\n",
    "    outputs = [\n",
    "        sagemaker.processing.ProcessingOutput(output_name='model_baseline', source=f'{processing_dir}/output/baselineresults')\n",
    "    ],\n",
    "    job_arguments=[\"--baseline-data-uri\", val_data_uri,\n",
    "                   \"--bucket-name\", bucket,\n",
    "                   \"--bucket-prefix\", prefix,\n",
    "                   \"--endpoint\", pipeline_endpoint_name,\n",
    "                   \"--region\", region,\n",
    "                   \"--schedule-name\", mon_schedule_name_base\n",
    "                  ],\n",
    "    code=model_monitor_script_uri,\n",
    "    depends_on=[deploy_step.name]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Steps and Run Pipeline\n",
    "\n",
    "Once all of our steps are defined, we can put them together using the SageMaker `Pipeline` object. While we pass the steps in order so that it is easier to read, technically the order that we pass them does not matter since the pipeline DAG will parse it out properly based on any dependencies between steps. If the input of one step is the output of another step, the Pipelines understands which must come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_name = f'MusicRecommendationPipeline'\n",
    "dataprep_pipeline_name = f'MusicRecommendationDataPrepPipeline'\n",
    "train_deploy_pipeline_name = f'MusicRecommendationTrainDeployPipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: The Entire Pipeline End to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f'MusicRecommendationPipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        train_instance_param, \n",
    "        model_approval_status],\n",
    "    steps=[\n",
    "        flow_step_tracks,\n",
    "        flow_step_user_preferences,\n",
    "        flow_step_ratings,\n",
    "        create_dataset_step,\n",
    "        train_step, \n",
    "        create_model_step, \n",
    "        register_step,\n",
    "        deploy_step,\n",
    "        monitor_model_step  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: You may want to refrain from running an entire data prep and train deploy pipeline as one long pipeline and rather consider breaking the pipeline into two parts:\n",
    "\n",
    "#### Data Prep Pipeline\n",
    "\n",
    "Sometimes we may want to run a number of data prep steps and split the data, getting it ready for training and beyond. This may require multiple iterations. We can separate this process from the rest of the pipeline by including only these data prep steps in their own smaller data prep pipeline.\n",
    "\n",
    "#### Train Deploy Monitor Pipeline\n",
    "\n",
    "This allows you to have separation of concerns around the preparation of data distinct from that of training, tuning, deployment, inference and monitoring until you want to kick off a retraining only, data prep only, or the complete pipeline. With SageMaker Pipelines you have the flexibility of doing any one of these in a modular and iterative manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Data Prep Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dataprep = Pipeline(\n",
    "    name=dataprep_pipeline_name,\n",
    "    parameters=[\n",
    "        train_instance_param, \n",
    "        model_approval_status],\n",
    "    steps=[\n",
    "        flow_step_tracks,\n",
    "        flow_step_user_preferences,\n",
    "        flow_step_ratings,\n",
    "        create_dataset_step\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3: Train Deploy Monitor Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_step_no_depend = ProcessingStep(\n",
    "    name='SplitData',\n",
    "    processor=create_dataset_processor,\n",
    "    outputs = [\n",
    "        sagemaker.processing.ProcessingOutput(output_name='train_data', source=f'{processing_dir}/output/train'),\n",
    "        sagemaker.processing.ProcessingOutput(output_name='test_data',  source=f'{processing_dir}/output/test')\n",
    "    ],\n",
    "    job_arguments=[\"--feature-group-name-tracks\", fg_name_tracks,\n",
    "                   \"--feature-group-name-ratings\", fg_name_ratings,\n",
    "                   \"--feature-group-name-user-preferences\", fg_name_user_preferences,\n",
    "                   \"--bucket-name\", bucket,\n",
    "                   \"--bucket-prefix\", prefix,\n",
    "                   \"--region\", region\n",
    "                  ],\n",
    "    code=create_dataset_script_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_train_deploy_monitor = Pipeline(\n",
    "    name=train_deploy_pipeline_name,\n",
    "    parameters=[\n",
    "        train_instance_param, \n",
    "        model_approval_status],\n",
    "    steps=[\n",
    "        create_dataset_step_no_depend,\n",
    "        train_step, \n",
    "        create_model_step, \n",
    "        register_step,\n",
    "        deploy_step,\n",
    "        monitor_model_step  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline definition to the SageMaker Pipeline service\n",
    "Note: If an existing pipeline has the same name it will be overwritten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's choose the pipeline we want to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=sagemaker_role)\n",
    "pipeline_dataprep.upsert(role_arn=sagemaker_role)\n",
    "pipeline_train_deploy_monitor.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the entire pipeline definition\n",
    "Viewing the pipeline definition will all the string variables interpolated may help debug pipeline bugs. It is commented out here due to length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#json.loads(pipeline.describe()['PipelineDefinition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "Note this will take about 1 hour to complete. You can watch the progress of the Pipeline Job on your SageMaker Studio Components panel\n",
    "\n",
    "<!-- ![image.png](attachment:image.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special pipeline parameters can be defined or changed here\n",
    "parameters = {'TrainingInstance': 'ml.m5.4xlarge'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in the notebook, we defines several `ProcessingStep()`s and a `TrainingStep()` which our `Pipeline()` instance here will reference and kick off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start_response = pipeline.start(parameters=parameters)\n",
    "# start_response = pipeline_dataprep.start(parameters=parameters)\n",
    "# start_response = pipeline_train_deploy_monitor.start(parameters=parameters)\n",
    "start_response.wait(delay=60, max_attempts=200)\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completion we can use Sagemaker Studio's **Components and Registries** tab to see our Pipeline graph and any further error or log messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_helpers.delete_project_resources(\n",
    "    sagemaker_boto_client=sagemaker_boto_client, \n",
    "    sagemaker_session=sagemaker_session,\n",
    "    endpoint_names=[pipeline_endpoint_name],\n",
    "    pipeline_names=[pipeline_name, dataprep_pipeline_name, train_deploy_pipeline_name], \n",
    "    mpg_name=mpg_name,\n",
    "    prefix=prefix,\n",
    "    delete_s3_objects=True,\n",
    "    bucket_name=bucket\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
