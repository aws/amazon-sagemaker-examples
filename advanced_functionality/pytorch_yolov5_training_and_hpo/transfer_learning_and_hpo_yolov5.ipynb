{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8808ec4",
   "metadata": {},
   "source": [
    "# Transfer Learning and Hyperparameter Optimization for YOLOv5 using Amazon SageMaker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c7789",
   "metadata": {},
   "source": [
    "In Computer Vision (CV), object detection is a technique where a model predicts the presence of objects and locates them in an image using bounding boxes. YOLO (You Only Look Once) belongs to the family of models used for object detection.\n",
    "\n",
    "There are two approaches that are commonly adopted for this task: a two-step approach and a single shot approach. [YOLO](https://arxiv.org/abs/1506.02640) is a single shot approach to object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a053d59",
   "metadata": {},
   "source": [
    "This notebook takes you through how to use Amazon SageMaker for transfer learning a YOLOv5 model on custom data. In this notebook you use AWS [Deep Learning Containers (DLC)](https://github.com/aws/deep-learning-containers/blob/master/available_images.md) to customize your own training container. You use transfer learning to train a YOLOv5 model to detect random objects from a home (custom data), mostly from bathroom, kitchen and living-room environments. If you are using Amazon SageMaker Notebook instance to run this notebook, you should use the **Conda Python 3** kernel, if you are in a SageMaker Studio environment use the **Python 3 Data Science** kernel.\n",
    "\n",
    "You also use Amazon SageMaker's Automated Model Tuning feature to perform hyperparameter optimization to arrive at the best possible model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961dbcd2",
   "metadata": {},
   "source": [
    "In this notebook you use the Caltech Home Objects dataset [[1]Moreels and Perona, \u201cCaltech Home Objects 2006\u201d. CaltechDATA, Apr. 07, 2022. doi: 10.22002/D1.20089.](https://data.caltech.edu/records/bckkv-8my10). \n",
    "\n",
    "Broadly, you cover the following topics in this notebook:\n",
    "\n",
    "1. [Pre-requisites if you are in a SageMaker Studio environment](#Pre-requisites-if-you-are-in-a-SageMaker-Studio-environment)\n",
    "\n",
    "2. [Data Preparation](#Data-Preparation)\n",
    "    \n",
    "    a. [Converting from MATLAB to .txt](#Converting-from-MATLAB-to-.txt)\n",
    "    \n",
    "    b. [Converting from Amazon SageMaker Ground Truth bounding box to .txt [OPTIONAL]](#Converting-from-Amazon-SageMaker-Ground-Truth-bounding-box-to-.txt-[OPTIONAL])\n",
    "    \n",
    "    c. [Training and validation sets](#Training-and-validation-sets)\n",
    "    \n",
    "    \n",
    "3. [Set up the PyTorch environment for training](#Set-up-the-PyTorch-environment-for-training)\n",
    "\n",
    "    a. [Customizing a DLC for training](#Customising-a-DLC-for-training)\n",
    "    \n",
    "    b. [Get and then upload weights to S3 location](#Get,-and-then-upload-weights-to-a-S3-location)\n",
    "    \n",
    "\n",
    "4. [Training](#Training)\n",
    "\n",
    "\n",
    "5. [Hyperparameter Optimization with Amazon SageMaker Automated Model Tuning](#Hyperparameter-Optimization-with-Amazon-SageMaker-Automated-Model-Tuning)\n",
    "\n",
    "\n",
    "6. [Conclusion : Deploying YOLOv5 on Amazon SageMaker](#Conclusion-:-Deploying-YOLOv5-on-Amazon-SageMaker)\n",
    "\n",
    "\n",
    "\n",
    "You can download the dataset you will use in this notebook, you can do this in the [Data Preparation](#Data-Preparation) section below. Alternatively, you can download the dataset from [this page](https://data.caltech.edu/records/bckkv-8my10). If you download the dataset yourself, make sure you upload the downloaded file ```Home_Objects_06.zip``` to your working directory on the notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae1084",
   "metadata": {},
   "source": [
    "## Pre-requisites if you are in a SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba88dd",
   "metadata": {},
   "source": [
    "**Check your SageMaker execution role**, ensure that you have the following trust policy with AWS CodeBuild in place:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": [\n",
    "          \"codebuild.amazonaws.com\"\n",
    "        ]\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "Use this guidance on [editing trust relationship for an existing role](https://docs.aws.amazon.com/directoryservice/latest/admin-guide/edit_trust.html) to edit the trust policy for your SageMaker execution role.\n",
    "\n",
    "Add this inline policy, use this guidance from the AWS IAM documentation to [add a permission policy inline](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html#add-policies-console) to your SageMaker execution role.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"codebuild:DeleteProject\",\n",
    "                \"codebuild:CreateProject\",\n",
    "                \"codebuild:BatchGetBuilds\",\n",
    "                \"codebuild:StartBuild\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:codebuild:*:*:project/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"logs:CreateLogStream\",\n",
    "            \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/codebuild/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:GetLogEvents\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/codebuild/sagemaker-studio*:log-stream:*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"logs:CreateLogGroup\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ecr:CreateRepository\",\n",
    "                \"ecr:BatchGetImage\",\n",
    "                \"ecr:CompleteLayerUpload\",\n",
    "                \"ecr:DescribeImages\",\n",
    "                \"ecr:DescribeRepositories\",\n",
    "                \"ecr:UploadLayerPart\",\n",
    "                \"ecr:ListImages\",\n",
    "                \"ecr:InitiateLayerUpload\",\n",
    "                \"ecr:BatchCheckLayerAvailability\",\n",
    "                \"ecr:PutImage\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:ecr:*:*:repository/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"ecr:GetAuthorizationToken\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "              \"s3:GetObject\",\n",
    "              \"s3:DeleteObject\",\n",
    "              \"s3:PutObject\"\n",
    "              ],\n",
    "            \"Resource\": \"arn:aws:s3:::sagemaker-*/*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:CreateBucket\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::sagemaker*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:GetRole\",\n",
    "                \"iam:ListRoles\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"arn:aws:iam::*:role/*\",\n",
    "            \"Condition\": {\n",
    "                \"StringLikeIfExists\": {\n",
    "                    \"iam:PassedToService\": \"codebuild.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "You are now ready to proceed with executing the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58603b",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad18b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/Sample-pytorch-YOLOv5-HO-C3-Detection\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"Bucket Name: {} and the role is {}\".format(bucket, role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from botocore.exceptions import ClientError\n",
    "from sagemaker import image_uris\n",
    "from zipfile import ZipFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136fa651",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAGEMAKERWDIR = os.getcwd() + \"/\"\n",
    "DATASETLOCALBASE = SAGEMAKERWDIR + \"data\"\n",
    "ANNOTATIONSPATH = DATASETLOCALBASE + \"/\" + \"Home_Objects_06/Train/Gtruth\"\n",
    "YOLO5ANNOTATIONS = ANNOTATIONSPATH + \"/YOLOv5\"\n",
    "IMAGESPATH = DATASETLOCALBASE + \"/\" + \"Home_Objects_06/Train\"\n",
    "\n",
    "TESTANNOTATIONSPATH = DATASETLOCALBASE + \"/\" + \"Home_Objects_06/Test/Gtruth\"\n",
    "TESTYOLO5ANNOTATIONS = TESTANNOTATIONSPATH + \"/YOLOv5\"\n",
    "TESTIMAGESPATH = DATASETLOCALBASE + \"/\" + \"Home_Objects_06/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some utility functions to get the data ready for training.\n",
    "\n",
    "\n",
    "def unzipdataset(wrkdir, archname):\n",
    "    with ZipFile(archname, \"r\") as arch:\n",
    "        if not os.path.exists(DATASETLOCALBASE):\n",
    "            os.makedirs(DATASETLOCALBASE)\n",
    "        arch.extractall(path=DATASETLOCALBASE)\n",
    "        print(\"Done extracting zip archive!\")\n",
    "\n",
    "\n",
    "def createClassMap(annots=ANNOTATIONSPATH):\n",
    "    fllist = (\n",
    "        file\n",
    "        for file in os.listdir(ANNOTATIONSPATH)\n",
    "        if os.path.isfile(os.path.join(ANNOTATIONSPATH, file))\n",
    "    )\n",
    "    classlist = []\n",
    "    for i, fl in enumerate(fllist):\n",
    "        classlist.append({\"name\": fl, \"clas\": i})\n",
    "    return classlist\n",
    "\n",
    "\n",
    "def readannotfile(givenfile, silent=False):\n",
    "    \"\"\"\n",
    "    This function is good only for the test dataset.\n",
    "    This is because, for this dataset, the test dataset\n",
    "    keys are slightly different.\n",
    "    givenfile: full path to <filename>.JPG.mat\n",
    "    \"\"\"\n",
    "    if os.path.isdir(givenfile):\n",
    "        return False, 0.0, 0.0, 0.0, 0.0\n",
    "    annotations = scipy.io.loadmat(givenfile)\n",
    "    filesplit = givenfile.split(\"/\")[-1]\n",
    "    ante, post = filesplit.split(\".\")[0], filesplit.split(\".\")[1]\n",
    "    doesimageexist = os.path.exists(IMAGESPATH + \"/\" + ante + \".\" + post)\n",
    "    coords = []\n",
    "    for coord in annotations[\"outline\"][0][0][0]:\n",
    "        coords.append(coord)\n",
    "    if silent == False:\n",
    "        print(coords)\n",
    "        print(\"Checking if we have the corresponding image file\")\n",
    "        if doesimageexist:\n",
    "            print(\"Corresponding image file exists.\")\n",
    "        else:\n",
    "            print(\"Please check, corresponding image file does not exist.\")\n",
    "    topleft, topright, bottomright, bottomleft = coords[0], coords[1], coords[2], coords[3]\n",
    "    return doesimageexist, topleft, topright, bottomright, bottomleft\n",
    "\n",
    "\n",
    "def bounding_box_old_annotations(imagefile, annotationfile):\n",
    "    \"\"\"\n",
    "    Again, good for the training dataset.\n",
    "    imagefile: full or relative path to image file (.JPG)\n",
    "    annotationfile: full or relative path to the annotation file i.e. the *.mat file\n",
    "    \"\"\"\n",
    "    ## Get size of the image file\n",
    "    img = Image.open(imagefile)\n",
    "    ## Get annotations\n",
    "    die, tl, _, br, _ = readannotfile(annotationfile, silent=True)\n",
    "    width, height = img.size\n",
    "    plotted_image = ImageDraw.Draw(img)\n",
    "    plotted_image.rectangle(((tl[0], tl[1]), (br[0], br[1])), outline=\"red\", width=2)\n",
    "    plt.imshow(np.array(img))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_bounding_yolov5_annotations(imagefile, annotationfile):\n",
    "    \"\"\"\n",
    "    imagefile: full path to the image file *.JPG\n",
    "    annotationfile: full path to the *.mat file\n",
    "    \"\"\"\n",
    "    assert os.path.exists(imagefile)\n",
    "    image = Image.open(imagefile)\n",
    "\n",
    "    assert os.path.exists(imagefile)\n",
    "    f = open(annotationfile, \"r\")\n",
    "    bx = \"\"\n",
    "    for line in f:\n",
    "        bx = line  # We want to teest a single box in any image\n",
    "    f.close()\n",
    "\n",
    "    annotation_list = bx.strip().split(\" \")\n",
    "\n",
    "    annotations = np.array(annotation_list, dtype=np.float64)\n",
    "\n",
    "    w, h = image.size\n",
    "\n",
    "    plotted_image = ImageDraw.Draw(image)\n",
    "\n",
    "    chged_annots = np.array(annotations)\n",
    "    chged_annots[1] = annotations[1] * w\n",
    "    chged_annots[2] = annotations[2] * h\n",
    "    chged_annots[3] = annotations[3] * w\n",
    "    chged_annots[4] = annotations[4] * h\n",
    "    chged_annots[1] = chged_annots[1] - chged_annots[3] / 2  # xmin\n",
    "    chged_annots[2] = chged_annots[2] - chged_annots[4] / 2  # ymin\n",
    "    chged_annots[3] = chged_annots[1] + chged_annots[3]  # xmax\n",
    "    chged_annots[4] = chged_annots[2] + chged_annots[4]  # ymax\n",
    "\n",
    "    xmin, ymin, xmax, ymax = chged_annots[1], chged_annots[2], chged_annots[3], chged_annots[4]\n",
    "\n",
    "    plotted_image.rectangle(((xmin, ymin), (xmax, ymax)), outline=\"red\", width=2)\n",
    "\n",
    "    plt.imshow(np.array(image))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def convertMATLABToYolov5(annotationfile, imagefile):\n",
    "    \"\"\"\n",
    "    annotationsfile: full or relative path to the annotation file i.e. the *.mat file\n",
    "    imagefile: full or relative path to the image file\n",
    "    \"\"\"\n",
    "    clas, xcenter, ycenter, width, height = 0, 0.0, 0.0, 0.0, 0.0\n",
    "    width, height = Image.open(imagefile).size\n",
    "    ## Get the class index\n",
    "    cl = next((item for item in CLASSENUM if item[\"name\"] == annotationfile.split(\"/\")[-1]), None)\n",
    "    if cl != None:\n",
    "        clas = cl[\"clas\"]\n",
    "        yolofilename = cl[\"name\"].split(\".\")[0] + \".txt\"\n",
    "        _, tl, _, _, _ = readannotfile(annotationfile, silent=True)\n",
    "        xmin, ymin = tl[0], tl[1]\n",
    "        _, _, _, br, _ = readannotfile(annotationfile, silent=True)\n",
    "        xmax, ymax = br[0], br[1]\n",
    "        xcenter = (xmin + (xmax - xmin) / 2.0) / width\n",
    "        ycenter = (ymin + (ymax - ymin) / 2.0) / height\n",
    "        box_width = (xmax - xmin) / width\n",
    "        box_height = (ymax - ymin) / height\n",
    "\n",
    "        ## writing to the new annotation file\n",
    "        if not os.path.exists(YOLO5ANNOTATIONS):\n",
    "            os.makedirs(YOLO5ANNOTATIONS)\n",
    "        f = open(YOLO5ANNOTATIONS + \"/\" + yolofilename, \"w\")\n",
    "        print(\"Writing to {}\".format(YOLO5ANNOTATIONS + \"/\" + yolofilename))\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = f\n",
    "        print(\"{} {} {} {} {}\".format(clas, xcenter, ycenter, box_width, box_height))\n",
    "        sys.stdout = original_stdout\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5577cf5",
   "metadata": {},
   "source": [
    "Get the dataset for this notebook, you run the cell that follows. You can also download the dataset directly as stated in the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226adbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_client = boto3.client(\"s3\")\n",
    "dataset_client.download_file(\n",
    "    f\"sagemaker-sample-files\",\n",
    "    \"datasets/image/caltech-home-objects-2006/Home_Objects_06.zip\",\n",
    "    \"Home_Objects_06.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f96ea0",
   "metadata": {},
   "source": [
    "After you download the dataset, we unzip the bundle we just downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc668ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unzipdataset(SAGEMAKERWDIR, \"Home_Objects_06.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc3b892",
   "metadata": {},
   "source": [
    "You will now read one of the annotation files. These files contain coordinates stored in MATLAB .mat files. You have to change these files to the .txt format that YOLOv5 uses for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f70f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is an example of an annotations file\n",
    "readannotfile(ANNOTATIONSPATH + \"/\" + \"P1020595.JPG.mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500fab1",
   "metadata": {},
   "source": [
    "Make sure that the data is good, one of the things you can do is make sure that you have a MATLAB file for every JPG file. You should not get any output for the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d05e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fyle in os.listdir(ANNOTATIONSPATH):\n",
    "    valid, _, _, _, _ = readannotfile(ANNOTATIONSPATH + \"/\" + fyle, silent=True)\n",
    "    if not valid:\n",
    "        print(\"Filename: {}, invalid data: {}\".format(fyle, str(valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ce4dc",
   "metadata": {},
   "source": [
    "### Converting from MATLAB to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b991137",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSENUM = createClassMap()\n",
    "\n",
    "for annots in os.listdir(ANNOTATIONSPATH):\n",
    "    if os.path.isdir(ANNOTATIONSPATH + \"/\" + annots):\n",
    "        continue\n",
    "    parts = annots.split(\".\")\n",
    "    baseimage = parts[0] + \".\" + parts[1]\n",
    "    imagefile = IMAGESPATH + \"/\" + baseimage\n",
    "    convertMATLABToYolov5(ANNOTATIONSPATH + \"/\" + annots, imagefile)\n",
    "print(\"\\n\\n***Conversion to YOLOv5 TXT format complete!***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfbbca2",
   "metadata": {},
   "source": [
    "Check if you have got the converted annotations right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca6b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_bounding_yolov5_annotations(\n",
    "    IMAGESPATH + \"/\" + \"P1020595.JPG\", YOLO5ANNOTATIONS + \"/P1020595.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fab16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are done with the training dataset. Getting the validation dataset ready\n",
    "\n",
    "\n",
    "def getClassNum(givenfile):\n",
    "    cl = next(\n",
    "        (item for item in CLASSENUM if item[\"name\"] == givenfile.split(\"/\")[-1] + \".mat\"), None\n",
    "    )\n",
    "    return cl[\"clas\"]\n",
    "\n",
    "\n",
    "def createValidationData(givenfile, imagefile):\n",
    "    \"\"\"\n",
    "    givenfile: full path to the test <filename>.JPG.mat\n",
    "    imagefile: full path to the test <filename>.JPG\n",
    "    \"\"\"\n",
    "    width, height = Image.open(imagefile).size\n",
    "    # Lets get all the bounding boxes\n",
    "    bandcs = scipy.io.loadmat(givenfile)\n",
    "    for vertice in bandcs[\"outline\"][0]:\n",
    "        tl, tr, br, bl = vertice[0]\n",
    "        cls = getClassNum(vertice[1][0])\n",
    "        xmin, ymin, xmax, ymax = tl[0], tl[1], br[0], br[1]\n",
    "        xcenter = (xmin + (xmax - xmin) / 2.0) / width\n",
    "        ycenter = (ymin + (ymax - ymin) / 2.0) / height\n",
    "        box_width = (xmax - xmin) / width\n",
    "        box_height = (ymax - ymin) / height\n",
    "\n",
    "        ## writing to the new annotation file\n",
    "        if not os.path.exists(TESTYOLO5ANNOTATIONS):\n",
    "            os.makedirs(TESTYOLO5ANNOTATIONS)\n",
    "        yolofilename = givenfile.split(\"/\")[-1].split(\".\")[0] + \".txt\"\n",
    "        f = open(TESTYOLO5ANNOTATIONS + \"/\" + yolofilename, \"a\")\n",
    "        print(\"Writing to {}\".format(TESTYOLO5ANNOTATIONS + \"/\" + yolofilename))\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = f\n",
    "        print(\"{} {} {} {} {}\".format(cls, xcenter, ycenter, box_width, box_height))\n",
    "        sys.stdout = original_stdout\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def plot_bounding_yolov5_annotations_testimage(imagefile, annotationfile, num):\n",
    "    \"\"\"\n",
    "    imagefile: full path to image file\n",
    "    annotationfile: full path to annotation file\n",
    "    num: the annotation to display\n",
    "    \"\"\"\n",
    "    assert os.path.exists(imagefile)\n",
    "    image = Image.open(imagefile)\n",
    "\n",
    "    assert os.path.exists(imagefile)\n",
    "    f = open(annotationfile, \"r\")\n",
    "    bx = \"\"\n",
    "    for number, line in enumerate(f):\n",
    "        if number == num:\n",
    "            bx = line  # We want to teest a single box in any image\n",
    "    f.close()\n",
    "\n",
    "    annotation_list = bx.strip().split(\" \")\n",
    "\n",
    "    annotations = np.array(annotation_list, dtype=np.float64)\n",
    "\n",
    "    w, h = image.size\n",
    "\n",
    "    plotted_image = ImageDraw.Draw(image)\n",
    "\n",
    "    chged_annots = np.array(annotations)\n",
    "    chged_annots[1] = annotations[1] * w\n",
    "    chged_annots[2] = annotations[2] * h\n",
    "    chged_annots[3] = annotations[3] * w\n",
    "    chged_annots[4] = annotations[4] * h\n",
    "    chged_annots[1] = chged_annots[1] - chged_annots[3] / 2  # xmin\n",
    "    chged_annots[2] = chged_annots[2] - chged_annots[4] / 2  # ymin\n",
    "    chged_annots[3] = chged_annots[1] + chged_annots[3]  # xmax\n",
    "    chged_annots[4] = chged_annots[2] + chged_annots[4]  # ymax\n",
    "\n",
    "    xmin, ymin, xmax, ymax = chged_annots[1], chged_annots[2], chged_annots[3], chged_annots[4]\n",
    "\n",
    "    plotted_image.rectangle(((xmin, ymin), (xmax, ymax)), outline=\"red\", width=2)\n",
    "\n",
    "    plt.imshow(np.array(image))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for annots in os.listdir(TESTANNOTATIONSPATH):\n",
    "    if os.path.isdir(TESTANNOTATIONSPATH + \"/\" + annots):\n",
    "        continue\n",
    "    parts = annots.split(\".\")\n",
    "    baseimage = parts[0] + \".\" + parts[1]\n",
    "    imagefile = TESTIMAGESPATH + \"/\" + baseimage\n",
    "    createValidationData(TESTANNOTATIONSPATH + \"/\" + annots, imagefile)\n",
    "print(\"\\n\\n***Conversion to YOLOv5 TXT format complete!***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you have the test annotations right as well\n",
    "plot_bounding_yolov5_annotations_testimage(\n",
    "    TESTIMAGESPATH + \"/P1020827.JPG\", TESTYOLO5ANNOTATIONS + \"/P1020827.txt\", 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d91dd",
   "metadata": {},
   "source": [
    "### Converting from Amazon SageMaker Ground Truth bounding box to .txt [OPTIONAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c8fe4c",
   "metadata": {},
   "source": [
    "You can use [Amazon SageMaker Ground Truth](https://aws.amazon.com/sagemaker/data-labeling/) to build your data labeling workflows, it is a data labeling service that enables you to use [Amazon Mechanical Turk](https://www.mturk.com/), third party vendors, or your own private workforce for your data labeling tasks. SageMaker Ground Truth can be used to label images, text, videos and video frames, as well as 3D point clouds. It can also generate labeled synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad6547",
   "metadata": {},
   "source": [
    "If you are using SageMaker Ground Truth, you can create a [bounding box labeling job](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-bounding-box.html), this notebook demonstrates how to transform the output from a ground truth labeling job to an input format suitable for your YOLOv5 training job. Please refer to the [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-bounding-box.html) for guidance on how you can create a bounding box labeling job. \n",
    "\n",
    "Below is a sample output line from such a labeling job, formatted for easy reading. This is one line from a  ```output.manifest``` file. This file is in the [JSON lines format](https://jsonlines.org/). Along with this notebook you can find a sample output manifest file named ```sample_output.manifest```. This sample Ground Truth labeling job output file will be used to demonstrate how you can transform annotations to a format appropriate for YOLOv5 training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295457f",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "    \"source-ref\": \"s3://my-groundtruth-bucket/ground-truth-od-full-demo/images/000062a39995e348.jpg\",\n",
    "    \"category\": {\n",
    "        \"image_size\": [\n",
    "            {\n",
    "                \"width\": 680,\n",
    "                \"height\": 1024,\n",
    "                \"depth\": 3\n",
    "            }\n",
    "        ],\n",
    "        \"annotations\": [\n",
    "            {\n",
    "                \"class_id\": 0,\n",
    "                \"top\": 164.60000000000002,\n",
    "                \"left\": 138.8,\n",
    "                \"height\": 859,\n",
    "                \"width\": 443.8\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"category-metadata\": {\n",
    "        \"objects\": [\n",
    "            {\n",
    "                \"confidence\": 0.93\n",
    "            }\n",
    "        ],\n",
    "        \"class-map\": {\n",
    "            \"0\": \"Bird\"\n",
    "        },\n",
    "        \"type\": \"groundtruth/object-detection\",\n",
    "        \"human-annotated\": \"yes\",\n",
    "        \"creation-date\": \"2022-09-12T09:51:00.148118\",\n",
    "        \"job-name\": \"labeling-job/ground-truth-od-demo-1662974840\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606496ab",
   "metadata": {},
   "source": [
    "The above output shows a single class bounding box labeling job which is a single json line in the output manifest. The above output was extracted from the output manifest generated after running [this notebook](https://github.com/aws/amazon-sagemaker-examples/blob/6ac5bb28dcbe29e16d3cb8fe7169cabe1c6f34eb/ground_truth_labeling_jobs/ground_truth_object_detection_tutorial/object_detection_tutorial.ipynb) using a SageMaker Notebook instance. The function below ```convertGT2TXT```, transforms an output.manifest file to .TXT files that the YOLOv5 training job expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertGT2TXT(manifestfile, topannotationloc):\n",
    "    \"\"\"\n",
    "    Converts an output manifest file\n",
    "    from its existing JSONlines format\n",
    "    to the TXT format for YOLOv5\n",
    "    training.\n",
    "    manifestfile: The name of the output manifest file from the Ground Truth labeling job.\n",
    "    topannotationloc: The parent directory below which the TXT annotations files are placed.\n",
    "    \"\"\"\n",
    "    annotobj = []\n",
    "    with open(manifestfile) as mf:\n",
    "        for line in mf:\n",
    "            annotobj.append(json.loads(line))\n",
    "    for ob in annotobj:\n",
    "        ## Get the TXT filename\n",
    "        txtflname = (ob[\"source-ref\"].split(\"/\")[-1]).split(\".\")[-2] + \".txt\"\n",
    "        print(\"Writing to {}\".format(topannotationloc + \"/\" + txtflname))\n",
    "        ## Writing to the new annotation file\n",
    "        annotf = open(topannotationloc + \"/\" + txtflname, \"w\")\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = annotf\n",
    "        input_width = ob[\"category\"][\"image_size\"][0][\"width\"]\n",
    "        input_height = ob[\"category\"][\"image_size\"][0][\"height\"]\n",
    "        for annots in ob[\"category\"][\"annotations\"]:\n",
    "            clas = annots[\"class_id\"]\n",
    "            xcenter = (annots[\"left\"] + annots[\"width\"] / 2.0) / input_width\n",
    "            ycenter = (annots[\"top\"] + annots[\"height\"] / 2.0) / input_height\n",
    "            width = annots[\"width\"] / input_width\n",
    "            height = annots[\"height\"] / input_height\n",
    "            print(\"{} {} {} {} {}\".format(clas, xcenter, ycenter, width, height))\n",
    "        sys.stdout = original_stdout\n",
    "        annotf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating TXT files, assuming a directory does not exist\n",
    "!mkdir gt2txt\n",
    "convertGT2TXT(\"./sample_output.manifest\", \"gt2txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b78d77",
   "metadata": {},
   "source": [
    "Eventually, you need to build the same file structure with the TXT files generated from the above function execution, and image files, as demonstrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cc3cf",
   "metadata": {},
   "source": [
    "### Training and validation sets\n",
    "\n",
    "Here, you are creating the training and validation sets, with 20% set aside for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e073c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageList(direc):\n",
    "    fyls = []\n",
    "    for fyl in os.listdir(direc):\n",
    "        if os.path.isdir(direc + \"/\" + fyl):\n",
    "            continue\n",
    "        fyls.append(fyl)\n",
    "    random.shuffle(fyls)\n",
    "    return fyls\n",
    "\n",
    "\n",
    "fyls = getImageList(TESTIMAGESPATH)\n",
    "annots = [fyl.split(\".\")[0] + \".txt\" for fyl in fyls]\n",
    "\n",
    "## We will use only 50% of this test dataset for validation\n",
    "X_val, _, y_val, _ = train_test_split(fyls, annots, test_size=0.50, random_state=42)\n",
    "\n",
    "print(\"A set of 5 validation image files : \")\n",
    "X_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagefiles = getImageList(IMAGESPATH)\n",
    "annotationfiles = [fyl.split(\".\")[0] + \".txt\" for fyl in imagefiles]\n",
    "\n",
    "X_train = imagefiles\n",
    "y_train = annotationfiles\n",
    "\n",
    "print(\"A set of 5 training image files : \")\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a84a17",
   "metadata": {},
   "source": [
    "Right now, the data is organized like so:\n",
    "\n",
    "**Images**: ```IMAGESPATH/*.JPG```\n",
    "\n",
    "**Annotations**: ```YOLO5ANNOTATIONS/*.txt```\n",
    "\n",
    "You need to separate these into training and validation datasets. This is what it should look like on S3, and eventually, on the training container.\n",
    "\n",
    "**Training**: ```<base path>/[images|labels]/train```\n",
    "\n",
    "**Validation**: ```<base path>/[images|labels]/val```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3TRAINIMAGESLOC = prefix + \"/data/train/\"\n",
    "S3TRAINANNOTSLOC = prefix + \"/data/train/\"\n",
    "S3OTHERDATA = prefix + \"/classes/\"\n",
    "S3VALIMAGESLOC = prefix + \"/data/val/\"\n",
    "S3VALANNOTSLOC = prefix + \"/data/val/\"\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "for i in X_train:\n",
    "    try:\n",
    "        response = s3_client.upload_file(IMAGESPATH + \"/\" + i, bucket, S3TRAINIMAGESLOC + i)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "\n",
    "for i in X_val:\n",
    "    try:\n",
    "        response = s3_client.upload_file(TESTIMAGESPATH + \"/\" + i, bucket, S3VALIMAGESLOC + i)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "\n",
    "for i in y_train:\n",
    "    try:\n",
    "        response = s3_client.upload_file(YOLO5ANNOTATIONS + \"/\" + i, bucket, S3TRAINANNOTSLOC + i)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "\n",
    "for i in y_val:\n",
    "    try:\n",
    "        response = s3_client.upload_file(TESTYOLO5ANNOTATIONS + \"/\" + i, bucket, S3VALANNOTSLOC + i)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "\n",
    "TRAIN_CHANNEL = \"s3://\" + bucket + \"/\" + prefix + \"/data/train/\"\n",
    "VAL_CHANNEL = \"s3://\" + bucket + \"/\" + prefix + \"/data/val/\"\n",
    "\n",
    "## Upload the CLASSENUM list\n",
    "jsonclasses = json.dumps(CLASSENUM)\n",
    "with open(\"classenum.json\", \"w\") as outfile:\n",
    "    outfile.write(jsonclasses)\n",
    "try:\n",
    "    response = s3_client.upload_file(\"classenum.json\", bucket, S3OTHERDATA + \"classenum.json\")\n",
    "except ClientError as e:\n",
    "    logging.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec4794f",
   "metadata": {},
   "source": [
    "## Set up the PyTorch environment for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f06e9a",
   "metadata": {},
   "source": [
    "#### Customizing a DLC for training\n",
    "\n",
    "You are creating your own container, using one of the Deep Learning Containers as base image. Look for an image suitable for your use case. You can use a GPU or CPU image, you specify the kind of image you want to use, specify the the framework you intend to use, the version of the framework, region and scope.\n",
    "\n",
    "Ensure that the image you are going with as your base image, is the same image as the one you specify as the base image in the ```FROM``` instruction in your Dockerfile. Check ```docker/Dockerfile``` to make sure, and modify the first line if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=\"us-east-1\",\n",
    "    version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    image_scope=\"training\",\n",
    "    instance_type=\"ml.c5.4xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e39f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize docker/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3f2ee",
   "metadata": {},
   "source": [
    "#### If you are in a SageMaker Studio notebook environment..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de07100",
   "metadata": {},
   "source": [
    "**DO NOT** run the next three cells below. Instead, follow the instructions on [building and pushing your docker container in the studio notebook environment](#Building-your-docker-image-and-pushing-it-to-Amazon-EC2-Container-Registry-(ECR)-in-a-SageMaker-Studio-notebook-environment) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82290a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize docker/build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x docker/build_and_push.sh && docker/build_and_push.sh\n",
    "\n",
    "# check that your image exists\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf1f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "\n",
    "algorithm_name = \"pytorch-training-container-extension-yolov5-cpu\"\n",
    "\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, algorithm_name)\n",
    "\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc12fb",
   "metadata": {},
   "source": [
    "#### In the SageMaker Studio notebook environment : Building your docker image and pushing it to Amazon EC2 Container Registry (ECR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc5c87",
   "metadata": {},
   "source": [
    "1. Give your SageMaker execution role all the required permissions, go through the [pre-requisites](#Pre-requisites-if-you-are-in-a-SageMaker-Studio-environment) section of this notebook above before you proceed.\n",
    "\n",
    "2. Next, install the ```sagemaker-studio-image-build``` package using pip.\n",
    "\n",
    "3. Finally, build and register the container image using the following command:\n",
    "\n",
    "   ```sm-docker build . --file /path/to/Dockerfile```\n",
    "   \n",
    "**Uncomment and run the next three cells ONLY if you are in a SageMaker Studio notebook environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8589f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sm-docker build . --file docker/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e40758",
   "metadata": {},
   "source": [
    "If you are in a studio environment copy the **Image URI** from the output of the previous cell and use it to populate the ```image_uri``` variable in the next cell (below) and then uncomment the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5551efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected format of the image URI : <<AWS_ACCOUNT_ID>>.dkr.ecr.<<REGION>>.amazonaws.com/<<ECR_REPO_NAME>>:<<SAGEMAKER_STUDIO_USER>>\n",
    "# Assign the value you get for Image URI from the execution of the previous cell to the ```image_uri``` variable below before\n",
    "# proceeding further. Uncomment the code below before proceeding.\n",
    "\n",
    "# image_uri = <<Image URI value copy/pasted from the execution of the previous cell>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23473197",
   "metadata": {},
   "source": [
    "__At this point__ in the notebook, irrespective of the environment you are running in i.e. SageMaker Studio or on a SageMaker notebook instance, your ```image_uri``` variable should be populated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffece33",
   "metadata": {},
   "source": [
    "Since you are performing transfer learning using a custom dataset, you require weights for initialization. Pretrained weights (checkpoints) can be found on the [YOLOv5 release page](https://github.com/ultralytics/yolov5/releases). Store the downloaded weights at an S3 location. For this notebook you use the yolov5s.pt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6067a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTSLOC = prefix + \"/data/weights/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ae0b0",
   "metadata": {},
   "source": [
    "#### Get, and then upload weights to a S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e8f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt\n",
    "!zip yolov5s.pt yolov5s.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff397b6",
   "metadata": {},
   "source": [
    "##### Upload weights to S3 location for use during training later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952df3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "try:\n",
    "    response = s3_client.upload_file(\"yolov5s.pt\", bucket, WEIGHTSLOC + \"yolov5s.pt\")\n",
    "except ClientError as e:\n",
    "    logging.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de83921",
   "metadata": {},
   "source": [
    "#### Almost ready to initiate training\n",
    "\n",
    "A little primer about how to leverage hyperparameters to run training and automated model tuning for YOLOv5 on Amazon SageMaker.\n",
    "\n",
    "```freeze``` is used to freeze the weights of the backbone layers. The number of backbone layers will change based on the model you choose, in this case, we are freezing 10 layers, these layers serve as feature extractors. The head layers, which are **not** frozen compute the output predictions. For a guide to how you can decide on how many layers to freeze for transfer learning you may want to go through this thorough guide on [Transfer Learning with Frozen Layers](https://github.com/ultralytics/yolov5/issues/1314)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c40bc6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee59c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# JSON encode hyperparameters.\n",
    "def json_encode_hyperparameters(hyperparameters):\n",
    "    return {str(k): json.dumps(v) for (k, v) in hyperparameters.items()}\n",
    "\n",
    "\n",
    "# The values for hyperparameters are just examples and you are advised to change them to what suits your use case.\n",
    "hyperparameters = json_encode_hyperparameters(\n",
    "    {\n",
    "        \"epochs\": 10,\n",
    "        \"batchsize\": 8,\n",
    "        \"freeze\": 10,\n",
    "        \"patience\": 10,\n",
    "        \"lr0\": 0.01,\n",
    "        \"lrf\": 0.01,\n",
    "        \"weights\": \"s3://\" + bucket + \"/\" + prefix + \"/data/weights/\" + \"yolov5s.pt\",\n",
    "        \"classes\": \"s3://\" + bucket + \"/\" + prefix + \"/classes/classenum.json\",\n",
    "    }\n",
    ")\n",
    "\n",
    "training_uuid = uuid.uuid1()\n",
    "training_job_name = \"yolov5-project-\" + str(training_uuid)\n",
    "\n",
    "print(\"Starting training job : {}\".format(training_job_name))\n",
    "\n",
    "pt_estimator = PyTorch(\n",
    "    entry_point=\"yolov5/training-wrapper.py\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.c5.4xlarge\",\n",
    "    volume_size=100,\n",
    "    instance_count=1,\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    image_uri=image_uri,\n",
    "    debugger_hook_config=False,\n",
    "    output_path=\"s3://\" + bucket + \"/\" + prefix + \"/output\",\n",
    ")\n",
    "\n",
    "pt_estimator.fit(\n",
    "    {\n",
    "        \"train\": \"s3://\" + bucket + \"/\" + prefix + \"/data/train\",\n",
    "        \"val\": \"s3://\" + bucket + \"/\" + prefix + \"/data/val\",\n",
    "    },\n",
    "    job_name=training_job_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83df14",
   "metadata": {},
   "source": [
    "The following hyperparameters are used during training :\n",
    "\n",
    "**lr0**: Initial learning rate\n",
    "\n",
    "**lrf**: Final OneCycleLR learning rate\n",
    "\n",
    "**momentum**: SGD momentum/Adam beta1\n",
    "\n",
    "**weight_decay**: Optimizer weight decay\n",
    "\n",
    "**warmup_epochs**: Warmup epochs \n",
    "\n",
    "**warmup_momentum**: Warmup initial momentum\n",
    "\n",
    "**warmup_bias_lr**: Warmup initial bias lr\n",
    "\n",
    "**box**: Box loss gain\n",
    "\n",
    "**cls**: cls loss gain\n",
    "\n",
    "**cls_pw**: cls BCELoss positive_weight\n",
    "\n",
    "**obj**: obj loss gain \n",
    "\n",
    "**obj_pw**: obj BCELoss positive_weight\n",
    "\n",
    "**iou_t**: IoU training threshold\n",
    "\n",
    "**anchor_t**: anchor-multiple threshold\n",
    "\n",
    "**anchors**: anchors per output grid \n",
    "\n",
    "**fl_gamma**: focal loss gamma \n",
    "\n",
    "**hsv_h**: image HSV-Hue augmentation \n",
    "\n",
    "**hsv_s**: image HSV-Saturation augmentation\n",
    "\n",
    "**hsv_v**: image HSV-Value augmentation \n",
    "\n",
    "**degrees**: image rotation \n",
    "\n",
    "**translate**: image translation \n",
    "\n",
    "**scale**: image scale \n",
    "\n",
    "**shear**: image shear \n",
    "\n",
    "**perspective**: image perspective\n",
    "\n",
    "**flipud**: image flip up-down \n",
    "\n",
    "**fliplr**: image flip left-right \n",
    "\n",
    "**mosaic**: image mosaic \n",
    "\n",
    "**mixup**: image mixup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bbf26",
   "metadata": {},
   "source": [
    "**How did the training job go?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15195c6f",
   "metadata": {},
   "source": [
    "Extract the ```results.png``` from the ```model.tar.gz``` file. You download the ```model.tar.gz``` from the S3 location you specified in the output path when you create the estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    bucket, prefix + \"/output/\" + str(training_job_name) + \"/output/model.tar.gz\", \"model.tar.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eed06e",
   "metadata": {},
   "source": [
    "#### Contents of the ```model.tar.gz```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b19a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586316ca",
   "metadata": {},
   "source": [
    "Go through the visualizations of metrics and losses from the ```results.png```. Other relevant information is also available can also be examined like the Precision-Recall curve, weights, F1 curve and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650eb13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 12), dpi=120)\n",
    "image = plt.imread(\"./exp/results.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c851c",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Amazon SageMaker Automated Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572f409",
   "metadata": {},
   "source": [
    "Here, you use a few metrics as a way for SageMaker to find the best model, you can consider as many as 20 metrics! For guidance on how to set up the objective metrics, please refer to this documentation on [Defining Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html) for Automated Model Tuning (AMT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d2539",
   "metadata": {},
   "source": [
    "The YOLOv5 uses hyperparameter evolution to optimize hyperparameters. A detailed guide is provided on how this can be [achieved here](https://docs.ultralytics.com/tutorials/hyperparameter-evolution/). In this notebook, you address HPO using Amazon SageMaker AMT. The hyperparameters used by YOLOv5 are sourced from ```hyp.scratch.yaml``` file. Below, is a list of hyperparameters that can be tuned using evolution during training, along with their names and default values:\n",
    "\n",
    "```\n",
    "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "lrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "momentum: 0.937  # SGD momentum/Adam beta1\n",
    "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
    "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
    "warmup_momentum: 0.8  # warmup initial momentum\n",
    "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
    "box: 0.05  # box loss gain\n",
    "cls: 0.5  # cls loss gain\n",
    "cls_pw: 1.0  # cls BCELoss positive_weight\n",
    "obj: 1.0  # obj loss gain (scale with pixels)\n",
    "obj_pw: 1.0  # obj BCELoss positive_weight\n",
    "iou_t: 0.20  # IoU training threshold\n",
    "anchor_t: 4.0  # anchor-multiple threshold\n",
    "anchors: 0  # anchors per output grid (0 to ignore)\n",
    "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
    "degrees: 0.0  # image rotation (+/- deg)\n",
    "translate: 0.1  # image translation (+/- fraction)\n",
    "scale: 0.5  # image scale (+/- gain)\n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.0  # image flip up-down (probability)\n",
    "fliplr: 0.5  # image flip left-right (probability)\n",
    "mosaic: 1.0  # image mosaic (probability)\n",
    "mixup: 0.0  # image mixup (probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448b030a",
   "metadata": {},
   "source": [
    "Fitness is the criterion used in YOLOv5 to arrive at the best model, and the value that is maximized. The challenge with using evolution is that it requires at least 300 generations (recommended), this can get pretty time-consuming and expensive as it will need 100s or 1000s of GPU hours. Nevertheless, if you do want to use evolution, all you have to do is add --evolve when ```train.py``` is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef99658",
   "metadata": {},
   "source": [
    "These are the hyper parameters you can tune using Amazon SageMaker AMT:\n",
    "\n",
    "```\n",
    "lr0: continuous\n",
    "lrf: continuous\n",
    "momentum: continuous\n",
    "weight_decay: continuous\n",
    "warmup_epochs: integer\n",
    "warmup_momentum: continuous\n",
    "warmup_bias_lr: continuous\n",
    "box: continuous\n",
    "cls: continuous\n",
    "cls_pw: integer\n",
    "obj: integer\n",
    "obj_pw: integer\n",
    "iou_t: continuous\n",
    "anchor_t: integer\n",
    "anchors: integer\n",
    "fl_gamma: continuous\n",
    "hsv_h: continuous\n",
    "hsv_s: continuous\n",
    "hsv_v: continuous\n",
    "degrees: integer\n",
    "translate: continuous\n",
    "scale: continuous\n",
    "shear: integer\n",
    "perspective: continuous\n",
    "flipud: continuous\n",
    "fliplr: continuous\n",
    "mosaic: continuous\n",
    "mixup: continuous\n",
    "\n",
    "```\n",
    "\n",
    "Identify the metric you will use to evaluate. In this case you will use the objective metric of mAP@.5. You will be maximizing this metric. This is a very common metric used in object detection. You can learn more about mAP and other advanced metrics in this Stanford CS230 page on [Advanced Evaluation Metrics](https://cs230.stanford.edu/section/8/). The code below demonstrates how you can perform AMT in SageMaker for tuning the YOLOv5 model for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fabdf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameters = json_encode_hyperparameters(\n",
    "    {\n",
    "        \"freeze\": 10,\n",
    "        \"patience\": 10,\n",
    "        \"lr0\": 0.01,\n",
    "        \"lrf\": 0.01,\n",
    "        \"weights\": \"s3://\" + bucket + \"/\" + prefix + \"/data/weights/\" + \"yolov5s.pt\",\n",
    "        \"classes\": \"s3://\" + bucket + \"/\" + prefix + \"/classes/classenum.json\",\n",
    "    }\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"epochs\": IntegerParameter(10, 20),\n",
    "    \"batchsize\": IntegerParameter(8, 64),\n",
    "    \"iou_t\": ContinuousParameter(0.15, 0.25),\n",
    "}\n",
    "\n",
    "objective_metric_name = \"mAP.5\"\n",
    "objective_type = \"Maximize\"\n",
    "metric_definitions = [{\"Name\": \"mAP.5\", \"Regex\": \"(0\\.[0-9]{1,6}).{7,12}$\"}]\n",
    "\n",
    "tuning_uuid = uuid.uuid1()\n",
    "tuning_job_name = \"yolov5-project-hpo-\" + str(training_uuid)\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    pt_estimator,\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    objective_type=objective_type,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_jobs=2,  # Change this to a suitable number that makes sense in your case\n",
    "    max_parallel_jobs=1,  # Change this to a suitable number that makes sense in your case\n",
    "    base_tuning_job_name=tuning_job_name,\n",
    ")\n",
    "\n",
    "tuner.fit(\n",
    "    {\n",
    "        \"train\": \"s3://\" + bucket + \"/\" + prefix + \"/data/train\",\n",
    "        \"val\": \"s3://\" + bucket + \"/\" + prefix + \"/data/val\",\n",
    "    },\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dee1d4",
   "metadata": {},
   "source": [
    "Now that the tuning job is complete, find the best model from the training job that gave us the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640169cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c166a1",
   "metadata": {},
   "source": [
    "Download the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c43431",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    bucket,\n",
    "    prefix + \"/output/\" + tuner.best_training_job() + \"/output/model.tar.gz\",\n",
    "    \"hpo.model.tar.gz\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800a078",
   "metadata": {},
   "source": [
    "## Conclusion : Deploying YOLOv5 on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76b7fb",
   "metadata": {},
   "source": [
    "Whether you run a training job, or perform automated model tuning (AMT), you need to create a ```model.tar.gz``` file that has to follow a specific structure. In this structure you will need to have the model, in this case the ```best.pt``` file. This is available in the ```weights``` directory in your ```model.tar.gz``` file. For a deeper understanding of how to create the model directory structure refer to [Use PyTorch with SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f808e59",
   "metadata": {},
   "source": [
    "Further guidance on deploying a YOLOv5 model is available in this AWS Machine Learning blog post about scaling [YOLOv5 inference with Amazon SageMaker endpoints and AWS Lambda](https://aws.amazon.com/blogs/machine-learning/scale-yolov5-inference-with-amazon-sagemaker-endpoints-and-aws-lambda/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/advanced_functionality|pytorch_yolov5_training_and_hpo|transfer_learning_and_hpo_yolov5.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}