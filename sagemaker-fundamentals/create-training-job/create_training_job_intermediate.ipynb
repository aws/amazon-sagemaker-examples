{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Job (Hyperparamter Injection) \n",
    "\n",
    "In this notebook, we discuss more complicated set ups for `CreateTrainingJob` API. It assumes you are confortable with the set ups discussed in [the notebook on basics of `CreateTrainingJob`](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create_training_job.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Hyperparameter Injection?\n",
    "\n",
    "With hyperparamter injection, you don't need to hard code hyperparameters of your ML training in the training image, instead you can pass your hyperparamters through `CreateTrainingJob` API and SageMaker will makes them available to your training container. This way you can experiment a list of hyperparameters for your training job without rebuilding the image for each experiment. More importantly, this is the mechanism used by `CreateHyperParameterTuningJob` API to (you guessed right) create many training jobs to search for the best hyperparameters. We will discuss `CreateHyperParameterTuningJob` in a different notebook. \n",
    "\n",
    "If you remember from [the notebook on basics of `CreateTrainingJob`](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create_training_job.ipynb), SageMaker reserves `/opt/ml` directory \"to talk to your container\", i.e. provide training information to your training job and retrieve output from it. \n",
    "\n",
    "You will pass hyperparamters of your training job as a dictionary to the `create_training_job` of boto3 SageMaker client, and it will become availble in `/opt/ml/input/config/hyperparameters.json`. See [reference in the official docs](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set ups\n",
    "\n",
    "You will build a training image and push it to ECR like in [the notebook on basics of `CreateTrainingJob`](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create_training_job.ipynb). The only difference is, the python script for runing the training will print out the hyperparamters in `/opt/ml/input/config/hyperparameters.json` to confirm that container does have access to the hyperparamters you passed to `CreateTrainingJob` API. \n",
    "\n",
    "This training job does not require any data. Therefore, you don't need to confgure `InputDataConfig` parameter for `CreateTrainingJob`. However, SageMaker always needs an S3 URI to save your model artifact, i.e. you still need to configure `OutputDataConfig` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 # your gateway to AWS APIs\n",
    "import datetime\n",
    "import pprint\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "iam = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "def current_time():\n",
    "    ct = datetime.datetime.now() \n",
    "    return str(ct.now()).replace(\":\", \"-\").replace(\" \", \"-\")[:19]\n",
    "\n",
    "def account_id():\n",
    "    return boto3.client('sts').get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a service role for SageMaker\n",
    "\n",
    "Review [notebook on execution role](https://github.com/hsl89/amazon-sagemaker-examples/blob/execution-role/sagemaker-fundamentals/execution-role/execution-role.ipynb) for step-by-step instructions on how to create an IAM Role.\n",
    "\n",
    "The service role is intended to be assumed by the SageMaker service to procure resources in your AWS account on your behalf. \n",
    "\n",
    "1. If you are running this this notebook on SageMaker infrastructure like Notebook Instances or Studio, then we will use the role you used to spin up those resources\n",
    "\n",
    "2. If you are running this notebook on an EC2 instance, then we will create a service role attach `AmazonSageMakerFullAccess` to it. If you already have a SageMaker service role, you can paste its `role_arn` here. \n",
    "\n",
    "First, let's get some helper functions for creating execution role. We discussed those functions in the [notebook on execution role](https://github.com/hsl89/amazon-sagemaker-examples/blob/execution-role/sagemaker-fundamentals/execution-role/execution-role.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-03-23 23:35:37--  https://raw.githubusercontent.com/hsl89/amazon-sagemaker-examples/sagemaker-fundamentals/sagemaker-fundamentals/execution-role/iam_helpers.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3659 (3.6K) [text/plain]\n",
      "Saving to: ‘iam_helpers.py’\n",
      "\n",
      "     0K ...                                                   100% 63.4M=0s\n",
      "\n",
      "2021-03-23 23:35:37 (63.4 MB/s) - ‘iam_helpers.py’ saved [3659/3659]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "file=$(ls . | grep iam_helpers.py)\n",
    "\n",
    "if [ -f \"$file\" ]\n",
    "then\n",
    "    rm $file\n",
    "fi\n",
    "\n",
    "wget https://raw.githubusercontent.com/hsl89/amazon-sagemaker-examples/sagemaker-fundamentals/sagemaker-fundamentals/execution-role/iam_helpers.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up service role for SageMaker\n",
    "from iam_helpers import create_execution_role\n",
    "\n",
    "sts = boto3.client('sts')\n",
    "caller = sts.get_caller_identity()\n",
    "\n",
    "if ':user/' in caller['Arn']: # as IAM user\n",
    "    # either paste in a role_arn with or create a new one and attach \n",
    "    # AmazonSageMakerFullAccess\n",
    "    role_name = 'sm'\n",
    "    role_arn = create_execution_role(role_name=role_name)['Role']['Arn']\n",
    "    \n",
    "    # attach the permission to the role\n",
    "    # skip it if you want to use a SageMaker service that \n",
    "    # already has AmazonFullSageMakerFullAccess\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess'\n",
    "    )\n",
    "elif 'assumed-role' in caller['Arn']: # on SageMaker infra\n",
    "    assumed_role = caller['Arn']\n",
    "    role_arn = re.sub(r\"^(.+)sts::(\\d+):assumed-role/(.+?)/.*$\", r\"\\1iam::\\2:role/\\3\", assumed_role)\n",
    "else:\n",
    "    print(\"I assume you are on an EC2 instance launched with an IAM role\")\n",
    "    role_arn = caller['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a training image and push to ECR\n",
    "\n",
    "You will build a training image here like in [the notebook on basics of `CreateTrainingJob`](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create_training_job.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the Dockerfile\n",
    "!cat container_intermediate/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%WriteFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# build the image\n",
    "cd container_intermediate/\n",
    "\n",
    "# tag it as example-image:latest\n",
    "docker build -t example-image:latest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a repo in ECR called example-image\n",
    "\n",
    "ecr = boto3.client('ecr')\n",
    "\n",
    "try:\n",
    "    # The repository might already exist\n",
    "    # in your ECR\n",
    "    cr_res = ecr.create_repository(\n",
    "        repositoryName='example-image')\n",
    "    pp.pprint(cr_res)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "account=$(aws sts get-caller-identity --query Account | sed -e 's/^\"//' -e 's/\"$//')\n",
    "region=$(aws configure get region)\n",
    "ecr_account=${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Give docker your ECR login password\n",
    "aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $ecr_account\n",
    "\n",
    "# Fullname of the repo\n",
    "fullname=$ecr_account/example-image:latest\n",
    "\n",
    "#echo $fullname\n",
    "# Tag the image with the fullname\n",
    "docker tag example-image:latest $fullname\n",
    "\n",
    "# Push to ECR\n",
    "docker push $fullname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the ECR repository\n",
    "repo_res = ecr.describe_images(\n",
    "    repositoryName='example-image')\n",
    "pp.pprint(repo_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data\n",
    "\n",
    "SageMaker provides training data to your image through an S3 bucket that your service role has read access to. This means before triggering a training job, you need to make your training available in such an S3 bucket.\n",
    "\n",
    "In this notebook, we will use preloaded data on a public bucket `sagemaker-sample-files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the bucket\n",
    "public_bucket = \"sagemaker-sample-files\"\n",
    "s3 = boto3.client('s3')\n",
    "obj_res = s3.list_objects_v2(\n",
    "    Bucket=\"sagemaker-sample-files\")\n",
    "\n",
    "# print out object keys compactly\n",
    "for obj in obj_res['Contents']:\n",
    "    if '/tabular/fraud_detection/synthethic_fraud_detection_SA' in obj['Key']:\n",
    "        print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend the data under `datasets/tabular/synthetic_fraud_detection_SA` is the data for your ML project.\n",
    "\n",
    "The public bucket `sagemaker-sample-files` is located in us-east-1. We first need to copy the data to a bucket of yours that share the same region with the SageMaker instance you will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bucket\n",
    "def create_tmp_bucket():\n",
    "    \"\"\"Create an S3 bucket that is intended to be used for short term\"\"\"\n",
    "    bucket = f\"sagemaker-{current_time()}\" # accessible by SageMaker\n",
    "    region = boto3.Session().region_name\n",
    "    boto3.client('s3').create_bucket(\n",
    "        Bucket=bucket,\n",
    "        CreateBucketConfiguration={\n",
    "            'LocationConstraint': region\n",
    "        })\n",
    "    return bucket\n",
    "\n",
    "bucket = create_tmp_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bucket is created by you. By default, all objects in the bucket are private and are accessible by the owner of the bucket. To make the bucket accessible by SageMaker service, normally you would need to explicitly add the permission to access this bucket to the SageMaker service role. But we do not have to it here, because the bucket is prefixed by \"sagemaker\" and in the policy `AmazonSageMakerFullAccess`, we allow the service role to acccess all S3 prefixed by \"sagemaker\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prefix = 'input_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from sagemaker-samplef-files to {bucket}\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# copy remote csv files to local\n",
    "files = []\n",
    "data_dir = '/tmp'\n",
    "for obj in obj_res['Contents']:\n",
    "    if '/tabular/fraud_detection/synthethic_fraud_detection_SA' in obj['Key']:\n",
    "        key = obj['Key']\n",
    "        if key.endswith('.csv'):\n",
    "            filename=key.split('/')[-1]\n",
    "            files.append(filename)\n",
    "            with open(os.path.join(data_dir, filename), 'wb') as f:\n",
    "                s3.download_fileobj(public_bucket, key, f)\n",
    "\n",
    "# upload from local to the bucket you just created\n",
    "for fname in files:\n",
    "    with open(os.path.join(data_dir, fname), 'rb') as f:\n",
    "        key = input_prefix + fname\n",
    "        s3.upload_fileobj(f, bucket, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect your bucket\n",
    "obj_res = s3.list_objects_v2(\n",
    "    Bucket=bucket)\n",
    "\n",
    "for obj in obj_res['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put everything together\n",
    "\n",
    "Now you have everything you need to create a training job. Let's review what you have done. You have \n",
    "* created an execution role for SageMaker service\n",
    "* built and tested a docker image that includes the runtime and logic of your model training\n",
    "* made the image accessible to SageMaker by hosting it on ECR\n",
    "* made the training data available to SageMaker by hosting it on S3\n",
    "* pointed SageMaker to an S3 bucket to write output \n",
    "\n",
    "Let pull the trigger and create a training job. We will invoke `CreateTrainingJob` API via boto3. You are strongly encouraged to read through the [description of the API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job) before moving on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "\n",
    "# name training job\n",
    "training_job_name = 'example-training-job-{}'.format(current_time())\n",
    "\n",
    "# input data prefix\n",
    "data_path = \"s3://\" + bucket + '/' + input_prefix\n",
    "\n",
    "# location that SageMaker saves the model artifacts\n",
    "output_prefix = 'example/output/'\n",
    "output_path = \"s3://\" + bucket + '/' + output_prefix\n",
    "\n",
    "# ECR URI of your image\n",
    "region = boto3.Session().region_name\n",
    "account = account_id()\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/example-image:latest\".format(account, region)\n",
    "\n",
    "algorithm_specification = {\n",
    "    'TrainingImage': image_uri,\n",
    "    'TrainingInputMode': 'File',\n",
    "}\n",
    "\n",
    "\n",
    "input_data_config = [\n",
    "    {\n",
    "        'ChannelName': 'train',\n",
    "            'DataSource':{\n",
    "                'S3DataSource':{\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': data_path,\n",
    "                    'S3DataDistributionType': 'FullyReplicated',\n",
    "                }\n",
    "        }\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'ChannelName': 'test',\n",
    "        'DataSource':{\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': data_path,\n",
    "                'S3DataDistributionType': 'FullyReplicated',\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "output_data_config = {\n",
    "    'S3OutputPath': output_path\n",
    "}\n",
    "\n",
    "resource_config = {\n",
    "    'InstanceType': 'ml.m5.large',\n",
    "    'InstanceCount':1,\n",
    "    'VolumeSizeInGB':10\n",
    "}\n",
    "\n",
    "stopping_condition={\n",
    "    'MaxRuntimeInSeconds':120,\n",
    "}\n",
    "\n",
    "enable_network_isolation=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_res = sm_boto3.create_training_job(\n",
    "    TrainingJobName=training_job_name,\n",
    "    AlgorithmSpecification=algorithm_specification,\n",
    "    RoleArn=role_arn,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    ResourceConfig=resource_config,\n",
    "    StoppingCondition=stopping_condition,\n",
    "    EnableNetworkIsolation=enable_network_isolation,\n",
    "    EnableManagedSpotTraining=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the status of the training job\n",
    "tj_state = sm_boto3.describe_training_job(\n",
    "    TrainingJobName=training_job_name)\n",
    "pp.pprint(tj_state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check training job status every 30 seconds\n",
    "stopped = False\n",
    "while not stopped:\n",
    "    tj_state = sm_boto3.describe_training_job(\n",
    "        TrainingJobName=training_job_name)\n",
    "    if tj_state['TrainingJobStatus'] in ['Completed', 'Stopped', 'Failed']:\n",
    "        stopped=True\n",
    "    else:\n",
    "        print(\"Training in progress\")\n",
    "        time.sleep(30)\n",
    "\n",
    "if tj_state['TrainingJobStatus'] == 'Failed':\n",
    "    print(\"Training job failed \")\n",
    "    print(\"Failed Reason: {}\".tj_state['FailedReason'])\n",
    "else:\n",
    "    print(\"Training job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the trained model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"== Output config:\")\n",
    "print(tj_state['OutputDataConfig'])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"== Model artifact:\")\n",
    "pp.pprint(s3.list_objects_v2(Bucket=bucket, Prefix=output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = boto3.client('logs')\n",
    "\n",
    "log_res= logs.describe_log_streams(\n",
    "    logGroupName='/aws/sagemaker/TrainingJobs',\n",
    "    logStreamNamePrefix=training_job_name)\n",
    "\n",
    "for log_stream in log_res['logStreams']:\n",
    "    # get one log event\n",
    "    log_event = logs.get_log_events(\n",
    "        logGroupName='/aws/sagemaker/TrainingJobs',\n",
    "        logStreamName=log_stream['logStreamName'])\n",
    "    \n",
    "    # print out messages from the log event\n",
    "    for ev in log_event['events']:\n",
    "        for k, v in ev.items():\n",
    "            if k == 'message':\n",
    "                print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You now understand the basics of a training job on SageMaker. It's funny to think that after this long notebook, you get a trained model artifact, which is a pickled None instance. But keep in mind that you can follow the exact same process to train a state-of-art model with billions of parameters and the compute cost is proportional to how long you train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the ECR repo\n",
    "del_repo_res = ecr.delete_repository(\n",
    "    repositoryName='example-image',\n",
    "    force=True)\n",
    "\n",
    "pp.pprint(del_repo_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the S3 bucket\n",
    "def delete_bucket_force(bucket_name):\n",
    "    objs = s3.list_objects_v2(Bucket=bucket_name)['Contents']\n",
    "    for obj in objs:\n",
    "        s3.delete_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=obj['Key'])\n",
    "    \n",
    "    return s3.delete_bucket(Bucket=bucket_name)\n",
    "\n",
    "del_buc_res = delete_bucket_force(bucket)\n",
    "\n",
    "pp.pprint(del_buc_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
