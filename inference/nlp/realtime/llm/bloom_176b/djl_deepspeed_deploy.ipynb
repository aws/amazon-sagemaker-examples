{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e17f8d",
   "metadata": {},
   "source": [
    "\n",
    "### Serve large models on SageMaker with DeepSpeed Container. In this notebook we show Bloom-176B model hosting\n",
    "\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the latest container launched using DeepSpeed and DJL. DJL provides for the serving framework while DeepSpeed is the key sharding library we leverage to enable hosting of large models. We use DJLServing as the model serving solution in this example. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent blog post (https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/).\n",
    "\n",
    "Language models have recently exploded in both size and popularity. In 2018, BERT-large entered the scene and, with its 340M parameters and novel transformer architecture, set the standard on NLP task accuracy. Within just a few years, state-of-the-art NLP model size has grown by more than 500x with models such as OpenAI’s 175 billion parameter GPT-3 and similarly sized open source Bloom 176B raising the bar on NLP accuracy. This increase in the number of parameters is driven by the simple and empirically-demonstrated positive relationship between model size and accuracy: more is better. With easy access from models zoos such as Hugging Face and improved accuracy in NLP tasks such as classification and text generation, practitioners are increasingly reaching for these large models. However, deploying them can be a challenge because of their size.\n",
    "\n",
    "Model parallelism can help deploy large models that would normally be too large for a single GPU. With model parallelism, we partition and distribute a model across multiple GPUs. Each GPU holds a different part of the model, resolving the memory capacity issue for the largest deep learning models with billions of parameters. This notebook uses tensor parallelism techniques which allow GPUs to work simultaneously on the same layer of a model and achieve low latency inference relative to a pipeline parallel solution.\n",
    "\n",
    "SageMaker has rolled out DeepSpeed container which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting.\n",
    "\n",
    "In this notebook, we deploy the open source Bloom 176B quantized model across GPU's on a ml.p4d.24xlarge instance. DeepSpeed is used for tensor parallelism inference while DJLServing handles inference requests and the distributed workers. For further reading on DeepSpeed you can refer to https://arxiv.org/pdf/2207.00032.pdf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4bc5d",
   "metadata": {},
   "source": [
    "## Licence agreement\n",
    "View license information https://huggingface.co/spaces/bigscience/license for this model including the use-based restrictions in Section 5 before using the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instal boto3 library to create model and run inference workloads\n",
    "%pip install -Uqq boto3 awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d550095",
   "metadata": {},
   "source": [
    "## Optional Section to Download Model from Hugging Face Hub\n",
    "\n",
    "Use this section of you are interested in downloading the model directly from Huggingface hub and storing in your own S3 bucket. Please change the variable \"install_model_locally\" to True in that case.\n",
    "\n",
    "**However this notebook currently leverages the model stored in AWS public S3 location for ease of use. So you can skip this step**\n",
    "\n",
    "The below step to download and then upload to S3 can take several minutes since the model size is extremely large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169deea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "install_model_locally = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f07d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if install_model_locally:\n",
    "    %pip install huggingface-hub -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c480bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if install_model_locally:\n",
    "\n",
    "    from huggingface_hub import snapshot_download\n",
    "    from pathlib import Path\n",
    "\n",
    "    # - This will download the model into the ./model directory where ever the jupyter file is running\n",
    "    local_model_path = Path(\"./model\")\n",
    "    local_model_path.mkdir(exist_ok=True)\n",
    "    model_name = \"microsoft/bloom-deepspeed-inference-int8\"\n",
    "    commit_hash = \"aa00a6626f6484a2eef68e06d1e089e4e32aa571\"\n",
    "\n",
    "    # - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "    snapshot_download(repo_id=model_name, revision=commit_hash, cache_dir=local_model_path)\n",
    "\n",
    "    # - Upload to S3 using AWS CLI\n",
    "    s3_model_prefix = \"hf-large-model-djl-ds/model\"  # folder where model checkpoint will go\n",
    "    model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]\n",
    "\n",
    "    !aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d1d36",
   "metadata": {},
   "source": [
    "## Create SageMaker compatible Model artifact and Upload Model to S3\n",
    "\n",
    "SageMaker needs the model to be in a Tarball format. In this notebook we are going to create the model with the Inference code to shorten the end point creation time. In the Inference code we kick of a multi threaded approach to download the model weights into the container using awscli\n",
    "\n",
    "The tarball is in the following format\n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── model.py\n",
    "│   └── requirements.txt\n",
    "│   └── serving.properties\n",
    "\n",
    "```\n",
    "\n",
    "The actual model is stored in S3 location and will be downloaded into the container directly when the endpoint is created. For that we will pass in two environment variables\n",
    "\n",
    "1.  \"MODEL_S3_BUCKET\" Specify the S3 Bucket where the model artifact is\n",
    "2.  \"MODEL_S3_PREFIX\" Specify the S3 prefix for where the model artifacts file are actually located\n",
    "\n",
    "This will be used in the model.py file to read in the actual model artifacts. \n",
    "\n",
    "- `model.py` is the key file which will handle any requests for serving. It is also responsible for loading the model from S3\n",
    "- `requirements.txt` has the awscli library needed to be installed when the container starts up.\n",
    "- `serving.properties` is the script that will have environment variables which can be used to customize model.py at run time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582652f2",
   "metadata": {},
   "source": [
    "#### Create required variables and initialize them to create the endpoint, we leverage boto3 for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = \"sagemaker-sample-files\"\n",
    "s3_code_prefix = \"hf-large-model-djl-ds/code\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = \"models/bloom-176B/raw_model_microsoft/\"  # \"bloom-176B/raw_model_microsoft/\"  # folder where model checkpoint will go\n",
    "# --  s3://sagemaker-sample-files/models/bloom-176B/raw_model_microsoft/ -\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3440e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e90f96",
   "metadata": {},
   "source": [
    "**Image URI of the DJL Container to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb34cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/djl-ds:latest\"\n",
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.19.0-deepspeed0.7.3-cu113\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f6e79c",
   "metadata": {},
   "source": [
    "**Create the Tarball and then upload to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9005e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p code_bloom176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code_bloom176/model.py\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "\n",
    "def check_config():\n",
    "    local_rank = os.getenv(\"LOCAL_RANK\")\n",
    "    curr_pid = os.getpid()\n",
    "    print(\n",
    "        f\"__Number CUDA Devices:{torch.cuda.device_count()}:::local_rank={local_rank}::curr_pid={curr_pid}::\"\n",
    "    )\n",
    "\n",
    "    if not local_rank:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    if not check_config():\n",
    "        raise Exception(\n",
    "            \"DJL:DeepSpeed configurations are not default. This code does not support non default configurations\"\n",
    "        )\n",
    "\n",
    "    deepspeed.init_distributed(\"nccl\")\n",
    "\n",
    "    tensor_parallel = int(os.getenv(\"TENSOR_PARALLEL_DEGREE\", \"1\"))\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "    model_dir = \"/tmp/model\"\n",
    "    bucket = os.environ.get(\"MODEL_S3_BUCKET\")\n",
    "    key_prefix = os.environ.get(\"MODEL_S3_PREFIX\")\n",
    "    curr_pid = os.getpid()\n",
    "    print(f\"tensor_parallel={tensor_parallel}::curr_pid={curr_pid}::\")\n",
    "    print(\n",
    "        f\"Current Rank: {local_rank}:: pid={curr_pid}::Going to load the model weights on rank 0: bucket={bucket}::key={key_prefix}::\"\n",
    "    )\n",
    "\n",
    "    if local_rank == 0:\n",
    "\n",
    "        if f\"{model_dir}/DONE\" not in glob(f\"{model_dir}/*\"):\n",
    "            print(\"Starting Model downloading files pid={curr_pid}::\")\n",
    "            print(f\"Starting Model pid={curr_pid}::\")\n",
    "\n",
    "            try:\n",
    "                # --\n",
    "                proc_run = subprocess.run(\n",
    "                    [\"aws\", \"s3\", \"cp\", \"--recursive\", f\"s3://{bucket}/{key_prefix}\", model_dir],\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                )  # python 7 onwards\n",
    "                print(f\"Model download finished: pid={curr_pid}::\")\n",
    "\n",
    "                # write file when download complete. Could use dist.barrier() but this makes it easier to check if model is downloaded in case of retry\n",
    "                with open(f\"{model_dir}/DONE\", \"w\") as f:\n",
    "                    f.write(\"download_complete\")\n",
    "\n",
    "                print(\n",
    "                    f\"Model download checkmark written out pid={curr_pid}::return_code:{proc_run.returncode}:stderr:-- >:{proc_run.stderr}\"\n",
    "                )\n",
    "                proc_run.check_returncode()  # to throw the error in case there was one\n",
    "\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(\n",
    "                    \"Model download failed: Error:\\nreturn code: \",\n",
    "                    e.returncode,\n",
    "                    \"\\nOutput: \",\n",
    "                    e.stderr,\n",
    "                )\n",
    "                raise  # FAIL FAST\n",
    "\n",
    "    dist.barrier()  # - to ensure all processes load fine\n",
    "\n",
    "    print(f\"Load the Model  pid={curr_pid}::\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    # has to be FP16 as Int8 model loading not yet supported\n",
    "    with deepspeed.OnDevice(dtype=torch.float16, device=\"meta\"):\n",
    "        model = AutoModelForCausalLM.from_config(\n",
    "            AutoConfig.from_pretrained(model_dir), torch_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    model = deepspeed.init_inference(\n",
    "        model,\n",
    "        mp_size=tensor_parallel,\n",
    "        dtype=torch.int8,\n",
    "        base_dir=model_dir,\n",
    "        checkpoint=os.path.join(model_dir, \"ds_inference_config.json\"),\n",
    "        replace_method=\"auto\",\n",
    "        replace_with_kernel_inject=True,\n",
    "    )\n",
    "\n",
    "    model = model.module\n",
    "    dist.barrier()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    print(\"Model In handle\")\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = get_model()\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        print(\"Model warm up: inputs were empty:called by Model server to warmup\")\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "\n",
    "    inputs = inputs.get_as_json()\n",
    "\n",
    "    # print(inputs)\n",
    "    data = inputs[\"input\"]\n",
    "    generate_kwargs = inputs.get(\"gen_kwargs\", {})\n",
    "    padding = inputs.get(\"padding\", False)\n",
    "\n",
    "    input_tokens = tokenizer(data, return_tensors=\"pt\", padding=padding)\n",
    "    print(input_tokens)\n",
    "    for t in input_tokens:\n",
    "        if torch.is_tensor(input_tokens[t]):\n",
    "            input_tokens[t] = input_tokens[t].to(torch.cuda.current_device())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**input_tokens, **generate_kwargs)\n",
    "\n",
    "    print(output)\n",
    "\n",
    "    output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    return Output().add_as_json(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aafe8a2",
   "metadata": {},
   "source": [
    "#### Serving.properties has engine parameter which tells the DJL model server to use the DeepSpeed engine to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code_bloom176/serving.properties\n",
    "engine = DeepSpeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01110027",
   "metadata": {},
   "source": [
    "#### Requirements will tell the container to load these additional libraries into the container. We need these to download the model into the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code_bloom176/requirements.txt\n",
    "boto3\n",
    "awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a75031",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz code_bloom176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e224bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea572fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"S3 Model Prefix where the model files are -- > {s3_model_prefix}\")\n",
    "print(f\"S3 Model Bucket is -- > {model_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09bca2d",
   "metadata": {},
   "source": [
    "### This is optional in case you want to use VpcConfig to specify when creating the end points\n",
    "\n",
    "For more details you can refer to this link https://docs.aws.amazon.com/sagemaker/latest/dg/host-vpc.html\n",
    "\n",
    "The below is just an example to extract information about Security Groups and Subnets needed to configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 describe-security-groups --filter Name=vpc-id,Values=<use vpcId> | python3 -c \"import sys, json; print(json.load(sys.stdin)['SecurityGroups'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - provide networking configs if needed.\n",
    "security_group_ids = []  # add the security group id's\n",
    "subnets = []  # add the subnet id for this vpc\n",
    "privateVpcConfig = {\"SecurityGroupIds\": security_group_ids, \"Subnets\": subnets}\n",
    "print(privateVpcConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce9918",
   "metadata": {},
   "source": [
    "### To create the end point the steps are:\n",
    "\n",
    "1. Create the Model using the Image container and the Model Tarball uploaded earlier\n",
    "2. Create the endpoint config using the following key parameters\n",
    "\n",
    "    a) Instance Type is ml.p4d.24xlarge \n",
    "    \n",
    "    b) ModelDataDownloadTimeoutInSeconds is 2400 which is needed to ensure the Model downloads from S3 successfully,\n",
    "    \n",
    "    c) ContainerStartupHealthCheckTimeoutInSeconds is 2400 to ensure health check starts after the model is ready\n",
    "    \n",
    "3. Create the end point using the endpoint config created    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209dcc05",
   "metadata": {},
   "source": [
    "One of the key parameters here is **TENSOR_PARALLEL_DEGREE** which essentially tells the DeepSpeed library to partition the models along 8 GPU's. This is a tunable and configurable parameter. For the purpose of this notebook we would like to leave these as **default settings**.\n",
    "\n",
    "This parameter also controls the no of workers per model which will be started up when DJL serving runs. As an example if we have a 8 GPU machine and we are creating 8 partitions then we will have 1 worker per model to serve the requests. For further reading on DeepSpeedyou can follow the link https://www.deepspeed.ai/tutorials/inference-tutorial/#initializing-for-inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b01cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(f\"bloom-djl-ds\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact,\n",
    "        \"Environment\": {\n",
    "            \"MODEL_S3_BUCKET\": model_bucket,\n",
    "            \"MODEL_S3_PREFIX\": s3_model_prefix,\n",
    "            \"TENSOR_PARALLEL_DEGREE\": \"8\",\n",
    "        },\n",
    "    },\n",
    "    # Uncomment if providing networking configs\n",
    "    # VpcConfig=privateVpcConfig\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4e468",
   "metadata": {},
   "source": [
    "VolumnSizeInGB has been left as commented out. You should use this value for Instance types which support EBS volume mounts. The current instance we are using comes with a pre configured space and does not support additional volume mounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.p4d.24xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"VolumeSizeInGB\" : 400,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 2400,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9266f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d7e88",
   "metadata": {},
   "source": [
    "#### Wait for the end point to be created.This can be take couple of minutes or longer. Please be patient\n",
    "However while that happens, let us look at the critical areas of the helper files we are using to load the model\n",
    "1. We will look at the code snippets for model.py to see the model downloading mechanism\n",
    "2. Requirements.txt to see the required libraries to be loaded\n",
    "3. Serving.properties to see the environment related properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e36053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code snippet which is responsible to load the model from S3\n",
    "! sed -n '40,60p' code_bloom176/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9752b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code snippet which loads the libraries into the container needed for run\n",
    "! sed -n '1,3p' code_bloom176/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943eaa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code snippet which shows the environment variables being used to customize runtime\n",
    "! sed -n '1,3p' code_bloom176/serving.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa549a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5664ba8",
   "metadata": {},
   "source": [
    "#### Leverage the Boto3 to invoke the endpoint. \n",
    "\n",
    "This is a generative model so we pass in a Text (specified in the 'input' field in the json ) as a prompt and Model will complete the sentence and return the results. More details on these parameters can be found at https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task. Some quick explainations are below\n",
    "1. temperature -- > The temperature of the sampling operation. 1 means regular sampling, 0 means always take the highest score and 100 means uniform probability\n",
    "2. max_new_tokens -- > The amount of new tokens or text to be gnerated. More tokens will increase the prediction time\n",
    "3. num_beams -- > Beam Search keeps track of the n-th most likely word sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466904b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"input\": \"Amazon.com is the best \",\n",
    "            \"gen_kwargs\": {\n",
    "                \"min_length\": 5,\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"temperature\": 0.8,\n",
    "                \"num_beams\": 5,\n",
    "                \"no_repeat_ngram_size\": 2,\n",
    "            },\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec893f1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this post, we demonstrated how to use SageMaker large model inference containers to host two large language models, BLOOM-176B and OPT-30B. We used DeepSpeed’s model parallel techniques with multiple GPUs on a single SageMaker machine learning instance. For more details about Amazon SageMaker and its large model inference capabilities, refer to the following:\n",
    "\n",
    "* Amazon SageMaker now supports deploying large models through configurable volume size and timeout quotas (https://aws.amazon.com/about-aws/whats-new/2022/09/amazon-sagemaker-deploying-large-models-volume-size-timeout-quotas/)\n",
    "* Real-time inference – Amazon SageMake (https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cccc3e",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfcbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Delete the end point\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efea27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - In case the end point failed we still want to delete the model\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd621e",
   "metadata": {},
   "source": [
    "#### Optionally delete the model checkpoint from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive s3://{bucket}/{s3_model_prefix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d24929",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s3_client.list_objects(Bucket=bucket, Prefix=f\"{s3_model_prefix}/\")[\"Contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e32dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
