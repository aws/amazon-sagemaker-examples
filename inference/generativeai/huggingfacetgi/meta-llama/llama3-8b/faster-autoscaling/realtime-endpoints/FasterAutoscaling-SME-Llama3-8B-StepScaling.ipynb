{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f236c9",
   "metadata": {},
   "source": [
    "# Faster autoscaling on Amazon SageMaker realtime endpoints (Step Scaling)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we show how the new faster autoscaling feature helps scale sagemaker inference endpoints by almost 6x faster than earlier.\n",
    "\n",
    "We deploy Meta's `Llama3-8B-Instruct` model to an Amazon SageMaker realtime endpoint using Text Generation Inference (TGI) Deep Learning Container (DLC) and apply <span style='color:green'><b>Step Scaling</b></span> autoscaling policies to the endpoint.\n",
    "\n",
    "\n",
    "<span class=\"alert alert-block alert-warning\">Please use at least <strong>`m5.2xlarge`</strong> or larger instance types if running this on Amazon SageMaker Notebook Instance.</span>\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "<div style=\"border: 1px solid #f00; border-radius: 5px; padding: 10px; background-color: #fee;\">\n",
    "Before using this notebook please ensure you have access to an active access token from HuggingFace and have accepted the license agreement from Meta.\n",
    "\n",
    "- **Step 1:** Create user access token in HuggingFace (HF). Refer [here](https://huggingface.co/docs/hub/security-tokens) on how to create HF tokens.\n",
    "- **Step 2:** Login to [HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/tree/main) and navigate to *Meta-Llama-3-8B-Instruct** home page.\n",
    "- **Step 3:** Accept META LLAMA 3 COMMUNITY LICENSE AGREEMENT by following the instructions [here](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/tree/main)\n",
    "- **Step 4:** Wait for the approval email from META (Approval may take any where b/w 1-3 hrs)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650293f5-969e-4019-ba54-be21a363915d",
   "metadata": {},
   "source": [
    "Ensure python version of kernel is 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d71a7-50d2-45b2-8b9f-b02ed52218b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932827cb-caaa-4baa-9c95-9cf057468cf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install packages using uv, an extremely fast python package installer. Read more about uv here <https://astral.sh/blog/uv>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d260cb1-1355-448e-8fd9-3eebb1584ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install uv && uv pip install -U ipywidgets\n",
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe9e981-5df5-4b01-9b64-945e01c06423",
   "metadata": {
    "tags": []
   },
   "source": [
    "Restart kernel after installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abe082-5ce0-4a26-bae8-68f9bff4104c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n",
    "print(\"Kernel restarted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ec770-d05e-474d-80da-d2f2bab63db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load rich extension\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97edd4-8bba-4806-bce5-c559e23da05d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "from uuid import uuid4\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "from botocore.config import Config\n",
    "from rich import box, print\n",
    "from rich.console import Console\n",
    "from rich.progress import Progress, SpinnerColumn, TimeElapsedColumn\n",
    "from rich.table import Table\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "from utils.autoscaling import (\n",
    "    monitor_scaling_events,\n",
    "    print_scaling_times,\n",
    "    test_concurrency_level,\n",
    ")\n",
    "\n",
    "from utils.llmperf import (\n",
    "    print_llmperf_results,\n",
    "    trigger_auto_scaling,\n",
    "    monitor_process,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02d12b-2109-4f01-8da8-8972ba493398",
   "metadata": {},
   "source": [
    "## Initiate sagemaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d1c2d-787e-4792-a276-897a9cd183cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "config = Config(retries=dict(max_attempts=10))\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = sess.sagemaker_client\n",
    "sagemaker_runtime_client = sess.sagemaker_runtime_client\n",
    "cloudwatch_client = boto3.client(\"cloudwatch\", region_name=region, config=config)\n",
    "\n",
    "hf_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "print(f\"HF Model ID: [b green]{hf_model_id}\")\n",
    "print(f\"Region: [b blue]{region}\")\n",
    "print(f\"Role: [b red]{role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f7d75-de75-4687-a2e2-ab4aa7168ef6",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "Create and deploy model using Amazon SageMaker HuggingFace TGI DLC\n",
    "\n",
    "<https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>NOTE:</b> Remember to copy your Hugging Face Access Token from <a href=\"https://hf.co/\">https://hf.co/</a> before running the below cell.<br/><br/>\n",
    "Refer <a href=\"https://huggingface.co/docs/hub/security-tokens\">here</a> to learn about creating HF tokens.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89da4d-9ce7-4e5b-a02a-3f2c690cd26d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "suffix = f\"{str(uuid4())[:5]}-{datetime.now().strftime('%d%b%Y')}\"\n",
    "model_name = f\"Llama3-8B-fas-{suffix}\"\n",
    "endpoint_name = model_name\n",
    "health_check_timeout = 900\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGING_FACE_HUB_TOKEN\") or getpass(\"Enter HUGGINGFACE Access Token: \")\n",
    "\n",
    "# retrieve the llm image uri\n",
    "# tgi_dlc = f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\"\n",
    "tgi_dlc = get_huggingface_llm_image_uri(\"huggingface\", version=\"2.0.0\")\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"meta-llama/Meta-Llama-3-8B-Instruct\",  # model_id from hf.co/models\n",
    "    \"SM_NUM_GPUS\": \"1\",  # Number of GPU used per replica\n",
    "    \"MAX_INPUT_LENGTH\": \"2048\",  # Max length of input text\n",
    "    \"MAX_TOTAL_TOKENS\": \"4096\",  # Max length of the generation (including input text)\n",
    "    \"MAX_BATCH_TOTAL_TOKENS\": \"8192\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "    \"MESSAGES_API_ENABLED\": \"true\",  # Enable the messages API\n",
    "    \"HUGGING_FACE_HUB_TOKEN\": HF_TOKEN,\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "print(f\"Creating model: [b green]{model_name}...\")\n",
    "llm_model = HuggingFaceModel(name=model_name, role=role, image_uri=tgi_dlc, env=config)\n",
    "\n",
    "# Deploy model to Amazon SageMaker endpoint\n",
    "print(f\"Deploying model to endpoint: [b magenta]{endpoint_name}...\")\n",
    "predictor = llm_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,  # 15 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1af5c-e713-4cf8-bc23-1c96f1e61327",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Invoke and test endpoint using messages API. Refer to HF [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9ecc2-fffe-4ff1-b78b-1222fe6d32de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": hf_model_id,  # model id is required\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "chat = predictor.predict({\"messages\": messages, **parameters})\n",
    "\n",
    "# Unpack and print response\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb619d-b402-46bf-9451-62f50f70e878",
   "metadata": {},
   "source": [
    "## Baseline average latency at various concurrency levels (Optional)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>NOTE:</b> Running the following cell is optional<br/><br/>\n",
    "By capturing average latency across various concurrency levels, we can get a fair idea on after how many concurrent request does endpoint performance would degrade significantly.<br/><br/>\n",
    "Having this information can help define values for scaling policy accordingly.    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>INFO: ℹ️</b> Signal here is, at a given concurrency level you start to see average latency increase significantly.<br/>\n",
    "At this concurrency level the endpoint gets overloaded and cannot serve requests in a timely fashion.<br/>\n",
    "We use these values to set as threshold values for autoscaling.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7232ecd-bc78-4d0d-bf44-17c3e060cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of prompts\n",
    "prompts = [\n",
    "    \"what is deep learning?\",\n",
    "    \"what are various inference modes in Amazon SageMaker?\",\n",
    "    \"Can I host Large language models on Amazon SageMaker?\",\n",
    "    \"Does Amazon SageMaker support TensorRT-LLM?\",\n",
    "    \"what is step scaling policy in the context of autoscaling ec2 instances on AWS?\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \"List 5 benefits of incorporating limes into the diet.\",\n",
    "]\n",
    "\n",
    "# Test different concurrency levels and measure average latency\n",
    "concurrency_levels = [10, 50, 75, 100]  # Adjust these values as needed\n",
    "\n",
    "for concurrency_level in concurrency_levels:\n",
    "    try:\n",
    "        avg_latency = test_concurrency_level(\n",
    "            concurrency_level,\n",
    "            prompts,\n",
    "            messages,\n",
    "            parameters,\n",
    "            endpoint_name,\n",
    "            sagemaker_runtime_client,\n",
    "        )\n",
    "        print(\n",
    "            f\"[b]Concurrency:[/b] {concurrency_level} requests,\"\n",
    "            f\" [b]Average latency:[/b] {avg_latency:.2f} seconds\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[b]At Concurrency[/b] {concurrency_level} requests,\" f\"[b]Exception:[/b] \\n{e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7a5ab-0264-4b12-8243-b4aa649335b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Apply Step-Scaling autoscaling policies to endpoint\n",
    "\n",
    "- **Step 1:** Register Scalable Target\n",
    "- **Step 2:** Create Scale-Out Policy\n",
    "- **Step 3:** Create Scale-In Policy\n",
    "- **Step 4:** Create CloudWatch Alarms\n",
    "\n",
    "Define and apply the step-scaling policy for scaling out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf762f-beec-42ed-9ff8-5b06f76269ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variant_name = \"AllTraffic\"\n",
    "as_min_capacity = 1\n",
    "as_max_capacity = 2\n",
    "\n",
    "resource_id = f\"endpoint/{endpoint_name}/variant/{variant_name}\"\n",
    "\n",
    "autoscaling_client = boto3.client(\"application-autoscaling\", region_name=region)\n",
    "\n",
    "# Register scalable target\n",
    "scalable_target = autoscaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=as_min_capacity,\n",
    "    MaxCapacity=as_max_capacity,  # Replace with your desired maximum instances\n",
    ")\n",
    "\n",
    "scalable_target_arn = scalable_target[\"ScalableTargetARN\"]\n",
    "print(f\"Resource ID: [b blue]{resource_id}\")\n",
    "print(f\"Scalable_target_arn:\\n[b green]{scalable_target_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2e234-d1c7-4575-b943-5291c70c326d",
   "metadata": {},
   "source": [
    "### Create StepScaling <span style='color:green'>Scale-out</span> Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f32bf-126c-41ab-8213-10052f5351e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure step scaling scale-out policy\n",
    "scale_out_policy_response = autoscaling_client.put_scaling_policy(\n",
    "    PolicyName=f\"{endpoint_name}-ScaleOutPolicy\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"StepScaling\",\n",
    "    StepScalingPolicyConfiguration={\n",
    "        \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "        \"Cooldown\": 300,  # 5 minutes cooldown\n",
    "        \"MetricAggregationType\": \"Maximum\",\n",
    "        \"StepAdjustments\": [\n",
    "            {\n",
    "                \"MetricIntervalLowerBound\": 0,\n",
    "                \"MetricIntervalUpperBound\": 20,\n",
    "                \"ScalingAdjustment\": 1,  # Increase by one instance\n",
    "            },\n",
    "            {\n",
    "                \"MetricIntervalLowerBound\": 20,\n",
    "                \"ScalingAdjustment\": 2,  # Increase by 2 instances\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "# print(scale_out_policy_response)\n",
    "scale_out_policy_arn = scale_out_policy_response[\"PolicyARN\"]\n",
    "print(f\"Step scaling policy ARN: [i green]{scale_out_policy_arn}[/i green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc40cae-fe85-4e3b-8bfe-c1ef238ea76f",
   "metadata": {},
   "source": [
    "### Create StepScaling <span style='color:green'>Scale-In</span> Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b06c1e-c126-4203-b149-473e033ae879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale_in_policy_response = autoscaling_client.put_scaling_policy(\n",
    "    PolicyName=f\"{endpoint_name}-ScaleInPolicy\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"StepScaling\",\n",
    "    StepScalingPolicyConfiguration={\n",
    "        \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "        \"Cooldown\": 300,  # Cooldown period after scale-in activity\n",
    "        \"MetricAggregationType\": \"Maximum\",\n",
    "        \"StepAdjustments\": [\n",
    "            {\n",
    "                \"MetricIntervalUpperBound\": 0,\n",
    "                \"MetricIntervalLowerBound\": -20,\n",
    "                \"ScalingAdjustment\": -1,  # Decrease by 1 instance\n",
    "            },\n",
    "            {\"MetricIntervalUpperBound\": -20, \"ScalingAdjustment\": -2},  # Decrease by 2 instances\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "# print(scale_in_policy_response)\n",
    "scale_in_policy_arn = scale_in_policy_response[\"PolicyARN\"]\n",
    "print(f\"Step scaling policy ARN: [i green]{scale_in_policy_arn}[/i green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3f1ec-f4cb-4a1f-ad4d-5e6a1d4f7aee",
   "metadata": {},
   "source": [
    "### Create CloudWatch alarms (Step-Scaling)\n",
    "\n",
    "Create CloudWatch Alarms using new <span style='color:green'><b>ConcurrentRequestsPerModel</b></span> high-resolution Metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fdea0-6d59-4369-8dc3-db301daacf5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the alarm parameters for scale-out\n",
    "alarm_name_scale_out = f\"Step-Scaling-AlarmHigh-SageMaker:{resource_id}\"\n",
    "metric_name = \"ConcurrentRequestsPerModel\"\n",
    "namespace = \"AWS/SageMaker\"  # CloudWatch Namespace to write metric data\n",
    "statistic = \"Maximum\"\n",
    "period = 60  # 10 seconds\n",
    "evaluation_periods = 3\n",
    "threshold = 20.0  # Threshold for scale-out\n",
    "comparison_operator = \"GreaterThanOrEqualToThreshold\"\n",
    "dimensions = [\n",
    "    {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "    {\"Name\": \"VariantName\", \"Value\": \"AllTraffic\"},\n",
    "]\n",
    "alarm_actions = [scale_out_policy_response[\"PolicyARN\"]]\n",
    "treat_missing_data = \"ignore\"\n",
    "\n",
    "# create CloudWatch alarm for scale-out\n",
    "response = cloudwatch_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name_scale_out,\n",
    "    MetricName=metric_name,\n",
    "    Namespace=namespace,\n",
    "    Statistic=statistic,\n",
    "    Period=period,\n",
    "    EvaluationPeriods=evaluation_periods,\n",
    "    Threshold=threshold,\n",
    "    ComparisonOperator=comparison_operator,\n",
    "    Dimensions=dimensions,\n",
    "    AlarmActions=alarm_actions,\n",
    "    TreatMissingData=treat_missing_data,\n",
    ")\n",
    "\n",
    "print(f\"CloudWatch alarm created for scale-out:\\n[b blue]{alarm_name_scale_out}\")\n",
    "\n",
    "# Define the alarm parameters for scale-in\n",
    "alarm_name_scale_in = f\"Step-Scaling-AlarmLow-SageMaker:{resource_id}\"\n",
    "comparison_operator = \"LessThanOrEqualToThreshold\"\n",
    "threshold = 10.0  # Adjust based on your requirements\n",
    "alarm_actions = [scale_in_policy_response[\"PolicyARN\"]]\n",
    "\n",
    "# Create CloudWatch alarm for scale-in\n",
    "response = cloudwatch_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name_scale_in,\n",
    "    MetricName=metric_name,\n",
    "    Namespace=namespace,\n",
    "    Statistic=statistic,\n",
    "    Period=period,\n",
    "    EvaluationPeriods=evaluation_periods,\n",
    "    Threshold=threshold,\n",
    "    ComparisonOperator=comparison_operator,\n",
    "    Dimensions=dimensions,\n",
    "    AlarmActions=alarm_actions,\n",
    "    TreatMissingData=treat_missing_data,\n",
    ")\n",
    "\n",
    "print(f\"CloudWatch alarm created for scale-in:\\n[b blue]{alarm_name_scale_in}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a4cba-8aec-4b5c-b9ea-97d4ea82d9f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trigger autoscaling action\n",
    "\n",
    "### Use LLMPerf to generate traffic to the endpoint\n",
    "\n",
    "Refer to <https://github.com/philschmid/llmperf> for more details on LLMPerf.\n",
    "\n",
    "Run the LLMPerf traffic generation script in the background using `subprocess.Popen`\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>INFO:ℹ️</b> Refer to <a href=\"utils/llmperf.py\">utils.llmperf</a> for `trigger_autoscaling` function implementation\n",
    "</div>\n",
    "\n",
    "\n",
    "### Monitor Alarm Trigger times and Scaling event times\n",
    "As llmperf generates traffic to the endpoint continuously this trigger auto-scaling.\n",
    "\n",
    "The `monitor_scaling_events` function does the following:\n",
    "- Calculates time taken for alarm to go into InAlarm state.\n",
    "- checks if alarm is InAlarm state. If yes, then starts the scaling timer\n",
    "- continuously monitors the `DesiredInstanceCount` property of the endpoint\n",
    "  - waits till `CurrentInstanceCount == DesiredInstanceCount` and `EndpointStatus` is `InService`\n",
    "- Calculates time taken to scale out instances prints the times in a table\n",
    "\n",
    "The below cell triggers auto scaling action and calls the monitor_scaling_events immediately on the AlarmHigh\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>INFO:ℹ️</b> Refer to <a href=\"utils/autoscaling.py\">utils.autoscaling</a> for `monitor_scaling_events` function implementation\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE: ⚠️</b>Per the <b>ScaleOut</b> Alarm, scale-out actions only start after the threshold of <b>ConcurrentRequestsPerModel >= 20</b> for 3 datapoints within <b>3 minutes</b> is breached.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d00ca1-f058-4dfb-9993-e231b58e413c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trigger LLMPerf script to generate traffic to endpoint\n",
    "num_concurrent_requests = 100\n",
    "# LLMperf requires session credentials be passed in via environment variables.\n",
    "# We'll use the current session to get these credentials.\n",
    "creds = boto_session.get_credentials()\n",
    "process = trigger_auto_scaling(creds, region, endpoint_name, num_concurrent_requests)\n",
    "print(f\"[b green]Process ID for LLMPerf: {process.pid}\")\n",
    "\n",
    "# Start monitoring scaling events\n",
    "SLEEP_TIME = 5  # time to sleep\n",
    "scaling_times = monitor_scaling_events(\n",
    "    endpoint_name, alarm_name_scale_out, SLEEP_TIME, cloudwatch_client, sagemaker_client\n",
    ")\n",
    "\n",
    "# Print scaling times\n",
    "console = Console()\n",
    "table = print_scaling_times(scaling_times)\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b43ce1-dde3-42aa-9cbe-0716b5f85496",
   "metadata": {},
   "source": [
    "### Monitor if the background process (llmperf) is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93abbbca-a3b4-49ee-9994-8ccfe7a13874",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Monitor the background traffic generation process for completion\n",
    "monitor_process(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6829fa5-4a91-472e-8c3b-905612e778a0",
   "metadata": {},
   "source": [
    "## Print LLMPerf results\n",
    "\n",
    "LLMPerf writes the results to **\"results/\"** directory. `summary.json` file has the endpoint benchmarking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d502c-e8d6-4023-a9bc-9e011b63c2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_llmperf_results(num_concurrent_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd184b-fcfe-4260-95ce-5bdd557ad6e2",
   "metadata": {},
   "source": [
    "### Monitor Scale-in action scaling times (Optional)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE: ⚠️</b>Per the <b>ScaleIn</b> Alarm, scale-in actions only start after the threshold of <b>ConcurrentRequestsPerModel <= 10</b> for 3 datapoints within <b>3 minutes</b> is breached.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883924cc-9f29-48cf-85ac-1d96c0a3dd16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start monitoring scaling events\n",
    "SLEEP_TIME = 5  # time to sleep\n",
    "scaling_times = monitor_scaling_events(\n",
    "    endpoint_name,\n",
    "    alarm_name_scale_in,  # scale_in cloudwatch metric alarm name\n",
    "    SLEEP_TIME,\n",
    "    cloudwatch_client,\n",
    "    sagemaker_client,\n",
    ")\n",
    "\n",
    "# Print scaling times\n",
    "console = Console()\n",
    "table = print_scaling_times(scaling_times)\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2d5b0-dc4b-40e3-8ada-ceddecfdac1a",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "- Delete cloudwatch alarms\n",
    "- Delete scaling policies\n",
    "- Deregister scalable target\n",
    "- Delete model\n",
    "- Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44ef56-dbcc-4e23-97c2-af6cb062b498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete CloudWatch alarms created for Step scaling policy\n",
    "alarm_names = [alarm_name_scale_out, alarm_name_scale_in]\n",
    "\n",
    "for alarm in alarm_names:\n",
    "    try:\n",
    "        cloudwatch_client.delete_alarms(AlarmNames=[alarm])\n",
    "        print(f\"Deleted CloudWatch scale-out alarm [b]{alarm} ✅\")\n",
    "    except cloudwatch_client.exceptions.ResourceNotFoundException:\n",
    "        print(f\"CloudWatch scale-out alarm [b]{alarm}[/b] not found.\")\n",
    "\n",
    "\n",
    "# Delete scaling policies\n",
    "print(\"---\" * 10)\n",
    "step_policies = [f\"{endpoint_name}-ScaleInPolicy\", f\"{endpoint_name}-ScaleOutPolicy\"]\n",
    "for policy_name in step_policies:\n",
    "    try:\n",
    "        autoscaling_client.delete_scaling_policy(\n",
    "            PolicyName=policy_name,\n",
    "            ServiceNamespace=\"sagemaker\",\n",
    "            ResourceId=resource_id,\n",
    "            ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "        )\n",
    "        print(f\"Deleted scaling policy [i green]{policy_name} ✅\")\n",
    "    except autoscaling_client.exceptions.ObjectNotFoundException:\n",
    "        print(f\"Scaling policy [i]{policy_name}[/i] not found.\")\n",
    "\n",
    "# Deregister scalable target\n",
    "try:\n",
    "    autoscaling_client.deregister_scalable_target(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    )\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] deregistered. ✅\")\n",
    "except autoscaling_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] not found!.\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "# Delete model and endpoint\n",
    "try:\n",
    "    print(f\"Deleting model: [b green]{model_name} ✅\")\n",
    "    predictor.delete_model()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting endpoint: [b magenta]{predictor.endpoint_name} ✅\")\n",
    "    predictor.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d8011",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-SME-Llama3-8B-StepScaling.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
