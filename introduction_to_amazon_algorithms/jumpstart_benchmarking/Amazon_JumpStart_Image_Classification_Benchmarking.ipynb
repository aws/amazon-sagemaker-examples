{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e9f377d",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart TensorFlow Image Classification Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0dfd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as cf\n",
    "import json\n",
    "import itertools\n",
    "import queue\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List, NamedTuple, Optional, Tuple\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import sagemaker.hyperparameters\n",
    "import sagemaker.model_uris\n",
    "import sagemaker.script_uris\n",
    "import sagemaker.image_uris\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355786c5",
   "metadata": {},
   "source": [
    "***\n",
    "Let's fist identify some top-level constants that will be utilized throughout this notebook. These constants are gathered at the top of this notebook so adjustments can be made in one place.\n",
    "\n",
    "The first set of constants control SageMaker training job behaviors:\n",
    "- __EC2_INSTANCE_TYPE__: EC2 instance type used for training.\n",
    "- __SM_AMT_MAX_JOBS__: Maximum total number of training jobs to start per hyperparameter tuning job.\n",
    "- __SM_AMT_MAX_PARALLEL_TRAINING_JOBS_PER_TUNER__: Maximum number of parallel training jobs per hyperparameter tuning job.\n",
    "- __SM_AMT_MAX_PARALLEL_TUNING_JOBS__: Maximum number of parallel hyperparameter tuning jobs.\n",
    "- __SM_AMT_OBJECTIVE_METRIC_NAME__: Name of the metric for evaluating training jobs.\n",
    "- __SM_SESSION__: SageMaker Session object with custom configuration to resolve [SDK rate exceeded and throttling exceptions](https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-python-throttlingexception/).\n",
    "\n",
    "The next set of constants control the behavior of the training script:\n",
    "- __HYPERPARAMETERS__: Set of hyperparameters overriding any [default built-in value](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-TF-Hyperparameter.html).\n",
    "\n",
    "Finally, this notebook provides features to re-attach previously launched training jobs and load previously saved metrics for further analysis. The following constants control this behavior:\n",
    "- __SAVE_TUNING_JOB_NAMES_FILE_PATH__: Path of the [JSON Lines](https://jsonlines.org/) file that keeps track of the tuning job name assoicated with a unique model name and dataset name.\n",
    "- __SAVE_METRICS_FILE_PATH__: Path of the JSON Lines file that records metrics associated with each tuning job.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52791961",
   "metadata": {},
   "outputs": [],
   "source": [
    "EC2_INSTANCE_TYPE = \"ml.g4dn.xlarge\"\n",
    "SM_AMT_MAX_JOBS = 2\n",
    "SM_AMT_MAX_PARALLEL_TRAINING_JOBS_PER_TUNER = 2\n",
    "SM_AMT_MAX_PARALLEL_TUNING_JOBS = 20\n",
    "SM_AMT_OBJECTIVE_METRIC_NAME = \"val_accuracy\"\n",
    "SM_SESSION = sagemaker.Session(\n",
    "    sagemaker_client=boto3.client(\n",
    "        \"sagemaker\",\n",
    "        config=Config(\n",
    "            connect_timeout=5,\n",
    "            read_timeout=60,\n",
    "            retries={\"max_attempts\": 20}\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "HYPERPARAMETERS = {\n",
    "    \"epochs\": 10,\n",
    "    \"early_stopping\": \"True\",\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"early_stopping_min_delta\": 0.001,\n",
    "    \"augmentation\": \"False\",  # For now, augmentation is CPU-bound and slow\n",
    "}\n",
    "\n",
    "SAVE_TUNING_JOB_NAMES_FILE_PATH = Path.cwd() / \"benchmarking_tuning_job_names.jsonl\"\n",
    "SAVE_METRICS_FILE_PATH = Path.cwd() / \"benchmarking_metrics.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d45e9",
   "metadata": {},
   "source": [
    "## Identify models and datasets\n",
    "***\n",
    "In this section, we will define two lists, `models` and `datasets`, which contain unique identifiers for all models and all datasets we wish to perform this benchmarking task on. The hyperparameter tuning jobs attempted to be instantiated will be the [cartesian product](https://docs.python.org/3/library/itertools.html#itertools.product) between these two lists.\n",
    "\n",
    "***\n",
    "First, we will identify all built-in image classification model IDs to run this benchmarking task on. Because SageMaker JumpStart maintains a large number of models for this task, the default code in this notebook identifies only a few models by model ID. If desired, it is possible to run a thorough benchmarking analysis on all TensorFlow image classification models made available by SageMaker Built-In Algorithms via:\n",
    "```python\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "from sagemaker.jumpstart.filters import And\n",
    "\n",
    "filter_value = And(\"task == ic\", \"framework == tensorflow\")\n",
    "models = list_jumpstart_models(filter=filter_value)\n",
    "```\n",
    "This may be desired if you have a unique dataset and would like to perform large-scale benchmarking or model selection tasks on your custom dataset. However, please be cautious as a benchmarking task with this many models will require the deployment of a large number of resources.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a12d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "from sagemaker.jumpstart.filters import And\n",
    "\n",
    "# Retrieves all TensorFlow Image Classification models made available by SageMaker Built-In Algorithms.\n",
    "filter_value = And(\"task == ic\", \"framework == tensorflow\")\n",
    "models = list_jumpstart_models(filter=filter_value)\n",
    "models.remove(\"tensorflow-ic-cait-m48-448\")\n",
    "models.remove(\"tensorflow-ic-cait-m36-384\")\n",
    "models.remove(\"tensorflow-ic-cait-s36-384\")\n",
    "models.remove(\"tensorflow-ic-bit-s-r152x4-ilsvrc2012\")\n",
    "models.remove(\"tensorflow-ic-bit-m-r152x4-ilsvrc2012\")\n",
    "models.remove(\"tensorflow-ic-bit-m-r152x4-imagenet21k\")\n",
    "models.remove(\"tensorflow-ic-bit-s-r101x3-ilsvrc2012-classification-1\")\n",
    "models.remove(\"tensorflow-ic-bit-m-r101x3-ilsvrc2012-classification-1\")\n",
    "models.remove(\"tensorflow-ic-bit-m-r101x3-imagenet21k-classification-1\")\n",
    "models.remove(\"tensorflow-ic-efficientnet-v2-imagenet21k-xl\")\n",
    "models.remove(\"tensorflow-ic-efficientnet-v2-imagenet21k-ft1k-xl\")\n",
    "\n",
    "# models = ic_models[:8]#[::20]\n",
    "# models = ['tensorflow-ic-imagenet-mobilenet-v1-075-224-classification-4']\n",
    "# models = ['tensorflow-ic-swin-s3-tiny-224']\n",
    "# models = ['tensorflow-ic-swin-base-patch4-window7-224']\n",
    "# models = ['tensorflow-ic-swin-small-patch4-window7-224']\n",
    "# models = ['tensorflow-ic-imagenet-mobilenet-v1-075-224-classification-4', 'tensorflow-ic-efficientnet-v2-imagenet1k-b3', 'tensorflow-ic-efficientnet-v2-imagenet21k-l', 'tensorflow-ic-deit-base-patch16-384', 'tensorflow-ic-cait-s24-224', 'tensorflow-ic-cait-m48-448'] # 'tensorflow-ic-swin-large-patch4-window12-384', 'tensorflow-ic-swin-small-patch4-window7-224', 'tensorflow-ic-deit-base-patch16-224',\n",
    "# models = ['tensorflow-ic-cait-m48-448', 'tensorflow-ic-swin-large-patch4-window12-384', 'tensorflow-ic-cait-m36-384']\n",
    "models = ['tensorflow-ic-imagenet-mobilenet-v1-075-224-classification-4', 'tensorflow-ic-swin-small-patch4-window7-224']\n",
    "print(len(models))\n",
    "pprint(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291829f",
   "metadata": {},
   "source": [
    "***\n",
    "We also need to identify the datasets to perform benchmarking on. Unlike built-in models, JumpStart does not provide a default API to query available datasets. It is also likely that you may have your own dataset hosted on S3 that you would like to benchmark. The following data structures provide a consistent framework to define dataset locations for the scope of this notebook. This is important because the benchmarking task is most beneficial with a training/validation/test dataset split. While possible, it is not recommended to let the model transfer learning script perform this split. Fitting a SageMaker Estimator requires channel definitions, which these objects create automatically via the `S3DatasetSplit.channels` method.\n",
    "\n",
    "*Notes on dataset channel behaviors*: Training will utilize only the data provided in the \"training\" channel, model selection across hyperparameters and epochs will use data in the \"validation\" channel, and the final evaluation of model performance will be based on data provided in the \"test\" channel. If a \"test\" channel is not provided, then training should complete successfully, but metric definitions with a name matching the pattern \"test_\\*\" will not be available in the training job logs. If a \"validation\" channel is not provided, then the default behavior of the JumpStart TensorFlow Image Classification algorithm is to perform a split of the \"training\" channel dataset into training and validation datasets.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22987387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3Dataset:\n",
    "    def __init__(self, bucket: str, prefix: str) -> None:\n",
    "        self.bucket = bucket\n",
    "        self.prefix = prefix\n",
    "        \n",
    "    def path(self) -> str:\n",
    "        return f\"s3://{self.bucket}/{self.prefix}\"\n",
    "\n",
    "\n",
    "class S3DatasetSplit:\n",
    "    def __init__(self, train: S3Dataset, validation: Optional[S3Dataset] = None, test: Optional[S3Dataset] = None) -> None:\n",
    "        self.train = train\n",
    "        self.validation = validation\n",
    "        self.test = test\n",
    "    \n",
    "    @classmethod\n",
    "    def from_prefixes(cls, bucket: str, prefix_train: str, prefix_validation: Optional[str] = None, prefix_test: Optional[str] = None) -> 'S3TrainValTestDataset':\n",
    "        return cls(\n",
    "            S3Dataset(bucket, prefix_train),\n",
    "            S3Dataset(bucket, prefix_validation) if prefix_validation is not None else None,\n",
    "            S3Dataset(bucket, prefix_test) if prefix_test is not None else None\n",
    "        )\n",
    "    \n",
    "    def channels(self) -> Dict[str, str]:\n",
    "        res = {\"training\": self.train.path()}\n",
    "        if self.validation is not None:\n",
    "            res[\"validation\"] = self.validation.path()\n",
    "        if self.test is not None:\n",
    "            res[\"test\"] = self.test.path()\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36df43-4c42-4b18-8837-4ae6256ba8d8",
   "metadata": {},
   "source": [
    "***\n",
    "Next, a dictionary of available datasets is created and one of these datasets is selected to perform analysis. To get a feel for the performance of different models with respect to different datasets, simply run this notebook for a different selected list of datasets! If you have your own dataset, just create a new entry that specifies the bucket along with prefixes for the train, validation, and test datasets. The dataset should be structured according to the [built-in algorithm training data input format](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification-tensorflow.html).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DICT = {\n",
    "    \"tf-flowers\": S3DatasetSplit.from_prefixes(\n",
    "        bucket=f\"jumpstart-cache-prod-{SM_SESSION.boto_region_name}\",\n",
    "        prefix_train=\"training-datasets/tf_flowers/\",\n",
    "        prefix_test=\"training-datasets/tf_flowers/\"\n",
    "    ),\n",
    "    \"ants-and-bees\": S3DatasetSplit.from_prefixes(\n",
    "        bucket=f\"jumpstart-cache-prod-{SM_SESSION.boto_region_name}\",\n",
    "        prefix_train=\"training-datasets/ants-and-bees/\",\n",
    "        prefix_test=\"training-datasets/ants-and-bees/\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "datasets = [\"tf-flowers\", \"ants-and-bees\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc883f99",
   "metadata": {},
   "source": [
    "## Create assets for training\n",
    "\n",
    "***\n",
    "This section contains a variety of helper functions that will be utilized for this SageMaker TensorFlow image classification benchmarking task, including functions to:\n",
    "1) create a SageMaker `Estimator` object from the JumpStart model hub\n",
    "2) create a SageMaker`HyperparameterTuner` for a specified model from the JumpStart model hub\n",
    "3) re-attach a SageMaker `HyperparameterTuner` if a tuning job has already started\n",
    "4) extract metrics from `Estimator` logs\n",
    "5) save tuning job information to file to enable re-attaching jobs in new sessions\n",
    "6) save resulting benchmarking metrics to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cd272",
   "metadata": {},
   "source": [
    "***\n",
    "The following block contains a helper function to obtain a SageMaker `Estimator` for a given JumpStart built-in `model_id`. This includes obtaining the appropriate URIs for the training docker image, the training script tarball, and the pre-trained model tarball to further fine-tune. This retrieval is provided by the SageMaker JumpStart built-in algorithms and allows for the creation of a SageMaker `Estimator` instance directly from these URIs.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jumpstart_estimator(\n",
    "    model_id: str,\n",
    "    role: str,\n",
    "    job_name: str,\n",
    "    s3_output_location: str,\n",
    "    model_version: str,\n",
    "    instance_type: str = EC2_INSTANCE_TYPE,\n",
    "    metric_definitions: Optional[List[Dict[str, str]]] = None,\n",
    "    hyperparameters: Optional[Dict[str, str]] = None,\n",
    ") -> sagemaker.estimator.Estimator:\n",
    "    \"\"\"Obtain a SageMaker Estimator for a given model ID.\"\"\"\n",
    "    \n",
    "    # Retrieve the docker image\n",
    "    train_image_uri = sagemaker.image_uris.retrieve(\n",
    "        region=None,\n",
    "        framework=None,\n",
    "        model_id=model_id,\n",
    "        model_version=model_version,\n",
    "        image_scope=\"training\",\n",
    "        instance_type=instance_type,\n",
    "    )\n",
    "    \n",
    "    # Retrieve the training script\n",
    "    train_source_uri = sagemaker.script_uris.retrieve(\n",
    "        model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    "    )\n",
    "    \n",
    "    # Retrieve the pre-trained model tarball to further fine-tune\n",
    "    train_model_uri = sagemaker.model_uris.retrieve(\n",
    "        model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    "    )\n",
    "    \n",
    "    # Create and return SageMaker Estimator instance\n",
    "    return sagemaker.estimator.Estimator(\n",
    "        role=role,\n",
    "        image_uri=train_image_uri,\n",
    "        source_dir=train_source_uri,\n",
    "        model_uri=train_model_uri,\n",
    "        entry_point=\"transfer_learning.py\",\n",
    "        instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        max_run=360000,\n",
    "        hyperparameters=hyperparameters,\n",
    "        output_path=s3_output_location,\n",
    "        base_job_name=job_name,\n",
    "        metric_definitions=metric_definitions,\n",
    "        sagemaker_session=SM_SESSION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57659785-5979-494f-babc-36ee8ab5ed9f",
   "metadata": {},
   "source": [
    "***\n",
    "While we now have a means to create a SageMaker `Estimator`, default hyperparameter values may not be optimal for the considered task. Therefore, to obtain the best benchmarking results, we would like to wrap this `Estimator` within a SageMaker hyperparameter tuning job. The following function wraps `create_jumpstart_estimator` to obtain a SageMaker `HyperparameterTuner` for a given JumpStart build-in model_id with properties for this benchmarking task. Because tuning jobs have a 32-character name length limit and this benchmarking task can create a large number of tuning jobs with similar (or identical) names after truncation, a unique-id is provided for each model to enforce unique tuning job names.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_benchmarking_tuner(\n",
    "    model_id: str,\n",
    "    unique_id: int,\n",
    "    session: sagemaker.session.Session = SM_SESSION,\n",
    "    model_version: str = \"*\",\n",
    "    objective_type: str = \"Maximize\",\n",
    ") -> sagemaker.tuner.HyperparameterTuner:\n",
    "    \"\"\"Obtain a SageMaker HyperparameterTuner with properties for this benchmarking task.\n",
    "    \n",
    "    A unique ID is helpful to distinguish names of benchmarking jobs.\n",
    "    \"\"\"\n",
    "    role = session.get_caller_identity_arn()\n",
    "    output_bucket = session.default_bucket()\n",
    "    output_prefix = \"jumpstart-example-tf-ic-benchmarking\"\n",
    "    job_name = sagemaker.utils.name_from_base(f\"bm-{unique_id}-{model_id.replace('tensorflow-ic-', '')}\")\n",
    "    s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\"\n",
    "    \n",
    "    metrics_multiclass = (\"top_5_accuracy\",)\n",
    "    metrics_binary = (\"precision\", \"recall\", \"auc\", \"prc\",)\n",
    "    metrics = (\"accuracy\", \"loss\", *metrics_multiclass, *metrics_binary)\n",
    "    metric_definitions = [\n",
    "        *({\"Name\": f\"train_{metric}\", \"Regex\": f\"- {metric}: ([0-9\\\\.]+)\"} for metric in metrics),\n",
    "        *({\"Name\": f\"val_{metric}\", \"Regex\": f\"- val_{metric}: ([0-9\\\\.]+)\"} for metric in metrics),\n",
    "        *({\"Name\": f\"test_{metric}\", \"Regex\": f\"- Test {metric}: ([0-9\\\\.]+)\"} for metric in metrics),\n",
    "        {\"Name\": \"num_params\", \"Regex\": \"- Number of parameters: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"num_trainable_params\", \"Regex\": \"- Number of trainable parameters: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"num_non_trainable_params\", \"Regex\": \"- Number of non-trainable parameters: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"train_duration\", \"Regex\": \"- Total training duration: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"train_duration_per_epoch\", \"Regex\": \"- Average training duration per epoch: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"test_evaluation_latency\", \"Regex\": \"- Test evaluation latency: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"test_latency_per_sample\", \"Regex\": \"- Average test latency per sample: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"test_throughput\", \"Regex\": \"- Average test throughput: ([0-9\\\\.]+)\"},\n",
    "    ]\n",
    "\n",
    "    hyperparameters = sagemaker.hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "    \n",
    "    estimator = create_jumpstart_estimator(\n",
    "        model_id,\n",
    "        role,\n",
    "        job_name,\n",
    "        s3_output_location,\n",
    "        model_version=model_version,\n",
    "        metric_definitions=metric_definitions,\n",
    "        hyperparameters=hyperparameters\n",
    "    )\n",
    "    estimator.set_hyperparameters(**HYPERPARAMETERS)\n",
    "    \n",
    "    learning_rate = float(hyperparameters[\"learning_rate\"])\n",
    "    hyperparameter_ranges = {\n",
    "        \"learning_rate\": sagemaker.tuner.CategoricalParameter([learning_rate, learning_rate / 5])\n",
    "    }\n",
    "\n",
    "    tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "        estimator,\n",
    "        SM_AMT_OBJECTIVE_METRIC_NAME,\n",
    "        hyperparameter_ranges,\n",
    "        metric_definitions,\n",
    "        max_jobs=SM_AMT_MAX_JOBS,\n",
    "        max_parallel_jobs=SM_AMT_MAX_PARALLEL_TRAINING_JOBS_PER_TUNER,\n",
    "        objective_type=objective_type,\n",
    "        base_tuning_job_name=job_name,\n",
    "    )\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3fa53",
   "metadata": {},
   "source": [
    "***\n",
    "With these helper functions established, it is easy to create a `HyperparameterTuner` object for each specified model. But what happens if there is an error or the kerenel for this script is terminated? The hyperparameter tuning jobs would still run to completion and we would not want to re-launch these jobs in order to obtain our results. Therefore, we need yet another helper function that will either re-attach the hyperparameter tuning job if it exists or create a new one via `create_benchmarking_tuner`. To accomplish this, the JSON Lines file specified in `SAVE_TUNING_JOB_NAMES_FILE_PATH` is read and checked for whether a tuning job already exists for `model_name` and `dataset_name`. If it does exist, then the job is re-attached and returned. Otherwise, a new tuner is created and the `fit()` method is invoked with the `wait=False` argument and channels specified per the previously defined `S3DatasetSplit` object we used to store our dataset S3 location. We will have the thread wait for this job to complete later, but we first need to put this job information on the `queue_save_tuning_job` queue, which will indicate to the primary thread to append this job information to `SAVE_TUNING_JOB_NAMES_FILE_PATH`. Writing to this file needs to be done by the primary thread listener because multiple threads simultaneously writing to a file is not thread safe.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobInformation(NamedTuple):\n",
    "    model_name: str\n",
    "    dataset_name: str\n",
    "    tuning_job_name: Optional[str] = None\n",
    "\n",
    "\n",
    "def create_or_attach_tuner(\n",
    "    model_name: str,\n",
    "    dataset_name: str,\n",
    "    unique_id: int,\n",
    "    dataset_dict: Dict[str, S3DatasetSplit] = DATASET_DICT,\n",
    "    tuning_job_names_file_path: Path = SAVE_TUNING_JOB_NAMES_FILE_PATH,\n",
    "    session: sagemaker.Session = SM_SESSION\n",
    ") -> Tuple[sagemaker.tuner.HyperparameterTuner, JobInformation]:\n",
    "    if tuning_job_names_file_path.exists():\n",
    "        tuning_jobs_df = pd.read_json(tuning_job_names_file_path, lines=True).set_index([\"model_name\", \"dataset_name\"])\n",
    "        if (model_name, dataset_name) in tuning_jobs_df.index:\n",
    "            tuning_job_name = tuning_jobs_df.loc[(model_name, dataset_name), \"tuning_job_name\"]\n",
    "            tuner = sagemaker.tuner.HyperparameterTuner.attach(tuning_job_name, session)\n",
    "            job_information = JobInformation(model_name, dataset_name, tuning_job_name)\n",
    "            print(f\"> Re-attached previous SageMaker tuning job, {job_information}\")\n",
    "            return tuner, job_information\n",
    "        \n",
    "    tuner = create_benchmarking_tuner(model_name, unique_id)\n",
    "    dataset = dataset_dict[dataset_name]\n",
    "    tuner.fit(dataset.channels(), wait=False)\n",
    "    tuning_job_name = tuner.latest_tuning_job.name\n",
    "    job_information = JobInformation(model_name, dataset_name, tuning_job_name)\n",
    "    queue_save_tuning_job.put(job_information)\n",
    "    print(f\"> Starting new SageMaker tuning job, {job_information}\")\n",
    "    return tuner, job_information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc26ac0-0d8d-4a5e-a747-b2ca3125cc5a",
   "metadata": {},
   "source": [
    "***\n",
    "Once a tuning job is complete, we need to obtain a description of the best training job. While the objective metric for the hyperparameter tuner is easily obtained via the `HyperparameterTuner.analytics()` method, which returns a `HyperparameterTuningJobAnalytics` object, the additional auxiliary metrics provided to the original estimator are not extracted with this object. The following function will probe the training job description in order to extract all metrics of interest to this benchmarking scenario from the key `FinalMetricDataList`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb86293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics_from_logs(\n",
    "    tuner: sagemaker.tuner.HyperparameterTuner,\n",
    "    job_information: JobInformation,\n",
    "    session: sagemaker.Session = SM_SESSION\n",
    ") -> Dict[str, Any]:\n",
    "    description = session.describe_training_job(tuner.best_training_job())\n",
    "    metrics = {metric['MetricName']:  metric['Value'] for metric in description['FinalMetricDataList']}\n",
    "    return {**metrics, **job_information._asdict()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5bbfff",
   "metadata": {},
   "source": [
    "***\n",
    "Next, we need to define a function that runs a single tuning job. For a given `model_id`, this function will do three things: 1) obtain a `HyperparameterTuner` object for this model, 2) launch the hyperparameter tuning job and wait for the job to complete, and 3) extract the relevant metrics from CloudWatch logs for this training job. Additionally, this function requests access to the `queue_currently_running` queue, which has a maximum capacity and will block without timeout until there is an available spot on the queue. This allows us to cap the number of sumultaneously running hyperparameter tuning jobs.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tuner(model_name: str, dataset_name: str, unique_id: int) -> Dict[str, Any]:\n",
    "    queue_currently_running.put(None)\n",
    "    tuner, job_information = create_or_attach_tuner(model_name, dataset_name, unique_id)\n",
    "    tuner.wait()\n",
    "    metrics = extract_metrics_from_logs(tuner, job_information)\n",
    "    print(f\"> Completed SageMaker tuning job, {job_information}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8513488-d922-4807-b2fe-0ccf663d4c8e",
   "metadata": {},
   "source": [
    "***\n",
    "Finally, we need a couple of helper functions to log information to file. The first is intended to be triggered whenever the `create_or_attach_tuner` function puts job information onto the `queue_save_tuning_job` queue. Because we are using multithreading in this example and it is not thread safe to have multiple threads write to file simultaneously, we will have the primary script listening to the futures threads pass job information to be saved to this function. The second helper function here is intended to be called whenever a tuning job completes. It extracts the metrics as the return value of the future and writes a json line to file. It also prints out any exceptions generated by the future without raising an error to allow the remainder of jobs to complete. This prevents a single job failure from preventing any future analyses.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_tuning_job_to_file(\n",
    "    job_information: JobInformation,\n",
    "    file_path: Path = SAVE_TUNING_JOB_NAMES_FILE_PATH\n",
    ") -> None:\n",
    "    with open(file_path, \"a+\") as file:\n",
    "        file.write(f\"{json.dumps(job_information._asdict())}\\n\")\n",
    "    print(f\"> Saved job information to file, {job_information}\")\n",
    "\n",
    "\n",
    "def append_metrics_to_file(\n",
    "    future: cf.Future,\n",
    "    job_information: JobInformation,\n",
    "    file_path: Path = SAVE_METRICS_FILE_PATH\n",
    ") -> None:\n",
    "    try:\n",
    "        metrics = future.result()\n",
    "        with open(file_path, \"a+\") as file:\n",
    "            file.write(f\"{json.dumps(metrics)}\\n\")\n",
    "        print(f\"> Saved metrics to file, {job_information}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"> Exception generated for {job_information}: {exc}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c793bd",
   "metadata": {},
   "source": [
    "## Train models\n",
    "***\n",
    "Everything is now in place to launch training jobs and aggregate performance metrics for the benchmarking evaluation. This notebook makes use of the Python standard library's [concurrent futures](https://docs.python.org/3/library/concurrent.futures.html) module, which is a high-level interface for asynchronously executing callables. The `run_tuner` function will be repetitively executed on a thread pool and the `queue_currently_running` queue will block any threads from launching aditional training instances until the number of currently running tuning jobs is less than `SM_AMT_MAX_PARALLEL_TUNING_JOBS`. Note that this queue would not be necessary if a `ProcessPoolExecutor` was used in place of `ThreadPoolExecutor`, but a process pool cannot share global state and therefore calling the functions `append_tuning_job_to_file` and `append_metrics_to_file` would not be thread safe.\n",
    "\n",
    "Once all jobs are submitted to the executor, this script listens to the futures job pool. Until all jobs are completed, it will perform two tasks: 1) call `append_tuning_job_to_file` with any job information that gets populated into `queue_save_tuning_job`, and 2) call `append_metrics_to_file` for any future that has finished execution.\n",
    "\n",
    "__FINAL NOTE__: Depending on the number of models and datasets defined above, this block may take a long time to run and consume a large number of resources. Please double check your settings!\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eee554",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_METRICS_FILE_PATH.exists():\n",
    "    SAVE_METRICS_FILE_PATH.unlink()\n",
    "\n",
    "queue_save_tuning_job = queue.Queue()\n",
    "queue_currently_running = queue.Queue(maxsize=SM_AMT_MAX_PARALLEL_TUNING_JOBS)\n",
    "\n",
    "jobs = itertools.product(models, datasets)\n",
    "\n",
    "with cf.ThreadPoolExecutor(max_workers=SM_AMT_MAX_PARALLEL_TUNING_JOBS) as executor:\n",
    "    futures_to_job_information = {\n",
    "        executor.submit(run_tuner, model_name, dataset_name, unique_id): JobInformation(model_name, dataset_name)\n",
    "        for unique_id, (model_name, dataset_name) in enumerate(jobs)\n",
    "    }\n",
    "    \n",
    "    while futures_to_job_information:\n",
    "        done, not_done = cf.wait(futures_to_job_information, timeout=5.0, return_when=cf.FIRST_COMPLETED)\n",
    "\n",
    "        while not queue_save_tuning_job.empty():\n",
    "            job_information = queue_save_tuning_job.get()\n",
    "            append_tuning_job_to_file(job_information)\n",
    "\n",
    "        for future in done:\n",
    "            queue_currently_running.get()\n",
    "            job_information_before_execution = futures_to_job_information.pop(future)\n",
    "            append_metrics_to_file(future, job_information_before_execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7caaa52-082e-4fad-bfdb-e74d60abef87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analyze results\n",
    "***\n",
    "At this point, all tuning jobs should have completed execution. Congratulations! Please check the file `SAVE_METRICS_FILE_PATH` to see that each job should have appended a JSON object to a new row in the file. Here, we read the contents of this file into a pandas `DataFrame` to view results in tabular form.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48397986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_name_clean(model_name: str):\n",
    "    model_name = model_name.replace(\"tensorflow-ic-\", \"\")\n",
    "    model_name = model_name.replace(\"imagenet-\", \"\")\n",
    "    model_name = model_name.split(\"-classification\")[0]\n",
    "    return model_name\n",
    "\n",
    "metrics_df = pd.read_json(SAVE_METRICS_FILE_PATH, lines=True)\n",
    "metrics_df[\"model_name\"] = metrics_df[\"model_name\"].apply(model_name_clean)\n",
    "metrics_df[\"model_category\"] = metrics_df[\"model_name\"].apply(lambda x: x.replace(\"tf2-preview-\", \"\").split(\"-\")[0])\n",
    "\n",
    "display(metrics_df.sort_values(by=[\"dataset_name\", \"model_category\", \"model_name\"]).set_index([\"dataset_name\", \"model_category\", \"model_name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057125f2",
   "metadata": {},
   "source": [
    "***\n",
    "With a pandas DataFrame of all performance metrics populated, you can perform whatever analysis is of interest. Here, we show a quick example of how to create a figure illustrating the pareto front tradeoff between validation accuracy and throughput. If using Jupyter Lab, be sure to enable to plotly Jupyter extension for best viewing results.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f912d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from plotly.graph_objs import Figure\n",
    "\n",
    "\n",
    "def benchmarking_figure(\n",
    "    df: pd.DataFrame,\n",
    "    dataset_name: str,\n",
    "    x: str = \"test_throughput\",\n",
    "    y: str = \"test_accuracy\",\n",
    "    title: str = \"SageMaker JumpStart TensorFlow Image Classification Benchmarking\",\n",
    "    model_name: str = \"model_name\",\n",
    "    xaxis_title: str = \"throughput (images per second)\",\n",
    "    yaxis_title: str = \"test accuracy\",\n",
    "    size: str = \"num_params\",\n",
    "    color: str = \"model_category\",\n",
    "    width=800,\n",
    "    height=600\n",
    ") -> Figure:\n",
    "    \n",
    "    df[f\"sqrt_{size}\"] = np.sqrt(df[size])\n",
    "    df = df.sort_values(by=[model_name])\n",
    "    df = df[df[\"dataset_name\"]==dataset_name]\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        color=color,\n",
    "        size=f\"sqrt_{size}\",\n",
    "        title=f\"{title} ({dataset_name})\",\n",
    "        hover_name=model_name,\n",
    "        log_x=True,\n",
    "        width=width,\n",
    "        height=height,\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=xaxis_title,\n",
    "        yaxis_title=yaxis_title,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e998a9-fff5-4bc6-abf5-e079c9e6b5ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataset_name in datasets:\n",
    "    fig = benchmarking_figure(metrics_df, dataset_name)\n",
    "    fig.write_html(f\"jumpstart_tf_ic_benchmarking_pareto_{dataset_name}.html\")\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "sagemaker-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7ff9a9ea3e83809cfa793a4d277db8f955e36396013e18b3fb28ad219da1757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
