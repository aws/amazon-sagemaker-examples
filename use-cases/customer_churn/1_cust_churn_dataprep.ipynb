{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Customer Churn Model for Music Streaming App Users: Date Pre-processing with SageMaker Data Wrangler and Processing Job\n",
    "\n",
    "In this demo, you are going to learn how to use various SageMaker functionalities to build, train, and deploy the model from end to end, including data pre-processing steps like ingestion, cleaning and processing, feature engineering, training and hyperparameter tuning, model explainability, and eventually deploy the model. There are two parts of the demo: in part 1: Prepare Data, you will process the data with the help of Data Wrangler, then create features from the cleaned data. By the end of part 1, you will have a complete feature data set that contains all attributes built for each user, and it is ready for modeling. Then in part 2: Modeling and Reference, you will use the data set built from part 1 to find an optimal model for the use case, then test the model predictability with the test data. To start with Part 2, you can either read in data from the output of your Part 1 results, or use the provided 'data/full_feature_data.csv' as the input for the next steps.\n",
    "\n",
    "\n",
    "For how to set up the SageMaker Studio Notebook environment, please check the [onboarding video]( https://www.youtube.com/watch?v=wiDHCWVrjCU&feature=youtu.be). And for a list of services covered in the use case demo, please check the documentation linked in each section.\n",
    "\n",
    "\n",
    "## Content\n",
    "\n",
    "* [Overview](#Overview)\n",
    "* [Data Selection](#2)\n",
    "* [Ingest Data](#4)\n",
    "* [Data Cleaning and Data Exploration](#5)\n",
    "* [Pre-processing with SageMaker Data Wrangler](#7)\n",
    "* [Feature Engineering with SageMaker Processing](#6)\n",
    "* [Data Splitting](#8)\n",
    "* [Model Selection](#9)\n",
    "* [Training with SageMaker Estimator and Experiment](#10)\n",
    "* [Hyperparameter Tuning with SageMaker Hyperparameter Tuning Job](#11)\n",
    "* [Deploy the model with SageMaker Batch-transform](#12)\n",
    "* [Model Explainability with SageMaker Clarify](#15)\n",
    "* [Optional: Automate your training and model selection with SageMaker Autopilot (Console)](#13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### What is Customer Churn and why is it important for businesses?\n",
    "\n",
    "Customer churn, or customer retention/attrition, means a customer has the tendency to leave and stop paying for a business. It is one of the primary metrics companies want to track to get a sense of their customer satisfaction, especially for a subscription-based business model. The company can track churn rate (defined as the percentage of customers churned during a period) as a health indicator for the business, but we would love to identify the at-risk customers before they churn and offer appropriate treatment to keep them with the business, and this is where machine learning comes into play.\n",
    "\n",
    "### Use Cases for Customer Churn\n",
    "\n",
    "Any subscription-based business would track customer churn as one of the most critical Key Performance Indicators (KPIs). Such companies and industries include Telecom companies (cable, cell phone, internet, etc.), digital subscriptions of media (news, forums, blogposts platforms, etc.), music and video streaming services, and other Software as a Service (SaaS) providers (e-commerce, CRM, Mar-Tech, cloud computing, video conference provider, and visualization and data science tools, etc.)\n",
    "\n",
    "### Define Business problem\n",
    "\n",
    "To start with, here are some common business problems to consider depending on your specific use cases and your focus:\n",
    " * Will this customer churn (cancel the plan, cancel the subscription)?\n",
    " * Will this customer downgrade a pricing plan?\n",
    " * For a subscription business model, will a customer renew his/her subscription?\n",
    "\n",
    "### Machine learning problem formulation\n",
    "\n",
    "#### Classification: will this customer churn?\n",
    "\n",
    "To goal of classification is to identify the at-risk customers and sometimes their unusual behavior, such as: will this customer churn or downgrade their plan? Is there any unusual behavior for a customer? The latter question can be formulated as an anomaly detection problem.\n",
    "\n",
    "#### Time Series: will this customer churn in the next X months? When will this customer churn?\n",
    "\n",
    "You can further explore your users by formulating the problem as a time series one and detect when will the customer churn.\n",
    "\n",
    "### Data Requirements\n",
    "\n",
    "#### Data collection Sources\n",
    "\n",
    "Some most common data sources used to construct a data set for churn analysis are:\n",
    "* Customer Relationship Management platform (CRM), \n",
    "* engagement and usage data (analytics services), \n",
    "* passive feedback (ratings based on your request), and active feedback (customer support request, feedback on social media and review platforms).\n",
    "\n",
    "#### Construct a Data Set for Churn Analysis\n",
    "\n",
    "Most raw data collected from the sources mentioned above are huge and often needs a lot of cleaning and pre-processing. For example, usage data is usually event-based log data and can be more than a few gigabytes every day; you can aggregate the data to user-level daily for further analysis. Feedback and review data are mostly text data, so you would need to clean and pre-process the natural language data to be normalized, machine-readable data. If you are joining multiple data sources (especially from different platforms) together, you would want to make sure all data points are consistent, and the user identity can be matched across different platforms.\n",
    "           \n",
    "#### Challenges with Customer Churn\n",
    "\n",
    "* Business related\n",
    "    * Importance of domain knowledge: this is critical when you start building features for the machine learning model. It is important to understand the business enough to decide which features would trigger retention.\n",
    "* Data issues\n",
    "    * fewer churn data available (imbalanced classes): data for churn analysis is often very imbalanced as most of the customers of a business are happy customers (usually).\n",
    "    * User identity mapping problem: if you are joining data from different platforms (CRM, email, feedback, mobile app, and website usage data), you would want to make sure user A is recognized as the same user across multiple platforms. There are third-party solutions that help you tackle this problem.\n",
    "    * Not collecting the right data for the use case or Lacking enough data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case Study - Music Streaming User Churn Prediction\n",
    "\n",
    "<a id='2'></a>\n",
    "\n",
    "## Data Selection\n",
    "\n",
    "You will use generated music streaming data that is simulated to imitate music streaming user behaviors. The data simulated contains 1100 users and their user behavior for one year (2019/10/28 - 2020/10/28). Data is simulated using the [EventSim](https://github.com/Interana/eventsim) and does not contain any real user data.\n",
    "\n",
    "* Observation window: you will use 1 year of data to generate predictions.\n",
    "* Explanation of fields:\n",
    "    * `ts`: event UNIX timestamp\n",
    "    * `userId`: a randomly assigned unique user id\n",
    "    * `sessionId`: a randomly assigned session id unique to each user\n",
    "    * `page`: event taken by the user, e.g. \"next song\", \"upgrade\", \"cancel\"\n",
    "    * `auth`: whether the user is a logged-in user\n",
    "    * `method`: request method, GET or PUT\n",
    "    * `status`: request status\n",
    "    * `level`: if the user is a free or paid user\n",
    "    * `itemInSession`: event happened in the session\n",
    "    * `location`: location of the user's IP address\n",
    "    * `userAgent`: agent of the user's device\n",
    "    * `lastName`: user's last name\n",
    "    * `firstName`: user's first name\n",
    "    * `registration`: user's time of registration\n",
    "    * `gender`: gender of the user\n",
    "    * `artist`: artist of the song the user is playing at the event\n",
    "    * `song`: song title the user is playing at the event\n",
    "    * `length`: length of the session\n",
    " \n",
    " \n",
    " * the data will be downloaded from Github and contained in an [_Amazon Simple Storage Service_](https://aws.amazon.com/s3/) (Amazon S3) bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this specific use case, you will focus on a solution to predict whether a customer will cancel the subscription. Some possible expansion of the work includes:\n",
    "* predict plan downgrading\n",
    "* when a user will churn\n",
    "* add song attributes (genre, playlist, charts) and user attributes (demographics) to the data\n",
    "* add user feedback and customer service requests to the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Diagram\n",
    "\n",
    "The services covered in the use case and an architecture diagram is shown below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"image/use_case_diagram_v2.png\" width=\"800\"/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The output from Data Wrangler is also provided in the github repo (data/data_wrangler_output.csv).\n",
    "## You can also read the provided csv directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "\n",
    "## Feature engineering with SageMaker Processing Job\n",
    "\n",
    "\n",
    "For user churn analysis, usually, you can consider build features from the following aspects:\n",
    "\n",
    "* Generate base features:\n",
    "     * user behavior features (listening behavior, app behavior).\n",
    "     * customer demographic features.\n",
    "     * customer support features (interactions, ratings, etc.)\n",
    "* Formulate time series as features:\n",
    "    * construct streaming time as time series.\n",
    "    * build features in the different time windows (e.g. total songs listened in the last 7 days, 30 days, 180 days, etc.)\n",
    "   \n",
    "For this use case, after exploring the data and with all the findings you gathered, now is the time to create features used for your model. Since the data set is time series, you can enrich your features by adding a time factor to it: e.g., for the total number of songs listened, you can create features like total songs listened in the last 7 days, last 30 days, last 90 days, last 180 days, etc. The features built for these use cases will be at the user level - each row represents one user, and will include the following:\n",
    "\n",
    "* daily features:\n",
    "     * average_events_weekday (numerical): average number of events per day during weekday\n",
    "     * average_events_weekend (numerical): average number of events per day during the weekend\n",
    "     * num_ads_7d: number of ads in last 7 days\n",
    "     * num_error_7d: total errors encountered in last 7 days\n",
    "     * num_songs_played_7d: total songs played in last 7 days\n",
    "     * num_songs_played_30d: total songs played in last 30 days\n",
    "     * num_songs_played_90d: total songs played in last 90 days\n",
    "* user features:\n",
    "     * num_artists (numerical): number of artists the user has listened to\n",
    "     * num_songs (numerical): number of songs played\n",
    "     * num_ads (numerical): number of ads played\n",
    "     * num_thumbsup (numerical): number of times the user likes a song\n",
    "     * num_thumbsdown (numerical): number of times the user dislikes a song\n",
    "     * num_playlist (numerical): number of times user adds a song to a playlist\n",
    "     * num_addfriend (numerical): number of times user adds a friend\n",
    "     * num_error (numerical): number of times user encountered an error\n",
    "     * user_downgrade (binary): user has downgraded plan\n",
    "     * user_upgrade (binary): user has upgraded plan\n",
    "     * percentage_song: percentage of the user's action is 'NextSong' (only listens to songs) \n",
    "     * percentage_ad: percentage of the user's action is 'Roll Advert'\n",
    "     * repeats_ratio: percentage of total songs that are repeats\n",
    "     * days_since_active: days since the user registered and leave (if the user cancels)\n",
    "* Session features:\n",
    "     * num_sessions: number of total sessions\n",
    "     * avg_time_per_session: average time spent per session\n",
    "     * avg_events_per_session: average number of events per session\n",
    "     * avg_gap_between_session: average time between sessions\n",
    "   \n",
    "The following function will create the processing job with SageMaker Processing, a new Python SDK that lets data scientists and ML engineers easily run preprocessing, postprocessing and model evaluation workloads on Amazon SageMaker. This SDK uses SageMaker’s built-in container for scikit-learn, possibly the most popular library for data set transformation.\n",
    "You can find a complete guide to the SageMaker Processing job in [this blog](https://aws.amazon.com/blogs/aws/amazon-sagemaker-processing-fully-managed-data-processing-and-model-evaluation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pg8000 1.17.0 has requirement scramp==1.2.0, but you'll have scramp 1.2.2 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas=='1.1.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip -uQ install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "s3 = sagemaker_session.boto_session.resource(\"s3\")\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.Session().client(\"sagemaker\")\n",
    "\n",
    "output_path = f\"s3://{bucket}/{prefix}/data/processing/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    #     framework_version='0.20.0',\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE OUTPUT FILE NAME FROM PROCESSING JOB\n",
    "processing_job_output_name = 'processing_job_output.csv'\n",
    "%store processing_job_output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"awswrangler\"])\n",
    "import awswrangler as wr\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dw-output-path\")\n",
    "    parser.add_argument(\"--processing-output-filename\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "\n",
    "    data_s3_uri = args.dw_output_path\n",
    "    output_filename = args.processing_output_filename\n",
    "\n",
    "    #     data_path = os.path.join('/opt/ml/processing/input', dw_output_name)\n",
    "    #     df = pd.read_csv(data_path)\n",
    "    df = wr.s3.read_csv(path=data_s3_uri, dataset=True)\n",
    "    ## convert to time\n",
    "    df[\"date\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\")\n",
    "    df[\"ts_dow\"] = df[\"date\"].dt.weekday\n",
    "    df[\"ts_date_day\"] = df[\"date\"].dt.date\n",
    "    df[\"ts_is_weekday\"] = [1 if x in [0, 1, 2, 3, 4] else 0 for x in df[\"ts_dow\"]]\n",
    "    df[\"registration_ts\"] = pd.to_datetime(df[\"registration\"], unit=\"ms\").dt.date\n",
    "    ## add labels\n",
    "    df[\"churned_event\"] = [1 if x == \"Cancellation Confirmation\" else 0 for x in df[\"page\"]]\n",
    "    df[\"user_churned\"] = df.groupby(\"userId\")[\"churned_event\"].transform(\"max\")\n",
    "\n",
    "    ## convert pages categorical variables to numerical\n",
    "    events_list = [\n",
    "        \"NextSong\",\n",
    "        \"Thumbs Down\",\n",
    "        \"Thumbs Up\",\n",
    "        \"Add to Playlist\",\n",
    "        \"Roll Advert\",\n",
    "        \"Add Friend\",\n",
    "        \"Downgrade\",\n",
    "        \"Upgrade\",\n",
    "        \"Error\",\n",
    "    ]\n",
    "    usage_column_name = []\n",
    "    for event in events_list:\n",
    "        event_name = \"_\".join(event.split()).lower()\n",
    "        usage_column_name.append(event_name)\n",
    "        df[event_name] = [1 if x == event else 0 for x in df[\"page\"]]\n",
    "    ## feature engineering\n",
    "    # average_events_weekday (numerical): average number of events per day during weekday\n",
    "    # average_events_weekend (numerical): average number of events per day during the weekend\n",
    "    base_df = (\n",
    "        df.groupby([\"userId\", \"ts_date_day\", \"ts_is_weekday\"])\n",
    "        .agg({\"page\": \"count\"})\n",
    "        .groupby([\"userId\", \"ts_is_weekday\"])[\"page\"]\n",
    "        .mean()\n",
    "        .unstack(fill_value=0)\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"average_events_weekend\", 1: \"average_events_weekday\"})\n",
    "    )\n",
    "\n",
    "    # num_ads_7d, num_songs_played_7d, num_songs_played_30d, num_songs_played_90d, num_ads_7d, num_error_7d\n",
    "    base_df_daily = (\n",
    "        df.groupby([\"userId\", \"ts_date_day\"])\n",
    "        .agg({\"page\": \"count\", \"nextsong\": \"sum\", \"roll_advert\": \"sum\", \"error\": \"sum\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    feature34 = (\n",
    "        base_df_daily.groupby([\"userId\", \"ts_date_day\"])\n",
    "        .tail(7)\n",
    "        .groupby([\"userId\"])\n",
    "        .agg({\"nextsong\": \"sum\", \"roll_advert\": \"sum\", \"error\": \"sum\"})\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"nextsong\": \"num_songs_played_7d\",\n",
    "                \"roll_advert\": \"num_ads_7d\",\n",
    "                \"error\": \"num_error_7d\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    feature5 = (\n",
    "        base_df_daily.groupby([\"userId\", \"ts_date_day\"])\n",
    "        .tail(30)\n",
    "        .groupby([\"userId\"])\n",
    "        .agg({\"nextsong\": \"sum\"})\n",
    "        .reset_index()\n",
    "        .rename(columns={\"nextsong\": \"num_songs_played_30d\"})\n",
    "    )\n",
    "    feature6 = (\n",
    "        base_df_daily.groupby([\"userId\", \"ts_date_day\"])\n",
    "        .tail(90)\n",
    "        .groupby([\"userId\"])\n",
    "        .agg({\"nextsong\": \"sum\"})\n",
    "        .reset_index()\n",
    "        .rename(columns={\"nextsong\": \"num_songs_played_90d\"})\n",
    "    )\n",
    "    # num_artists, num_songs, num_ads, num_thumbsup, num_thumbsdown, num_playlist, num_addfriend, num_error, user_downgrade,\n",
    "    # user_upgrade, percentage_ad, days_since_active\n",
    "    base_df_user = (\n",
    "        df.groupby([\"userId\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"page\": \"count\",\n",
    "                \"nextsong\": \"sum\",\n",
    "                \"artist\": \"nunique\",\n",
    "                \"song\": \"nunique\",\n",
    "                \"thumbs_down\": \"sum\",\n",
    "                \"thumbs_up\": \"sum\",\n",
    "                \"add_to_playlist\": \"sum\",\n",
    "                \"roll_advert\": \"sum\",\n",
    "                \"add_friend\": \"sum\",\n",
    "                \"downgrade\": \"max\",\n",
    "                \"upgrade\": \"max\",\n",
    "                \"error\": \"sum\",\n",
    "                \"ts_date_day\": \"max\",\n",
    "                \"registration_ts\": \"min\",\n",
    "                \"user_churned\": \"max\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    base_df_user[\"percentage_ad\"] = base_df_user[\"roll_advert\"] / base_df_user[\"page\"]\n",
    "    base_df_user[\"days_since_active\"] = (\n",
    "        base_df_user[\"ts_date_day\"] - base_df_user[\"registration_ts\"]\n",
    "    ).dt.days\n",
    "    # repeats ratio\n",
    "    base_df_user[\"repeats_ratio\"] = 1 - base_df_user[\"song\"] / base_df_user[\"nextsong\"]\n",
    "\n",
    "    # num_sessions, avg_time_per_session, avg_events_per_session,\n",
    "    base_df_session = (\n",
    "        df.groupby([\"userId\", \"sessionId\"])\n",
    "        .agg({\"length\": \"sum\", \"page\": \"count\", \"date\": \"min\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    base_df_session[\"prev_session_ts\"] = base_df_session.groupby([\"userId\"])[\"date\"].shift(1)\n",
    "    base_df_session[\"gap_session\"] = (\n",
    "        base_df_session[\"date\"] - base_df_session[\"prev_session_ts\"]\n",
    "    ).dt.days\n",
    "    user_sessions = (\n",
    "        base_df_session.groupby(\"userId\")\n",
    "        .agg({\"sessionId\": \"count\", \"length\": \"mean\", \"page\": \"mean\", \"gap_session\": \"mean\"})\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"sessionId\": \"num_sessions\",\n",
    "                \"length\": \"avg_time_per_session\",\n",
    "                \"page\": \"avg_events_per_session\",\n",
    "                \"gap_session\": \"avg_gap_between_session\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # merge features together\n",
    "    base_df[\"userId\"] = base_df[\"userId\"].astype(\"int\")\n",
    "    final_feature_df = base_df.merge(feature34, how=\"left\", on=\"userId\")\n",
    "    final_feature_df = final_feature_df.merge(feature5, how=\"left\", on=\"userId\")\n",
    "    final_feature_df = final_feature_df.merge(feature6, how=\"left\", on=\"userId\")\n",
    "    final_feature_df = final_feature_df.merge(user_sessions, how=\"left\", on=\"userId\")\n",
    "    final_feature_df = final_feature_df.merge(base_df_user, how=\"left\", on=\"userId\")\n",
    "\n",
    "    final_feature_df = final_feature_df.fillna(0)\n",
    "    # renaming columns\n",
    "    final_feature_df.columns = [\n",
    "        \"userId\",\n",
    "        \"average_events_weekend\",\n",
    "        \"average_events_weekday\",\n",
    "        \"num_songs_played_7d\",\n",
    "        \"num_ads_7d\",\n",
    "        \"num_error_7d\",\n",
    "        \"num_songs_played_30d\",\n",
    "        \"num_songs_played_90d\",\n",
    "        \"num_sessions\",\n",
    "        \"avg_time_per_session\",\n",
    "        \"avg_events_per_session\",\n",
    "        \"avg_gap_between_session\",\n",
    "        \"num_events\",\n",
    "        \"num_songs\",\n",
    "        \"num_artists\",\n",
    "        \"num_unique_songs\",\n",
    "        \"num_thumbs_down\",\n",
    "        \"num_thumbs_up\",\n",
    "        \"num_add_to_playlist\",\n",
    "        \"num_ads\",\n",
    "        \"num_add_friend\",\n",
    "        \"num_downgrade\",\n",
    "        \"num_upgrade\",\n",
    "        \"num_error\",\n",
    "        \"ts_date_day\",\n",
    "        \"registration_ts\",\n",
    "        \"user_churned\",\n",
    "        \"percentage_ad\",\n",
    "        \"days_since_active\",\n",
    "        \"repeats_ratio\",\n",
    "    ]\n",
    "    # only keep created feature columns\n",
    "    final_feature_df = final_feature_df[\n",
    "        [\n",
    "            \"userId\",\n",
    "            \"user_churned\",\n",
    "            \"average_events_weekend\",\n",
    "            \"average_events_weekday\",\n",
    "            \"num_songs_played_7d\",\n",
    "            \"num_ads_7d\",\n",
    "            \"num_error_7d\",\n",
    "            \"num_songs_played_30d\",\n",
    "            \"num_songs_played_90d\",\n",
    "            \"num_sessions\",\n",
    "            \"avg_time_per_session\",\n",
    "            \"avg_events_per_session\",\n",
    "            \"avg_gap_between_session\",\n",
    "            \"num_events\",\n",
    "            \"num_songs\",\n",
    "            \"num_artists\",\n",
    "            \"num_thumbs_down\",\n",
    "            \"num_thumbs_up\",\n",
    "            \"num_add_to_playlist\",\n",
    "            \"num_ads\",\n",
    "            \"num_add_friend\",\n",
    "            \"num_downgrade\",\n",
    "            \"num_upgrade\",\n",
    "            \"num_error\",\n",
    "            \"percentage_ad\",\n",
    "            \"days_since_active\",\n",
    "            \"repeats_ratio\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    print(\"shape of file to append:\\t\\t{}\".format(final_feature_df.shape))\n",
    "    iter_end_time = time.time()\n",
    "    end_time = time.time()\n",
    "    print(\"minutes elapsed: {}\".format(str((end_time - start_time) / 60)))\n",
    "\n",
    "    final_features_output_path = os.path.join(\"/opt/ml/processing/output\", output_filename)\n",
    "    print(\"Saving processed data to {}\".format(final_features_output_path))\n",
    "    final_feature_df.to_csv(final_features_output_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = processing_output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processing_job_output_path = f\"s3://{bucket}/{prefix}/data/processing\"\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"preprocessing.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"processed_data\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=processing_job_output_path,\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--dw-output-path\",\n",
    "        processing_job_output_path,\n",
    "        \"--processing-output-filename\",\n",
    "        processing_job_output_name,\n",
    "    ],\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_job_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have completed Part1: Prepare the data, and now you should have created the complete feature set that is ready for modeling. You can proceed to Part2: modeling and Reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Modeling and Reference\n",
    "\n",
    "now that you have created the complete feature set, you can start to explore and find a best-working model for your churn use case. By the end of part 2, you will select an algorithm, find the best sets of hyperparameter for the model, examine how well the model performs, and finally find the top influential features.\n",
    "\n",
    "To start with Part 2, you can either read in data from the output of your Part 1 results, or use the provided 'data/full_feature_data.csv' as the input (variable dataframe `processed_data`) for the next steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "\n",
    "### Data Splitting\n",
    "\n",
    "You formulated the use case as a classification problem on user level, so you can randomly split your data from last step into train/validation/test. If you want to predict \"will user X churn in the next Y days\" on per user per day level, you should think about spliting data in chronological order instead of random. \n",
    "\n",
    "You should split the data and make sure that data of both classes exist in your train, validation and test sets, to make sure both classes are represented in your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the output of Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_output_uri = f\"{processing_job_output_path}/{processing_job_output_name}\"\n",
    "processing_job_output_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $processing_job_output_uri ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pd.read_csv(processing_job_output_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: you can also load the processed data from the provided feature set\n",
    "# processed_data = pd.read_csv('./data/full_feature_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>user_churned</th>\n",
       "      <th>average_events_weekend</th>\n",
       "      <th>average_events_weekday</th>\n",
       "      <th>num_songs_played_7d</th>\n",
       "      <th>num_ads_7d</th>\n",
       "      <th>num_error_7d</th>\n",
       "      <th>num_songs_played_30d</th>\n",
       "      <th>num_songs_played_90d</th>\n",
       "      <th>num_sessions</th>\n",
       "      <th>...</th>\n",
       "      <th>num_thumbs_up</th>\n",
       "      <th>num_add_to_playlist</th>\n",
       "      <th>num_ads</th>\n",
       "      <th>num_add_friend</th>\n",
       "      <th>num_downgrade</th>\n",
       "      <th>num_upgrade</th>\n",
       "      <th>num_error</th>\n",
       "      <th>percentage_ad</th>\n",
       "      <th>days_since_active</th>\n",
       "      <th>repeats_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.875</td>\n",
       "      <td>152.608696</td>\n",
       "      <td>8270</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>8270</td>\n",
       "      <td>8270</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>586</td>\n",
       "      <td>280</td>\n",
       "      <td>14</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>359</td>\n",
       "      <td>0.589722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.000</td>\n",
       "      <td>153.333333</td>\n",
       "      <td>952</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>952</td>\n",
       "      <td>952</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>265</td>\n",
       "      <td>0.526261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>197.500</td>\n",
       "      <td>241.750000</td>\n",
       "      <td>7734</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>7734</td>\n",
       "      <td>7734</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>544</td>\n",
       "      <td>206</td>\n",
       "      <td>24</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>66</td>\n",
       "      <td>0.587665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.000</td>\n",
       "      <td>240.888889</td>\n",
       "      <td>2168</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2168</td>\n",
       "      <td>2168</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>48</td>\n",
       "      <td>0.538284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  user_churned  average_events_weekend  average_events_weekday  \\\n",
       "0   11001           0.0                 189.875              152.608696   \n",
       "1   11002           0.0                 141.000              153.333333   \n",
       "2   11003           1.0                 197.500              241.750000   \n",
       "3   11004           1.0                 140.000              240.888889   \n",
       "\n",
       "   num_songs_played_7d  num_ads_7d  num_error_7d  num_songs_played_30d  \\\n",
       "0                 8270          14             2                  8270   \n",
       "1                  952           2             0                   952   \n",
       "2                 7734          24            18                  7734   \n",
       "3                 2168           4             2                  2168   \n",
       "\n",
       "   num_songs_played_90d  num_sessions  ...  num_thumbs_up  \\\n",
       "0                  8270            51  ...            586   \n",
       "1                   952             7  ...             82   \n",
       "2                  7734            37  ...            544   \n",
       "3                  2168             7  ...            136   \n",
       "\n",
       "   num_add_to_playlist  num_ads  num_add_friend  num_downgrade  num_upgrade  \\\n",
       "0                  280       14             162              1            1   \n",
       "1                   32        2              28              1            0   \n",
       "2                  206       24             138              1            1   \n",
       "3                   60        4              18              1            0   \n",
       "\n",
       "   num_error  percentage_ad  days_since_active  repeats_ratio  \n",
       "0          2       0.001392                359       0.589722  \n",
       "1          0       0.001664                265       0.526261  \n",
       "2         18       0.002576                 66       0.587665  \n",
       "3          2       0.001546                 48       0.538284  \n",
       "\n",
       "[4 rows x 27 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data to train/validation/test by 70/20/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = processed_data.sample(frac=1, random_state=1729)\n",
    "grouped_df = data.groupby(\"user_churned\")\n",
    "arr_list = [np.split(g, [int(0.7 * len(g)), int(0.9 * len(g))]) for i, g in grouped_df]\n",
    "\n",
    "train_data = pd.concat([t[0] for t in arr_list])\n",
    "validation_data = pd.concat([t[1] for t in arr_list])\n",
    "test_data = pd.concat([v[2] for v in arr_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, name, header=False):\n",
    "    data = data.drop(columns=[\"userId\"])\n",
    "    data = pd.concat([data[\"user_churned\"], data.drop([\"user_churned\"], axis=1)], axis=1)\n",
    "    data.to_csv(name, header=header, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(train_data, \"data/train_updated.csv\")\n",
    "process_data(validation_data, \"data/validation_updated.csv\")\n",
    "process_data(test_data, \"data/test_updated.csv\")\n",
    "\n",
    "process_data(train_data, \"data/train_w_header.csv\", header=True)\n",
    "process_data(validation_data, \"data/validation_w_header.csv\", header=True)\n",
    "process_data(test_data, \"data/test_w_header.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save splitted data to S3\n",
    "The splitted data is provided in the /data folder. You can also upload the provided files (`data/train_updated.csv`,`data/validation_updated.csv`, `data/test_updated.csv`) and proceed to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "s3_input_train = (\n",
    "    boto3.Session()\n",
    "    .resource(\"s3\")\n",
    "    .Bucket(bucket)\n",
    "    .Object(os.path.join(prefix, \"train/train.csv\"))\n",
    "    .upload_file(\"data/train_updated.csv\")\n",
    ")\n",
    "s3_input_validation = (\n",
    "    boto3.Session()\n",
    "    .resource(\"s3\")\n",
    "    .Bucket(bucket)\n",
    "    .Object(os.path.join(prefix, \"validation/validation.csv\"))\n",
    "    .upload_file(\"data/validation_updated.csv\")\n",
    ")\n",
    "s3_input_validation = (\n",
    "    boto3.Session()\n",
    "    .resource(\"s3\")\n",
    "    .Bucket(bucket)\n",
    "    .Object(os.path.join(prefix, \"test/test_labeled.csv\"))\n",
    "    .upload_file(\"data/test_updated.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "The data used in this notebook is synthetic and does not contain real user data. The results (all the names, emails, IP addresses, and browser information) of this simulation are fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "\n",
    "The data used in this notebook is simulated using the [EventSim](https://github.com/Interana/eventsim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
