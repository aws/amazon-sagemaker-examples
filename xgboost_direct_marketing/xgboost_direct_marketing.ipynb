{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeting Direct Marketing with XGBoost\n",
    "_**Supervised Learning with Gradient Boosted Trees: A Binary Prediction Problem With Unbalanced Classes**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Prepration](#Preparation)\n",
    "1. [Data](#Data)\n",
    "    1. [Exploration](#Exploration)\n",
    "    1. [Transformation](#Transformation)\n",
    "1. [Training](#Training)\n",
    "1. [Evaluation](#Evaluation)\n",
    "1. [Tuning](#Tuning)\n",
    "1. [Extensions](#Extensions)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "Direct marketing, either through mail, email, phone, etc., is a common tactic to acquire customers.  Because resources and a customer's attention is limited, the goal is to only target the subset of prospects who are likely to engage with a specific offer.  Predicting those potential customer's based on readily available information like demographics, past interactions, and environmental factors is a common machine learning problem.\n",
    "\n",
    "This notebook presents an example problem to predict if a customer will enroll for a term deposit at a bank, after one or more phone calls.  The steps include:\n",
    "\n",
    "* Preparing your Amazon SageMaker notebook\n",
    "* Downloading data from the internet into Amazon SageMaker\n",
    "* Investigating and transforming the data so that it can be fed to algorithms\n",
    "* Estimating a model using the Gradient Boosting algorithm\n",
    "* Evaluating the effectiveness of the model\n",
    "* Tuning the model's performance\n",
    "\n",
    "---\n",
    "\n",
    "## Preparation\n",
    "To start, let's setup a few environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-west-2'\n",
    "os.environ['JOBLIB_START_METHOD'] = 'forkserver'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, shell commands can be invoked to install any necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring in the Python libraries that we'll use throughout the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                              # For matrix operations and numerical processing\n",
    "import pandas as pd                             # For munging tabular data\n",
    "import matplotlib.pyplot as plt                 # For charts and visualizations\n",
    "import sklearn as sk                            # For access to a variety of machine learning models\n",
    "import xgboost as xgb                           # For gradient boosted trees algorithm\n",
    "from IPython.display import Image               # For displaying images in the notebook\n",
    "from IPython.display import display             # For displaying outputs in the notebook\n",
    "from scipy.stats import randint as sp_randint   # For sampling in HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data\n",
    "Let's start by downloading a dataset from UCI's ML Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
    "!unzip bank-additional.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets read this into a Pandas data frame and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./bank-additional/bank-additional-full.csv', sep=';')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 20)         # Keep the output on one page\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the data.  At a high level, we can see:\n",
    "\n",
    "* We have a little over 40K customer records, and 20 features for each customer\n",
    "* The features are mixed; some numeric, some categorical\n",
    "* The data appears to be sorted, at least by `time` and `contact`, maybe more\n",
    "\n",
    "_**Specifics on each of the features:**_\n",
    "\n",
    "*Demographics:*\n",
    "* `age`: Customer's age (numeric)\n",
    "* `job`: Type of job (categorical: 'admin.', 'services', ...)\n",
    "* `marital`: Marital status (categorical: 'married', 'single', ...)\n",
    "* `education`: Level of education (categorical: 'basic.4y', 'high.school', ...)\n",
    "\n",
    "*Past customer events:*\n",
    "* `default`: Has credit in default? (categorical: 'no', 'unknown', ...)\n",
    "* `housing`: Has housing loan? (categorical: 'no', 'yes', ...)\n",
    "* `loan`: Has personal loan? (categorical: 'no', 'yes', ...)\n",
    "\n",
    "*Past direct marketing contacts:*\n",
    "* `contact`: Contact communication type (categorical: 'cellular', 'telephone', ...)\n",
    "* `month`: Last contact month of year (categorical: 'may', 'nov', ...)\n",
    "* `day_of_week`: Last contact day of the week (categorical: 'mon', 'fri', ...)\n",
    "* `duration`: Last contact duration, in seconds (numeric). Important note: If duration = 0 then `y` = 'no'.\n",
    " \n",
    "*Campaign information:*\n",
    "* `campaign`: Number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "* `pdays`: Number of days that passed by after the client was last contacted from a previous campaign (numeric)\n",
    "* `previous`: Number of contacts performed before this campaign and for this client (numeric)\n",
    "* `poutcome`: Outcome of the previous marketing campaign (categorical: 'nonexistent','success', ...)\n",
    "\n",
    "*External environment factors:*\n",
    "* `emp.var.rate`: Employment variation rate - quarterly indicator (numeric)\n",
    "* `cons.price.idx`: Consumer price index - monthly indicator (numeric)\n",
    "* `cons.conf.idx`: Consumer confidence index - monthly indicator (numeric)\n",
    "* `euribor3m`: Euribor 3 month rate - daily indicator (numeric)\n",
    "* `nr.employed`: Number of employees - quarterly indicator (numeric)\n",
    "\n",
    "*Target variable:*\n",
    "* `y`: Has the client subscribed a term deposit? (binary: 'yes','no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "Let's start exploring the data.  First, let's understand how the features are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=data[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "display(data.describe())\n",
    "%matplotlib inline\n",
    "hist = data.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "* Almost 90% of the values for our target variable `y` are \"no\", so most customers did not subscribe to a term deposit.\n",
    "* Many of the predictive features take on values of \"unknown\".  Some are more common than others.  We should think carefully as to what causes a value of \"unknown\" (are these customers non-representative in some way?) and how we that should be handled.\n",
    "  * Even if \"unknown\" is included as it's own distinct category, what does it mean given that, in reality, those observations likely fall within one of the other categories of that feature?\n",
    "* Many of the predictive features have categories with very few observations in them.  If we find a small category to be highly predictive of our target outcome, do we have enough evidence to make a generalization about that?\n",
    "* Contact timing is particularly skewed.  Almost a third in May and less than 1% in December.  What does this mean for predicting our target variable next December?\n",
    "* There are no missing values in our numeric features.  Or missing values have already been imputed.\n",
    "  * `pdays` takes a value near 1000 for almost all customers.  Likely a placeholder value signifying no previous contact.\n",
    "* Several numeric features have a very long tail.  Do we need to handle these few observations with extremely large values differently?\n",
    "* Several numeric features (particularly the macroeconomic ones) occur in distinct buckets.  Should these be treated as categorical?\n",
    "\n",
    "Next, let's look at how our features relate to the target that we are attempting to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    if column != 'y':\n",
    "        display(pd.crosstab(index=data[column], columns=data['y'], normalize='columns'))\n",
    "\n",
    "for column in data.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = data[[column, 'y']].hist(by='y', bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "* Customers who are-- \"blue-collar\", \"married\", \"unknown\" default status, contacted by \"telephone\", and/or in \"may\" are a substantially lower portion of \"yes\" than \"no\" for subscribing.\n",
    "* Distributions for numeric variables are different across \"yes\" and \"no\" subscribing groups, but the relationships may not be straightforward or obvious.\n",
    "\n",
    "Now let's look at how our features relate to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.corr())\n",
    "pd.plotting.scatter_matrix(data, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "* Features vary widely in their relationship with one another.  Some with highly negative correlation, others with highly positive correlation.\n",
    "* Relationships between features is non-linear and discrete in many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "\n",
    "Cleaning up data is part of nearly every machine learning project.  It arguably presents the biggest risk if done incorrectly and is one of the more subjective aspects in the process.  Several common techniques include:\n",
    "\n",
    "* Handling missing values: Some machine learning algorithms are capable of handling missing values, but most would rather not.  Options include:\n",
    " * Removing observations with missing values: This works well if only a very small fraction of observations have incomplete information.\n",
    " * Remove features with missing values: This works well if there are a small number of features which have a large number of missing values.\n",
    " * Imputing missing values: Entire [books](https://www.amazon.com/Flexible-Imputation-Missing-Interdisciplinary-Statistics/dp/1439868247) have been written on this topic, but common choices are replacing the missing value with the mode or mean of that column's non-missing values.\n",
    "* Converting categorical to numeric: The most common method is one hot encoding, which for each feature maps every distinct value of that column to its own feature which takes a value of 1 when the categorical feature is equal to that value, and 0 otherwise.\n",
    "* Oddly distributed data: Although for non-linear models like Gradient Boosted Trees, this has very limited implications, parametric models like regression can produce wildly inaccurate estimates when fed highly skewed data.  In some cases, simply taking the natural log of the features is sufficient to produce more normally distributed data.  In others, bucketing values into discrete ranges is helpful.  These buckets can then be treated as categorical variables and included in the model when one hot encoded.\n",
    "* Handling more complicated data types: Mainpulating images, text, or data at varying grains is left for other notebook templates.\n",
    "\n",
    "Luckily, some of these aspects have already been handled for us, and the algorithm we are showcasing tends to do well at handling sparse or oddly distributed data.  Therefore, let's keep pre-processing simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['no_previous_contact'] = np.where(data['pdays'] == 999, 1, 0)                                 # Indicator variable to capture when pdays takes a value of 999\n",
    "data['not_working'] = np.where(np.in1d(data['job'], ['student', 'retired', 'unemployed']), 1, 0)   # Indicator for individuals not actively employed\n",
    "model_data = pd.get_dummies(data)                                                                  # Convert categorical variables to sets of indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another question to ask yourself before building a model is whether certain features will add value in your final use case.  For example, if your goal is to deliver the best prediction, then will you have access to that data at the moment of prediction?  Knowing it's raining is highly predictive for umbrella sales, but forecasting weather far enough out to plan inventory on umbrellas is probably just as difficult as forecasting umbrella sales without knowledge of the weather.  So, including this in your model may give you a false sense of precision.\n",
    "\n",
    "Following this logic, let's remove the economic features and `duration` from our data as they would need to be forecasted with high precision to use as inputs in future predictions.\n",
    "\n",
    "Even if we were to use values of the economic indicators from the previous quarter, this value is likely not as relevant for prospects contacted early in the next quarter as those contacted later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_data = model_data.drop(['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a model whose primary goal is to predict a target value on new data, it is important to understand overfitting.  Supervised learning models are designed to minimize error between their predictions of the target value and actuals, in the data they are given.  This last part is key, as frequently in their quest for greater accuracy, machine learning models bias themselves toward picking up on minor idiosyncrasies within the data they are shown.  These idiosyncrasies then don't repeat themselves in subsequent data, meaning those predictions can actually be made less accurate, at the expense of more accurate predictions in the training phase.\n",
    "\n",
    "The most common way of preventing this is to build models with the concept that a model shouldn't only be judged on its fit to the data it was trained on, but also on \"new\" data.  There are several different ways of operationalizing this, holdout validation, cross-validation, leave-one-out validation, etc.  For our purposes, we'll simply randomly split the data into 3 uneven groups.  The model will be trained on 70% of data, it will then be evaluated on 20% of data to give us an estimate of the accuracy we hope to have on \"new\" data, and 10% will be held back as a final testing dataset which will be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])   # Randomly sort the data then split out first 70%, second 20%, and last 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, most machine learning packages expect features and the target variable to be provided as separate arguments.  Let's split these apart.  Notice that although repetitive it's easiest to do this after the train|validation|test split rather than before.  This avoids any misalignment issues due to random reordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = train_data.drop(['y_no', 'y_yes'], axis=1)\n",
    "train_y = train_data['y_yes']\n",
    "validation_X = validation_data.drop(['y_no', 'y_yes'], axis=1)\n",
    "validation_y = validation_data['y_yes']\n",
    "test_X = test_data.drop(['y_no', 'y_yes'], axis=1)\n",
    "test_y = test_data['y_yes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training\n",
    "Now we know most of our features have skewed distributions, some are highly correlated with one another, and some appear to have non-linear relationships with our target variable.  Also, for targeting future prospects, good predictive accuracy is preferred to being able to explain why that prospect was targeted.  Taken together, these aspects make gradient boosted trees a good candidate algorithm.\n",
    "\n",
    "There are several intricacies to understanding the algorithm, but at a high level, gradient boosted trees works by combining predictions from many simple models, each of which tries to address the weaknesses of the previous models.  By doing this the collection of simple models can actually outperform large, complex models.  Say we started with a simple decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix the lack of dot here (yum install graphviz maybe)\n",
    "import sklearn.tree\n",
    "tr = sk.tree.DecisionTreeClassifier(max_depth = 2)      # Setup a decision tree classifier with only 2 cuts\n",
    "tr = tr.fit(train_X, train_y)                           # Train that decision tree on our data\n",
    "sk.tree.export_graphviz(tr,\n",
    "                        feature_names=train_X.columns, \n",
    "                        impurity=False, \n",
    "                        proportion=True, \n",
    "                        rounded=True)                    # Output the tree for visualization\n",
    "!dot -Tjpg tree.dot -o tree.png                          # Convert to an image\n",
    "#Image('tree.png')                                        # Display the tree image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that:\n",
    "\n",
    "* If we've contacted the customer about a previous campaign recently, then 62% of them subscribed to a term deposit.  This is great information, but it's really only relevant for 3.8% of our population.  We should also note that continuing to focus on the same subset over and over risks missing out on new, incremental prospects (even if the success rate is lower).\n",
    "* If we have not contacted the customer about a previous campaign and it's not March, then 91% of contacts did not subscribe to a term deposit.  Not contacting the ~95% of the population that meets these criteria in the future could potentially save substantial cost with minimal loss of opportunity.\n",
    "\n",
    "This extremely simple model is reasonably accurate, but if we wanted to increase accuracy further, we could either:\n",
    "\n",
    "1. Continue adding cuts to our above tree.\n",
    "2. Move to a different algorithm.\n",
    "\n",
    "If we continue to add cuts to the above tree, we could continue to understand, within that 95%, what features distinguish the 9% that did subscribe to a term deposit versus the 91% that did not.  Knowing this would potentially allow us to cut a large portion of the prospects out, with even less risk of lost subscribers.  However, doing so also runs the risk that we find ever more obscure subsets of prospects which do not generalize well when making future predictions.  Note that whether a prospect was contacted in March isn't even a customer attribute.  It could be correlated with other attributes though, depending on how data was collected.  Have we already built a model that's too specific?\n",
    "\n",
    "One method that might help us understand this is random forests.  These models build a large number of simple decision trees, each time taking a sample of the observations and features, and then averaging their predictions together.  By doing so we limit the chances that our model has focused in on any one feature or small subset of observations.\n",
    "\n",
    "Let's build a simple random forest, keeping the depth of the tree at 2, but now averaging across 50 trees.  Since 50 trees would be difficult to look at, let's just see if we notice different features show up in our random forest versus our simple decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "rf = sk.ensemble.RandomForestClassifier(n_estimators=50, \n",
    "                                        max_depth=2, \n",
    "                                        random_state=1729)   # Setup a 50 tree random forest each with only 2 cuts\n",
    "rf.fit(train_X, train_y)                                     # Train this forest on our data\n",
    "fig, ax = plt.subplots(figsize=(10, 13))                     # Plot the importance of each feature in predicting our target\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.barh(range(rf.feature_importances_.shape[0]), \n",
    "        rf.feature_importances_, \n",
    "        tick_label=train_X.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the month features don't show up as very import.  This suggests that our initial, single, small tree may have already become overly specific.\n",
    "\n",
    "An alternative algorithm we could use would be gradient boosted trees.  Again, we're combining multiple simple trees to produce better results than a single large complex model.  Unlike random forests with gradient boosting the trees are not independent.  Boosting fits trees sequentially where mistakes from previous trees are given more importance for future trees to classify accurately.  This can improve predictive accuracy substantially.\n",
    "\n",
    "`xgboost` is an extremely popular, open-source package for gradient boosted trees.  It is computationally powerful, fully featured, and has been successfully used in many machine learning competitions.  Let's start with a simple `xgboost` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = xgb.XGBClassifier(max_depth=2,\n",
    "                       learning_rate=0.3,\n",
    "                       n_estimators=50,\n",
    "                       objective='binary:logistic')   # Setup xgboost model\n",
    "bt.fit(train_X, train_y, \n",
    "       eval_set=[(validation_X, validation_y)], \n",
    "       verbose=False)                                 # Train it to our data\n",
    "fig, ax = plt.subplots(figsize=(8, 8))                # Plot feature importance\n",
    "xgb.plot_importance(bt, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting finds a very different set of features which are predictive.  Let's compare our models based on predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation\n",
    "There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values.  In this case, we're simply predicting whether the customer subscribed to a term deposit (`1`) or not (`0`), which produces a simple confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.crosstab(test_y, tr.predict(test_X), colnames=['tree']))               # Tree model predictions\n",
    "display(pd.crosstab(test_y, rf.predict(test_X), colnames=['forest']))             # Random forest predictions\n",
    "display(pd.crosstab(test_y, bt.predict(test_X), colnames=['boosted']))            # xgboost predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above, none of the models are doing particularly well.  Boosted trees have the most true positives with 85, but this is only slightly more than the 78 from our exceptionally simple decision tree.  Random forests have the fewest false positives (none) but also the most false negatives.  A key point is that our confusion matrices are comparing binary prediction accuracy (i.e. we only predict the prospect will be a subscriber if the model thinks the probability is at or above 50%).\n",
    "\n",
    "But, because there's most likely a high value on acquiring a new subscriber, we're probably willing to tolerate multiple rejections to get one.  So, we're much happier to have a false positive than a false negative (within reason).  How much more we should favor false negatives than false positives will need to be a business decision based on the economic return of acquiring a subscriber versus the cost of a call.\n",
    "\n",
    "Let's address this by comparing how very similar models can perform very differently with a fixed threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning\n",
    "Machine learning algorithms frequently come with many knobs that can be tweaked and tuned.  Adjusting these hyperparameters can be handled several different ways.  Tuning them by hand using a high degree of subjectivity or through extensive trial and error used to be a common task for data scientists.\n",
    "\n",
    "A hasty decision we made early on in our models was keeping the depth of our trees at 2.  We started with this for ease of implementation and interpretation, but as we look to improve performance, we can tune this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf4 = sk.ensemble.RandomForestClassifier(n_estimators=50, \n",
    "                                         max_depth=4, \n",
    "                                         random_state=1729)                         # 4 deep tree random forest instead of 2\n",
    "rf4.fit(train_X, train_y)\n",
    "\n",
    "bt4 = xgb.XGBClassifier(max_depth=4,\n",
    "                        learning_rate=0.3,\n",
    "                        n_estimators=50,\n",
    "                        objective='binary:logistic')                                # Setup 4 deep tree xgboost model\n",
    "bt4.fit(train_X, train_y, \n",
    "        eval_set=[(validation_X, validation_y)], \n",
    "        verbose=False)\n",
    "\n",
    "display(pd.crosstab(test_y, rf4.predict(test_X), colnames=['forest4']))\n",
    "display(pd.crosstab(test_y, bt4.predict(test_X), colnames=['boosted4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, changing one hyperparameter (tree depth) produces substantially different results for random forests, but only a slightly increase in true positives for `xgboost`.\n",
    "\n",
    "The driver for the substantial shift in random forests predictions is subtle issue of thresholds that we mentioned above.  Instead of looking at predictions as a hard binary class, let's look at the probability predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rf.predict_proba(test_X)[:, 1], range=(0, 1))\n",
    "plt.title('max_depth = 2 forest')\n",
    "plt.show()\n",
    "plt.hist(rf4.predict_proba(test_X)[:, 1], range=(0, 1))\n",
    "plt.title('max_depth = 4 forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in both cases, the distribution of prospects is heavily skewed toward a low predicted probability of subscribing (less than 20%).  Both models produce a distinct second, smaller group of prospects with a higher predicted probability, but notice that the forest with trees with 4 cuts has more prospects above the 0.5 threshold.  This drives the substantial shift in true positives that we see above.\n",
    "\n",
    "Let's look at how our false positive and false negative rate changes as we vary our threshold from something other than 0.5.  The typical way of doing this is a ROC curve, which measures the true positive and false positive rates across the range of possible thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_false_pos, tr_true_pos, _ = sk.metrics.roc_curve(test_y, tr.predict_proba(test_X)[:, 1])      # ROC metrics for decision tree\n",
    "rf_false_pos, rf_true_pos, _ = sk.metrics.roc_curve(test_y, rf.predict_proba(test_X)[:, 1])      # ROC metrics for random forest\n",
    "rf4_false_pos, rf4_true_pos, _ = sk.metrics.roc_curve(test_y, rf4.predict_proba(test_X)[:, 1])   # ROC metrics for random forest (4 deep)\n",
    "bt_false_pos, bt_true_pos, _ = sk.metrics.roc_curve(test_y, bt.predict_proba(test_X)[:, 1])      # ROC metrics for xgboost\n",
    "bt4_false_pos, bt4_true_pos, _ = sk.metrics.roc_curve(test_y, bt4.predict_proba(test_X)[:, 1])   # ROC metrics for xgboost (4 deep)\n",
    "\n",
    "plt.plot(tr_false_pos, tr_true_pos, label='tree')                                                # Plot ROC curves for all\n",
    "plt.plot(rf_false_pos, rf_true_pos, label='forest')\n",
    "plt.plot(rf4_false_pos, rf4_true_pos, label='forest4')\n",
    "plt.plot(bt_false_pos, bt_true_pos, label='boosted')\n",
    "plt.plot(bt4_false_pos, bt4_true_pos, label='boosted4')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, by adjusting the threshold, we can bring in more true positives at the expense of more false positives.  This also shows that, as hoped, random forests and gradient boosted trees can outperform our simple decision tree.  In addition, although the two random forest models had very different performance as judged by confusion matrices, they perform relatively similarly overall.\n",
    "\n",
    "Note though, that none of the models is giving us spectacular results; in order to get 90%+ true positives, we'd need to exceed 80% false positives.  Given that we'd like to improve model performance, but hand tuning is laborious and didn't yield substantial improvements (at least in our first attempt), let's take a more systematic approach which takes advantage of the compute resources we have at our disposal.\n",
    "\n",
    "Grid search is a naive method of hyperparameter optimization, but is still quite common.  This sets hyperparameters to different values across a fixed interval you specify, then runs the model at each combination, collects the feedback, and returns the best performing model.\n",
    "\n",
    "Randomized search is similar, but doesn't test all points in a grid and therefore is a bit easier to place an upper bound on total computational expenditure.  Let's keep things simple and explore random search across a variety of parameters in our gradient boosted tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "bthpo = xgb.XGBClassifier(objective='binary:logistic', seed=1729)\n",
    "\n",
    "hp = {'max_depth': [2, 5, 10],\n",
    "      'learning_rate': [0.01, 0.1, 0.3],\n",
    "      'n_estimators': [50, 150],\n",
    "      'scale_pos_weight': [1, 2, 3],\n",
    "      'subsample': [0.8, 1],\n",
    "      'colsample_bytree': [0.6, 1],\n",
    "      'reg_lambda': [1, 10]}\n",
    "\n",
    "btrs = sk.model_selection.RandomizedSearchCV(bthpo,\n",
    "                                             param_distributions=hp,\n",
    "                                             n_iter=100,\n",
    "                                             scoring='roc_auc',\n",
    "                                             n_jobs=4,\n",
    "                                             cv=2,\n",
    "                                             verbose=2,\n",
    "                                             random_state=1729)\n",
    "btrs.fit(pd.concat([train_X, validation_X]), pd.concat([train_y, validation_y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(btrs.cv_results_['mean_test_score'])\n",
    "plt.title('Overall accuracy for 100 models in search space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that depending on the hyperparameters, there can be a fairly substantial swing in our overall accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "btrs_proba = btrs.predict_proba(test_X)[:, 1]\n",
    "print(np.sum(np.where(btrs_proba > np.median(btrs_proba), 1, 0) * test_y) / np.sum(test_y))\n",
    "display(pd.crosstab(test_y, btrs.predict(test_X), colnames=['tuned']))                              # Confusion matrix for tuned model\n",
    "btrs_false_pos, btrs_true_pos, _ = sk.metrics.roc_curve(test_y, btrs.predict_proba(test_X)[:, 1])   # Plot ROC curve for tuned model\n",
    "plt.plot(bt_false_pos, bt_true_pos, label='boosted')\n",
    "plt.plot(btrs_false_pos, btrs_true_pos, label='tuned')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see a shift in our confusion matrix.  The ROC curve tells a slightly less prommising story as accuracy relative to the original gradient boosted tree model is only slightly better.  Our initial naive parameters turned out to be reasonable for this task.  Nevertheless, at large scale, small improvements in overall accuracy can produce substantial impact for a business.  Critically, the overall ROC curve is less important as upon implementation, a fixed threshold needs to be fixed.  And that threshold will be based on specific costs and returns within the business.\n",
    "\n",
    "Overall, it's important to note that, with minimal effort, our model produced accuracies similar to those published in this [paper](http://media.salford-systems.com/video/tutorial/2015/targeted_marketing.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extensions\n",
    "\n",
    "This example was contained within the Notebook environment entirely.  As data sizes grow, utilizing other Amazon SageMaker features such as distributed, serverless training and our hyperparameter optimization service makes more sense.  In addition, if the model needs to be used to provide real-time, online predictions, Amazon SageMakers's auto-scaling hosting should be used.  Please check out the other Amazon SageMaker direct marketing notebook for a more functionally detailed walkthrough of those features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
