{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Experiment Trials for Distributed Training of Mask-RCNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook is a step-by-step tutorial on Amazon SageMaker Experiment Trials for distributed training of [Mask R-CNN](https://arxiv.org/abs/1703.06870) implemented in [TensorFlow](https://www.tensorflow.org/) framework. \n",
    "\n",
    "Concretely, we will describe the steps for SageMaker Experiment Trials for training [TensorPack Faster-RCNN/Mask-RCNN](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN) and [AWS Samples Mask R-CNN](https://github.com/aws-samples/mask-rcnn-tensorflow) in [Amazon SageMaker](https://aws.amazon.com/sagemaker/) using [Amazon S3](https://aws.amazon.com/s3/) and [Amazon EFS](https://aws.amazon.com/s3/) as data sources.\n",
    "\n",
    "The outline of steps is as follows:\n",
    "\n",
    "1. Stage COCO 2017 dataset on [Amazon S3](https://aws.amazon.com/s3/). \n",
    "2. Stage COCO 2017 dataset data in [Amazon EFS](https://aws.amazon.com/s3/), if EFS is attached to the notebook.\n",
    "2. Build SageMaker training image and push it to [Amazon ECR](https://aws.amazon.com/ecr/)\n",
    "3. Configure data input channels\n",
    "4. Configure hyper-prarameters\n",
    "5. Define training metrics\n",
    "6. Define training job \n",
    "7. Define SageMaker Experiment Trials to start the training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SageMaker Session\n",
    "\n",
    "First, let us specify the ```s3_bucket``` that we will use throughout the notebook. The ```s3_bucket``` must be located in the region of this notebook instance. If you do not specify you S3 bucket name in `s3_bucket`, default SageMaker bucket is used, if it exists. We will also initialize the SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow.estimator import TensorFlow\n",
    "\n",
    "s3_bucket  = None # your-s3-bucket-name\n",
    "\n",
    "role = get_execution_role() # you may provide a pre-existing role ARN here\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "aws_region = session.region_name\n",
    "print(f\"AWS Region: {aws_region}\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "if s3_bucket is None:\n",
    "    s3_bucket = sagemaker_session.default_bucket()\n",
    "    \n",
    "print(f\"Using S3 bucket: {s3_bucket}\")\n",
    "\n",
    "try:\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.get_bucket_location(Bucket=s3_bucket)\n",
    "    bucket_region = response['LocationConstraint']\n",
    "    bucket_region = 'us-east-1' if bucket_region is None else bucket_region\n",
    "    \n",
    "    print(f\"Bucket region: {bucket_region}\")\n",
    "except:\n",
    "    print(f\"Access Error: Check if '{s3_bucket}' S3 bucket is in '{aws_region}' region\")\n",
    "    \n",
    "sts = boto3.client(\"sts\")\n",
    "aws_account_id = sts.get_caller_identity()[\"Account\"]\n",
    "\n",
    "print(f\"Account: {aws_account_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Attached EFS File-system\n",
    "\n",
    "We check to see if an EFS file-system is attached to this notebook. If an EFS file-system is attached to this notebook, we use the attached EFS file-system for data input, otherwise, we use Amazon S3.\n",
    "\n",
    "**Note:**\n",
    "If you created this notebook instance using the [stack-sm.sh](stack-sm.sh) script in this repository, an EFS file-system is automatically attached to this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "notebook_attached_efs=!df -kh | grep 'fs-' | sed 's/\\(fs-[0-9a-z]*\\)\\.efs\\..*/\\1/'\n",
    "\n",
    "efs_enabled = False\n",
    "if notebook_attached_efs and re.match(r'fs-[0-9a-z]+', notebook_attached_efs[0]):\n",
    "    efs_enabled=True\n",
    "    print(f\"SageMaker notebook has attached EFS: {notebook_attached_efs}\")\n",
    "else:\n",
    "    print(\"No EFS file-system is attached to this notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage COCO 2017 dataset on Amazon S3\n",
    "\n",
    "We use [COCO 2017](http://cocodataset.org/#home) dataset. This step downloads COCO 2017 training and validation dataset to this notebook instance, extracts the files from the dataset, and uploads the extracted files to your Amazon S3 bucket. Expected time to execute this step is 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sys, os, subprocess\n",
    "\n",
    "key=\"mask-rcnn/sagemaker/input/train/pretrained-models/ImageNet-R50-AlignPadding.npz\"\n",
    "response = None\n",
    "\n",
    "try:\n",
    "    response = s3_client.head_object(Bucket=s3_bucket, Key=key)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "file_size = response.get('ContentLength', 0) if response else 0\n",
    "\n",
    "if file_size == 0:\n",
    "    print(f\"Uploading data to s3://{s3_bucket}/mask-rcnnS/sagemaker/input/train/\")\n",
    "    print(f\"Estimated time: 30 minutes\")\n",
    "    subprocess.check_call(['./prepare-s3-bucket.sh', s3_bucket], \n",
    "                          stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n",
    "    print(f\"Uploaded data to s3://{s3_bucket}/mask-rcnn/sagemaker/input/train/\")\n",
    "else:\n",
    "    print(f\"Data already available in {s3_bucket} bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage COCO 2017 dataset on Amazon EFS\n",
    "\n",
    "Next, we stage [COCO 2017](http://cocodataset.org/#home) dataset on Amazon EFS, if EFS file-system is attached. The [prepare-efs.sh](prepare-efs.sh) script executes this step. The expected time to execute this step is 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import sys, os, subprocess\n",
    "\n",
    "if efs_enabled:\n",
    "    # Specify relative directory path for input data on the EFS file system.\n",
    "    file_system_directory_path = \"mask-rcnn/sagemaker/input/train\"\n",
    "    print(f\"EFS file-system data input path: {file_system_directory_path}\")\n",
    "    train_path = os.path.join(os.getenv('HOME'), 'efs', file_system_directory_path)\n",
    "    \n",
    "    if not os.path.exists(train_path):\n",
    "        print(f\"Staging data on efs file-system: {train_path}\")\n",
    "        subprocess.check_call(['./prepare-efs.sh', s3_bucket], \n",
    "                              stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n",
    "    else:\n",
    "        print(f\"Data already available in efs file-system: {train_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Model Type\n",
    "\n",
    "We have a choice of two different models:\n",
    "\n",
    "1. [TensorPack Faster-RCNN/Mask-RCNN](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN) implementation supports a maximum per-GPU batch size of 1.\n",
    "\n",
    "2. [AWS Samples Mask R-CNN](https://github.com/aws-samples/mask-rcnn-tensorflow) is an optimized implementation that supports a maximum per GPU batch size of 4, assuming per GPU memory of 32 GB.\n",
    "\n",
    "Below, set the `model_type` to `\"aws-samples-mask-rcnn\"`, or `\"tensorpack-mask-rcnn\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the model type you want to use\n",
    "model_type = \"tensorpack-mask-rcnn\" # \"aws-samples-mask-rcnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and push SageMaker Training Image to ECR\n",
    "\n",
    "Next, we build and push the training image to Amazon ECR, based on the selected model type. This may take several minutes on first-time build on this notebook. We also set the `training_script` based on the selected model type.\n",
    "\n",
    "**Note:**\n",
    "For this step, the [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) attached to this notebook instance needs full access to Amazon ECR service. If you created this notebook instance using the [stack-sm.sh](stack-sm.sh) script, the IAM Role attached to this notebook instance is already setup with full access to ECR service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import sys, os, subprocess\n",
    "\n",
    "with open(\"training-image-build.log\", \"w\") as logfile:\n",
    "    if \"tensorpack\" in model_type:\n",
    "        print(\"Building and pushing Tensorpack Faster-RCNN/Mask-RCNN docker image to ECR\")\n",
    "        subprocess.check_call(['./container-script-mode/build_tools/build_and_push.sh', \n",
    "                               aws_region], stdout=logfile, stderr=subprocess.STDOUT)\n",
    "        \n",
    "        image_tag = !cat ./container-script-mode/build_tools/set_env.sh \\\n",
    "            | grep 'IMAGE_TAG' | sed 's/.*IMAGE_TAG=\\(.*\\)/\\1/'\n",
    "        \n",
    "        image_name=\"mask-rcnn-tensorpack-sagemaker-script-mode\"\n",
    "        full_name=f\"{aws_account_id}.dkr.ecr.{aws_region}.amazonaws.com/{image_name}\"\n",
    "        tensorpack_image = f\"{full_name}:{image_tag[0]}\"\n",
    "        training_image = tensorpack_image\n",
    "        training_script= \"tensorpack-mask-rcnn.py\"\n",
    "\n",
    "    else:\n",
    "        print(\"Building and pushing AWS Samples Mask R-CNN docker image to ECR\")\n",
    "        subprocess.check_call(['./container-optimized-script-mode/build_tools/build_and_push.sh',\n",
    "                               aws_region], stdout=logfile, stderr=subprocess.STDOUT)\n",
    "        \n",
    "        image_tag = !cat ./container-optimized-script-mode/build_tools/set_env.sh \\\n",
    "            | grep 'IMAGE_TAG' | sed 's/.*IMAGE_TAG=\\(.*\\)/\\1/'\n",
    "        \n",
    "        image_name=\"mask-rcnn-tensorflow-sagemaker-script-mode\"\n",
    "        full_name=f\"{aws_account_id}.dkr.ecr.{aws_region}.amazonaws.com/{image_name}\"\n",
    "        aws_samples_image = f\"{full_name}:{image_tag[0]}\"\n",
    "       \n",
    "        training_image = aws_samples_image\n",
    "        training_script= \"aws-mask-rcnn.py\" \n",
    "\n",
    "print(f\"Training Image: {training_image}\")\n",
    "print(f\"Training Script: {training_script}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Data Channels\n",
    "\n",
    "We define `train` data channels for Amazon S3, and Amazon EFS. For any given training job, you need to either use Amazon S3 `train` data channel, or use Amazon EFS `train` data channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define S3 Train Data Channel\n",
    "\n",
    "We first define S3 `train` data channel below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "prefix = \"mask-rcnn/sagemaker\"  # prefix in your S3 bucket\n",
    "\n",
    "s3train = f\"s3://{s3_bucket}/{prefix}/input/train\"\n",
    "train_input = TrainingInput(\n",
    "    s3_data=s3train, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\", input_mode=\"File\"\n",
    ")\n",
    "\n",
    "s3_data_channels = {\"train\": train_input}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Amazon EFS Train Data Channel \n",
    "\n",
    "Next, we define the *train* data channel using EFS file-system, if Amazon EFS file-system attached to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "if efs_enabled:\n",
    "    # Specify EFS file system id.\n",
    "    file_system_id = notebook_attached_efs[0]\n",
    "    print(f\"EFS file-system-id: {file_system_id}\")\n",
    "\n",
    "    # Specify the access mode of the mount of the directory associated with the file system.\n",
    "    # Directory must be mounted  'ro'(read-only).\n",
    "    file_system_access_mode = \"ro\"\n",
    "\n",
    "    # Specify your file system type\n",
    "    file_system_type = \"EFS\"\n",
    "\n",
    "    train = FileSystemInput(\n",
    "        file_system_id=file_system_id,\n",
    "        file_system_type=file_system_type,\n",
    "        directory_path=f\"/{file_system_directory_path}\",\n",
    "        file_system_access_mode=file_system_access_mode,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Output Location\n",
    "\n",
    "Next, we define the model output location in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"mask-rcnn/sagemaker\"  # prefix in your bucket\n",
    "s3_output_location = f\"s3://{s3_bucket}/{prefix}/output\"\n",
    "print(f\"Model output location: {s3_output_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Security Group and Subnets\n",
    "\n",
    "If an EFS file-system is attached to this notebook, we retrieve the security groups and subnets associated with the EFS file-system mount-targets, and use them in defining the training job.\n",
    "\n",
    "**Note:**\n",
    "For this step, the IAM Role attached to this notebook instance needs permission to describe EFS mount targets, and mount target security groups. If you created this notebook instance using the [stack-sm.sh](stack-sm.sh) script, the IAM Role attached to this notebook instance is already setup with required permissions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "security_group_ids=None\n",
    "subnets=None\n",
    "\n",
    "if efs_enabled:\n",
    "    file_system_id = notebook_attached_efs[0]\n",
    "    efs_client = boto3.client(\"efs\")\n",
    "    response = efs_client.describe_mount_targets(FileSystemId=file_system_id)\n",
    "    mount_targets = response.get('MountTargets', [])\n",
    "        \n",
    "    for mount_target in mount_targets:\n",
    "        subnet_id = mount_target['SubnetId']\n",
    "        if subnets is None:\n",
    "            subnets = [subnet_id]\n",
    "        else:\n",
    "            subnets.append(subnet_id)\n",
    "        \n",
    "        mt_id = mount_target['MountTargetId']\n",
    "        response = efs_client.describe_mount_target_security_groups(MountTargetId=mt_id)\n",
    "        security_groups = response['SecurityGroups']\n",
    "        if security_group_ids is None:\n",
    "            security_group_ids = security_groups\n",
    "        else:  \n",
    "            security_group_ids.extend(security_groups)\n",
    "    \n",
    "    \n",
    "subnets = list(set(subnets)) if isinstance(subnets, list) else None\n",
    "security_group_ids = list(set(security_group_ids)) if isinstance(security_group_ids, list) \\\n",
    "                        else None\n",
    "\n",
    "print(f\"Subnets: {subnets}\")\n",
    "print(f\"Security groups: {security_group_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Hyper-parameters\n",
    "Next, we define the hyper-parameters. \n",
    "\n",
    "Note, some hyper-parameters are different between the two implementations. The batch size per GPU in TensorPack Faster-RCNN/Mask-RCNN is fixed at 1, but is configurable in AWS Samples Mask-RCNN. The learning rate schedule is specified in units of steps in TensorPack Faster-RCNN/Mask-RCNN, but in epochs in AWS Samples Mask-RCNN.\n",
    "\n",
    "The default learning rate schedule values shown below correspond to training for a total of 24 epochs, at 120,000 images per epoch.\n",
    "\n",
    "### TensorPack Faster-RCNN/Mask-RCNN Hyper-parameters\n",
    "\n",
    "| Hyper-parameter | Description | Default |\n",
    "|-----------|-------------|---------------|\n",
    "| backbone_weights | ResNet backbone pre-trained weights file | 'ImageNet-R50-AlignPadding.npz' |\n",
    "| batch_norm | Batch normalization option ('FreezeBN', 'SyncBN', 'GN', 'None') | 'FreezeBN' |\n",
    "| config: | Any hyper-parameter prefixed with **config:** is set as a model config parameter | - |\n",
    "| data_train | Training data | 'coco_train2017' |\n",
    "| data_val | Validation data | 'coco_val2017' |\n",
    "| eval_period | Number of epochs period for evaluation during training | 1 |\n",
    "| images_per_epoch | Images per epoch | 120000 |\n",
    "| load_model | Pre-trained model to load | - |\n",
    "| lr_schedule | Learning rate schedule in training steps | '[240000, 320000, 360000]' |\n",
    "| mode_fpn | Use Feature Pyramid Network (FPN) mode | True |\n",
    "| mode_mask | Compute masks | True |\n",
    "| resnet_arch | Must be 'resnet50' or 'resnet101' | 'resnet50' |\n",
    "\n",
    "\n",
    "### AWS Samples Mask-RCNN Hyper-parameters\n",
    "\n",
    "| Hyper-parameter | Description | Default |\n",
    "|-----------|-------------|---------------|\n",
    "| backbone_weights | ResNet backbone pre-trained weights file | 'ImageNet-R50-AlignPadding.npz' |\n",
    "| batch_norm | Batch normalization option ('FreezeBN', 'SyncBN', 'GN', 'None') | 'FreezeBN' |\n",
    "| batch_size_per_gpu | Batch size per gpu, 1 - 6 | 16 GB: 2, 32 GB: 4, > 32 GB : 6|\n",
    "| config: | Any hyper-parameter prefixed with **config:** is set as a model config parameter | - |\n",
    "| data_train | Training data | 'train2017' |\n",
    "| data_val | Validation data | 'val2017' |\n",
    "| eval_period | Number of epochs period for evaluation during training | 1 |\n",
    "| lr_schedule | Learning rate schedule in training steps | '[(16, 0.1), (20, 0.01), (24, None)]' |\n",
    "| images_per_epoch | Images per epoch | 120000 |\n",
    "| load_model | Pre-trained model to load | - |\n",
    "| mode_fpn | Use Feature Pyramid Network (FPN) mode. Must be True. | True |\n",
    "| mode_mask | Compute masks | True |\n",
    "| resnet_arch | Must be 'resnet50' or 'resnet101' | 'resnet50' |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"mode_fpn\": \"True\",\n",
    "    \"mode_mask\": \"True\",\n",
    "    \"eval_period\": 1,\n",
    "    \"batch_norm\": \"FreezeBN\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Metrics\n",
    "Next, we define the regular expressions that SageMaker uses to extract algorithm metrics from training logs and send them to [AWS CloudWatch metrics](https://docs.aws.amazon.com/en_pv/AmazonCloudWatch/latest/monitoring/working_with_metrics.html). These algorithm metrics are visualized in SageMaker console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {\"Name\": \"fastrcnn_losses/box_loss\", \"Regex\": \".*fastrcnn_losses/box_loss:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"fastrcnn_losses/label_loss\", \"Regex\": \".*fastrcnn_losses/label_loss:\\\\s*(\\\\S+).*\"},\n",
    "    {\n",
    "        \"Name\": \"fastrcnn_losses/label_metrics/accuracy\",\n",
    "        \"Regex\": \".*fastrcnn_losses/label_metrics/accuracy:\\\\s*(\\\\S+).*\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"fastrcnn_losses/label_metrics/false_negative\",\n",
    "        \"Regex\": \".*fastrcnn_losses/label_metrics/false_negative:\\\\s*(\\\\S+).*\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"fastrcnn_losses/label_metrics/fg_accuracy\",\n",
    "        \"Regex\": \".*fastrcnn_losses/label_metrics/fg_accuracy:\\\\s*(\\\\S+).*\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"fastrcnn_losses/num_fg_label\",\n",
    "        \"Regex\": \".*fastrcnn_losses/num_fg_label:\\\\s*(\\\\S+).*\",\n",
    "    },\n",
    "    {\"Name\": \"maskrcnn_loss/accuracy\", \"Regex\": \".*maskrcnn_loss/accuracy:\\\\s*(\\\\S+).*\"},\n",
    "    {\n",
    "        \"Name\": \"maskrcnn_loss/fg_pixel_ratio\",\n",
    "        \"Regex\": \".*maskrcnn_loss/fg_pixel_ratio:\\\\s*(\\\\S+).*\",\n",
    "    },\n",
    "    {\"Name\": \"maskrcnn_loss/maskrcnn_loss\", \"Regex\": \".*maskrcnn_loss/maskrcnn_loss:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"maskrcnn_loss/pos_accuracy\", \"Regex\": \".*maskrcnn_loss/pos_accuracy:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(bbox)/IoU=0.5\", \"Regex\": \".*mAP\\\\(bbox\\\\)/IoU=0\\\\.5:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(bbox)/IoU=0.5:0.95\", \"Regex\": \".*mAP\\\\(bbox\\\\)/IoU=0\\\\.5:0\\\\.95:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(bbox)/IoU=0.75\", \"Regex\": \".*mAP\\\\(bbox\\\\)/IoU=0\\\\.75:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(bbox)/large\", \"Regex\": \".*mAP\\\\(bbox\\\\)/large:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(bbox)/medium\", \"Regex\": \".*mAP\\\\(bbox\\\\)/medium:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(bbox)/small\", \"Regex\": \".*mAP\\\\(bbox\\\\)/small:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(segm)/IoU=0.5\", \"Regex\": \".*mAP\\\\(segm\\\\)/IoU=0\\\\.5:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(segm)/IoU=0.5:0.95\", \"Regex\": \".*mAP\\\\(segm\\\\)/IoU=0\\\\.5:0\\\\.95:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(segm)/IoU=0.75\", \"Regex\": \".*mAP\\\\(segm\\\\)/IoU=0\\\\.75:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(segm)/large\", \"Regex\": \".*mAP\\\\(segm\\\\)/large:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(segm)/medium\", \"Regex\": \".*mAP\\\\(segm\\\\)/medium:\\\\s*(\\\\S+).*\"},\n",
    "    {\"Name\": \"mAP(segm)/small\", \"Regex\": \".*mAP\\\\(segm\\\\)/small:\\\\s*(\\\\S+).*\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Experiment\n",
    "\n",
    "To define SageMaker Experiment, we first install `sagemaker-experiments` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade pip\n",
    "! pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the SageMaker Experiment modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a `Tracker` for tracking input data used in the SageMaker Trials in this Experiment. Specify the S3 URL of your dataset in the `value` below and change the name of the dataset if you are using a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm = session.client(\"sagemaker\")\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    # we can log the s3 uri to the dataset used for training\n",
    "    tracker.log_input(\n",
    "        name=\"coco-2017-dataset\",\n",
    "        media_type=\"s3/uri\",\n",
    "        value=f\"s3://{s3_bucket}/{prefix}/input/train\",  # specify S3 URL to your dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a SageMaker Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mrcnn_experiment = Experiment.create(\n",
    "    experiment_name=f\"mask-rcnn-experiment-{int(time.time())}\",\n",
    "    description=\"Mask R-CNN experiment\",\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "print(mrcnn_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Experiment Trials\n",
    "\n",
    "Next, we define SageMaker experiment trials for the experiment we just defined. For each experiment trial, we use SageMaker [TensorFlow](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html) API to define a SageMaker Training Job that uses SageMaker script mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Pre-trained Model Weights for Trial\n",
    "\n",
    "For one of the trials in our experiment, we use the ResNet-101 pretrained model weights: [ImageNet-R101-AlignPadding.npz](http://models.tensorpack.com/FasterRCNN/ImageNet-R101-AlignPadding.npz), so we download and stage the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from  tempfile import NamedTemporaryFile\n",
    "import sys, os, subprocess\n",
    "\n",
    "with NamedTemporaryFile(mode=\"w+b\", prefix=\"ImageNet-R101-AlignPadding\", suffix=\".npz\") as file:\n",
    "    print(\"Downloading ImageNet-R101-AlignPadding.npz\")\n",
    "    imagenet_101_url = \"http://models.tensorpack.com/FasterRCNN/ImageNet-R101-AlignPadding.npz\"\n",
    "    urllib.request.urlretrieve(imagenet_101_url, file.name)\n",
    "    \n",
    "    file.seek(0)\n",
    "    print(\"Uploading ImageNet-R101-AlignPadding.npz to S3\")\n",
    "    s3_client.upload_file(file.name, f\"{s3_bucket}\",\n",
    "        \"mask-rcnn/sagemaker/input/train/pretrained-models/ImageNet-R101-AlignPadding.npz\")\n",
    "    \n",
    "    file.seek(0)\n",
    "    if efs_enabled:\n",
    "        print(\"Copying ImageNet-R101-AlignPadding.npz to EFS file-system\")\n",
    "        home = os.getenv('HOME')\n",
    "        dst_path = os.path.join(home, \"efs\", file_system_directory_path,\n",
    "                                \"pretrained-models/ImageNet-R101-AlignPadding.npz\")\n",
    "        subprocess.check_call(['sudo', 'cp', file.name, dst_path])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define SageMaker TensorFlow Estimator for Trials\n",
    "\n",
    "Next, we use SageMaker TensorFlow Estimator API to define a SageMaker Training Job for each SageMaker Trial we need to run within the SageMaker Experiment.\n",
    "\n",
    "We recommend using 16 GPUs for each training job, so we set ```instance_count=2```. We recommend using 100 GB [Amazon EBS](https://aws.amazon.com/ebs/) storage volume with each training instance, so we set ```volume_size = 100```. \n",
    "\n",
    "Next, we will iterate through the Trial parameters and start two trials, one for ResNet architecture `resnet50`, and a second Trial for `resnet101`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial_params = [ ('resnet50', 'ImageNet-R50-AlignPadding.npz'), \n",
    "                ('resnet101', 'ImageNet-R101-AlignPadding.npz')]\n",
    "\n",
    "instance_type = 'ml.p3.16xlarge'  # You may optionally use 'ml.p3dn.24xlarge' or larger instance\n",
    "assert instance_type in ['ml.p3.16xlarge', 'ml.p3dn.24xlarge']\n",
    "\n",
    "if 'aws-samples' in model_type:\n",
    "    hyperparameters['batch_size_per_gpu'] = 2 if instance_type == 'ml.p3.16xlarge' else 4\n",
    "\n",
    "mpi_distribution = None\n",
    "instance_count = 2 # Between 1 - 4\n",
    "if instance_count > 1:\n",
    "    device_min_sys_mem_mb = 2560\n",
    "    custom_mpi_options = f\"--verbose --output-filename /opt/ml/model/logs \\\n",
    "        -x TF_DEVICE_MIN_SYS_MEMORY_IN_MB={device_min_sys_mem_mb}\"\n",
    "    mpi_distribution = {\"mpi\": { \"enabled\": True, \"custom_mpi_options\": custom_mpi_options } }  \n",
    "    \n",
    "training_jobs = []\n",
    "for resnet_arch, backbone_weights in trial_params:\n",
    "    \n",
    "    hyperparameters['resnet_arch'] = resnet_arch\n",
    "    hyperparameters['backbone_weights'] = backbone_weights\n",
    "    \n",
    "    trial_name = f\"mask-rcnn-script-mode-{resnet_arch}-{int(time.time())}\"\n",
    "    mrcnn_trial = Trial.create(\n",
    "                        trial_name=trial_name, \n",
    "                        experiment_name=mrcnn_experiment.experiment_name,\n",
    "                        sagemaker_boto_client=sm,\n",
    "    )\n",
    "    \n",
    "    # associate the proprocessing trial component with the current trial\n",
    "    mrcnn_trial.add_trial_component(tracker.trial_component)\n",
    "    print(mrcnn_trial)\n",
    "        \n",
    "    mask_rcnn_estimator = TensorFlow(image_uri=training_image,\n",
    "                                role=role, \n",
    "                                py_version='py3',\n",
    "                                instance_count=instance_count, \n",
    "                                instance_type=instance_type,\n",
    "                                distribution=mpi_distribution,\n",
    "                                entry_point=training_script,\n",
    "                                volume_size = 100,\n",
    "                                max_run = 400000,\n",
    "                                output_path=s3_output_location,\n",
    "                                sagemaker_session=sagemaker_session, \n",
    "                                hyperparameters = hyperparameters,\n",
    "                                metric_definitions = metric_definitions,\n",
    "                                subnets=subnets,\n",
    "                                security_group_ids=security_group_ids)\n",
    "    \n",
    "    if efs_enabled:\n",
    "        # Specify directory path for log output on the EFS file system.\n",
    "        # You need to provide normalized and absolute path below.\n",
    "        # For example, '/mask-rcnn/sagemaker/output/log'\n",
    "        # Log output directory must not exist\n",
    "        file_system_directory_path = f'/mask-rcnn/sagemaker/output/{mrcnn_trial.trial_name}'\n",
    "        print(f\"EFS log directory:{file_system_directory_path}\")\n",
    "\n",
    "        # Create the log output directory. \n",
    "        # EFS file-system is mounted on '$HOME/efs' mount point for this notebook.\n",
    "        home_dir=os.environ['HOME']\n",
    "        local_efs_path = os.path.join(home_dir,'efs', file_system_directory_path[1:])\n",
    "        print(f\"Creating log directory on EFS: {local_efs_path}\")\n",
    "\n",
    "        assert not os.path.isdir(local_efs_path)\n",
    "        ! sudo mkdir -p -m a=rw {local_efs_path}\n",
    "        assert os.path.isdir(local_efs_path)\n",
    "\n",
    "        # Specify the access mode of the mount of the directory associated with the file system. \n",
    "        # Directory must be mounted 'rw'(read-write).\n",
    "        file_system_access_mode = 'rw'\n",
    "\n",
    "\n",
    "        log = FileSystemInput(file_system_id=file_system_id,\n",
    "                                        file_system_type=file_system_type,\n",
    "                                        directory_path=file_system_directory_path,\n",
    "                                        file_system_access_mode=file_system_access_mode)\n",
    "\n",
    "    data_channels = {'train': train, 'log': log} \\\n",
    "        if (efs_enabled and security_group_ids and subnets) else s3_data_channels\n",
    "\n",
    "    mask_rcnn_estimator.fit(inputs=data_channels, \n",
    "                            job_name=mrcnn_trial.trial_name,\n",
    "                            logs=True,  \n",
    "                            experiment_config={\"TrialName\": mrcnn_trial.trial_name, \n",
    "                                           \"TrialComponentDisplayName\": \"Training\"},\n",
    "                            wait=False)\n",
    "\n",
    "    training_jobs.append(mrcnn_trial.trial_name)\n",
    "    \n",
    "    # sleep in between starting two trials\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Training Jobs are Completed\n",
    "\n",
    "Next we check that the training jobs have completed. We can not analyze the experiment trials until the training jobs have completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "status = 'InProgress'\n",
    "\n",
    "for training_job in training_jobs:\n",
    "    response = client.describe_training_job(TrainingJobName=training_job)\n",
    "    print(response)\n",
    "    status = response['TrainingJobStatus']\n",
    "    if status != 'Completed':\n",
    "        break\n",
    "        \n",
    "if status != 'Completed':\n",
    "    print(f\"Training jobs {training_jobs} have not yet completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Search Expression for Training Metrics\n",
    "\n",
    "Below we define search expression for the training metrics of interest that we want to glean from the trials' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression = {\n",
    "    \"Filters\": [\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": \"Training\",\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"metrics.maskrcnn_loss/accuracy.max\",\n",
    "            \"Operator\": \"LessThan\",\n",
    "            \"Value\": \"1\",\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Trial Data\n",
    "\n",
    "Below we analyze the experiment trial analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "\n",
    "if status == 'Completed':\n",
    "    trial_component_analytics = ExperimentAnalytics(\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        experiment_name=mrcnn_experiment.experiment_name,\n",
    "        search_expression=search_expression,\n",
    "        sort_by=\"metrics.maskrcnn_loss/accuracy.max\",\n",
    "        sort_order=\"Descending\",\n",
    "        parameter_names=[\"resnet_arch\"],\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if status == 'Completed':\n",
    "    analytic_table = trial_component_analytics.dataframe()\n",
    "    for col in analytic_table.columns:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if status == 'Completed':\n",
    "    bbox_map = analytic_table[\n",
    "        [\"resnet_arch\", \"mAP(bbox)/small - Max\", \"mAP(bbox)/medium - Max\", \"mAP(bbox)/large - Max\"]\n",
    "    ]\n",
    "    bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if status == 'Completed':\n",
    "    segm_map = analytic_table[\n",
    "        [\"resnet_arch\", \"mAP(segm)/small - Max\", \"mAP(segm)/medium - Max\", \"mAP(segm)/large - Max\"]\n",
    "    ]\n",
    "    segm_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-experiment-trials.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
