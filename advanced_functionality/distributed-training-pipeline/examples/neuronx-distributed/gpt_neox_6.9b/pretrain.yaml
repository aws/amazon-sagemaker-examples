resources:
  instance_count: 4
  instance_type: ml.trn1.32xlarge 
  volume_size: 200
git:
  repo_url: "https://github.com/aws-neuron/neuronx-distributed.git"
  branch: main
  commit: 8520967d0a39e703e03ba0edbe595230f7e16bde
pre_script: 
  - pip3 install --upgrade pip
  - SCRIPT_DIR=$GIT_CLONE_DIR/examples/training/tp_dp_gpt_neox_hf_pretrain/tp_dp_gpt_neox_6.9b_hf_pretrain
  - cp $GIT_CLONE_DIR/examples/training/tp_dp_gpt_neox_hf_pretrain/tp_dp_gpt_neox_20b_hf_pretrain/modeling_gpt_neox_nxd.py $SCRIPT_DIR/
  - cp $GIT_CLONE_DIR/examples/training/tp_dp_gpt_neox_hf_pretrain/tp_dp_gpt_neox_20b_hf_pretrain/utils.py $SCRIPT_DIR/
  - cp $GIT_CLONE_DIR/examples/training/tp_dp_gpt_neox_hf_pretrain/common/adamw_fp32_optim_params.py $SCRIPT_DIR/
  - cp $GIT_CLONE_DIR/examples/training/tp_dp_gpt_neox_hf_pretrain/common/requirements.txt $SCRIPT_DIR/
  - cd $SCRIPT_DIR
  - pip3 install -r requirements.txt
  - LOGS_DIR=$LOG_ROOT/$PET_NODE_RANK
  - mkdir -p $LOGS_DIR
  - export OUTPUT_LOG=$LOGS_DIR/pretrain.log
  - CACHE_DIR=$CACHE_ROOT/$PET_NODE_RANK
  - mkdir -p $CKPT_ROOT
  - export DATA_PATH="$DATA_ROOT/examples_datasets/wikicorpus_gpt_neox_tokenized_2k"
  - export DISTRIBUTED_ARGS="--nproc_per_node $PET_NPROC_PER_NODE --nnodes $PET_NNODES --node_rank $PET_NODE_RANK --master_addr $PET_MASTER_ADDR --master_port $PET_MASTER_PORT"
  - echo "DISTRIBUTED_ARGS=$DISTRIBUTED_ARGS"
  - TP_DEGREE=8
  - GBS=256
  - MBS=1
  - TOTAL_STEPS=1550
  - WARMUP_STEPS=15 
  - LR=1.2e-4 
  - DP=$(($PET_NPROC_PER_NODE * $PET_NNODES / $TP_DEGREE))
  - ACC_STEPS=$(($GBS / $MBS / $DP)) 
  - STEPS_THIS_RUN=-1
  - export GPT_ARGS="--tensor_parallel_size $TP_DEGREE
    --batch_size $MBS
    --steps_this_run $STEPS_THIS_RUN
    --max_steps $TOTAL_STEPS
    --warmup_steps $WARMUP_STEPS
    --lr $LR
    --grad_accum_usteps $ACC_STEPS"
  - sysctl -w net.ipv4.ip_local_reserved_ports=48620
  - export NEURON_RT_ROOT_COMM_ID=$HOSTNAME:48620
  - echo "NEURON_RT_ROOT_COMM_ID=$NEURON_RT_ROOT_COMM_ID"
  - TMP_CACHE_DIR=$SM_OUTPUT_DATA_DIR/cache
  - cp -r $CACHE_DIR $TMP_CACHE_DIR
  - export NEURON_CC_FLAGS="--model-type=transformer -O1 --enable-saturate-infinity --cache_dir=$TMP_CACHE_DIR"
  - NEURON_COMPILE_CACHE_URL=$TMP_CACHE_DIR neuron_parallel_compile --command clear-locks
train:
  distribution: "torch"
  env:
    - name: HOME
      value: $SM_OUTPUT_DATA_DIR
    - name: LOG_ROOT
      value: $SM_CHANNEL_EFS/home/$RELEASE_NAME/logs
    - name: DATA_ROOT
      value: $SM_CHANNEL_FSX/home/$RELEASE_NAME
    - name: CACHE_ROOT
      value: $SM_CHANNEL_EFS/home/$RELEASE_NAME/cache
    - name: CKPT_ROOT
      value: "$SM_CHANNEL_EFS/home/$RELEASE_NAME/checkpoints"
    - name: CCOM_SOCKET_IFNAME
      value: "eth0"
    - name: FI_EFA_USE_DEVICE_RDMA
      value: "1"
    - name: FI_PROVIDER
      value: "efa"
    - name: FI_EFA_FORK_SAFE
      value: "1"
    - name:  NEURON_FUSE_SOFTMAX
      value: "1"
    - name: XLA_DOWNCAST_BF16
      value: "1"
    - name: NEURON_RT_STOCHASTIC_ROUNDING_EN
      value: "1"
    - name: NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS
      value: "3"
  command:
    -  torchrun 
  args: 
    - $DISTRIBUTED_ARGS  
    - tp_dp_gpt_neox_6.9b_hf_pretrain.py
    - $GPT_ARGS
    - '--data_dir $DATA_PATH'  
    - '--output_dir $CKPT_ROOT'
    - '2>&1 | tee $OUTPUT_LOG'
