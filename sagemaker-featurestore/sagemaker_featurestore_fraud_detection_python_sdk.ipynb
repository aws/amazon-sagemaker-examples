{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection with Amazon SageMaker FeatureStore\n",
    "\n",
    "Kernel `Python 3 (Data Science)` works well with this notebook.\n",
    "\n",
    "The following policies are needed:\n",
    "- AmazonSageMakerFullAccess\n",
    "- AmazonS3FullAccess\n",
    "\n",
    "## Contents\n",
    "1. [Background](#Background)\n",
    "1. [Setup SageMaker FeatureStore](#Setup-SageMaker-FeatureStore)\n",
    "1. [Inspect Dataset](#Inspect-Dataset)\n",
    "1. [Ingest Data into FeatureStore](#Ingest-Data-into-FeatureStore)\n",
    "1. [Build_Training_Dataset](#Build-Training-Dataset)\n",
    "1. [Train_and Deploy_the Model](#Train-and-Deploy-the-Model)\n",
    "1. [SageMaker FeatureStore At Inference](#SageMaker-FeatureStore-During-Inference)\n",
    "1. [Cleanup Resources](#Cleanup-Resources)\n",
    "\n",
    "## Background\n",
    "\n",
    "Amazon SageMaker FeatureStore is a new SageMaker capability that makes it easy for customers to create and manage curated data for machine learning (ML) development. SageMaker FeatureStore enables data ingestion via a high TPS API and data consumption via the online and offline stores. \n",
    "\n",
    "This notebook provides an example for the APIs provided by SageMaker FeatureStore by walking through the process of training a fraud detection model. The notebook demonstrates how the dataset's tables can be ingested into the FeatureStore, queried to create a training dataset, and quickly accessed during inference. \n",
    "\n",
    "\n",
    "### Terminology\n",
    "\n",
    "A **FeatureGroup** is the main resource that contains the metadata for all the data stored in SageMaker FeatureStore. A FeatureGroup contains a list of FeatureDefinitions. A **FeatureDefinition** consists of a name and one of the following data types: a integral, string or decimal. The FeatureGroup also contains an **OnlineStoreConfig** and an **OfflineStoreConfig** controlling where the data is stored. Enabling the online store allows quick access to the latest value for a Record via the GetRecord API. The offline store, a required configuration, allows storage of historical data in your S3 bucket. \n",
    "\n",
    "Once a FeatureGroup is created, data can be added as Records. **Records** can be thought of as a row in a table. Each record will have a unique **RecordIdentifier** along with values for all other FeatureDefinitions in the FeatureGroup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SageMaker FeatureStore\n",
    "\n",
    "Let's start by setting up the SageMaker Python SDK and boto client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: sagemaker in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (2.18.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (3.11.4)\n",
      "Requirement already satisfied, skipping upgrade: boto3>=1.16.27 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.16.29)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=1.4.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf3-to-dict>=0.1.5 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: smdebug-rulesconfig==0.1.5 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker) (45.2.0.post20200210)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.20.0,>=1.19.29 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker) (1.19.29)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.29->boto3>=1.16.27->sagemaker) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.29->boto3>=1.16.27->sagemaker) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "# to get the latest sagemaker python sdk\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 Bucket Setup For The OfflineStore\n",
    "\n",
    "SageMaker FeatureStore writes the data in the OfflineStore of a FeatureGroup to a S3 bucket owned by you. To be able to write to your S3 bucket, SageMaker FeatureStore assumes an IAM role which has access to it. The role is also owned by you.\n",
    "Note that the same bucket can be re-used across FeatureGroups. Data in the bucket is partitioned by FeatureGroup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default s3 bucket name and it will be referenced throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-west-2-688520471316\n"
     ]
    }
   ],
   "source": [
    "# You can modify the following to use a bucket of your choosing\n",
    "default_s3_bucket_name = feature_store_session.default_bucket()\n",
    "prefix = 'sagemaker-featurestore-demo'\n",
    "\n",
    "print(default_s3_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the IAM role. This role gives SageMaker FeatureStore access to your S3 bucket. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> In this example we use the default SageMaker role, assuming it has both <b>AmazonSageMakerFullAccess</b> and <b>AmazonSageMakerFeatureStoreAccess</b> managed policies. If not, please make sure to attach them to the role before proceeding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name RL to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::688520471316:role/RL\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# You can modify the following to use a role of your choosing. See the documentation for how to create this.\n",
    "role = get_execution_role()\n",
    "print (role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Dataset\n",
    "\n",
    "The provided dataset is a synthetic dataset with two tables: identity and transactions. They can both be joined by the `TransactionId` column. The transaction table contains information about a particular transaction such as amount, credit or debit card while the identity table contains information about the user such as device type and browser. The transaction must exist in the transaction table, but might not always be available in the identity table.\n",
    "\n",
    "The objective of the model is to predict if a transaction is fraudulent or not, given the transaction record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "fraud_detection_bucket_name = 'sagemaker-sample-files'\n",
    "identity_file_key = 'datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_identity.csv'\n",
    "transaction_file_key = 'datasets/tabular/fraud_detection/synthethic_fraud_detection_SA/sampled_transactions.csv'\n",
    "\n",
    "identity_data_object = s3_client.get_object(Bucket=fraud_detection_bucket_name, Key=identity_file_key)\n",
    "transaction_data_object = s3_client.get_object(Bucket=fraud_detection_bucket_name, Key=transaction_file_key)\n",
    "\n",
    "identity_data = pd.read_csv(io.BytesIO(identity_data_object['Body'].read()))\n",
    "transaction_data = pd.read_csv(io.BytesIO(transaction_data_object['Body'].read()))\n",
    "\n",
    "identity_data = identity_data.round(5)\n",
    "transaction_data = transaction_data.round(5)\n",
    "\n",
    "identity_data = identity_data.fillna(0)\n",
    "transaction_data = transaction_data.fillna(0)\n",
    "\n",
    "# Feature transformations for this dataset are applied before ingestion into FeatureStore.\n",
    "# One hot encode card4, card6\n",
    "encoded_card_bank = pd.get_dummies(transaction_data['card4'], prefix = 'card_bank')\n",
    "encoded_card_type = pd.get_dummies(transaction_data['card6'], prefix = 'card_type')\n",
    "\n",
    "transformed_transaction_data = pd.concat([transaction_data, encoded_card_type, encoded_card_bank], axis=1)\n",
    "# blank space is not allowed in feature name\n",
    "transformed_transaction_data = transformed_transaction_data.rename(columns={\"card_bank_american express\": \"card_bank_american_express\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>id_01</th>\n",
       "      <th>id_02</th>\n",
       "      <th>id_03</th>\n",
       "      <th>id_04</th>\n",
       "      <th>id_05</th>\n",
       "      <th>id_06</th>\n",
       "      <th>id_07</th>\n",
       "      <th>id_08</th>\n",
       "      <th>id_09</th>\n",
       "      <th>...</th>\n",
       "      <th>id_11</th>\n",
       "      <th>id_12</th>\n",
       "      <th>id_13</th>\n",
       "      <th>id_14</th>\n",
       "      <th>id_15</th>\n",
       "      <th>id_16</th>\n",
       "      <th>id_17</th>\n",
       "      <th>id_18</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2990130</td>\n",
       "      <td>-5</td>\n",
       "      <td>38780.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>80</td>\n",
       "      <td>253</td>\n",
       "      <td>241</td>\n",
       "      <td>260</td>\n",
       "      <td>125</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2990266</td>\n",
       "      <td>-10</td>\n",
       "      <td>69246.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-67</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>122</td>\n",
       "      <td>33</td>\n",
       "      <td>38</td>\n",
       "      <td>60</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2992553</td>\n",
       "      <td>-45</td>\n",
       "      <td>348819.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>143</td>\n",
       "      <td>268</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2994568</td>\n",
       "      <td>-15</td>\n",
       "      <td>337170.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>127</td>\n",
       "      <td>253</td>\n",
       "      <td>202</td>\n",
       "      <td>135</td>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2994749</td>\n",
       "      <td>-5</td>\n",
       "      <td>680670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>257</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>254</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  id_01     id_02  id_03  id_04  id_05  id_06  id_07  id_08  \\\n",
       "0        2990130     -5   38780.0    0.0    0.0    0.0    -70      0      1   \n",
       "1        2990266    -10   69246.0    0.0    0.0    0.0    -67      0      2   \n",
       "2        2992553    -45  348819.0    0.0    0.0    0.0    -73      0      0   \n",
       "3        2994568    -15  337170.0    0.0    0.0    0.0    -10      1      2   \n",
       "4        2994749     -5  680670.0    0.0    0.0    8.0     -1      2      2   \n",
       "\n",
       "   id_09  ...  id_11  id_12  id_13  id_14  id_15  id_16  id_17 id_18 id_19  \\\n",
       "0  100.0  ...     32     80    253    241    260    125      T     F     F   \n",
       "1  100.0  ...     47     47    122     33     38     60      T     F     T   \n",
       "2  100.0  ...     21    143    268    111      2    135      F     F     T   \n",
       "3  100.0  ...     55    127    253    202    135     49      F     F     T   \n",
       "4  100.0  ...     52     43    257      7     19    254      F     F     T   \n",
       "\n",
       "  id_20  \n",
       "0     T  \n",
       "1     F  \n",
       "2     F  \n",
       "3     T  \n",
       "4     T  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>...</th>\n",
       "      <th>N8</th>\n",
       "      <th>N9</th>\n",
       "      <th>card_type_0</th>\n",
       "      <th>card_type_credit</th>\n",
       "      <th>card_type_debit</th>\n",
       "      <th>card_bank_0</th>\n",
       "      <th>card_bank_american_express</th>\n",
       "      <th>card_bank_discover</th>\n",
       "      <th>card_bank_mastercard</th>\n",
       "      <th>card_bank_visa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3343087</td>\n",
       "      <td>0</td>\n",
       "      <td>8810855</td>\n",
       "      <td>29.00</td>\n",
       "      <td>12469</td>\n",
       "      <td>360.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>126.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3307318</td>\n",
       "      <td>0</td>\n",
       "      <td>7955295</td>\n",
       "      <td>107.95</td>\n",
       "      <td>16188</td>\n",
       "      <td>178.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>224.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3555327</td>\n",
       "      <td>0</td>\n",
       "      <td>15084339</td>\n",
       "      <td>159.95</td>\n",
       "      <td>1825</td>\n",
       "      <td>555.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3310736</td>\n",
       "      <td>0</td>\n",
       "      <td>8017157</td>\n",
       "      <td>159.95</td>\n",
       "      <td>10057</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>224.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3034711</td>\n",
       "      <td>0</td>\n",
       "      <td>1127470</td>\n",
       "      <td>117.00</td>\n",
       "      <td>11444</td>\n",
       "      <td>555.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt  card1  card2  card3  \\\n",
       "0        3343087        0        8810855           29.00  12469  360.0  150.0   \n",
       "1        3307318        0        7955295          107.95  16188  178.0  150.0   \n",
       "2        3555327        0       15084339          159.95   1825  555.0  150.0   \n",
       "3        3310736        0        8017157          159.95  10057  225.0  150.0   \n",
       "4        3034711        0        1127470          117.00  11444  555.0  150.0   \n",
       "\n",
       "        card4  card5  card6  ...  N8  N9  card_type_0  card_type_credit  \\\n",
       "0  mastercard  126.0  debit  ...   F   T            0                 0   \n",
       "1  mastercard  224.0  debit  ...   F   T            0                 0   \n",
       "2        visa  226.0  debit  ...   T   F            0                 0   \n",
       "3  mastercard  224.0  debit  ...   F   F            0                 0   \n",
       "4        visa  226.0  debit  ...   F   F            0                 0   \n",
       "\n",
       "   card_type_debit  card_bank_0  card_bank_american_express  \\\n",
       "0                1            0                           0   \n",
       "1                1            0                           0   \n",
       "2                1            0                           0   \n",
       "3                1            0                           0   \n",
       "4                1            0                           0   \n",
       "\n",
       "   card_bank_discover  card_bank_mastercard  card_bank_visa  \n",
       "0                   0                     1               0  \n",
       "1                   0                     1               0  \n",
       "2                   0                     0               1  \n",
       "3                   0                     1               0  \n",
       "4                   0                     0               1  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_transaction_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data into FeatureStore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will create the FeatureGroups representing the transaction and identity tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define FeatureGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "identity_feature_group_name = 'identity-feature-group-' + strftime('%d-%H-%M-%S', gmtime())\n",
    "transaction_feature_group_name = 'transaction-feature-group-' + strftime('%d-%H-%M-%S', gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "identity_feature_group = FeatureGroup(name=identity_feature_group_name, sagemaker_session=feature_store_session)\n",
    "transaction_feature_group = FeatureGroup(name=transaction_feature_group_name, sagemaker_session=feature_store_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "current_time_sec = int(round(time.time()))\n",
    "\n",
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == 'object':\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")\n",
    "\n",
    "# cast object dtype to string. The SageMaker FeatureStore Python SDK will then map the string dtype to String feature type.\n",
    "cast_object_to_string(identity_data)\n",
    "cast_object_to_string(transformed_transaction_data)\n",
    "\n",
    "# record identifier and event time feature names\n",
    "record_identifier_feature_name = \"TransactionID\"\n",
    "event_time_feature_name = \"EventTime\"\n",
    "\n",
    "# append EventTime feature\n",
    "identity_data[event_time_feature_name] = pd.Series([current_time_sec]*len(identity_data), dtype=\"float64\")\n",
    "transformed_transaction_data[event_time_feature_name] = pd.Series([current_time_sec]*len(transaction_data), dtype=\"float64\")\n",
    "\n",
    "# load feature definitions to the feature group. SageMaker FeatureStore Python SDK will auto-detect the data schema based on input data.\n",
    "identity_feature_group.load_feature_definitions(data_frame=identity_data); # output is suppressed\n",
    "transaction_feature_group.load_feature_definitions(data_frame=transformed_transaction_data); # output is suppressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create FeatureGroups in SageMaker FeatureStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "FeatureGroup identity-feature-group-05-02-05-54 successfully created.\n",
      "FeatureGroup transaction-feature-group-05-02-05-54 successfully created.\n"
     ]
    }
   ],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "\n",
    "identity_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "transaction_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group=identity_feature_group)\n",
    "wait_for_feature_group_creation_complete(feature_group=transaction_feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the FeatureGroup has been created by using the DescribeFeatureGroup and ListFeatureGroups APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_feature_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_feature_group.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.list_feature_groups() # use boto client to list FeatureGroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PutRecords into FeatureGroup\n",
    "\n",
    "After the FeatureGroups have been created, we can put data into the FeatureGroups by using the PutRecord API. This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to S3 in chunks. The files will be written to the offline store within a few minutes of ingestion. For this example, to accelerate the ingestion process, we are specifying multiple workers to do the job simultaneously. It will take ~1min to ingest data to the 2 FeatureGroups, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_feature_group.ingest(\n",
    "    data_frame=identity_data, max_workers=3, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_feature_group.ingest(\n",
    "    data_frame=transformed_transaction_data, max_workers=5, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that data has been ingested, we can quickly retrieve a record from the online store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_value = str(2990130)\n",
    "\n",
    "featurestore_runtime.get_record(FeatureGroupName=transaction_feature_group_name, RecordIdentifierValueAsString=record_identifier_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SageMaker Python SDK’s FeatureStore class also provides the functionality to generate Hive DDL commands. Schema of the table is generated based on the feature definitions. Columns are named after feature name and data-type are inferred based on feature type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(identity_feature_group.as_hive_ddl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transaction_feature_group.as_hive_ddl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's wait for the data to appear in our offline store before moving forward to creating a dataset. This will take approximately 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "print(account_id)\n",
    "\n",
    "identity_feature_group_s3_prefix = prefix + '/' + account_id + '/sagemaker/' + region + '/offline-store/' + identity_feature_group_name + '/data'\n",
    "transaction_feature_group_s3_prefix = prefix + '/' + account_id + '/sagemaker/' + region + '/offline-store/' + transaction_feature_group_name + '/data'\n",
    "\n",
    "offline_store_contents = None\n",
    "while (offline_store_contents is None):\n",
    "    objects_in_bucket = s3_client.list_objects(Bucket=default_s3_bucket_name,Prefix=transaction_feature_group_s3_prefix)\n",
    "    if ('Contents' in objects_in_bucket and len(objects_in_bucket['Contents']) > 1):\n",
    "        offline_store_contents = objects_in_bucket['Contents']\n",
    "    else:\n",
    "        print('Waiting for data in offline store...\\n')\n",
    "        sleep(60)\n",
    "    \n",
    "print('Data available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker FeatureStore adds metadata for each record that's ingested into the offline store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Dataset\n",
    "\n",
    "SageMaker FeatureStore automatically builds the Glue Data Catalog for FeatureGroups (you can optionally turn it on/off while creating the FeatureGroup). In this example, we want to create one training dataset with FeatureValues from both identity and transaction FeatureGroups. This is done by utilizing the auto-built Catalog. We run an Athena query that joins the data stored in the offline store in S3 from the 2 FeatureGroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_query = identity_feature_group.athena_query()\n",
    "transaction_query = transaction_feature_group.athena_query()\n",
    "\n",
    "identity_table = identity_query.table_name\n",
    "transaction_table = transaction_query.table_name\n",
    "\n",
    "query_string = 'SELECT * FROM \"'+transaction_table+'\" LEFT JOIN \"'+identity_table+'\" ON \"'+transaction_table+'\".transactionid = \"'+identity_table+'\".transactionid'\n",
    "print('Running ' + query_string)\n",
    "\n",
    "# run Athena query. The output is loaded to a Pandas dataframe.\n",
    "#dataset = pd.DataFrame()\n",
    "identity_query.run(query_string=query_string, output_location='s3://'+default_s3_bucket_name+'/'+prefix+'/query_results/')\n",
    "identity_query.wait()\n",
    "dataset = identity_query.as_dataframe()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare query results for training.\n",
    "query_execution = identity_query.get_query_execution()\n",
    "query_result = 's3://'+default_s3_bucket_name+'/'+prefix+'/query_results/'+query_execution['QueryExecution']['QueryExecutionId']+'.csv'\n",
    "print(query_result)\n",
    "\n",
    "# Select useful columns for training with target column as the first.\n",
    "dataset = dataset[[\"isfraud\", \"transactiondt\", \"transactionamt\", \"card1\", \"card2\", \"card3\", \"card5\", \"card_type_credit\", \"card_type_debit\", \"card_bank_american_express\", \"card_bank_discover\", \"card_bank_mastercard\", \"card_bank_visa\", \"id_01\", \"id_02\", \"id_03\", \"id_04\", \"id_05\"]]\n",
    "\n",
    "# Write to csv in S3 without headers and index column.\n",
    "dataset.to_csv('dataset.csv', header=False, index=False)\n",
    "s3_client.upload_file('dataset.csv', default_s3_bucket_name, prefix+'/training_input/dataset.csv')\n",
    "dataset_uri_prefix = 's3://'+default_s3_bucket_name+'/'+prefix+'/training_input/';\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Deploy the Model\n",
    "\n",
    "Now it's time to launch a Training job to fit our model. We use the gradient boosting algorithm provided by XGBoost libary to fit our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "public_ecr=257758044811.dkr.ecr.us-east-2.amazonaws.com\n",
    "image=sagemaker-xgboost\n",
    "tag=1.0-1-cpu-py3\n",
    "\n",
    "# Add the public ECR for XGBoost image to authenticated registries\n",
    "aws ecr get-login-password --region us-east-2 | \\\n",
    "    docker login --username AWS --password-stdin $public_ecr\n",
    "\n",
    "# Pull the XGBoost image\n",
    "docker pull $public_ecr/$image:$tag\n",
    "\n",
    "# Push the image to your ECR\n",
    "my_region=$(aws configure get region)\n",
    "my_account=$(aws sts get-caller-identity --query Account | tr -d '\"')\n",
    "my_ecr=$my_account.dkr.ecr.$my_region.amazonaws.com \n",
    "\n",
    "# Authenticate your ECR\n",
    "aws ecr get-login-password --region $my_region | \\\n",
    "    docker login --username AWS --password-stdin $my_ecr\n",
    "\n",
    "# Create a repository in your ECR to host the XGBoost image\n",
    "repository_name=sagemaker-xgboost\n",
    "\n",
    "if aws ecr create-repository --repository-name $repository_name ; then\n",
    "    echo \"Repository $repository_name created!\"\n",
    "else\n",
    "    echo \"Repository $repository_name already exists!\"\n",
    "fi\n",
    "\n",
    "# Push the image to your ECR\n",
    "docker tag $public_ecr/$image:$tag $my_ecr/$image:$tag\n",
    "docker push $my_ecr/$image:$tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an `Estimator` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "ecr = '{}.dkr.ecr.{}.amazonaws.com'.format(account_id, region)\n",
    "\n",
    "training_image=ecr + '/' + 'sagemaker-xgboost:1.0-1-cpu-py3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'role' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5c1bdf95e277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m training_model = Estimator(training_image,\n\u001b[0;32m----> 3\u001b[0;31m                            \u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                            \u001b[0minstance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m5.2xlarge'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'role' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "training_model = Estimator(training_image,\n",
    "                           role, \n",
    "                           instance_count=1, \n",
    "                           instance_type='ml.m5.2xlarge',\n",
    "                           volume_size = 5,\n",
    "                           max_run = 3600,\n",
    "                           input_mode= 'File',\n",
    "                           output_path=training_output_path,\n",
    "                           sagemaker_session=feature_store_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.set_hyperparameters(objective = \"binary:logistic\",\n",
    "                                   num_round = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify training dataset, which is the dataset we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.inputs\n",
    "\n",
    "train_data = sagemaker.inputs.TrainingInput(dataset_uri_prefix, distribution='FullyReplicated', \n",
    "                                            content_type='text/csv', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Hosting for the Model\n",
    "\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same instance (or type of instance) that we used to train. The endpoint deployment can be accomplished as follows. This takes 8-10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = training_model.deploy(initial_instance_count = 1, instance_type = 'ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker FeatureStore During Inference\n",
    "\n",
    "SageMaker FeatureStore can be useful in supplementing data for inference requests because of the low-latency GetRecord functionality. For this demo, we will be given a TransactionId and query our online FeatureGroups for data on the transaction to build our inference request. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incoming inference request.\n",
    "transaction_id = str(3450774)\n",
    "\n",
    "# Helper to parse the feature value from the record.\n",
    "def get_feature_value(record, feature_name):\n",
    "    return str(list(filter(lambda r: r['FeatureName'] == feature_name, record))[0]['ValueAsString'])\n",
    "\n",
    "transaction_response = featurestore_runtime.get_record(FeatureGroupName=transaction_feature_group_name, RecordIdentifierValueAsString=transaction_id)\n",
    "transaction_record = transaction_response['Record']\n",
    "\n",
    "transaction_test_data = [\n",
    "    get_feature_value(transaction_record, 'TransactionDT'),\n",
    "    get_feature_value(transaction_record, 'TransactionAmt'),\n",
    "    get_feature_value(transaction_record, 'card1'),\n",
    "    get_feature_value(transaction_record, 'card2'),\n",
    "    get_feature_value(transaction_record, 'card3'),\n",
    "    get_feature_value(transaction_record, 'card5'),\n",
    "    get_feature_value(transaction_record, 'card_type_credit'),\n",
    "    get_feature_value(transaction_record, 'card_type_debit'),\n",
    "    get_feature_value(transaction_record, 'card_bank_american_express'),\n",
    "    get_feature_value(transaction_record, 'card_bank_discover'),\n",
    "    get_feature_value(transaction_record, 'card_bank_mastercard'),\n",
    "    get_feature_value(transaction_record, 'card_bank_visa')\n",
    "]\n",
    "\n",
    "identity_response = featurestore_runtime.get_record(FeatureGroupName=identity_feature_group_name, RecordIdentifierValueAsString=transaction_id)\n",
    "identity_record = identity_response['Record']\n",
    "id_test_data = [\n",
    "    get_feature_value(identity_record, 'id_01'),\n",
    "    get_feature_value(identity_record, 'id_02'),\n",
    "    get_feature_value(identity_record, 'id_03'),\n",
    "    get_feature_value(identity_record, 'id_04'),\n",
    "    get_feature_value(identity_record, 'id_05')\n",
    "]\n",
    "\n",
    "# Join all pieces for inference request.\n",
    "inference_request = []\n",
    "inference_request.extend(transaction_test_data[:])\n",
    "inference_request.extend(id_test_data[:])\n",
    "\n",
    "inference_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = predictor.predict(','.join(inference_request), initial_args = {\"ContentType\": \"text/csv\"})\n",
    "prediction = json.loads(results)\n",
    "print (prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_feature_group.delete()\n",
    "transaction_feature_group.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
