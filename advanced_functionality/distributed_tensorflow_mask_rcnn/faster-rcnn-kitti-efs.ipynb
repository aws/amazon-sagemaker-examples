{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning of Faster R-CNN on KITTI dataset\n",
    "\n",
    "This notebook is a step-by-step tutorial on transfer learning of [Faster R-CNN](https://arxiv.org/abs/1506.01497) model on [Kitti](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d) 2-D object dataset starting with a pre-trained Mask R-CNN model trained on [COCO 2017 dataset](http://cocodataset.org/#download). \n",
    "\n",
    "The outline of steps is as follows:\n",
    "\n",
    "1. Stage Kitti dataset in COCO format on S3\n",
    "2. Copy Kitti dataset from Amazon S3 to Amazon EFS file-system mounted on this notebook instance\n",
    "3. Build Docker training image and push it to [Amazon ECR](https://aws.amazon.com/ecr/)\n",
    "4. Configure data input channels\n",
    "5. Configure hyper-prarameters\n",
    "6. Define training metrics\n",
    "7. Define training job and start training\n",
    "\n",
    "Before we get started, let us initialize two python variables ```aws_region``` and ```s3_bucket``` that we will use throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_region =  # <aws-region>\n",
    "s3_bucket  =  # <your-s3_bucket>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Kitti dataset in COCO format in Amazon S3\n",
    "\n",
    "Below are steps to convert Kitti dataset to COCO format and stage it in S3:\n",
    "\n",
    "1. Download [Kitti object dataset left camera color images](http://www.cvlibs.net/download.php?file=data_object_image_2.zip). \n",
    "2. Download [Kitti object dataset training labels](http://www.cvlibs.net/download.php?file=data_object_label_2.zip)\n",
    "3. `unzip` the downloaded zip files, which should create a folder structure as shown below:\n",
    "\n",
    "```\n",
    "   |-- training\n",
    "   |   |-- images_2\n",
    "   |   |__ label2\n",
    "   |\n",
    "   |-- testing\n",
    "   |__|__images_2\n",
    "      \n",
    "   ```\n",
    "3. Split the Kitti `training` dataset `image_2` and `label_2` files randomly into `train` and `val` datasets using a 90-10 split. We do not provide any tool for doing this split.\n",
    "4. Use [convert-dataset](https://github.com/eweill/convert-datasets/blob/master/convert-dataset.py) or any other tool to convert `train` and `val` datasets from `Kitti` to `Voc` format. \n",
    "5. Use [voc2coco](https://github.com/yukkyo/voc2coco) or any other tool to convert data from `Voc` to `Coco` format.\n",
    "    * Name the converted COCO annotations files for `train` and `val`  as `instances_trainKitti.json` and `instances_valKitti.json` respectively\n",
    "6. Copy the converted Kitti dataset and [pretrained models](http://models.tensorpack.com/#FasterRCNN) to your S3 bucket. The required prefix structure in your S3 bucket is shown below:\n",
    "   \n",
    "```\n",
    "|-- faster-rcnn-kitti\n",
    "|   |-- testing\n",
    "|   |__ ...\n",
    "|   |\n",
    "|   |-- sagemaker\n",
    "|   |   |-- input\n",
    "|   |   |   |-- train\n",
    "|   |   |   |   |-- trainKitti\n",
    "|   |   |   |   |   |__ ...\n",
    "|   |   |   |   |\n",
    "|   |   |   |   |-- valKitti\n",
    "|   |   |   |   |   |__ ...\n",
    "|   |   |   |   |\n",
    "|   |   |   |   |-- annotations\n",
    "|   |   |   |   |   |-- instances_trainKitti.json\n",
    "|   |   |   |   |   |__ instances_valKitti.json\n",
    "|   |   |   |   |\n",
    "|   |   |   |   |-- pretrained-models\n",
    "|   |   |   |   |   |-- COCO-MaskRCNN-R101FPN1x.npz\n",
    "|   |   |   |   |   |-- COCO-MaskRCNN-R50FPN2x.npz\n",
    "|   |   |   |   |   |-- ImageNet-R101-AlignPadding.npz\n",
    "|___|___|___|___|___|__ ImageNet-R50-AlignPadding.npz\n",
    "       \n",
    "   ```\n",
    "The split `train` and `val` dataset images are copied under `faster-rcnn-kitti/sagemaker/input/train/trainKitti` and `faster-rcnn-kitti/sagemaker/input/train/valKitti` prefixes in S3, respectively. The `testing/image_2` images are copied under `faster-rcnn-kitti/testing` prefix in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Kitti dataset from S3 to Amazon EFS\n",
    "\n",
    "Next, we copy Kitti dataset from S3 to EFS file-system.  The ```prepare-kitti-efs.sh``` script executes this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./prepare-kitti-efs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already copied COCO 2017 dataset from S3 to your EFS file-system, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!./prepare-kitti-efs.sh {s3_bucket}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and push SageMaker training images\n",
    "\n",
    "For this step, the [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) attached to this notebook instance needs full access to Amazon ECR service. If you created this notebook instance using the ```./stack-sm.sh``` script in this repository, the IAM Role attached to this notebook instance is already setup with full access to ECR service. \n",
    "\n",
    "Below, we will build an image for [TensorPack Faster-RCNN/Mask-RCNN](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN) implementation and push it to Amazon ECR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorPack Faster-RCNN/Mask-RCNN\n",
    "\n",
    "Use ```./container-kitti/build_tools/build_and_push.sh``` script to build and push the TensorPack Faster-RCNN/Mask-RCNN  training image to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./container-kitti/build_tools/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your *AWS region* as argument, run the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! ./container-kitti/build_tools/build_and_push.sh {aws_region}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set ```tensorpack_image``` below to Amazon ECR URI of the image you pushed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorpack_image =  #<amazon-ecr-uri>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Initialization \n",
    "We have staged the data and we have built and pushed the training docker image to Amazon ECR. Now we are ready to start using Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "role = get_execution_role() # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f'SageMaker Execution Role:{role}')\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "print(f'AWS account:{account}')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f'AWS region:{region}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the Amazon ECR image URI used for training. You saved this URI in a previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = tensorpack_image\n",
    "print(f'Training image: {training_image}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Data Channels\n",
    "\n",
    "Next, we define the *train* and *log* data channels using EFS file-system. To do so, we need to specify the EFS file-system id, which is shown in the output of the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -kh | grep 'fs-' | sed 's/\\(fs-[0-9a-z]*\\).*/\\1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the EFS ```file_system_id``` below to the ouput of the command shown above. In the cell below, we define the `train` data input channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "# Specify EFS ile system id.\n",
    "file_system_id = # 'fs-xxxxxxxx'\n",
    "print(f\"EFS file-system-id: {file_system_id}\")\n",
    "\n",
    "# Specify directory path for input data on the file system. \n",
    "# You need to provide normalized and absolute path below.\n",
    "file_system_directory_path = '/faster-rcnn-kitti/sagemaker/input/train'\n",
    "print(f'EFS file-system data input path: {file_system_directory_path}')\n",
    "\n",
    "# Specify the access mode of the mount of the directory associated with the file system. \n",
    "# Directory must be mounted  'ro'(read-only).\n",
    "file_system_access_mode = 'ro'\n",
    "\n",
    "# Specify your file system type\n",
    "file_system_type = 'EFS'\n",
    "\n",
    "train = FileSystemInput(file_system_id=file_system_id,\n",
    "                                    file_system_type=file_system_type,\n",
    "                                    directory_path=file_system_directory_path,\n",
    "                                    file_system_access_mode=file_system_access_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create the log output directory and define the `log` data output channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory path for log output on the EFS file system.\n",
    "# You need to provide normalized and absolute path below.\n",
    "# For example, '/mask-rcnn/sagemaker/output/log'\n",
    "# Log output directory must not exist\n",
    "file_system_directory_path = f'/faster-rcnn-kitti/sagemaker/output/log-{int(time.time())}'\n",
    "\n",
    "# Create the log output directory. \n",
    "# EFS file-system is mounted on '$HOME/efs' mount point for this notebook.\n",
    "home_dir=os.environ['HOME']\n",
    "local_efs_path = os.path.join(home_dir,'efs', file_system_directory_path[1:])\n",
    "print(f\"Creating log directory on EFS: {local_efs_path}\")\n",
    "\n",
    "assert not os.path.isdir(local_efs_path)\n",
    "! sudo mkdir -p -m a=rw {local_efs_path}\n",
    "assert os.path.isdir(local_efs_path)\n",
    "\n",
    "# Specify the access mode of the mount of the directory associated with the file system. \n",
    "# Directory must be mounted 'rw'(read-write).\n",
    "file_system_access_mode = 'rw'\n",
    "\n",
    "\n",
    "log = FileSystemInput(file_system_id=file_system_id,\n",
    "                                    file_system_type=file_system_type,\n",
    "                                    directory_path=file_system_directory_path,\n",
    "                                    file_system_access_mode=file_system_access_mode)\n",
    "\n",
    "data_channels = {'train': train, 'log': log}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the model output location in S3. Set ```s3_bucket``` to your S3 bucket name prior to running the cell below. \n",
    "\n",
    "The model checkpoints, logs and Tensorboard events will be written to the log output directory on the EFS file system you created above. At the end of the model training, they will be copied from the log output directory to the `s3_output_location` defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"faster-rcnn-kitti/sagemaker\" #prefix in your bucket\n",
    "s3_output_location = f's3://{s3_bucket}/{prefix}/output'\n",
    "print(f'S3 model output location: {s3_output_location}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Hyper-parameters\n",
    "Next we define the hyper-parameters. \n",
    "\n",
    "Note, some hyper-parameters are different between the two implementations. The batch size per GPU in TensorPack Faster-RCNN/Mask-RCNN is fixed at 1, but is configurable in AWS Samples Mask-RCNN. The learning rate schedule is specified in units of steps in TensorPack Faster-RCNN/Mask-RCNN, but in epochs in AWS Samples Mask-RCNN.\n",
    "\n",
    "The detault learning rate schedule values shown below correspond to training for a total of 24 epochs, at 120,000 images per epoch.\n",
    "\n",
    "<table align='left'>\n",
    "    <caption>TensorPack Faster-RCNN/Mask-RCNN  Hyper-parameters</caption>\n",
    "    <tr>\n",
    "    <th style=\"text-align:center\">Hyper-parameter</th>\n",
    "    <th style=\"text-align:center\">Description</th>\n",
    "    <th style=\"text-align:center\">Default</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">mode_fpn</td>\n",
    "        <td style=\"text-align:left\">Flag to indicate use of Feature Pyramid Network (FPN) in the Mask R-CNN model backbone</td>\n",
    "        <td style=\"text-align:center\">\"True\"</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td style=\"text-align:center\">mode_mask</td>\n",
    "        <td style=\"text-align:left\">A value of \"False\" means Faster-RCNN model, \"True\" means Mask R-CNN moodel</td>\n",
    "        <td style=\"text-align:center\">\"True\"</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td style=\"text-align:center\">eval_period</td>\n",
    "        <td style=\"text-align:left\">Number of epochs period for evaluation during training</td>\n",
    "        <td style=\"text-align:center\">1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">lr_schedule</td>\n",
    "        <td style=\"text-align:left\">Learning rate schedule in training steps</td>\n",
    "        <td style=\"text-align:center\">'[240000, 320000, 360000]'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">batch_norm</td>\n",
    "        <td style=\"text-align:left\">Batch normalization option ('FreezeBN', 'SyncBN', 'GN', 'None') </td>\n",
    "        <td style=\"text-align:center\">'FreezeBN'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">images_per_epoch</td>\n",
    "        <td style=\"text-align:left\">Images per epoch </td>\n",
    "        <td style=\"text-align:center\">120000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">data_train</td>\n",
    "        <td style=\"text-align:left\">Training data under data directory</td>\n",
    "        <td style=\"text-align:center\">'coco_train2017'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">data_val</td>\n",
    "        <td style=\"text-align:left\">Validation data under data directory</td>\n",
    "        <td style=\"text-align:center\">'coco_val2017'</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td style=\"text-align:center\">resnet_arch</td>\n",
    "        <td style=\"text-align:left\">Must be 'resnet50' or 'resnet101'</td>\n",
    "        <td style=\"text-align:center\">'resnet50'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">backbone_weights</td>\n",
    "        <td style=\"text-align:left\">Pretrained RestNet Backbone weights</td>\n",
    "        <td style=\"text-align:center\">'ImageNet-R50-AlignPadding.npz'</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td style=\"text-align:center\">load_model</td>\n",
    "        <td style=\"text-align:left\">Load pretrained model weights</td>\n",
    "        <td style=\"text-align:center\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "                    \"mode_fpn\": \"True\",\n",
    "                    \"mode_mask\": \"False\",\n",
    "                    \"eval_period\": 1,\n",
    "                    \"batch_norm\": \"FreezeBN\",\n",
    "                    \"backbone_weights\": 'ImageNet-R50-AlignPadding.npz',\n",
    "                    \"load_model\": \"COCO-MaskRCNN-R50FPN2x.npz\",\n",
    "                    \"data_train\": \"coco_trainKitti\",\n",
    "                    \"data_val\": \"coco_valKitti\",\n",
    "                    \"lr_schedule\": '[14400, 18000, 21600]',\n",
    "                    \"images_per_epoch\": 7200,\n",
    "                    \"resnet_arch\": 'resnet50'\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Metrics\n",
    "Next, we define the regular expressions that SageMaker uses to extract algorithm metrics from training logs and send them to [AWS CloudWatch metrics](https://docs.aws.amazon.com/en_pv/AmazonCloudWatch/latest/monitoring/working_with_metrics.html). These algorithm metrics are visualized in SageMaker console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "             {\n",
    "                \"Name\": \"fastrcnn_losses/box_loss\",\n",
    "                \"Regex\": \".*fastrcnn_losses/box_loss:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fastrcnn_losses/label_loss\",\n",
    "                \"Regex\": \".*fastrcnn_losses/label_loss:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fastrcnn_losses/label_metrics/accuracy\",\n",
    "                \"Regex\": \".*fastrcnn_losses/label_metrics/accuracy:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fastrcnn_losses/label_metrics/false_negative\",\n",
    "                \"Regex\": \".*fastrcnn_losses/label_metrics/false_negative:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fastrcnn_losses/label_metrics/fg_accuracy\",\n",
    "                \"Regex\": \".*fastrcnn_losses/label_metrics/fg_accuracy:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fastrcnn_losses/num_fg_label\",\n",
    "                \"Regex\": \".*fastrcnn_losses/num_fg_label:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/IoU=0.5\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/IoU=0\\\\.5:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/IoU=0.5:0.95\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/IoU=0\\\\.5:0\\\\.95:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/IoU=0.75\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/IoU=0\\\\.75:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/large\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/large:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/medium\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/medium:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/small\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/small:\\\\s*(\\\\S+).*\"\n",
    "            }\n",
    "            \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Training Job\n",
    "\n",
    "Next, we use SageMaker [Estimator](https://sagemaker.readthedocs.io/en/stable/estimators.html) API to define a SageMaker Training Job. \n",
    "\n",
    "We recommned using 32 GPUs, so we set ```train_instance_count=4``` and ```train_instance_type='ml.p3.16xlarge'```, because there are 8 Tesla V100 GPUs per ```ml.p3.16xlarge``` instance. We recommend using 100 GB [Amazon EBS](https://aws.amazon.com/ebs/) storage volume with each training instance, so we set ```train_volume_size = 100```. \n",
    "\n",
    "We run the training job in your private VPC, so we need to set the ```subnets``` and ```security_group_ids``` prior to running the cell below. You may specify multiple subnet ids in the ```subnets``` list. The subnets included in the ```sunbets``` list must be part of the output of  ```./stack-sm.sh``` CloudFormation stack script used to create this notebook instance. Specify only one security group id in ```security_group_ids``` list. The security group id must be part of the output of  ```./stack-sm.sh``` script.\n",
    "\n",
    "For ```train_instance_type``` below, you have the option to use ```ml.p3.16xlarge``` with 16 GB per-GPU memory and 25 Gbs network interconnectivity, or ```ml.p3dn.24xlarge``` with 32 GB per-GPU memory and 100 Gbs network interconnectivity. The ```ml.p3dn.24xlarge``` instance type offers significantly better performance than ```ml.p3.16xlarge``` for Mask R-CNN distributed TensorFlow training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give Amazon SageMaker Training Jobs Access to FileSystem Resources in Your Amazon VPC.\n",
    "security_group_ids =  # ['sg-xxxxxxxx'] \n",
    "subnets =    # [ 'subnet-xxxxxxx', ]\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "faster_rcnn_kitti_estimator = Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=4, \n",
    "                                         train_instance_type='ml.p3.16xlarge',\n",
    "                                         train_volume_size = 100,\n",
    "                                         train_max_run = 400000,\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sagemaker_session, \n",
    "                                         hyperparameters = hyperparameters,\n",
    "                                         metric_definitions = metric_definitions,\n",
    "                                         base_job_name=\"faster-rcnn-kitti-efs\",\n",
    "                                         subnets=subnets,\n",
    "                                         security_group_ids=security_group_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we launch the SageMaker training job. \n",
    "\n",
    "The time to complete the training depends on type and number of training instances, and the training image used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_rcnn_kitti_estimator.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
