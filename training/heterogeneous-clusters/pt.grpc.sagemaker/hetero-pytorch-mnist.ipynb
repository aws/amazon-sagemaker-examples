{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1aa7aed",
   "metadata": {},
   "source": [
    "## PyTorch example to demonstrate Amazon SageMaker Heterogeneous Cluster for model training\n",
    "\n",
    "---\n",
    "### Description\n",
    "Heterogeneous clusters enables launching training jobs that use multiple instance types in a single job. This  capability can improve your training cost and speed by running different parts of the model training on the most suitable instance type. This use case typically happens in computer vision DL training, where training is bottlnecked on CPU resources needed for data augmentation, leaving the expensive GPU underutilized. Heterogeneous clusters allows you to add more CPU resources to fully utilize GPUs increase training speed and cost-efficiency. For more details, you can find the documentation of this feature [here](https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html).\n",
    "\n",
    "This notebook demonstrates how to use Heterogeneous Cluster feature of SageMaker Training with PyTorch 1.10.  The notebook works on Python 3 (_Pytorch 1.10 Python 3.8 CPU Optimized_) image of SageMaker Studio Notebook instance, and runs on _ml.t3.medium_ instance type.\n",
    "\n",
    "In this sample notebook, we have taken the PyTorch model based on this [official MNIST example](https://github.com/pytorch/examples/tree/main/mnist). We modified the training code to be heavy on data pre-processing. We are going to train this model in both Homogeneous and Heterogeneous Cluster modes. The flag to train on any of these modes can be set using `IS_HETERO = False or True` in section **B.2 Configure environment variables**. \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td width=43%><b>Homogeneous training job</b><br/>\n",
    "    We first run the homogeneous training job to get baseline performance number, and observe that it is unable to fully utilize GPU of ml.p3.2xlarge instance due to a CPU being the bottleneck.</td>\n",
    "    <td width=57%><b>Heterogeneous training job</b><br/>\n",
    "    Then we'll switch to a heterogeneous training job where we'll add ml.c5.9xlarge instance for extra CPU cores, to allow increased GPU usage of ml.p3.2xlarge instance, and improve cost-efficiency. Both the jobs runs the training code, train data set, pre-processing, and other relevant parameters.\n",
    "    </td>\n",
    "   </tr> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <img src=images/homogeneous-cluster-diagram.png alt=\"homogeneous-training job\" /></td>\n",
    "    <td> <img src=images/heterogeneous-cluster-diagram.png alt=\"heterogeneous-training job\" /></td>\n",
    "   </tr> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "In homogeneous cluster training job, the data pre-processing and Deep Neural Network (DNN) training code runs on the same instance. However, in heterogeneous cluster training job, the data pre-processing code runs on the CPU nodes (here by referred as **data_group or data group**), whereas the Deep Neural Network (DNN) training code runs on the GPU nodes (here referred as **dnn_group or dnn group**). The inter-node communication between the data and dnn groups is handled by generic implementation of [gRPC client-server interface](https://grpc.io/docs/languages/python/basics/).  \n",
    "\n",
    "The script (`launcher.py`) has the logic to detect (using SageMaker environment variables) whether the node it is running on belongs to data_group or dnn_group. If it is data_group, it spawns a separate process by executing `train_data.py`. This script runs grpc-server service for extracting processed training batches using [Protocol Buffers](https://developers.google.com/protocol-buffers/docs/overview). The gRPC server running on the data_group listens on a specific port (ex. 6000). In the code (`train_data.py`) documentation, we have chosen an implementation that keeps the data loading logic intact  where data batches are entered into a shared queue. The `get_samples` function of the `DataFeedService` pulls batches from the same queue and sends them to the client in the form of a continuous data stream. While fetching the data, the main entrypoint script `launcher.py` listens on port 16000 for a shutdown request coming from gRPC client i.e data group. The `train_data.py` waits for shutdown action from the parent process. \n",
    "\n",
    "If the node belongs to dnn_group, the main training script (`launcher.py`) spawns a separate set of processes by executing `train_dnn.py`. The script runs gRPC client code and DNN component of the training job. It consumes the processed training data from the gRPC server. We have defined an iterable PyTorch dataset, RemoteDataset, that opens a connection to the gRPC server, and reads from a stream of data batches. Once the model is trained with all the batches of training data, the gRPC client exits, and the parent process`launcher.py` sends a shutdown request on port 16000. This indicates the gRPC server to shutdown, and signals ends of the training job. \n",
    "\n",
    "Here is how the workflow looks like:\n",
    "\n",
    "<img src=images/pytorch-heterogeneous-workflow.png width=600px>\n",
    "\n",
    "This example notebook runs a training job on 2 instances, 1 in each node group. The data_group uses ml.c5.9xlarge whereas dnn_group uses ml.p3.2xlarge.\n",
    "\n",
    "This notebook refers following files and folders:\n",
    "\n",
    "- Folders: \n",
    "  - `code`: this has the training (data pre-processing and dnn) scripts, and grpc client-server start and shutdown scripts\n",
    "  - `images`: contains images referred in notebook\n",
    "- Files: \n",
    "  - `launcher.py`: entry point training script. This script is executed on all the nodes irrespective of which group it belongs to. This is a parent process that makes a decision on where to spawn a data pre-processing or dnn component of the training job. The script runs on all the nodes as entrypoint. It also handles the shutdown logic for gRPC server. \n",
    "  - `train_data.py`, `dataset_feed_pb2.py`, `dataset_feed_pb2_grpc.py`: these scripts run on the data_group nodes and responsible for setting up grpc-server, start and shutdown.\n",
    "  - `train_dnn.py`: this script runs dnn code on the training data set. It fetches preprocessed data from the data_group node as a stream using gRPC client-server communication. It also sends a shutdown request after all the iterations on the preprocessed training data set. \n",
    "  - `requirement.txt`: defines package required for gRPC \n",
    "  - `train.py`: this script is the entrypoint script for SageMaker homogeneous cluster training. This script is picked up when you choose IS_HETERO = False. This uses a local dataset and runs both data pre-processing and a dnn component on the same node. \n",
    "\n",
    "At a high level, the notebook covers:\n",
    "-  Setting up SageMaker Studio Notebook \n",
    "-  Setting up the Training environment \n",
    "-  Submit a Training job\n",
    "-  Monitor and visualize the CloudWatch metrics\n",
    "-  Comparing time-to-train and cost-to-train\n",
    "-  Conclusion \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8683b",
   "metadata": {},
   "source": [
    "### A. Setting up SageMaker Studio notebook\n",
    "\n",
    "#### Step 1 - Upgrade SageMaker SDK and dependent packages \n",
    "Heterogeneous Clusters for Amazon SageMaker model training was [announced](https://aws.amazon.com/about-aws/whats-new/2022/07/announcing-heterogeneous-clusters-amazon-sagemaker-model-training) on 07/08/2022. As a first step, ensure you have updated SageMaker SDK, PyTorch, and Boto3 client that enables this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ea0231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/boto3-1.21.32-py3.9.egg (1.21.32)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.24.72-py3-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.5/132.5 kB 862.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: botocore in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/botocore-1.24.32-py3.9.egg (1.24.32)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.27.72-py3-none-any.whl (9.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 16.2 MB/s eta 0:00:00\n",
      "Collecting awscli\n",
      "  Downloading awscli-1.25.73-py3-none-any.whl (3.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 36.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sagemaker in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (2.99.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.109.0.tar.gz (571 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 571.8/571.8 kB 13.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/jmespath-1.0.0-py3.9.egg (from boto3) (1.0.0)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Using cached s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/python_dateutil-2.8.2-py3.9.egg (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/urllib3-1.26.9-py3.9.egg (from botocore) (1.26.9)\n",
      "Collecting colorama<0.4.5,>=0.2.5\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting docutils<0.17,>=0.10\n",
      "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 548.2/548.2 kB 11.9 MB/s eta 0:00:00\n",
      "Collecting PyYAML<5.5,>=3.10\n",
      "  Using cached PyYAML-5.4.1-cp39-cp39-macosx_10_9_x86_64.whl (259 kB)\n",
      "Collecting rsa<4.8,>=3.1.2\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/attrs-20.3.0-py3.9.egg (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: google-pasta in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/google_pasta-0.2.0-py3.9.egg (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from sagemaker) (1.22.4)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from sagemaker) (3.20.1)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/protobuf3_to_dict-0.1.5-py3.9.egg (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/smdebug_rulesconfig-1.0.1-py3.9.egg (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/importlib_metadata-4.11.3-py3.9.egg (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/packaging-21.3-py3.9.egg (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pandas-1.4.2-py3.9-macosx-10.9-x86_64.egg (from sagemaker) (1.4.2)\n",
      "Requirement already satisfied: pathos in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pathos-0.2.8-py3.9.egg (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/zipp-3.7.0-py3.9.egg (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pyparsing-3.0.7-py3.9.egg (from packaging>=20.0->sagemaker) (3.0.7)\n",
      "Requirement already satisfied: six in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pytz-2022.1-py3.9.egg (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: dill>=0.3.4 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/dill-0.3.4-py3.9.egg (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/multiprocess-0.70.12.2-py3.9.egg (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pox-0.3.0-py3.9.egg (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/ppft-1.6.6.4-py3.9.egg (from pathos->sagemaker) (1.6.6.4)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py): started\n",
      "  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.109.0-py2.py3-none-any.whl size=787707 sha256=6cb0a501a87ebb0ba16cd2f28b80f15c63c91f9fa56a40c1c76462ade3d6f14e\n",
      "  Stored in directory: /Users/gili/Library/Caches/pip/wheels/a7/c1/fa/eab3ed7597f624fbf6a9588a98288e7acf04eaa2c99c4af3d3\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: rsa, PyYAML, docutils, colorama, botocore, s3transfer, boto3, awscli, sagemaker\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.8\n",
      "    Uninstalling rsa-4.8:\n",
      "      Successfully uninstalled rsa-4.8\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.24.32\n",
      "    Uninstalling botocore-1.24.32:\n",
      "      Successfully uninstalled botocore-1.24.32\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.5.2\n",
      "    Uninstalling s3transfer-0.5.2:\n",
      "      Successfully uninstalled s3transfer-0.5.2\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.21.32\n",
      "    Uninstalling boto3-1.21.32:\n",
      "      Successfully uninstalled boto3-1.21.32\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.99.0\n",
      "    Uninstalling sagemaker-2.99.0:\n",
      "      Successfully uninstalled sagemaker-2.99.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-training 4.2.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed PyYAML-5.4.1 awscli-1.25.73 boto3-1.24.72 botocore-1.27.72 colorama-0.4.4 docutils-0.16 rsa-4.7.2 s3transfer-0.6.0 sagemaker-2.109.0\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m pip install --upgrade boto3 botocore awscli sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c24d24",
   "metadata": {},
   "source": [
    "#### Step 2 - Restart the notebook kernel \n",
    "From the Jupyter Lab menu bar **Kernel > Restart Kernel...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28916a3",
   "metadata": {},
   "source": [
    "#### Step 3 - Valdiate SageMaker Python SDK and PyTorch versions\n",
    "Ensure the output of the cell below reflects:\n",
    "\n",
    "- SageMaker Python SDK version 2.98.0 or above, \n",
    "- boto3 1.24 or above \n",
    "- botocore 1.27 or above \n",
    "- PyTorch 1.10 or above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115cf0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sagemaker\n",
      "Version: 2.109.0\n",
      "---\n",
      "Name: torch\n",
      "Version: 1.11.0\n",
      "---\n",
      "Name: boto3\n",
      "Version: 1.24.72\n",
      "---\n",
      "Name: botocore\n",
      "Version: 1.27.72\n"
     ]
    }
   ],
   "source": [
    "!pip show sagemaker torch boto3 botocore |egrep 'Name|Version|---'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d9f4e",
   "metadata": {},
   "source": [
    "--------------\n",
    "### B. Setting up the Training environment\n",
    "\n",
    "#### Step 1 - Import SageMaker components and setup the IAM role and Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e51da09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "output_path = \"s3://\" + sess.default_bucket() + \"/DEMO-mnist\"\n",
    "print(role)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b05e37",
   "metadata": {},
   "source": [
    "#### Step 2 - Configure environment variables \n",
    "This step defines whether you want to run training job in heterogenous cluster mode or not. Also, defines instance groups, multiple nodes in group, and hyperparameter values. For baselining, if you want to run both the data pre-processing and DNN on the same node set `IS_HETERO = False`. \n",
    "\n",
    "\n",
    "Test configuration (if running training on p3.2xl or g5.2xl as dnn_group instance type, and c5.2xl as data_group instance type: (training duration: 7-8 mins)  \n",
    "`num-data-workers: 4`  \n",
    "`grpc-workers: 4`   \n",
    "`num-dnn-workers: 4`  \n",
    "`pin-memory\": True`   \n",
    "`iterations : 100`   \n",
    "\n",
    "Perf configuration (if running training on p3.2xl as dnn_group instance type, and c5.9xl as data_group instance type OR training in homogeneous cluster mode i.e. g5.8xl): (training duration - 30 mins)  \n",
    "`num-data-workers: 32`  \n",
    "`grpc-workers: 2`   \n",
    "`num-dnn-workers: 2`  \n",
    "`pin-memory\": True`   \n",
    "`iterations : 4800`\n",
    "\n",
    "Perf configuration (if running training on p3.2xl in homogeneous cluster mode):   \n",
    "`num-data-workers: 8`  \n",
    "`grpc-workers: 2`   \n",
    "`num-dnn-workers: 2`  \n",
    "`pin-memory\": True`   \n",
    "`iterations : 2400`\n",
    "\n",
    "Note: This PyTorch example has not been tested with multiple instances in an instance group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b964a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_CLOUD_JOB = True\n",
    "IS_HETERO = True  # if set to false, uses homogeneous cluster\n",
    "PT_DATA_MODE = \"service\" if IS_HETERO else \"local\"  # local | service\n",
    "IS_DNN_DISTRIBUTION = False  # Distributed Training with DNN nodes not tested, set it to False\n",
    "\n",
    "data_group = InstanceGroup(\n",
    "    \"data_group\", \"ml.c5.9xlarge\", 1\n",
    ")  # 36 vCPU #change the instance type if IS_HETERO=True\n",
    "dnn_group = InstanceGroup(\n",
    "    \"dnn_group\", \"ml.p3.2xlarge\", 1\n",
    ")  # 8 vCPU #change the instance type if IS_HETERO=True\n",
    "\n",
    "kwargs = dict()\n",
    "kwargs[\"hyperparameters\"] = {\n",
    "    \"batch-size\": 8192,\n",
    "    \"num-data-workers\": 4,  # This number drives the avg. step time. More workers help parallel pre-processing of data. Recommendation: Total no. of cpu 'n' = 'num-data-wokers'+'grpc-workers'+ 2 (reserved)\n",
    "    \"grpc-workers\": 4,  # No. of workers serving pre-processed data to DNN group (gRPC client). see above formula.\n",
    "    \"num-dnn-workers\": 4,  # Modify this no. to be less than the cpu core of your training instances in dnn group\n",
    "    \"pin-memory\": True,  # Pin to GPU memory\n",
    "    \"iterations\": 100,  # No. of iterations in an epoch (must be multiple of 10).\n",
    "}\n",
    "\n",
    "if IS_HETERO:\n",
    "    kwargs[\"instance_groups\"] = [data_group, dnn_group]\n",
    "    entry_point = \"launcher.py\"\n",
    "else:\n",
    "    kwargs[\"instance_type\"] = (\n",
    "        \"ml.p3.2xlarge\" if IS_CLOUD_JOB else \"local\"\n",
    "    )  # change the instance type if IS_HETERO=False\n",
    "    kwargs[\"instance_count\"] = 1\n",
    "    entry_point = \"train.py\"\n",
    "\n",
    "if IS_DNN_DISTRIBUTION:\n",
    "    processes_per_host_dict = {\n",
    "        \"ml.g5.xlarge\": 1,\n",
    "        \"ml.g5.12xlarge\": 4,\n",
    "        \"ml.p3.8xlarge\": 4,\n",
    "        \"ml.p4d.24xlarge\": 8,\n",
    "    }\n",
    "    kwargs[\"distribution\"] = {\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": processes_per_host_dict[dnn_instance_type],\n",
    "            \"custom_mpi_options\": \"--NCCL_DEBUG INFO\",\n",
    "        },\n",
    "    }\n",
    "    if IS_HETERO:\n",
    "        kwargs[\"distribution\"][\"instance_groups\"] = [dnn_group]\n",
    "\n",
    "    print(f\"distribution={kwargs['distribution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485679d2",
   "metadata": {},
   "source": [
    "#### Step 3: Set up the Estimator\n",
    "In order to use SageMaker to fit our algorithm, we'll create an `Estimator` that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4246c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    framework_version=\"1.11.0\",  # 1.10.0 or later\n",
    "    py_version=\"py38\",  # Python v3.8\n",
    "    role=role,\n",
    "    entry_point=entry_point,\n",
    "    source_dir=\"code\",\n",
    "    volume_size=10,\n",
    "    max_run=4800,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38626e9",
   "metadata": {},
   "source": [
    "#### Step 4: Download the MNIST Data and Upload it to S3 bucket\n",
    "\n",
    "This is an optional step for now. The training job downloads the data on its run directly from MNIST website to the data_group node (grpc server). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "729a0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Download training and testing data from a public S3 bucket\n",
    "\n",
    "\n",
    "def download_from_s3(data_dir=\"./data\", train=True):\n",
    "    \"\"\"Download MNIST dataset and convert it to numpy array\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): directory to save the data\n",
    "        train (bool): download training set\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    if train:\n",
    "        images_file = \"train-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "    else:\n",
    "        images_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    # download objects\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    bucket = f\"sagemaker-sample-files\"\n",
    "    for obj in [images_file, labels_file]:\n",
    "        key = os.path.join(\"datasets/image/MNIST\", obj)\n",
    "        dest = os.path.join(data_dir, obj)\n",
    "        if not os.path.exists(dest):\n",
    "            s3.download_file(bucket, key, dest)\n",
    "    return\n",
    "\n",
    "\n",
    "download_from_s3(\"./data\", True)\n",
    "download_from_s3(\"./data\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e90ea016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to the default bucket\n",
    "\n",
    "prefix = \"DEMO-mnist\"\n",
    "bucket = sess.default_bucket()\n",
    "loc = sess.upload_data(path=\"./data\", bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "channels = {\"training\": loc, \"testing\": loc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b01926",
   "metadata": {},
   "source": [
    "## C. Submit the training job\n",
    "\n",
    "The job runs for the predefined iterations. DNN instance group sends a shutdown request to data group after done with the training. You can see the following entries in the cloudwatch logs of dnn instance. A job with 4800 iterations finishes in 29 mins in a Heterogeneous cluster composed of 1x ml.c5.9xlarge as data node and 1x ml.p3.2xlarge as DNN node.\n",
    "\n",
    "Note: The console output of billing seconds can be ignored. See the AWS console > SageMaker > Training Job for the exact billing seconds.\n",
    "\n",
    "Log excerpt from algo-1 (DNN instance)\n",
    "```\n",
    "4780: avg step time: 0.19709917231025106\n",
    "INFO:__main__:4780: avg step time: 0.19709917231025106\n",
    "4790: avg step time: 0.19694106239373696\n",
    "INFO:__main__:4790: avg step time: 0.19694106239373696\n",
    "4800: avg step time: 0.196784295383125\n",
    "Saving the model\n",
    "INFO:__main__:4800: avg step time: 0.196784295383125\n",
    "INFO:__main__:Saving the model\n",
    "Training job completed!\n",
    "INFO:__main__:Training job completed!\n",
    "Process train_dnn.py closed with returncode=0\n",
    "Shutting down data service dispatcher via: [algo-2:16000]\n",
    "Shutdown request sent to algo-2:16000\n",
    "2022-08-16 01:15:05,555 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
    "2022-08-16 01:15:05,555 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
    "2022-08-16 01:15:05,556 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce424c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-13 20:39:57 Starting - Starting the training job......\n",
      "2022-09-13 20:40:46 Starting - Preparing the instances for training.........\n",
      "2022-09-13 20:42:12 Downloading - Downloading input data...\n",
      "2022-09-13 20:42:31 Training - Downloading the training image........................\n",
      "2022-09-13 20:46:43 Training - Training image download completed. Training in progress.............\n",
      "2022-09-13 20:49:00 Uploading - Uploading generated training model\n",
      "2022-09-13 20:49:00 Completed - Training job completed\n",
      "..Training seconds: 0\n",
      "Billable seconds: 0\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(\n",
    "    inputs=channels,\n",
    "    job_name=\"pt-hetero\"\n",
    "    + \"-\"\n",
    "    + \"H-\"\n",
    "    + str(IS_HETERO)[0]\n",
    "    + \"-\"\n",
    "    + datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2bd225",
   "metadata": {},
   "source": [
    "## D. Monitoring Instance Metrics for GPU and CPU utilization\n",
    "\n",
    "Click on **View instance metrics** from the **Training jobs** node in **Amazon SageMaker Console**. In the run above, all 30 vCPU of Data node (algo-1) is approx. 100% utilized, and the GPU utilization is at 100% at frequent intervals in the DNN node (algo-2). To rescale the CloudWatch Metrics to 100% on CPU utilization for algo-1 and algo-2, use CloudWatch \"Add Math\" feature and average it out by no. of cores on those instance types.\n",
    "\n",
    "<img src=images/heterogeneous-instance-metrics.png width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef4995",
   "metadata": {},
   "source": [
    "## E. Comparing time-to-train and cost-to-train\n",
    "\n",
    "Let's continue with the above example i.e. train a heavy data pre-processing (CPU intensive) model (mnist) requiring only 1 GPU. We start with ml.p3.2xlarge (1xV100 GPU, 8x vCPU) in homogeneous cluster mode to get the baseline perf numbers. Due to the no. of CPU cores, we could not go beyond 8 data loader/workers for data pre-processing. The avg. step cost was `7.6 cents` and avg. step time is `1.19 seconds`. \n",
    "\n",
    "Our objective is to reduce the cost and speed up the model training time. The first choice here is to scale up the instance type in the same family. If we leverage the next instance type (4 GPU) in the P3 family, the GPUs would have gone underutilized. In this case, we needed more vCPU to GPU ratio. Assuming we haven't had any instance type in another instance family or the model is incompatible with the CPU/GPU architectures of other instance families, we are constrained to use ml.p3.2xlarge. The only way then to have more vCPUs to GPU ratio is to use SageMaker feature, Heterogeneous Cluster, which enables customers to offload data pre-processing logic to CPU only instance types example ml.c5. In the next test, we offloaded CPU intensive work i.e data preprocessing to ml.c5.9xlarge (36 vCPU) and continued using ml.p3.2xlarge for DNN. The avg. step cost was `1.9 cents` and avg. step time is `0.18 seconds`. \n",
    "\n",
    "In summary, we reduced the training cost by 4.75 times, and the avg. step reduced by 6.5 times. This was possible because with higher cpu count, we could use 32 data loader workers (compared to 8 with p3.2xl) to preprocess the data, and kept GPU close to 100% utilized at frequent intervals. Note: These numbers are just taken as a sample, you have to do benchmarking with your own model and dataset to come up with the exact price-performance benefits. \n",
    "\n",
    "## F. Conclusion\n",
    "In this notebook, we demonstrated how to leverage Heterogeneous cluster feature of SageMaker Training to achieve better price performance. This feature best fits for scenario where: 1/ your training job is CPU intensive and getting bottlenecked by the fixed vCPU count of an accelerated computing instance type, 2/ your training script can be decoupled - data preprocessing and deep neural network components. 3/ scaling up to next instance type within the same family is not price-performant due to underutilized GPUs, 4/ if cannot scale up, scaling out to a same instance type leads to underutlized GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee5027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "77c0de85c2cb739aa5100af7b92fb9d2075368f0e653f4148499a56c989df5f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
