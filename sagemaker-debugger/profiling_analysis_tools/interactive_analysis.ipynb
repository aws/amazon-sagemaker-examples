{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon EagleEye Data Analysis\n",
    "\n",
    "This notebook provides an introduction to interactive analysis of the data captured by SageMaker EagleEye. It is organized in order of training phases: initialization, training, and finalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Install dependencies](#1)<br>\n",
    "* [2. Preparing data for analysis](#2)<br>\n",
    "* [3. Analysis by training job phase](#analysis)<br>\n",
    "    * [3.1 Initialization](#initialization)<br>\n",
    "    * [3.2. Training loop](#training)<br>\n",
    "        * [3.2.1 Data loading](#dataloading)<br>\n",
    "    * [3.2. Finalization](#finalization)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2\"></a>\n",
    "## 2. Preparing data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python libraries for reading data and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Access Profiler Data: System Metrics and Algorithm (Framework) Metrics\n",
    "\n",
    "Once the training job initiates, SageMaker Profiler starts collecting system and framework metrics. The `smdebug` library provides profiler analysis tools that enable you to access and analyze the profiling data. The following code cells are to set up a `TrainingJob` object to retrieve the system and framework metrics when they become available in the default S3 bucket. Once the metrics are available, you can query, plot, and analyze the profiling metrics data throughout this notebook. Specify the training job name in the following cell. The job name is provided at the end of the training example notebooks.\n",
    "\n",
    "#### Specify the training job name and the region name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = \"smprofiler-report-snmg-mirror-2020-09-23-05-37-57-786\"\n",
    "region = 'us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "\n",
    "tj = TrainingJob(training_job_name, region)\n",
    "tj.wait_for_sys_profiling_data_to_be_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smdebug provides a class PandasFrame that converts profiler data into Pandas frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.utils.profiler_data_to_pandas import PandasFrame\n",
    "\n",
    "pf = PandasFrame(tj.profiler_s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following functions retrieve all system and framework metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_metrics_df = pf.get_all_system_metrics()\n",
    "framework_metrics_df = pf.get_all_framework_metrics(selected_framework_metrics=['Step:ModeKeys.TRAIN', 'Step:ModeKeys.GLOBAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analysis'></a>\n",
    "## 3. Analysis by training job phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will set up some helper functions and python profile analysis to be used in the analysis later in the section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import IFrame, display, Markdown\n",
    "from smdebug.profiler.python_profile_utils import StepPhase, PythonProfileModes\n",
    "from smdebug.profiler.analysis.utils.python_profile_analysis_utils import Metrics\n",
    "from smdebug.profiler.analysis.python_profile_analysis import PyinstrumentAnalysis, cProfileAnalysis\n",
    "from smdebug.profiler.analysis.utils.pandas_data_analysis import PandasFrameAnalysis, StatsBy, Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown, Code, Image\n",
    "def pretty_print(df):\n",
    "    raw_html = df.to_html().replace(\"\\\\n\",\"<br>\").replace('<tr>','<tr style=\"text-align: left;\">')\n",
    "    return display(HTML(raw_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_analysis = PandasFrameAnalysis(system_metrics_df, framework_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_stats_dir = \"python_stats\"\n",
    "os.makedirs(python_stats_dir, exist_ok=True)\n",
    "general_metrics_config = eval(tj.profiler_config[\"ProfilingParameters\"].get(\"GeneralMetricsConfig\", \"{}\"))\n",
    "use_pyinstrument = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_python_profile_stats(stats):\n",
    "    if use_pyinstrument:\n",
    "        if stats:\n",
    "            for step_stats in stats:\n",
    "                display(IFrame(src=step_stats.html_file_path, width= 1000, height=500))\n",
    "    else:\n",
    "        if stats:\n",
    "            stats.print_top_n_functions(Metrics.CUMULATIVE_TIME, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_analysis_class = PyinstrumentAnalysis if use_pyinstrument else cProfileAnalysis\n",
    "python_analysis = python_analysis_class(local_profile_dir=python_stats_dir, s3_path=tj.profiler_s3_output_path)\n",
    "step_stats_df = python_analysis.list_profile_stats()\n",
    "step_stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying the framework\n",
    "\n",
    "Some of the analysis can be framework-specific. Since the profiler data or the training job detail above does not indicate which framework was used, below we try to identify the framework used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework = \"tensorflow\"\n",
    "\n",
    "process_list = framework_metrics_df[\"process\"].unique()\n",
    "if 'Step:ModeKeys.GLOBAL' in process_list:\n",
    "    framework = \"pytorch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying the time interval of each phase\n",
    "\n",
    "The function `get_job_statistics()` shows training start and end time, duration of the initialization, training, and finalization stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_statistics = pf_analysis.get_job_statistics()\n",
    "\n",
    "initialization_start = job_statistics['start_time']\n",
    "initialization_end = job_statistics['training_loop_start']\n",
    "\n",
    "training_start = job_statistics['training_loop_start']\n",
    "training_end = job_statistics['training_loop_end']\n",
    "\n",
    "finalization_start = job_statistics['training_loop_end']\n",
    "finalization_end = job_statistics['end_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initialization'></a>\n",
    "## 3.1 Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase of the training job marks the time interval from the start of the training job in SageMaker to the start of the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python profiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the python profiling stats for functions that were called during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = python_analysis.fetch_pre_step_zero_profile_stats()\n",
    "display_python_profile_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "## 3.2 Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase of the training job indicates the time interval between the start and the end of the training loop. It includes the data loading process, the forward and backward pass, and synchronization.\n",
    "\n",
    "#### Utilization histograms\n",
    "\n",
    "MetricHistogram computes a histogram on GPU and CPU utilization values. Bins are between 0 and 100. Good system utilization means that the center of the distribtuon should be between 80 to 90. \n",
    "\n",
    "The following cell will plot the histograms per metric. In order to only plot specific metrics define the list  `select_dimensions` and `select_events`. A dimension can be CPUUtilization, GPUUtilization, GPUMemoryUtilization IOPS. If no event is specified then for CPU uiltization histogram for each single core and total cpu usage will be plotted. In case of GPU, it will visualize utilization and memory for each GPU. In case of IOPS it will plot io-wait time per cpu. If `select_events` is specified then only metrics that match the name in `select_metrics` will be shown. If neither `select_dimensions` nor `select_events` all available metrics will be visualized. One can also specify a start and endtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.notebook_utils.metrics_histogram import MetricsHistogram\n",
    "\n",
    "system_metrics_reader = tj.get_systems_metrics_reader()\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "\n",
    "metrics_histogram = MetricsHistogram(system_metrics_reader)\n",
    "metrics_histogram.plot(starttime=0, \n",
    "                       endtime=system_metrics_reader.get_timestamp_of_latest_available_file(), \n",
    "                       select_dimensions=[\"CPU\", \"GPU\"],\n",
    "                       select_events=[\"total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step durations over time\n",
    "\n",
    "SageMaker Profiler records the durations of each step, which is the time spent in one forward and backward pass. The following code cell plots step durations (y-axis) over training job duration (x-axis). Typically we would expect the step duration to be very similar across the training run. Signficant outliers are an indication of a bottleneck. `StepTimelineChart` helps to identify if such outliers happen in regular intervals. Following image shows an example, where the step duration mostly lasts about 200 to 250ms but every 10th step a spike occurs where step duration is significantly higher (600-800ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tj.wait_for_framework_profiling_data_to_be_available()\n",
    "\n",
    "from smdebug.profiler.analysis.notebook_utils.step_timeline_chart import StepTimelineChart\n",
    "\n",
    "framework_metrics_reader = tj.get_framework_metrics_reader()\n",
    "framework_metrics_reader.refresh_event_file_list()\n",
    "\n",
    "view_step_timeline_chart = StepTimelineChart(framework_metrics_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If detailed profiling was enabled, then python profiling was done on the steps specified for detailed profiling. Let's look at the python profiling stats for functions that were called during the first step profiled. We will look at the stats for each phase of the step. \n",
    "\n",
    "If you profiled with cProfile, you will see the function stats for the top 10 functions with greatest cumulative time spent. If you prefer to use a different metric or a different `n`, feel free to modify the `display_python_profile_stats` function defined at the beginning of the section.\n",
    "\n",
    "If you profiled with Pyinstrument, you will see the stats separated by each phase of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if general_metrics_config == {}:\n",
    "    step = 9\n",
    "else:\n",
    "    step = int(general_metrics_config[\"StartStep\"])\n",
    "phase = StepPhase.STEP_START\n",
    "mode = PythonProfileModes.GLOBAL if framework == \"pytorch\" else PythonProfileModes.TRAIN\n",
    "python_step_stats = python_analysis.fetch_profile_stats_by_step(\n",
    "    start_step=step, \n",
    "    end_step=step + 1, \n",
    "    start_phase=phase, \n",
    "    end_phase=phase,\n",
    "    mode=mode\n",
    ")\n",
    "display_python_profile_stats(python_step_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If detailed profiling was enabled and cProfile was used as the Python profiler, we can also just look at the stats for the training loop as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pyinstrument:\n",
    "    print(\"Pyinstrument was used as the Python profiler, no training loop stats are available.\")\n",
    "else:\n",
    "    python_job_stats = python_analysis.fetch_profile_stats_by_job_phase()\n",
    "    training_loop_stats = python_job_stats[\"training_loop\"]\n",
    "    display_python_profile_stats(training_loop_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU/GPU utilization\n",
    "\n",
    "SageMaker Profiler also records system metrics (resource utilization) for the training job. The following timeline charts depict the utilization per core and GPU. It shows the last 1000 datapoints. You can inspect previous datapoints by zooming out of the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.notebook_utils.timeline_charts import TimelineCharts\n",
    "\n",
    "system_metrics_reader = tj.get_systems_metrics_reader()\n",
    "framework_metrics_reader.refresh_event_file_list()\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "\n",
    "view_timeline_charts  = TimelineCharts(system_metrics_reader, \n",
    "                                       framework_metrics_reader,\n",
    "                                       select_dimensions=[\"CPU\", \"GPU\"],\n",
    "                                       select_events=[\"total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the BoxSelectTool to make a selection in the timeline chart.\n",
    "\n",
    "<img src='images/boxselect.png' width=\"840\" height=\"180\" border=\"10\" />\n",
    "\n",
    "\n",
    "The following code cell identifies which time annotations have been recorded in the training job for the selected timerange:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note change index range below with selected index range from above cell\n",
    "view_timeline_charts.find_time_annotations([300,310]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell creates a detailed view of framework metrics for the selected timerange. To avoid issues with out of memory, it will only plot the first 1000 datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note change index range below with selected index range from above cell\n",
    "view_timeline_charts.plot_detailed_profiler_data([300,310])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying CPU bottlenecks\n",
    "\n",
    "A heatmap is a representation where each row corresponds to one metric (CPU core and GPU utilizations) and x-axis is the duration of the training job. It allows to more easily spot CPU bottlenecks e.g. if utilization on GPU is low but a utilization of one or more cores is high. \n",
    "\n",
    "In the below heatmap, Yellow indicates maximum utilization, purple means that utilization was 0. GPUs have frequent stalled cycles where utilization is dropping to 0 while at the same time utilization on CPU cores is at a maximum. This is a clear indication of a CPU bottleneck where GPUs are waiting for the data to arrive. Such a bottleneck can be caused for instance by a compute-heavy pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.notebook_utils.heatmap import Heatmap\n",
    "\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "view_heatmap = Heatmap(system_metrics_reader, plot_height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging all the trace events \n",
    "\n",
    "The profiler captures events from different sources, such as framework, debugger hook, Horovod (if applicable). Putting them together, aligning the events and visualizing, will give an idea of events occurring in different components of the training job, and identify potential issues. The merge_timeline() API below combines the different types of trace event files into one trace event file, which can then be viewed in the browser.\n",
    "\n",
    "Below, we merge the events that occurred between step 1 and 10. You can modifying this step interval, or alternatively, merge events within a time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.utils.merge_timelines import MergedTimeline, MergeUnit\n",
    "\n",
    "start_step, end_step = 1, 10\n",
    "combined_timeline = MergedTimeline(tj.profiler_s3_output_path, output_directory=\"./\")\n",
    "combined_timeline.merge_timeline(start_step, end_step, unit=MergeUnit.STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the above file and load it in a browser (chrome://tracing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, since data loading is one of the most compute-intensive parts of the training process, below, we dive deep into analysing profiler data related to data loading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataloading'></a>\n",
    "## 3.2.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if framework == \"pytorch\":\n",
    "    display(Markdown(\"\"\"### Data loading analysis for PyTorch\\nWe will be instantiating the PT_dataloader_analysis class and invoking the methods in this class to perform the analysis.\n",
    "    The PT_dataloader_analysis class looks for dataloader events that are specific to PyTorch. Running this analysis for other frameworks will yield incorrect data.\n",
    "    \"\"\"))\n",
    "    \n",
    "    from smdebug.profiler.analysis.utils.pytorch_dataloader_analysis import PT_dataloader_analysis\n",
    "    pt_analysis = PT_dataloader_analysis(pf)\n",
    "    \n",
    "    display(Markdown(\"\"\"### Analysis of DataloaderIter initializations.\\nThe following cell analyzes\\n1. Which type of Dataloader iterators were initialized.\\n2. The number of workers per iterator.\\n3. Log the status of pin_memory.\\n4. Number of times the iteratos were initilized during training.  In PyTorch, \n",
    "    iterators are initialized every time iterations over the dataset is to be started. \n",
    "    (i.e. typically at the beginning of every epoch.) During initialization, PyTorch \n",
    "    spins of worker processes depending upon the configured number of workers, \n",
    "    establishes data queue to fetch data and pin_memory thread, if pin_memory is set.\\nThe analysis outputs the median and maximum duration for these initializations. If \n",
    "    there are outliers, (i.e duration is greater than 2 * median), the function prints \n",
    "    the start and end times for those durations. These can be used to inspect system \n",
    "    metrics during those time intervals.\n",
    "    \"\"\"))\n",
    "    \n",
    "    pt_analysis.analyze_dataloaderIter_initialization()\n",
    "    \n",
    "    display(Markdown(\"\"\"### Analysis of Dataloader worker processes\\nIn PyTorch, every time DataLoaderIterator is initalized, it spins of the worker processes that feed the data to iterator through attached data queue. These worker processes have lifetime similar to that of DataLoaderIterator.\n",
    "    The following analysis shows\\n1. The number of worker processes that were spun off during the entire training.\\n2. Median and maximum duration for the worker processes.\\n3. Start and end time for the worker processes that are outliers.\n",
    "    \"\"\"))\n",
    "    \n",
    "    md = pt_analysis.analyze_dataloaderWorkers()\n",
    "    \n",
    "    display(Markdown(\"\"\"### Analysis of DataLoaderIter::GetNext\\nIn PyTorch, the GetNext method is responsible for fetching the data from worker \n",
    "    processes through the data queue.\\nThese calls are run in the main training thread.\\nThe analysis of these events show\\n1. Number of GetNext calls made during the training.\\n2. Median and maximum duration in micoseconds for GetNext calls.\\n3. Start time, End time, duration and worker id for the outlier GetNext call duration.    \n",
    "    \"\"\"))\n",
    "    \n",
    "    md = pt_analysis.analyze_dataloader_getnext()\n",
    "    if md is not None:\n",
    "        pretty_print(md)\n",
    "    \n",
    "    display(Markdown(\"\"\"### Analyze a specific outlier in DataLoaderIter::GetNext\\nTo analyze specific outlier, select the row location corresponding to the outlier \n",
    "    that we want to analyze and run the following cells. Currently we will be plotting \n",
    "    the first outlier.\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if framework == \"pytorch\":\n",
    "    display(Markdown(\"\"\"### Analysis of training activity for each batch of data.\\nSince, we have the start and end times of all the GetNext calls, we can find the amount of time spent by the training script on one batch of data.\\n1. We will get the time spent on each data batch by finding the difference between start time of current GetNext call and subsequent GetNext call. Let's call it 'BatchTime_in_seconds'\\n2. We will find the outliers in 'BatchTime_in_seconds' and start and end time for those outliers.\\n3. Obtain the framework and system metrics during those timestamps. This will indicate where the time was spent.\n",
    "    \\n\\n### Analyze the BatchTime_in_seconds\\nFollowing cell\\n1. plots the BatchTime_in_seconds\\n2. Prints the median 'BatchTime_in_seconds'\\n3. Creates a dataframe that contains outliers.\n",
    "    \"\"\"))\n",
    "    \n",
    "    md_batch = pt_analysis.analyze_batchtime()\n",
    "    if md_batch is not None:\n",
    "        pretty_print(md_batch)\n",
    "    \n",
    "    display(Markdown(\"\"\"### Analyze a specific outlier in BatchTime_in_seconds\\nTo analyze specific outlier, select the row location corresponding to the outlier \n",
    "    that we want to analyze and run the following cells. Currently we will be plotting \n",
    "    the first outlier.\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Plotting the TimeLine charts for the first outlier reported in the above dataframe.\n",
    "    if md_batch is not None and md_batch.size > 0:\n",
    "        index = 0\n",
    "        start_timestamp = pf.convert_datetime_to_timestamp(md_batch.loc[index]['previous_batch_start'])\n",
    "        end_timestamp = pf.convert_datetime_to_timestamp(md_batch.loc[index]['start_time'])\n",
    "        view_timeline_charts=pt_analysis.plot_the_window(start_timestamp, end_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1.1 Tensorflow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "tj = TrainingJob(training_job_name, region)\n",
    "tj.wait_for_sys_profiling_data_to_be_available()\n",
    "system_metrics_reader = tj.get_systems_metrics_reader()\n",
    "\n",
    "tj.wait_for_framework_profiling_data_to_be_available()\n",
    "framework_metrics_reader = tj.get_framework_metrics_reader()\n",
    "framework_metrics_reader.refresh_event_file_list()\n",
    "\n",
    "from smdebug.profiler.analysis.utils.profiler_data_to_pandas import PandasFrame\n",
    "\n",
    "pf = PandasFrame(tj.profiler_s3_output_path)\n",
    "system_metrics_df = pf.get_all_system_metrics()\n",
    "\n",
    "step_metrics_df = pf.get_all_framework_metrics(selected_framework_metrics=['Step:ModeKeys.TRAIN'])\n",
    "\n",
    "step_metrics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We filter events of type GeneratorDatasetOp::Dataset::Iterator::GetNext.\\n\n",
    "## This is the event that performs data loading for a given train step.\\n\n",
    "## Measuring the time span of these events are useful indicators of how efficient the data loading is.\n",
    "\n",
    "iterator_metrics = pf.get_all_framework_metrics(selected_framework_metrics=['tensorflow::data::GeneratorDatasetOp::Dataset::Iterator::GetNext'])\n",
    "iterator_metrics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prints the time taken by first 10 time steps to perform data loading.\n",
    "## It also lists the number of workers doing the data loading.\n",
    "\n",
    "for i in range(10):\n",
    "    cur_step = step_metrics_df.iloc[i]\n",
    "    step_pid = cur_step[\"pid\"]\n",
    "    step_tid = cur_step[\"tid\"]\n",
    "    cur_step_number = cur_step[\"step\"]\n",
    "    step_start_time = cur_step[\"start_time_us\"]\n",
    "    step_end_time = cur_step[\"end_time_us\"]\n",
    "    step_duration = step_end_time - step_start_time\n",
    "\n",
    "    data_iter_cur_step = iterator_metrics.loc[(iterator_metrics['start_time_us'] >= step_start_time) & (iterator_metrics['end_time_us'] <= step_end_time)]\n",
    "    num_workers = data_iter_cur_step.shape[0]\n",
    "    dl_time_mean = data_iter_cur_step[\"end_time_us\"].mean() - data_iter_cur_step[\"start_time_us\"].mean()\n",
    "    print(f\"Step number: {cur_step_number}, time taken by train step: {step_duration}, Number of dataloading workers: {num_workers}, Average dataloading time of all the workers: {dl_time_mean} microseconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='finalization'></a>\n",
    "## 3.3 Finalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase of the training job marks the time interval from the end of the training loop to the end of the training job in SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python profiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the python profiling stats for functions that were called during finalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = python_analysis.fetch_post_hook_close_profile_stats()\n",
    "display_python_profile_stats(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
