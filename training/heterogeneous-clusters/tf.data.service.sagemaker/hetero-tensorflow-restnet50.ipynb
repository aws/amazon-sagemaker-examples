{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow's tf.data.service with Amazon SageMaker Training Heterogeneous Clusters\n",
    "\n",
    "---\n",
    "### Introduction\n",
    "\n",
    "Heterogeneous clusters enable launching training jobs that use multiple instance types in a single job. This capability can improve your training cost and speed by running different parts of the model training on the most suitable instance type. This use case typically happens in computer vision (CV) deep learning (DL) training, where training is bottleneck on CPU resources needed for data augmentation, leaving the expensive GPU underutilized. Heterogeneous clusters enable you to add more CPU resources to fully utilize GPUs to increase training speed and cost-efficiency. For more details, you can find the documentation of this feature [here](https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html).\n",
    "\n",
    "This notebook demonstrates how to use Heterogeneous Clusters with TensorFlow's [tf.data.service](https://www.TensorFlow.org/api_docs/python/tf/data/experimental/service). It includes training a CPU intensive DL CV workload. Comparing cost and performance between homogeneous and heterogeneous training configurations.  \n",
    "\n",
    "ðŸ’¡To get started quickly with heterogeneous clusters, we suggest you'll reuse the provided code as a quick way to migrate your workload from a local tf.data pipeline to a distributed tf.data.service pipeline. You'll need to change [code/train_dnn.py](./code/train_dnn.py), while keeping [code/train_data.py](./code/train_data.py) and [code/launcher.py](code/launcher.py) intact. This is explained below in the [Workload Details] section.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers:\n",
    "- A guide to switching from a homogeneous job (single instance type) to a heterogeneous job (multiple instance types)\n",
    "- Explaining to use Heterogeneous clusters with TensorFlow's tf.data.service\n",
    "- Set up Amazon SageMaker Studio Notebook \n",
    "- Run homogeneous cluster training job \n",
    "- Run heterogeneous cluster training job \n",
    "- Compare time and cost to train between homogeneous and heterogeneous clusters\n",
    "- Considerations\n",
    "- Conclusion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A guide to switching from a homogeneous to a heterogeneous job\n",
    "\n",
    "This notebook runs and compares these two workloads:\n",
    "\n",
    "Homogeneous Training Job - The image shows a ml.p4d.24xlarge instance GPUs is under-utilized due to a CPU bottleneck.  \n",
    "<img src=images/basic-homogeneous-job.png alt=\"homogeneous training job\" />\n",
    "  \n",
    "Heterogeneous Training Job - The image shows two ml.c5.18xlarge instances with extra CPU cores, to reduce the CPU bottleneck and increase GPU usage, to improve training speed cost-efficiency.  \n",
    " <img src=images/basic-heterogeneous-job.png alt=\"Heterogeneous training job\" />\n",
    "\n",
    "In each workload: Training data is an artificially generated dataset consisting of 32x32x3 images with random pixel values, and a corresponding random label representing 10 different classes. As this dataset is randomly generated, you should not expect the model to converge in a meaningful way. This shouldn't matter as our intent is only to measure data pipeline and neural network optimization throughput expressed in epoch/step time.  \n",
    "The model we used is [Resnet50](https://www.TensorFlow.org/api_docs/python/tf/keras/applications/ResNet50). The workloads uses an 8 GPUs instance, ml.p4d.24xlarge, and uses Horovod for data parallelization. \n",
    "\n",
    "The heterogeneous job will include two instance groups:\n",
    "- **data_group** - A group of CPU instances that will run data pre-processing code.\n",
    "- **dnn_group** - A group of GPU instances that will run Deep Neural Network training code.\n",
    "\n",
    "In this example, the inter-node communication between CPU and GPU instance groups is implemented using [TensorFlow data service feature](https://www.TensorFlow.org/api_docs/python/tf/data/experimental/service). This feature allows offloading a configurable amount of preprocessing work to worker machines. Note that SageMaker's Heterogeneous cluster does not provide out-of-the-box support for inter-instance_group communication, and it is up to the user to implement (we provide reference implementation here).\n",
    "\n",
    "This notebook refers following files and folders:\n",
    "- [code/train_dnn.py](./code/train_dnn.py) - this is standard TF training script, it has a single reference to tf.data.service when setting up the tf.data pipeline. This script will be executed on GPU instances belonging to the dnn_group.\n",
    "- [code/train_data.py](./code/train_data.py) - this script starts tf.data.service services like a tf.data.service Dispatcher and tf.data.service Worker processes. You shouldn't edit this script when adjusting to your workload.\n",
    "- [code/launcher.py](./code/launcher.py) - Entry point training script. This is the script that SageMaker Training will start on all instances (all instances groups must share the same entry point script in heterogeneous clusters). `launcher.py` is responsible for detecting the instance group the instance belong to, and start `train_dnn.py` and `train_data.py` accordingly. It is also responsible for shutting down tf.data.services the training script completes (`train_dnn.py`) so all instances exit allowing the SageMaker training job to complete. \n",
    "In every instance `luncher.py` will use `train_data.py` to start a tf.data.service worker server (As all instance types have CPUs that could be used for preprocessing). `luncher.py` will start a single tf.data.service dispatcher server (on the first instance of the `data_group`).  \n",
    "`luncher.py` will start the `train_dnn.py` script in all GPU instances (`dnn_group` instances).\n",
    "\n",
    "#### Learn more about tf.data.service processes\n",
    "`tf.data.service Dispatcher` - The dispatcher server acts as the control plain for tf.data.service; Being responsible for registering worker servers and assigning preprocessing tasks to them. Each training job has a single Dispatcher running in the first instance of the `data_group` and listens on port 6000.\n",
    "`tf.data.service Workers` - Worker servers carry out the data processing. Each instance could have one or more workers (listen on port 6001/6002/...).\n",
    "\n",
    "##### Defining what part of your pipeline runs in which instance group\n",
    " When you apply `tf.data.experimental.service.distribute()` on your dataset, all preprocessing operations defined up to the apply will run on the tf.data.service workers, and all dataset operations defined afterwords will run on the local process. All instances will need access to a dataset you'll make available through a SageMaker training data channel. You do have the option of limiting which instance group will see which training data channel.\n",
    "\n",
    "The below figure shows sequence of events of setting up and running in a tf.data.service based heterogeneous cluster training job.\n",
    "\n",
    "<img src=images/tf.data.service-diagram.png width=600px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security groups update if running in private VPC\n",
    "This section is relevant if you plan to [run in a private VPC](https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html) (passing `subnets` and `security_group_ids` parameters when defining an Estimator).  \n",
    "SageMaker documentation recommends you [add](https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html#train-vpc-vpc) a rule for your security group that allows inbound connections between members of the same security group, for all TCP communication. This will also cover for the tf.data.service related traffic between instances:\n",
    "- `tf.data.service` Dispatcher node will listen for incoming connections on port 6000 (configurable) from all nodes.\n",
    "- `tf.data.service` Workers will listen on ports 6001-6006 from all nodes.\n",
    "- Each node listens on port 16000 for a tf.data.service shutdown signal from all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Set up SageMaker Studio notebook\n",
    "#### Before you start\n",
    "Ensure you have selected Python 3 (_TensorFlow 2.6 Python 3.8 CPU Optimized_) image for your SageMaker Studio Notebook instance, and running on _ml.t3.medium_ instance type.\n",
    "\n",
    "#### Step 1 - Upgrade SageMaker SDK and dependent packages \n",
    "Heterogeneous Clusters for Amazon SageMaker model training was [announced](https://aws.amazon.com/about-aws/whats-new/2022/07/announcing-heterogeneous-clusters-amazon-sagemaker-model-training) on 07/08/2022. This feature release requires you to have updated SageMaker SDK, Boto3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m pip install --upgrade boto3 botocore awscli sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Restart the notebook kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Validate SageMaker Python SDK and TensorFlow versions\n",
    "Ensure the output of the cell below reflects:\n",
    "\n",
    "- SageMaker Python SDK version 2.98.0 or above, \n",
    "- boto3 1.24 or above \n",
    "- botocore 1.27 or above \n",
    "- TensorFlow 2.6 or above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show sagemaker boto3 botocore tensorflow protobuf |egrep 'Name|Version|---'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Run a homogeneous training job\n",
    "#### Step 1: Set up the training environment\n",
    "In this step, we define and submit a homogeneous training job. It uses a single instance type (p4d.24xlarge) with 8 GPUs. The analysis of the job will shows that it is CPU bound and therefore its GPUs are underutilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "import os\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": 10,\n",
    "    \"steps_per_epoch\": 500,\n",
    "    \"batch_size\": 1024,\n",
    "    \"tf_data_mode\": \"local\",  # We won't be using tf.data.service ('service') for this homogeneous job\n",
    "    \"num_of_data_workers\": 0,  # We won't be using tf.data.service ('service') for this homogeneous job\n",
    "}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"launcher.py\",\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"2.9.1\",\n",
    "    py_version=\"py39\",\n",
    "    role=role,\n",
    "    volume_size=10,\n",
    "    max_run=1800,  # 30 minutes\n",
    "    disable_profiler=True,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    instance_count=1,\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": 8,  # 8 GPUs per host\n",
    "            \"custom_mpi_options\": \"--NCCL_DEBUG WARN\",\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Submit the training job\n",
    "\n",
    "Note: For the logs, click on **View logs** from the **Training Jobs** node in **Amazon SageMaker Console**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from start_job_utils import fit_with_retries\n",
    "\n",
    "fit_with_retries(\n",
    "    5,\n",
    "    estimator,\n",
    "    job_name=\"homogeneous-\" + datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Analyzing the homogeneous training job throughput and resource usage\n",
    "We'll examine: CPU and GPU usage. Epoch time and step time\n",
    "\n",
    "**CPU and GPU usage analysis** \n",
    "\n",
    "In the screenshot below we observe that close to all the 96 vCPU of the instance is utilized. While GPU utilization is only ~45%. Clearly if we had more vCPUs we could increase GPU usage significantly to increase job throughput\n",
    "\n",
    "Note: To view your own job Click on **View instance metrics** from the **Training jobs** in **Amazon SageMaker Console**. Then to rescale the CloudWatch Metrics to 100% on CPU utilization for algo-1 and algo-2, use CloudWatch \"Add Math\" feature and average it out by no. of vCPUs/GPUs on those instance types. We captured metrics definitions used to produce this graph [here](./cloudwatch-metric-definitions/homogenous-workload%20copy.json).  \n",
    "<img src=\"images/metrics homogeneous cpu and gpu usage.png\" width=75%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epoch time and step time analysis**\n",
    "\n",
    "For 2nd and 3rd epochs the below should print out: 105s/epoch - 209ms/step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture homogeneous_logs\n",
    "estimator.sagemaker_session.logs_for_job(estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Printing step time for epochs and steps for {estimator.latest_training_job.name}\")\n",
    "for line in homogeneous_logs.stdout.split(\"\\n\"):\n",
    "    if \"mpirank:0\" in line and \"/epoch\" in line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Run a heterogeneous cluster training job\n",
    "\n",
    "#### Step 1: Set up training environment\n",
    "We'll now run a training job in heterogeneous cluster mode.  \n",
    "Note the changes from the homogeneous cluster job:  \n",
    "- We define two new instance groups that are provided to the `estimator` as the `instance_groups` parameter that replaces the homogeneous parameters `instance_type` and `instance_count`.\n",
    "- In the `distribution` parameter for Horovod we added a new parameter `instance_groups` that is used to limit the MPI cluster to run in the `dnn_group`. The MPI cluster should include only the GPU nodes that run Horovod (which needs MPI). The `data_group` instances should not be part of the MPI cluster, as they set up their on `tf.data.service` cluster.\n",
    "\n",
    "More on the two instance groups config we use:\n",
    "- `data_group` - two ml.c5.18xlarge instances, each with 72 vCPUs to handle data preprocessing. Reading data from S3, preprocessing it, and forwarding it to the `dnn_group`.\n",
    "- `dnn_group` - a single p4d.24xlarge instance, with 8 GPUs and 96 vCPUs to handle deep neural network optimization (forward backward passes). To fully utilize 96 vCPUs in the `dnn_group`, we'll be starting data workers on all the instances in both groups, therefore we have 240 vCPUs (96+72+72) in total available for preprocessing (minus vCPUs used for the neural network optimization process).\n",
    "\n",
    "There are three Python scripts to know about:\n",
    "The 1st is `train_dnn.py` - This is your training script for the neural network, you should edit it to match your own use case. Note that this script isn't aware of the Heterogeneous cluster set up, except when it initializes the tf.data dataset calling this line: `ds = ds.apply(tf.data.experimental.service.distribute(...)`.  \n",
    "The 2nd and 3rd scripts, which should not need editing when adapting to your own use case, do the heavy lifting required for using tf.data.service over the Heterogeneous cluster feature.  \n",
    "`train_data.py` include functions to start/stop tf.service.data process like a dispatcher and WorkerServer. \n",
    "`launcher.py` has several responsibilities: \n",
    "- A single entry point script for all instances in all instance groups (SageMaker will start the same script on all instances).\n",
    "- Identifies which instance group the node belong to, and start the relevant script accordingly (`train_dnn.py` or `train_data.py` or sometimes both).\n",
    "- Takes measures to ensure that tf.data.service processes shutdown when training completes, as the training job completes only when all instances exit.\n",
    "- Allow to start more than one process (for example, on the dnn_group instances we'll run both the `train_dnn.py` and a tf.data.service worker to utilize the instance CPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import os\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": 10,\n",
    "    \"steps_per_epoch\": 500,\n",
    "    \"batch_size\": 1024,\n",
    "    \"tf_data_mode\": \"service\",  # Using tf.data.service for this Heterogeneous cluster job\n",
    "    \"num_of_data_workers\": 1,  # One tf.data.service worker per node\n",
    "}\n",
    "\n",
    "# Group for CPU instances to run tf.data.service dispatcher/workers processes.\n",
    "data_group = InstanceGroup(\"data_group\", \"ml.c5.18xlarge\", 2)\n",
    "# Group for deep neural network (dnn) with accleartors (e.g., GPU, FPGA, etc.)\n",
    "dnn_group = InstanceGroup(\"dnn_group\", \"ml.p4d.24xlarge\", 1)\n",
    "\n",
    "estimator2 = TensorFlow(\n",
    "    entry_point=\"launcher.py\",\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"2.9.1\",\n",
    "    py_version=\"py39\",\n",
    "    role=role,\n",
    "    volume_size=10,\n",
    "    max_run=1800,  # 30 minutes\n",
    "    disable_profiler=True,\n",
    "    # instance_type='ml.p4d.24xlarge',\n",
    "    # instance_count=1,\n",
    "    instance_groups=[data_group, dnn_group],\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": 8,  # p4d.24xlarge has 8 GPUs per host\n",
    "            \"custom_mpi_options\": \"--NCCL_DEBUG WARN\",\n",
    "        },\n",
    "        \"instance_groups\": [dnn_group],  # Apply distribution strategy to the dnn_group only\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Submit the training job\n",
    "\n",
    "Note1: For the logs, click on **View logs** from the **Training Jobs** node in **Amazon SageMaker Console**. \n",
    "Note2: Ignore the 0 billable seconds shown below. See actual billable seconds in the AWS web console > SageMaker > Training Jobs > this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from start_job_utils import fit_with_retries\n",
    "\n",
    "fit_with_retries(\n",
    "    5,\n",
    "    estimator2,\n",
    "    job_name=\"heterogeneous-\" + datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Analyze the heterogeneous cluster training job's throughput and resource usage\n",
    "We'll examine: CPU and GPU usage. Epoch time and step time.\n",
    "\n",
    "**CPU and GPU usage analysis** \n",
    "\n",
    " In the screenshot below we observe that GPU usage has increase to 74% (compared to ~45% in the homogeneous training run) which is what we were aiming for. The CPU usage on all 3 instances are close to 80% CPU usage.  \n",
    " \n",
    "Note: To view your own job Click on **View instance metrics** from the **Training jobs** node in **Amazon SageMaker Console**. Then to rescale the CloudWatch Metrics to 100% on CPU utilization for algo-1 and algo-2, use CloudWatch \"Add Math\" feature and average it out by no. of vCPUs/GPUs on those instance types.  We captured metrics definitions used to produce this graph [here](./cloudwatch-metric-definitions/heterogenenous-workload.json).  \n",
    "<img src=\"images/metrics Heterogeneous cpu and gpu usage.png\" width=75%/>\n",
    "\n",
    "**Epoch time and step time analysis** \n",
    "\n",
    "For 2nd epoch onwards you should see this print out in the logs of the dnn_group instance (p4d.24xlarge): 43s/epoch - 86ms/step.\n",
    "Note that the instances are named: Algo1, Algo2, Algo3 randomly on each execution, so you'll need to open all instances logs to find the dnn_group instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Comparing time-to-train and cost-to-train\n",
    "The table below summarizes both jobs. We can see that:\n",
    "- The Heterogeneous job is <b>2.2x faster to train</b> (86ms/step) than the homogeneous job (192ms/step).\n",
    "- The Heterogeneous job is <b>45% cheaper to train</b> than the homogeneous job. This is despite the heterogeneous costs more per hour ($45/hour vs $37/hour), due to the two extra c5.18xlarge instances included in the heterogeneous job `($45 = $37.7 + 2 * $3.67` \n",
    "The cost-to-train formula we used: change in hourly price `($45/$37.7) ` times `reduction-in-time-to-train (86ms/192ms)`  =  45% = `($45/$37.7) * (86ms/192ms)`. \n",
    "\n",
    "<img src=images/homogeneous-vs-heterogeneous-results-table.png alt=\"results table\" />\n",
    "\n",
    "## F. Considerations\n",
    "TensorFlowâ€™s tf.data.service can only run TensorFlow-based data augmentation functions (it canâ€™t run a user-defined Python function)\n",
    "\n",
    "## G. Conclusion\n",
    "In this notebook, we demonstrated how to leverage Heterogeneous cluster feature of SageMaker Training, with TensorFlow to achieve better price performance and increase training speed. To get started you can copy this example project and change `train_dnn.py` to match your workload. To run the job, you could use this notebook, or the `start_job.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
