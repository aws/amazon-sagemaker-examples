{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1a52b6",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart Foundation Models - OpenChatKit GPT-NeoXT-Chat-Base-20B Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acea92d",
   "metadata": {},
   "source": [
    "---\n",
    "Welcome to Amazon [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)! You can use SageMaker JumpStart to solve many Machine Learning tasks through one-click in SageMaker Studio, or through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#use-prebuilt-models-with-sagemaker-jumpstart).\n",
    "\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy the [GPT-NeoXT-Chat-Base-20B](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B) model and query the model within an [OpenChatKit](https://github.com/togethercomputer/OpenChatKit) interactive shell. This demonstration provides an open-source Foundation Model chatbot for use within your application.\n",
    "\n",
    "TODO: introduce and discuss benefits of OpenChatKit\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c0bc7",
   "metadata": {},
   "source": [
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Select a model](#2.-Select-a-model)\n",
    "3. [Retrieve Artifacts & Deploy an Endpoint](#3.-Retrieve-Artifacts-&-Deploy-an-Endpoint)\n",
    "4. [Query endpoint and parse response](#4.-Query-endpoint-and-parse-response)\n",
    "5. [Advanced features: How to use various parameters to control the generated text](#5.-Advanced-features:-How-to-use-various-advanced-parameters-to-control-the-generated-text)\n",
    "6. [Advanced features: How to use prompts engineering to solve different tasks](#6.-Advacned-features:-How-to-use-prompts-engineering-to-solve-different-tasks)\n",
    "5. [Clean up the endpoint](#5.-Clean-up-the-endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e35194",
   "metadata": {},
   "source": [
    "Note: This notebook was tested on ml.t3.medium instance in Amazon SageMaker Studio with Python 3 (Data Science) kernel and in Amazon SageMaker Notebook instance with conda_python3 kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8dfad",
   "metadata": {},
   "source": [
    "### 1. Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f31be0",
   "metadata": {},
   "source": [
    "---\n",
    "Before executing the notebook, there are some initial steps required for set up. This notebook requires ipywidgets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67d497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f5d81",
   "metadata": {},
   "source": [
    "#### Permissions and environment variables\n",
    "\n",
    "---\n",
    "To host on Amazon SageMaker, we need to set up and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook as the AWS account role with SageMaker access. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67131eee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69849d02",
   "metadata": {},
   "source": [
    "## 2. Select a pre-trained model\n",
    "***\n",
    "You can continue with the default model, or can choose a different model from the dropdown generated upon running the next cell. A complete list of SageMaker pre-trained models can also be accessed at [SageMaker pre-trained Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b2d4f",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b08aa4a",
   "metadata": {},
   "source": [
    "### 3. Retrieve Artifacts & Deploy an Endpoint\n",
    "\n",
    "***\n",
    "\n",
    "Using SageMaker, we can perform inference on the pre-trained model, even without fine-tuning it first on a new dataset. We start by retrieving the `instance_type`, `image_uri`, and `model_uri` for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it. This may take a few minutes.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ae768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, instance_types\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "# Retrieve the inference instance type for the specified model.\n",
    "# instance_type = instance_types.retrieve_default(\n",
    "#     model_id=model_id, model_version=model_version, scope=\"inference\"\n",
    "# )\n",
    "instance_type = \"ml.g5.24xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "# image_uri = image_uris.retrieve(\n",
    "#     region=None,\n",
    "#     framework=None,  # automatically inferred from model_id\n",
    "#     image_scope=\"inference\",\n",
    "#     model_id=model_id,\n",
    "#     model_version=model_version,\n",
    "#     instance_type=instance_type,\n",
    "# )\n",
    "image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.3-cu117\"\n",
    "\n",
    "# Retrieve the model uri.\n",
    "# model_uri = model_uris.retrieve(\n",
    "#     model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    "# )\n",
    "model_uri = \"jumpstart-1p/textgeneration2/infer-huggingface-textgeneration2-huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16-20230419-1418-repack.tar.gz\"\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "# We already repack the inference script and model artifacts, so the `source_dir` argument to Model is not required.\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2554851-cbcc-4ef9-864e-776a3550ceca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Query endpoint and parse response\n",
    "\n",
    "***\n",
    "This model also supports many advanced parameters while performing inference. They include:\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **max_time:** The maximum amount of time you allow the computation to run for in seconds. Generation will still finish the current pass after allocated time has been passed. This setting can help to generate a response prior to endpoint invocation response time out errors.\n",
    "* **num_return_sequences:** Number of output sequences returned. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **seed:** Fix the randomized state for reproducibility. If specified, it must be an integer.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6f7b0-4093-48c9-acdb-54b05886b2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"text_inputs\": \"<human>: Tell me the steps to make a pizza\\n<bot>:\",\n",
    "    \"max_length\": 50,\n",
    "    \"max_time\": 50,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(\n",
    "    json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[0][0][\"generated_text\"]\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "generated_texts = parse_response(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4e096",
   "metadata": {},
   "source": [
    "### 5. Use an OpenChatKit shell to interact with your deployed endpoint\n",
    "\n",
    "***\n",
    "OpenChatKit provides a command line shell to interact with the chatbot. Here, we show how to utilize this shell with your deployed endpoint\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e91b5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone --branch v0.16 --depth 1 https://github.com/togethercomputer/OpenChatKit.git\n",
    "touch OpenChatKit/__init__.py\n",
    "touch OpenChatKit/inference/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70dacd4",
   "metadata": {},
   "source": [
    "***\n",
    "Here, we provide a bare-bones simplification of the inference scripts in this OpenChatKit repository that can interact with our deployed SageMaker endpoint. There are three main components to this:\n",
    "1. A model object (`JumpStartChatModel`) as a light wrapper around our endpoint query and parsing scripts,\n",
    "2. A shell interpreter (`JumpStartOpenChatKitShell`) that allows for iterative inference invocations, and\n",
    "3. A conversation object (`Conversation`) that stores previous human/chatbot interactions within the interactive shell.\n",
    "\n",
    "The `Conversation` object is used as-is from the OpenChatKit repository. The model and shell objects, however, are explicitly defined in the following cell since they have some notable simplifications from the OpenChatKit implementation. We encourage you to explore the previously cloned repository to see how more in-depth features, such as token streaming, moderation models, and retrieval augmented generation may be used within this context.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmd\n",
    "import json\n",
    "from typing import List, Optional\n",
    "\n",
    "from OpenChatKit.inference.conversation import Conversation\n",
    "\n",
    "\n",
    "class JumpStartChatModel:\n",
    "    human_id = \"<human>\"\n",
    "    bot_id = \"<bot>\"\n",
    "\n",
    "    def __init__(self, endpoint_name: str):\n",
    "        self.endpoint_name = endpoint_name\n",
    "\n",
    "    def do_inference(self, prompt, **kwargs):\n",
    "        payload = {\"text_inputs\": prompt, **kwargs}\n",
    "        payload_json = json.dumps(payload).encode(\"utf-8\")\n",
    "        query_response = query_endpoint_with_json_payload(payload_json, endpoint_name=endpoint_name)\n",
    "        generated_text = parse_response(query_response)\n",
    "        return generated_text[len(prompt):]  # remove the context from the output\n",
    "\n",
    "\n",
    "class JumpStartOpenChatKitShell(cmd.Cmd):\n",
    "    intro = (\n",
    "        \"Welcome to the OpenChatKit chatbot shell, modified to use a SageMaker JumpStart endpoint! Type /help or /? to \"\n",
    "        \"list commands. For example, type /quit to exit shell.\\n\"\n",
    "    )\n",
    "    prompt = \">>> \"\n",
    "    \n",
    "    def __init__(self, endpoint_name: str, cmd_queue: Optional[List[str]] = None, **kwargs):\n",
    "        super().__init__()\n",
    "        self._endpoint_name = endpoint_name\n",
    "        self._payload_kwargs = kwargs\n",
    "        if cmd_queue is not None:\n",
    "            self.cmdqueue = cmd_queue\n",
    "\n",
    "    def preloop(self):\n",
    "        self._model = JumpStartChatModel(self._endpoint_name)\n",
    "        self._convo = Conversation(self._model.human_id, self._model.bot_id)\n",
    "\n",
    "    def precmd(self, line):\n",
    "        command = line[1:] if line.startswith('/') else 'say ' + line\n",
    "        return command\n",
    "\n",
    "    def do_say(self, arg):\n",
    "        self._convo.push_human_turn(arg)\n",
    "        output = self._model.do_inference(self._convo.get_raw_prompt(), **self._payload_kwargs)\n",
    "        self._convo.push_model_response(output)\n",
    "        print(self._convo.get_last_turn())\n",
    "\n",
    "    def do_reset(self, arg):\n",
    "        self._convo = Conversation(self._model.human_id, self._model.bot_id)\n",
    "\n",
    "    def do_hyperparameters(self, arg):\n",
    "        print(f\"Hyperparameters: {self._payload_kwargs}\\n\")\n",
    "\n",
    "    def do_quit(self, arg):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0a3da",
   "metadata": {},
   "source": [
    "***\n",
    "We can now launch this shell as a command loop. This will repeatedly issue a prompt, accept input, parse the input command, and dispatch actions. Because the resulting shell may be utilized in an infinite loop, this notebook provides a default command queue (`cmdqueue`) as a queued list of input lines; because the last input is the command `/quit`, the shell will exit upon exhaustion of the queue. To dynamically interact with this chatbot, please remove the `cmdqueue`.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_queue = [\n",
    "    \"Hello!\",\n",
    "    \"Make a markdown table of national parks with their location and date established.\",\n",
    "    \"/quit\",\n",
    "]\n",
    "JumpStartOpenChatKitShell(\n",
    "    endpoint_name=endpoint_name,\n",
    "    cmd_queue=cmd_queue,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_k=40,\n",
    ").cmdloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5de21f",
   "metadata": {},
   "source": [
    "### 7. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b588d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "model_predictor.delete_model()\n",
    "model_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
