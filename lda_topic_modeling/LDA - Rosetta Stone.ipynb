{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Latent Dirichlet Allocation - An End-to-End SageMaker Example\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Training](#Training)\n",
    "1. [Inference](#Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "\n",
    "Amazon SageMaker LDA is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. Latent Dirichlet Allocation (LDA) is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics.\n",
    "\n",
    "In this notebook we will use the Amazon SageMaker LDA algorithm to train an LDA model on some example synthetic data. We will then use this model to classify (perform inference on) the data. The main goals of this notebook are to,\n",
    "\n",
    "* learn how to obtain and store data for use in Amazon SageMaker,\n",
    "* create an AWS SageMaker training job on a data set to produce an LDA model,\n",
    "* use the LDA model to perform inference with an Amazon SageMaker endpoint.\n",
    "\n",
    "The following are ***not*** goals of this notebook:\n",
    "\n",
    "* understand the LDA model,\n",
    "* understand how the Amazon SageMaker LDA algorithm works,\n",
    "* interpret the meaning of the inference output\n",
    "\n",
    "If you would like to know more about these things take a minute to run this notebook and then check out the SageMaker LDA Documentation and the [LDA - Science](http://www.example.com) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "***\n",
    "\n",
    "Before we do anything at all, we need data! We also need to setup our AWS credentials so that AWS SageMaker can store and access data. In this section we will do four things:\n",
    "\n",
    "1. [Setup AWS Credentials](#SetupAWSCredentials)\n",
    "1. [Obtain Example Dataset](#ObtainExampleDataset)\n",
    "1. [Inspect Example Data](#InspectExampleData)\n",
    "1. [Store Data on S3](#StoreDataonS3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup AWS Credentials\n",
    "\n",
    "> **NOTE** To run this notebook all you need to provide is an AWS S3 bucket to store the training input and output along with an access role for SageMaker to access this bucket. The only other user input is the location of the AWS SageMaker LDA algorithm Docker image. You shouldn't need to edit this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_role = 'arg:aws:iam::<<<ACCESS ROLE>>>'\n",
    "bucket = '<<<BUCKET>>>'\n",
    "\n",
    "s3_access_role = 'arn:aws:iam::874786414999:role/ease-access-role'\n",
    "bucket = 'lda-notebook-example'\n",
    "\n",
    "training_image = '462891221994.dkr.ecr.us-west-2.amazonaws.com/lda:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Example Data\n",
    "\n",
    "\n",
    "We generate some example synthetic document data. For the purposes of this notebook we will omit the details of this process. All we need to know is that each piece of data, commonly called a \"document\", is a vector of integers representing \"word counts\" within the document. In this particular example there are a total of 25 words in the \"vocabulary\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from generate_example_data import generate_griffiths_data\n",
    "\n",
    "# generate the sample data\n",
    "num_documents = 2000\n",
    "known_alpha, known_beta, documents, topic_mixtures = generate_griffiths_data(\n",
    "    num_documents=num_documents, num_topics=10, seed=0)\n",
    "num_topics, vocabulary_size = known_beta.shape\n",
    "\n",
    "\n",
    "# separate the generated data into training and tests subsets\n",
    "num_documents_training = int(0.8*num_documents)\n",
    "num_documents_test = num_documents - num_documents_training\n",
    "\n",
    "documents_training = documents[:num_documents_training]\n",
    "documents_test = documents[num_documents_training:]\n",
    "\n",
    "topic_mixtures_training = topic_mixtures[:num_documents_training]\n",
    "topic_mixtures_test = topic_mixtures[num_documents_training:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Example Data\n",
    "\n",
    "*What does the example data actually look like?* Below we print an example document as well as its corresponding *known* topic mixture. Later, when we perform inference on the training data set we will compare the inferred topic mixture to this known one.\n",
    "\n",
    "As we can see, each document is a vector of word counts from the 25-word vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First training document =\\n\\t{}'.format(documents[0]))\n",
    "print('\\nVocabulary size = {}'.format(vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print('Known topic mixture of first document =\\n\\t{}'.format(topic_mixtures_training[0]))\n",
    "print('\\nNumber of topics = {}'.format(num_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are visual creatures, let's try plotting the documents. In the below plots, each pixel of a document represents a word. The greyscale intensity is a measure of how frequently that word occurs. Below we plot the first tes documents of the training set reshaped into 5x5 pixel grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from generate_example_data import plot_lda\n",
    "\n",
    "fig = plot_lda(documents_training, nrows=3, ncols=4, cmap='gray_r', with_colorbar=True)\n",
    "fig.suptitle('Example Document Word Counts')\n",
    "fig.set_dpi(160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data on S3\n",
    "\n",
    "A SageMaker training job needs access to training data stored in an S3 bucket. Although training can accept data of various formats we convert the documents MXNet RecordIO Protobuf format before uploading to the S3 bucket defined at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from mxnet.recordio import MXRecordIO\n",
    "from record_pb2 import Record\n",
    "\n",
    "def save_documents(fname, documents):\n",
    "    \"\"\"Saves a Numpy array of documents to RecordIO Protobuf format.\"\"\"\n",
    "    feature_size = documents.shape[1]\n",
    "    \n",
    "    # convert to protobuf\n",
    "    protobuf = [\n",
    "        list_to_record_bytes(\n",
    "            document.astype(np.float32).tolist(),\n",
    "            feature_size=feature_size)\n",
    "        for document in documents\n",
    "    ]\n",
    "\n",
    "    # write to recordio\n",
    "    recordio = MXRecordIO(fname, \"w\")\n",
    "    for datum in protobuf:\n",
    "        recordio.write(datum)\n",
    "    recordio.close()\n",
    "    \n",
    "\n",
    "def list_to_record_bytes(values, keys=None, label=None, feature_size=None):\n",
    "    \"\"\"Takes a list and returns a serialized bytestring (using the vector/record representation)\"\"\"\n",
    "    record = Record()\n",
    "    record.features['values'].float32_tensor.values.extend(values)\n",
    " \n",
    "    if keys is not None:\n",
    "        if feature_size is None:\n",
    "            raise ValueError(\"For sparse tensors the feature size must be specified.\")\n",
    "        record.features['values'].float32_tensor.keys.extend(keys)\n",
    "\n",
    "    if feature_size is not None:\n",
    "        record.features['values'].float32_tensor.shape.extend([feature_size])\n",
    " \n",
    "    if label is not None:\n",
    "        record.label['values'].float32_tensor.values.extend([label])\n",
    "        \n",
    "    return record.SerializeToString()\n",
    "\n",
    "    \n",
    "def libsvm_record_converter(label, keys, values, feature_size=None):\n",
    "    record = Record()\n",
    "    record.features['values'].float32_tensor.values.extend(values)\n",
    " \n",
    "    if keys is not None:\n",
    "        if feature_size is None:\n",
    "            raise ValueError(\"For sparse tensors the feature size must be specified.\")\n",
    "        record.features['values'].float32_tensor.keys.extend(keys)\n",
    "\n",
    "    if feature_size is not None:\n",
    "        record.features['values'].float32_tensor.shape.extend([feature_size])\n",
    " \n",
    "    if label is not None:\n",
    "        record.label['values'].float32_tensor.values.extend([label])\n",
    " \n",
    "    return record\n",
    "\n",
    "\n",
    "# convert and upload the training document\n",
    "fname = 'data.pbr'\n",
    "save_documents(fname, documents_training)\n",
    "key = 'lda-rosetta-stone-notebook/training/' + fname\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_file(fname)\n",
    "print('Uploaded document data \"{}\" to \"{}/{}\"'.format(fname, bucket, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "***\n",
    "\n",
    "Once the data is preprocessed and available in a recommended format the next step is to train our model on the data. There are number of parameters required by SageMaker LDA configurng the model and defining the computational environment in which training will take place.\n",
    "\n",
    "An LDA model uses the following hyperparameters:\n",
    "\n",
    "* **`num_topics`** - The number of topics or categories in the LDA model. Usually, this is not known a prior. However, here we know that this example data is generated by five topics.\n",
    "\n",
    "* **`feature_dim`** - The size of the *\"vocabulary\"*, in LDA parlance. In this case, this is equal 25. (Each pixel represents one word in the vocabulary.)\n",
    "\n",
    "* **`mini_batch_size`** - The number of training *\"documents\"*, in LDA parlance. In this case, the total number of documents in the training set. (5000)\n",
    "\n",
    "* **`alpha0`** - *(optional)* a measurement of how \"mixed\" the topics are for each digit. When `alpha0` is small the data tends to be represented by one or few topics. When `alpha0` is large the data tends to be an even combination of several or many topics. In this context it makes sense that an image is representative of only one digit, not a combination of multiple digits.\n",
    "\n",
    "In addition to these LDA model hyperparameters, we provide additional parameters defining things like the EC2 instance type on which training will run, the S3 bucket containing the data, and the AWS access role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# create a name for this training job. to better distinguish this training job\n",
    "# from others we append a timestamp.\n",
    "job_name_prefix = 'lda-rosetta-stone-notebook'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "\n",
    "# set up the parameters for the SageMaker create_raining_job call. This includes\n",
    "# things like\n",
    "#   * which algorithm image to use\n",
    "#   * algorithm hyperparameters\n",
    "#   * S3 locations of input/output data\n",
    "training_params = {\n",
    "    'AlgorithmSpecification': {\n",
    "        'TrainingImage': training_image,\n",
    "        'TrainingInputMode': 'File',\n",
    "    },\n",
    "    'HyperParameters': {\n",
    "        'num_topics': str(num_topics),\n",
    "        'feature_dim': str(vocabulary_size),\n",
    "        'mini_batch_size': str(num_documents_training),\n",
    "        'alpha0': str(1.0),\n",
    "    },\n",
    "    'InputDataConfig': [\n",
    "        {\n",
    "            'ChannelName': 'train',\n",
    "            'CompressionType': 'None',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': 's3://{}/{}/training/'.format(bucket, job_name_prefix),\n",
    "                    'S3DataDistributionType': 'FullyReplicated',\n",
    "                }\n",
    "            },\n",
    "            'RecordWrapperType': 'None',\n",
    "        }\n",
    "    ],\n",
    "    'OutputDataConfig': {\n",
    "        'S3OutputPath': 's3://{}/{}/output'.format(bucket, job_name_prefix),\n",
    "    },\n",
    "    'ResourceConfig': {\n",
    "        'InstanceCount': 1,\n",
    "        'InstanceType': 'ml.c4.2xlarge',\n",
    "        'VolumeSizeInGB': 50,\n",
    "    },\n",
    "    'RoleArn': s3_access_role,\n",
    "    'StoppingCondition': {\n",
    "        'MaxRuntimeInSeconds': 60*60,\n",
    "    },\n",
    "    'TrainingJobName': job_name,\n",
    "}\n",
    "\n",
    "\n",
    "print('Training job name: {}'.format(job_name))\n",
    "print('\\nInput Data Location: {}'.format(training_params['InputDataConfig'][0]['DataSource']['S3DataSource']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above configuration create a SageMaker client and use the client to create a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Amazon SageMaker training job\n",
    "sagemaker = boto3.client(service_name='sagemaker')\n",
    "sagemaker.create_training_job(**training_params)\n",
    "\n",
    "\n",
    "# confirm that the training job has started\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n",
    "\n",
    "\n",
    "# wait for the job to finish and report the ending status\n",
    "sagemaker.get_waiter('TrainingJob_Created').wait(TrainingJobName=job_name)\n",
    "training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "status = training_info['TrainingJobStatus']\n",
    "print(\"Training job ended with status: \" + status)\n",
    "\n",
    "\n",
    "# if the job failed, determine why\n",
    "if status == 'Failed':\n",
    "    message = sagemaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))\n",
    "    raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message,\n",
    "\n",
    "> `Training job ended with status: Completed`\n",
    "\n",
    "then that means training sucessfully completed and the output LDA model was stored in the output path specified by `training_params['OutputDataConfig']`.\n",
    "\n",
    "You can also view information about and the status of a training job using the AWS SageMaker console. Just click on the \"Jobs\" tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "***\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the topic mixture representing a given document.\n",
    "\n",
    "This section involves several steps,\n",
    "\n",
    "1. [Create Endpoint Configuration](#CreateEndpointConfiguration) - Create a configuration defining an endpoint.\n",
    "1. [Create Endpoint](#CreateEndpoint) - Use the configuration to create an inference endpoint.\n",
    "1. [Perform Inference](#Perform Inference) - Perform inference on some input data using the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "We now create a SageMaker Model from the training output. Using the model we can create an Endpoint Configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# get the location of the model generated by the above training job\n",
    "model_name = job_name\n",
    "model_data = training_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "model_params = {\n",
    "    'ExecutionRoleArn': s3_access_role,\n",
    "    'ModelName': model_name,\n",
    "    'PrimaryContainer': {\n",
    "        'Image': training_image,\n",
    "        'ModelDataUrl': model_data,\n",
    "    },\n",
    "}\n",
    "model_response = sagemaker.create_model(**model_params)\n",
    "print('Model name: {}'.format(model_name))\n",
    "print('ModelArn:   {}\\n'.format(model_response['ModelArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Endpoint Configuration\n",
    "\n",
    "Use the model to create an endpoint configuration. The endpoint configuration also contains information about the type and number of EC2 instances to use when hosting the algorithm.\n",
    "\n",
    "SageMaker LDA is compute-bound so we will use an `ml.c4.2xlarge` instanace in this example. On problems with larger vocabulary size or large volume of data consider using `ml.c4.4xlarge` or `ml.c4.8xlarge` instances. (Or the `ml.c5` instances, once available!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, time\n",
    "\n",
    "# define the endpoint configuration parameters. these include things like\n",
    "#   * the type of instance to use at the endpoint\n",
    "#   * the number of instances to spin up at the endpoint\n",
    "#   * the name of the model to use    \n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = job_name_prefix + '-endpoint-config' + timestamp\n",
    "endpoint_config_params = {\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "    'ProductionVariants': [\n",
    "        {\n",
    "            'InstanceType': 'ml.c4.xlarge',\n",
    "            'InitialInstanceCount': 1,\n",
    "            'ModelName': model_name,\n",
    "            'VariantName': 'AllTraffic'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# create the endpoint configuration\n",
    "endpoint_config_response = sagemaker.create_endpoint_config(**endpoint_config_params)\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Endpoint\n",
    "\n",
    "Use the configuration to create an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = job_name_prefix + '-endpoint' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sagemaker.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the status of the endpoint\n",
    "response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "\n",
    "\n",
    "# wait until the status has changed\n",
    "sagemaker.get_waiter('Endpoint_Created').wait(EndpointName=endpoint_name)\n",
    "\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If you see the message,\n",
    "\n",
    "> `Endpoint creation ended with EndpointStatus = InService`\n",
    "\n",
    "then congratulations! You now have a functioning inference endpoint. You can confirm the endpoint configuration and status by navigating to the \"Endpoints\" tab in the AWS SageMaker console.\n",
    "\n",
    "We will finally create a runtime object from which we can invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "lda_runtime = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference\n",
    "\n",
    "With this realtime endpoint at our fingertips we can finally perform inference on our training and test data.\n",
    "\n",
    "### LDA Inference\n",
    "\n",
    "We should first discuss the meaning of the SageMaker LDA inference output. For more information see [How LDA Works](http://www.example.com) and the [LDA-Science](http://www.example.com) notebook.\n",
    "\n",
    "For each document we wish to compute its corresponding `topic_mixture`. Each topic mixture is a probability distribution over the number of topics, which is five in this example. Of the five topics discovered during LDA training each element of the topic mixture is the proportion to which the input document is represented by the corresponding topic.\n",
    "\n",
    "For example, if the topic mixture of an input document $\\mathbf{w}$ is,\n",
    "\n",
    "$$\\theta = \\left[ 0.3, 0.2, 0, 0.5, 0 \\right]$$\n",
    "\n",
    "then $\\mathbf{w}$ is 30% generated from the first topic, 20% from the second topic, and 50% from the fourth topic. Below, we compute the topic mixtures for the first ten traning documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io, json\n",
    "\n",
    "# for demonstration purposes we show that input data can be passed in csv\n",
    "# format. additional available formats are JSON, JSON sparse format, and \n",
    "# RecordIO Protobuf\n",
    "def np2csv(arr):\n",
    "    csv = io.BytesIO()\n",
    "    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n",
    "    return csv.getvalue().decode().rstrip()\n",
    "\n",
    "payload = np2csv(documents_training[:10])\n",
    "print('Input text/csv document payload:\\n{}'.format(payload))\n",
    "\n",
    "print('\\nInvoking endpoint...')\n",
    "invoke_endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'ContentType': 'text/csv',\n",
    "    'Body': payload,\n",
    "}\n",
    "response = lda_runtime.invoke_endpoint(**invoke_endpoint_params)\n",
    "\n",
    "\n",
    "print('\\nObtaining results...')\n",
    "results = json.loads(response['Body'].read().decode())\n",
    "\n",
    "\n",
    "print('\\nPrinting results...\\n')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be hard to see but the output format of SageMaker LDA inference endpoint is a Python dictionary with the following format.\n",
    "\n",
    "```\n",
    "{\n",
    "  'predictions': [\n",
    "    {'topic_mixture': [ ... ] },\n",
    "    {'topic_mixture': [ ... ] },\n",
    "    {'topic_mixture': [ ... ] },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "We extract the topic mixtures, themselves, corresponding to each of the input documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_topic_mixtures = np.array([prediction['topic_mixture'] for prediction in results['predictions']])\n",
    "\n",
    "print(computed_topic_mixtures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you decide to compare these results to the known topic mixtures generated in [Obtain Example Data](#ObtainExampleData) keep in mind that SageMaker LDA discovers topics in no particular order. That is, the approximate topic mixtures computed above may be permutations of the known topic mixtures corresponding to the same documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_mixtures_training[0])  # known topic mixture\n",
    "print(computed_topic_mixtures[0])  # computed topic mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop / Close the Endpoint\n",
    "\n",
    "Finally, we should delete the endpoint before we close the notebook.\n",
    "\n",
    "To restart the endpoint you can follow the code above using the same `endpoint_name` we created or you can navigate to the \"Endpoints\" tab in the SageMaker console, select the endpoint with the name stored in the variable `endpoint_name`, and select \"Delete\" from the \"Actions\" dropdown menu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
