{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Deploy scalable streaming tokens solution on SageMaker\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the latest container that packages some of the most popular open source libraries for model parallel inference like DeepSpeed and HuggingFace Accelerate. We use DJLServing as the model serving solution in this example. DJLServing is a high-performance universal model serving solution powered by the [Deep Java Library](https://github.com/deepjavalibrary/djl) (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our [recent blog post](https://aws.amazon.com/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/).\n",
    "\n",
    "In this notebook, we will deploy the gpt-neox-7b model on a ml.g5.2xlarge machine. We will also demostrate a streaming experience to have model run end2end in a pagination fashion.\n",
    "\n",
    "\n",
    "## Licence agreement\n",
    "- View model license information: Apache 2.0 before using the model.\n",
    "- This notebook is a sample notebook and not intended for production use. Please refer to the licence at https://github.com/aws/mit-0.\n",
    "\n",
    "\n",
    "## Permission\n",
    "\n",
    "In order to conduct this lab, we will need the following permissions:\n",
    "\n",
    "- ECR Push/Pull access\n",
    "- S3 bucket push access\n",
    "- SageMaker access\n",
    "- DynamoDB access (create DB and query)\n",
    "\n",
    "\n",
    "## Let's bump up SageMaker and import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker boto3 awscli --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71542f98",
   "metadata": {},
   "source": [
    "## Bring your own container to ECR repository\n",
    "\n",
    "*Note: Please make sure you have the permission in AWS credential to push to ECR repository*\n",
    "\n",
    "In this step, we will pull the LMI nightly container from dockerhub and then push it to the ECR repository.\n",
    "\n",
    "This process may take a while, depends on the container size and your network bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efb852",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# The name of our container\n",
    "repo_name=djlserving-byoc\n",
    "# Target container\n",
    "target_container=\"deepjavalibrary/djl-serving:deepspeed-nightly\"\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${repo_name}:latest\"\n",
    "echo \"Creating ECR repository ${fullname}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${repo_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${repo_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin \"${account}.dkr.ecr.${region}.amazonaws.com\"\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "echo \"Start pulling container: ${target_container}\"\n",
    "\n",
    "docker pull ${target_container}\n",
    "docker tag ${target_container} ${fullname}\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Create SageMaker compatible Model artifact, upload Model to S3 and use DJL builtin streaming handler for your model.\n",
    "\n",
    "SageMaker Large Model Inference containers can be used to host models without providing your own inference code. This is extremely useful when there is no custom pre-processing of the input data or postprocessing of the model's predictions.\n",
    "\n",
    "However in this notebook, we demonstrate how to deploy a model with custom inference code.\n",
    "\n",
    "In LMI contianer, we expect some artifacts to help setting up the model\n",
    "- `serving.properties` is the configuration file that can be used to configure the model server.\n",
    "- `requirements.txt` (optional) contains the pip wheel need to install in runtime\n",
    "\n",
    "For more details on the configuration options and an exhaustive list, you can refer the documentation - https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011bf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "option.dtype=fp16\n",
    "option.model_id=stabilityai/stablelm-base-alpha-7b\n",
    "option.tensor_parallel_degree=1\n",
    "option.enable_streaming=True\n",
    "option.low_cpu_mem_usage=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0142973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Start building SageMaker endpoint\n",
    "\n",
    "### Upload artifact on S3 and create SageMaker model\n",
    "\n",
    "The tarball that we created will be sent to an s3bucket that SageMaker created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1e5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_prefix = \"large-model-lmi/nocode\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "repo_name = \"djlserving-byoc\"\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{repo_name}\"\n",
    "env = {\"SERVING_DDB_CACHE\": \"true\"}  # use DynamoDB for response caching\n",
    "# extra env you can set\n",
    "# SERVING_DDB_BATCH=5 [default] writing to DDB every 5 tokens\n",
    "# DDB_TABLE_NAME=djl-page [default] DDB name is djl-page\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, env=env, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f39f6",
   "metadata": {},
   "source": [
    "### Create SageMaker endpoint\n",
    "\n",
    "Here, we use g5.2xlarge instance. The endpoint name is `lmi-model-deploy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fea064-cbaa-40e7-b36c-e65abd8b35c6",
   "metadata": {},
   "source": [
    "#### This step can take ~ 10 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e61cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-model\")\n",
    "print(f\"endpoint_name is {endpoint_name}\")\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26c736f",
   "metadata": {},
   "source": [
    "Let's define a few helper function, you will use `predic_async` to make an initial asynchronous invocation,\n",
    "and then use `fetch_next_page` to iterate through pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d7cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_async(input_data, endpoint_name):\n",
    "    data = serializers.JSONSerializer().serialize(input_data)\n",
    "    request_args = {\n",
    "        \"CustomAttributes\": \"x-synchronous=false\",\n",
    "        \"EndpointName\": endpoint_name,\n",
    "        \"Body\": data,\n",
    "    }\n",
    "\n",
    "    resp = sess.sagemaker_runtime_client.invoke_endpoint(**request_args)\n",
    "    return parse_response(resp)\n",
    "\n",
    "\n",
    "def fetch_next_page(next_page_id, endpoint_name):\n",
    "    input_data = {\"inputs\": \"fetch\"}\n",
    "    # adds \"x-max-items=X\", to limit max number items to return per page\n",
    "    # \"x-max-items\" is useful for returning image output\n",
    "    request_args = {\n",
    "        \"CustomAttributes\": f\"x-starting-token={next_page_id}\",\n",
    "        \"EndpointName\": endpoint_name,\n",
    "        \"Body\": serializers.JSONSerializer().serialize(input_data),\n",
    "    }\n",
    "    resp = sess.sagemaker_runtime_client.invoke_endpoint(**request_args)\n",
    "    return parse_response(resp)\n",
    "\n",
    "\n",
    "def get_next_page_id(resp):\n",
    "    custom_attr = resp[\"ResponseMetadata\"][\"HTTPHeaders\"].get(\"x-amzn-sagemaker-custom-attributes\")\n",
    "    if custom_attr:\n",
    "        return custom_attr.split(\"=\")[1]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_response(resp):\n",
    "    next_page_id = get_next_page_id(resp)\n",
    "    response_body = resp[\"Body\"].read().decode(\"utf-8\")\n",
    "    return (next_page_id, response_body)\n",
    "\n",
    "\n",
    "def parse_outputs(body):\n",
    "    result = []\n",
    "    if body:\n",
    "        lines = body.rstrip().split(\"\\n\")\n",
    "        for line in lines:\n",
    "            result.append(json.loads(line).get(\"outputs\"))\n",
    "    return result\n",
    "\n",
    "\n",
    "def merge_content(outputs, parsed_outputs):\n",
    "    for content in parsed_outputs:\n",
    "        for idx, token in enumerate(content):\n",
    "            outputs[idx] += token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63ee65",
   "metadata": {},
   "source": [
    "## Test and benchmark the inference\n",
    "\n",
    "In here, we use a SageMaker endpoint + DynamoDB simple fetcher to get the response result.\n",
    "\n",
    "- send prompt request and receive a x-next-token header\n",
    "- use x-starting-token to retrieve the streamed tokens with pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcef095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "input_text = [\"Large language model is\", \"Amazon is a company\"]\n",
    "input_data = {\"inputs\": input_text}\n",
    "\n",
    "outputs = [\"\" for _ in range(len(input_text))]\n",
    "next_page_id, body = predict_async(input_data, endpoint_name)\n",
    "\n",
    "merge_content(outputs, parse_outputs(body))\n",
    "\n",
    "while next_page_id:\n",
    "    next_page_id, body = fetch_next_page(next_page_id, endpoint_name)\n",
    "    merge_content(outputs, parse_outputs(body))\n",
    "    print(\"Fetching some tokens after 200ms...\")\n",
    "    time.sleep(0.2)\n",
    "\n",
    "for content in outputs:\n",
    "    print(\"\\nGenerated text: \" + content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Clean up the environment\n",
    "\n",
    "If you have lambda and API gateway environment, do the following to clean up:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d297acb-ef3c-4610-985e-a638a7694a06",
   "metadata": {},
   "source": [
    "Clean up the SageMaker endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
