HPO
===

Examples on how to use hyperparameter tuning on SageMaker.

Apache MXNet
===============

.. toctree::
   :maxdepth: 1

   ../hyperparameter_tuning/mxnet_mnist/hpo_mxnet_mnist
   ../hyperparameter_tuning/mxnet_gluon_cifar10_random_search/hyperparameter_tuning_mxnet_gluon_cifar10_random_search


PyTorch
============

.. toctree::
   :maxdepth: 1

   ../hyperparameter_tuning/pytorch_mnist/hpo_pytorch_mnist


R
===

.. toctree::
   :maxdepth: 1

   ../hyperparameter_tuning/r_bring_your_own/tune_r_bring_your_own

..
   notebook does not render (no title)
   Rapids
   =======

   .. toctree::
      :maxdepth: 1

      ../hyperparameter_tuning/rapids_bring_your_own/rapids_sagemaker_hpo


TensorFlow
============

.. toctree::
   :maxdepth: 1

   ../hyperparameter_tuning/tensorflow_mnist/hpo_tensorflow_mnist
   ../hyperparameter_tuning/keras_bring_your_own/hpo_bring_your_own_keras_container


XGBoost
=========

.. toctree::
   :maxdepth: 1

   ../hyperparameter_tuning/xgboost_direct_marketing/hpo_xgboost_direct_marketing_sagemaker_APIs
   ../hyperparameter_tuning/xgboost_direct_marketing/hpo_xgboost_direct_marketing_sagemaker_python_sdk
   ../hyperparameter_tuning/xgboost_random_log/hpo_xgboost_random_log


Use cases
=========

Analyze results
---------------

.. toctree::
   :maxdepth: 1

   ../hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results

Early stopping
--------------------

.. toctree::
   :maxdepth: 1

   ../hyperparameter_tuning/image_classification_early_stopping/hpo_image_classification_early_stopping


Warm start
--------------------

.. toctree::
   :maxdepth: 1

   ../hyperparameter_tuning/image_classification_warmstart/hpo_image_classification_warmstart
