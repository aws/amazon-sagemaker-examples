{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab981fc",
   "metadata": {},
   "source": [
    "# Classify SEC 10K/Q Filings to Industry Codes Based on the MDNA Text Column\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Objective\n",
    "\n",
    "The purpose of this notebook is to address the following question: Can we train a model to detect the broad industry category of a company from the text of Management Discussion & Analysis (**MD&A**) section in SEC filings? \n",
    "\n",
    "This notebook provides a template for the use of text data in U.S. Securities and Exchange Commission (SEC) filings, matching industry codes, adding NLP scores, and creating a *multimodal* training dataset. The multimodal dataset is then used for training a model for *multiclass* classification tasks.\n",
    "\n",
    "### Curating Input Data\n",
    "\n",
    "This example notebook demonstrates how to train a model on a synthetic training dataset that's curated using the SEC Forms retrieval tool provided by the [SageMaker JumpStart Industry Python SDK](https://pypi.org/project/smjsindustry/). You'll download a large number of SEC 10-K/Q forms for companies in the S&P 500 from 2000 to 2019. A separate column of the dataframe contains the **MD&A** section of the filings. The **MD&A** section is chosen because it is the most popular section used in the finance industry for natural language processing (NLP). The [SIC industry codes](https://www.osha.gov/data/sic-manual) are also used for matching to those in the [NAICS system](https://www.census.gov/naics/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df647a7",
   "metadata": {},
   "source": [
    ">**<span style=\"color:RED\">Legal Disclaimer</span>**: \n",
    ">This example notebook is for demonstrative purposes only. It is not financial advice and should not be relied on as financial or investment advice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2c383",
   "metadata": {},
   "source": [
    "### General Steps\n",
    "This notebook takes the following steps:\n",
    "1. Prepare training and testing datasets.\n",
    "2. Add NLP scores to the MD&A text features.\n",
    "3. Train the AutoGluon model for classification on the extended dataframe of MD&A text and NLP scores.\n",
    "4. Deploy the endpoint for model inference.\n",
    "5. Test the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501fd44",
   "metadata": {},
   "source": [
    "**Note**: You can also access this notebook through SageMaker JumpStart that is executable on SageMaker Studio. For more information, see [Amazon SageMaker JumpStart Industry](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart-industry.html) in the <em>Amazon SageMaker Developer Guide</em>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d89f97",
   "metadata": {},
   "source": [
    "## Kernel and SageMaker Setup\n",
    "\n",
    "Recommended kernel is **conda_python3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f64d2",
   "metadata": {},
   "source": [
    "Ensure AutoGluon images information is available in SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f2b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U \"sagemaker>=2.66\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76bc193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = session._region_name\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "mnist_folder = \"jumpstart_industry_mnist\"\n",
    "train_instance_type = \"ml.c5.2xlarge\"\n",
    "inference_instance_type = \"ml.m5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709b7ab",
   "metadata": {},
   "source": [
    "## Load Data, SDK, and Dependencies\n",
    "\n",
    "The following code cells download the `smjsindustry` SDK, dependencies, and dataset from an Amazon S3 bucket prepared by SageMaker JumpStart Industry. You will learn how to use the `smjsindustry` SDK which contains various APIs to curate SEC datasets. The dataset in this example was synthetically generated using the `smjsindustry` package's SEC Forms Retrieval tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_artifact_bucket = f\"jumpstart-cache-prod-{region}\"\n",
    "notebook_data_prefix = \"smfinance-notebook-data/mnist\"\n",
    "notebook_sdk_prefix = \"smfinance-notebook-dependency/smjsindustry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "data_bucket = f\"s3://{notebook_artifact_bucket}/{notebook_data_prefix}\"\n",
    "!aws s3 sync $data_bucket ./ --exclude \"*\" --include \"*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a15be",
   "metadata": {},
   "source": [
    "Install `smjsindustry` package from `whl` artifact running the following code block. Alternatively, we can also use `pip install smjsindustry==1.0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e0ea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install smjsindustry SDK\n",
    "sdk_bucket = f\"s3://{notebook_artifact_bucket}/{notebook_sdk_prefix}\"\n",
    "!aws s3 sync $sdk_bucket ./\n",
    "\n",
    "!pip install --no-index smjsindustry-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ebeff",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import tarfile\n",
    "import smjsindustry\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "from smjsindustry import NLPScoreType\n",
    "from smjsindustry import NLPScorer\n",
    "from smjsindustry import NLPScorerConfig\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from ag_model import (\n",
    "    AutoGluonTraining,\n",
    "    AutoGluonInferenceModel,\n",
    "    AutoGluonTabularPredictor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a02c21",
   "metadata": {},
   "source": [
    ">**Note**:\n",
    "Step 1 and Step 2 will show you how to preprocess the training data and how to add MD&A Text features and NLP scores. \n",
    "</br>You can also opt to use our provided preprocessed data `sample_train_nlp_scores.csv` and `sample_test_nlp_scores.csv` skip Step 1&2 and directly go to Step 3.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c5f4c",
   "metadata": {},
   "source": [
    "## Step 1: Prepare a Dataset\n",
    "\n",
    "Here, we read in the dataframe curated by the SEC Retriever that is already prepared as an example. The use of the Retriever is described in another notebook provided, `SEC_Retrieval_Summarizer_Scoring.ipynb`. The industry codes shown here correspond to those in the [NAICS system](https://www.census.gov/naics/). We also attached the industry codes from [Standard Industrial Classification (SIC) Manual](https://www.osha.gov/data/sic-manual).\n",
    "\n",
    "Because 10-K/Q firms are filed once a quarter, each firm shows up several instances in the dataset. \n",
    "When separating the dataset into train and test sets, we made sure that firms only appear in either the train or the test dataset, not in both. This ensures that the models are not able to memorize the text of a firm in the train dataset and then use it to classify firms in the test dataset. \n",
    "\n",
    "The classification task here appears trivial, but it is not; the MD&A section of the forms includes very long texts. In a separate analysis, we count the number of tokens (words) in each MD&A section for 12,144 filings, and obtain a mean of 5,307 tokens (sd=3,598 and interquartile range of 3140 to 6505). Transformer models, such as BERT, usually handle maximum sequence lengths of 512 or 1024 tokens. Therefore, it is unlikely that this classification task will benefit from recent advances in transformer models.\n",
    "\n",
    ">**<span style=\"color:RED\">Legal Disclaimer</span>**: \n",
    ">This example notebook uses data obtained from the SEC EDGAR database. You are responsible for complying with EDGARâ€™s access terms and conditions located in the [Accessing EDGAR Data](https://www.sec.gov/os/accessing-edgar-data) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866e1a5",
   "metadata": {},
   "source": [
    "#### Process the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# READ IN THE DATASETS (The file sizes are large. They are about 1 GB in total)\n",
    "train_df = pd.read_csv(\"sec_ind_train.csv\", low_memory=False)\n",
    "test_df = pd.read_csv(\"sec_ind_test.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d824e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the very small classes to simplify, if needed\n",
    "train_df = train_df[train_df.industry_code != \"C\"]\n",
    "train_df = train_df[train_df.industry_code != \"F\"]\n",
    "test_df = test_df[test_df.industry_code != \"C\"]\n",
    "test_df = test_df[test_df.industry_code != \"F\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8661f9",
   "metadata": {},
   "source": [
    "You can find in the following cells that there are over 11,000 for the train dataset and over 3,000 for the test dataset. Note that there's a label (class) imbalance underlying in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classes\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.groupby(\"industry_code\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupby(\"industry_code\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72407483",
   "metadata": {},
   "source": [
    "For demonstration purposes, take a sample from the original dataset to reduce the time for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_df = train_df.groupby(\"industry_code\", group_keys=False).apply(\n",
    "    pd.DataFrame.sample, n=80, random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_df.groupby(\"industry_code\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f71b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_df = test_df.groupby(\"industry_code\", group_keys=False).apply(\n",
    "    pd.DataFrame.sample, n=20, random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d75086",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_df.groupby(\"industry_code\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2cfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the smaller datasets for use\n",
    "sample_train_df.to_csv(\"sample_train.csv\", index=False)\n",
    "sample_test_df.to_csv(\"sample_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c6c99c",
   "metadata": {},
   "source": [
    "## Step 2: Add NLP scores to the MD&A Text Features\n",
    "\n",
    "Here we use the NLP scoring API to add three additional numerical features to the dataframe for a better classification performance. The columns will carry scores of the various attributes of the text. \n",
    "\n",
    "NLP scoring delivers a score as the fraction of words in a document that are in one of our internal scoring word lists. You can provide your own word list to calculate the NLP scores, such as negative, positive, risk, uncertainty, certainty, litigious, fraud and safe word lists. \n",
    "\n",
    "The approach taken here does not use human-curated word lists such as the popular dictionary from [Loughran and McDonald](https://sraf.nd.edu/textual-analysis/resources/), widely used in academia. Instead, the word lists here are generated from word embeddings trained on standard large text corpora where each word list comprises words that are close to the concept word (e.g. \"risk\") in embedding space. These word lists may contain words that a human may list out, but may still occur in the context of the concept word. \n",
    "\n",
    "You can also calculate your own scoring type by specifying a new word list.\n",
    "\n",
    "**Technical notes**:\n",
    "\n",
    "1. The data loader accesses a container to process the request. There might be some latency when starting up the container, which accounts for a few initial minutes. The actual filings extraction occurs after this. \n",
    "2. The data loader only supports processing jobs with only one instance at the moment.\n",
    "3. Users are not charged for the waiting time used when the instance is initializing (this takes 3-5 minutes). \n",
    "4. The name of the processing job is shown in the run time log.  \n",
    "5. You can also access the processing job from the [SageMaker console](https://console.aws.amazon.com/sagemaker). On the left navigation pane, choose Processing, Processing job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4d575",
   "metadata": {},
   "source": [
    "### Construct a SageMaker processor for NLP scoring\n",
    "\n",
    "The processing job runs on a `ml.c5.18xlarge` instance to reduce the running time. If `ml.c5.18xlarge` is not available in your AWS Region, change to a different CPU-based instance. If you encounter error messages that you've exceeded your quota, contact AWS Support to request a service limit increase for [SageMaker resources](https://console.aws.amazon.com/support/home#/) you want to scale up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f633b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CODE TO CALL THE SMJSINDUSTRY CONTAINER TO ADD NLP SCORE COLUMNS\n",
    "\n",
    "score_types = [NLPScoreType.POSITIVE, NLPScoreType.NEGATIVE, NLPScoreType.SAFE]\n",
    "\n",
    "score_type_list = list(NLPScoreType(score_type, []) for score_type in score_types)\n",
    "\n",
    "nlp_scorer_config = NLPScorerConfig(score_type_list)\n",
    "\n",
    "nlp_score_processor = NLPScorer(\n",
    "    role,  # loading job execution role\n",
    "    1,  # number of ec2 instances to run the loading job\n",
    "    \"ml.c5.18xlarge\",  # ec2 instance type to run the loading job\n",
    "    volume_size_in_gb=30,  # size in GB of the EBS volume to use\n",
    "    volume_kms_key=None,  # KMS key ID to encrypt the processing volume\n",
    "    output_kms_key=None,  # KMS key ID to encrypt processing job outputs\n",
    "    max_runtime_in_seconds=None,  # timeout in seconds. Default is 24 hours.\n",
    "    sagemaker_session=session,  # session object\n",
    "    tags=None,  # a list of key-value pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841864aa",
   "metadata": {},
   "source": [
    "### Run the NLP-scoring processing job on the training set\n",
    "\n",
    "The processing job takes around 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ed9d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp_score_processor.calculate(\n",
    "    nlp_scorer_config,\n",
    "    \"MDNA\",  # input column\n",
    "    \"sample_train.csv\",  # input from s3 bucket\n",
    "    \"s3://{}/{}/{}\".format(\n",
    "        bucket, mnist_folder, \"output\"\n",
    "    ),  # output s3 prefix (both bucket and folder names are required)\n",
    "    \"sample_train_nlp_scores.csv\",  # output file name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d0884",
   "metadata": {},
   "source": [
    "Examine the dataframe of the tabular-and-text (TabText) data.\n",
    "\n",
    "Note that it has a column for MD&A text, a categorical column for industry code, and three numerical columns (`POSITIVE`, `NEGATIVE`, and `SAFE`). In the next step, you'll use this multimodal dataset to train a model of AWS Gluon, which can accommodate the multimodal data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"s3\")\n",
    "client.download_file(\n",
    "    bucket,\n",
    "    \"{}/{}/{}\".format(mnist_folder, \"output\", \"sample_train_nlp_scores.csv\"),\n",
    "    \"sample_train_nlp_scores.csv\",\n",
    ")\n",
    "df = pd.read_csv(\"sample_train_nlp_scores.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97056f4",
   "metadata": {},
   "source": [
    "### Run the NLP-scoring processing job on the test set\n",
    "\n",
    "The processing job takes around 12 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_score_processor.calculate(\n",
    "    nlp_scorer_config,\n",
    "    \"MDNA\",  # input column\n",
    "    \"sample_test.csv\",  # input from s3 bucket\n",
    "    \"s3://{}/{}/{}\".format(\n",
    "        bucket, mnist_folder, \"output\"\n",
    "    ),  # output s3 prefix (both bucket and folder names are required)\n",
    "    \"sample_test_nlp_scores.csv\",  # output file name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ec6f0",
   "metadata": {},
   "source": [
    "Examine the dataframe of the TabText data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"s3\")\n",
    "client.download_file(\n",
    "    bucket,\n",
    "    \"{}/{}/{}\".format(mnist_folder, \"output\", \"sample_test_nlp_scores.csv\"),\n",
    "    \"sample_test_nlp_scores.csv\",\n",
    ")\n",
    "df = pd.read_csv(\"sample_test_nlp_scores.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11069436",
   "metadata": {},
   "source": [
    "## Step 3: Train the AutoGluon Model for Classification on the TabText Data Consists of the MD&A Texts, Industry Codes, and the NLP scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2455e8f3",
   "metadata": {},
   "source": [
    "#### The steps for training the AutoGluon classification model:\n",
    "\n",
    "1. Read in the extended TabText dataframes created in the previous code blocks. \n",
    "2. Normalize the NLP scores, as this usually helps improve the ML model.\n",
    "3. Upload the training, test dataset and config file to the session bucket.\n",
    "4. Train and evaluate the model in AutoGluon. See more details in the ***train.py***.\n",
    "5. Generate the leaderboard to examine all the different models for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%pylab inline\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Read in the prepared data files\n",
    "sample_train_nlp_df = pd.read_csv(\"sample_train_nlp_scores.csv\")\n",
    "sample_test_nlp_df = pd.read_csv(\"sample_test_nlp_scores.csv\")\n",
    "\n",
    "# Normalize the NLP score columns\n",
    "nlp_scores_names = [\"negative\", \"positive\", \"safe\"]\n",
    "for col in nlp_scores_names:\n",
    "    x = array(sample_train_nlp_df[col]).reshape(-1, 1)\n",
    "    sample_train_nlp_df[col] = scaler.fit_transform(x)\n",
    "    x = array(sample_test_nlp_df[col]).reshape(-1, 1)\n",
    "    sample_test_nlp_df[col] = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20164150",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_nlp_df.to_csv(\"train_data.csv\", index=False)\n",
    "sample_test_nlp_df.to_csv(\"test_data.csv\", index=False)\n",
    "\n",
    "train_s3_path = session.upload_data(\n",
    "    \"train_data.csv\", bucket=bucket, key_prefix=mnist_folder + \"/\" + \"data\"\n",
    ")\n",
    "test_s3_path = session.upload_data(\n",
    "    \"test_data.csv\", bucket=bucket, key_prefix=mnist_folder + \"/\" + \"data\"\n",
    ")\n",
    "config_s3_path = session.upload_data(\n",
    "    os.path.join(\"code\", \"config.yaml\"), bucket=bucket, key_prefix=mnist_folder + \"/\" + \"config\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76640d",
   "metadata": {},
   "source": [
    "#### Run a SageMaker training job\n",
    "\n",
    "The training job takes around 10 minutes with the sample dataset. If you want to train a model with your own data, you may need to update the training script `train.py` or configuration file `config.yaml` in the` code` folder. If you want to use a GPU instance to achieve a better accuracy, please replace `train_instance_type` with the desired GPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ff4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = AutoGluonTraining(\n",
    "    role=role,\n",
    "    entry_point=\"code/train.py\",\n",
    "    region=region,\n",
    "    instance_count=1,\n",
    "    instance_type=train_instance_type,\n",
    "    framework_version=\"0.3.1\",\n",
    "    py_version=\"py37\",\n",
    "    base_job_name=\"jumpstart-example-classic-gecko-mnist\",\n",
    "    enable_network_isolation=True,  # Set enable_network_isolation=True to ensure a security running environment\n",
    ")\n",
    "\n",
    "ag.fit(\n",
    "    {\"config\": config_s3_path, \"train\": train_s3_path, \"test\": test_s3_path},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6cff3",
   "metadata": {},
   "source": [
    "#### Download Model Outputs\n",
    "\n",
    "We download the following files (training job artifacts) from the SageMaker session's default S3 bucket:\n",
    "* `leaderboard.csv`\n",
    "* `predictions.csv`\n",
    "* `feature_importance.csv`\n",
    "* `evaluation.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "job_name = ag._current_job_name\n",
    "s3_client.download_file(bucket, f\"{job_name}/output/output.tar.gz\", \"output.tar.gz\")\n",
    "\n",
    "with tarfile.open(\"output.tar.gz\", \"r:gz\") as so:\n",
    "    so.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896ab57",
   "metadata": {},
   "source": [
    "#### Score details of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b958fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard = pd.read_csv(\"leaderboard.csv\")\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f189aa",
   "metadata": {},
   "source": [
    "#### The result of the training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc525d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evaluation.json\") as f:\n",
    "    data = json.load(f)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ddafa",
   "metadata": {},
   "source": [
    "#### Classification report and Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14107ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = sample_test_nlp_df[\"industry_code\"]\n",
    "y_pred = pd.read_csv(\"predictions.csv\")[\"industry_code\"]\n",
    "\n",
    "# Classification report\n",
    "# Labels are the classes, i.e., industry_code column in the dataset\n",
    "report_dict = classification_report(\n",
    "    y_true, y_pred, output_dict=True, labels=[\"B\", \"D\", \"E\", \"G\", \"H\", \"I\"]\n",
    ")\n",
    "report_dict.pop(\"accuracy\", None)\n",
    "report_dict_df = pd.DataFrame(report_dict).T\n",
    "print(report_dict_df)\n",
    "report_dict_df.to_csv(\"classification_report.csv\", index=True)\n",
    "\n",
    "# Confusion matrix\n",
    "# Labels are the classes, i.e., industry_code column in the dataset\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[\"B\", \"D\", \"E\", \"G\", \"H\", \"I\"])\n",
    "cm_df = pd.DataFrame(cm, [\"B\", \"D\", \"E\", \"G\", \"H\", \"I\"], [\"B\", \"D\", \"E\", \"G\", \"H\", \"I\"])\n",
    "sns.set(font_scale=1)\n",
    "cmap = \"coolwarm\"\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=cmap)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.show()\n",
    "plt.savefig(\"confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f36876",
   "metadata": {},
   "source": [
    "## Step 4: Deploy the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d783afd",
   "metadata": {},
   "source": [
    "In this step, we deploy the model artifact from **Step 3** and use for inference. We use `AutoGluonInferenceModel` defined in `ag_model.py` to create an AutoGluon model and [SageMaker model deployment](https://sagemaker.readthedocs.io/en/stable/frameworks/mxnet/using_mxnet.html#deploy-mxnet-models) APIs to deploy an endpoint. If you bring your own data for inference, you may also need to update the inference script `inference.py` in the `code` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc018e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = ag.latest_training_job.name\n",
    "print(\"Training job name: \", training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_estimator = Estimator.attach(training_job_name)\n",
    "ag_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a83438",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"jumpstart-example-classic-gecko-mnist-endpoint\"\n",
    "\n",
    "ag_model = AutoGluonInferenceModel(\n",
    "    model_data=ag.model_data,\n",
    "    role=role,\n",
    "    region=region,\n",
    "    framework_version=\"0.3.1\",\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"code/inference.py\",\n",
    "    predictor_cls=AutoGluonTabularPredictor,\n",
    "    name=\"jumpstart-example-classic-gecko-mnist-model\",\n",
    ")\n",
    "\n",
    "mnist_predictor = ag_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeaf227",
   "metadata": {},
   "source": [
    "## Step 5: Test the endpoint\n",
    "\n",
    "We randomly select some data from the test dataset and test the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e04969",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_endpoint_data = sample_test_nlp_df.sample(n=5).drop([\"industry_code\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12185903",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_endpoint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5641d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_predictor.predict(test_endpoint_data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abe03c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. We curated a TabText dataframe concatenating text, tabular, and categorical data.\n",
    "2. We demonstrated how to do ML on a TabText (multimodal) data using [AutoGluon](https://github.com/awslabs/autogluon).\n",
    "3. We showed how to deploy your trained model for inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa4503",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "After you are done using this notebook, delete the model artifacts and other resources to avoid any incurring charges.\n",
    "\n",
    ">**Caution:** You need to manually delete resources that you may have created while running the notebook, such as Amazon S3 buckets for model artifacts, training datasets, processing artifacts, and Amazon CloudWatch log groups.\n",
    "\n",
    "For more information about cleaning up resources, see [Clean Up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html) in the *Amazon SageMaker Developer Guide*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_predictor.delete_model()\n",
    "mnist_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42059a",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "The SageMaker JumpStart Industry product and its related materials are under the [Legal License Terms](https://jumpstart-cache-alpha-us-west-2.s3.us-west-2.amazonaws.com/smfinance-notebook-dependency/legal_file.txt)."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
