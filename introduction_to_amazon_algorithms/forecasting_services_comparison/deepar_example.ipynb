{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2545e4d4",
   "metadata": {},
   "source": [
    "# Time Series Modeling with Amazon Forecast and DeepAR on SageMaker - DeepAR on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c15e43",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Amazon offers customers a multitude of time series prediction services, including **DeepAR on SageMaker** and the fully managed service **Amazon Forecast**. Both services are similar in some aspects, yet differ in others. This notebook series aims to highlight the similarities and differences between both services by demonstrating how each service is used as well as describing the features each service offers. As a result, both notebooks in the series will use the same dataset. We will consider a real use case using the [Beijing Multi-Site Air-Quality Data Set](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data) which features hourly air pollutants data from 12 air-quality monitoring sites from March 1st, 2013 to February 28th, 2017, and is featured in the [[1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5627385/)] academic paper. This particular notebook will focus on **DeepAR on SageMaker**, and will:\n",
    "- Demonstrate how to train a DeepAR model on SageMaker\n",
    "- Create inferences from the DeepAR model\n",
    "\n",
    "One feature of **Amazon Forecast** is that the service can be used without any code. However, this notebook will outline how to use the service within a notebook format. Before you start, please note that training an **Amazon Forecast** may take several hours; this particular notebook took approximately `6 hours 30 minutes` to complete. Also, make sure that your SageMaker Execution Role has the following policies:\n",
    "\n",
    "- `AmazonSageMakerFullAccess`\n",
    "\n",
    "For convenience, here is an overview of the structure of this notebook:\n",
    "1. [Introduction](#Introduction)\n",
    " - [Preparation](#Preparation)\n",
    "2. [Data Preprocessing](#Data-Preprocessing)\n",
    " - [Data Import](#Data-Import)\n",
    " - [Data Visualization](#Data-Visualization)\n",
    " - [Train/Test Split](#Train/Test-Split)\n",
    " - [Upload to S3](#Upload-to-S3)\n",
    "3. [Model](#Model)\n",
    "7. [Resource Cleanup](#Resource-Cleanup)\n",
    "8. [Next Steps](#Next-Steps)\n",
    "\n",
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c0536",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ebd649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sagemaker\n",
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c776c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "s3_client = session.client(\"s3\")\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce92a20",
   "metadata": {},
   "source": [
    "All paths and resource names are defined below for a simple overview for where each resource will be located:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3235af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove paths if notebook was run before\n",
    "!rm -r data\n",
    "!rm -r deepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b66f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker.Session().default_bucket()\n",
    "sagemaker_sample_bucket = \"sagemaker-sample-files\"\n",
    "version = datetime.now().strftime(\"_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "dirs = [\"data\", \"deepar\", \"deepar/to_export\"]\n",
    "\n",
    "for dir_name in dirs:\n",
    "    os.makedirs(dir_name)\n",
    "\n",
    "dataset_s3_path = \"datasets/timeseries/beijing_air_quality/PRSA2017_Data_20130301-20170228.zip\"\n",
    "dataset_save_path = \"data/dataset.zip\"  # path where the zipped dataset is imported to\n",
    "dataset_path = \"data/dataset\"  # path where unzipped dataset is located\n",
    "deepar_export_path = \"deepar/to_export\"\n",
    "deepar_training_path = \"{}/training.json\".format(deepar_export_path)\n",
    "deepar_test_path = \"{}/test.json\".format(deepar_export_path)\n",
    "deepar_s3_training_path = \"deepar/train.json\"\n",
    "deepar_s3_test_path = \"deepar/test.json\"\n",
    "deepar_s3_output_path = \"deepar/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bb5ca",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "This section prepares the dataset for use in **DeepAR on SageMaker**. It will cover:\n",
    "- Target/Test dataset splitting\n",
    "- Target/Related time series splitting\n",
    "- S3 uploading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6c9df",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "\n",
    "This section will be demonstrating how to import data from an S3 bucket, but one can import their data whichever way is convenient. The data for this example will be imported from the `sagemaker-sample-files` **S3 Bucket**. \n",
    "\n",
    "\n",
    "To communicate with S3 outside of our console, we'll use the **Boto3** python3 library. More functionality between **Boto3** and **S3** can be found here: [Boto3 Amazon S3 Examples](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-examples.html)\n",
    "\n",
    "This particular dataset decompresses into a single folder named `PRSA_Data_20130301-20170228`. It contains 12 `csv` files, each containing air quality data for a single location. Each DataFrame will contain the following columns:\n",
    "- No: row number\n",
    "- year: year of data in this row\n",
    "- month: month of data in this row\n",
    "- day: day of data in this row\n",
    "- hour: hour of data in this row\n",
    "- PM2.5: PM2.5 concentration (ug/m^3)\n",
    "- PM10: PM10 concentration (ug/m^3)\n",
    "- SO2: SO2 concentration (ug/m^3)\n",
    "- NO2: NO2 concentration (ug/m^3)\n",
    "- CO: CO concentration (ug/m^3)\n",
    "- O3: O3 concentration (ug/m^3)\n",
    "- TEMP: temperature (degree Celsius)\n",
    "- PRES: pressure (hPa)\n",
    "- DEWP: dew point temperature (degree Celsius)\n",
    "- RAIN: precipitation (mm)\n",
    "- wd: wind direction\n",
    "- WSPM: wind speed (m/s)\n",
    "- station: name of the air-quality monitoring site\n",
    "\n",
    "#### Citations\n",
    "- Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.download_file(sagemaker_sample_bucket, dataset_s3_path, dataset_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data/dataset.zip -d data && mv data/PRSA_Data_20130301-20170228 data/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e554257",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    pd.read_csv(\"{}/{}\".format(dataset_path, file_name)) for file_name in os.listdir(dataset_path)\n",
    "]\n",
    "\n",
    "display(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8fe33",
   "metadata": {},
   "source": [
    "Both **SageMaker DeepAR** and **Amazon Forecast** use `datetime` objects for their time series cataloging, so we'll convert our `year`,`month`,`day`,`hour` columns into `datetime` column. Since we've represented these columns into our new `datetime` column, we can drop our `year`,`month`,`day`,`hour` columns from earlier. We can also drop the `No` column as our data is already in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dataset:\n",
    "    df.insert(0, \"datetime\", pd.to_datetime(df[[\"year\", \"month\", \"day\", \"hour\"]]))\n",
    "    df.drop(columns=[\"No\", \"year\", \"month\", \"day\", \"hour\"], inplace=True)\n",
    "\n",
    "display(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cf931",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "For this example, we'll use the temperature, or `TEMP` column, as our target variable to predict on. Let's first take a look at what each of our time series looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle(\"Target Values\")\n",
    "\n",
    "for i, axis in zip(range(len(dataset))[:6], axes.ravel()):\n",
    "    sns.lineplot(data=dataset[i], x=\"datetime\", y=\"TEMP\", ax=axis)\n",
    "    axis.set_title(dataset[i][\"station\"].iloc[0])\n",
    "    axis.set_ylabel(\"Temperature (Celsius)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac21d0",
   "metadata": {},
   "source": [
    "![Dataset Visual](./images/dataset_visual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c42ff",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4100a",
   "metadata": {},
   "source": [
    "Now we'll demonstrate how to use this dataset in **SageMaker DeepAR** and predict. \n",
    "\n",
    "SageMaker's DeepAR expects input in a JSON format with these specific fields for **each** time series:\n",
    "- `start`\n",
    "- `target`\n",
    "- `cat` (optional)\n",
    "- `dynamic_feat` (optional)\n",
    "\n",
    "Further information about the DeepAR input formatting can be found here: [DeepAR Input/Output Interface](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html#deepar-inputoutput).\n",
    "\n",
    "SageMaker DeepAR recommends a prediction length of `<=400` as large values decrease the algorithms accuracy and speed. Thus, let's set the length of our test time series and prediction length to the last two weeks of our data, or `14*24 = 336` observations. Useful information about best practices for DeepAR can be found here: [DeepAR Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html#deepar_best_practices). Since we have missing values in our time series, we must account for these. Luckily, DeepAR accepts missing values as long as they're `\"NaN\"` strings or encoded as `null` literals, as we will be exporting our time series to `JSON` to train the DeepAR model. One could also choose to replace all missing values with the mean of each time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = 14 * 24  # 14 days\n",
    "\n",
    "deepar_training = [\n",
    "    {\n",
    "        \"start\": str(df[\"datetime\"].min()),\n",
    "        \"target\": df[\"TEMP\"].fillna(\"NaN\").tolist()[:-prediction_length],\n",
    "    }\n",
    "    for df in dataset\n",
    "]\n",
    "\n",
    "deepar_test = [\n",
    "    {\"start\": str(df[\"datetime\"].min()), \"target\": df[\"TEMP\"].fillna(\"NaN\").tolist()}\n",
    "    for df in dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c5e5f",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "\n",
    "SageMaker DeepAR gets its data for training from S3, so we'll use the previously defined **Boto3 S3 Client** to upload our JSON files to **S3**. However, uploading files through the AWS console is another option and does not require code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf02e1",
   "metadata": {},
   "source": [
    "Let's define a function to export our dictionaries into JSON files to make our data properly input into SageMaker DeepAR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_json(path, data):\n",
    "    with open(path, \"wb\") as file_path:\n",
    "        for ts in data:\n",
    "            file_path.write(json.dumps(ts).encode(\"utf-8\"))\n",
    "            file_path.write(\"\\n\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b4e60",
   "metadata": {},
   "source": [
    "Now we can export our dictionaries in a JSON format into the paths we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dicts_to_json(deepar_training_path, deepar_training)\n",
    "write_dicts_to_json(deepar_test_path, deepar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c92d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(deepar_training_path, bucket, deepar_s3_training_path)\n",
    "s3_client.upload_file(deepar_test_path, bucket, deepar_s3_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a35586",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a538b",
   "metadata": {},
   "source": [
    "Now that we've formatted our data properly, we can train our model. When initializing our estimator, we must specify an instance type. Available options as well as pricing can be viewed here: [Available SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/). We also need to pass an Image URI to specify which algorithm we want to use, as well as pass required parameters to our `Estimator`. Further documentation on retrieving Image URIs and the `sagemaker.estimator.Estimator` class can be found here:\n",
    "\n",
    "- [Image URI API](https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html)\n",
    "\n",
    "- [SageMaker Estimator API](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)\n",
    "\n",
    "In this case, it was found that an `ml.c5.2xlarge` had the minimum amount of memory required for the training to complete, but one should use any instance type that fits their use case. In addition, using faster **EC2** instances may in some cases be cheaper than using the minimum required as the model will take less time to train. Amazon SageMaker also offers discounted EC2 pricing if Amazon EC2 Spot instances are used, which is unused EC2 capacity in the AWS cloud. This can be toggled with the `use_spot_instances` parameter. Further information on Managed Spot Training can be found here: [Model Managed Spot Training](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.2xlarge\",\n",
    "    base_job_name=\"DEMO-DeepAR\",\n",
    "    use_spot_instances=False,\n",
    "    output_path=\"s3://{}/{}\".format(bucket, deepar_s3_output_path),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e946cf",
   "metadata": {},
   "source": [
    "Now we need to configure the DeepAR instance's hyperparameters to our specific needs. There are four required hyperparameters that we must define, but there are 16 total tunable hyperparameters. All tunable hyperparameters and detailed descriptions can be found here: [DeepAR Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccca27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": \"50\",\n",
    "    \"time_freq\": \"H\",\n",
    "    \"prediction_length\": prediction_length,\n",
    "    \"context_length\": prediction_length,\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2588ac",
   "metadata": {},
   "source": [
    "After setting the hyperparameters, we can train our model. One run of the training job took `1543 seconds`, or approximately `25 minutes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49600979",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": \"s3://{}/{}\".format(bucket, deepar_s3_training_path),\n",
    "        \"test\": \"s3://{}/{}\".format(bucket, deepar_s3_test_path),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ef60f",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce910d8",
   "metadata": {},
   "source": [
    "After training our model, we must initialize an endpoint to call our model. This particular endpoint uses an `ml.c5.large` instance and took `3 minutes 2 seconds` to initialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "job_name = estimator.latest_training_job.name\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c5.large\",\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19629ff",
   "metadata": {},
   "source": [
    "Then, we can initialize a predictor from our endpoint to receive time series predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fcbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=endpoint_name, sagemaker_session=sagemaker_session, serializer=JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f1127",
   "metadata": {},
   "source": [
    "**DeepAR** requires our request be in a `JSON` request format as input to receive predictions. The following example is from the [DeepAR JSON Request Formats](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html) documentation page where request definition is outlined:\n",
    "\n",
    "    {\n",
    "        \"instances\": [\n",
    "            {\n",
    "                \"start\": \"2009-11-01 00:00:00\",\n",
    "                \"target\": [4.0, 10.0, \"NaN\", 100.0, 113.0],\n",
    "                \"cat\": [0, 1],\n",
    "                \"dynamic_feat\": [[1.0, 1.1, 2.1, 0.5, 3.1, 4.1, 1.2, 5.0, ...]]\n",
    "            },\n",
    "            {\n",
    "                \"start\": \"2012-01-30\",\n",
    "                \"target\": [1.0],\n",
    "                \"cat\": [2, 1],\n",
    "                \"dynamic_feat\": [[2.0, 3.1, 4.5, 1.5, 1.8, 3.2, 0.1, 3.0, ...]]\n",
    "            },\n",
    "            {\n",
    "                \"start\": \"1999-01-30\",\n",
    "                \"target\": [2.0, 1.0],\n",
    "                \"cat\": [1, 3],\n",
    "                \"dynamic_feat\": [[1.0, 0.1, -2.5, 0.3, 2.0, -1.2, -0.1, -3.0, ...]]\n",
    "            }\n",
    "        ],\n",
    "        \"configuration\": {\n",
    "             \"num_samples\": 50,\n",
    "             \"output_types\": [\"mean\", \"quantiles\", \"samples\"],\n",
    "             \"quantiles\": [\"0.5\", \"0.9\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "Only types specified in the request will be present in the predictor's response. Valid values for the `output_types` field are: `\"mean\"`,`\"quantiles\"`, and `\"samples\"`. Furthermore, the `\"cat\"` and/or `\"dynamic_feat\"` fields of each instance should be omitted if these fields were not used to train the model. Let's define our request, where we'll request predictions for the `0.1`, `0.5`, and `0.9` quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ee82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_input = {\n",
    "    \"instances\": deepar_training,\n",
    "    \"configuration\": {\"output_types\": [\"quantiles\"], \"quantiles\": [\"0.1\", \"0.5\", \"0.9\"]},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d5f09",
   "metadata": {},
   "source": [
    "Finally, we can obtain a prediction from our model for the `prediction_length` number of instances following our requested time series, and conforming to the `time_freq` (time frequency) specified in our hyperparameters. This prediction took approximately `8 seconds` to receive a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prediction = predictor.predict(predictor_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b9d75",
   "metadata": {},
   "source": [
    "### Interpreting Results\n",
    "The resulting prediction will come in a `JSON` format. The response is within a dictionary formatted like so: [DeepAR JSON Response Formats](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html#deepar-json-response). The following example is from the previously mentioned page:\n",
    "\n",
    "    {\n",
    "        \"predictions\": [\n",
    "            {\n",
    "                \"quantiles\": {\n",
    "                    \"0.9\": [...],\n",
    "                    \"0.5\": [...]\n",
    "                },\n",
    "                \"samples\": [...],\n",
    "                \"mean\": [...]\n",
    "            },\n",
    "            {\n",
    "                \"quantiles\": {\n",
    "                    \"0.9\": [...],\n",
    "                    \"0.5\": [...]\n",
    "                },\n",
    "                \"samples\": [...],\n",
    "                \"mean\": [...]\n",
    "            },\n",
    "            {\n",
    "                \"quantiles\": {\n",
    "                    \"0.9\": [...],\n",
    "                    \"0.5\": [...]\n",
    "                },\n",
    "                \"samples\": [...],\n",
    "                \"mean\": [...]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "Let's define a method to help us decode the predictor's `JSON` response and load it onto a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_df(response):\n",
    "\n",
    "    data = json.loads(response)\n",
    "    dataframes = []\n",
    "\n",
    "    for ts in data[\"predictions\"]:\n",
    "        if \"quantiles\" in ts:\n",
    "            # Since the quantiles response comes in a list within the dictionary, we will append the quantiles\n",
    "            # dictionary of each time series to the mean and samples(if requested) of those respective time series\n",
    "            ts.update(ts[\"quantiles\"])\n",
    "            ts.pop(\"quantiles\")\n",
    "        dataframes.append(pd.DataFrame(data=ts))\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8a7b5",
   "metadata": {},
   "source": [
    "Now that we've obtained our predictions(that came in a `JSON` format) and defined a method to decode these predictions, we can see our results in a pandas `DataFrame` format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27834f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepar_results = prediction_to_df(prediction)\n",
    "\n",
    "display(deepar_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31909e",
   "metadata": {},
   "source": [
    "Let's visualize our predictions after acquisition. We'll plot our first station to see how we did. First, let's append our `target` values to our results for convenient comparison. Then, we'll plot all requested quantiles onto the same plot with the `target` values to see how DeepAR did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = []\n",
    "\n",
    "for i in range(len(deepar_results)):\n",
    "    temp = pd.concat(\n",
    "        [\n",
    "            dataset[i][[\"TEMP\", \"datetime\", \"station\"]]\n",
    "            .tail(prediction_length)\n",
    "            .reset_index(drop=True),\n",
    "            deepar_results[i],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    temp = temp.rename(columns={\"TEMP\": \"target\"})\n",
    "    df_results.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a9639",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_comparison(query):\n",
    "    sns.set_style(\"dark\")\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.plot(query[\"datetime\"], query[\"0.1\"], color=\"r\", lw=1)\n",
    "    plt.plot(query[\"datetime\"], query[\"0.5\"], color=\"orange\", linestyle=\":\", lw=2)\n",
    "    plt.plot(query[\"datetime\"], query[\"0.9\"], color=\"r\", lw=1)\n",
    "    plt.plot(query[\"datetime\"], query[\"target\"], color=\"b\", lw=1)\n",
    "    plt.fill_between(\n",
    "        query[\"datetime\"].tolist(),\n",
    "        query[\"0.9\"].tolist(),\n",
    "        query[\"0.1\"].tolist(),\n",
    "        color=\"y\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    plt.title(query[\"station\"][0])\n",
    "    plt.xlabel(\"Datetime\")\n",
    "    plt.ylabel(\"Temperature (Celsius)\")\n",
    "\n",
    "    plt.legend([\"10% Quantile\", \"50% Quantile\", \"90% Quantile\", \"Target\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58983bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(df_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd916e43",
   "metadata": {},
   "source": [
    "![DeepAR Results](./images/deepar_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e142f63",
   "metadata": {},
   "source": [
    "As we can see, the 0.1 and 0.9 quantiles create an 80% confidence interval for our predictions, which our target generally stays within. However, as mentioned in the [DeepAR Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html#deepar_best_practices), our confidence interval becomes less accurate towards the end due to our relatively high `prediction_length` value. To remediate this, lowering the frequency of data, such as changing `1min` to `5min`, or `H` to `D` (hourly to daily), is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1fac49",
   "metadata": {},
   "source": [
    "## Resource Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64522eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938388d9",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "This notebook illustrates the features offered by **DeepAR on SageMaker**, and is part of the [Time Series Modeling with Amazon Forecast and DeepAR on SageMaker](.) series. The notebook series aims to demonstrate how to use the **Amazon Forecast** and **DeepAR on SageMaker** time series modeling services as well as outline their features. Be sure to read the [Amazon Forecast](./forecast_example.ipynb) example, and view a top-level comparison of both services in the [README](./README.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
