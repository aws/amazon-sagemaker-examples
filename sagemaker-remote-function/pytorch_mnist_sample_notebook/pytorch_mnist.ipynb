{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train a PyTorch model with MNIST dataset.\n",
    "\n",
    "The notebook shows how to use the @remote and RemoteExecutor introduced to SageMaker SDK\n",
    "to train Pytorch models with remote jobs and track the training as SageMaker experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install the dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "%pip install -r ./requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.experiments.run import Run, load_run\n",
    "from sagemaker.remote_function import remote, RemoteExecutor\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "s3_root_folder = f's3://{sm_session.default_bucket()}/lc2j_demo/pytorch_mnist'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the MNIST data\n",
    "\n",
    "Download data to ./data folder, load and normalize them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "datasets.MNIST.mirrors = [\n",
    "    \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/\"\n",
    "]\n",
    "\n",
    "train_set = datasets.MNIST(\n",
    "    \"./data\",\n",
    "    train=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    ),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_set = datasets.MNIST(\n",
    "    \"./data\",\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    ),\n",
    "    download=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the model architecture and training logic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Define the CNN architecture.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model by iterating through the data once\n",
    "# If dry_run is set to True, it only goes through one batch of the data set\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval, dry_run):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test the trained model using the test data set and also log test metrics to SageMaker experiment runs.\n",
    "def check_performance(run, model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    # log metrics\n",
    "    run.log_metric(name=\"test:loss\", value=test_loss, step=epoch)\n",
    "    run.log_metric(name=\"test:accuracy\", value=test_accuracy, step=epoch)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model for specified number of epochs and test the performance after each epoch.\n",
    "def perform_train(train_data,\n",
    "                  test_data,\n",
    "                  *,\n",
    "                  batch_size: int = 64,\n",
    "                  test_batch_size: int = 1000,\n",
    "                  epochs: int = 3,\n",
    "                  lr: float = 1.0,\n",
    "                  gamma: float = 0.7,\n",
    "                  no_cuda: bool = True,\n",
    "                  no_mps: bool = True,\n",
    "                  dry_run: bool = False,\n",
    "                  seed: int = 1,\n",
    "                  log_interval: int = 10,\n",
    "                  ):\n",
    "    \"\"\"PyTorch MNIST Example\n",
    "\n",
    "    :param train_data: the training data set\n",
    "    :param test_data: the test data set\n",
    "    :param batch_size: input batch size for training (default: 64)\n",
    "    :param test_batch_size: input batch size for testing (default: 1000)\n",
    "    :param epochs: number of epochs to train (default: 14)\n",
    "    :param lr: learning rate (default: 1.0)\n",
    "    :param gamma: Learning rate step gamma (default: 0.7)\n",
    "    :param no_cuda: disables CUDA training\n",
    "    :param no_mps: disables macOS GPU training\n",
    "    :param dry_run: quickly check a single pass\n",
    "    :param seed: random seed (default: 1)\n",
    "    :param log_interval: how many batches to wait before logging training status\n",
    "    :return: the trained model\n",
    "    \"\"\"\n",
    "\n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "    use_mps = not no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    test_kwargs = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "    # load the experiment run from the context\n",
    "    with load_run() as run:\n",
    "\n",
    "        run.log_parameters({\n",
    "            \"epochs\": epochs,\n",
    "            \"lr\": lr,\n",
    "            \"gamma\": gamma\n",
    "        })\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, device, train_loader, optimizer, epoch, log_interval, dry_run)\n",
    "            check_performance(run, model, device, test_loader, epoch)\n",
    "            scheduler.step()\n",
    "\n",
    "        # log confusion matrix\n",
    "        with torch.no_grad():\n",
    "            data, target = next(iter(test_loader))\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            run.log_confusion_matrix(target, pred, \"confusion-matrix-test-data\")\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execute and test the training function locally\n",
    "\n",
    "Set the dry_run to `True` and execute the function locally to verify the correctness of the code and dependencies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with Run(experiment_name=\"local-tests\", run_name=\"local-tests\", sagemaker_session=sm_session) as run:\n",
    "    trained_model = perform_train(train_set, test_set, epochs=1, dry_run=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execute the function remotely in parallel\n",
    "\n",
    "Here we run a series of jobs to explore how the hyperparameter `lr` impact the performance.\n",
    "The argument `max_parallel_job` controls the max number of jobs that can run in parallel.\n",
    "We also take advantage of warm pooling to reduce the job start up time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sagemaker.remote_function import RemoteExecutor\n",
    "\n",
    "with RemoteExecutor(max_parallel_job=2, keep_alive_period_in_seconds=60,\n",
    "                    s3_root_uri=s3_root_folder,\n",
    "                    # Set the default region for boto3 to use the SageMaker Experiment APIs in the function.\n",
    "                    # This will not be required after public release\n",
    "                    environment_variables={\"AWS_DEFAULT_REGION\": sm_session.boto_region_name}) as executor:\n",
    "    futures = {}\n",
    "    for run_name, lr in [(\"run-0\", 0.3), (\"run-1\", 1), (\"run-2\", 3.0)]:\n",
    "        with Run(experiment_name=\"pytorch-mnist\", run_name=run_name, sagemaker_session=sm_session) as run:\n",
    "            run.log_artifact(name=\"raw_data\", value=\"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/\", is_output=False)\n",
    "            futures[run_name] = executor.submit(perform_train, train_set, test_set, lr=lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Go to SageMaker Studio to view the experiment runs\n",
    "\n",
    "![charts of run-0](./images/experiment_snapshot.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trained_models = [future.result() for future in futures.values()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execute the function remotely with function decorator\n",
    "\n",
    "Note that the duplication of the `perform_train` function implementation is only for demonstration purpose. `@remote` can be applied to the original `perform_train` once the local test runs successfully."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sagemaker.remote_function import remote\n",
    "\n",
    "@remote(\n",
    "    # Set the default region for boto3 to use the SageMaker experiment APIs in the function\n",
    "    # This will not be required after public release\n",
    "    environment_variables={\"AWS_DEFAULT_REGION\": sm_session.boto_region_name}\n",
    ")\n",
    "def perform_train(train_data,\n",
    "                  test_data,\n",
    "                  *,\n",
    "                  batch_size: int = 64,\n",
    "                  test_batch_size: int = 1000,\n",
    "                  epochs: int = 3,\n",
    "                  lr: float = 1.0,\n",
    "                  gamma: float = 0.7,\n",
    "                  no_cuda: bool = True,\n",
    "                  no_mps: bool = True,\n",
    "                  dry_run: bool = False,\n",
    "                  seed: int = 1,\n",
    "                  log_interval: int = 10,\n",
    "                  ):\n",
    "    \"\"\"PyTorch MNIST Example\n",
    "\n",
    "    :param train_data: the training data set\n",
    "    :param test_data: the test data set\n",
    "    :param batch_size: input batch size for training (default: 64)\n",
    "    :param test_batch_size: input batch size for testing (default: 1000)\n",
    "    :param epochs: number of epochs to train (default: 14)\n",
    "    :param lr: learning rate (default: 1.0)\n",
    "    :param gamma: Learning rate step gamma (default: 0.7)\n",
    "    :param no_cuda: disables CUDA training\n",
    "    :param no_mps: disables macOS GPU training\n",
    "    :param dry_run: quickly check a single pass\n",
    "    :param seed: random seed (default: 1)\n",
    "    :param log_interval: how many batches to wait before logging training status\n",
    "    :return: the trained model\n",
    "    \"\"\"\n",
    "\n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "    use_mps = not no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    test_kwargs = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "    # load the experiment run from the context\n",
    "    with load_run() as run:\n",
    "        run.log_parameters({\n",
    "            \"epochs\": epochs,\n",
    "            \"lr\": lr,\n",
    "            \"gamma\": gamma\n",
    "        })\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, device, train_loader, optimizer, epoch, log_interval, dry_run)\n",
    "            check_performance(run, model, device, test_loader, epoch)\n",
    "            scheduler.step()\n",
    "\n",
    "        # log confusion matrix\n",
    "        with torch.no_grad():\n",
    "            data, target = next(iter(test_loader))\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            run.log_confusion_matrix(target, pred, \"confusion-matrix-test-data\")\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with Run(experiment_name=\"pytorch-mnist-decorator\", run_name=\"run-1\", sagemaker_session=sm_session) as run:\n",
    "    trained_model = perform_train(train_set, test_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
