{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track an Airflow Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses [fashion-mnist dataset](https://www.tensorflow.org/datasets/catalog/fashion_mnist) classification task as an example to show how one can track Airflow Workflow executions using Sagemaker Experiments.\n",
    "\n",
    "Overall, the notebook is organized as follow:\n",
    "\n",
    "1. Download dataset and upload to Amazon S3.\n",
    "2. Create a simple CNN model to do the classification.\n",
    "3. Define the workflow as a DAG with two executions, a SageMaker TrainingJob for training the CNN model, followed by a SageMaker TransformJob to run batch predictions on model.\n",
    "4. Host and run the workflow locally, and track the workflow run as an Experiment.\n",
    "5. List executions. \n",
    "\n",
    "Note that if you are running the notebook in SageMaker Studio, please select `Python3 (Tensorflow CPU Optimized)` Kernel; if you are running in SageMaker Notebook, please select `conda_tensorflow_py36` kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# append source code directory\n",
    "sys.path.insert(0, os.path.abspath('./code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip uninstall -y enum34\n",
    "!{sys.executable} -m pip install werkzeug==0.15.4\n",
    "!{sys.executable} -m pip install apache-airflow\n",
    "!{sys.executable} -m pip install sagemaker-experiments\n",
    "!{sys.executable} -m pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "from model import get_model\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "sagemaker_sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a S3 bucket to hold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a s3 bucket to hold data, note that your account might already created a bucket with the same name\n",
    "account_id = sess.client('sts').get_caller_identity()[\"Account\"]\n",
    "bucket = 'sagemaker-experiments-{}-{}'.format(sess.region_name, account_id)\n",
    "prefix = 'fashion-mnist'\n",
    "\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(\n",
    "            Bucket=bucket,\n",
    "            CreateBucketConfiguration={'LocationConstraint': sess.region_name}\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the fashion-mnist dataset\n",
    "# the dataset will be downloaded to ~/.datasets/\n",
    "!aws s3 sync s3://sagemaker-sample-files/datasets/image/fashion-MNIST/ ~/.datasets/fashion-mnist/\n",
    "with gzip.open(os.path.expanduser('~/.datasets/fashion-mnist/train-labels-idx1-ubyte.gz'), 'rb') as y_train_path:\n",
    "    y_train = np.frombuffer(y_train_path.read(), np.uint8, offset=8)\n",
    "with gzip.open(os.path.expanduser('~/.datasets/fashion-mnist/train-images-idx3-ubyte.gz'), 'rb') as x_train_path:\n",
    "    x_train = np.frombuffer(x_train_path.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28)\n",
    "with gzip.open(os.path.expanduser('~/.datasets/fashion-mnist/t10k-labels-idx1-ubyte.gz'), 'rb') as y_test_path:\n",
    "    y_test = np.frombuffer(y_test_path.read(), np.uint8, offset=8)\n",
    "with gzip.open(os.path.expanduser('~/.datasets/fashion-mnist/t10k-images-idx3-ubyte.gz'), 'rb') as x_test_path:\n",
    "    x_test = np.frombuffer(x_test_path.read(), np.uint8, offset=16).reshape(len(y_test), 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image example\n",
    "plt.imshow(x_train[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating a SageMaker Training Job and fitting by `(x_train, y_train)`, and then a SageMaker Transform Job to perform batch inference over a large-scale (10K) test data. To do the batch inference, we need first flatten each sampl image (28x28) in `x_test` into an float array with 784 features, and then concatenate all flattened samples into a `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
    "np.savetxt('./x_test.csv', x_test_flat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the dataset to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload training data to s3\n",
    "# you may need to modifiy the path to .datasets dir\n",
    "train_input = S3Uploader.upload(\n",
    "    local_path=f'{os.path.expanduser(\"~\")}/.datasets/fashion-mnist/',\n",
    "    desired_s3_uri=f\"s3://{bucket}/{prefix}/data/train\",\n",
    "    sagemaker_session=sagemaker_sess,\n",
    ")\n",
    "print('train input spec: {}'.format(train_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload test data to s3 for batch inference\n",
    "test_input = S3Uploader.upload(\n",
    "    local_path='./x_test.csv', \n",
    "    desired_s3_uri=f\"s3://{bucket}/{prefix}/data/test\",\n",
    "    sagemaker_session=sagemaker_sess,\n",
    ")\n",
    "print('test input spec: {}'.format(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple CNN\n",
    "\n",
    "The CNN we use in this example contains two consecutive (Conv2D - MaxPool - Dropout) modules, followed by a feed-forward layer, and a softmax layer to normalize the output into a valid probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use default parameters\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create workflow configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of demonstration, we will be executing our workflow locally. Lets first create a dir under airflow root to store our DAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.expanduser('~/airflow')):\n",
    "    # to generate airflow dir\n",
    "    !airflow -h\n",
    "\n",
    "if not os.path.exists(os.path.expanduser('~/airflow/dags')):\n",
    "    !mkdir {os.path.expanduser('~/airflow/dags')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an experiment named `fashion-mnist-classification-experiment` to track our workflow execution first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment.create(\n",
    "    experiment_name=f\"fashion-mnist-classification-experiment\",\n",
    "    description=\"An classification experiment on fashion mnist dataset using tensorflow framework.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines our DAG, which is a workflow with two steps. One is running a training job on SageMaker, then followed by running a transform job to perform batch inference on the fashion-mnist testset we created before. \n",
    "\n",
    "We will write the DAG defnition into the `airflow/dags` we just created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/fashion-mnist-dag.py\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "from sagemaker.workflow.airflow import training_config, transform_config_from_estimator\n",
    "\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "experiment_name = \"fashion-mnist-classification-experiment\"\n",
    "\n",
    "sess = boto3.Session()\n",
    "account_id = sess.client('sts').get_caller_identity()[\"Account\"]\n",
    "bucket_name = 'sagemaker-experiments-{}-{}'.format(sess.region_name, account_id)\n",
    "\n",
    "# for training job\n",
    "train_input = f\"s3://{bucket_name}/fashion-mnist/data/train\"\n",
    "# for batch transform job\n",
    "test_input = f\"s3://{bucket_name}/fashion-mnist/data/test/x_test.csv\"\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "base_job_name = 'fashion-mnist-cnn'\n",
    "\n",
    "py_version = 'py3'\n",
    "tf_framework_version = '1.13'\n",
    "\n",
    "# callable for SageMaker training in TensorFlow\n",
    "def train(data, **context):\n",
    "    estimator = TensorFlow(\n",
    "        base_job_name=base_job_name,\n",
    "        source_dir=\"code\",\n",
    "        entry_point='train.py',\n",
    "        role=role,\n",
    "        framework_version=tf_framework_version,\n",
    "        py_version=py_version,\n",
    "        hyperparameters={\n",
    "            'epochs': 10, \n",
    "            'batch-size' : 256\n",
    "        },\n",
    "        train_instance_count=1, \n",
    "        train_instance_type=\"ml.m4.xlarge\"\n",
    "    )\n",
    "    estimator.fit(data, experiment_config={\"ExperimentName\": experiment_name, \"TrialComponentDisplayName\": \"Training\"})\n",
    "    return estimator.latest_training_job.job_name\n",
    "\n",
    "\n",
    "# callable for SageMaker batch transform\n",
    "def transform(data, **context):\n",
    "    training_job = context['ti'].xcom_pull(task_ids='training')\n",
    "    estimator = TensorFlow.attach(training_job)\n",
    "    # create a model\n",
    "    tensorflow_serving_model = Model(\n",
    "        model_data=estimator.model_data,\n",
    "        role=role,\n",
    "        framework_version=tf_framework_version,\n",
    "        sagemaker_session=sagemaker.Session(),\n",
    "    )\n",
    "    transformer = tensorflow_serving_model.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m4.xlarge\",\n",
    "        max_concurrent_transforms=5,\n",
    "        max_payload=1,\n",
    "    )\n",
    "    transformer.transform(\n",
    "        data, \n",
    "        job_name=f\"{base_job_name}-{int(time.time())}\", \n",
    "        content_type='text/csv', \n",
    "        split_type=\"Line\", \n",
    "        experiment_config={\"ExperimentName\": experiment_name, \"TrialComponentDisplayName\": \"Transform\"}\n",
    "    )\n",
    "\n",
    "    \n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': airflow.utils.dates.days_ago(2),\n",
    "    'provide_context': True\n",
    "}\n",
    "\n",
    "dag = DAG('fashion-mnist', default_args=default_args, schedule_interval='@once')\n",
    "\n",
    "train_op = PythonOperator(\n",
    "    task_id='training',\n",
    "    python_callable=train,\n",
    "    op_args=[train_input],\n",
    "    provide_context=True,\n",
    "    dag=dag)\n",
    "\n",
    "transform_op = PythonOperator(\n",
    "    task_id='transform',\n",
    "    python_callable=transform,\n",
    "    op_args=[test_input],\n",
    "    provide_context=True,\n",
    "    dag=dag)\n",
    "\n",
    "transform_op.set_upstream(train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets init the airflow db and host it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!airflow initdb\n",
    "!airflow webserver -p 8080 -D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we start a backfill job to execute our workflow. Note, we use backfill job simply because we dont want to wait until the airflow scheduler to trigger the workflow to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!airflow backfill fashion-mnist -s 2020-01-01 --reset_dagruns -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List workflow executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each execution in the workflow is modeled by a trial, lets list our workflow executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executions = experiment.list_trials(\n",
    "    sort_by=\"CreationTime\", \n",
    "    sort_order=\"Ascending\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execs_details = []\n",
    "for exe in executions:\n",
    "    execs_details.append([exe.trial_name, exe.trial_source['SourceArn'], exe.creation_time])\n",
    "execs_table = pd.DataFrame(execs_details, columns=['Name', 'Source', 'CreationTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the jobs we created and executed by our workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = ExperimentAnalytics(\n",
    "    sagemaker_session=sagemaker_sess, \n",
    "    experiment_name=experiment.experiment_name,\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleanup\n",
    "\n",
    "Run the following cell to clean up the sample experiment, if you are working on your own experiment, please ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(experiment):\n",
    "    for trial_summary in experiment.list_trials():\n",
    "        trial = Trial.load(sagemaker_boto_client=sm, trial_name=trial_summary.trial_name)\n",
    "        for trial_component_summary in trial.list_trial_components():\n",
    "            tc = TrialComponent.load(\n",
    "                sagemaker_boto_client=sm,\n",
    "                trial_component_name=trial_component_summary.trial_component_name)\n",
    "            trial.remove_trial_component(tc)\n",
    "            try:\n",
    "                # comment out to keep trial components\n",
    "                tc.delete()\n",
    "            except:\n",
    "                # tc is associated with another trial\n",
    "                continue\n",
    "            # to prevent throttling\n",
    "            time.sleep(.5)\n",
    "        trial.delete()\n",
    "    experiment.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup(experiment)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
