{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Your Data (Part 2/4)\n",
    "Download | **Structure** | Preprocessing | Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will properly structure your image files for ingestion by SageMaker Built-in Algorithms, TensorFlow or PyTorch data loaders. To do this, we will split out data into train, validation and test sets. Then, we will use Python to create the new folder structure and copy the files into the correct set and label folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* #### [Proper folder structure](#ipg2.1)\n",
    "* #### [Load annotation category labels](#ipg2.2)\n",
    "* #### [Make train, validation and test splits](#ipg2.3)\n",
    "* #### [Make new folder structure](#ipg2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg2.1'></a>\n",
    "## Proper folder structure\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although most tools can accommodate data in any file structure with enough tinkering, it makes most sense to use the sensible defaults that frameworks like MXNet, TensorFlow and PyTorch all share to make data ingestion as smooth as possible. By default, most tools will look for image data in the file structure depicted below:\n",
    "```\n",
    "+-- train\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|\n",
    "+-- val\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|\n",
    "+-- test\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "```\n",
    "You will notice that the COCO dataset does not come structured like above so we must use the annotation data to help restructure the folders of the COCO dataset so they match the pattern above. Once the new directory structures are created you can use your desired framework's data loading tool to gracefully load and define transformation for your image data. Many datasets may already be in this structure in which case you can skip this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg2.2'></a>\n",
    "## Load annotation category labels\n",
    "___\n",
    "The `sample_annos` and `category_labels` files were generated from the first notebook in this series `01_download_data.ipynb`. You will need to run that notebook before running the code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pickled_data/sample_annos.pickle\", \"rb\") as f:\n",
    "    sample_annos = pickle.load(f)\n",
    "\n",
    "with open(\"pickled_data/category_labels.pickle\", \"rb\") as f:\n",
    "    category_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg2.3'></a>\n",
    "## Make train, validation and test splits\n",
    "___\n",
    "We should divide our data into train, validation and test splits. A typical split ratio is 80/10/10. Our image classification algorithm will train on the first 80% (training) and evaluate its performance at each epoch with the next 10% (validation) and we'll give our model's final accuracy results using the last 10% (test). It's important that before we split the data we make sure to shuffle it randomly so that class distribution among splits is roughly proportional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "image_ids = sorted(list(sample_annos.keys()))\n",
    "np.random.shuffle(image_ids)\n",
    "first_80 = int(len(image_ids) * 0.8)\n",
    "next_10 = int(len(image_ids) * 0.9)\n",
    "train_ids, val_ids, test_ids = np.split(image_ids, [first_80, next_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg2.4'></a>\n",
    "## Make new folder structure and copy image files\n",
    "___\n",
    "This new folder structure can then be read by data loaders for SageMaker's built-in algorithms, TensorFlow or PyTorch for easy loading of the image data into your framework of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstruct_dir = Path(\"data_sample_2750\")\n",
    "struct_dir = Path(\"data_structured\")\n",
    "struct_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for name, split in zip([\"train\", \"val\", \"test\"], [train_ids, val_ids, test_ids]):\n",
    "    split_dir = struct_dir / name\n",
    "    split_dir.mkdir(exist_ok=True)\n",
    "    for image_id in tqdm(split):\n",
    "        category_dir = split_dir / f'{category_labels[sample_annos[image_id][\"category_id\"]]}'\n",
    "        category_dir.mkdir(exist_ok=True)\n",
    "        source_path = (unstruct_dir / sample_annos[image_id][\"file_name\"]).as_posix()\n",
    "        target_path = (category_dir / sample_annos[image_id][\"file_name\"]).as_posix()\n",
    "        shutil.copy(source_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "Now that the images can be easily loaded by the framework of your choice, the next step is to choose a framework. In this series, we cover SageMaker's built-in algorithms, TensorFlow and PyTorch. You can choose the next notebook depending on the framework you want to learn more about. Once the data is loaded into the framework, we'll cover preprocessing, file formats, transformations and augmentations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
