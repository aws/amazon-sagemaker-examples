{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df84fc4",
   "metadata": {},
   "source": [
    "# Run mulitple deep learning models on GPUs with Amazon SageMaker Multi-model endpoints (MME)\n",
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) multi-model endpoints(MME) provide a scalable and cost-effective way to deploy large number of deep learning models. Previously, customers had limited options to deploy 100s of deep learning models that need accelerated compute with GPUs. Now customers can deploy 1000s of deep learning models behind one SageMaker endpoint. Now, MME will run multiple models on a GPU core, share GPU instances behind an endpoint across multiple models and dynamically load/unload models based on the incoming traffic. With this, customers can significantly save cost and achieve best price performance.\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Note </strong>\n",
    "This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `g4dn.4xlarge`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6beb5ff",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Multi-Model endpoints with GPU Support\n",
    "\n",
    "Amazon SageMaker multi-model endpoints with GPU works using [NVIDIA Triton Inference Server](https://github.com/triton-inference-server/server/). NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance. Triton supports all major training and inference frameworks, such as TensorFlow, NVIDIAÂ® TensorRTâ„¢, PyTorch, MXNet, Python, ONNX, XGBoost, scikit-learn, RandomForest, OpenVINO, custom C++, and more. It offers dynamic batching, concurrent execution, post-training quantization, optimal model configuration to achieve high performance inference.\n",
    "When SageMaker receives an invocation request for a particular model, it does the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce3098e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "iVBORw0KGgoAAAANSUhEUgAABhgAAAUyCAYAAAD8zf5MAAAMP2lDQ1BJQ0MgUHJvZmlsZQAASImVVwdYU8kWnluSkEBCCURASuhNEJESQEoILYCAVMFGSAKEEmMgiNiRRQUrKqJgQ1dFFF0LIGtF7C6KvS+KqCjroi525U0K6LKvfG++79z588+Z/5w5d+7NHQA0T/AkkmxUC4AccZ40JiSAOT4pmUl6CshAF2iDEcCax8+VsKOjIwBsA/3f27ubAJH31xzlWv8c/69NWyDM5QOAREOcKsjl50B8EAC8mi+R5gFAlPMW0/MkcgwN6EphghAvkuN0Ja6W41Ql3qfwiYvhQNwKgJoGjydNB4B2BfLMfH461KD1QuwsFojEAGgyIfbNyZkqgDgFYlvoI4FYrs9K/UEn/W+aqYOaPF76IFauRdHUAkW5kmzejP+zHP+75WTLBmJYQ9PIkIbGyNcM63Y7a2q4HGtA3CNOjYyCWAfiDyKBwh9ilJIhC41X+qNG/FwOrBlgQOws4AWGQ2wEcbA4OzJCxaemiYK5EMMdghaI8rhxEOtDvEiYGxSr8tksnRqjioU2pEk5bBV/jidVxJXHeijLimer9N9kCLkqfYxWmBGXCDEFYst8UUIkxDSInXKzYsNVPmMKMziRAz5SWYw8f0uIY4TikAClPpafJg2OUfmX5uQOrBfbnCHiRqrw/ryMuFBlfbBWPk+RP1wLdkUoZscP6Ahzx0cMrEUgDAxSrh17LhTHx6p0PkjyAmKUc3GKJDta5Y+bC7ND5Lw5xK65+bGquXhCHtyQSn08TZIXHafMEy/M5IVFK/PBV4AIwAGBgAlk0FLBVJAJRG09jT3wl3IkGPCAFKQDIXBUMQMzEhUjYniNBYXgD4iEIHdwXoBiVAjyIf91kFVeHUGaYjRfMSMLPIU4B4SDbPhbppglHoyWAJ5ARvSP6DxofJhvNjT5+L/nB9jvDBsyESpGNhCRqTngSQwiBhJDicFEO9wQ98W98Qh49YfmgrNwz4F1fPcnPCW0Ex4TbhA6CHemiIqkQ7IcCzqgfrCqFqk/1gK3hppueADuA9WhMs7ADYEj7grjsHE/GNkNshxV3vKqMIdo/20FP9wNlR/ZmYySh5H9ybZDZ9LsaW6DKvJa/1gfZa6pg/XmDI4Mjc/5ofoC2IcP9cQWYQews9hJ7Dx2BGsETOw41oRdwo7K8eDueqLYXQPRYhT5ZEEd0T/iDdxZeSVzneucu52/KMfyhAXydzTgTJXMkIrSM/KYbPiPIGRyxXynEUwXZxcXAOT/L8rX11uG4n8DYVz4zhUtBcDHtb+//8h3LkITgIPwmaF0fudsveBrogCAc8v4Mmm+ksPlFwJ8S2jCJ80AmAALYAvX4wLcgTfwB0EgDESBOJAEJsPsM+A+l4LpYBaYD0pAGVgB1oD1YBPYCnaCPWA/aARHwElwBlwEV8ANcA/uni7wEvSCd+AzgiAkhIrQEQPEFLFCHBAXhIX4IkFIBBKDJCEpSDoiRmTILGQBUoaUI+uRLUgt8gtyGDmJnEfakTvII6QbeYN8QjFUA9VFjVFrdCTKQtloOBqHTkLT0WloIVqMLkMr0Rp0N9qAnkQvojfQDvQl2ocBTB1jYGaYI8bCOFgUloylYVJsDlaKVWA1WD3WDO/zNawD68E+4kScjjNxR7iDQ/F4nI9Pw+fgS/D1+E68AW/Fr+GP8F78G4FKMCI4ELwIXMJ4QjphOqGEUEHYTjhEOA2fpS7COyKRyCDaED3gs5hEzCTOJC4hbiDuJZ4gthM7iX0kEsmA5EDyIUWReKQ8UglpHWk36TjpKqmL9EFNXc1UzUUtWC1ZTaxWpFahtkvtmNpVtWdqn8laZCuyFzmKLCDPIC8nbyM3ky+Tu8ifKdoUG4oPJY6SSZlPqaTUU05T7lPeqqurm6t7qo9TF6nPU69U36d+Tv2R+kcNHQ17DY7GRA2ZxjKNHRonNO5ovKVSqdZUf2oyNY+6jFpLPUV9SP1Ao9OcaFyagDaXVkVroF2lvdIka1ppsjUnaxZqVmge0Lys2aNF1rLW4mjxtOZoVWkd1rql1adN1x6lHaWdo71Ee5f2ee3nOiQda50gHYFOsc5WnVM6nXSMbkHn0Pn0BfRt9NP0Ll2iro0uVzdTt0x3j26bbq+ejp6rXoJegV6V3lG9DgbGsGZwGdmM5Yz9jJuMT8OMh7GHCYctHlY/7Oqw9/rD9f31hfql+nv1b+h/MmAaBBlkGaw0aDR4YIgb2huOM5xuuNHwtGHPcN3h3sP5w0uH7x9+1wg1sjeKMZpptNXoklGfsYlxiLHEeJ3xKeMeE4aJv0mmyWqTYybdpnRTX1OR6WrT46YvmHpMNjObWclsZfaaGZmFmsnMtpi1mX02tzGPNy8y32v+wIJiwbJIs1ht0WLRa2lqOdZylmWd5V0rshXLKsNqrdVZq/fWNtaJ1gutG62f2+jbcG0Kbeps7ttSbf1sp9nW2F63I9qx7LLsNthdsUft3ewz7KvsLzugDu4OIocNDu0jCCM8R4hH1Iy45ajhyHbMd6xzfOTEcIpwKnJqdHo10nJk8siVI8+O/Obs5pztvM353iidUWGjikY1j3rjYu/Cd6lyuT6aOjp49NzRTaNfuzq4Cl03ut52o7uNdVvo1uL21d3DXepe797tYemR4lHtcYuly4pmLWGd8yR4BnjO9Tzi+dHL3SvPa7/Xn96O3lneu7yfj7EZIxyzbUynj7kPz2eLT4cv0zfFd7Nvh5+ZH8+vxu+xv4W/wH+7/zO2HTuTvZv9KsA5QBpwKOA9x4szm3MiEAsMCSwNbAvSCYoPWh/0MNg8OD24Lrg3xC1kZsiJUEJoeOjK0FtcYy6fW8vtDfMImx3WGq4RHhu+PvxxhH2ENKJ5LDo2bOyqsfcjrSLFkY1RIIobtSrqQbRN9LToX8cRx0WPqxr3NGZUzKyYs7H02Cmxu2LfxQXELY+7F28bL4tvSdBMmJhQm/A+MTCxPLFj/Mjxs8dfTDJMEiU1JZOSE5K3J/dNCJqwZkLXRLeJJRNvTrKZVDDp/GTDydmTj07RnMKbciCFkJKYsivlCy+KV8PrS+WmVqf28jn8tfyXAn/BakG30EdYLnyW5pNWnvY83Sd9VXp3hl9GRUaPiCNaL3qdGZq5KfN9VlTWjqz+7MTsvTlqOSk5h8U64ixx61STqQVT2yUOkhJJxzSvaWum9UrDpdtzkdxJuU15uvBD/pLMVvaT7FG+b35V/ofpCdMPFGgXiAsuzbCfsXjGs8Lgwp9n4jP5M1tmmc2aP+vRbPbsLXOQOalzWuZazC2e2zUvZN7O+ZT5WfN/K3IuKi/6a0HiguZi4+J5xZ0/hfxUV0IrkZbcWui9cNMifJFoUdvi0YvXLf5WKii9UOZcVlH2ZQl/yYWlo5ZWLu1flrasbbn78o0riCvEK26u9Fu5s1y7vLC8c9XYVQ2rmatLV/+1Zsqa8xWuFZvWUtbK1nZURlQ2rbNct2Ldl/UZ629UBVTtrTaqXlz9foNgw9WN/hvrNxlvKtv0abNo8+0tIVsaaqxrKrYSt+ZvfbotYdvZn1k/12433F62/esO8Y6OnTE7W2s9amt3Ge1aXofWyeq6d0/cfWVP4J6mesf6LXsZe8v2gX2yfS9+Sfnl5v7w/S0HWAfqD1odrD5EP1TagDTMaOhtzGjsaEpqaj8cdril2bv50K9Ov+44Ynak6qje0eXHKMeKj/UfLzzed0Jyoudk+snOlikt906NP3W9dVxr2+nw0+fOBJ85dZZ99vg5n3NHznudP3yBdaHxovvFhktulw795vbboTb3tobLHpebrnheaW4f037sqt/Vk9cCr525zr1+8Ubkjfab8Tdv35p4q+O24PbzO9l3Xt/Nv/v53rz7hPulD7QeVDw0eljzu93vezvcO44+Cnx06XHs43ud/M6XT3KffOkqfkp9WvHM9Fntc5fnR7qDu6+8mPCi66Xk5eeekj+0/6h+Zfvq4J/+f17qHd/b9Vr6uv/NkrcGb3f85fpXS19038N3Oe8+vy/9YPBh50fWx7OfEj89+zz9C+lL5Ve7r83fwr/d78/p75fwpDzFpwAGDU1LA+DNDgCoSQDQ4fmMMkF5/lM0RHlmVSDwn7DyjKho7gDUw07+Gc85AcA+aNbQqPMAkH/Cx/kDdPToQRs4qynOlfJGhOeAzT5ydENfMA8Macoz5w95D+2BXNUVDO3/BUy+ele9vjzzAAAAimVYSWZNTQAqAAAACAAEARoABQAAAAEAAAA+ARsABQAAAAEAAABGASgAAwAAAAEAAgAAh2kABAAAAAEAAABOAAAAAAAAAJAAAAABAAAAkAAAAAEAA5KGAAcAAAASAAAAeKACAAQAAAABAAAGGKADAAQAAAABAAAFMgAAAABBU0NJSQAAAFNjcmVlbnNob3SXM2hFAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB2GlUWHRYTUw6Y29tLmFkb2JlLnhtcAAAAAAAPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iWE1QIENvcmUgNi4wLjAiPgogICA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPgogICAgICA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIgogICAgICAgICAgICB4bWxuczpleGlmPSJodHRwOi8vbnMuYWRvYmUuY29tL2V4aWYvMS4wLyI+CiAgICAgICAgIDxleGlmOlBpeGVsWURpbWVuc2lvbj4xMzMwPC9leGlmOlBpeGVsWURpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjE1NjA8L2V4aWY6UGl4ZWxYRGltZW5zaW9uPgogICAgICAgICA8ZXhpZjpVc2VyQ29tbWVudD5TY3JlZW5zaG90PC9leGlmOlVzZXJDb21tZW50PgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4KgY1RWwAAABxpRE9UAAAAAgAAAAAAAAKZAAAAKAAAApkAAAKZAAG6AB5H5ukAAEAASURBVHgB7N0JmBTF2cDxl11gF1hu2EXuQ8ATFRSviMbbeMcjiho1fppo1KiJR/Jp1EQTEzUqGnN4Yg4PNJ7Rz/tWFMEDQeWS5V5ggT3ZXXaZr98y3Zldendmlunp6pl/P4/ObB9V1b8ay5l6u6o6xJxN2BBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQACBFAQ6EGBIQYtTEUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAwAgQYOCDgAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAikLEGBImYwLEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAgAADnwEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBIWYAAQ8pkXIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIEGPgMIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAQMoCBBhSJuMCBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQIMDAZwABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQRSFiDAkDIZFyCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAABBj4DCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgggkLIAAYaUybgAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEECDDwGUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAIGUBQgwpEzGBQgggAACCCCAAAIIIIAAAggggAACCCCAAAIIIECAgc8AAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIpCxAgCFlMi5AAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABAgx8BhBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQCBlAQIMKZNxAQIIIIAAAggggAACCCCAAAIIIIAAAggggAACCBBg4DOAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACKQsQYEiZjAsQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEECAAAOfAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEhZgABDymRcgAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgQY+AwggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIBAygIEGFIm4wIEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBAgwMBnAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBFIWIMCQMhkXIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAEGPgMIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCQsgABhpTJuAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQIMPAZQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAgZQFCDCkTMYFCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgggQICBzwACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgikLECAIWUyLkAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAECDHwGEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAIGUBAgwpk3EBAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIEGDgM4AAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIpCxBgSJmMCxBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQIAAA58BBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQSFmAAEPKZFyAAAIIIIAAAggggAACCCCQCwI1NTWyYMEC6datmwwbNkw6deqUC7fNPSKAAAIIIIAAAkkLEGBImooTEUAAAQQQQAABBBBAAAEEsl2gsbFRfvvb38rf//53mTdvXrPbPeSQQ+QPf/iD7LTTTs32jx071vw9Z84c6dixo9xxxx1y9913y+WXXy7/8z//0+zcsP84+eST5dNPP5WioiJ59dVXpVevXlsUqaKiQiZOnNjsnrY4iR0IIIAAAggggIAjQICBjwECCKQswJNcKZNxAQIIIOArQHvqy8JOBBBAIDSBsrIymTx5srz22mumDN27d5ddd91V6urqZMaMGV65HnvsMTnppJPM37FYTPLy8sz7hoYGM8rhmmuukRtuuMEEKq666irvuqDeTJgwQTZs2GDK2KdPnzaz0cCBey8XXnih3HnnnVucX15eLv369TP76+vrpXPnzlucww4EEEAAAQQQQEAFCDDwOUAAgaQEsv1JrpYIb7zxhnli7cwzz5SCgoKWh/kbAQQQaLdArrSnX3/9tXz88cfyxRdfyKBBg2S77baTvfbaq91uXIgAAghkQmDSpEny9ttvm6wefvhh0af93eCBBh9++MMfytNPP22Oz58/X7bddlvxCzBo+zd79mzZbbfdZOeddw686D169JCqqipZtWqVlJSUtJlffIBBT5w+fbrsueeeza4hwNCMgz8QQAABBBBAoA0BAgxt4HAIAQS+EciFJ7ni63rz5s1mjt1ly5bJ2rVrpW/fvvGHeY8AAgi0WyAX2lNtQy+66CIzNUhLqGOPPVbuv/9+SfR0bcvr+BsBBBDIhMBzzz0nRx99tMnKr9NdD2gH/jbbbGPO0VEKv/rVr3wDDOaENv6lbeW6devM98wOHTq0ceZ/D+n3Ug0k+I0m2JoAw7hx42TmzJlmaic3NwIMrgSvCCCAAAIIIJBQwHnagg0BBBBoU2C//faLOY2J+cd5kivW1NTkne/8yIo5HUbecedJLnPM+dHk7XOGipt9s2bNik2dOjX22WefedcH+cYZ0m7KoGVMdnN+6MUuvfRSr+zOD7lkL+U8BBBAIKFALrSnV1xxhdeGanvqzEEeO/LII719Z5xxRkInTkAAAQTCENh3331NW+VMfdRm9k6gNHbggQea74x6ot/3XmfaodgOO+wQu/fee5ultXDhwmZton5fPf3002POgy3Nzrv55ptjxcXFsSeffDJ20003xQYPHuy1o/r/kqVLl5rzn3rqKXOe+11dr9Hv5m1te+yxh0nrL3/5i5fmrbfe2uwS/Q7spulMkdTsGH8ggAACCCCAAALxAvq0BRsCCCDQqsCzzz7r/bhwnuTyPW/lypXeOc6TXOYcvx9avhfH7dTAxZo1a8yPtLjdbb7V81v70ZNKgOHFF180PwLdH1LuKwGGNvk5iAACKQjkSnuqnVvahjpTiDTT+dOf/uT9v2LJkiXNjvEHAgggELZA/HdXZyRDSsWJv9Z9sObqq682bZ6zWLSX1vvvvx9zv5+OHDkydsQRR3h/a9sZ/xCOe717vgYY3ACItrFjxowx6ep3WGf6Oa99HT9+fOzcc8/18vR74wYYnn/++Vh827x48WLvdAIMHgVvEEAAAQQQQCCBAAGGBEAcRiDXBdwfMtn+JJezUJ95+kt/3LmdY/rjjQBDrv8XwP0jkD6BXGhPnXUXTCeXdoi13JzpNrwOMO3UYkMAAQRsEoh/YObTTz9NqWjJBBic9XdizlREph285JJLYps2bTJ5aEBCv2fr905neiYvXzfAoPuvv/5673wtm+7Tf+JHPbiBiGRG7sYHGLRc7v+f4vMnwOBVBW8QQAABBBBAIIEAAYYEQBxGIJcF4n8sZfuTXPH1XFlZ6f1wI8AQL8N7BBBor0CutKfvvvuueZLWWYNhC6q6ujqvbX3nnXe2OM4OBBBAIEyB+I57HSGbyhbfxrc2guGFF17w2sCW6S9fvtw75qzVY7J2Aww6HVLLTUcpaIDh5Zdf9g61N8CgCTiLUXv5P/HEEyZNAgweLW8QQAABBBBAIIEAAYYEQBxGIJcFculJrvh6JsAQr8F7BBBIh0CutqfxdldddZXXgVVRURF/iPcIIIBA6ALr16/32qhFixalVJ5kAgy6xoEGBXSqo/vuu2+Lf9xRCW+//bbJ2w0w6GvL7eSTTzZpPfLII96hrQkwaCLXXnutSVNH8mobTYDBo+UNAggggAACCCQQIMCQAIjDCOSyQC49yRVfzwQY4jV4jwAC6RDI1fZU7fRJ3eOPP97ruNOONTYEEEDARgF3msw33nijzeLNmTMnduGFF8auvPJKc14yAYYLLrjAawfdYILfq66poJsbYGi5+LIeO+WUU9IeYKitrY3puhBaJp3CiQCDSrMhgAACCCCAQDICBBiSUeIcBHJUIJee5IqvYgIM8Rq8RwCBdAjkYnuq84v/5S9/8RYw1U6r+++/Px2cpIEAAggEInDggQd6HextZaDBBW3TDjnkEHNaMgGGX/ziF+YaXe9g+vTprf7jjvByAwy33XbbFkUJIsCgmbz++uumjHpvulaOvuo/9fX1W5SBHQgggAACCCCAgCtAgMGV4BUBBHwFcuVJrvibJ8AQr8F7BBBIl0AutacLFy6MuYuIaueUTuexdOnSdFGSDgIIIBCIwJQpU7xO9dbarHXr1nmBU3d0QTIBhgceeMCkrVMZ6cLK8ZsGZDWgcPnll5vpifRYGAEGzfecc84x5XT/n0WAQVXYEEAAAQQQQKAtAQIMbelwDAEEYrnyJFd8VRNgiNfgPQIIpEsgV9rT+OmgxowZE9OFn9kQQACBKAhoR/9ee+1lOth1rYRZs2Y1K7Yuxjxu3DhzXKcTqq6uNseTCTAsXrzYXKcd9jo6IH678847vTTd/e0NMMyfP99NotVXNwDcshx6QXl5uRdA0bISYGiVkQMIIIAAAggg8B8BAgx8FBBAoE2BXHmSKx6BAEO8Bu8RQCBdArnQnupTuRpU0A4pXXdh48aN6eIjHQQQQCAjArq+gtuxrq/apumURG7gQffpKIT333/fK08yAQY9+de//rWX9kUXXRS75ZZbYieddJK374UXXvDSTDXAcMABB5h0xo8fH7vpppu8dPzetBVg0PMffvhhr0x6v0yR5KfIPgQQQAABBBBwBQgwuBK8IoCAr0CuPMkVf/MEGOI1eI8AAukSyIX2dPbs2aZTSjvfysrK0kVHOggggEBGBUpLS5t1/McHHDR4umrVqmbliQ8waFuv2zXXXGPaw9/97nfeuU1NTTFdUyE+PX2voyLigwvx12twuuU2efJkk8Zjjz3mHZo2bZo38kADCG1tboDBXVDa79wjjjjCKycBBj8h9iGAAAIIIICAK0CAwZXgFQEEWhXIhSe54m+eAEO8Bu8RQCCdAtnenv71r3/1OqQ0yNDaP2+//XY6WUkLAQQQCERA11vQBZmfeOIJM11STU1NWvLRhZzfeecdk66OhGi5JkN7M9HvsDpFkrtQdHvT4ToEEEAAAQQQQCAVAQIMqWhxLgI5LJDtT3LFV21VVZXXQaY/LNkQQACBdApkc3vqLg7a8uncln+/+uqr6SQlLQQQQAABBBBAAAEEEEAAgZAEOmi+zo8+NgQQQCApgfXr18u8efPEWeRORowYIWPHjpWuXbsmdW1bJzlPXIkztYY4U2rIwIEDxRm6Lfn5+W1dktQxJ1hg0iwuLpYePXokdQ0nIYAAApkQoD3NhDJ5IIAAAggggAACCCCAAAIIBClAgCFIXdJGAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQCBLBQgwZGnFclsIIIAAAggggAACCCCAAAIIIIAAAggggAACCAQpQIAhSF3SRgABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAgSwUIMGRpxXJbCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggEKUCAIUhd0kYAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAIEsFCDBkacVyWwgggAACCCCAAAIIIIAAAggggAACCCCAAAIIBClAgCFIXdJGAAEEEEAAAQQQQAABBBCwQqBxU5PUVtWZf/R9U+NmiW2OWVG2bChEfqc86dgpX7p0K5Cinl2lY+f8bLgt7gEBBBBAAAEEEggQYEgAxGEEsl2goW6TbGpoND+wNjdtzvbbzdj95XfMNz+wCrp0krz8vIzlS0YIIBCeAO1pMPa0p8G4kioCuSRQ73zfrVhTJbXV9SLEEzJW9V2KOkvPft2lsGvnjOVJRggggAACCCCQeQECDJk3J0cEQhfQJ7aq1tdITUWd6Hu2YAW6FDlPcfXqIt16dAk2I1JHAIGMC9CeZpac9jSz3uSGQDYIrCurlMrymmy4lcjeg34P7rtNL+nQIbK3QMERQAABBBBAoA0BAgxt4HAIgWwT0CHg5asqpHrDxmy7tUjcT6eCfOk7oKcUOsPG2RBAINoCtKfh1h/tabj+5I5AVATKlqyTjTpqgS10gQJnFMOAoX2kQx5RhtArgwIggAACCCCQZgECDGkGJTkEbBVobGiSsqXrZFN9o61FzI1yOb+p+pT0kB59uuXG/XKXCGShAO2pJZVKe2pJRVAMBOwUYOSCffVS2M0JMgzra1/BKBECCCCAAAIIbJUAAYat4uNiBKIhoGsrLF+4xqyzEI0SZ38pe/Uvkl79u2f/jXKHCGSZAO2pfRVKe2pfnVAiBMIWqK2uk9VL1oddDPL3EejZr0h6F/Md2IeGXQgggAACCERWgABDZKuOgiOQvMCq0nKpq2lI/gLOzIhAv4G9zNoMGcmMTBBAIC0CtKdpYUx7IrSnaSclQQQiKxCLxWT5gjWsM2ZpDXbIExk0qlg6dsq3tIQUCwEEEEAAAQRSFSDAkKoY5yMQMQFdzLl8ZWXESp0bxc3L7yCDR5dIHnPR5kaFc5eRF6A9tbcKaU/trRtKhkCmBXRBZ50eic1ege59upp1yewtISVDAAEEEEAAgVQECDCkosW5CERMQKfyWLZgtWxuikWs5LlT3J59u0lvZ00GNgQQsFuA9tTu+tHS0Z7aX0eUEIFMCKxYtEYa6hKvOdapoKN06VYgefnOI/VsWy2gI0ca6jYltah2Xsc8GTqmZKvzJAEEEEAAAQQQsEOAAIMd9UApEAhEgCe4AmFNa6L61O3QsQPSmiaJIYBA+gVoT9Nvmu4UaU/TLUp6CERPoHFTkyybvzphwbv1KJT+g3snPI8TUheoqdgoa5ZvSHjhNiP6SkGXzgnP4wQEEEAAAQQQsF+AAIP9dUQJEWi3wMrFa6W+dlPC63Wx4aJeXZgLNaFU8idsqm+U9WuqpLayLuFFJcP6mCfoEp7ICQggEJoA7Wlo9EJ7Gp49OSMQNYGN1fVStmRdm8Xu7IxcGDiqf5vncHDrBDasqZYNzvfgtrY+A3pIjz7d2jqFYwgggAACCCAQEQECDBGpKIqJQKoCOky59ItVCS/jy31Coq06Yc2y9VKTIMjQw5kmqQ/TJG2VMxcjEKQA7WmQusmnTXuavBVnIpCrAtXO0/NrEzw937u4u/TsV5SrRBm572RGkvD9NyNVQSYIIIAAAghkRIAAQ0aYyQSBzAsk88W+c2EnGTiyX+YLl0M56rztS74qa/OOu3YvkOIhfdo8h4MIIBCeAO1pePbxOdOexmvwHgEE/ASq1tdK+coKv0Pevn4De5mRu94O3gQisHjuyjbT7epMU1XMNFVtGnEQAQQQQACBqAgQYIhKTVFOBFIU0EXWVixa2+ZVOi2S/shiC1ZAF9pubGhqNZPOXZxAzwgCPa0CcQCBkAVoT0OugLjsaU/jMHiLAAJbCBBg2IIktB2JAgxdnAdsSnjAJrT6IWMEEEAAAQTSKUCAIZ2apIWARQL1ToBhZYIAg857qlMksQUroIEe7aBsbcvvlCdDRpe0dpj9CCAQsgDtacgVEJc97WkcBm8RQGALAQIMW5CEtoMAQ2j0ZIwAAggggEDGBQgwZJycDBHIjAAdYplxTiaXhB1iHZ0AwxgCDMlYcg4CYQjQnoah7p8n7am/C3sRQOAbAQIM9nwSCDDYUxeUBAEEEEAAgaAFCDAELUz6CIQkQIdYSPA+2dIh5oPCLgQiJEB7ak9l0Z7aUxeUBAEbBQgw2FMrBBjsqQtKggACCCCAQNACBBiCFiZ9BEISoEMsJHifbOkQ80FhFwIREqA9taeyaE/tqQtKgoCNAgQY7KkVAgz21AUlQQABBBBAIGgBAgxBC5M+AiEJ0CEWErxPtnSI+aCwC4EICdCe2lNZtKf21AUlQcBGAQIM9tQKAQZ76oKSIIAAAgggELQAAYaghUkfgZAE6BALCd4nWzrEfFDYhUCEBGhP7aks2lN76oKSIGCjAAEGe2qFAIM9dUFJEEAAAQQQCFqAAEPQwqSPQEgCdIiFBO+TLR1iPijsQiBCArSn9lQW7ak9dUFJELBRgACDPbVCgMGeuqAkCCCAAAIIBC1AgCFoYdJHICQBOsRCgvfJlg4xHxR2IRAhAdpTeyqL9tSeuqAkCNgoQIDBnlohwGBPXVASBBBAAAEEghYgwBC0MOkjEJIAHWIhwftkS4eYDwq7EIiQAO2pPZVFe2pPXVASBGwUIMBgT60QYLCnLigJAggggAACQQsQYAhamPQRCEmADrGQ4H2ypUPMB4VdCERIgPbUnsqiPbWnLigJAjYKEGCwp1YIMNhTF5QEAQQQQACBoAUIMAQtTPoIhCRAh1hI8D7Z0iHmg8IuBCIkQHtqT2XRntpTF5QEARsFCDDYUysEGOypC0qCAAIIIIBA0AIEGIIWJn0EQhKgQywkeJ9s6RDzQWEXAhESoD21p7JoT+2pC0qCgI0CBBjsqRUCDPbUBSVBAAEEEEAgaAECDEELkz4CIQnQIRYSvE+2dIj5oLALgQgJ0J7aU1m0p/bUBSVBwEYBAgz21AoBBnvqgpIggAACCCAQtAABhqCFSR+BkAToEAsJ3idbOsR8UNiFQIQEaE/tqSzaU3vqgpIgYKMAAQZ7aoUAgz11QUkQQAABBBAIWoAAQ9DCpI9ASAJ0iIUE75MtHWI+KOxCIEICtKf2VBbtqT11QUkQsFGAAIM9tUKAwZ66oCQIIIAAAggELUCAIWhh0kcgJAE6xEKC98mWDjEfFHYhECEB2lN7Kov21J66oCQI2ChAgMGeWiHAYE9dUBIEEEAAAQSCFiDAELQw6SMQkgAdYiHB+2RLh5gPCrsQiJAA7ak9lUV7ak9dUBIEbBQgwGBPrRBgsKcuKAkCCCCAAAJBCxBgCFqY9BEISYAOsZDgfbKlQ8wHhV0IREiA9tSeyqI9tacuKAkCNgoQYLCnVggw2FMXlAQBBBBAAIGgBQgwBC1M+giEJECHWEjwPtnSIeaDwi4EIiRAe2pPZdGe2lMXlAQBGwUIMNhTKwQY7KkLSoIAAggggEDQAgQYghYmfQRCEqBDLCR4n2zpEPNBYRcCERKgPbWnsmhP7akLSoKAjQIEGOypFQIM9tQFJUEAAQQQQCBoAQIMQQuTPgIhCdAhFhK8T7Z0iPmgsAuBCAnQntpTWbSn9tQFJUHARgECDPbUCgEGe+qCkiCAAAIIIBC0AAGGoIVJH4GQBOgQCwneJ1s6xHxQ2IVAhARoT+2pLNpTe+qCkiBgowABBntqhQCDPXVBSRBAAAEEEAhagABD0MKkj0BIAnSIhQTvky0dYj4o7EIgQgK0p/ZUFu2pPXVBSRCwUYAAgz21QoDBnrqgJAgggAACCAQtQIAhaGHSRyAkATrEQoL3yZYOMR8UdiEQIQHaU3sqi/bUnrqgJAjYKECAwZ5aIcBgT11QEgQQQAABBIIWIMAQtDDpIxCSAB1iIcH7ZEuHmA8KuxCIkADtqT2VRXtqT11QEgRsFCDAYE+tEGCwpy4oCQIIIIAAAkELEGAIWpj0EQhJgA6xkOB9sqVDzAeFXQhESID21J7Koj21py4oCQI2ChBgsKdWCDDYUxeUBAEEEEAAgaAFCDAELUz6CIQkQIdYSPA+2dIh5oPCLgQiJEB7ak9l0Z7aUxeUBAEbBQgw2FMrBBjsqQtKggACCCCAQNACBBiCFiZ9BEISoEMsJHifbOkQ80FhFwIREqA9taeyaE/tqQtKgoCNAgQY7KkVAgz21AUlQQABBBBAIGgBAgxBC5M+AiEJ0CEWErxPtnSI+aCwC4EICdCe2lNZtKf21AUlQcBGAQIM9tQKAQZ76oKSIIAAAgggELQAAYaghUkfgZAE6BALCd4nWzrEfFDYhUCEBGhP7aks2lN76oKSIGCjAAEGe2qFAIM9dUFJEEAAAQQQCFqAAEPQwqSPQEgCdIiFBO+TLR1iPijsQiBCArSn9lQW7ak9dUFJELBRgACDPbVCgMGeuqAkCCCAAAIIBC1AgCFoYdJHICQBOsRCgvfJlg4xHxR2IRAhAdpTeyqL9tSeuqAkCNgoQIDBnlohwGBPXVASBBBAAAEEghYgwBC0MOkjEJIAHWIhwftkS4eYDwq7EIiQAO2pPZVFe2pPXVASBGwUIMBgT60QYLCnLigJAggggAACQQsQYAhamPQRCEmADrGQ4H2ypUPMB4VdCERIgPbUnsqiPbWnLigJAjYKEGCwp1YIMNhTF5QEAQQQQACBoAUIMAQtTPoIhCRAh1hI8D7Z0iHmg8IuBCIkQHtqT2XRntpTF5QEARsFCDDYUysEGOypC0qCAAIIIIBA0AIEGIIWJn0EQhKgQywkeJ9s6RDzQWEXAhESoD21p7JoT+2pC0qCgI0CBBjsqRUCDPbUBSVBAAEEEEAgaAECDEELkz4CIQnQIRYSvE+2dIj5oLALgQgJ0J7aU1m0p/bUBSVBwEYBAgz21AoBBnvqgpIggAACCCAQtAABhqCFSR+BkAToEAsJ3idbOsR8UNiFQIQEaE/tqSzaU3vqgpIgYKMAAQZ7aoUAgz11QUkQQAABBBAIWoAAQ9DCpI9ASAJ0iIUE75MtHWI+KOxCIEICtKf2VBbtqT11QUkQsFGAAIM9tUKAwZ66oCQIIIAAAggELUCAIWhh0kcgJAE6xEKC98mWDjEfFHYhECEB2lN7Kov21J66oCQI2ChAgMGeWiHAYE9dUBIEEEAAAQSCFiDAELQw6SMQkgAdYiHB+2RLh5gPCrsQiJAA7ak9lUV7ak9dUBIEbBQgwGBPrRBgsKcuKAkCCCCAAAJBCxBgCFqY9BEISYAOsZDgfbKlQ8wHhV0IREiA9tSeyqI9tacuKAkCNgoQYLCnVggw2FMXlAQBBBBAAIGgBQgwBC1M+giEJECHWEjwPtnSIeaDwi4EIiRAe2pPZdGe2lMXlAQBGwUIMNhTKwQY7KkLSoIAAggggEDQAgQYghYmfQRCEqBDLCR4n2zpEPNBYRcCERKgPbWnsmhP7akLSoKAjQIEGOypFQIM9tQFJUEAAQQQQCBoAQIMQQuTPgIhCdAhFhK8T7Z0iPmgsAuBCAnQntpTWbSn9tQFJUHARoFkAgx9SnpIj77dbCx+1pSpcVOTLJu/us376dK9QEqG9GnzHA4igAACCCCAQDQECDBEo54oJQIpC9AhljJZYBfQIRYYLQkjkBEB2tOMMCeVCe1pUkychEDOCqxdtV6q19W1ef9dujkd28Po2G4TaSsPVpTXyPqyyjZTiXVskhFjBrd5DgcRQAABBBBAIBoCBBiiUU+UEoGUBWqqN8qaJRvavK5r90IpHtK7zXM4uPUCpV+sklgs1mpCMdksI3YY1OpxDiCAQLgCtKfh+sfnTnsar8F7BBBoKbC8dKVsqmm5d8u/+w/uJd16dNnyAHu2WqC+tkFWla5r87uvZrKxsUa2H7ftVudHAggggAACCCAQvgABhvDrgBIgEIjAuvINUlm2sc208/I6yKBtiyW/Y16b53Gw/QLJDNXfWF8r2+82qv2ZcCUCCAQqQHsaKG/SidOeJk3FiQjkrMAXn8+TAukmeXmJv9t279VVdJqevPzE5+YsaAo3vrlxs1RXbpTayrZHkLhJLln5tUw6aB/3T14RQAABBBBAIMICBBgiXHkUHYG2BFaXrXUCDLXSsWOntk6TLkUFUjy4t3Rwgg1s6RWoqdgo5SsrZPPm1kcvaI7rNpTL+H12Sm/mpIYAAmkToD1NG2W7E6I9bTcdFyKQUwJzP/9Kqss3SnH/kpy676jdbHVNtSwvWyKHHXVQ1IpOeRFAAAEEEEDAR4AAgw8KuxDIBgHtEFs4p1S2GTAw4e3oCIYuRYXSsVN+wnM5IbFAbPNmqa2ql00NjYlPds74cv5cOfxYfmAlhcVJCIQgQHsaAvp/sqQ9Dc+enBGIosDXCxbLxx/OlvG77h7F4udMmed88bmZouqAQ76VM/fMjSKAAAIIIJDNAgQYsrl2ubecFli7ulzeef0D2XXn3XLawfabb9jUILM+/UhO+f4JtheV8iGQswK0p9GoetrTaNQTpUQgSIGylWvkuSdfkF123lX69e0fZFak3U6BDRUbZObHM2TcbjvKHntPaGcqXIYAAggggAACNgkQYLCpNigLAmkUqK3ZKA9PnSa7j58oPXv0TGPKJJVOgS/nfSF1DbVy3MlHpTNZ0kIAgTQK0J6mETPApGhPA8QlaQQiIlC3sU7+8cBjUlhYKBMn7CWdOrU9VWhEbitritnQ0CAzZn0gdXV1csDB+8moMSOy5t64EQQQQAABBHJZgABDLtc+9571Avfd/ZD06N5D9piwZ9bfaxRvsKqqUj6c+YEMGTZYDj3ywCjeAmVGIGcEaE/trmraU7vrh9IhkEmBpx//t+jIs+7du8uEXfeQ/HymAM2kf2t5NTY2mpELuv6CLqx9xjmnOGvFdWztdPYjgAACCCCAQIQECDBEqLIoKgKpCvzrkWdk/boNMmjgYNluzPapXs75AQrU19ebJ7j0dfc9d5NdJuwcYG4kjQACWytAe7q1gsFdT3sanC0pIxBFgc9mfS4zps8yRdcHbXZxpgvt3LlzFG8la8pcu7FWPp39sdTW1pp7GrntcPn2oZOy5v64EQQQQAABBHJdgABDrn8CuP+sFpg5/WP5ZNZsc48jho2UkSNGZfX9RuXmdHi4rrtQU1Njivy9M06Qou7dolJ8yolATgrQntpZ7bSndtYLpUIgTIGGhk0y7R9Pik6XpFtB5wIZM3qsFPcvCbNYOZv38hXLZMHC+dLY1GgMdPTCSZOP57tvzn4iuHEEEEAAgWwUIMCQjbXKPSHwH4EN6yvkiYef9jxKigfIDtvtKHl5ed4+3mRWoNKZFkmf4NJOMd0GDt5GjjjmkMwWgtwQQCBlAdrTlMkCv4D2NHBiMkAgsgKL5i2W1195q1n5u3btJkMHDxX9PszUPM1oAvljzdrVUrpksVRUVjRLf9fx42TCXrs228cfCCCAAAIIIBBtAQIM0a4/So9AQoG3Xn1X5n+10DuvsKBQhg0dLgO3GUSgwVMJ/s3mzZvl69JF5odWLBYzGXbo0EG+e8qx0qt3j+ALQA4IILDVArSnW02YlgRoT9PCSCIIZL3A+29/KHNnf+l7n7169nIWgu4iBQUFfB/2FWrHTufrbX1DvdTX18mGDRukaXPTFolsM3CAHHHsIaLfgdkQQAABBBBAIHsECDBkT11yJwj4CtTX1cvjDz/lDBOvb3ZcF7zr369YBpQMkJ49evEkVzOd9P1RW1sjq1avkpUrV0id84Mrftt5lx1l4r4T4nfxHgEELBagPQ23cmhPw/UndwSiKNAyMBzFe8iWMvfu20uOOu5w6VzAehjZUqfcBwIIIIAAAq4AAQZXglcEslhg3dr18txTL8imhm/mPvW7VR02zgJ4fjLt27dpU4PU1dVJU9OWT29pitsMdKZGOvZgnuBqHy9XIRCaAO1p5ulpTzNvTo4IZJPAJx99JjNnfCLyzQDSbLq1yNzLkGGD5cDDJvFAU2RqjIIigAACCCCQmgABhtS8OBuByAqUrSyTF//9aptBhsjeXMQKXjygvxx21EEEdCJWbxQXAVeA9tSVCP+V9jT8OqAECERBYNWKMvlo+sdStmp1FIqbNWXs17+v7Lr7OBk2YkjW3BM3ggACCCCAAAJbChBg2NKEPQhkrUBlZZW89O/XpMJZ/JktHIHtdxwr++y/ZziZkysCCKRNgPY0bZTtToj2tN10XIhAzgqsKVsrS0uXm0BDVUWV1NTWyuamzTnrke4b79K1UIqKusk2gwbIyNEjpG+/PunOgvQQQAABBBBAwEIBAgwWVgpFQiBogS8/nyefzPpMaqprg86K9B2B/I75MnzEMBk/cRfp0bM7JgggkEUCtKeZrUza08x6kxsCCCCAAAIIIIAAAgggkEiAAEMiIY4jkMUCa1eXyypn6qQNzoiGmqoaadi0KYvvNnO31kE6SGFhgRR17yY6fYc+wcWGAALZLUB7Gkz90p4G40qqCCCAAAIIIIAAAggggEC6BAgwpEuSdBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQyCEBAgw5VNncKgIIIIAAAggggAACCCCAAAIIIIAAAggggAAC6RIgwJAuSdJBAAEEEEAAAQQQQAABBBCwTqBxU5PU1zaIvsZiMevKl60F6tgpXwq6dJZOBR2z9Ra5LwQQQAABBBBwBAgw8DFAAAEEEEAAAQQQQAABBBDIOoGN1fWyfk2VNGxknbEwK1cDDEW9ukj33t0kL69DmEUhbwQQQAABBBAIQIAAQwCoJImAzQK1VXVS5zzBtam+kSe4MlhR+fl50rmwk3TtUSidOvMUVwbpyQqBwARoTwOjbTNh2tM2eTiIAAL/EShfWSFV62vxsEhAAw0lQ/uIjmxgQwABBBBAAIHsESDAkD11yZ0g0KZA5boa2eA8wbW5iWHhbUJl4GBBl07Sq3936VJUkIHcyAIBBNItQHuabtH2p0d72n47rkQgmwXWraqQynUEF2ys486FHWWbEf2kQwdGMthYP5QJAQQQQACB9ggQYGiPGtcgEDGBNcvWS01lXcRKnf3F7btNT2eoeNfsv1HuEIEsEqA9tbMyaU/trBdKhUAYAjotUtmSdWFkTZ5JCvQq7i69+hUleTanIYAAAggggIDtAgQYbK8hyofAVgqsK6uUyvKarUyFy4MSGDC8rxR27RxU8qSLAAJpFKA9TSNmAEnRngaASpIIRFBg+cI1ZirQCBY9Z4qcl99Bho4dkDP3y40igAACCCCQ7QIEGLK9hrm/nBaodxa0W/n12pw2sP3mdS7aQaP6215MyodAzgvQntr/EaA9tb+OKCECQQvoGmMaYGCzX6B4SG/p2r3Q/oJSQgQQQAABBBBIKECAISERJyAQXYFVpeVSV9MQ3RvIkZIPGNZHCruxHkOOVDe3GVEB2tNoVBztaTTqiVIiEJRATcVGWbN8Q1DJk24aBXr26ya9i3ukMUWSQgABBBBAAIGwBAgwhCVPvggELBDbHJPSL1cFnAvJp0Oge5+u0ndAz3QkRRoIIBCAAO1pAKgBJUl7GhAsySIQEYGqDbVSvqIiIqXd+mLm5eeZUQAdO+VvfWJpSGFz02apraqTxk1NCVPr1rNQ+g/qnfA8TkAAAQQQQAAB+wUIMNhfR5QQgXYJNDhDxFcwRLxddpm+qKBrJ9lmeL9MZ0t+CCCQpADtaZJQFpxGe2pBJVAEBEIUqFrvBBhW5kaAoXNhJylxRsHmO0EG27bVS9ebQENb5erSvUBKhvRp6xSOIYAAAggggEBEBAgwRKSiKCYCqQrU1znrLyzKnfUXOnbOlx59uknngk4iHVLVSv/5+sTzxpr6pBbY1qfOBo8uTn8hSBEBBNIiQHuaFsZ2J0J72m46LkQg5wRyKcAweNti0e+/Nm6xWEyWzVstTc6IhtY2AgytybAfAQQQQACB6AkQYIhenVFiBJISyKUOMX2Ca5vhfaVDngWRhRa1s7G6XsqWrGuxt/mf+R3zZMiYkuY7+QsBBKwRoD21oypoT+2oB0qBgM0CuRJgiMJ3x0SjGAgw2PxfEmVDAAEEEEAgNQECDKl5cTYCkRHIpQ6xAU5wobBrZ2vrZs2yDVJTubHV8kXhR2KrhecAAjkgQHtqTyXTntpTF5QEARsFciXAoGsvDB1r98MpqxaXS11tQ6sfEwIMrdJwAAEEEEAAgcgJEGCIXJVRYASSE8ilDjH9gaU/tGzdKsqrZX1ZVavFI8DQKg0HELBCgPbUimowhaA9tacuKAkCNgrkSoBB7QcMcx6w6WbnAzYaWNAAQ1sbAYa2dDiGAAIIIIBAtAQIMESrvigtAkkL5FKH2MBR/Z21FzombZPpE9cu3yDVFYxgyLQ7+SGQLgHa03RJbn06tKdbb0gKCGSzQC4FGPQBlWJnkeSCLs76Y5Zsm501yGqr6mSds9C2vm9rI8DQlg7HEEAAAQQQiJYAAYZo1RelRSBpgVzqECvq1UX6DeyVtE0mT0zmCS5GMGSyRsgLgdQFaE9TNwviCtrTIFRJE4HsEsilAEPUa65jYQcZPHJA1G+D8iOAAAIIIICAI0CAgY8BAlkqkEsdYlqF3Xt3lW49u1hTmzHnqS3tDKtYW52wTAQYEhJxAgKhCtCehsovtKfh+pM7AlESWL2iXGo3tD7vf5TuJdvL2ij1su0Ow7P9Nrk/BBBAAAEEckKAAENOVDM3mYsCNdUbZc2SDbl465G758amBtl252GRKzcFRiBXBGhPo1PTtKfRqStKikAQAvPmLpBOsa7SoUOHIJInzTQKrCpfLnvtt3saUyQpBBBAAAEEEAhLgABDWPLki0DAAuVr10vV6rqAcyH5dAhUVlfIuInbpSMp0kAAgQAEaE8DQA0oSdrTgGBJFoGICMyd/ZXUrK+T/v2KI1Li3Cxm7cZaWbLiazn86INzE4C7RgABBBBAIMsECDBkWYVyOwi4AqvL1srKRWukZ0871yZwy8mryMLF8+Wg70yCAgEELBWgPbW0YnyKRXvqg8IuBHJIQAMMn8z4TPbcY2/Jz8/PoTuP1q1+8tks6VJUKIcddVC0Ck5pEUAAAQQQQMBXgACDLws7EYi+gHaIffj2LNlph52jfzNZfAf19fXy8WcfySlnnpjFd8mtIRBtAdrTaNQf7Wk06olSIhCkwPwvFspbr78rA0oGyI7b8x04SOv2pr1kWanMXzBPRo0eIQccsl97k+E6BBBAAAEEELBIgACDRZVBURBIp8AaJ8DwzBPPy/hdJkjv3n3SmTRppVFg9pzPpKa2igBDGk1JCoF0C9Cepls0mPRoT4NxJVUEoiSwcnmZPP/0i6bIJcUDZPuxOzCSwZIKbGpqksVLvpbFpV+bEo3ffRfZbeIulpSOYiCAAAIIIIDA1ggQYNgaPa5FwGKByg1VMu2fT0rHjh1lj/F7SteuXS0ubW4WrXTJYlmwaL707ddHjjv5qNxE4K4RiIAA7an9lUR7an8dUUIEMiUw9a//lMbGRpNdp06dZNiQ4WZEQ0FBYaaKQD5xAjq6rHzdWuc77wLZtKnBO/LdU46R3n2YytUD4Q0CCCCAAAIRFiDAEOHKo+gIJBJ48K//kKbGJhNk2H7sjlLcnwXvEpll4rj+0Jq/cJ6UrV5lstt27EjZ/6BvZSJr8kAAgXYK0J62Ey7gy2hPAwYmeQQiKPDBOzPk88++2KLknTt3li5dukqHDh22OMaOAARiMamuqfaCPfE5DBhYIkced1j8Lt4jgAACCCCAQIQFCDBEuPIoOgKJBF567jVZumSZd1qP7j2cJ7i2kV69eku3rt0kLy/PO8abYAXq6uqkqrpK1m9YJ0uXLWmW2aFHHiRDhg1qto8/EEDALgHaU3vqg/bUnrqgJAjYKNDQsEmefPQZqa6qsbF4lMkROPakI6Vf/75YIIAAAggggECWCBBgyJKK5DYQ8BNYu3qtPP34836H2GeJQB9neqTjmR7JktqgGAi0LkB72rqNLUdoT22pCcqBQPgC5WvXm7UYGur/OyVP+KWiBCrwrW/vI2O33xYMBBBAAAEEEMgiAQIMWVSZ3AoCfgIfvjdTZn8yx+8Q+ywQOPqE70hxST8LSkIREEAgkQDtaSKhcI/TnobrT+4I2Cag6+e88eo7sqZsjW1Fy8nyFHYplH0n7SnDRw3LyfvnphFAAAEEEMhmAQIM2Vy73BsC/xF4/aW3ZNGCxXhYJrD/wfvJtmNGWFYqioMAAm0J0J62pRPeMdrT8OzJGQHbBZaWLpcF8xbJ2tXlUllRaXtxs6p83bsXSf8B/Z3vuyOZDjSrapabQQABBBBAoLkAAYbmHvyFQNYKzP3sS5n50SfSUMdQ8bAruWSbYtl9z91EF7hjQwCB6AnQntpTZ7Sn9tQFJUEAgWAFHnzwQZPBWWedFWxGpI4AAggggAACCKQoQIAhRTBORyDqAl8vLJVVK8pkw/oKaWxsjOTtNDR8EyTp3LlzZMpf1K2b9C3ua57g6tqtS2TKTUERQKB1gWxoT1u/O3uP0J7aWzeUDAEEghMYPny4dOjQQb7++uvgMiFlBBBAAAEEEECgHQIEGNqBxiUIIBCugPvklvskV7ilIXcEEEAAAQQQQAABBIIT0O+8Z599tsnggQceEPe7cHA5kjICCCCAAAIIIJC8AAGG5K04EwEELBBYvHixjBjxzboF+gSXPs3FhgACCCCAAAIIIIBAtgro993S0lJze/qeUQzZWtPcFwIIIIAAAtEUIMAQzXqj1AjkrIA+sTV16lRz/2eeeaYwiiFnPwrcOAIIIIAAAgggkPUC8aMX3JtlFIMrwSsCCCCAAAII2CBAgMGGWqAMCCCQlED86AX3AkYxuBK8IoAAAggggAACCGSbQPzoBffeGMXgSvCKAAIIIIAAAjYIEGCwoRYoAwIIJCUQP3rBvYBRDK4ErwgggAACCCCAAALZJOA3esG9P0YxuBK8IoAAAggggEDYAgQYwq4B8kcAgaQE/EYvuBcyisGV4BUBBBBAAAEEEEAgWwT8Ri+498YoBleCVwQQQAABBBAIW4AAQ9g1QP4IIJCUgN/oBffCn/zkJ3L77be7f/KKAAIIIIAAAggggECkBdoaveDeGKMYXAleEUAAAQQQQCBMAQIMYeqTNwIIJCXQ1ugFTaBnz56i5/Tq1Sup9DgJAQQQQAABBBBAAAGbBdoaveCWm1EMrgSvCCCAAAIIIBCmAAGGMPXJGwEEkhJoa/SCm8C1114r1113nfsnrwgggAACCCCAAAIIRFIgmdEL7o0xisGV4BUBBBBAAAEEwhIgwBCWPPkigEBSAolGL7iJZOMohrPPPlvee+89ufrqq+WMM85wb7XZ69KlS+Xggw82+2bMmCFz584VXfg62W3nnXeW2bNnJ3u6Oe/jjz+WqqoqmTRpUqvXjR49WvbZZx+5+OKLpaioqNXzOIAAAggggAACdgrwPSS8eklm9IJbOkYxuBK8IoAAAggggEBYAgQYwpInXwQQSEogmdELbkLZNorhoIMOktdee82sL6HrTPhtCxYsEO3M123dunUya9YsL+Dgd37LfRMmTJCZM2e23N3m3xUVFVJZWSlDhgxp8zw9OHjwYNGARL9+/RKeywkIIIAAAgggYI8A30PCqQu/0Qv68MjUqVNNgfbff3958803mxWOUQzNOPgDAQQQQAABBDIsQIAhw+BkhwACyQskO3rBTVHXYPj666+zZi2G9vyw19ECa9ascUnM6zPPPCPnn3++FBcXm87++IMFBQVSX18fv0tWrVolGnjQ7f3335ehQ4c2Oz5w4EBZtmyZF2CYN29eswCCHvvggw/k3HPPNdedc845cu+99zZLgz8QQAABBBBAwG4BvoeEUz/xoxc0sKBTgOq+Dh06mALFYjF54403zH430KDH9TswGwIIIIAAAgggEIqA8wWFDQEEELBSwHlqP+Y0jN4/w4YNizlPaHl/63vdF3+OM4rByntpT6EOPPBAc2+33357q5fPnz/fu39nBIPvedOmTTPnOKMJfI+33LlixQovzdLS0paHzd/O1EzeOU5Aw/ecO++8M6V8fRNhJwIIIIAAAgiEIsD3kMyzu99zncBCzAkYNCuA+303fufrr78ec0Y0mO9bei0bAggggAACCCAQhgAjGEIJ65ApAggkEtiwYYOMGDFC9NUJIpintHS6JN3in+DSv3UouT7d5XSGm9EL2TKKoT1PDvbu3VtJmm2PP/64nHTSSWa6Il2zIdG2cuVK0VEKuqlpyxEMuj9+BIOOmPCbAumVV16RQw45RE+XxsZGyc/PN+/5FwIIIIAAAgjYL8D3kMzXkX6f1e+7OiKh5dby+2/8cR3RoN+H9R82BBBAAAEEEEAg0wIEGDItTn4IIJCUgP7A0h9J7g+t+Ita+4EVf75eF/Ut6j/snREMZpHnkSNHysKFC6NeHZQfAQQQQACBnBLge4hd1d3a91+7SklpEEAAAQQQQCAXBQgw5GKtc88IREDgqaeekuOOO863pIl+YLV1rW+Clu50f9jr2gm6WLLfVl1dLboGgm66yHMYIxjmzp0r/fv3N2VwhuKJ/u3ODaw7r7zySrnpppvMcf6FAAIIIIAAAtEQ4HuIXfWU6PuvXaWlNAgggAACCCCQSwIEGHKptrlXBLJEIFd+YLk/7JOttrACDG2Vb9y4cfLWW29Jz5492zqNYwgggAACCCBgmQDfQ+yqkFz5/muXOqVBAAEEEEAAgWQECDAko8Q5CCBglUCu/MByf9hfddVVcs455/jWweLFi711DsIKMOywww5SWFjolU/XWhg1apRMnDhRzj///GbHvJN4gwACCCCAAAJWC/A9xK7qyZXvv3apUxoEEEAAAQQQSEaAAEMySpyDAAJWCeTKDyz3h/3tt98uP/nJT3zrYMGCBTJ69GhzLKwAQ2uLPPsWmJ0IIIAAAgggEAkBvofYVU258v3XLnVKgwACCCCAAALJCBBgSEaJcxBAwCqBXPmBxQ97qz52FAYBBBBAAIGcEuB7iF3VnSvff+1SpzQIIIAAAgggkIwAAYZklDgHAQSsEsiVH1j8sLfqY0dhEEAAAQQQyCkBvofYVd258v3XLnVKgwACCCCAAALJCBBgSEaJcxBAwCqBXPmBxQ97qz52FAYBBBBAAIGcEuB7iF3VnSvff+1SpzQIIIAAAgggkIwAAYZklDgHAQSsEsiVH1j8sLfqY0dhEEAAAQQQyCkBvofYVd258v3XLnVKgwACCCCAAALJCBBgSEaJcxBAwCqBXPmBdeihh8rLL78sU6ZMkYsuusi3DhYtWiSjRo0yx1pb5Plf//qXnHDCCTJy5EhZuHChbzrxO8vKymTAgAFm15IlS2TIkCHxh837FStWyKBBg8z78vJy6dOnzxbnsAMBBBBAAAEEoivA9xC76i5Xvv/apU5pEEAAAQQQQCAZAQIMyShxDgIIWCXADyyrqoPCIIAAAggggAACCAQswPffgIFJHgEEEEAAAQTaLUCAod10XIgAAmEJuD+wwsqffBFAAAEEEEAAAQQQCEMgFouFkS15IoAAAggggAACrQoQYGiVhgMIIGCrAAEGW2uGciGAAAIIIIAAAggEKUCAIUhd0kYAAQQQQACB9ggQYGiPGtcggAACCCCAAAIRFnADtXRURbgSKToCCCCAAAIIIIAAAgggYIEAAQYLKoEiIIAAAggggAACmRQgwJBJbfJCAAEEEEAAAQQQQAABBLJXgABD9tYtd4YAAggggAACCPgKEGDwZWEnAggggAACCCCAAAIIIIBAigIEGFIE43QEEEAAAQQQQCDqAgQYol6DlB8BBBBAAAEEEEAAAQQQsEOAAIMd9UApEEAAAQQQQACBjAkQYMgYNRkhgAACCCCAAAIIIIAAAlktQIAhq6uXm0MAAQQQQAABBLYUIMCwpQl7EEAAAQQQQAABBBBAAAEEUhcgwJC6GVcggAAC7RKoqqqS3XffvdVri4qKZOLEiTJp0iQ55ZRTxO0AbPUCDiCAAALtFHDbl1gs1s4UuAwBBKImwPeQqNUY5UUAAQQQQAABBKIhQIAhGvVEKRFAIAsE1q9fL3369EnqTs444wy55557pKCgIKnzo353uAtIAABAAElEQVTShAkTZMOGDTJjxoykjaJ+z5QfgTAFCDCEqU/eCIQjwPeQ1t35HtK6DUcQQAABBBBAAIFEAgQYEglxHAEEEEiTQPwP+1mzZsnw4cO9lPUp4tLSUnn88cflN7/5jdn/2GOPyUknneSdk81vevToIfpk5apVq6SkpCSbb5V7Q8AKAQIMVlQDhUAgowJ8D2mdm+8hrdtwBAEEEEAAAQQQSCRAgCGREMcRQACBNAnE/7D/+uuvmwUY4rM46qij5N///rdcdNFFMmXKlPhD3vt169ZJ9+7dpVOnTt6+1t6Ul5ebUQFuh2Jr57Vnf7LlqK6ulrKyMhk0aJAUFhZukRU/7LcgYQcCgQq47QFTJAXKTOIIWCXA9xC+h1j1gaQwCCCAAAIIIJA9As4PSzYEEEAAgQwIOJ3xOtm5+ccJMLSa45VXXmnOOfnkk5ud4zzhH7vgggtixcXFXjpHHHFE7O233252nv7R2NgYu/baa71z9ZoLL7wwNm3aNLPvF7/4hblGy6THRo4cuUUab731ljl2zjnnNDuWSjnefffd2B577OGVV+9//PjxsZkzZ5o0n3rqKa+MekzLcuyxxzbLjz8QQCD9Am5blP6USREBBGwV4HvIN9/B+B5i6yeUciGAAAIIIIBAVAUkqgWn3AgggEDUBJL5Yb958+bYAQccYDrk//jHP3q3uHbt2tiYMWPMfmfkQkwDCxoUcDsJH3nkEe9cfXP66ad7x/bdd9/Y4MGDvb/1mvPPP9+cv3r1am9/swScP1588UVz7Oijj/YOpVKOL774wkt73LhxsbPOOssEF9wyL1iwwOSx1157eefpj/5zzz3Xy483CCAQjID732EwqZMqAgjYKMD3EL6H2Pi5pEwIIIAAAgggEH0BAgzRr0PuAAEEIiIQ/8P+1Vdfjc2fP9/757PPPovdd9995ul97fjTIEL8KIdLL73UdMJrsEDTcbfbbrvN7Nfz6+rqzG4d0eCm8cEHH7inxu6//36vI7+9AYZUynHNNdeY/JwFq70yaABFR1Jo+X7/+997+7X8us9Zg8HbxxsEEAhOQP9703/YEEAgdwT4HhKL8T0kdz7v3CkCCCCAAAIIZE6AX5aZsyYnBBDIcYH4H/Zu515rr59//rmnFX/dc8895+1337gjG55++mmzy1kY2nQc/upXv3JP8V515IPm2Z4AQ6rl0Dw0Lx2V4AY/tCDLly+PTZ06Nfbmm2965SLA4FHwBoGMCLhtT0YyIxMEELBCIP7/424b0Nor30OsqDIKgQACCCCAAAIIREKARZ6db9VsCCCAQCYE4hdXdKZBMos0u/k2NDTInDlzZNmyZWaX0ykv06dPN4s4z5gxQyZOnGj2//nPf95iYWdnKiWZNWuW3HjjjeKsrSBjx46VefPmiTN6wbvOzccJOoizNoM4nf9y9913y5o1a8RZ98Acdv6v5Z5mXl966SU57LDDxJkiSZ555hlJtRzPPvusHHPMMSYtJ4AgzrRN8u1vf1sOPvhg6d27d7O8WOS5GQd/IBC4AIs8B05MBghYJ8D3EL6HWPehpEAIIIAAAgggkB0CkQiDUEgEEEAgCwTinxyMn/4o/tbcdQ+c/8PE3n//fXPoscceMyMBdF9b/+jCzU1NTd45usZBy+2hhx4yx5MZwaAjIjQ/dw2GVMrh5vu3v/1ti/UfNM0jjzzSjGRwz2MEgyvBKwKZEXDbkszkRi4IIGCDAN9D/vs9iu8hNnwiKQMCCCCAAAIIZIsAIxicX9hsCCCAQCYE4p8cdAIMMnz4cN9sd9xxR5k7d67cdddd8uMf/1jckQR6shN0EPfJ45YXOws5y6BBg8QdDaDnOgsoNztNRzlcffXVSY1gcNZ3kMsuu8wbwZBqOdyMnf9hyuzZs+Wtt94yIyFefvllc2i//fYz+/QPt8zOGgxSUlLiXsorAggEJOC2I/rfJxsCCOSGAN9D+B6SG5907hIBBBBAAAEEMi6QLZES7gMBBBCwXSCZJwf1Ho4//ngzcuCmm24yt6SLQTv/czD/6GLQLbd77rkndvnll8ecaZLMISeoYM594IEHWp4aO+WUU8wxdwRDfJlqamqanX/OOeeYc90RDKmW4/rrrzflqq6ubpZu/CiN2tpac4wRDM2I+AOBwAXcNiXwjMgAAQSsEYj/f35rIym1sHwPsabKKAgCCCCAAAIIIBAJARZ5jkQ1UUgEEMgGgWR/2J911lmmY1+nPNJt06ZNsR122MHsu+qqq5pRfPTRR2a/dhYuXbrUHHNGKJh9I0eOjG3YsME7/5133vHOdQMMetBZg8Hsj19A2lm/wTvXDTCkWo59993XpPHII494ZdA3r7/+utmvQYXGxkZzzA0waBCDDQEEghcgwBC8MTkgYJsA30O+qRG+h9j2yaQ8CCCAAAIIIBB1AQIMUa9Byo8AApERSPaH/cUXX2w64N2Ofb3BN954w+vwP+GEE2K33HJL7KKLLvL2/fznP/ccKioqvKCBBhmuuOKK2JlnnhlzO/G1YzE+wDB58mQvnR/84AdmzQW381Ff21uO++67z0tXy3D//febsrgBjQsvvNArs7PotTnXWdw65o7c8A7yBgEE0i7g/jee9oRJEAEErBXgewjfQ6z9cFIwBBBAAAEEEIi0AAGGSFcfhUcAgSgJOHMfex3upaWlrRb95ptv9s778MMPvfOctQu8wIHbOahBgz/+8Y9mlIN3ovNGRwK4UyW552qw4corrzRpxwcYtMPBHW3gnqsd/e6izscee2x80rFky6ELTl9yySXevbhp66uO0oifkmnatGleAGSPPfZolh9/IIBA+gXc/x7TnzIpIoCArQJ8D/lmukm+h9j6CaVcCCCAAAIIIBBVARZ5dr5hsiGAAAJREWhoaJAvvvhCFixYIH379pXdd99dioqKfIvv/I9JFi5cKIsWLZLRo0ebRaUffPBBcUYpeIs8x1+4evVqcQIT4owwkFGjRkleXl784WbvUynHypUrxZnKSZyRFSbtMWPG+C5wXVVVJWVlZeYcXfSZDQEEghNgkefgbEkZgWwWSOX//3wPyeZPAveGAAIIIIAAAgj8V4AAw38teIcAAghkvYCz8HOrAYasv3luEAEEPAECDB4FbxBAIIMCfA/JIDZZIYAAAggggAACGRIgwJAhaLJBAAEEbBDgh70NtUAZEAhfgABD+HVACRDIRQG+h+RirXPPCCCAAAIIIJDtAgQYsr2GuT8EEEAgToAf9nEYvEUghwUIMORw5XPrCIQowPeQEPHJGgEEEEAAAQQQCEiAAENAsCSLAAII2CiwZs0as87CgAEDxFn02cYiUiYEEMiAAAGGDCCTBQIIbCHA95AtSNiBAAIIIIAAAghEXoAAQ+SrkBtAAAEEEEAAAQRSEyDAkJoXZyOAAAIIIIAAAggggAACCPgLEGDwd2EvAggggAACCCCQtQIEGLK2arkxBBBAAAEEEEAAAQQQQCCjAgQYMspNZggggAACCCCAQPgCBBjCrwNKgAACCCCAAAIIIIAAAghkgwABhmyoRe4BAQQQQAABBBBIQYAAQwpYnIoAAggggAACCCCAAAIIINCqAAGGVmk4gAACCCCAAAIIZKcAAYbsrFfuCgEEEEAAAQQQQAABBBDItAABhkyLkx8CCCCAAAIIIBCyAAGGkCuA7BFAAAEEEEAAAQQQQACBLBEgwJAlFcltIIAAAggggAACyQoQYEhWivMQQAABBBBAAAEEEEAAAQTaEiDA0JYOxxBAAAEEEEAAgSwUIMCQhZXKLSGAAAIIIIAAAggggAACIQgQYAgBnSwRQAABBBBAAIEwBQgwhKlP3ggggAACCCCAAAIIIIBA9ggQYMieuuROEEAAAQQQQACBpAQIMCTFxEkIIIAAAggggAACCCCAAAIJBAgwJADiMAIIIIAAAgggkG0CBBiyrUa5HwQQQAABBBBAAAEEEEAgHAECDOG4kysCCCCAAAIIIBCaAAGG0OjJGAEEEEAAAQQQQAABBBDIKgECDFlVndwMAggggAACCCCQWIAAQ2IjzkAAAQQQQAABBBBAAAEEEEgsQIAhsRFnIIAAAggggAACWSVAgCGrqpObQQABBBBAAAEEEEAAAQRCEyDAEBo9GSOAAAIIIIAAAuEIEGAIx51cEUAAAQQQQAABBBBAAIFsEyDAkG01yv0ggAACCCCAAAIJBAgwJADiMAIIIIAAAggggAACCCCAQFICBBiSYuIkBBBAAAEEEEAgewQIMGRPXXInCCCAAAIIIIAAAggggECYAgQYwtQnbwQQQAABBBBAIASBXAkwfPDBBzJ9+nSZPXu2LFy4UFauXCmVlZXS1NQkhYWF0q9fPxkyZIhsv/32Mn78eNl3331l4MCBIdQIWSKAAAIIIIAAAggggAAC0RQgwBDNeqPUCCCAAAIIIIBAuwWyOcDw8ssvy6OPPipPP/2UrF1bnrLR3nvvLSeccIKcdtppMmDAgJSv5wIEEEAAAQQQQAABBBBAIJcECDDkUm1zrwgggAACCCCAgCOQjQGGqVOnypQpU2TWrFleHY8Z3Ef23nGQ7DqyWEY77wf2LZKe3QokP6+DbGxolLUVG2VxWYXMLV0rH81bJW99tlTqnf3udt5558mll14q2223nbuLVwQQQAABBBBAAAEEEEAAgTgBAgxxGLxFAAEEEEAAAQRyQSCbAgw6YuGaa64RnQ5Jt6ElPWTyQTvKCd8aIzsM65dSdTZtjsmz782XR9/8Up5xXt3t8ssvlxtvvFE6derk7uIVAQQQQAABBBBAAAEEEEDAESDAwMcAAQQQQAABBBDIMYFsCTD89Kc/lT/84Q+m9rYd1FsuO2EPOfOwndNSm18tLZc7npwpU1+cbdIbM2a03HnnXXLooYemJX0SQQABBBBAAAEEEEAAAQSyQYAAQzbUIveAAAIIIIAAAgikIBD1AENpaamceeaZ8uabb5q7/vnkveXq0/ZJQSD5U6fPXSHXPPiWvDdnubnolltuEQ1ssCGAAAIIIIAAAggggAACCDCCgc8AAggggAACCCCQcwJRDjB8/PHH8t3vHi+LF5fK2CF95a6LDpF9nHUWgt6uffAduWXaN9Mw/exnP5Obb7456Cwzkv6aNWtkyZIlUlJSIgMHDpS8vLyM5EsmCCCAAAIIIIAAAgggkB0CjGDIjnrkLhBAAAEEEEAAgaQFohpg+PTTT+Xwww6VVWWr5ZDdR8jUK440izYnfeNbeeI/Xpkj5932fyaVSy65RG677batTDGcy9etW2dGYTz//POyevXqZoX40Y9+JL/+9a+lX7/U1q9olgh/tEvg1FNPNYuUP/LII7Lbbru1K410X/Tqq6/KBRdcIKeffrpZ6yTd6ZMeAggggAACCCCAQPQFCDBEvw65AwQQQAABBBBAICWBKAYYVq1aJZP220/mL1ggx+wzWh7+32NSuud0nfzs+wvklBueNsn96le/ilyn60cffSTHH3+8LFu2zNxDcXGx7LLLLrJ8+XKZO3eu2de9e3eZOXOmjB49Ol1svuk8/PDDcvXVV8vZZ59tXn1PyqGde++9t0yfPl3effdd2WefYKb8SpXzqaeeMp+XCy+80FmD5M5UL+d8BBBAAAEEEEAAgRwQIMCQA5XMLSKAAAIIIIAAAvECUQwwHHHEEfJ///d/8u1dh8pzN54UfzsZf//Ym1/K2b//t8n3sccek5NOCrc8yQKUl5fLiBEjpKqqSkaOHCla9gkTJniXf/bZZ6YzedGiRTJu3Dj58MMPpaCgwDue7jd/+tOfzNPxF198sdxxxx3pTj5y6b3wwguiU1bpZ71///5WlJ8AgxXVQCEQQAABBBBAAAGrBQgwWF09FA4BBBBAAAEEEEi/QNQCDNdff71cd911MrSkp7x562Qp7t01/SgppnjLYx/KtVPflj59esvs2Z+b9QtSTCLjp19xxRVm7QgdtTBv3jzp2bPnFmXQII52cOv28ssvy8EHH7zFOcnu2Lx5s6xYsUI6duwoAwYM2OKyZAIMFRUVZhqnIUOGSGFh4RZptNzR2NhoAii9e/dudkj369RQeu+JNj1PR3F06tQp0anmuI6u0Sml9D7dbe3atVJdXS1a7vz8fHf3Vr82NDSImqQrAKF1pGtwdO7c2fcz3N4Ag6arjn379hW3vdnqmycBBBBAAAEEEEAAASsFCDBYWS0UCgEEEEAAAQQQCE7A7fCLxWLBZZKmlHWqnt13392k9uR135VD9xiRppS3PplTbnxann1vgZx22mny97//fesTDDAFHbXQo0cPk8Mf//hHM3Kgtex0RMGcOXPkzDPPlO9///vy0ksvyRlnnCFHH3203Hvvvc0uu+WWW0zQ4re//a384Ac/MMc2btwoN9xwg/zmN7/xztUOe53LX9d30I77b33rW6IjJrRcumnH/+OPPy77OdNg6aZ5XnTRRSYQYnY4/zrkkENEyx4/ddNxxx0n77//vjzxxBPy0EMPia5foGnuu+++cvnll8ukSZPkhz/8oUybNs0ko/n88pe/lB//+MdusuZVgwFXXnmlKYO7LoUGWn7xi1+Ysronz5o1ywRgTj75ZOOh02TplEZffvmljB07Vh588EGTjpuGXqdTUv35z39OGNzQz9Enn3wi//znP820VbW1tSbNYcOGmXT1Pl577TVTFL2Pq666Si699FLz98SJE6W0tFR+97vfyVlnnWX2uf+aMWOGHHXUUVJUVCRfffWVCYSokV573333uaeZoMqNN95o6skNirQWYLjnnnvMtFYamHjyySe9/0Z19It+fv79729G+Gi9H3vssXLTTTfJoEHBL8bu3QxvEEAAAQQQQAABBDImQIAhY9RkhAACCCCAAAII2CEQpQCDdmo/99xz8qNjdpNbf3igHYD/KcXiVRWy6w8fkE2NTWb6psMOO8yq8sUX5uOPP5bx48ebXfpkecsn/OPPbfne7WTWunjmmWeaHb7mmmtMMEEXvNaFr3U7//zzTYe6di7rCAh9st/t4D/vvPPkL3/5i5x44onywQcfmLUg9Lwdd9zRTJOkHeUaxDj33HNNWnvttZcMHTpU3nvvPW/diNmzZ8tOO+1kjrvrFpg/nH9px3t8575OBaWd3ppHly5dvGM6HdHhhx9uLtOpo3TNAx3Voedp8EM74vU63TRo8b3vfc+813Jo8KJlPvPnzzfrJ2ggRrcDDjjAjF54/fXXTbm1HHpOXl6eOe73L/de3DUYampqTFBAz9VyaVBAAzC6XoZbNneKLnd0yoEHHii6MHP89rOf/UxuvfVWE3h44IEHTDoatHPvVx10FMbbb79tLtN70GCNbm7dx6/B4I480eP6edDPhW66fsShhx5q0tf71YDLO++8Y/5Wr1deeUV23nlncy7/QgABBBBAAAEEEMgeAQIM2VOX3AkCCCCAAAIIIJCUQFQCDPoUu3ba9+xWIF88cK55TeoGM3jSTY9Ml1//7V3x69jNYDESZqVP+GunvnZUV1ZWJjw//gS3kzmZAENdXZ3pyNfrtQPbHW2gT9cPHz7cJKsd5127dhW3ozp+DQYNfuh52pmuneHu0/g6xdHkyZNNoEI7+LUzXD/Hbqe8JqxBA+3g1gWsdbSD5q/b3/72N9ERBxro0BEPzz77rHn6X0dd6HbZZZeJBkg0XT3mBl9uv/1285S/munaCLoehRtg0Ot0nQodAaCd5nrMLYvuix/NoQELHfmgIy00YNLa5l7vF2AYM2aM/Otf/zKBGB15dPrpp5uRDhrMufvuu+XTTz+VXXfd1SStwYKSkhLzvqmpyUx9pEEXDXZo4ENHXVx77bUm4KTTYPXp08ecG39vL774orF0694NMOgIEn2v2/PPP+9Np6X5aABLR6VooOnmm2823ps2bTIjfDTA5Pf5MQnxLwQQQAABBBBAAIFICxBgiHT1UXgEEEAAAQQQQCB1gagEGHRqFX1C+uoz9pWfn9J6x2zqAum7YlPTZhl1xl+kvKLW68BNX+rpS0k7oXVaoB122MFMf5RKym4ns18HccsRDLrmgjsVjnYqa1DD3dxFjI888kgzN79fgOGuu+4yUyMNHjxYli5d6l5qXleuXOmtE6ABCx3Z4HbKa6e5lsXddKSETuOjnyEtv7s9/PDDJlCh0x9pB/n69eu9DnYdKaNli9/0KXwNVDz99NNyzDHHNAswvPXWW96UTnrNqFGjzMgCndppypQpXjI6ekRHXey5557mqX7vQIs37r34BRh0aqRvf/vb3hU6BZFOe6QBCw1c6KajQObOnWtGgJxzzjlmn5Zx//33F/VUM/1v3x1FoVMxnXrqqeY8919adq0Dd6SJW/caVND7c6dkark+R/zaHRqM0TUp3C3+M1FWVpZwqij3Ol4RQAABBBBAAAEEoiFAgCEa9UQpEUAAAQQQQACBtAlEIcCg08noU9t5eR1kyT8vkN7dEy/wmwho/rJ18vbsZXLwhOEytPib9QgSXZPM8eudEQy/d0Yy6JoFDzpz8Nu4uR3Sfh33icrrdjInE2DQtHbZZRfzJLu+1yl9tCNcO8f1CXd3bn895hdgaNnBrefFb/pkvj6N73a4u53yLYMDbuBDXzX44G4a5PjOd75jyqWd77o+gU7LpJuuk9ByYWd9Yl9HH+jaBLoeg/uUv075owEPt7Ner3enItL3+tk94YQTzMgWNdARDok29178Agy6RkS3bt28JD7//HMzckKnIlq4cKHZ74640FE/2uGvm+vpBmDiO/vdII058T//0s/v2Wef7fm4dR9/jhucid/3hz/8QX7605+aQIYuyt5ycwMeOvJER3SwIYAAAggggAACCGSPAAGG7KlL7gQBBBBAAAEEEEhKIAoBBp2+Rjt0Tz1wB7n3p0ckdV+JTrpgyksy9cXZ8tg1x8mRe41KdHrSxxeuWC/jzr3fdCLr9EO68K1tm64psN1225li6TRGbXV466gRfUJ9t912M1P9uJ3MfgEG7VTWzuX4NRh0ip6rr7662QLCmrF2yuvCz25ns1+AQdcD0Ol54qdHird0O+HdtQfcvz/88EPZY489vFPdAIN+jnQxZHdrGWDQURY6fVKiTT+LGmRwAww61ZBOORS/6TROOvVQ/OLW7nHt6Nd1EFoGMNzj+urei1+AoeWC7DpSQUcsxAcY4oMHOoqgZ8+eZqSITjelQQg9V9dJ0Hy0LnQ0QcvNnZbMTdet+5bnxa+9oMd0dIyOkkm0uVMvJTqP4wgggAACCCCAAALRESDAEJ26oqQIIIAAAggggEBaBKIQYND58LUz95Grj5Wj9952q+67wVmE+fE3v5Jz//CCSSfdAQZN9KDLH5bpc1eY6Xh0Wh7btoaGBi+ooHP5H3/88b5F1I5sDUTotEBu57zbyewXYNBpg3TdgvgAg5uwdmxrJ7z+o9PxuIsvu6MP/AIMupiyBg/cJ+7dtPRVO/DdDnq3E97tlP/oo49kwoQJ3unJBhjcDnW9UKcacv/b8BL6zxsd+aFTP7UVYHCvqa+vNwtYv/HGG/Loo4+aaYv0mD7Z/8tf/tI9bYtX917ce4tf5DmZAIMmqKNFdLSKBmi0zLoWRXwwxB35oOe6a2Hoe3e75ZZb5PLLLzdrn+goCLfudR0KrTcNyPz+9783AQr9jGgQQ7f//d//NYEV/e9WAymtbdtvv7306JG+0UOt5cN+BBBAAAEEEEAAgcwJEGDInDU5IYAAAggggAACVgi4nagtOy2tKJxTiIqKCunVq5fT2StS/uQlUtApv11FW1leLd+74WmZOW9Vs+uDCDDc+I/35Tf/fM8scKud7TZuusCxriWgixPPnDnTLMLbspwaDNAFq3VzRwW40ytp5/E777zjXaKfH10HQRdVdgMM+oS8BjD22Wcfs6Cye7IuAqxrAWjnudvR7hdgcAMDLfPSdHQtA51mSTd9Wn+bbbbxnvpvb4BhwYIF3kLUukCxLtgcv917770m2KJrFeiIjtYCDDpiQBc21pEBOlVS/OZ2vutUSTotU2tbOgIM7ogMXUtiwIABZhSJLnKti0LrFh+0ePXVV726dsv03e9+V5588kmzkPOdd97pBRjcRZ51qiZdi0GDRe4C03qtO7WSBiJ0XYv4qbA0MKR1roEXHdlCgMHV5hUBBBBAAAEEEMgOAQIM2VGP3AUCCCCAAAIIIJC0gO0BBu34PPjgg2XP7QfKa7c0X4Q26Zt0Tiwtq5T9L/uHd8maDbXmfRABhtc/LpWjrn7cdKxrJ7qN2/Lly0WfINeRBRpE0Kfr4xfj1eCBrk+gxydPniz/+Mc3djq9zrbbfjOKRKc/0nUQdLvjjjtMQEXfuwEGfcr9oIMOMmsQzJkzp1kQY9KkSaJz8Osiwjqljq55oJ3U8WtXqJ07R79er4tSu9sFF1xg1m3QznNdc0E3t1O+vQEG7fzWNSN0yiGdSklHbbibBmF2331386cuOK0jAloLMNTW1nprJCxZskSGDBniJmNGY+jUSS0XnPZO+M8b9162ZgSDlkMDC1qH7qbvi4qK3D/l0EMPNVNgqfv999/vrSMRP42WG1xyRzC4AQZNRKdHckfpvPnmm6L1qus5DB8+3OShi2frOg3u5i7c7U675O7nFQEEEEAAAQQQQCA7BAgwZEc9chcIIIAAAggggEDSArYHGPTJ6YsvvljO+c4uMuXHByd9X4lOnHTpP8xohiACDOWVG2XoqXeLPsGt6zDYuunT7N///ve94um6BaNHjxbtoNcpb3TTEQ6vvPKK9O/f3/ytIxW001qfWtdOdp0qSZ/2jw+kuAGGjRs3yk477SSLFi2SvfbaS0477TQz7ZCu6aCjJ3TTYyNGjBDtnNbpe3TTzu4rrrjCBBROPPFEeeKJJ4ylBiD69u1rRk7oVEy66cLMbse/2ynf3gCDphdfDl2YWdPUDnP9HOr285//3FtXobUAg56nruqroxguvfRS46cd9X/961/1sJleSO+ttc29l60JMGjaaqbBG93OPfdcL3+zw/lX/EgQnUJJA0Lr1q0zoxC0jtXg8ccfN6f7BRj0gDsaRoMGs2fPlq5du8oNN9wgOgJFN11zYtiwYWaqKB1VoZuuf6FrbLAhgAACCCCAAAIIZJcAAYbsqk/uBgEEEEAAAQQQSChge4DhsssuM0/E3/iD/eWSE755gjzhTSVxQpABBs1+8Cl/lPVVdaYj3u2cT6JYGT9FO8m1E1qDBC037SDWaWxaLlSt52pntLuOgl6niyPrugdXXnmlTJkyxXQq637twNYpeXRUQPymwYn77rvPPEGv+3Wxab3ePc9dAFifwtdphnQKpfhNr9fpe9zggh5zO+VnzZplpjByz9cRA7qOw0033WTK5+7XdQX06fr4dQn0mAZUNBgSf38aLNLrzzvvPG8khrtIso4A0ZE28dvatWtF15DQURwtN13bQBfEbmtz70XXgtDgTPyoiJbTmX355ZdmNIrfqID4USA6KkWnm2q56X4NJMTfr56jgT2d6smtf3e0ggYMtI7dLX7Egjvl1ebNm805GlyJ3zRg9bvf/Y7gQjwK7xFAAAEEEEAAgSwSIMCQRZXJrSCAAAIIIIAAAskI2B5g0M5pnZ7nvp8eIacc+N8pcpK5t7bOCTrAMP5HD8hXS9dJy6l92ipTmMd0HQOdFken0Bk7dqyZW79jx46tFkk7uRcvXmzWP9ARCAMHDmz13E2bNokuKKwd0fpen2bX9Q26dOnS7Bpdm0HLoZsuopyXl+cdLysrM2loR7tO7aR5xs/t752Ypje6EPYXX3whui6DjprQQEb81ELJZKNGOhJEAzKanq5RodM8aXq2bVo+rX8tr9qrsbto89aUVUfw6KgGrT/9jOgomSDrbWvKyrUIIIAAAggggAACWy9AgGHrDUkBAQQQQAABBBCIlIDtAYbjjz/eLC77yNXHytF7fzP3fzqAgw4w7OdMwTTLWVDanb8+HWUmDQQQQAABBBBAAAEEEEDAZgECDDbXDmVDAAEEEEAAAQQCEIhKgOFRJ8BwVIQCDG4AgwBDAB9akkQAAQQQQAABBBBAAAErBQgwWFktFAoBBBBAAAEEEAhOwPYAg86F/89//lPu/9l35Hvf3j5tEG4AIIhFnrWQE85/UL5cUm6m9dlxxx3TVm4SQgABBBBAAAEEEEAAAQRsFSDAYGvNUC4EEEAAAQQQQCAgAdsDDJdcconccccd8tv/OUAuPn5C2hSCDjAMPfVuKa/cKKtWrZKSkpK0lZuEEEAAAQQQQAABBBBAAAFbBQgw2FozlAsBBBBAAAEEEAhIwPYAw+233y6XXnqpnHfUrnLb+QelTSHIAMOG6noZ9L27pGvXrlJTU5O2MpMQAggggAACCCCAAAIIIGCzAAEGm2uHsiGAAAIIIIAAAgEI2B5gePHFF+Xwww+XfXcaLC/97ntpEwgywPD2Z0vl8J8/JhMnTpQPPvggbWUmIQQQQAABBBBAAAEEEEDAZgECDDbXDmVDAAEEEEAAAQQCELA9wFBeXi79+vWTTh3zZd2TP5G8vA5pUXADDI9fe7wcMXFkWtJ0E/n9ox/I9Q+9IxdeeKHceeed7m5eEUAAAQQQQAABBBBAAIGsFiDAkNXVy80hgAACCCCAAAJbCtgeYNAS60iAGTNmyL+u+64ctseILW/Csj06ekFHMUybNk1OPPFEy0pHcRBAAAEEEEAAAQQQQACBYAQIMATjSqoIIIAAAggggIC1AlEIMFx//fVy3XXXyZmH7Sx3X3yotZZasKVrKmW7s+4Rda2srJSioiKry5uJwt19991moe5k8zr99NPlmmuuSfZ0a8678cYb5aGHHjLlmTx5slx77bWtlu2uu+7yRrccddRRcuutt7Z6bnsOaFmmTJkiuobJqaeemnIS5513nrz55ptyzz33yKRJk1K+ngsQQAABBBBAAAEEclOAAENu1jt3jQACCCCAAAI5LPD/7J0HvM3lH8e/194jDTPZskJCISShv+wtMzIK2UIkMhqkMrNH9oqQTXYyMzOySiIjm+v+f5/nen73d849995zrnv25+nlnN94fs94P7974/t5vt+vPwgMv/32mxQsWFCSJk4oZ2a1k2TGt6+WobO3y8DpW6RBgwYya9YsXx2mR8dVo0YNWbJkidN9jh8/Xlq3bu10fU9XXLx4sXTt2lWqVasmI0aMMLt/+eWXZfv27eq8SpUqsnz5cvOe9eCff/6RHDlyyH///acuDxkyRHr16mWt8tjHFSpUkHXr1qnxlChRwqX2wsLCJHXq1Gp8f/zxh2TNmtWl51mZBEiABEiABEiABEggeAlQYAjetefMSYAESIAESIAEgpSAPwgMWJpKlSrJqlWrZFDLV6Vz7Zd8drVyG94L5w0vhhUrVqjk1D47UA8O7N9//5U7d+6YPd69e1eyZw/PezF37lwpVaqUeQ8HTzzxhCRJksTmmi+ddOnSRQkLVmEAc7KOOXfu3HL06FGHw9bP65urV6+W119/XZ8+9veDBw8kYcJwEe7WrVuSNGlSl9o8efKkEkBSpkypvHBcepiVSYAESIAESIAESIAEgpoABYagXn5OngRIgARIgARIIBgJ+IvAgB3w2Amf/onkcnhya0lkJH32tTJy4S7pPXGjYCf71q1bfW14PjOePXv2SNGiRdV4ID6kTZvW4dhgKD979qxkyJDBxnjvsLJxESLGzZs3JV26dDZV0E5oaKgkTpzY5npUJ/AswC7+VKlSOayic4KsWbNG4CmA8uuvv0qxYsVs6qPf+PFt39Pjx49Lrly5bOohkTlEFWt5+PChnD59WgkFmTNntt5yeIx5J0+eXN3THj8vvfSS7Ny502H9S5cuqfklSpQo0v358+dL3bp1pXr16gJvDRYSIAESIAESIAESIAEScJYABQZnSbEeCZAACZAACZAACQQIAX8RGIAbu7zXrl0rXeoUl4EtyvjUCvxz9Za88O5kuXbzjixcuFBq1qzpU+PzpcEgrj9i/MOL4cSJEzZDgyfAhAkT5KeffpKlS5ea92Dw7tatm0r4rS++8cYbsm/fPlm2bJnMmDFDJk+eLPXq1VPPow7CFfXp00eFCsI51qRHjx4yePBggbCBHANaAEC/n3/+uUyfPl2OHTuG6lKyZEnp3r271KpVS51jXGjv4sWL6hw7/GGExzMI69SmTRvVx5YtW1Sd8+fPS8aMGVVd/YHxIfk3QmjNnj07EoMLFy6oPjEfXdAPcjY0bdpUX1Lf8E5AbhKIbxjz008/reZfoEABadu2rXTs2NEm98Xff/+txg+ueg4I5YR558+f32y7Z8+e8tlnn4nVQ8O8yQMSIAESIAESIAESIAESiIYABYZo4PAWCZAACZAACZAACQQiAX8SGH7++Wcz4eyqYfWlVIGYd3Z7as3e+XK5zF53WBmYITCwRE2gVatWMnHiRGnevLkSBXRN7NqvXLmyIGQQCkInIUk2xAYUGNqvXr0q8eLFU54KOoE2hAqE9UEZN26cEi+QGwGhiFBgeE+TJo0pHOBaoUKFlDiBY3hJoN9Dhw6pui+++KIcOHBAzp07h9tKwPjf//4nw4YNU4Z+fR0CRP369eWDDz4QPadBgwap8FgQGbZt26ZECtWI8QHBA94t6BuCCRJZWxls3rxZ3nzzTZX7IF++fFK4cGHZtWuXOe5FixYpLx60d+bMGYE4gDGjYCxgoIUDXIPwgYTZKBDmILDAOwPhm7Jly6a8bHAOPhgbrqGULl1aMH6rh4a6wQ8SIAESIAESIAESIAESiIEABYYYAPE2CZAACZAACZAACQQaAX8SGMAeO9Cx4/r5rE/KphGNfCLh89ile6Tr2HWS2Ag3c8BISG0fAifQ3pnHnU+ePHmU0XzMmDFqp71uDzv7scMf4ZMWLFggzz33nLqFcFM6T8O9e/dU2CDrNRjsscMfwgDCIK1fv14qVqyonoU4gWO857p93MAOf/QPz4UyZcrIL7/8oq59+eWXkixZMvVskyZNlGeENWHz2LFjpV27djbCACrrOSGx85w5c2Tq1KkqyTc8FVAQcgn9wHCPMWG88CQYPXq0ag+eCy+88IISCL755ht5//331XP4gOeCFgvwjXBP5cuXFwhuEA3gWYEQS7dv35aqVauaHhuHDx+WvHnzKuEBYgXEhEmTJqmxg8f169cFYZTg/QCPBXhr3L9/X3TYpOjCV5mD4wEJkAAJkAAJkAAJkAAJWAhQYLDA4CEJkAAJkAAJkAAJBAMBfxMYsCYw1GK391sv55TZfat7dZlW7jwptQcsUmOA8bZFixZeHY+vdw4PBJ1zAUZ9a94CGOZ37NghDRs2VIZvPRed38CadBhGeIQAQvnjjz8ka9as6hh5DzJlyqQM9cglULt2bXVdfzzzzDPqHsIpwXsAHgfwJIAXxMGDB21yPWB8yLeAHf4IL4SC9Z0yZYoSJyBSoFy5csXMoQChAF4U/fv3l6FDhwrCDaEglwHEACQrRwLw1KlTK4M/ciTAyK/FDEd5DxAGCgKBnj/aR9/wcgAba3Jp3Q/61DkgIEZs2LAhkiiCOsOHD5euXbua93R+DEfhq1CfhQRIgARIgARIgARIgASiI0CBITo6vEcCJEACJEACJEACAUjAHwWG33//XV41RIYLhtG38ev5ZXznyl5ZmY37zkitjxfJnXsPlGcFQuiwRE8Ahm4YvFGw495qHMc15BWAkfvo0aMqDBByNOh8BDDQ6/BTjRs3lu+//14GDBgg/fr1w6OqwHvhtddeU6LAn3/+aeZY0Pfh5bB7924VHgm5CiBMIOQRQhYhTJK1IIcC2kZIIYwHJUeOHMojwCqO6D6RjBnhlqZNmybNmjUzvSTgFaA9Cfbu3auEArSDAgbwokAIJxR4JSBEkbVoo78WGPQc4OWh80Po+povvDZWrVqlclzkzJlT3f7kk0+U+KLr4hv14RUBjwmINghdhXBPEF8gwrCQAAmQAAmQAAmQAAmQgCsEKDC4Qot1SYAESIAESIAESCAACPijwADs8GBAkl8YaGu/mkem9azq0dVYuu24NB26TO49CJWWLVsqw6xHB+CnnSG8FcJcYdc+du9bC8IG9e7dW+3st17Xx1aPgCxZsihhAF4H2MmvS4cOHVT4IbTz6aef6svq+86dO5I0aVJ1DKM/+tehl2wq2p1orwKEDEqXLp3ZFsIxoSC8EDwVGjVqJDNnzlRGe4go2siPUEzt27c3jfbwrICgoRnMmjVLPavPVaOWDxj68Y6VK1dOvWdanLhx44YkT57cUlNUCKR33nlHeVAgATREr169etnUcXTy9ddfC9ghUTUSVtuHr3L0DK+RAAmQAAmQAAmQAAmQgD0BCgz2RHhOAiRAAiRAAiRAAgFOwF8FBizLxo0bjbAzNYwQNVeleN4MMqrDG5LvuSfdvmLD5/8iH03epPpp3bq1Msi6vdMA6aBOnToqv0Lnzp1VeB49rZUrV6qkxTjv27evlC1bVnkLPPvss+rYmnQYiYwR6ghFhwFSJ8aHDge0bNkyQWJma5k9e7YKvwRDPbwOtOEeIgNyL0RVECIJCZCR9BiiARIqI4GzLvAiQAJmbaRHsmWIAPBo+M3IyQExBPkPkJgZxzD4w/CvGcBLYuDAgfLhhx/K4MGDdbPmN3Iz7N+/X0aNGiUFCxZUic6tSarNisYBcjD8+OOPKr8DjnX+BiSi1vkgrPX1MfKGII9D/vz5VeJoHbpJ3+c3CZAACZAACZAACZAACThDgAKDM5RYhwRIgARIgARIgAQCiIA/CwxYBhhemzZtYoS82W+Ew4kng1uWlfdrFHXLCh09e1n6TP5ZVuw4odpH7H6EnWFxnoDOgYBd+9rgjbBIMJzDMK9zI+gWYaDHPRSddBg5DN58800VCmnt2rW6qvq2JltGcmZdIERAWIBQAQP/kCFDVEgg5HGwJnHW9ZEAGcb+hw8fKuM/vBXwDDwjunTpYiNI6Dmh7VdeeUWQiFp7N8CzAWICRBOICCgIgYS6CPGEfBMYA8ITacFBjwHfmzZtUgILjhHyCaGZ4FEBrw14b1gLzhH2CQXhnTJmzCg1atSQJUuWmEmcrfUPHDigwiNBxEGIpGvXrpmhmhyFr7I+y2MSIAESIAESIAESIAEScESAAoMjKrxGAiRAAiRAAiRAAgFMwN8FBiwNQt+89957KjwMzovlySBd67wk1V7JhdPHLhev3pKRC3fJVwt+UW099dSTRhieUVKvXr3HbjuYGkB+AhizUY4cOSIQA1CQEwF5BVCw0z9FihTqGMmbwRhGdWseBBjqsevfarRXDxgfCA+EZNtIgowwPygQF5CcWedyQB4H5HOw5jaAd4HOgwDjOnIozJs3zyb8lfa+sCbzxnM6wbQ1ZJEO4YT+kTtBt4/QTIkSJcJlk4EOkWSftPny5ctK/MD8IXbAw2Hr1q1mWCcrw3/++UflkABLa1JqnQwbnhfwEokXL57qG2IFkqVD1EHOCCSZ1mJGVKGa1IP8IAESIAESIAESIAESIIFoCFBgiAYOb5EACZAACZAACZBAIBIIBIFBr8vcuXPlI2On+DEjCTTKCzmelsYV8ksdI0fDM2ltY9XrZ6L73rT/rMzZcFimrvpNwsLCVFUkwIWx96mnnoruUd5zQAA76bGjHgb3q1evmsZuJO2GgICCBM0Ic4Td9VhPXRBuaOTIkSqpsQ4DtHjxYrWbX9fBN0IV6cTHaOvJJ58UeDxAuNAFwgVEAXgnZMiQQRByCcZ9JI5GwmUIETC8w9COsEipUqVSj+pQSKiL0EPwTsAYIFbYhyyqUKGCrFu3Tj2H0EbIwYCyb98+KVy4sA0DGPszZcqk7iNcExhdunRJ5XNAAupKlSoJQj4lSJBAjVknpoaQgNwMeDfh+YF5oECUmTNnjjpGcmokmEZByCiwhXfDhAkTFBOMC7kv8HsAYaK6devm0JNCNcAPEiABEiABEiABEiABEoiJgPGXUxYSIAESIAESIAESIIEgImD8/RCW84Ca8fDhw8MMI6yal55fyXyZwrrUKR42vVfVsO3fNA07M6t92NUlncP+W9Yl7J8FHcMOT2odtmJIvbAv2rwWVr/882EZ0qWwed7YvR5m7B4PKE6enoyRdFgxNUISRerayBFgw9sQIcIMD4WwQ4cOhRmGdHXPMIar53AP62p4BURqxxANwgyDeZiug3qG0T7M8BJQzxhChc0zxo5/s339ruBZIxxSmBEyyKaukczZHKPh4aDu6TnpsekH3n33XVU3e/bsYYZooS+HGd4C6rohGpjXcGAII2bbehz4/uKLL8IMrwebuoZIYTM/1ANT1MWxIRTY1DcEk0htG4JOGK6HhoaadY0k1aoeWLGQAAmQAAmQAAmQAAmQQGwI0IPB+Bs5CwmQAAmQAAmQAAkEE4FA8mCwXzfs4kZiX+wyj01B4luExXn77bfVDvfYtMFnnCeAkD/Hjx8XwyivwifFjx9fPYwcDdiJDy+H5Mmd90RB2KC0adOqnf/wIkCeAeQ7gCeEtcBrAfkLTp06pTxTEK4pqn7gbYCQXPCo0KGOrG09zjHyPiDnxIULF9Rc8f7pXA727YIJwiHBM6NIkSKSPn16+yo258ivAO8JhF2Ct0SxYsVMDxKbijwhARIgARIgARIgARIggccgQIHhMeDxURIgARIgARIgARLwRwKBLDDo9YAxdv369bJjxw6VFPrEiRPy119/CQy6xg5uSZIkiaRLl07lB3j++eelaNGiKhGvTi6s2+G3bxNAiCOENEKoH4hCuiAkUokSJVQIIQgJCHHEQgIkQAIkQAIkQAIkQAIkEPcEKDDEPVO2SAIkQAIkQAIkQAI+TSAYBAafXgAOLs4ItGnTRsaPH688IIxwRippM3btDxs2TIkL+O7Ro0ec9ceGSIAESIAESIAESIAESIAEbAlQYLDlwTMSIAESIAESIAESCHgCFBgCfomDZoIIr4RExseOHYs053nz5qlwV5Fu8AIJkAAJkAAJkAAJkAAJkECcEaDAEGco2RAJkAAJkAAJkAAJ+AcBCgz+sU4cpfMETp8+LevWrUPmcilcuLAKiYQwWCwkQAIkQAIkQAIkQAIkQALuJUCBwb182ToJkAAJkAAJkAAJ+BwBCgw+tyQcEAmQAAmQAAmQAAmQAAmQAAn4JQEKDH65bBw0CZAACZAACZAACcSeAAWG2LPjkyRAAiRAAiRAAiRAAiRAAiRAAhEEKDBEsOARCZAACZAACZAACQQFAQoMQbHMnCQJkAAJkAAJkAAJkAAJkAAJuJ0ABQa3I2YHJEACJEACJEACJOBbBCgw+NZ6cDQkQAIkQAIkQAIkQAIkQAIk4K8EKDD468px3CRAAiRAAiRAAiQQSwIUGGIJjo+RAAmQAAmQAAmQAAmQAAmQAAnYEKDAYIODJyRAAiRAAiRAAiQQ+AQoMAT+GnOGJEACJEACJEACJEACJEACJOAJAhQYPEGZfZAACZAACZAACZCADxGgwOBDi8GhkAAJkAAJkAAJkAAJkAAJkIAfE6DA4MeLx6GTAAmQAAmQAAmQQGwIUGCIDTU+QwIkQAIkQAIkQAIkQAIkQAIkYE+AAoM9EZ6TAAmQAAmQAAmQQIAToMAQ4AvM6ZEACZAACZAACZAACZAACZCAhwhQYPAQaHZDAiRAAiRAAiRAAr5CgAKDr6wEx0ECJEACJEACJEACJEACJEAC/k2AAoN/rx9HTwIkQAIkQAIkQAIuE6DA4DIyPkACJEACJEACJEACJEACJEACJOCAAAUGB1B4iQRIgARIgARIgAQCmQAFhkBeXc6NBEiABEiABEiABEiABEiABDxHgAKD51izJxIgARIgARIgARLwCQIUGHxiGTgIEiABEiABEiABEiABEiABEvB7AhQY/H4JOQESIAESIAESIAEScI0ABQbXeLE2CZAACZAACZAACZAACZAACZCAYwIUGBxz4VUSIAESIAESIAES8GsCzZs3F/wpV65cpHlEJzBs2LBBpkyZov5EepAXSIAESIAESIAESIAESIAESIAESMBCgAKDBQYPSYAESIAESIAESCBQCJQuXVq2bNmiBIb+/fvbCA2OBAYICwMGDBB8lypVSjZv3hwoKDgPEiABEiABEiABEiABEiABEiABNxGgwOAmsGyWBEiABEiABEiABLxJAF4ILVq0MIcATwYtNFgFBquwoCtPnjxZeT/oc36TAAmQAAmQAAmQAAmQAAmQAAmQgCMCFBgcUeE1EiABEiABEiABEggAAk8++aRcvnzZZiYQGiAqoFiP1QXjI126dHLp0iV9ym8SIAESIAESIAESIAESIAESIAESiJIABYYo0fAGCZAACZAACZAACfg3AXsvBmdmQ+8FZyixDgmQAAmQAAmQAAmQAAmQAAmQAAhQYOB7QAIkQAIkQAIkQAIBTMCRF0NU06X3QlRkeJ0ESIAESIAESIAESIAESIAESMARAQoMjqjwGgmQAAmQAAmQAAkECAFXvBjovRAgi85pkAAJkAAJkAAJkAAJkAAJkICHCFBg8BBodkMCJEACJEACJEAC3iLgjBcDvRe8tTrslwRIgARIgARIgARIgARIgAT8lwAFBv9dO46cBEiABEiABEiABJwi4IwXA70XnELJSiRAAiRAAiRAAiRAAiRAAiRAAhYCFBgsMHhIAiRAAiRAAiRAAoFKIDovBnovBOqqc14kQAIkQAIkQAIkQAIkQAIk4F4CFBjcy5etkwAJkAAJkAAJkIBPEIjOi4HeCz6xRBwECZAACZAACZAACZAACZAACfgdAQoMfrdkHDAJkAAJkAAJkAAJxI6AIy8Gei/EjiWfIgESIAESIAESIAESIAESIAESEKHAwLeABEiABEiABEiABIKEgCMvBnovBMnic5okQAIkQAIkQAIkQAIkQAIk4AYCFBjcAJVNkgAJkAAJkAAJkICvErB6MdB7wVdXieMiARIgARIgARIgARIgARIgAf8gQIHBP9aJoyQBEiABEiABEiCBOCFg9WKg90KcIGUjJEACJEACJEACJEACJEACJBC0BCgweHnpz+wZL9cu7PLyKNg9CZAACZAACZCAJwhkKfyupMlQzBNdRdsHvBhQLl26FG09T93k34c8RZr9kAAJkAAJkID3CfjK34e8T4IjIAESIIHAIECBwcvryH9Qe3kB2D0JkAAJkAAJeJCAr/yDGl4MKM2bN1ff3v7g34e8vQLsnwRIgARIgAQ8R8BX/j7kuRmzJxIgARIIbAIUGLy8vvof1JnzVJLUT+b08mjYPQmQAAmQAAmQgDsJhCTLKiEJ0sh3v6yVX86fFAkzegsx/gTp97vFKkixzNkl7PZpCbt/1Z3o2TYJkAAJkAAJkICXCZw7+pNcu3RcKDB4eSHYPQmQAAnEMQEKDHEM1NXmKDC4Soz1SYAESIAESMB/CWiBYfzONfLLuRORJqJ1hkg3Hl0ItPttir9uCAw5KDBEteC8TgIkQAIkQAIBRIACQwAtJqdCAiRAAhYCFBgsMLxxSIHBG9TZJwmQAAmQAAl4h4ApMOwwBIbzJyQMisGjEmJ4MgTb+buGwPASBQb9CvCbBEiABEiABAKaAAWGgF5eTo4ESCCICVBg8PLiU2Dw8gKwexIgARIgARLwIAEtMIzbsVp2noUHg/ZJCM7vtiUMgSFLTnowePAdZFckQAIkQAIk4C0CFBi8RZ79kgAJkIB7CVBgcC/fGFunwBAjIlYgARIgARIggYAhYAoM2w2B4dzxYNcXpG2JihQYAubt5kRIgARIgARIIHoCFBii58O7JEACJOCvBCgweHnlKDB4eQHYPQmQAAmQAAl4kIAWGMZuW2V4MBw3/BfCjBzPIRY/huA6b/vyG1KcHgwefAPZFQmQAAmQAAl4jwAFBu+xZ88kQAIk4E4CFBjcbRUb3QAAQABJREFUSdeJtikwOAGJVUiABEiABEggQAhECAw/yQ5DYFDKgp6bkYMh2M7blawkxZ9liCT9CvCbBEiABEiABAKZAAWGQF5dzo0ESCCYCVBg8PLqU2Dw8gKwexIgARIgARLwIAEtMIzZaggMZ373YM++2VX7VyAw5GIOBt9cHo6KBEiABEiABOKUAAWGOMXJxkiABEjAZwhQYPDyUlBg8PICsHsSIAESIAES8CABU2DYsvKRB4OR3DnEcF0IC85vCgwefPnYFQmQAAmQAAl4mQAFBi8vALsnARIgATcRoMDgJrDONkuBwVlSrEcCJEACJEAC/k9ACwyjt6yQHad/t+ReMHQGY3qGzBBU3+1LVZESWenB4P9vNmdAAnFDIOzOXbnWrINTjSWpUUWSNKzpVF1rpdATf8h/3T5Rl+LnfE5Sft5PHd9dtlpuT5yljpPUNNpuWtf62GMd/9epr4Se+dNsIyR1Kkk9eUS4wPzo6sMz5+V6p4/MOjhIUv8tSdLA9TnaNBJAJ2H37su1Ju+ZM0o95WsJSZrEPOeB7xOgwOD7a8QRkgAJkEBsCFBgiA21OHyGAkMcwmRTJEACJEACJODjBEyBYfMK2W4IDDaSgvZkiEpiCMD775WubAgMuRkiycffWw6PBDxFIOzWbbnyv7ed6i5pgxqStE0Tp+paK4Ue+V2uteulLsXPmkVST/lKHd+Z94PcGj1VHSep+5Yka99cHcfFx7WmHST0bITAgDbTzBgl8TKlN5u/t3K93Bj2rXmOg6SNa0nSVo1trgXzSdjdu3KlciMTQdpl0yUkeTLznAe+T4ACg++vEUdIAiRAArEhQIEhNtTi8BkKDHEIk02RAAmQAAmQgI8T0ALDqJ+XGwLDMR8frfuH936ZNykwuB8zeyABvyFgLzCEJEsa5diT1q8eKy8DXxEYUvTpJIlef9Wc383PR8vd5WvNcxxQYLDBIRQYbHn44xkFBn9cNY6ZBEiABGImQIEhZkZurUGBwa142TgJkAAJkAAJ+BSBSAJDsMVEspvv+6UNgeE5ejD41EvKwZCAFwnYCwypx38h8XNli3ZEYTduijx8qOqo3ezx45v1w/67EZ7jxrgSkjKFCknktMDQrpmo5x+1FqltI1yP3LkTftfoM7qd9I48GJJUryzJPmhtjvVao/YS+tff5jkOHAkMGNPD83+pscXPnlVC0j1h84w6efDAMMbfU4chiROLJIgv4BR6+JjBIaXEy5ZF1PXIT6p2Y2xfP/cgVEJPn5WwS/9K/KyZJST904I1FKN/FHtmuBb271X1jBj84ufMZow/LS47LAiZ9dAIaRUW+lDi58mu6tCDwSEqv7lIgcFvlooDJQESIAGXCFBgcAlX3FemwBD3TNkiCZAACZAACfgqAS0wfLvpR9n2x1FfHabHxtXh1f9JyefyMESSx4izIxLwbQKxERishvlUIwdKgkL5zEn+W762eaxDEjkrMCRt2VCuVIkIx5Oi5/uSqHJ5s73bE76X2zMXqPNEpYpJikEfmvfsD6wCA8IywSgfP0tGST3tG1U17Oo1uVKzpTrW93FiIzBcuy43ho+Ve5t2qHr6I97T6STRyy9JUiOkU0iihOrynRnz5dajfBLJWjWSBwePyr1tv+pHBJ4hKXp3lISlipvXxIX28dD9LTvlv77DIp43jpJ3aiX3tu+W+zt2q+upRw2R+Plyq2PM8eaIcZHGDw4pB/aUeIZAYZawMLkz3ZjD5NnmJRwk79pWbn451rzGEEkmCr85oMDgN0vFgZIACZCASwQoMLiEK+4rU2CIe6ZskQRIgARIgAR8lYApMGx0IDDoLM9RDT4A73coS4EhquXmdRIIRgK+JDAgB8PNQV/J3bU/q6VI9GpJSTGgu7ks197pIqEnT6tz+3BHZqVHB1aBIUm1SnLnh5/UnbQ/TFWeFfe37ZL/eg9R15JUN+4vCb9vFRj+69xP7u89aN+0eZ6kakVJZhjgUe5MmxfJOG9WtByknTNOQp5+Ul1xpX3reC3NRTrUAgNCG4HBw4uXI9XBBQgeqYYPMLwUcqj7t8fPkNuzFjmsa71IgcFKwz+OKTD4xzpxlCRAAiTgKgEKDK4Si+P6FBjiGCibIwESIAESIAEfJqAFhm82LpVtp5CDwS5mUJCddyxbVUpmoweDD7+yHBoJeJSAvcCAzh3lYUhcqZwk69hKjc1dHgwQGO5vNQz/fcIN/+gs7U+zlZdA2OV/5UqdiPBGaX+c4XCcaoDGh1VgSPFhR7kx5Gt1K9XQPpKgRFG5PWGm4Q2xUHk1JDbyMuid+1pgQAiiK3Uj+kveoaUkyJVd7q7fIncWrVBtxUuTWtIsmqSO7QWGRK+WkMQVykjoH+fMtlExRd/OkqhCaRXiyOn2De+Ca626muIK+k1Ss4oRhimeMYdF4SGS1ChEtMBgHY+q37C64Pv2nB/MdhK9/KKkGNw70lgS5M0piV8vI/d/M7wwNmx91HL4FwUGGxx+cUKBwS+WiYMkARIgAZcJUGBwGVncPkCBIW55sjUSIAESIAES8GUCpsCwwRAYjBBJhp1GQrRngvEdbOcdy75FgcGXX1iOjQQ8TMCRwOBoCImNUEXJjZBFKO4UGMKMPAFXa7YwjeapvuwvCYoWknurN8mNwSNV/4nKvSIp+ndVx1F9WAWG1GOGyvWuA1SbSZvVlaTNG8j1jn3kwYEjAi+EeE+lM0UALTCEHv7d8GpYqZpP8HxuSWx4OaBYx4HzJ9bOE4kXz8aDQYVimvSVysOAOlZPhWStG0mSRrWN3AzOt//wj7Ny1fDe0AXziZ83lzq9v9UIm9QnImySFhisa5Six3uSqMprqn6okV8BYoUuaed/J/c2bpeb30xUlyAupVkwUUKSGHkkjHL7O8Oz4fsIzwYKDAqLX31QYPCr5eJgSYAESMBpAhQYnEblnooUGNzDla2SAAmQAAmQgC8S0ALD1+t/kK2nkIMhuD0YOpV/S17Olpc5GHzxZeWYSMALBOwFBuQXCEmaLNJIEpUtIUlbNFTXrcbruMzBAA8GlFsjxpshjZLUfUvsQyel/LSnJHzFkstAPWX7YSMwjP1Mbk+fK/e27JKEhfNLimEfyZVKDdQDyPPw8OKlSAKDumkksg79/ZTc371f7u87JKFHT8hDI6+BtTgSGBK/WUGSd29vVrPOJ1mLBpKkad3we062r8IjPcq9gPVJM2e82bb9+imBIXcO+bdiPbNOgvy5bRJi39+517yX6vN+cm/nHrkzb6m6Bs+LFAN6mPfDLlyUKw3bmecUGEwUfnNAgcFvlooDJQESIAGXCFBgcAlX3FemwBD3TNkiCZAACZAACfgqARuB4eQRXx2mx8bVqXw1eTk7BQaPAWdHJODjBCIZqMd/IfFzZYt21O4WGB4YeQ+uG/kPUJRB/fsxcqVaM+WBoHbYL5wkIYkTRT9GI/9A6Nk/VZ3UhsBwf89+uTVuhjpH7oHrXfqr4zRTRho7+LdFFhgePJAbA76Ue5t3RtuPI4EhafN6krRZffO5W19PMMMqmQKDC+3fXbbaSNYcLiokLPS8pBw5yGwbB9bcFBAY4JFxpd67NnWiOkk5qKfcXbnenKc5vkcPwKNEizG4RIEhKpK+e50Cg++uDUdGAiRAAo9DgALD49CLg2cpMMQBRDZBAiRAAiRAAn5CQAsMI9ctka0UGOSD16pTYPCTd5fDJAFPEHhsgeFRCCOMNezmLblStYk57DQzRkm8TOkl9Mjvcq1dL3U9ftYsknqKET7IKHfm/SC3Rk9Vx9pTQZ0YO/uv1m5legsgxM+Nz0apW/beAeqigw97D4aw23dM0SJR6eLKoA6xAgbzO9PnRxIY7i5cboYNQvPJWtSXBIULSvz0T8mV+m3MHmMrMLjS/v3tvxphkIaqPtWYjfwTZjHi/F2t9Y7JSgkMhkB05Y1wDw3US9G7kyHIJDQfsR4kyJ9Hbk2aLXeXr1WX7cNPWdcOFSgwWOn5xzEFBv9YJ46SBEiABFwlQIHBVWJxXJ8CQxwDZXMkQAIkQAIk4MMEtMAwYu0S2X7qsJlzQedeUN/G+FXgJEtOhkC9Hy4wPM8QST78znJoJOBJArERGK636ykPjhxXw0ze+V1JXC08P8H9HYYhvNdgc/ixFhiMFm6Pmy63Zy8229IHOieDPo/q215giJclo1z539s21XWSY2tCZJ2DQXkvPEpwrPM24OHQg0fl2vu9zXZiKzC40v7D0+fkasvOZp8p+hmJosuXVkmE7v64Rm5+Oda85ygHgw0zw3MCeSTU/wyNpxKWKSFo45bBG0UJGEsM0SdBfHV+Z4YhvkycpY7xQYHBROE3BxQY/GapOFASIAEScIkABQaXcMV9ZQoMcc+ULZIACZAACZCArxKIEBgWy9YTh311mB4bV+cKNeSVHBQYPAacHZGAjxOIjcBgNY7DIJ3s3SaGaHnLyHOwwEzOjGk/jsAQeuyEXGsTkQsA7Snj9w+G8Tt+uPEb16Iq9gJD/Dw5bEIJ4blkrRsbCZdr2SRoNgWG/p/JvU07VPOJK5SRZJ1aGfkYTsrNUVMk9ORps9sn1sxV47ERKZwIkXTDlfaNJNLXO/VVSal1xxBHQv+8KKGnz+pL6lsLDLfHTTMEmiXqWrw0qSXpOw2N0ElPyN1la8xwSCoZ9eSv5OFff8vVJh3MdhIWLyxI6v3w7F+mZ4e+SYFBk/CfbwoM/rNWHCkJkAAJuEKAAoMrtNxQlwKDG6CySRIgARIgARLwUQKmwLBmEQUGY406v16TAoOPvqscFgl4g0BsBAaVdLj3kBiH+zgCAxq35nrAeZI6VSXZey1wGGNxJDDc+mai3DFCH+mCXAwJihRwKDDYhzDSz9h/p100WULSpLJtwwmBwdX2Qw2PkWuG50hMRQsMYXfvyvUWnSXUEA+iKikH9pCEpUuo27dGTzETPUdVH9cpMERHxzfvUWDwzXXhqEiABEjgcQlQYHhcgo/5PAWGxwTIx0mABEiABEjAjwhogWH4aggMhyKNXIVGinQ14kKg3e9SEQJDPoZIilhiHpFAUBOIjcAAYHcXr5Bb3800PRbgXZC8U2u5M3+ZPDB2+qOkmWnkYMho5GA4angjtA33RoifPauknjhc3b87f6nyCMBJ0nrVJGm7Zuq6/rAPz6ON5/p+dN82AsP4z43E1dnl3vrNcuOTEeZjaY1cBhi3jfdB0zqStEVDIxZSqNw08j7cXbXRrA9PgORd2xi7+ueYXgw6ofOdmQvk1oTvVV37RMlWYSOZ4UmQ5O06LrePhh+evyC3xk+T+7v2K+7IJZG4SnkjhNFsczypxwyV+HlzqXGE/XvVuDfTyK+wTp3rD8wjxUcfSIKihfQlo/GHcnvaXLk9dV7ENeMo5ae95ObnY8wcD5qZTSWe+DQBCgw+vTwcHAmQAAnEmgAFhliji5sHKTDEDUe2QgIkQAIkQAL+QCBCYFioPBjCLMkVQkJCjDDUhoRgfCMedTCcd4EHQ04KDP7w7nKMJODzBIzfmw//NHbIhz2UkGeekpCEjhMJx3YeD3YfkOtdP1aPx8/wjKQ2BAv1+zq2DcbiuYcX/paH5y5IPCO5c7xMGeK8f6faNziH3b1njj4kcSIlCKhQUUZOhX8r1jfvpZ01RkLSP22e4yDsvxvy8Mx5CbtxU+JlzSTxnjHu4/97DgoSdYcePyUhSZMIxCBJkMBBLV7yJwIUGPxptThWEiABEnCeAAUG51m5pSYFBrdgZaMkQAIkQAIk4JMETIFh1QLZfDyyB4NPDtqNg+r6Rm0pRYHBjYTZNAmQwOMSgEE87NK/ctMI23N/1z7VXLJ2TSVJveqP27RfPh929bpcqRkRGgoeEolrVFYiwe0pc42wTz+qecEbI81iI2RTHAs9fgmNgzYJUGAwUfCABEiABAKKAAUGLy8nBQYvLwC7JwESIAESIAEPEjAFhp+0wKCDHoV/G/tCJcT4z9jjafyJ/B1o97tWqmUIDPkZIsmD7yC7IgEScI2ANaSQfjLtwokSkjaNPg267/96DpT7O/dGO+9krRpJksa1o63Dm8FHgAJD8K05Z0wCJBAcBCgweHmdKTB4eQHYPQmQAAmQAAl4kIAWGD7/ab5s/f1gFDJCVPJC4F3vVqmOlMpFgcGDryC7IgEScJGAvcCQvFMrY8d+FRdbCazqCF2EnBD3Nm13OLEUPd6TRFVec3iPF4ObAAWG4F5/zp4ESCBwCVBg8PLaUmDw8gKwexIgARIgARLwIAFTYFg5X7YcPxiuGOj+tcNCEJ13p8CgV5vfJEACPkog1EgSHXr6nISkTC7xs2Y28h8846Mj9fywwi5ektBTZ+ShkcA5JHUKiZ8po4RkfIZhkTy/FH7TIwUGv1kqDpQESIAEXCJAgcElXHFfmQJD3DNliyRAAiRAAiTgqwRMgWHFPNn8+2++OkyPjat7lbpSOlcBhkjyGHF2RAIkQAIkQALeI0CBwXvs2TMJkAAJuJMABQZ30nWibQoMTkBiFRIgARIgARIIEAI2AsOx34I+RJISGHJTYAiQ15vTIAESIAESIIFoCVBgiBYPb5IACZCA3xKgwODlpaPA4OUFYPckQAIkQAIk4EECWmAYtnyubFECQ3hSZ528Odi+e7xpeDDkLkgPBg++g+yKBEiABEiABLxFgAKDt8izXxIgARJwLwEKDO7lG2PrFBhiRMQKJEACJEACJBAwBEyB4ce5RoikA0Gfg6Hnm/UoMATM282JkAAJkAAJkED0BCgwRM+Hd0mABEjAXwlQYPDyylFg8PICsHsSIAESIAES8CCBCIFhtvx8FDkYwow/Ortz8H33/F99KZOHHgwefAXZFQmQAAmQAAl4jQAFBq+hZ8ckQAIk4FYCFBjcijfmxikwxMyINUiABEiABEggUAiYAsMyCAwHglxeEOlZtQEFhkB5uTkPEiABEiABEoiBAAWGGADxNgmQAAn4KQEKDF5eOAoMXl4Adk8CJEACJEACHiRgCgxLZ8vGo/s92LNvdtXrrQbyap5CzMHgm8vDUZEACZAACZBAnBKgwBCnONkYCZAACfgMAQoMXl4KCgxeXgB2TwIkQAIkQAIeJBAhMMySjUeMHAwOfRj0gKIKmRQ495XAkJcCg15RfpMACZAACZBAIBOgwBDIq8u5kQAJBDMBCgxeXn0KDF5eAHZPAiRAAiRAAh4koAWGoT98bwgM+015QQ9BZ2QIlvMPqzWSVykw6OXmNwmQAAmQAAkENAEKDAG9vJwcCZBAEBOgwODlxafA4OUFYPckQAIkQAIk4EECWmAYsiRcYHDswRCV50LgXe9dvaEhMLzAEEkefAfZFQmQAAmQAAl4iwAFBm+RZ78kQAIk4F4CFBjcyzfG1ikwxIiIFUiABEiABEggYAhECAwzZcPhfQEzr9hOpHf1xlL2eQoMseXH50iABEiABEjAnwhQYPCn1eJYSYAESMB5AhQYnGfllpoUGNyClY2SAAmQAAmQgE8SMAWGxRQYsEC9a1Bg8MkXlYMiARIgARIgATcQoMDgBqhskgRIgAR8gAAFBi8vAgUGLy8AuycBEiABEiABDxLQAsOni2bIxsN7zRwMOvdCmHEQYkRCMs+NsdkERgqw+31qvm14MBRmiCQPvoPsigRIgARIgAS8RYACg7fIs18SIAEScC8BCgzu5Rtj6xQYYkTECiRAAiRAAiQQMASsAsOGQ3sDZl6xnUhfCAz5KDDElh+fIwESIAESIAF/IkCBwZ9Wi2MlARIgAecJUGBwnpVbalJgcAtWNkoCJEACJEACPknAFBgWTpP1h5CDAb4KumhfheA571uriZSjwKAXnN8kQAIkQAIkENAEKDAE9PJyciRAAkFMgAKDlxefAoOXF4DdkwAJkAAJkIAHCZgCwwIIDPRg6Fu7KQUGD75/7IoESIAESIAEvEmAAoM36bNvEiABEnAfAQoM7mPrVMsUGJzCxEokQAIkQAIkEBAEtMAwcMFU2XAQORgisi2EGNkWgu38IwgM+YswB0NAvN2cBAmQAAmQAAlET4ACQ/R8eJcESIAE/JUABQYvrxwFBi8vALsnARIgARIgAQ8SMAWG+VNl/cE9HuzZN7vqV6cZBQbfXBqOigRIgARIgATinAAFhjhHygZJgARIwCcIUGDw8jJQYPDyArB7EiABEiABEvAgAVNgmDdF1lFgkH51mkv5AvRg8OAryK5IgARIgARIwGsEKDB4DT07JgESIAG3EqDA4Fa8MTdOgSFmRqxBAiRAAiRAAoFCwEZg+G13pGnpgEmRbjy6EGj3+9WFwFCUIZKiWnBeJwESIAESIIEAIkCBIYAWk1MhARIgAQsBCgwWGN44pMDgDerskwRIgARIgAS8Q0ALDAPmTpZ1EBjsFYMgO/+4bgspX5ACg3feRvZKAiRAAiRAAp4lQIHBs7zZGwmQAAl4igAFBk+RjqIfCgxRgOFlEiABEiABEghAAhECwyRZdyCyB0MATjnaKX1cryUFhmgJ8SYJkAAJkAAJBA4BCgyBs5acCQmQAAlYCVBgsNLwwjEFBi9AZ5ckQAIkQAIk4CUCpsAwZ5KsPfCrl0bhO91+XL+lvFbwRYZI8p0l4UhIgARIgARIwG0EKDC4DS0bJgESIAGvEqDA4FX8IhQYvLwA7J4ESIAESIAEPEhACwwfz56oPBjCjBhJIcZ/wfr9cQMKDB58/dgVCZAACZAACXiVAAUGr+Jn5yRAAiTgNgIUGNyG1rmGKTA4x4m1SIAESIAESCAQCGiBof/sCbJ2v+HBEGQ5F+zn+0mDVvJaIXowBMK7zTmQAAmQAAmQQEwEKDDERIj3SYAESMA/CVBg8PK6UWDw8gKwexIgARIgARLwIAFTYJj1SGAwLe5aaQiu708avmMIDMUYIsmD7yC7IgESIAESIAFvEaDA4C3y7JcESIAE3EuAAoN7+cbYOgWGGBGxAgmQAAmQAAkEDAFTYPj+O1mzf1fAzCu2E/mkUWupQIEhtvj4HAmQAAmQAAn4FQEKDH61XBwsCZAACThNgAKD06jcU5ECg3u4slUSIAESIAES8EUCWmDoZwgMa/ftCnL/BRElMLxADwZffFc5JhIgARIgARKIawIUGOKaKNsjARIgAd8gQIHBy+tAgcHLC8DuSYAESIAESMCDBLTA8NGM8YYHwy8RORiCKzKSOe9Bjd+VCi+8xBBJHnwH2RUJkAAJkAAJeIsABQZvkWe/JEACJOBeAhQY3Ms3xtYpMMSIiBVIgARIgARIIGAI2AgM+wyBIcjLoLcpMAT5K8DpkwAJkAAJBBEBCgxBtNicKgmQQFARoMDg5eWmwODlBWD3JEACJEACJOBBAlpg6DtjrKzZixBJYRJi/Bes34PebiOvF6YHgwdfQXZFAiRAAiRAAl4jQIHBa+jZMQmQAAm4lQAFBrfijblxCgwxM2INEiABEiABEggUAqbAMH2srN5LD4ZPm7SlwBAoLzfnQQIkQAIkQAIxEKDAEAMg3iYBEiABPyVAgcHLC0eBwcsLwO5JgARIgARIwIMEtMDQZ9qYcIEhzEi+EBJi5CQIzu/BEBiKFGcOBg++g+yKBEiABEiABLxFgAKDt8izXxIgARJwLwEKDO7lG2PrFBhiRMQKJEACJEACJBAwBGwEhj07jdBIhr5gmV2wnX/atJ1UpMBgeQN4SAIkQAIkQAKBS4ACQ+CuLWdGAiQQ3AQoMHh5/SkweHkB2D0JkAAJkAAJeJCAFhh6Tx0tqy0CgxYWgu17cLP2FBg8+P6xKxIgARIgARLwJgEKDN6kz75JgARIwH0EKDC4j61TLVNgcAoTK5EACZAACZBAQBAwBYYpo2WVITAYsZGMP/BhsH7rqdpf1+eBc39Ic3gwlGCIJL2k/CYBEiABEiCBACZAgSGAF5dTIwESCGoCFBi8vPwUGLy8AOyeBEiABEiABDxIQAsMH04eZQgMO2x1Ba0fBNH30ObvScWiFBg8+AqyKxIgARIgARLwGgEKDF5Dz45JgARIwK0EKDC4FW/MjVNgiJkRa5AACZAACZBAoBDQAkOgzCeu5hF2+7SE3b8aV82xHRIgARIgARIgAR8kQIHBBxeFQyIBEiCBOCBAgSEOID5OExQYHocenyUBEiABEiAB/yJAgcHxelFgcMyFV0mABEiABEggkAhQYAik1eRcSIAESCCCAAWGCBZeOaLA4BXs7JQESIAESIAESIAESIAESIAESIAESMCDBCgweBA2uyIBEiABDxKgwOBB2I66osDgiAqvkQAJkAAJkAAJkAAJkAAJkAAJkAAJBBIBCgyBtJqcCwmQAAlEEKDAEMHCK0cUGLyCnZ2SAAmQAAmQAAmQAAmQAAmQAAmQAAl4kAAFBg/CZlckQAIk4EECFBg8CNtRVxQYHFHhNRIgARIgARIgARIgARIgARIgARIggUAiQIEhkFaTcyEBEiCBCAIUGCJYeOWIAoNXsLNTEiABEiABEiABEiABEiABEiABEiABDxKgwOBB2OyKBEiABDxIgAKDB2E76ooCgyMqvEYCJEACJEACJEACJEACJEACJEACJBBIBCgwBNJqci4kQAIkEEGAAkMEC68cUWDwCnZ2SgIkQAIkQAIkQAIkQAIkQAIkQAIk4EECFBg8CJtdkQAJkIAHCVBg8CBsR11RYHBEhddIgARIgARIgARIgARIgARIgARIgAQCiQAFhkBaTc6FBEiABCIIUGCIYOGVIwoMXsHOTkmABEiABEiABEiABEiABEiABEiABDxIgAKDB2GzKxIgARLwIAEKDB6E7agrCgyOqPAaCZAACZAACZAACZAACZAACZAACZBAIBGgwBBIq8m5kAAJkEAEAQoMESy8ckSBwSvY2SkJkAAJkAAJkAAJkAAJkAAJkAAJkIAHCVBg8CBsdkUCMRC4ceOGJEmSRBIkSBBDTd4mgZgJUGCImZFba1BgcCteNk4CJEACJEACJEACJEACJEACJEACJOADBCgw+MAi+OAQVq1aJR06dFAjy5UrlyxbtizKUW7ZskVatmyp7mfNmlXwbFyWNWvWSOPGjVUfQ4YMcbnpCRMmyOeffy4ffPCBtGvXzqnnDxw4IHXq1FF1W7duLd26dXPquZgqjRw5UkaPHi3du3eXVq1amdWHDRsmn376qfz333/qvrPjNBvggc8QuHr1quTJk0fwc7N582avjosCg1fxi1Bg8PICsHsSIAESIAESIAESIAESIAESIAESIAG3E6DA4HbEftnBvHnzpF69eubYjxw5ooym5gXLAYSIb7/9Vl3JnDmznD171nL38Q+XLl0q1apVU+IAjPOulqFDh8qHH34on3zyiXz00UdOPd6jRw8lSqAy5nT69GmJFy+eU8/qSp06dVLCzJgxY+SNN95Ql9H/oEGDBEJJr1691LXz58+rPnCSMmVKmTx5stSuXVvdi+uPWbNmSd++faVFixbqO67bZ3si//77r6RLl06yZ88uJ06c8CoSCgxexU+Bwcv42T0JkAAJkAAJkAAJkAAJkAAJkAAJkIAHCFBg8ABkP+zCXmDA7vrevXtHmsmDBw/kiSeeUDvvcTMQBIZ79+7Jk08+ac4J89q4caO8+uqrOHS6QKABxwULFkitWrXUc3v27BF4RxQpUkQKFiyorsE75K233pKiRYvKtm3bJFGiRE734WpFiB3t27eXjh07CrwpWOKeAAWGuGfqty3Sg8Fvl44DJwESIAESIAESIAESIAESIAESIAEScJIABQYnQQVZNXuBIV++fHLw4MFIFNatWycVKlQwr0clMDx8+FDOnDmjjOcZM2Y060d1cPv2bUmaNKm6HZMHA9rWRt2QkJBITbrqwaAN/rlz55YXXnhBiQTvvvuujBs3LlLb9hdu3bold+/elbRp0yoPEHuBwb4+ziFAIBxT3bp1Ze7cuY6qyPXr1wXzTJMmjcP71os3b96Uc+fOydNPP63GYb0Xk8AAcQXPYgd+6tSprY86dezK89euXZOLFy9KlixZVN4JpzpwodKFCxeUUGTNZwFB7I8//pBkyZJJ+vTpo/VKwTpiPbGWMZX79+8L3j30pd9FejDERC0I7lNgCIJF5hRJgARIgARIgARIgARIgARIgARIIMgJUGAI8hcgiulrgaFKlSpqxz2Mzth5X6BAAZsnkCtg7NixAgP8+PHjI3kwIKdA586dZeLEieZzCAMEjwjspI8fP755PTQ0VIUP+v777+XYsWPKQF69enV5+eWXVf4F9GUNkXTy5Em1E//HH39UbaBd1IegkClTJrNdVwUGGPth9P/ss89UmBuco20YrGGYtpYmTZqonBMrVqwQjBsMKlWqJJs2bVLGc9TFszDWHz16VCZNmiQw8nfp0kXeeecd5dmAHBYwtKNAyOnXr5/Ur19fGbe/+uorWb58uaAOSqFCheS1116Ttm3bRgpZdejQIdXm9u3bVV18wCvi66+/llKlSknp0qVl//79pmcGBIj58+dLmTJlVFgr5JmwChwQi8CgYcOGZntRHSAslrPP6/weWGNdKlasKKNGjVJ5C/Q1hLTCNYST0jk+9L0aNWoobw/k54AnyO7duwXvKrxG4A2CZ8FMh/a6cuWKvP/++2qNdBuYP94NhIuyFrQJb51ffvlFXYbQVLNmTRkwYIAkTpzYWlWJT8jvgbpY59dff13ee+899U2BwQZVcJ5QYAjOdeesSYAESIAESIAESIAESIAESIAESCCYCFBgCKbVdn6uWmCAsfzFF1+UwYMHKwMrjN+6YNc2drpDRIAR/M0337QRGHC9WLFiSiyA8bVy5crKSP/zzz+rJmCcnzZtmm5OGbJnz56tzmFoR4HRXBerwAAjOvIaoA8YcpFUFwl1cQ7DsTY841lXBAYY+p955hnVJfIuIFRS8uTJ1TmM8fa5ETCG1atXq3lDhEGBkRveGvD40OPDmODtAY7WHAxNmzZVhnCIJWCUP39+Qf4HGLRhqIagguv/+9//BDv+rXNEfP8UKVKoPmHgLl68uDpGX/C8wLhQ8PzevXtVuzt27FAeCrovhEmCaFGiRAklPkBUKFeunFy+fFkgmqBAOIlOZLhz547TzyPhNpJmo5QsWVKeffZZ2bp1qxoTrllFLIgwI0aMUH+QoNtaIDrhHdi1a5d6P9EGRBTMXYs1qP/777/LU089pYQWMEaBmIFnsTYo1hBW4A3uKC+99JJa+w0bNqhzCDE//PCD6UUCYU0n4wa3DBkymKIEHqDAoLAF9wcFhuBef86eBEiABEiABEiABEiABEiABEiABIKBAAWGYFhl1+eoBQbslv/iiy+UgdbeYAojPoy1ECFgNIdB1hoiCbvI+/fvr56FsRu5GlC0MRjHP/30kxIKdFu4BkN8+fLlcah2iOtk01pggKcDduZjNz4Mz9hBjtA0EDwaN26snsEudhiDUVwRGLBjHjvdMe+1a9eq5yECTJ8+XXlHLF68WF3TH1pgwDl2+zdv3lyJLkgI7SgHg6MkzzpEEp5FgmcUiBUIHaSPtUfGpUuXlMEc1+ElAaN3WFiY+saOfXhwoD14hmDXPjjgOtbh448/Vt4T9jkY0E7ZsmXV2sEInzBhQtWvfgewxvA6iKo4+zxCBz333HPKsI95Yr4oCFvUqFEjtW4QCSBAIdxQbAQGtAfBBB4z8GyAxwHmDe8DvJsQWiCKIZQTEn8PHz5cCRMQbqzM4cmBkFUof/75p3of4XEBwQPvnA6DhPv4+ejatSsOlUACLwaIHPY/L6qChz+Y5NnDwO27o8BgT4TnJEACJEACJEACJEACJEACJEACJEACgUaAAkOgrWjczEcbl7WhPUeOHALjMwy02B2PgrBI3333nTKKI3SSVWCA0RtGdhRHO+A7dOgg3377rWoDuQ2qVasmyLUAYzCM4dYC0QBtaIFh5cqVKhwO6vzzzz/Ky0DXhzFYG+P//vtvtaPdFYEBc4NwAc8KeFigIART1apV1TEMx9gRr4sWGDQnfR3fjyswwPidK1cuc5e8bhviC3bVL1myRHFD4mgILij2PDB2hBeCgACjuaMcDHqt8TzWOFu2bDhUhv9Zs2apdcQaRFWcfR7rjXW3ilC6zb/++kt0bg54jsCzIbYCgxZe0Lb1PdS8dJ94PyBGoJw/f16FpkI4JrD89ddfdTX1reeI+vv27ROErkLoLwgiECesBT8T+NmgwGClEqTHFBiCdOE5bRIgARIgARIgARIgARIgARIgARIIIgIUGIJosV2YqjaoasM5doDD+I8d+PBMwA5whA9CmBnsqj916pSNwGA19GuDsbX7KVOmqNj32IEPgzDCEsF4D8OuNpbr+jD2N2vWzBQYYHjHjnEYqjEu+4LcBijYCY+8A84KDFZDPTwudEJlhADCOFEQQkeHxcG5FhisO/JxHeVxBIbwFkSxRVgjJCYGY4T20fkYtMFcr5UzBm1HAgOM62CpC3buYxc+1j5nzpz6cpTfzj5vLyrZN6jfAe3BEhuBASGSIFZoccv6HiJslfYKse8b5xCRIMgglwPyblgL2uzbt6+6BE8ZhHnCOwyhoVOnTtaqgtBV4ObMetg86IYTejC4AaorTVJgcIUW65IACZAACZAACZAACZAACZAACZAACfgjAQoM/rhq7h+zNlprgeG3335TIWdgiIahFqGNYIhFboBly5apWPhWDwYYwhEnHwZf7BS3Lwi5g9BKMMIiz0KSJElUFRjREUbHWrTHgvZg0LkJrHUcHevwS84KDBAtIF5EVzDHnTt3mlW0wACPAngJWMvjCgwDBw5Uu+qtbSJ3gs4doAUGhIjSeRsWLlxorR7p2JHAgErI4QDPEZ13QT+IHfsQVbBTP7rizPPIwYE1cSTGoG2dV0GHJ4pOYNCeJvY5GJA/Yv369eZQ9XuICw8fPlShl8ybdgfaS8fucqRT8Ee+EQhYCJ319ttv29TB/VSpUlFgsKESpCcUGIJ04TltEiABEiABEiABEiABEiABEiABEggiAhQYgmixXZiqvcCAR5FIGXHoYUyGoXrSpElmKCEYeq0CgxYk8NzNmzclWbJkODQL4tZ3795diQwwausd51oUMCsaBwjTgxj9WmDo06ePSpYMo/eXX35prWpz/PzzzytDrzMCw927d1XoIxiHsYPfGgYJjd66dUvtWMfxkSNHFAscu0tgQNgd7TXRs2dP5VGAXfEIHdSgQQOVrwA5JpBjQScmthc/MD4UbVhHXoOoBIbwmuH5BpB7AqIOwlKhQNSAF0ratGl1tSi/4TEQ1fNt27ZVYZrgAQNPGGtBHgad+wEeGq+88kqUIZIwH+SYQIlJYEC4Kx3SC0myYfi3FrSFgvcPSbLxbiP0Uf369a3VbI5RD0m4IfD07t1bPv30U5v7EOCyZs1KgcGGSpCeUGAI0oXntEmABEiABEiABEiABEiABEiABEggiAhQYAiixXZhqo4EhsGDBwuM+x07dlS70GGMR7JbGJ7tBQaICilSpFA9wuAMTwhrqVWrlixatEglVP7mm2/kxRdflN27d6vEu+jHWtAf6miBAaFpWrRooQzfSGSsjc14BoZqhE2CYICQNjAoOyMwwFhco0YN1SZCPiVKlMg6BHWsd7hb80S4S2DQ3g9IHA0hxlr0OLQHg/YGQZ2rV69K6tSpzeozZ85UO+y1p4kjgQF1kFcAO/F1TgI0AM8T5IDAOsMrAN4Bjoqzz+M9QDJwR3kLrOGpIFJkyJBBIKxg/jpBte4b4aJ0noiYBIYbN26oNcWz9uIV8k2AJYo17BEEJnhRWAtEA+SQQIJojEuPzZGoo5N2M0SSlWCQHlNgCNKF57RJgARIgARIgARIgARIgARIgARIIIgIUGAIosV2YaqOBIajR49K3rx5zVaqV68uixcvVuf2AgMuauM78ifA20F7KVjbQbghGGl1OCAYZbGL/IknnlDtWutqgQG76XUYpeXLl5sJn/GATiRsNe46IzBAXIDB/oMPPpARI0aovu0/sPMexm6EicIYMB89R0chkrALHoZqa0gg7NyHkR3JhHv16qW60Abp5s2bq7q4iBA88OxAmCCrl4ZOIIw6WmCAAIDE1vhGwmwkGEZB7gjs3ofXCeaEuY0dO1YJNVgTCDUoOr+G5qsuGh+XL19Whny0a03ure/rb2efh7EfOTFQDh48KPny5dNNSPv27ZV3hRZCcEPn3ihZsqQKR5QgQQKVtLl27dpKnEKdmAQG1NG5FSCgTJ06Va0bkj+3adNGJSmHFwi8QfQ7j2eQcyF9+vQ4VKIVklxjLTWjjRs3moKLlQ1YYY7wnLC+g6ohL3wwB4MXoFu7pMBgpcFjEiABEiCBQCFw4+Yt+eP0n3LuzwtG0rJUku3ZTPLM0+kCZXqcBwmQAAmQAAmQAAmQgIsEKDC4CCxIqmtjq87BoKetY9/jHCF0GjZsqG45Ehisu9IrVqwoFSpUUB4PMGwjoTMMxfPnz1fPw/iM3eS4DsMsQiJhVznC/8Boi6KNuziGkV6H2UHyYISkQTJkjBsFxnnE/EeJSWC4cOGC2jGPuojZX6JECRxGKgcOHDB3+COEEXbiRycwaNEEeSggxkD8wDVnBAYIAhAXUJC0GqGR4EUAIUMXGMYRogcGeAg4Ork1DOnwKgFnsIMgAg8FiDZWwzhEBuRuQMmfP7/6Bnd4Kvzzzz8C4QNeJfBqQPJtGPgdFeTQcPZ5JE9Guwi7hPWERwBYLl26VDUNcalYsWLq+PDhw6YIgTkWLlxYkAAagokuzggM1nBdmBvWF94zup1t27YphhAdkNwafWDNINQkT55ckNcC48KYMVedEFt74eA6PGqQ9HzGjBlmuxQY9CoF8TcFhiBefE6dBEjApwiMGDVdPhsx2RzTRz3bSPvWUcdDNCvywIbA+T8vysdDjDip0xbZXMdJuTLF5JOP3pdSJQpHuucrF/p9Okq+m7zAZjgbVkySPLmeM6/hL4T5itWQq9fC/wGCG8WK5pelc78x6zh78OU3U2XClPAEaR/3bif1a4f/48TZ5+O63sV//pUXXq5j02zNahVk9PA+Ntemz14qPfra7riaPfVzKVvqRZt6zp60aPeRrFy9RVWf9t2nUrH8y+oYsUofPgxTx/HjxzOTpW3ftV9KVWiqrufOmVUO/7pEHUf34em1jW4suPfV6Bky4tvpqtp77zaQHh+0iOmRSPe/m7JA+g0apa7j9xV+b0VXsuarJPfu3Y+uinnv7JHVxj/uwmPOmhcf82Drzn3SsHn4Py4LF8orS2aPfMwWnX+8Sq32smrtVvXA/OlfCt7r2BT8/IeG6hi6IeYOydi0xWdIgARIINgIUGAIthV3br56Vz2EAYTg0UXnTsC5NRwPDNEIcwTj69mzZ3V1ZTyGkADhwFoQ9gjJia2hiBD6BnXRli65c+dWYXLgYYBd7qNGhf8dC38f/frrr1W8fF0X3zCGDxs2zBQXcA1hdhDSBoZ9hHiyL3pXPwzCx48fN/9ua18P5zoPhfZ00ImLN23aZOZM0M9hlz4SF2uBBN8YC0QGjFEb92HshrEaAsGECRPU4wj1BAFA50HARRiyEZ4J+QkgWIAp2kIoKBQkHG7aNPzv4uqC8YE8Dpif9haAVwPWCYZyFB02aPbs2cqgrseqbhofEFEgVCD/Q3TF2eeRy6Jbt27KW8HaHt4bcNDigr4Hg32TJk30qfqGJwlEA3gU4F0pUqSIEobA2l4Q0w9CIME7dO7cOX1JvasIGQUPB10wf6wtBBtrgZcNxC7r+DAX5JUAd2uBCAQhgwKDlUqQHlNgCNKF57RJgAR8jsDzL1aXY8dPm+MqVCC37NliGw/RvMkDhwQu/3tNChSvKTBSR1d+XPCtVH493GU1unreuNexx1AZNW62TdeTxnwizRpVM68dP3lW8hR5yzzHwUuGwLB9/Uyba86cfPjxSFPY+vbL3tKuVT1nHnNbnT8v/CNZ8lS0af/pp56Qv46vs7kGQWDa9+G7f/SN5QtGSaXXS+lTl77fqtdBlv/0s3pm0fdfSbX/lVPHrTsMMMUqCA+N6/1PXYehukzFZurYWYHB02urBhfNB4S4gUPHqRo9OreQIR93iqa241vDv50m3fsMVzc/eO9t+XJwN8cVH12Nn9p5ce/OpV1GArwE0bbn6s21G3bIG9XDRZCiLzwvv2ya5WoTsa5fqUZbWbN+u3p+7rTPpXZ12/fc2YZnzv1RmrYONxi0bFpTvvumv7OPsh4JkAAJBD0BfxIYEFcfRu179+6phLCINZ80adKgX0NfB4D1Qqgj7BhHKB8kX7bmCbCOPzQ0VLBzHYZ+1EMOAB1ayVpPH1+/fl3gWYB8ARkzZlThlqw5GXQ9b33jnT1//rzKR4Fd8a4WcAMLeDBAJNBzg3Eb98BH57pA2xAmwA+5KRBGCs/ZFzBGngMUrIfmC5YQHhD+CUm58XyBAgWiFVysbbvyPNYLIgHmgXVGTgU9N2ubOIYnCxJroy4EnjRp0thXceocG1IgYuEP3hV4zETllYEQSXivsH4YGzhEVZCfAWGSwLJgwYI2ollUz3jqOkMkeYp0FP1QYIgCDC+TAAmQgAcJ/Lr3kBQv2yhSj/u2zZcC+aLfQRHpoSC+0KbTJ+ZufGB4s1IZKf5iAYFBfsmy9fLfjZsmnUunN0laI3SSrxVHRuhWzWvJuJH9zKFaDYz6YiALDJjjyYMrJGvmDHq6gp3w587/bZ7jwN0Cw9Txg+Tt+uG7fvYdOCYNmocnocud6zmndsJ7em1t4Dg48bbAkDJFcgejCr+UNGlicYcHgzcFBvx+2rT5VzVBiHkVyjkOSRAllEc3rD//LZrUkAnffhzTI7xPAiRAAiTwiICvCgyI/75mzRrZsmWLINQOjKkI22JfEGIFu9wRPgU7uxGGBwliWUiABEgg2AlQYPDyG0CBwcsLwO5JgARIwCDQve9wGf7NtEgs+vZ8Vwb0bh/p+rXrN9Q1hGxJkTyZCpexe99hSZw4oRR4Pqe5MwNhNA4fPSmXLl8xhIpc8mQ6xzsgbt2+IycMI/zZcxckR/YskivHs2Yb6AhuuVeuRoTjiTQg4wLGkiZ1SptbCOFz5PdT8rvhmfHUk2klb+7s8mzm9DZt4wH0f//+A/VsyhTJ1P0zxlgOHTkumTI8I/ny5lDt2zTu4MRqdEZInTbv1DVrHf39DxVWSF9YuXiMGQZHX/v3yjU5fuqsXDE8IQoaHiQZ0z+lb0X6xngPHTkpf/190QhflE2yZTWSjRkCxr174fNInSpFpPAuF/6+pNbjrrGz6YWCeSXDM09GateRETr7c5nl933LzLrvdxssY76z9W5xJDA4M5+oPBiw5nfvRoSySZQooc0aODMXvKcPHoSqcT+RNpXaEXTC4PuP8T6WLFbInI/1wJEHA+7PMcIf1akRvuM7qjpaYMDaXP8vXExKlCiBWA3Zd+/ekxs3b6sukyRJJMmThe8EdMaDwSowYF765xBhfLDeMZW4XFv8bJ86fV6OHDupxoF3MG/u59TvA0fjwC6mU2fOyzHj5wCCSPasmVUoseg8GMDw0NET8veFy5Lv+RySI1vmSD+7j+PBsGnVFKfClT3O7wd4Mx04+Ls8kTa1FMyfSzZu3uXQgwG/q3TYoXRPpFYI/zjzp5w2/uQ3fqdG9bsTFZ39PWf9/ZAqZXLlnYE+kS8GJVHChMbO1MQqhBRCcKE8nye7PJUurTrWHxQYNAl+kwAJkIDrBHxNYJg1a5aKY47kvfYlofH3i7QpEhv/f4gv9x88lKs37sjd++F/r7LWRRx1xJJHXHQWEiABEghWAhQYvLzyFBi8vADsngRIIOgJwFCZJW9FM6xPjbdek8VLw8PBZM70jJz6bYWNUe/S5avyTPZyihtCx/Tr1VY+7D/S3J2Pawu+H6EM9tXrdTSv44FvvvjQJq8DDLFDvpwgA4aMVe3pDxhky5d9Sb76rKfaNe4oJI+uq7+tRnDMaejwidL/09H6tvld8qVC8v2UYTa70StUbSUbft6l6kwZN9AQW6bL/t8iElohXNTMiUMNoSG72Y79AQx1iZ4oal62hrnRFwcOGycHj5xQp00bvKU8HHACpthdrLnr+uD/1ptl5YtBXSVJksT6svKGqNW4s3mOA7D90Qixs3L1ZnV9y9ppphEdRs52nQdFah+hdRYa4Xiez5PNbMtqhMZ8IWKg/Pn7WjNJdcEStdR1632rwODKfBwJDGDZoEUPWbhkjTmunRu/lxcL51PvqbNzeenVhgLhC2XdjxPkg56fqXVF+CGsj6NiLx7gvTr5xznp0LaRfDWsh3pk8bJ1UrtxeCI4KwMtMMxd+JM0bNFT1UXejbXLJphdjf5ujnToNkSdN2tcTSaN/kQd2wsMBQrklFyFwr0VzIcfHRzbu1T+Ntb0cUIkWcftytpiCHgn6jfrZr4b1vHhPUSYq5CQEPMyxLUaDTrZhGArVbKwFCqY2xSqrCGSkCNhiPHz+4mD3wv4+axR9TWzbU8IDLH5/QBxBGG0rD/T+N3YtWMz6flReO4Oa4ikXC9UVe8ZJob8CIO/mGC+u7jWtNFbMvKzXgJhQBdXf885CpG0aeuvUr7KO6rJWtVfl+eezRhJbO73YVvpb/yeR7GKqOrCow+r+GW9zmMSIAESIAFbAr4iMCAB7vDhw+XUqVPmACsUzSplCmSRYrnTS95n00mGdJE3L1y8ckuOnL0svx67IJt/Oyerfj1l5orKkCG9EaO/i3TvHu5haTbMAxIgARIIAgIUGLy8yBQYvLwA7J4ESCDoCVhDdsCwf/CXRfLs82+YXH5ePVVeKf6CeY7d3+mzlzfPHR2gHWs4IGsda3uDPhvvUATQ9WHchTH1xKlzkWL+6zr62yowNH23j8yc86O+Fekb49u/Y4HyZsBNqwExUuVHFyAy/PrzbBuxxb5ukVL1TGECfXT/oLm8Xr6kwJAYXSz3mPpv3aK2jP3qI9Xdjys3SbX6He27jnSuBQbsvkZ+DftwPvoBjHPNsu+kWJF86pJVYIAHxriJ89T1JXNGStXKZQVeCU89V1Zda9e6nmkgtgoMrszHkcBg71GD5NEIN+XqXKwCg/WddEVgQIx5JOy25iTp2W+EfDFyqroGz4Gft+5WPOJSYMCOffs8F6oT4+PonqVy8dLjCQyxXdv9B49JkVfq6aE4/O7WqZkM+yRcAINgk+/FGlH+PtANWAWGRu/0kjnzV+pbkb6/+7a/tGxSU11/HIEBuUXws+moPJslg+kVEtP7jOetvx/gffNKhSbyy+6Djpo2r0UlMJgV7A7Kly0ua34Yb1519fecI4Fh45Zf5bU3wwUGs2EHBwtmDlfCToacr5litLWafZ4W6z0ekwAJkAAJRBDwtsAAT4XevT+UffvCPdVeyPG0NK1YUOqXyytpUyaJGKiTRzfv3Jd5G4/ItNW/yY7D4XHmcxkJaj8dPFjq1q3rZCusRgIkQAL+T4ACg5fXkAKDlxeA3ZMACQQ9gVbvfyyTpy9WHHQ87dervSvrN+5U195v09DYORu+GxsX7AUGGG5haEZ9e4Pa4I87yvm/LtokDba2Z00sjQSt1auWlyNHT0m7DwapvvEBgeFJI0SHvWCw/udfbHa4a+ObNfktnn+vTQOp/mZ5OXv+gnzQ4zPT0GndPW5vQOzZpaXKnTB30SobQ+epgytNUQJt25cvvp5q7k62v/dGhVek4mslpZoxlpxGGChdzv950UbQGTGsuxQ1dupjF7xOtqyTDMNw+WKZBqaIgevtW9dXoZCGDZ9szg1ta4HBKuKgPoy/+LZ6acB4DyM+ilVg+GJwV5WAGR4QYDK4f0f5ac0WebP2e6oukg7rZK9aYHBlPmjEXmDAtfe7DsaXKtbExq7OxSow6PbgtYGd2p/266Av2XzbezDAmN36/QGqzr9nNyujc8nyjeJ9JHwAAEAASURBVNW7jndrv5ELwR0CwxsVXpZf9hyUwZ9PkFVrt6r+YYTHWr1UJL/s3n/ksTwYYrO2GESVWu3N8cDD5vNBXSStEX5qoiHCzFu4So0TH4d2LTZCdz0nXT78XEaOjkj+jTkkS5pEJkxdaCN6aYHB3uCN3wulShSW6XOWyQ8/blDt43cOkm4jnM/jCAyqsSg+pk8YLI3qvqnuuvr7werhggbq1npDShQrKPMXrZbtv4QbdHA9OoGhfp3KKnE6crfo9wvPaKEvNr/nnBEYIJR06dBE7Ubt1H2Y+TtFi5w7d/+mvDKGDZ+E4Qh+r/Xu3kpy58hqejipG/wgARIgARJwSMCbAkPXrl2V1wIGVjDbU9K9Xgmp/Woeh+OMzcXlO07I53N3yM4jf6nH27RpI2PGjLHxaoxNu3yGBEiABPyBAAUGL68SBQYvLwC7JwESCGoC2BGeMn1Jk4HeKT5mwlzTyAtj3sVTGwQx8FHsBYY9W+dKofy5VZ6ETLlfN3e3aoM0nrEakWFsmz35MyOnwnVlfMT9VClTqPAzCKtiHw5p40+TpXTJIqhmlt9PnBEYj7WXBAydO9bPlPRGToFmbfrKjNnh+QIQhmXTT1PM56bPXirN24R7AuCiTrRsNSBaExrfuXNXkj9Twnze0VjMm8YB4swjDM+342ZZL0c6htH0i0+7qn9w7dh1QMZODM9nUOLFgtLWCC+DYo1zjvN7/+42wisdt9k9vm39DCletABuK+NrzUYfqGN8aIHBGnpl4ugB0rxxdVUHSYKLlo7YiX726GqV88EqMHw5pJts27lPGUY1ywFDx6rQNWA+ddwgw/ujtWpPCwyuzAd5M6zvBsIJ6VBVaBRG8M7vNVHt48PVuVgFBni4IExSFiMHR3TFXmDYu3WeFH4lfAfeqiXjpGTxQpIqw8uqiZmThsjYCfNMA3BcejDAywKldYcByoMCx9YwNFYDM0STw78uEfxcXLh4CVVtSkEj/wnykzzu2qL9vEWrmW2vWTpeyr9aXJ3j3c+Yq4L58w/BAIJUgjQRP7tDP/lAundqrurbezZogeE9Q1waa/z+QXm7QVX1juEYYZOezlbO/JmfN+0LJRR5WmBw5vcDfg61GFK5Ymn5cf63mILKB1K2cgtTZIhKYLD2gZwdBYrXMsMnQdT62giVFJvfc84IDNZk5lZBTwu4mIf1d5MWpXGdhQRIgARIIGYC3hAY/v77b3n77bdVEmeM8KMmpaRXg4i//8c8atdqfL3oV/lwwgb1UIkSJVSOh5yGVwMLCZAACQQyAQoMXl5dCgxeXgB2TwIkENQE5i9ebcRS724yuHFhh9oVjATH2fJXNq//uOBbqfx6aXVuLzDoZ3DTaqi3JsW1ChbI8bBgxnCzbQgK6zbukLXGH+wGP2YkZLYWe6M+dtOXer2paXCDAPLLplkqMTSesxqVB/V7Xz7s2sps7vS5vyR7/irm+XZDlIBh3Dpu6455VLQatWGgLlu6mPl8VAcnT5+T5St/lhWrt5g5EezrwisE3hwoyDmwx9iRvnbDdtm0Zbfs3nvINNTq5yAwLFuxUXTuBRj4Tx/6Sd9WSYXTZi5lnkNgePGFfJLkyYjxIv9EKksyYL0zHg/ppNP2RmiIPl16fa7a/e/CdhWeCd4qiAnfrFG1SAIDKjo7H3uBQXXy6APr+ufxtWq3Oy4hX4erc7G+C8MGdpZuRvz7mIq9wHD38q+SON2L6rEBfdpLWUMEKVe5pTpHqKJW7/X3GYHB6o1knWdcre1yI8cHckXoYv3ZxzXkEZkwZaG6DU8LhM2xhlPbvXmukVw8t37cRjzRAoPVewqiUE4j4bsu23bsMwWGXl3fUV4ojyMw1KlZUbJkciw4Nar3phk+ydXfDzpHCcZtn3fGapyPSmBYv2KivPpK+DuHNrr2/kK+GjUDh8pjYMXC0bH6PReTwGD/O2XOgpXSqGUv1W+ZV4rKhhXhXgvWOVBgUHj4QQIkQAJOE/C0wHD8+HGpUaO6HDx4SHJkTCujO74hpQtmdnq8sa249/jf0u7rVbL/xEXJkiWzLF68RIoWjchVFtt2+RwJkAAJ+CoBCgxeXhkKDF5eAHZPAiQQ1ASsO20BAkYkXaxhOaw7ie0FBhhgEySIrx6zhk+xJjmOSmD47KvJKkG07tPRt1VggMcF4oVbQzFZd/Hj+TSZSplGSO2RYW3Xen/hzBEqLJPVgLj6h3HyWtkIrwVrGCdnBQZrf0jE+qshGEDMGf7NNPOWNi5iZ3b95t3NHc9mBbsDCAwTpiyQ9l0+VXesBj9d1ZoDAgJD5ozPSNbnK+nb0X5rFvYCQ3EjtItOJowd66+/9a5qZ+zIj5SoY+/B4Mp8ohMY0MnHvdvJRz3bqP7O/fm3y3OxCgyO3gXVsN2HvcAQem2vGRYI+TTwp1e/rwQCyJVzm6W88T7qnxVvezC4KjC4urbjJ883w5fp99eKD54H8EBAQRLpaeMHS7FXG5hV7lzaZZOLRHvDoIIWGKyCnvmggwO9k/9xBIZNq6ao8EsOmre55Orvh/ipC5vPrzXym5Qr85J5DjG1YrXwd9rK0Dpv+1BsCCfVpmN4MnD9jPX3mKN323pf/2zHJDBoLyU9WKsAbf19Q4FBE+I3CZAACbhOwJMCw9mzZ+WNihWN8KNHpVSBzDLjw7fk6TTJXB90LJ+4ffeBvD10qazceVIypE8vq9eskfz588eyNT4W7AQePHggd+7ckRQpIicf9wc2V65cEfxMpkqVSjJnzmz8+zmBPwybY3SBAAUGF2C5oyoFBndQZZskQAIkEDOBS5evyjPZy8Vc8VGNa39ulRTJk0UKkRRbgWHPviM2xkck061Z9TV5qVgBecMwwO3/7ZjqWQsMMNTXbdrVxhBv9azQE7Eala0GatyHZ0GuQlV1Vdm58Xt50ch34KoB0WzAcoDwMe06D1JXEhp/YYRRDzHirWX0d3OkQ7ch5iUYrxFOqVOPYea1/h+2FYQjeS5LRnnO4kUCgWH5T5ukRsPwMEgwcF89v8V8DvkZrCGqIDAUKZhXkj0dHsIGFeGdkSRxIvMZ68HLRpz7jOmfihRG590WdcwwWgjbo0O/YDf6lavXInkwuDIfRwIDDNOHjpw0h3Z8/4+SLWsmQagYV+difRfsd4WbHdgdOBIYBg4bJx8PHqNqIuY8PD+0J065Ki0jCQxWw6y90dYaEgpeIJPHDFTtwjMAHgIoVnHOlRBJk2csNseiGnr00eX9plIgX87HXlt7DwZ4tCCfgi5WgaNqlbIq7FnOQv/Tt2XftvlqHPqCdc5aYLD+LCJJOjyMHJXnjHcCP7u+KDBkzVfJzC8xengfQUJtXazj1WIB7lkFhl2bZkuRF/LqR2zWTb931nfb2d9zFBhMpDwgARIgAa8R8JTAgNCFpUuXlq1bt0rZF56VRQNqSeKE4RuCPD35+oOWyLJtxw1xIZ9s3rxF0qRJ4+khsD8/JgAvnObNm8uWLVskZcqUcv36db+ZTWhoqEybNk169eolFy9etBl3vXr11PUiRSLCieoKN2/eNBKx75M9e/aokLrZsmWTsmXLSrJknhMI9Vj47TwBCgzOs3JLTQoMbsHKRkmABEggRgLfGbvh23YKN27GWNmo8P2koVK/duXHFhiQYBfx0627na1GWHtDshYYrDvrMV5rPgHr+Fu27ydTZ/6gLiEk0JY1EV4DMMC2eu9js7pO2ms1asbWg+Ha9RvyRJbwMFLowD7UEq6NHDPTDDeEZKp7tsyVBi16mMlx+/Z8Vwb0bo+qYo2xj3MIDIePnpQXXq6DU1X0muAfkdjlbF1PRzkYkEOgQrlw7wx4Gnw/b7nKnYHGar5VQdKmSWVjzEQOhg/aG0l2jZBU1uS0qA9hafO23ZEEBlfmYy8wYFc64vbD2IpQWCjaoIpjqxHWmblYjbCPIzCsXr9NKtdohyGYReeHcCQwrFyzWf5X+31VF0LQv2d/lnjx4qkY/KUrNjU9cFwVGKaMGyhNGryl2rW+HzoHgzm4KA6sP0OxWVv7/Cg69BK6Q1iszHlsc7B80uc9M7wU6ljDBd28ddvMZYF7WmCw5mCw/jygztLlG9TvHxy/XLywPJ8nm08KDFZPLuv7i3Fbf9dEJTD06dFawA4Fwiq8qE7+cU6dd+nQVCXWjs3vOXcIDM0aV5NJo8O9K9QA+UECJEACJBAtAU8JDO+8845MmjRJChjJnFcNqy+pk9tueol2kG64+UbPObLlt3NSq1YtWbBggRt6YJOBSqB9+/YqWTjmV6pUKUOk2uw3U+3QoYN8+214Li54LSAXyZkzZ+TkyYjNVD///LMSA/WkICyUKVNG/vvvP31JfT/99NMyZcoUqVIlItyvTQWeeJ0ABQYvLwEFBi8vALsnARIIWgKvVjJ2gmzfq+bfsG4VmTEhYme9hlKyfGPTGIqY6gjF8bghkhD3fM6Uz2XUd7OlY7ehqivEWoeQ8Pffl+XLb6bKrHkr9BAEhuHTZ/60Sc4Mo2271hEJilEZuQL692orew8clVcqRCQGhgGsTvWK8ofRhtV7AB4T333TX/VjNfrFVmBAQ1amOMfO5UJGgt2HhgCwa89BU/jAPW1Qrf12F1m8dB0uCdYBRtjdew9Ltz5fml4cuAeDPgzySBKr1w3XsS5/nD5vs+sf17XA0LPfCPli5FRckqefekIGfvS+8lT4buoC0xsBBuoDOxaqUFeOjNDWNtAOwgT9tHiskZD5l0gCgyvzQWgt647+b7/sLe2MJNdTv/9BWrbrh65U0YZs6zicmUtcCQz/XrkmTz1XVg9HfevwOo4Eht8OHbcRgrCuEHZ+MAzk2gMEjbgqMCBBevvW9eUVw7i+c/f/2TsLACeuLQyf4rq4u0Ox4u5aKFKguDvF3d3d3d2lWHEv7u5uS5HFvbw5N+8Ok5Bkk41Mkv3veyQzd65+Z2Z3e/+555xXXVc5IjBomfKYrNmWr2t3HbANhvZvSzFjRKWZSuwFjhEiE8enSJk8ETVo0ZsWLNkgs2nk4A4iGDsLjNr7WD4Pm7bsE3E+ZAV+Oz+vsrtm266D6n3M1+TOFu2OAA6ePnpIJ1nV7LfWfRGPn3dlWUocJ4WfL3t/PpjevxxrhV0Mbd623+hngCWBgcfDwd5zZssgXKvJnS2cf3DnQsqluC3jYOr2/pxzhcDA9x4/t7zjInq0KDxEJBAAARAAASsE3CEw8CJkgwYNKHSoELRvbC3KlDy2lRG559Id/1dUsP1ievryHY0dO5batWvnno7Ri9cTyJw5M509e5bWrl2rxBOp6DXz4d1DLIhwWrlyJVWp8v0lsTt37lCtWrXUXRkBAQHiZaRHjx5RmjRphLjAdVmQY9dKS5YsUUUJdrPEYgWS5xGAwKCzTSAw6GwAdA8CIBAsCZi6CpI+uk1hjBw/T/ibl/mPru+in0L8ZBS41V4XSVJgOHnmkghUKtu29M1jO33+Cg0YOs1SETX/xf1/yC9yRKPgsepFzQELFBeOraUE8Q3/wWXvAqKmKaNDFjF+yfOHGgPC6KLmhHcv8M4Kdi9j6lJIU8zo8PHN3RQrRjQh+LDwE1iSAgPHrcicu4r6BrS5eqsXj6GKinsqTuYEhjV/7VDcU31fuJUuWcwJDPbOx5zAwG9t5yhUQxVYWIA6f3QNfVXcQNkzF2cJDMxFG4uDz6V7IHMCA+8oyVmoJvE9bi3ZIjBMmLaY2ncdadQML94/efrcKQKDPbblQVy4dIMy5a5sNB7TE+0b+KYB403LynMpMPB53aY9afHyTfLSD99aIcERgeGHhk0y5A4he38+8P2bp2jtQO1vTWAwGYo4lT875TWt+yyZp/02/TnnLIHB3M9uDujNQd+RQAAEQAAErBNwtcDAC5WpU6Wkf58+owkti1OjMpmtD8iNV1fvu0J1h29UXpgJSdeuXSN2+4IEAoER4AX3q1evCpEhY8aMZouzq1h/f3+KFy+e0fWPHz+KHQOxYsWy2TXXv//+S1zeNHH7nM+7km1JI0aMoK5du1LhwoVp9+7dP1S5fPkypUuXTuTz88C7G5YtW0Y1atQQdbZu3Uphwhhc27J7pThx4oiyLDZwGSTPIwCBQWebQGDQ2QDoHgRAIFgSGDt5IXXqMVqdu1wsVTP+f8C+8DPmqqRms1uicr8WptjJvr/NrRUYylZpRVu2G7at/rV8PP1W2lBu+uyVanBi7SKZucXoPkoMgpcvX9P4KYtFv+w+qUTRPKoPfHUwZg44JgEvqvEfmeyOSDtHWZzf0J47baB4i1/NK9+Udu89Kk5Ng7JqF5ZtcbNzS9lNMGzMbCUg8xrZvNE3L6R2addAuCPiC7wY2ahlX1q0bKNajt+snjquF/VXRBUZi4K58A4NTuyqht8837n7iBAzODZCg1oVqe+QKWp5bfDrx/5PqdfASTR34Tq1Dz7gfnjninSbxHkcD4Ltwmns8M7Upnktunf/sVE8CLmjYO+B41S0bGNRVrqjsnc+WoFB669+554jVLJCM9E2f0wY1Y1aNqlO9sxFKzBIV1tqgxYOuH2OZSETx8ngxMG1+T7mxHEBDu823J9agUFy4TL+T55RpZrtjVxLcVyCMiXyq8+C1r1Mhept1R0A2meHY0JUUuJuaAObs8Dw9PkLylesLndFtu5gcNS2ojPlg++/2o26GY1JXjO30MyiREVlftLND5fl57Bs6QKqeNKtYyMa3Ke1aObDh4/i+e3Rb4JsVv3mXQW8i0P+x924KYuoY/dR4rp0H6QWNnOg3cFg5rJRlhQYigfh58OLgFdCKNHuPkiYIA7xPV6+WhvRj/Y+0rr/4jlOnrGMrl6/o46H3YeNHNiBwmpiqNj7c07rumnVwtH0e/litF9xc1a4dEPRjzaQM2doxafCBbLTzo2zRDkW0Oo06WG008yc3UVhfIAACIAACBgRcLXA0KlTJxo9ejSVypGM1vT7/je80SB0PGk4ajMt331J7LBgF05IIGCJAC/Kt2rVii5evCiK8Fv7SZIkES6Spk6dSv369aMxY8YQi2r8/ebNGyEycGGO29CkSRPas2ePqMsfqVOnpokTJ1LJkiXVvFGjRtHIkSNp2LBh9ODBA5o7d67YKZA8eXJq3LgxdenShfr06UOzZs0SMRQ4BgTHTuD+Q4cOrbZj7kC6R2rRogVNmTLFXBHq2LEjsaDBQgQHQJd1uDzX0ybul3dC8Fzbt2+vvYRjDyEAgUFnQ0Bg0NkA6B4EQAAEdCbALpcuKguQ4SOEE0GJQ4cO5bQRcZwBDr58XfkXM1Y0SpsqGcWI7h43HhyT4c69h3Tv3mPhvilxoniUNEl8iy5ZeFfJDWXhNnHCeGLBmF0+mSZe2OMdCTKFDxdWEVO+CddGPNfwsXLIS6oLGTVDOWBXP5ev3VaCo71W3m5LKgJJy4VabTlnHNsyH0f6cedcHBnnq9dvhV15twwLOkFN3A6LCmFDh6E4sWMImwe1LWfVYxFF3E+KIMj3U4pkiSyOixfD+Vm8e/+REug5FcWLEzPQYfC9fuXqbXrwyJ+SJk5AqVIkNlpgD7QBDyjAAtGFS9fF+NlllLnnmoepFRguHl9HaRSeLOSwzTOkS2nx5wbX1evnHAtB/spOmp+U/7GbLG3Abx4XEgiAAAiAwI8EXCkw3L9/nxIlSiQ63TumJmVPY/w294+jcX/OjYcvKFMTg7DAbm8svZHu/pGhR08jwLEWOnfuTIcPHxZDy5Qpk9ihsGXLFiEIdO/eXbgK4vueE8co4F0GHBg5a9asIo9FhezZs9P58+fFDgjOZGGLXYhx6t27Nw0aNEgc8wcLCJxk/AOuz7snOLHoIGMncN9DhgwR+ZY+WJRgkYMTCxm1a9dWdyFYqrNt2zblvx3viTgL8ePHV4uxeMKuorj/c+fOUYYMGdRrOPAcAhAYdLYFBAadDYDuQQAEQAAEvIaAafyL/j3/VGIWVFMWLYkGDJtOE6ctEXPhXRz+iksl7dvOXjNJDBQEgiEBcwJDMMSAKYMACICAzxNwpcAgF0srF0xDC7r+5rEs207ZQbM2nSEO3jt58mSPHScG5hkEpIsk3pWQIkUKMSjeccCL/JyqV68uFvsTJ04s3G8VKVJE7FzgQOfTp08XeVxu/PjxIvYHiwgsGsSNG9dIYOCdAc2aNaOvX7+K3QsrVqzgatShQwfq1asXRYsWTQgFLHrkyJGDjh417H4Xhcx8cOyE/PnzqzswuAiLJMWKFaM8efKI7+jRLb98xPXZTdKNGzeIx8KCnLcFuTaDxaezIDDobF4IDDobAN2DAAiAAAh4FQGtmxNLA+dAzj06NbZ0GfkgAAIeRgACg4cZBMMBARAAARcRcKXAwLsX+G3uv4dWpYKZDDsZXDQNh5o9f+tfytWKY5GFpxeKexvpZ96hRlHZZwkEJjDwboNIkSKJ+Wt3L3AgZRYdtEm2JeMYSFHONE7C4sWLxY4DFiOePXumukO6e/eucNPEbX758kUVL7R9aI95RwXHYpg5c6a6K0J7vX79+sI9Utq0abXZ4pgFjFy5chnl79y5k4oWNcTNM7qAE48gAIFBZzNAYNDZAOgeBEAABEDAqwiw66XGrfoJ/+jmBs5xMurXqmDuEvJAAAQ8lAAEBg81DIYFAiAAAk4m4CqBYfv27cK3fLokMej4lPpOHfXdJ69o0Y4LlD5JTKqQL5VT2i7ZdTn9c/4+LV26VLyB7pRG0YhPEpCigLkdDPXq1aN58+ap8+YYBRyrgOM1sKsh08QL+vPnz6f+/fuL2ApSYOBYC8OHD1eLs6uiUqVKUe7cuenQoUNq/suXL9Vg0RxA2lZxjF2FspsmdvfErp/WrVunCg7s2unkyZOUIEECtR8+eP78Oa1atYpOnDhBO3bsUN0zLVy4UIgfRoVx4hEEIDDobAYIDDobAN2DAAiAAAh4JQEOvHzh8nUl6PEzih41CqVU/NOnSJYQbpG80poYdHAncPzURSW+ynuBIUeWDBQ+fNjgjgTzBwEQAAGfJOAqgYGDxbKLl05Vc1H/evmdxu7L1/+oVLcVdPjiA6paOB3N7VzGKW2PX3OceszeS3Xr1hULvk5pFI34JAFrAsOAAQOEmyM5cd4twAGTWUjggM2mSQoK0j2XPOfA6OwKSSYpMJQoUYL4WKagCgyyvvzm3Q8sHtSoUUNkmc5DltN+swjCAal5BwPvZEDyPAIQGHS2CQQGnQ2A7kEABEAABEAABEAABEAABEAABEAABFxOwFUCAwey5Ted/x6iuEfK7Dz3SIMXH6IhSw4KLs4UGKSbpCRJktDt27ddzh0deC8BewSGadOmUYsWLcjU5ZGcfZkyZejvv/8WQaJZiJACw9ixY0V8BlnOUYHh6dOnFCtWLNHc48ePLQZ3Ll++PG3YsIEqVKhAixYtEuJIiBAhhJAQLlw4ORzxzcGdOYYDp3fv3ikvo4QXx/jwHAIQGHS2BQQGnQ2A7kEABEAABEAABEAABEAABEAABEAABFxOwBUCA7tqkYuRz9e1o7ChQzplHocuPKDiXZapbTlTYOBGE1abTC/efKAHDx5Q/Pjx1X5wAAJaAvYIDFIY4Pra2Ax8zm6KokaNKvLZlVKVKlVcJjBwf35+fqIvFg5q1arFWT+kTp06Ee+eaNKkCU2ZMkWN9bB//34RIFpbQcZk4LgQAUrsEhYikDyLAAQGne0BgUFnA6B7EAABEAABEAABEAABEAABEAABEAABlxNwhcBw9uxZypw5M6VJFINOTqvvlDkEKAv/OVsuoAdPX1PXGrlp+NLDTnWRxIMsoYgXBxURA4FrnWIyn23EHoGBF945sDOLC7NmzaJGjRqpXJYtWyZcEvEC/aNHjyhixIguFRhkvAeOB7Fx40bxjKqDUQ44HkPJkiWNxlqwYEFicaF27drCxVOoUKFEFZ5X9erVaevWrVSuXDlav369tikcewgBCAw6GwICg84GQPcgAAIgAAIgAAIgAAIgAAIgAAIgAAIuJ+AKgYEXL3nRsWT2ZLS2fyWnzKH2sA20dv9VmtiqBEWOEIbqj9jkdIGh8ei/aemui2IhlRdjkUDAHAF7BAauP3nyZGrVqpVoqmHDhvTzzz/T1atXacaMGSJv0qRJ1LJlS3HsKhdJ3PizZ89E30+ePBF9/frrr5QxY0b6/PmzCPjMgdk55cuXTwgHLHiwuMAiA6esWbNSkSJFiINb//XXXyKPPy5dukRp06ZVz3HgOQQgMOhsCwgMOhsA3YMACIAACIAACIAACIAACIAACIAACLicgCsEhvnz54ugtjWLpaeZHUo7PIcF285Ti/FbqUyuFLSid0Vate+ySwSGbjP30MR1J2jUqFHEQaqRQMAcgfTp09PFixfp5s2blCxZMlFEBnMeNGgQ9ezZ06jat2/fhJjQvHlzo3w+4WeFA4vL1KdPHxo4cCBNmDCBWrduLbNpx44dxAGeS5UqRVu2bFHzX716RVGiRBHnnz59Ul0aqQVMDu7duyfGt3DhQpMrRLyTgl0j8Rhkm1yIxYQ6deqInQ3aSiw4TJ06lXLmzKnNxrEHEYDAoLMxIDDobAB0DwIgAAIgAAIgAAIgAAIgAAIgAAIg4HICrhAYZGDbxmUy0/iWxR2aw7X7z+mXZnMpUvgwdH5WI4oVNQKt3OsagWHAwn9o+LLDYoG3V69eDo0blUHAlAC7Sbpw4YKI8ZEqVSrinRBhw4Y1LeaWc3bJdOPGDRHQnIMzJ0qUiDJkyEARIkQw2//79+/p2rVrojzHV0mRIoUQVxB3wSwuj8mEwKCzKSAw6GwAdA8CIAACIAACIAACIAACIAACIAACIOByAq4QGNj1S7Nmzajhr5mES6OgTuLj569UtNNSOn3dX7haYpdLnFwlMPSbf4BGrjhCgwcPph49egR12KgHAiAAAh5BAAKDzmaAwKCzAdA9CIAACIAACIAACIAACIAACIAACICAywm4QmBYsmQJ1apVi/4olJbmdSkb5DmMXnmU+szbT7+kjENDGxVS29l58g6NWnmE8mZISL1r5aUoEcNS5hSx1etBPeg4fRdNW3+Kxo8fT23atAlqM6gHAiAAAh5BAAKDzmaAwKCzAdA9CIAACIAACIAACIAACIAACIAACICAywm4QmDYuXMnFS9enApkSkRbhlYN8hy6KDERJisxEQJLLDRsH14tsGKBXq81dAOtO3CVli1bRtWqOd5eoB2iAAiAAAi4kAAEBhfCtaVpCAy2UEIZEAABEAABEAABEAABEAABEAABEAABbybgCoGBfbunTJmSEsSMTFfnNw0ynp0nb9OJa/4/1D93619as/8KpUwQjWopgaQTx/Kj6kXT/VDO3ow8rRfS2ZtP6MiRIwhcay88lAcBEPA4AhAYdDYJBAadDYDuQQAEQAAEQAAEQAAEQAAEQAAEQAAEXE7AFQIDD9rPz484qO3NRc0pTrSITp3HhoPXqPrg9VS1cDqa27mMU9r+8vU/ilphHH379o3evHlDESM6d8xOGSQaAQEQAAE7CEBgsAOWK4pCYHAFVbQJAiAAAiAAAiAAAiAAAiAAAiAAAiDgSQRcJTCUKlWKtm3bRgu7/UaVCqRx6pRdITDsOX2XyvZcSVmzZqUTJwJ3y+TUCaExEAABEHABAQgMLoBqT5MQGOyhhbIgAAIgAAIgAAIgAAIgAAIgAAIgAALeSMBVAsPw4cOpW7duVKdEBprWrpRT0Ww8fJ2qDfxLuEWa3dE5Oxh6zN5L49ccp86dO9OIESOcOl40BgIgAAJ6EIDAoAd1TZ8QGDQwcAgCIAACIAACIAACIAACIAACIAACIOCTBFwlMJw/f54yZsxIUSKFo4fLW3o8uwyNZ9OtRwG0Z88eKlSokMePFwMEARAAgcAIQGAIjJCLr0NgcDFgNA8CIAACIAACIAACIAACIAACIAACIKA7AVcJDDyxfPny0cGDB8UOBt7J4Klp0+EbVHXgOkqbNg1dunTZU4eJcYEACICAXQQgMNiFy/mFITA4nylaBAEQAAEQAAEQAAEQAAEQAAEQAAEQ8CwCrhQY5syZQ40aNaKsqeLS/nG1PGvimtGU772adp68LVwjsYskJBAAARDwBQIQGHS2IgQGnQ2A7kEABEAABEAABEAABEAABEAABEAABFxOwJUCAw8+RYrkdPPmLZrZoTTVLJbe5fOxt4PNR27QHwPWUdQoUej+gwcUMWJEe5tAeRAAARDwSAIQGHQ2CwQGnQ2A7kEABEAABEAABEAABEAABEAABEAABFxOwNUCw8yZM6lp06aUJE4UOj29AYUJHdLlc7Kng7xtFtKZG0+Ig1J36dLFnqooCwIgAAIeTQACg87mgcCgswHQPQiAAAiAAAiAAAiAAAiAAAiAAAiAgMsJuFpg4AkUKVJEBE9uXDYzjf+zuMvnZGsHfecdoFErj1DmzJnp9OnTtlZDORAAARDwCgIQGHQ2EwQGnQ2A7kEABEAABEAABEAABEAABEAABEAABFxOwB0Cw4kTJyh79uxiLlPalKR6pTK6fF6BdbDuwFWqNXSDKLZjxw4qVqxYYFVwHQRAAAS8igAEBp3NBYFBZwOgexAAARAAARAAARAAARAAARAAARAAAZcTcIfAwJOYPHkytWrVSsznrwGVqXi2pOJYj4+jlx9RmR4r6f3HzzRo0CDq2bOnHsNAnyAAAiDgUgIQGFyKN/DGITAEzgglQAAEQAAEQAAEQAAEQAAEQAAEQAAEvJuAuwQGptS+fXsaN24chQ8bmlb1qUiFf0nsdngnrz6mKkpQZ/8Xb6lBgwY0Z84ct48BHYIACICAOwhAYHAHZSt9QGCwAgeXQAAEQAAEQAAEQAAEQAAEQAAEQAAEfIKAOwUGBtawYUOaO3cuhQzxE83rUpYqFUjjNo7bjt+i+iM308s3H6hy5cq0atUqt/WNjkAABEDA3QQgMLibuEl/EBhMgOAUBEAABEAABEAABEAABEAABEAABEDA5wi4W2BggC1atKBp06YJlt1r5qFetfK6nOu41ceo55x9op+aNWvS4sWLXd4nOgABEAABPQlAYNCTvtI3BAadDYDuQQAEQAAEQAAEQAAEQAAEQAAEQAAEXE5AD4GBJzV48GDq1auXmF/+DAmpf70ClPvn+E6f79kbT6jvggPEuxc4de3alYYNG+b0ftAgCIAACHgaAQgMOlsEAoPOBkD3IAACIAACIAACIAACIAACIAACIAACLiegl8DAE9u8eTO1adOGbty4IeZZq3h6alUhK2VKHtvheV+9/5wmrz9JszadEW3FjRuHxo+fQFWrVnW4bTQAAiAAAt5AAAKDzlaCwKCzAdA9CIAACIAACIAACIAACIAACIAACICAywnoKTDw5D59+kS9e/emESNGqHMtkS0pVSmQlsrmTkHRIodT8wM7ePP+E20+cpNW7b9Cmw5fV4u3atWKBg4cSFGjRlXzcAACIAACvk4AAoPOFobAoLMB0D0IgAAIgAAIgAAIgAAIgAAIgAAIgIDLCegtMMgJXr9+XdlhMF7EZvjy5YvMpuxp4lKWVHEpbcLolCiWH0VXBIcwoUPS5y9f6cWbj3Tv39d09f4zOnXdnw5ffKjW44PGjRuLHRIZM2Y0yseJawl8+PCBHj9+TC9fviStLV3bK1p3lEDYsGEpevToFD++812VOTo21A8aAQgMQePmtFoQGJyGEg2BAAiAAAiAAAiAAAiAAAiAAAiAAAh4KAFPERgkHl6UXrZsGa1du5a2bt0qs23+LlKkCFWsWJGqV69OsWM77mrJ5o6DccHTp0/Tjh076ODBg3Tq1Cm6fft2MKbh/VMPFSoU/fzzz5QtWzYqUKAAlSpVCqKDl5oVAoPOhoPAoLMB0D0IgAAIgAAIgAAIgAAIgAAIgAAIgIDLCXiawKCd8Pv37+nQoUPEC9hXr16lBw8e0IsXL4RbpdChQwuXRwkSJKBUqVJR5syZKXfu3OTn56dtAscuIvD06VOaPXs2LVq0iM6fP2/UCy9Qs13YJRUfI3kHAd558uzZM7H7xHTELDLUrVuXatasaXoJ5x5MAAKDzsaBwKCzAdA9CIAACIAACIAACIAACIAACIAACICAywl4ssDg8smjA7sJsMAzdOhQGjNmDH39+lXUjxcvHpUpU4YKFSpEOXLkoLRp09rdLip4DoF3797RuXPn6PDhw7Rr1y7asGEDffv2TQyQxbxOnTpR06ZNPWfAGIlFAhAYLKJxzwUIDO7hjF5AAARAAARAAARAAARAAARAAARAAAT0IwCBQT/23tbzjBkzqHv37vT8+XMx9AoVKlCjRo2oXLly3jYVjNcOAq9fv6aFCxfSzJkzxW4irpo3b14aNmyYcKFkR1Mo6mYCEBjcDNy0O18SGLauuUu3rr02nSLOQQAEQAAEQCBYEyhfMynFTxTRYxgcvn2Kjt095zHjwUBAAARAAASCJ4HE0eJRhYwlgufkg+msITAEU8PbMW2Oi9GsWTNavny5qPXbb79Rjx49KE+ePHa0gqK+QICFhkGDBgmXZTyfwYMHi3vBF+bmi3OAwKCzVX1NYLh28aXORNE9CIAACIAACHgWgUr1knucwHBIERmQQAAEQAAEQEBPAsljJILAoKcBdOgbAoMO0L2oyzNnzgi/+xcvXqQoUaLQuHHjqH79+l40AwzVFQQ6d+5Mo0aNEk3Xrl1b7HBwRT9o0zECEBgc4+dwbV8UGLIVzkBxE8Z0mA0aAAEQAAEQAAFvJnB4+2l65h9AniowpImfnVLE/8WbEWPsIAACIAACXkjg6asHdPTq3wSBwQuN5+CQITA4CNCHq//zzz/EbpA48G/BggVpzpw5lCJFCh+eMaZmD4E1a9ZQgwYN6NWrV1S6dGlav349cfB1JM8hAIFBZ1v4qsCQIGkcncmiexAAARAAARDQl8A/f5/weIEhdcLs+kJC7yAAAiAAAsGOgH/AHQgMwc7qhglDYAimhg9k2qdOnaJixYoRB3WuWrWq6h4pkGq4HMwI8H1SpUoVunnzpgj0vWnTpmBGwLOnC4FBZ/tAYNDZAOgeBEAABEAABFxEAAKDi8CiWRAAARAAAa8mAIHBq83n0OAhMDiEzycr846FfPny0ZUrV6hatWq0bNkyn5wnJuUcAnyf8A6G27dvU8OGDWn27NnOaRitOEwAAoPDCB1rAAKDY/xQGwRAAARAAAQ8lQAEBk+1DMYFAiAAAiCgJwEIDHrS17dvCAz68vfE3itVqkRr166lIkWK0K5duzxxiBiThxE4duwYFShQgD5+/Ehjx46ldu3aedgIg+dwIDDobHcIDDobAN2DAAiAAAiAgIsIQGBwEVg0CwIgAAIg4NUEIDB4tfkcGjwEBofw+VzlCRMmUNu2bSlWrFh0/PhxSpw4sc/NERNyDYFFixZRnTp1ROMcHDxTpkyu6Qit2kwAAoPNqFxTEAKDa7iiVRAAARAAARDQmwAEBr0tgP5BAARAAAQ8kQAEBk+0invGBIHBPZy9oZeHDx+KIM4fPnygJUuWUI0aNbxh2BijBxFo1qwZzZgxg8qWLUsbN270oJEFz6FAYNDZ7hAYdDYAugcBEAABEAABFxGAwOAisGgWBEAABEDAqwlAYPBq8zk0eAgMDuHzqcotW7akKVOm0B9//EErVqzwqblhMu4hEBAQQClTpiSO47FmzRr6/fff3dMxejFLAAKDWSzuy4TA4D7W6AkEQAAEQAAE3EkAAoM7aaMvEAABEAABbyEAgcFbLOX8cUJgcD5Tb2zx1q1blDx5cjF0uLfxRgt6zpjHjBlDHTt2FDEZ9u3b5zkDC4YjgcCgs9EhMOhsAHQPAiAAAiAAAi4iAIHBRWDRLAiAAAiAgFcTgMDg1eZzaPAQGBzC5zOVe/bsSUOGDKHatWvTwoULfWZemIg+BOLGjUv+/v60Z88eKlSokD6DQK8EgUHnmwACg84GQPcgAAIgAAIg4CICEBhcBBbNggAIgAAIeDUBCAxebT6HBg+BwSF8PlM5UaJEdP/+fSwI+4xF9Z1Ijx49aOjQodSkSRMRk0Hf0QTf3iEw6Gx7CAw6GwDdgwAIgAAIgICLCEBgcBFYNAsCIAACIODVBCAweLX5HBo8BAaH8PlE5d27d1PRokUpXbp0dPHiRZ+YEyahL4Fz585RpkyZKEaMGPT06VN9BxOMe4fAoLPxITDobAB0DwIgAAIgAAIuIgCBwUVg0SwIgAAIgIBXE4DA4NXmc2jwEBgcwucTlbt3707Dhg2jTp060ciRI31iTpiE/gQyZMhAFy5cII7DUKBAAf0HFAxHAIFBZ6NDYNDZAOgeBEAABEAABFxEAAKDi8CiWRAAARAAAa8mAIHBq83n0OCdJTAEBATQunXrqH79+g6NB5XdT6BgwYK0f/9+2rx5M/3666/uHwB69EkCrVq1osmTJwvxqmvXrj45R0+fFAQGnS0EgUFnA6B7EAABEAABEHARAQgMLgKLZkEABEAABLyaAAQGrzafQ4N3VGBgYWH8+PE0btw4Gjt2LAQGh6yhT+VIkSLR27dvhSsbdmmDBALOIDB//nzx86BatWq0bNkyZzSJNuwkAIHBTmDOLg6BwdlE0R4IgAAIgAAIeAYBCAyeYQeMAgRAAARAwLMIQGDwLHu4czRBFRi0wgIfJ0mShG7fvu3OoaMvJxC4e/eusF28ePHo4cOHTmgRTYCAgcCxY8coZ86clCVLFjp58iSw6EAAAoMO0LVdQmDQ0sCxNxN48eI53X94n/59+oTixI5LiRMmpsiR/bx5Sl4x9m/fvtH7D+8pXNhwFCJECKMxf/3vK3369Elc++mnn4yu4QQEQMD1BCAwuJ4xegABEAABEPA+AhAYvM9mzhqxvQKDqbAgxzF37lzsXpAwvOj78OHDlCdPHrEQfOTIES8aOYbq6QT8/f0pbty4FCtWLHry5ImnD9cnxweBQWezQmDQ2QDo3mECZ8+fpkEj+tOxEz/+gVC1UnVq1awdJVLEBk9P//33H3379p8YZogQIcnagvzMedNo+uwpNk+pXq0G1Lp5e5vL21Lw6PHDNHB4Xzp/8Zwo3qF1Z7WPdRtW09hJo+ju/Tvi2rTxsylzxl8oT9Fs4jxxwiS0d+shW7qxqcyFS+epXtOaFsuGDh2aEsZPSEULl6BGdZtSmDBhLJZ1xQUWYf5TxBZOP/0UQhViOD95hgRql8f2naGYMWKp5zgAAUcJQGBwlKBt9d+/e0+l85e1qXCtBjWoaesmNpXVFrp84Qo1rN5IZKVNn5bmLJsljru17UH7du0TxyMmDaf8hfJpq+HYgwi0bdqeTh8/I0Y0ctIwypk3pweNLvChnDp2ito16ygK/pwxHU2dPznwSiYlLN3HJsWCfLp80QoaP3yCqF+rQU1q2eHPILcVnCuKv0n/+yYQhAgZwurfpN7KCQKDt1rO8XHbKjDw7oT+/fvTvHnzfugUuxd+QOI1GVu2bBFxF0qVKkV8jAQCziLw+fNnsc4QKlQo4mMk9xOAwOB+5kY9QmAwwoETLyNw4vRxqlKrvNVRR4wYibau20UJlAVmT07d+nSk5auXiiGOHTaRKparbHG44yaPpvFTRlu8bnqhSf3m1KNzH9PsIJ+/fPWSfsmTzqh++1adqE2LDnT95jUqUa6Q0TUWGDKmz0T5iucQ+c4WGE6dOUGVapYz6tPSSYafM9LSuasoUqTIloo4PZ8Fl/bdWot2q1WuQcMGGGxnKjAc2XOKYseK4/T+0WDwJQCBwT22f/P6DWVLbdticeOWjahzL8MirT2jO3vqHP1RppqokjJ1Ctq0d4M4blanBe3ZsVccT5k7iYqVLmpPsyjrRgI1ytemk8cMW+ZnLp5OBYsWcGPvjnd1cN8halDNIHKlz/gzrdm2yu5GLd3HdjdkocKcaXNpeP+R4mr9pvWoe38EWbSAymp2z469adWS1aLMSEW4LF+5nNXy3ngRAoM3Ws05Yw5MYLAmLMgRYPeCJOF93xs2bKDy5ctTuXLlaP369d43AYzYowmwRwf+b3z+h+R+AhAY3M/cqEcIDEY4cOJlBPiN+Mf+j8SoWUgoWaw0JUmUhM5dOEs792xXZ5MjWy5asWCteu6JB1qBYczQCfR7+SoWhzlz7lQaP3Ws0fW3b9+o58xCm5o2aC4W/7V5jhwfOXaIqtf/LoCwgFG8SEnKmT03LVu1mLr37Sya53HUqFKLqv9Ri6L4RaEGzWuL/Hhx49OMiXMdGYJRXVOBgQUMmd6+e0vPnj+Vp+K7WaM/qVuHXkZ5rjzRCgy8q2b4wDFqdyXLF6YvX7+I89WLN1C0qNHUazgAAUcJQGBwlKBt9U0FhoiRIlqs2PjPhvRn+xYWr1u6YGlhFgKDJWKelw+BgcjSfewsa0FgcA5JrcAwYuIwqlDF+ss8zunVva1AYHAvb0/qzZLAYIuwwPPA7gVPsqb9Y4HAYD8z1LCdAAQG21m5oiQEBldQtaNNCAx2wEJRjyLw8NED9Y14HtiWtTspTervb9Wv37yO2nb+vjX+3JEr4q31r8pi7rt378RcQocOQ+HChRMxBM6eOy1cKcWP991lzYcPH+jSlQuibNo0P1P4cOHFsfaD1Wn/J/505+4tUrRqSp82g9nYD69fv1IWkg1ucqJGiSq2m9+5d5ueP39GWTJnI3sEBm3/fPzo8UPKWyy7ms07NlKnSque88GLgBfiPGTIkOSnxKbg7e/nL55VFv6jUpLEScU1W+eyZcdmatG2saiTP29BWjhzmTjmjwlTxwj3SHxcr1ZD6tdjEB+KxDw58S9ec26KAl4G0PUbV4lFARYhUiRLQSFDhhJ1rH2YCgy3LhgH7GLGzdo0pOOnjolmzO2g4HgR9+/fEzsw2FYpkqdU+k9JESIYLxR++fKZXr8xiDlhFNdLWjGH403w2DmFDRuWIoSPII6tCQw8Z/mGgxQXeEvlByWuBafw4cNTqFCh6ZUyJp4nl0mVMo3Ze5HLs9B06eolZVvmJ0qbKh1FixadZB/sdovvPaTgQwACg3tsbSowrN2+mn7O8P33kblRvHr1mv77/++EyJEjU8hQIdVir16+Ej+jOcMvip/4mWlpYdZUYChaqgi9DHiptmXa9sePH4ldOnHi3weR/SKrZU0PXitj5MTlIkSMQF+V8V48d0n5+R2aUqVNpbp74/yb127S82fPKXW61BQtunmh9P37D3Tv9l16+OARJUmWWPmXRG1D9i1+/inlOImff6FDEbM6c+IMRY0WlVKmSankh5PFjb553ndu3RXzT6OMI3bc2EbXtSdfPn+h61dv0BP/J5Q8ZTJKmDghvX3zVt3SbsqN6z598lTU4Z/16RQ3VbHixNI2aXT89ctXpex1+vffp8Rv+zMTewQGttGXLwbxOVLkSIa/GW7eoWfPnlGGzBmMfoc+vP+QbivXmGmCRN//hjEakHLy5PETuqHY6dHDR2K+KVKloBgxo5sWU8+fPX1OVy5eoajRo1KatKnpyMGjVncwMJfbN27THcXGPI5Uiq3YRaE2WbqPtWXkMf9ulPeyvFc57+L5S8T243vN9F4ITGCw5R6U/fM398/3ybu37yhu/LiUPEUyo2dVW9aW+8PZdrWFOT+fPH5OoZVnl//u5efstPJMcUqRKjlFj2F8H0BgEGjw4aMETAUGW4UFiaNWrVqUPn168buR/5uGfz6Z+zaXJ8tqr3nCsZxbcPiGwBAcrKzfHPl55r9V5H/f6zeS4NkzBAad7Q6BQWcDoPsgEzB1j3R49ykluHMctT0WEjp2b6su6ndu103Z3ZCUtu/aqvi/biDK8S4BXiSWrok4M60iUixRXOisWrechowcoLYXI3pMmjJ2hnhLX2ZeVhZy23RqQdeURXFt4jaqK2/u8wK7TOX+KKXGK1g6bxX1H9KbuH7+PAXpwCGD/2xZVn7v2XJQjFmeW/oOTGDgxenMuQ2CAy+u9+7WXxE0Ook3+1kA4HHaOhftrhHteOpUr08Ll83TZqnHIwePpXy5C6giCC/Knz/6nRkLAN36dhK2USspB3HjxKMenfpQuTIVtNk/HAcmMHCFtetXUYfubdS6WhGC7deyfdMf7MiF+/caTDw3GRNj49/rqXWn5qKd3DnyEttSpgVL51LfQT3FaZWK1YjnbYkX71JhN1jaGAzSRdKkaeNo9MQRop2ObboKYWHX3h2yGyFqcH3esSMTC0ZjJ48irisTc54/Y4mRGzHtvGU5fPsuAQgM7rFtUASG4rlL0b0798QAF61dQDlyfxeJ08T7WR34toNbxMKxpYVZU4Ehb8E89EuKbGr9oeMGU6Vqv6vnY4eNo2njZ4jzoiWLWPSj/+L5C8qdPp8ox4vQrTq2pFGDx4hFeM7kvMlzJtJnZRG8ed0/1Xy+1mdIL2L/9zLxYvC0CdNp4qjJMkt8806P3PlzUa+BPSh+wvgib8q4aaoP/Xbd2ioxC06rLqC4ANfhN6qLly4myvMHj7V3p760/e/vPyc5P268uMSCS7d+XYToy3mcdmzZSS0btDac/P+Tx8yupvbt2i9ylm9cSr9kyyyOeaG9b5d+P7SfNHlSmjJvklig1TbG7XRo0cmISZ1GtcSC7rnT50XRwFwklS1UTixsc2G24fQJMxUR4baoyx+9B/ekkmVKiAV/XgCXqXKNSsq1XkYL7yxY9Wjf64fxc50qNStTL+X3lnahnu9nju2h5cn2btiiIY0cOEp0ZeoiacuGrdRD2RnIIo02NWvTlNp1baMKSZbuY20deez/2J8KZikiTtP8nIZq1K0m7t3Hjx7LItRV2THJrpD4P+g5WRIY7LkHuR0Wy3opf3/s3LKLT9XE91QX5e+nshXLqHn23B/OtKutzI8dPk61f68rxluqbEkh/jAnbeLnu3WnluJnEv9sMpe2H9pKiZMmMnfJK/Owg8ErzeaUQWsFBgqfkipWrEh79xpcDTqlAy9rJLgthEJg8LIb1MuGC4FBX4NBYNCXP0Fg0NkA6D7IBEzjAPCifu3q9ShPznyULGlydUHYtINtO7eIt9lN87XnvLAtXS9p83nB9qjiJ5/fan+nvKmePkcq7eUfjsePnELly1QU+VqBgduRLo3y5SlA/xwyLGiYNrDnb0Vg+P/uAtNr2nN7BAZtPT5mgeGP36vZPJfsBTL94HKI26lVrQ4tXr6QD39I7BIovzJPGYNBKzC8V97UL1omv1nesqEFM5dSgbyF5OkP37YIDNpF+4L5CouFd26IhZVffy/2Q5vajKYN/6TuHXuJLHsFBku8WHyoXKGqWYFh4rSxNGaiwYe0dhymx//sOEZyx83wMYNp2mzjxTvT8nwOgcEcFd/Ng8DgHtt6ksDAMRg6texCG9ZsFJPnBcUJs74Lj+WL/S7eSueLoyaPoHKVfjMLiRdY82TIb/aazOTFftMFZXlt2frFlCVHFnE6Zazi1m/ERHnph+9ESRLR9kNbxO/twMrKyruP76T4CeKJ0zqV69NR5e16S6la7ao0YGQ/cXn39j1CELFUVuZLgYHfeC+drwxpF7VlGf5mBgtWzRW7CvicxQUWfQJL9ggMltriRX9e3DZNnXp2oCatDLsMefzlilRQxSzTsnye8ZcMtHzDUvFmPovVVctWJymEmCvPeVqBgQMr9+ncz1JR4V6HRSFOQRUYLDauXGirCBh/tjMI/5YEhsDuK+09GJjNeSyzl80UAdUDK2t6f2gFBktzssWu9jA/eugY1alUz1J3av6kORMotbJTpWTe7y8vqBeVAyl2avO8+RgCgzdbz7GxawWGqPEM4v6ePXuoX79+NgkN7Ls/Q4YMYlcf/8zkXULmvs3lWSor84NSx5G6TBICg2P3E2qDgJYABAYtDfcfQ2BwP3OjHiEwGOHAiZcRYJ/+e/Ybv13GU+AF7MIFiir/ilCxwiWN/NqbCgz8Rj+/Sb72ymZWAABAAElEQVR4+QKjhXMWGf5s0pp27N5G+/7Zo5KZMm4m/VqiLG3euoFadmgm8lnQaNeyE8WMEZMmTh1Hh48dFPnyLXY+0QoM4qLywfVKFi2tjLEETZo+Xu2neaOWVLRQccqU4Rejty5lPdPvoAoMHPC4ReNWojlb53L+4jmx44IXtDnxGPv1GEjRo8Wg5y+e0cq1y2npykXiGnOtW6O+2IXx4eMHswLDlJkTaeS4oaI8f3Rs3YVCKbtKZs2brtpDKwioBTUHpgKDNsbB+/fv6Mq1y+qYuNqAXkOojjIuTvWa1lS5s817du5LUaJEEbtaNm35Hvhr58b9lFxx2WSvwHDm3CnauuNvmjprkuiP59KqWVthe94VY24Hg6nAUKr4r1Sh7O9ih8XYSYa3R7kxKWCxi67cRQwLeZzPdmX2d+/doQVLjN9ShMDAhIJPgsDgHlubCgzcKy8smqbfq1YUb55zvqt2MLDAsGvbbmpRr6Xa/dnbp8TvEnaRUyBLYTX/xNWjxO53zCVTgYHnU7N+DTp84PAPi88de7Qnf6XtRXMWq03xG/v8ZjynUsoCvXz7nt82L/5rMeGqh3cFyCTfjjZdCC7xa3EhgrC7oQkjDT9Huc7oKSPpt9/LKv1+f8ud83sO6E7pM6enzX/9rY6HF2wPnjsgFoAqlqisCiyczzst2BXfjEkzjcQSKTBox8PlG/3ZSOzemDNtntpO4eKFaPrCqWbbb9i8gRjj6mVrjdq3V2DgXSjv37+nv9dv4WmqqWqtP8Qb6drx886Krf9sFmVmTpoldp7ICj0GdBOLyCeOnjDaUSJ3uvCuhVYNv+/2+7V8acqcNRPx2/LSpQ63JQWGgBcBVDRnCXVubK/yVcrRyaOnaO70ebJbWrl5OWXKktFhgYHvH7bD4rlLjUSf/af2CJdYlgQGe+7B6RNm0Jih30U5FjBChwol5iMFnfyF89PspTOUna3fxbPA7g+GYSowBMWu9jI3FRh4N0jD5vXF/TpI+XtIioQsxPUc1F0831PHTacDew4I+7FYxfc4C1Hs/tFXEgQGX7Gk/fMwJzDIVtatW0ft2rWjO3fuyKwfvpMmTUq3bt36Id/bMuTubAgM3mY5jNeTCUBg0Nc6EBj05Y8dDDrzR/eOEeBdDE1a1adjJ45YbWjssIliwZULaQUGFiLOHLooFhdu37lFRcoY3EFwubnTFgmRgo+14sDgvsOpZtU6woXS4aOH+DJVrviH2DnBb530Uv7jXS6w582VnxbPWSHKaNtgUWPZ/NUizoC4qHy4MgaD1kUS91eiaCllYWGa+h+K7A7Knrmwu55Gfxq227MQMnvKAjkNsZA+YuwQcd6yaRvq1LabONbGzNDuYChUKg/dvW/4I553CfBuAU7HTx6lP+oYdn/w+fWzd4Wd+Ng0mQoMpte15+1adhQL/LygZGpzthXbjBP/sZ2jYGZV5GDRp2uHnnYLDNyWpRgM3EdgAgOLUNv+2i3iMHBbNepXUQUsZsuMta6ZWLTYuWm/CKrN5QcN70ezF8zgQ5EgMEgSweMbAoN77GxOYDDXMy8m8kIuJ1cKDBxngXcfyIXDuctnE7tOWr96A3Vu1VX0zwvH46aPEcfmPkwFhvU71xIvTPLvufyZC6pvzjdt3YRYYODELpR4QZuTbJ/d8wzpY3h7PVLkiNRTcYfEixocK0H7pvSSvxZRtpxZjRZseaF80571iugcSrSp3anQvls7at62KZ05eZaWzl8mrvNCeI161cWxdq6ccfH+Obp+5TrxDg6Z5KI3n7MrnD8bGAR3PpcCg9ZOQ8YOpsrVDfUvX7hCFYp/b4sXuJ8+fUa/KwKGTLINPmfXS01qGV5K4HN7BAaty6kurbvRX6sM4jcv+K762/A3hrZ9FoNOXjvG3VDejPlVW7ELHHaFI1P3dj1pzfK14lQKBsxAugUqWLSAGCcX4JgSNSvWVkUGWZ6FnPbNO4o2eIF9/6m9aowCFiqkmyUpODmyg0G744bdi2ld+UyYOY5K/VbSrIske+9Brc079+5EHJid04mjJ6lmhdrimD8u3jtHpfKXUXeHBHZ/cEwQrcAQVLvay9xUYNh9bMd3l2QagYTdlc1faXgpATEYVDPjwAcJWBMY5HTnzZsndjRYEhrmzp1L9evXl8W98hsCQzlav/77y2ReaUQM2uMIQGDQ1yQQGPTlD4FBZ/7o3jkETp89SXsP7Fb+7RH+6s21+tfyzeJte63AkD1LDlq56C9RnGM2pMyUWK2qjenAb/fzjgVO/XsOpro1G4jjd8rb8by7YZ/S7/Wb1+ji5Quq6yMuYElg0C6ki4aUD3cKDGuWbBCBpWXf/G3PXJwlMHDA5FSZk6jD2LhqG6VPl0E9l0GhOYODElpK9ggMtarVpb7dB4rAk9p5cNuXTtw06qe74tt52SrDG7lSSLF3BwO364jAUK1yDRo2YDQ3IxILWLzbhlP7Vp2oTYsORiKCaXl2v1W7cTVRnj8gMKgogsUBBAb3mNlUYGA/7byYbprYZz6/Dc1Ju4jpzBgMvIOBU7+u/WnpguXimN/67t6/q5HrpClzJ5EsKwqZfJgKDGdunVR/PmoX+sfPGEuly5UStZfMW0r9lZ+vnPhNdna5IhMLCoeU3Q+H9h8iXpyXOxrkdXMCA8cHGDza0B6X086pTedWyi5CgyDN7iEuKYF/D+47ROxv/sLZC+qiumyfBQZ2jyRjL7CN9p78vgPS1IYsDmTIlJ7SJ84kmxAxGSIpAbllkm948zm7zPmguCOS7fMi/7HLh0XwT1m+UNai6lv39ggM81bMoTwFcotmtDEqeEdJ36G9RT4HY9aKJ1ceXSReWM+R1lCPC0nGooLywW602J2WTMyofNGKauwH7QI4l9GKNlJg4HgeHNeDE885S/Ys4pg/WNCRrqWkGOKIwHD+7hmjoNHaxXrpFsrSDgYejy33YOYsmYxsbhqwXft3SSjlRQV77o/8hfIZCQxBtau9zLUCg+l9v2ndZhEvhPlkz5WNFq9byIcEgUFgwIePErBFYJBTtyQ0+MIuBggMEBjkfY5v5xGAwOA8lkFpCQJDUKg5sQ5cJDkRJpryCALs0//QkX9ow99/iYVdOSj5BrpWYDAN0pssvSHIJNc5eeA8RYsWXVQ3EhiUoL91azSgFy+eU+Xa5enW7Zuyix++LQkM/MY/L1hrkzsFhtOHLqlvuPMY7J2LdmFeLrzLubArIFt3MDx4eJ/yl8gpq9Lx/WeJ38C3N5kKDCygyMS7N3i8WldBS+auFDtOlqxYSD2VRTdO7FZow8qtspr4XrRsPvUe2F0cp0qRmrat3+P2HQxt/+youN8yvB3KA+mrBPaUc5ECQ6uOzUm6c5J5YtDKR8DLAMqS92d5CoFBJRE8DiAwuMfOpovTpouS5kbhaoGBYxKwEMCJFxV3Hd1OOdLlFrsaeCH44Ln9qmAgCpl8mAoM/LZ2yFAhRalGNZqq7lO0QoUlgcHUTY9JV+JULn5rXc7IwLOy/EBF4JdumKTA8PnzZ2rbtL361r0sa/rNi+crF6+ivorwwkm7mCrLauNTsMAQN35cKpS1iLxs9Xvy3In06MFjGqT8jcDJXPs1ytemk8dOiuv2CAxaAUrLUgpH3KA5geGassD/W+Hyoj/+OHblCPn5fRdIrl66SuUUQUGmQ+cPGMXd4NgSufLlkpcVcegw1a/aUJxLgUEr+qgFzRxIt01BFRhMF8a5C22sEXbjwyKDJYFBy83M8EQW34PxFJsXyfH97zN2rcU7M8ylx4/87bo/ODC5VhQJql3tZa4VGLLmyEpL1y9Sp8Our/j54aS9ZyEwqIhw4IME7BEY5PTNCQ1r164VAaJlGW/7hsAAgcHb7llvGC8EBn2tBIFBX/7YwaAzf3QfdAJ/b99Ei5bOFw2kTZOOev9/0UDb4p/tmhCX48RugWZMnGvkIskRgaFxy3q0c8920Ta7PGpUvyll+yUH3X9wT3Hb0EjkWxIYls9fQzmzf3+rkAu7U2AwdTdk71ycJTC8efOaMuZKI1jxx1bFFVDqlIZz3lFy5+4dcY1/USdNkkwtZ3pgKjCYvqXProgKl86rumLq020ANajTWAgP0tUTt3nxxA0KHy682nzX3h1oxRqD6w2OkzFr8nyj2BvaHTBcSRtomQM4jxpieKvTkR0MtggMg0f0p1nzp4txc+wFdgkm041b16n4bwXlKQQGlUTwOIDA4B47OyowSBdGPNrXr15T9jTfF3VlYFVLC7McUJgDC3PSLvbzW/0Ffimkvsk/ZMwg6tGhlyhnujNAZJp8OEtguKjsLNC6DeK+eXcD++OvX7WRGscgqALDwtmL1UV9ngK7AcqdP7eIS1A4m2E3B+ezwLB3xz5qUd/gIkjrRoivm7p+YoHh54zpKGOSX/iySCMnDVddC8o8+Z0l+y90+J8jqgsqbv+4sqDPv784sRCSIXFmWdwuF0lBXYh+pdxLOTT30sLV8yhn3u+iPrtaYpdLMl16cJ6KZC+u7jroP7wvVa/7fQecdvFeCgxaQYj99LMbMHMpfITwxC6XLN3H5uqYxte4/PCCcK8ly2rjKsjdFtoxSgHGnnswTbrUlC31d0YbFRddqdKkFF2ym6i7d+6K4xA/haB4CePZdX+YukgKql3tZQ6BQd4xxt+IwWDMIzidBUVgkHy0QkPhwoVp9+7d8pLXfUNgCH4Cw7Vtm+jlYSWuYJESFL1AMa+7Z71hwBAY9LUSBAZ9+UNg0Jk/ug86AQ7uzEGeZdq79RDxQr9MvKjcrE1D2r7L8FZ643rNqGeXvk4RGOpUr2/kO3/RrOWUL08B0fW02ZPFQjOfBFVgGD1kPFWq8IecSqDf9gZ51goMpnEAbJmLswQGnliGnKlVt1JD+o2gGn8YbKrtg+26Z8tBo4UFLZTABAYuqw0I3qhuU+rVtZ8iYNymwr/mVZtaMHMpFchbSJx//e8r5SqURY3BwMGwu7TvIYKKy/uOY0mcPXxZLCCxIFKpZnk6e/60qG9JYNAG/jZlf2TPKYodKw5pgzzbIjCwyyR2nSSTDEjNftg5n2NsyGQqvsh8fPsmAQgM7rFrUASGKr9WVYMlaxdyWSxg0UCmoAoMXH/koNGKMDpbNqV+awUNNdPkwFGBoVTZkjRh1jgRH6GfIupy0r49/enTJ6PF2aAKDO2adVADH//ZvgW17dJa9HXq2CmqXr6WOOYPFhhuXrtJvxWpoOaNmTqKylYsI2LurFi8kvp07qdek/ETtDtNtNxYMFi/eiN9U2JScCpZtgTdvH6Lqv1WQ23jrx1rKW16g2jOrptq/15XveaOHQzcmTYGw5/tmqsuuviauXgO2t0ppm6utK6xpMCgjQfA9l28boEqqpw7fV4VkBImTqgIP7kcEhjYDRW7GeL08P5Do50GU+dPpqIli5jdwcAxOuy5B7OmyqHGLxkwsh9x8GNO2mczUZJEtP3QFiqRp7QagyGw+8Mvip9TdjDYy9xRgWH4hKFU8Y/vz42A4QMfEBh8wIhBnIIjAgN3GRAQQOPGjRP/OCg0Cw3emCAwBA+B4YOym/3CupV0dv0aevP8KSXxi0RJ/SJSyAgRKVbpChRVERrCxIzjjbewR44ZAoO+ZoHAoC9/CAw680f3QSfAbn0KlFLcPbx9IxrhxV5+Kz1hvIT06vVLZSF4Nx08ckDtQLolctRF0oBeQ6h29XpGAgO/Ec9vjh84uI+69+usjkm7Q0Ib5DmwHQxlS5dX3DDVp2xZslsMbKxOTDlwpsBgy1y0i/+OuEjiOWhd/vA58w0VKhTNnDdNdT/FC/u8wG8p2SIw8K6SrTv+Fk3UUdhyP5x4BwPPhxO7Z+JAztEV11gcqFvuUOFre/4+SEkSJ6Wr1y5TqYpFOUukCmV/p7y5C9CO3VtVMYsvWBIYOGjzwF5DKb3ikimKXxSj+yioAsPz588oW4GMhgEpn/wsFMpfRMQlkc+HvAiBQZIIHt8QGNxj56AIDNqFcX7bvXOvjsTtTB0/XV3c5NE7IjBwLIJKpYzFau7r2CUlNsD/3R1ZIuSowMBxGTg+A7s0YtdGnHhRdoni452DIc+ZOlfEAJD9y7e5tW9n2+IiSRtIuFyl34jfZD9/5gIN6z9CXdzmPtjFU4iQIZQgvXVUN0Wcz2/d3797X407wHmcpMAwcuAomjVljshjVznturWl2HFiEwsSMhgyu//ZvHcDsTBdpmA5dcGZ5/tn++Zi98L44RPU3STcmCsFBu3uDO0b/dwvj5/FgSOKCy2t+MS7M8pXLieCPnPwZ5k4OHP2XNlp7859akBoviYFBtNdBhWqlKfffi+riC03acLISeq9LMUcR3Yw8LwaNKsv3BjNVIQzGceD8znINn9r5yt3MNh7D2pdcfFcOc4FPy9zps5T++TA5hzg3J77g9twxEWStKu9zB0VGDhge60GNSmrEl8jsJ8bzMtbEgQGb7GU88fpqMAgR8RCw+nTpyEwSCBe8r1hwwYqX748lSvn2wKD//kzdHr5Irp6YI+RZaTAoM2Mki03xShVgSKl+/7fk9rrOLadAAQG21m5oiQEBldQtaNNxGCwAxaKehwBXgBm9z6BJX4rnt+O5+SowDCw91AhMDRt3cBoQdncGKTffr4WmMAwd+EsGjCsj1EzclHbKNPMiSMCAzdn71ycKTA89n9MeYpmNTMrQxYv+m9avZ3ixLb8ZoUtAoPWBZVWFLl6/QqVqlDEYv98oVXzdtSxdRdRhncdlK9ams5fPGe1jlZg4LJsf20aOXisECGSZ0igZgdVYOAGtPEk1AbNHEBgMAPFh7MgMLjHuEERGDjgcPO6hiDF1kbpiMDA7WrfwOfzek3qUg/NjifOM5ecJTBcOHeRKpWsYq4LozyOYcA+6u0VGExdJBk1qjnh+ALRY0QXu0Z490hgSQoM75XAzeWUXQ/37tyzWIWDWfPb/px2bNmpBnq2WEG54C6BgYMSVyhWSV0YNzcm3nmwaO18EZCa3QD9UaYasd2sJSkwcJlVS1aLoMCWyqf5OQ2t3bZKtO+IwGCpfRYAOOA1J3MCg733oOkCvmm/LDSt276G2OWRvfeHMwQGHo89zIMiMCyYuZAG9xlqNHX5s8go04tPIDB4sfEcHLqzBAYHh6F7dexg8E2B4dzqZXT2r5X0/N5ds/eYOYFBFuSdDDGVXQ3RlF0NvMMByX4CEBjsZ+bMGhAYnEkzCG1BYAgCNFTxKAInTh+nMRNGGO1WkAPkxelObbtSlYpVlbfiQ4vsHbu3UZNW9cWx1oURZ2iDPJ86eJGiRokqymmD6EqBgRfG2QWTdInDBTlQcK8u/ah6/cqiHn8smr2C8uXObyQwrFy4jrJnzamW4QP/J/5ioV/bnq0Cg+kiPQcjZnFDptevX1Gm3GnlKWldJHGmvXPRuqfSLtZzW9NnT6Fhiq9vTtqF+cf+jxQhIZvI5zfszx+9Ko754+atG9SyQ1O6fPWSmscHmTL8QtMnzKG4ceIa5ZuenD57kn6v8ZuabW4RXTsuLrhj4z5KkSylqMOuktp0/tPIlrIxFgLYrZE2PX32r2KrhsTChkwco6FIwWKqqyJTV0jtu7aivzatlcXJnMBwbN8ZihkjFk2aPp5GTxguypoGbe6nvJ07f7HhbVoWPZixTHsP7KbJMybQsRNHxC6G30qXo3K/VqTajb+P3xwbWR/fvkcAAoN7bPr2zVtityoy2RLkmcsunruERg8Zq77lzW8o82Lp/BkL1AXe7Ye2UuKkiZS38s9T5dKGhXFesF2/0/DzpEW9lrRr227RtXQTI8fB31PGTSN+e14muXAuzy19B7wIoFw/f3chpw3y3KRWM9q3a7+oqu1z2YLlahBluYOBC5kTAXh3AsebmD9zgWiHF7k5+Czv4Bg3bLzIk4GcxYnywQGUuS1Obbu2IXb5wwvi3dv3JI4nIBMvAPcf0Y8mjpqs7mLQ7oa4c+uuePP84P5Dgn2x0kWpcrVKNH7kRLX8ys3LRZwIbvPpk6c0Ztg4Wr10jexCfHM/oyaPpLwF8xjlb9u8nVo3amuU16xNU3r88LE6zllLZlCBIvmNymhPtAGnpfsovs67KfiteU4Nmzegrn07i+Nrl6+p7p/km+7igvLB9ye/lb92xTqZpX7zW/4de7anMGHCqHmvXr4SsSRkbA++wEGW+w3vo4piGX/JQKv+XqHW2bppGw3pPUyN3yAvcMyNHv27id0FnGfpPpbltd/ahX5mzYGcOVjzs6fP1WKDRw8k7kOmeTPm09C+ht+fWj723IPc1q0bt5Tgxx3U+0G2z/NmMSxO3O8vPdhzfzjTrrYyP37kBNWqWEdMQRvImTO4jTaNDX9HcHwOjtPB6cnjJ/Rng1aqGzfOg8DAFJB8gQAEBoMVITD4nsCwslkdenz1stXH1JrAICuyuJB++nJ5im87CEBgsAOWC4pCYHABVHuahMBgDy2U9WQCvOj78NEDevj4IUWOGJkSJkhE8eMloNChDcKCK8bO7hCuXb9K/z59QmlSpRX+8x3thwMfPw94TmFCh6VYMWPa5CLJ0T65vivmYs+4eGcA78TgoMSc0qZOp8w/tj1NOFyW76EbN6/Ta8UGyZOmUFwiJbHKn211594dIYCwmBVY4pgI3Af/QR9NccOkDSgdWF1r1zn+w0fFnzknbpsXin5S/sd/4LBgVaFaGXEtbpx4dGjXd1FEZOLDpwlAYPB883Jw4Xt37itBhr+KwMTahV5njP7Q/sNKMOWGoinpN14uKjijfVvb4B0R169cp3Dhw9PPGdJRqNChbK1qUzneYXDn9l1KkDA+scsic3Pk3zMflB0JMoUNF1aJofBNuH0xDcK888g24rgB2vQy4KWI4/BacWWVLEVSYS/+OWsucYyJq5eu0rt374U7IV701zux0MBxIh49fCTmlixFMgofPpzFYfEi89Ur1yhRooSUOFlis0y1lWUQZBZwokWPRilSJadIkSNpi9h1bCowHDx3QATjvqQEDmdXiilSpbDrPrL3HhR/lyiiELt74pQ2XRqKGdvy73p77g+7QFgp7Gzmpl3x7qwXz19QmLBhKCb/TRqIazXT+p58jh0Mnmwd144NAoOBr/w9yT/rglPyZRdJzhIY+H7ItHBjcLotnDZXCAxOQxmkhiAwBAmb8ypBYHAeS7QEAiAAAnoQ4EDm7OZKptlT5lOeXPnp2bOnxDsnjp86Ji7xLotZk+fLYvgOBgQgMAQDI1uYIr+F7v/In4b2G07/7D0oSnXp05katfj+s8JCVZ/NNnX7xAGh2bUOL7JMGj2ZFsxaJObOYsDhC/8YvdXvs1A8eGLmBAYPHi6G5mUEIDB4mcGcOFwIDAaYEBiwg8HaYwWBwRody9cgMFhm444rEBjcQdlKHxAYrMDBJRAAARDwAgLsAitPsexqcHFLQ148ZwWxWzCk4EMAAkPwsbXpTLXuhOS1f87up5ixYsjTYPndqEZTOrDngNW5cyDkFm2bWS2Di64nAIHB9YyDcw8QGIKv9SEwGGwPgQECg7WfAhAYrNGxfA0Cg2U27rgCgcEdlK30AYHBChxcAgEQAAEvIXD12mXq2KOt2eDT7L5p1uR59Esmy8G0vWSaGKadBCAw2AnMh4qbCgx9lPgttRrU9KEZBm0qHPehZ4fewve8uRaGjB1Mlav/bu4S8txMAAKDm4EHs+4gMAQzg2umC4HBAAMCAwQGzWPxwyEEhh+Q2JQBgcEmTC4rBIHBZWhtaxgCg22cUAoEQAAEvIHAFSVQ9t37d+lFwAuKFzceJU2cjOLHT0AhQ4T0huFjjE4mAIHByUC9qLkL5y6KeAGR/SILX/gcfwHpO4FHim99Doz8rxLAOWrUKJQkeRIRTNvZMTC+94gjewl8+fyFTp04LapxPK1fsmW2twmUBwGLBCAwWETj8xcgMBhMDIHBcwUG//Nn6PTyRRTvp68UPWFiCpckufgXKW0Gq8+nO2Mw3D18gP69comeKvEon965SfmzZqEYpSpQpHQZrY7R1y9CYNDXwhAY9OVPEBh0NgC6BwEQAAEQAAEXEYDA4CKwaBYEQAAEQMCrCUBg8GrzOTR4CAwGfBAYPE9gOLd6GV3evpkeKwv3nDLHikZRw4YxGOz/n+EVsSF8YoPgwMKDVnRwlcBgKiY8v3fXaEx8UihhHJEXJmYcil2pJkUvUOyHMsEhAwKDvlaGwKAvfwgMOvNH9yAAAiAAAiDgKgIQGFxFFu2CAAiAAAh4MwEIDN5sPcfGDoHBwA8Cg+cJDBOL5DC6uc0JDEYF/n/CokOYmLHp4OEj9NT/sbkial4Sv0iU1C+iem7p4FHSdGJngjkxwVwdKTDIa8HVxRIEBnkH6PMNgUEf7mqv2MGgosABCIAACIAACPgUAQgMPmVOTAYEQAAEQMBJBCAwOAmkFzYDgcFgNAgMviMwyMfwzL8vKODjJ3lq9ttWgWHvfX+z9S1lQmAwkIHAYOkOcU8+BAb3cLbYCwQGi2hwAQRAAARAAAS8mgAEBq82HwYPAiAAAiDgIgIQGFwE1guahcBgMBIEhuAhMHz59o1I+f+Hr1/p09f/6PXnzxQrfDiKHSGc8h1WHIcP9WOsPggMQfthBoEhaNycVQsCg7NIBrEdCAxBBIdqIAACIAACIODhBCAweLiBMDwQAAEQAAFdCEBg0AW7R3QKgcFgBggMvicwnHrynF58/EwfFTHh3Zcv9PbzV+Xf50CfO78wYShWhLAURxEeWHzg44MP/w20nrYAdjAYaEBg0N4V7j+GwOB+5kY9QmAwwoETEAABEAABEPAZAhAYfMaUmAgIgAAIgIATCUBgcCJML2sKAoPBYBAYfE9g+PDlq3CRFKCIDOwqSfz7ZDj+xjsZzKQIoUNRRGUHQ8ifQlCE0CEplPJNP5HyrXzYkSAwGGBBYLDjpnFBUQgMLoBqT5MQGOyhhbIgAAIgAAIg4D0EIDB4j60wUhAAARAAAfcRgMDgPtae1hMEBoNFIDD4nsBg7Vl7+X/R4f6bd/RG2dUQOgS7RfqmCAv2CQmW+oDAYCADgcHSHeKefAgM7uFssRcIDBbR4AIIgAAIgAAIeDUBCAxebT4MHgRAAARAwEUEIDC4CKwXNAuBwWAkCAzBS2CQj+btV2/pzqs38tRp3xAYDCghMDjtlgpSQxAYgoTNeZUgMDiPJVoCARAAARAAAU8iAIHBk6yBsYAACIAACHgKAQgMnmIJ948DAoOBOQQGCAzOfPogMBhoQmBw5l1lf1sQGOxn5tQaEBicihONgQAIgAAIgIDHEIDA4DGmwEBAAARAAAQ8iAAEBg8yhpuHAoHBABwCg+8LDG8+f6Ev//2nBH3+jzg+Ax+HDBGCQoX4STn+Rl+Vcy7DieM1OJIgMBjoQWBw5C5yvC4EBscZOtQCBAaH8KEyCIAACIAACHgsAQgMHmsaDAwEQAAEQEBHAhAYdISvc9cQGAwGgMDgewLDjYDXilDwWcRYsPSYJfGLREn9Ilq6LAQHFiLO/PvCYhlzFyAwGKhAYDB3d7gvDwKD+1ib7ckXBYYEyeKYnSsyQQAEQAAEQCA4Efjw7iM98w+gSvWSU/xElv9jwt1MDt8+RYeUf2niZ6fUCbO7u3v0BwIgAAIgEMwJQGAIvjcABAaD7SEw+J7AwKJAYDsRAhMY5E+Gvff95aFN3xAYDJggMNh0u7isEAQGl6G1rWFfFBhsmzlKgQAIgAAIgEDwIOCpAoNfhBjBwwCYJQiAAAiAgMcRePXuGSWPkYgqZCzhcWPDgFxHAAKDgS0EBggM1p4yCAzW6Fi+BoHBMht3XIHA4A7KVvrwJYHh2sWXVmaKSyAAAiAAAiAQPAlEjBzKI3cwBE9rYNYgAAIgAAKeQgACg6dYwn3jgMBgYA2BAQKDtacOAoM1OpavQWCwzMYdVyAwuIOylT58SWCwMk1cAgEQAAEQAAEQ8BACT98+95CRYBggAAIgAALBnUDMiNGDO4JgNX8IDAZzQ2CAwGDtwYfAYI2O5WsQGCyzcccVCAzuoGylDwgMVuDgEgiAAAiAAAiAAAiAAAiAAAiAAAiAgE8QgMBgMCMEBggM1h5oCAzW6Fi+BoHBMht3XIHA4A7KVvqAwGAFDi6BAAiAAAiAAAiAAAiAAAiAAAiAAAj4BAEIDAYzQmCAwGDtgYbAYI2O5WsQGCyzcccVCAzuoGylDwgMVuDgEgiAAAiAAAiAAAiAAAiAAAiAAAiAgE8QgMBgMCMEBggM1h5oCAzW6Fi+BoHBMht3XIHA4A7KVvqAwGAFDi6BAAiAAAiAAAiAAAiAAAiAAAiAAAj4BAEIDAYzQmCAwGDtgYbAYI2O5WsQGCyzcccVCAzuoGylDwgMVuDgEgiAAAiAAAiAAAiAAAiAAAiAAAiAgE8QgMBgMCMEBggM1h5oCAzW6Fi+BoHBMht3XIHA4A7KVvqAwGAFDi6BAAiAAAiAAAiAAAiAAAiAAAiAAAj4BAEIDAYzQmCAwGDtgYbAYI2O5WsQGCyzcccVCAzuoGylDwgMVuDgEgiAAAiAAAiAAAiAAAiAAAiAAAiAgE8QgMBgMCMEBt8TGN58/kJf/vuPAj5+FkZ++/mzcv5N/HujHHNK4heJkvpFFMcfvn6lD1++0lelDNflxOW4TsDHT+Lc1o9CCeMYFc20cKPReXA5gcCgr6UhMOjLnyAw6GwAdA8CIAACIAACIAACIAACIAACIAACIOByAhAYDIghMPiewGDLw3P71Vu68+qNLUXtKgOBwYALAoNdt43TC0NgcDpS+xqEwGAfL5QGARAAARAAARAAARAAARAAARAAARDwPgIQGAw2g8AAgcGZTy8EBgNNCAzOvKvsbwsCg/3MnFoDAoNTcaIxEAABEAABEAABEAABEAABEAABEAABDyQAgcFgFAgMEBic+XhCYDDQhMDgzLvK/rYgMNjPzKk1IDA4FScaAwEQAAEQAAEQAAEQAAEQAAEQAAEQ8EACEBgMRoHA4NsCw08hQ1LISH4UKrLyL1Lk/x8bvt+9fkXvAgLo1Yvn9CbgBb16/pyePXtGnz7ZF3dB+3hDYDDQgMCgvSvcfwyBwf3MjXqEwGCEAycgAAIgAAIgAAIgAAIgAAIgAAIgAAI+SAACg8GoEBh8T2D4lDwNRY4ZiyJGi0FhIhoCOdvzCH99+9YgOrx8QYd27aJv377ZXB0CgwEVBAabbxmXFITA4BKstjcKgcF2VigJAiAAAiAAAiAAAiAAAiAAAiAAAiDgnQQgMBjsBoHB9wSGM/++oICPhl0IUaJFpZhx4lK02HEouvKPhYdQYcKoDy2LB1+UHQyfAwz/vojvF+r1vff91WNbDiAwGChBYLDlbnFdGQgMrmNrU8sQGGzChEIgAAIgAAIgAAIgAAIgAAIgAAIgAAJeTAACg8F4EBh8W2Aw94jGVESGpIkSUNQvH+mzIi5YSxAYrNGxfA0Cg2U27rgCgcEdlK30AYHBChxcAgEQAAEQAAEQAAEQAAEQAAEQAAEQ8AkCEBgMZoTAEPwEBrZ8Er9IlNQvcPdJEBiC9uMOAkPQuDmrFgQGZ5EMYjsQGIIIDtVAAARAAARAAARAAARAAARAAARAAAS8hgAEBoOpIDB4nsCwpE4lenb/nvosZY4VjaKG/e7WSL2gHISJGZvCJ0lO4RInp4jpMorjNe2b0+Orl7XFfji2VWDItHAj3T20n55cuUTPblylZ7dvGo3NtGGtiyQeW9qxc0yLBItzCAz6mhkCg778CQKDzgZA9yAAAiAAAiAAAiAAAiAAAiAAAiAAAi4nAIHBgBgCg+cJDGwZ//Nn6PTyhXT1wF6SAoM5MSFkhB93IaxsVsepAoO5h9GS6MACQ5RsuSlGqQoUSRE8gmuCwKCv5SEw6MsfAoPO/NE9CIAACIAACIAACIAACIAACIAACICA6wlAYDAwhsDgmQKDfAI+vAyg93dvkV+K1GROTJDltN/uEBi0/cljFh3ipkqt7KqII7OC7TcEBn1ND4FBX/4QGHTmj+5BAARAAARAAARAAARAAARAAARAAARcTwACg4ExBAbPFhiC8iToJTAEZay+WgcCg76WhcCgL38IDDrzR/cgAAIgAAIgAAIgAAIgAAIgAAIgAAKuJwCBwcAYAgMEBmtPG8dgQLKfAAQG+5k5swYEBmfSDEJbiMEQBGioAgIgAAIgAAIgAAIgAAIgAAIgAAIg4FUEIDAYzAWBAQKDtQcXAoM1OpavQWCwzMYdVyAwuIOylT4gMFiBg0sgAAIgAAIgAAIgAAIgAAIgAAIgAAI+QQACg8GMEBggMFh7oCEwWKNj+RoEBsts3HEFAoM7KFvpAwKDFTi4BAIgAAIgAAIgAAIgAAIgAAIgAAIg4BMEIDAYzAiBwfcEhnOrl9LxJfPpzfNnFp/VJH6RKKlfRIvXOaB0zFIVKE6lmhbL4IJlAhAYLLNxxxUIDO6gbKUPCAxW4OASCIAACIAACIAACIAACIAACIAACICATxCAwGAwIwQG3xMY5AN6ddsmurJlI90+dVxmqd+WBIZIaTNQ1ILFKXqB4mpZHNhPAAKD/cycWQMCgzNpBqEtCAxBgIYqIAACIAACIAACIAACIAACIAACIAACXkUAAoPBXBAYfFdgkA/ky3u36cTieXRt32769P6dyDYVGKIXKEYxlB0L4ZMkl9Xw7QABCAwOwHNCVQgMToDoSBMQGByhh7ogAAIgAAIgAAIgAAIgAAIgAAIgAALeQAACg8FKEBh8X2DQPo/sPuncupUU6dULSp08mRAVois7FtglEpLzCEBgcB7LoLQEgSEo1JxYBwKDE2GiKRAAARAAARAAARAAARAAARAAARAAAY8kAIHBYBYIDMFLYJAP46sbV8kvRWp5im8nE4DA4GSgdjYHgcFOYM4uDoHB2UTRHgiAAAiAAAiAAAiAAAiAAAiAAAiAgKcRgMBgsAgEhuApMHja8+hr44HAoK9FITDoy58gMOhsAHQPAiAAAiAAAiAAAiAAAiAAAiAAAiDgcgIQGAyIITBAYHD5wxYMO4DAoK/RITDoyx8Cg8780T0IgAAIgAAIgAAIgAAIgAAIgAAIgIDrCUBgMDCGwACBwfVPW/DrAQKDvjaHwKAvfwgMOvNH9yAAAiAAAiAAAiAAAiAAAiAAAiAAAq4nAIHBwBgCAwQG1z9twa8HCAz62hwCg778ITDozB/dgwAIgAAIgAAIgAAIgAAIgAAIgAAIuJ4ABAYDYwgMEBhc/7QFvx4gMOhrcwgM+vKHwKAzf3QPAiAAAiAAAiAAAiAAAiAAAiAAAiDgegIQGAyMITBAYHD90xb8eoDAoK/NITDoyx8Cg8780T0IgAAIgAAIgAAIgAAIgAAIgAAIgIDrCUBgMDCGwACBwfVPW/DrAQKDvjaHwKAv/2AnMNy5/4hu3b5Pj/2f0es3b+nrl/8oXLgwFD1aFEqUIA6lTpmUIkYMr7NV0D0IgAAIgAAIgICtBPh3+o3b9+jR43/p5cvX9PnzVwoTJjRFjRKZEsSPTSmTJ6YY0aPY2hzKgQAIgAAIgAAI+CgBCAwGw0JggMDgo4+4rtOCwKArfoLAoC9/nxcYrt+8R5u27qOde47QoSNn6PmLl4ESz5g+FRXIl5VKFctHv5UuGGh5FAABEAABEAABEHAfgX+fPacNm/fSjt2H6Z/Dp+n+A/9AO0+eLCHlz5OVShbLQ+V/LYyXCQIlhgIgAAIgAAIg4HsEIDAYbAqBAQKD7z3d+s8IAoO+NoDAoC9/nxUYlq3eQnMWrBXCghZxnGgRKUWCaBQvekTyixCWQob4iT58+kLPXr2nu09e0eW7z+jrf9/UKvzGY92a5alpgyrK7oYkaj4OQAAEQAAEQAAE3Etg++5DNGv+Glq1drtRx1EjhaNUyu/2+DEjER+HDhWCPim7GF68/kD3n76mq/ee09sPn9U6IUOGoDo1ylHj+pUoT47Maj4OQAAEQAAEQAAEfJsABAaDfSEwQGDw7Sddn9lBYNCHu+wVAoMkodP33VMz6OXj45QwTSmKEjOlTqNwXrcsLAwbPZvOXbgmGg0bJhRVyJuSSmZLRvkzJqREsfysdvbft2909NJD2nX6Lm0+eoNOXfv+VmTThlWoZ+cmlDB+HKtt4CIIgAAIgAAIgIDzCOzad5SGjppNu/YeURstnTM5/Zo9ORXIlIjSJIqu5ls6OHvzCe05c4+2HLtJe8/cVYv9Xq4ode/YmLJl+VnNwwEIgAAIgAAIgIBvEoDAYLArBAYIDL75hOs7KwgM+vKHwKAvf5/ZwXD9xl3q1GuM4jJhjyCaNnEMalL2F6pfMgOFU0SGoKZjVx7R7C1naeG286KJsIpP5+ED21Pr5jWD2iTqgQAIOEDgw4ePona4cGGNWnnz9h2FCxuWQoUKaZSPExAAAe8l8Pbte+V3+2iaMWeVmESsqBGoGf9uL51R2YkYKcgTu/7gBc3dco6mbTwldjFyQ107NKQhfdsEuU1UBAEQAAF7CeBvF3uJoTwIOE4AAoOBIQQGCAyOP01owZQABAZTIu49h8DgXt4/9OYLOxgWr9hEzdoMoPfvP1IMv/DUs1ZeavbbLz/M1ZGMS4rrpKFLD9HqfVdEM5UqFKcZE/pQtKjWd0Q40qejdVf/tZ16DZhEmTKmpuXzRlpsrvCvDcn/yTOaNr43Fcqf3WK5oFwYNGIGTZ6xjGZO6qvEsygUlCZ8qo67eFSo3paOHj9H2/6aThxTxBcSu0b5s91guqkEaf+jUklaNneEmNaIcXNpyMhZImj75NE9qE7NcpS9QHWKFDECHdu31BemjjmAQLAksPfAcWrauj9xLCVO3WrkoW7Vcynuj5wnIj59+Z6GLTtMU9efFH3kyp6Rpiu/2/X+uZklX1WSYqoY2P8/YsWMRtmzphe7Lf6oWFIEstZe95XjCdMW09SZK6hT23rUqG4ll01r9oI1NGr8fGrbohY1b1xV9JMuWwXxfe7IGqcI1gcOn6ImLftR/HixaOfGWS6biyMNu+tvE0fGGNS6AUrQd7ZpqhSJad/WeXY3c+rMZarZsCvlzpmJ5k4daHd9SxVM/3aR95+l8t6Uv+/gCapWtzP9WjI/zZkywJuGLsbKcftqN+5ODepUhOjsddazbcAQGAycIDBAYLDtiUEpewhAYLCH1v/YuwrwuI2mPd9fTClJwwwNQ0MNo8PM1HAcZmYGh5mZmdlhZmZmZiylbZp/372OsifrzpJ99tmpNk8s3Wq1MLtamHfA82ltgMHzNLWUY3gHGPoOnkx9BkyUba7ik5KGNvChKBEjWKKBlcRLdl6k9pO3S58NKZMnooWCyeltRoSr+uPg3LBFX8qYLqVbRmusJPnp8ZPntGbxGCrhYafWHYRWyYixcwSdBlPl8kVcVTVE41t3GkLrN+6mCaO6USGf7CFaVmCZhwQ9Muf5mXCAPrRzPn0fOaKsQs6CtejgkdN0eNcCypQ+dMx+GNUjMHpYeZ40XUkJLuCdzu3qkV/PFnTv/mOKn7KwzObbb74WB9k+VMAnG30fL5eMe/fqpLyG5T9G4/OK0MgqXr4pJUuakNYvGxeWq2/XzaZAiFBg1vzVVK9pL5k3TCANa+hDaRJFC5GykOmeM3fE2r6Dzt54Ip0/L5wx2OProZXKfxIxcCGJnNnS05I5wyhmjKhWsg4XaXv6jSe/IVNpQO+W1KmNb4jVefDIGdS19xjq060pde/YkN4LM5mfRsogy/vj6VH67LOga8BypTdu3UslKjSn6NG+pwdXt3N0mLqGxN4krDTw+YtXFC1hXkqcMC5dObXOcrX2Hz5FuQvVpny5f/IYQGS0d4Hg0scSIBBStGwTKlksL61eNDrcNWvdxl1UpkorCTpCcMUOHx8FbIDB0ac2wGADDN76un/55Re6efMm3blzhyJHjkyJEiWiGDE+DjPkNsDgrVH177wmNvMfPOp6ty7/ydLDM8DAByJ03IB6ealVec9K37saELcevaaGI/1p75m7BGnCNUvHUpaMaVwl91q8DTA4SF+lTgfpEHTZ3OFUrnQBr/UHCuYx60nAJVKcnFJ6//6VbRQjehTZPm8ADEb18BSxX7x8TVET5JHZqe0EcFS6SksJou3bOkdK8/799ztauGwDff7ZZ1SlQlFPVSHE8jEan/Ahkz5HJelY/sKx1SFWtp2xTYGwSAFovbXsMEhWrWmZjDRUgAuhEd798w81HLGRFu24IItbOGsIVS7nADBDo3y1DAYY1i8fR1l/+lE++vvdOwL4eEiAx+27DpdxcePEoPNHV9HXX4WcYIVar9C6h9T42QtiHkybIkSFOGyAwdGjIbE3Ca2xElg5YRFgMNq7BNaO8PTcBhjCU2/9N+tqAwyOfrcBBvMAw4gRI2jw4MHaB9OzZ09q1qyZ9tu+MUeBe/fuUa9evWj69OkBXsiXLx/179+fcubMGeDZqVOnaPny5XTp0iWhXfopJU2alCpXrkypUoWOIGWACrmJsAEGN8QJhUe2BkMoENldEeEVYOjeb5xw+OhQNZ/VsQRVypvCXTND5Fn1AWto1b4rFFMwdbeunUopUyQOkXKCmqknAYY///yLXr35haJFiRxodWCqKkIEh338wA6tb9/+Sb/9/odpU1NIi3dU01So290HjyhK5EgU8buANrmNGLj6Rrx+8yv9I5hLkSJ+q39k+Bu4KMxKQSIRi4i7YJYesMOLPOPEik56/wLu8jdi7OsBhnfv/qGHj55SrJhRA62v1T7huhnVg5/xFTR+JLRlYukkbkHP+w+euKwf6h4nWUGClsLLe/s4O1qxeitVqtWeKpYr5NYMmPaCuAEAcfP2ffrqqy/ltxtY/6nv8j3G3DMhFalvBz9Xr+hTjKsvvvhcjdbujcanWYABjBPQxKyULegYVXzDRn4qnj57Sd99+3WQTa4E1k5uMPr67r1HBGZp3NgxAq07wCXQ7qsIX3IW8vpAtAVjNV6cmPTJJ66/wcDmB84UY/P+wyf06SeffJQS4dzOsH6dMXcVNWjeW1azb53c1K5SllCvcudpu2jsyqOy3JDQ7DPTIAYY9myZTTmypAvwyulzlylDDodJn2ED2lGbZjUDpLEa8eaXX4UEP8l5wN27gc3XRu8+efbCcP+AeQOCGlbnYeT36tUv0uyQfm5Qyw+srmEdYDA7z6HNgbVVpQvuze5NzM6h+vz1vzGXY73Srz9Yx9CHge17rNDir7/+JjDOUJYZgAH7L4xBo7HkToMhqOuG2b2LqzXbCi3QD9g7I+jbh3j0r9m9r8zk3z/u9qxGAAP6/xOxvmKf4S5YHceu9pUow8oeSf0ebA0Gdz30cTyzAQZHP9oAg3mAIXny5HT58mXtA/jxxx8JTG87mKfAs2fPJCDw+PFjty9t2LCBihUrpqUZP348NW/eXPut3gwcOJA6d+6sRnn9HvsJrGX4b4fQp4ANMIQ+zZ1KDI8Aw+SZS6UtdjRkbudSVD53Mqc2heaPqn6rae3+q0LKLjntFRLUEXSOZ0OzLvqyggow4MABe7UJ4sWiGcLebOPW/WjHrsMyezDU4QizddMaTsXh4ABTL5u37ZfmllIJsKVqxWJycz9q/LwAJpJg37RbnzF05Pg5mU+yJAmobKn81LtLEydGbK2G3WjL9gO0fvl4WrjUn6bOXE5lSvrQ7Mn96c7dh9ShxwhaumKzVhdIcw7u15qqVigmmYWZclWR9UECHGwjRvyGIBGOQxbaOXrCPPLfvJf2HXSY0vkxTTLyyZOZGvlWouTCPA2HctVa08HDp2mn/wwaOnqW1IgAMwZ5li9TgCaO7O5Ubyv0wOG1raAd0wJlwqzV5LE95ZXroL+uWb+TGrXqq7UPfQMbwSsXjCIGGMAcW7jMn9b575ZaDqhvlYpFaNywrgEYu2b7xEo9Jk1bQn0GTaLhA9vTy5dvaOS4uYRDKZuJgI31uYvWSRNWMNOF+vnkzUzFCuWiBnUqSAbBsDGzafL0pZp5JIytouI5fDHsF/2G9xAQ36NTI2nWJJtPdSnRe3DHfK266JMW7QfKccSRoNnAPq2oTnWHzW2O11/zFKlDeH/+jEHUZ+AkWrXWYeYC7/fs3Jga16sk68rvXb91V9oPX+e/iy5fvSWjYV4BPk7aNK8p2wlmttH4hH+JaTNXyP7CiyijXcva1F78RwD9OvcaRctXbdXaDnp0aV+PcmXLINPgz/FTF4RZjmbSX0UpYaag36DJcpxD2hmHZ9gdnzSqB126elP6SQHTHyF3jow0b/pAyfyXEW7+mGknvw6Qa+DwabJcfDscOrap6/TdJ0hVRJr6WiSkxzF2Fi/bSNPG96a6NcpKEHCU+GaHjpqltR35wHY1nPNG/O4DwBjY/MDlg6ngN2yqBlgjHuOwSYPK1LdbswDfCb9nXz1Pga07D1KRMo1lxv3q5qG2FTN7vhCTOXaeupPGrjomfLlEoH1b51KaVElMvumZZIEBDCgF8wC+BYzX53f20HBhjhAmCQvky0rzpg10qgjmjdSZy0lmInzzvHz9hirXbC9tjOfJmZG69BpDp886Ds5YjyeM7CbWQmdwx8x8jUKHj50tv3PMrTAFM2veajlfw0RNvdrlqEOrutRrwASaPnuF/I5Rf8x7E0Z007638VMXEdYPzJe+NctpbVm7YSc1F/M4z1d4AIB5WP92FC9uTC2d2boGBjBgfa5au4P06XPm0PIATPAKNdrKdcivVwunemoVETdWTSSBYWp2nkM5ZtuKtFb2JmbnUOSrD3sOHJfjC1qj+cU4GjNxvlx/0NfYgwwV/eW/eQ/1FiZO1TVy3vRBTsC9VVosW7VFjL1Zcj+Fsgr4ZKVmDapQodKNAphIwsF/7OQFcq3mOmQW/k0a+lZ06ksjgCE46waPGf3eJYnwEeFqzcZ+1CwteA+Pfe4CYeqt7+BJhP0iQukS+ah/jxb07p931KRVf2lKE/H45qeKdVbdQyDeKJjZs6oAQ3kxBsZNWij3JMjPVVlmx3Fg+0qzeyTUBfsSv6FT5L4QYwD7LdAouwB1YaIPewvbRBIo9fEFG2Bw9KkNMJgDGI4dO0Y//RTQUsaZM2coTZo0H98HEkItatiwIU2dOlXLvUSJEpQlSxa6evUqrVq1it68eaM9e/78uTSdBBAnfXr3ZkN3795NuXPn1t719o0NMHi3B2yAwbv0p/AGMBw+dpay53cwt0c2LUgNSwSU7AtNkr775z0V6LCQjlx8QDV/LkmzJvUPzeLdlhVUgOHX336n72I5fBXggAZmIJiOYBSAqYuwePZQqli2kLx/9vwV5SpUSzskZsv8o0zHhyckUk0CTRRMg+btBsh3cZj7WjBwdu5xSIuinFXCXitLUxUp25i27jhIAA6YoVBLOPEFQz97gZqSIYJnYNyiHhu37JX5zp8xkHxyZ6Hy1drQuQvXZBvA3MDhAdom0LBo3n6APFiijcWL5pbSkPsOnJBpkQ62euEoGIEZ9kwPAAAIYOIiNGtUlcYMcaDnVuhx8fINyfRBHgA3MqRLQWfOXtHyvXxyLf2QKB4eBwg4wPX2m6gdEFGnjBkEMDG6p1ZffgntQfvhlwGhdbMaNHxAe35MVvpEe+nfG3f1YOaN2n+oCwCGw8fFt+zj+JYxZnC4Pn7yPJ2/eF3mPHpIJ2re6GcaN3khTZ+zUmN+IW3B/Nno5q37krGDMYl+SZ3yB+rQuo5kiul9MLx6/Yt0/Mzjt6Dw03DoyBmNiR+Y+SzWzuD+h+3zBw+fat9D904NqU/XprLecMzKYxM0z5UjA10TTmoZxMLhFSAQJHeNxiecIi5evlGjA9pbu3ppali3ohzj/K2hLjmzZ6DLV25q9VC/M2aKgN7qt3jpxFqas3CNtG/O7UH/AFDkOoIBEJhpJrPt5PHCmhr4radfi8bVaNTggxljUwAAQABJREFUjlLSg+2fg3bcXzMm9qXa1UoTAMf5i9fLLAGqQDp1l5g7MEch/amDyyR4qNbN1fwAEBKhaVs/CWCBFmBGIc9lK7fIZw3qVpAgjPxh/wlRCrx48Zoy5a5Ct+48oJblfqKB9fOGaHlmMvcdtoEWC3NJ+AZhgi00gxmA4ez5q5Que0VZrTuXtkjQPYdYFxGe3Nyl+eTBb5aYxvdw46w/bdt1SNpI5zkA10xi/Th24oI2L145vY4SJ4iL103P10jL/hNwj4C8ERhYxPzCDF31O2ffOkjLeag+GMBYbPbv3gF5wOcQr2lY//YKbQ9oO5ldW1AOr1GufDBA4w1+qlB3mKsqWjAXXpMBUuRxhWYdgru12irAYHaeQ7lW2mplb2JlDkU99IEZzGq8uhapY0C9x55QFQywQgsIImA+R8A4h4ahKriBsab6YKhat6MmoOKTNwu9EkIQvKfjNQl58Vqq+mAIzrpRu1F3w71LdKENDV8PKp1QPtbsJInjmV7/jPbwoAfvoZE/ABKMaTUeZakmKPFbH8zuWY36H+s+TLzxfuTGuY0U/19Q0Mo45m9WrTvahH2lOsYx77jbI6Ft1ep1lkIMuIeQCgLvQXFvAwygwscZbIDB0a82wGAOYGjfvj0NH+4wTal+ET169KC+ffuqUfL+1atX8gqtrW+++UaAme/o+PHjYo/yhQQkWGsT8RcuXKAnT55Q2rRpKWrUqAHyQgSY7WDC4wrNidixYzulAwD94sULpzj9D9QlUqRITtEvX76kixcvSs2MaNGiUcqUKSl+/PgBtEp/++03+uuvv+S73377rXx++/ZtOnfuHMWNG1dqJSD/wEK8ePHo7l0HL2nixInUuLFDqAjvXRKmj1KkSKFlsWnTJipcuLDUTmDTVPXq1SOYqvrjjz+oUKFCdPq0g6/RtWtX8vNzrP9aBl68sQEGLxIfRQsJEjt4kQK3jk9+f3pDg/fPry17Lxyihvn/QkIbukbvG5ZM//7X9e3CxP9jE+u8//LzT2W9BCMszNBwytiesk7i4O22TmJjLtMJaXeZ7vWDA/I36CwOfu9PH1wu4/9+eeL9z5WKyWdi063l2aNzIxknNufvr5/zl/Fvnx17L5jYWj6C8Snjb13YpMUJkELLQzBHZFkoc8SgDlq8YARr6Qf3a/P+4fUd7/96cfz9Dv/pMl4cMN4LR4xaeuSJPPAej2ch3SjjBBNZi1PrcfvCZi3+0fWdWnk7N87Q4gWDSYtftXCUFj9xVHcZLw6uWpwVegjGtHy/RtWS2vugswAsZDzazO1wdRWHKJlWHAy1tGp9BVPkPfoD78+e0l+mRb9yfiotzPQJv6e/GtVDMIdkeeiTKhWLvr96ev37P58fl2ULMEA+wzhR8+rWsYGMFwd+Lf7e5a0yDmNVTbt0zjAZLxjwWvzzO3tlHMrktD27NJZxGC+Pb+yS8b8/OfJeSMfKeHHw1dLyO+qV24Y8xWFfSysk27SyBDNOxi+YMUjGCUbJ+18fHdLSblw1UUv72+PDWrzR+Dy5f6lMq/YT6sPfFOr79NZuLQ98M6gb6sl5C9MqWnkCvHp/aOcC7RnTGO/07tpEGx8n9i/R3lG/C5UWfG+lnVvWTNby3bdtjqw3xjl/r6jHq/v73yMO99yWFfNHavHb10/T4o/uXqS1HeMe3x/eQbtQP7PzA/qHy7t4fI2WJ+Yxjn/z8KAWz223r57fK9SqVlrSvEDGhGFiXcf+4tWaNu9TJ4wq6yWY36E6Dnj84Tt2Nd7wzajp8P3wtzBnqp/Te0JDQKbFXIj81PlIaA++F4CEjH92e4+WB6/bSG9lvlbnF6G99h7fkDBt957rgDpj7uU5bEj/NrJumDO5rZwH1hDEYW3meRh5cjrMx0wDzDNW68prlAAY5LvqHMR7i0b1KskyfGuV08pFObz+B7Z+YA1GHfXrF7dBvVqZ56y21crexOwcqtZdvVfHlxAKeX9XrOFY+4f6tdX6C2u8AGYkTfdvm6vFv7i7T8ZZoQXGL48DlMF1wVrKe1x1n4a1Bekxps4eXqml37VpppbP4V0LZDyvpQJgkL89sW4Y7V24HNRLv2ZboYW6h8f+H+s66KHmD5rw3IL0/G1hrDLtjK5m96xq/+PcwPsJzFk4K6CN6n7TyvzC3yzy0O8rreyRhCaX1tfb1k3V2q3uS9SzjhE97DjP7wVCi6a3Dg+W/I8X9494kQvj/aLxHeH/fy2sWbNGtrtUqVKBNl0w1t9Hjx5dmy/KlSun3Qvm+nsBEjjlIcAC7TneE+Z93gumvFPcvn373u/cKfY1Sjz6Ydy4cQHyUsvj/kK5TZs2ff/777/L9FeuXNHy5zT6a+LEibW80SYBjBi+ky1btvfC+bKWFjfCN4KWdvbs2e8FyKH9Rjn4LcAGp3f0P/7++2+nd4TGgj7J+z59+rwXfhXk/3Xr1snnwlSS9t6hQ4e0d4SvBi1eaEZo8WHhRgB3sm5hoS7/xTrYGgziq/RmCE8aDFD/79h9BKWIH4UEU9+bZAtQ9vQNp6jl+K3CdncUunJyvbTvHiBRKEd4QoNBbLopX+4PZio2bNpDpSq30CQ6Yec2WsK8UhJq69opTiYVYB89YeqiUlqJJau79R1Lg4ZPl6Z/juxe6EQRqLZXEaYIIMl/Yt8S+Yw1GCBdtnXNFC09p0WEYFpTogRx5DNIGy5a7i+R9WqViss4lpxWpdTv3n8kzfUkTRxfSijJhP/+KVCyvtSoEEAClSqeT8ayBoNgxkozPJwe5gbY+bA4pEnHwlboAWlMSGWq0pfIG+ZztglzIQnix6Y8OTJxcYZXlq5XJc9c1RfSXdET5ZX5oL5wDGq1TwwrISKN6sGSZnhHHCw1jRD8njlvFUECt6kwX6Bqaezad4zyF6/nNA7YB4M4FGvmlZAHS+VCwn/GBIcECbQVVA0GsbASS8WrfYr3oUUAZ8oIdy5ukdLr8ofuD7dt7LAusr7qY5hPguQ/1Ogh7Yb6r1m/g4oXzi1NlXBamC34NmY2+VOAHBTl+4jy3mh8GvlgUMfa2iVjqXgRZ1VQmDWDVDC3kaUuUYgAyyh39oyyPPxh6WBoDMHslxoy5/lZSnHCjApMrbgKVtoJE2Mw0QBzJ8P82jllma+YL10SWhirl4yhzBlSa33Vr0dz6tq+vpYWZiVgmspIq2D56i1UuVYHKY156/wmMjs/4DuLl9yhibVkzlCqUMZxj0Ihdfzk6QvRj3m0vtIqY994lALs8BS+NI5NqENJ40b2aP7ByWz36TtUrItjPRLAltQyC05+Zt81o8GAvDgd+4rwGzqVevYfTyWFWbTVQhsQQZVoPnNohZTUZQljwVikG+f8nfwaseklVTPLynzN84sq8Y16LFi6gWrW7yo1GqBhwb5jbgtzh4nEXgFBMKGlTxXOQzATqVMbX6nJ1qrjYLkuHN+72MkkHdLCfGK3Dg2k1puVuvIa5UqDAXXcvf8Y+RSrJ+v9+MZOzU8N7xUmje4hTfrJBhj8saLBYGWeQ1Fm22p1r2Z2DjVorozi8YUfAjDQ7O7fuvuAEqd2aI/pv6ek6UpKrTUBNgjH5mnJCi1GCxNMbTsPldpxuzfNknXgP9OEKa5GLfs6mUiCGUVoN8BE38DerTipvLbsOIjGT16krVe8lvJ49sS6YbR34XJQCf2abYUW6ve+fcN0ypvzwx4SJgihycAaotxwmELFNyTAPmrXojZHB7ia3bOq/S+EPpzMF7bvNlzuvwXgJc0johCz4xhp+ZvFvbqvtLpHKlO1lTAfuot6dWksTV0iPw416neRZpNsDQamyMd3tTUYHH1qazAErsGwdetWKS0PikF6HxoHgsGvfRQCLKAcOXJov6GNIIAF7bfRDfJRzQGpadT8fHx8SAAR6mOne5gcmjx5stRugNNjd0EADHTt2jWZpGbNmjRv3jyXyVG/s2fPSm0GJAqsHkgDzYoTJ04E0H7AMw7p0qXTtA5QRseOHSVtM2bMKPaEn3Eyp+vDhw+lxgIioQEBTQkBVlDZsmVp/XqHVrveZ4NTBl74YWsweIHoSpE2wKAQwxu34QVggBPeBKkK0+vXv9Ki7mWoVPYk3iCX2zJLdV9G20/cIiF5J213u00cCg+xYa/frLchM18tHur/UFlmpqV6OGEmNKdnkwxCEkyqml+7cYeSpS8lHwvJP6dDPyJ9m/ak2fPXaCaSAE4ApIB5k4plC3K28gqTMz2E824EIXEvmb0MMLCJFPlQ/IG5pvgpC/NPaYO5YL5sEuCAKrkajBi4/ByObQ8fPSOd/t64dU+YzTmtmYlhRi3SMsNeSGVRIR+H+SjOg5nPQkKL/nj71hI92JEc8gKjp3rVEsK0U2bJ2FUdWXNZRlcu3whgYKaT+h4zpFDfOLGjS8DISp+oean3RvXggyDMWs0U/jyMAvpy594j0uQR+gDmaaC6rwJNwQEYVGbATaGSr9rpNqqPURy3jRkfapouvUfTkJEzqW2LWsKudFvtEb6Nk6cvyrEFsy/+m/ZqJn+CAjCAGQKmCIKQnNWYc1zghCmLJTDQv2dz6tKuvmbWAaCM0ABx2vAx885ormKzEQwKcv6urmbayeCH0HqgKhUcjESj/FQwiM1CcDrOA+bP2LwRP7sqTFAlz+CYh355eEj6fjE7P2TIWVkzvwXApUTRPBJUBejnznk0l21fg0+BHAVrybm3T+3c1L6ys93/4Oce/BzaTdpOk9aeEABUQVoyZ1jwMzSRA8/TQsrY0MkzsoD98M+/dwCHQjOIsv30ozRBkiJjaVmC0EaQ5gZXrtlGFWu2c9oLMAPQyPwTm5oxYq6Zma95fhFSyTSoT2utta7KVEFhoVkmGficBwMMzPQV2gTSV4OWqZsbM3XlNcodwAA6x01eUO6TeA/AaxKKF9oVFDWKs9kBtVpWAAYr8xxMPXIIrK1W92rIz+wcynVQr9zXmEdVYRJ1fyk0W5ycDjO4LaT1pdlLK7Ro1s5P7jVHDu5ALRs71kmuD7ed960wJfFZZMd3IyTeA/hX2bH7iDTFx4ItzPhngAH5BnfdcAcwGK3ZVmjxz/t/NDOnQkvICTzkvaxegID3MXohGqYhX83uWbn/1X0c5wH/Ki3bDzI0PxTYOEYe/M3q95VW90h89sH4xDhVw9xFa6lOox6GdVTT2ffhlwI2wODoOxtgCBxggFmeGTMcwli+vr40ffp0KlCgAG3f7vDH16JFCxozZoz2MegBBjDShbaBTH/kyBEtHW7goPjevXskNBe0eM4P8SqQMWrUKMqUKRMtXrxYSw8g49GjR8LM86sAgMGOHTto+fLlWr758+enbdu20f79+ylnzpxaPJwnlylThu7cuUOtWrXSgI86derQzJkzZTo9wACnyvCdsGTJElq0aJGW161btzRQQotUboYOHSpBBSVKuy1SpIgEG1CXJEmM+XwwowSaHThwQHO4nSxZMoIvjM8//1zLy9s3NsDg3R6wAQbv0j/c+GBgqbzCPyWilX3Ke5lqxsXvOXOHinZeIg9M965s0yS2jFOHfCwfYPRS32rJkPj/IopDumn35lmUM2t6J2lHoaqqJpe2SdNmLa9JgsEpW/4S9TXJYafE4gdLxzOzkiXU9On0v1kqiQEGvRQW0uMwAQeB7HeB88CBZtyIrrItiHMFMPQfMoV6+U3g1+QVTH62E20EMMA2MGwEq4GZz2DYX71+2xI9kM/8Jeupa+8xmn1czhsS6pPH9KTYMaNxlOGVyzcCGIzqy4wrBhis9olhJUSkUT34IMjMG/VdgFX9hkzW7N3zM7arqx5MmZmjH8s8xt1pMBw8eppyFqglszcCwbhcd1duG2z8og5qmDJzGTVp3V/6SYAWxctXb6hlh0GarwBOy+3C76AADKpEKedpdO0ipP77C+l/I6YIp2fm3bAB7ahNM4fNdn7GNon5m+V4/dVsO+FPhZmgeolVfZ4qwCDMk2ggispI1QMPyEPVDoEtenwzZucHjK0eQuJ7hvDzoQb0szvHrWpa+z7oFFghmN+VBPM7YcyIdG56/aBnFIJvPn75GyWvPYX+FOvlASFdnUVIV4d04HnaHcBw6sxlypirsqyKas+cmYgMxrE0riq1zAxArDMQLlDD1FnLqXGrfk7MNSvztav5hcuED5xNqyZpRZoBGEpUbC7X+jHDOgunvVW1d41urNRVv0a5moO4TaxBxVLx7FPHqB4cZxZgCMo8Z7atVvdqqLvZOZTbqV65r/XjSwUY9OuxCjDkEhp3vG6YmfOr1e1Ee/YfJ2EajKpXLqFWRe7psIYzwKAKHTgl1P1gfxBGa2lw1w2jvYtROaiS1XER8btvNIBBv4fnuUE/r5gFGFAfM3tWV/2P9ydMXUwthKN2FcA0O47xvv6bRRyClT1Sj44N6avoDjD72pkNlFBoC6th09Z9VLxCM6c6qs/t+/BPARtgcPShDTC4Bxjge+Drrx1+pEAxYbaH4Jh4woQJ1KxZM0lEAAhPnz7VGNx6gAFOiiHdD3A7VqxY9PjxY/kemPRgliPgnv0MCBNBEkQQ5oAIfgoQsmbNSk2aNJH30DyABgIHSPPr/R8Ik0kSjGAtCQAVADdixoxJtWrVorlz58rXATTs3buXs6I5c+ZQ7doftNjY0bIKMDRo0ICmTHFYlYAvhAgRImjv79mzh3Ll+uCrSnvw7w32WAAxxo513nfq07Vp00b6vODxyc+hzYG6qAF0GTlypPRvocZ7894GGLxJfSIbYPAu/cMNwABJKki2rO5XgQpmTOhlqrkuvlyvFbT56A0SNoKpddMarhOGwhM+UKIoZrrpi4VpklQ/lZXRbDpBPQDqDydwfqYCDHCIh0Mhgl4NGnGsds3MSlZLh9RY5fJFkMQwZMmURmpDuAMY+EUcFrcLh5Ubt+yTKs2IV80+GAEMew+eoLxF6sosoCIP7YcfhOYDHM797NtJMr1XLx5NJYvmlWn4UCZs8lKm9KlkHP9h5jMY9o+ePLNED84DC+6Z81fkARmmZODYGsHIhA2/w1cu3whgMKovM64YYLDaJ1yu/mpUD1cHQTgYTJGptARVIEHrW6ss/ZQhDSVOFIegxZAhR2WPaTCcPndZ5of6qqYauP7Y8CFgM+AqcNuEbX5KEDeWU7KBw6dR977jiE368G8wqKHVACeDPwiNnxjRo2ggjOqA1Wh8GplI4gM7Coe08v/EP6MQN3YMqZniilmBd5hZJnw3UKsmztKeZgEGK+1k+rH0r1pvjH38580Ym7NSAQak5zyMwMZDQguJnduy9hOXEdj8wOkALO4Ukqs79hyR8wg7otSbieP09tUzFChWvilt3rafBjXIRy3KZvJMpiGQS+dpu2jsyqNUr1Y5mjK2VwiU4Jwlz9N6RqCaqlPPkTRs9GyCk1zVKTtrIAjfCjR36gDNNJu6D+D5RDWlxHnrAQar87Wr+YXLDArAwPMkA6hcV1x5DsFB9I8//rS0tujXKORlNAeBAQqH2thbYP4uVqEp7dh1mIQtfSovNFvcBbMAA/KwMs/B7JHZddTqXk1tj9k5VH2H+1o/vtT9pTuAIW+unyzRolKtdtIMn9H4YBNcDDBgrgedEQBIJBGmMo0C+hqOf92tpUFdN6wADKiblXHx9s8/QxRgQH3wnbjbs7rqf7yrBxiszi/6bxZ5InCZuA9sjxQ7VjTtOzfal8DUanXfLjbAAGJ+pMEGGBwdywxcfNP/pbB27VoqXbo0CR8MJPwxuGz60qVLCQx/DgAcwFCHg+MECRJwNKkmevQAA7+DxCqjHtL/lSo5zPSqgIXwuUArVqyQecMJNMwOwUzT7t276dixYxpAwYXrAQYAGNmzZ6fr16/LJABA8B6bUIIWBBxOI8AxMhwkc4AGQsKECfknHT58mDJnzuxUb4ATNWp84HH98MMPWlnQmhD+GrT3Xd2gbjBv5O/vL/8bpYNWCLQ51ACn0gAfYKaKnUXjeZcuXWjAgAFqUq/e85n2v/ZdeZXoSuE2wKAQwxu34cFE0up1O6h89TaU7ofotH/MB8TWG/QKrMxV+y5T9QFrnUwRBPZOSD2H+Z8YifPJ7F0dglkNGImEc1iB/n5uSYNBtXm6Yfl4KlLwg8odJtUfs1WQWg8MMLDJJOHYlhbPGirrxn9wCBw/ZaGwdx6JOrZ2MP9dAQyw5XzqzCWqXqUE/Zg6GWchbeonz1BaSqyxmj0zJlQfDBwnnChT+5a1tfdxwxL9RhoMRgx7PviBYf/VV19qPhnM0KPf4Mn0y6+/Sfuv8IfAQT0oweSLagqB0/CVyw8qwGC1T7hc/dWoHq4OgszswAH+/tVtTmYSFi/fSNV8O3sMYAB9I8Z22MbUHySv37pLSX8sKZuiZ0yr7eO2CQeIVLSgs2QGS9YKx4BUtmR+2f9gOrDJEs7njhjf8EmCEBQNBtUMkHBcSWlTO9vahM8V+GCA+aAM6VK4ZYq4YgCibmYABmg+wfeI2XYyQKdn/iAfmH64fvMuHdgxz8kHgx5g4DyM7CWzyQVmtJqdH7748nOC+ZgcWdJTmZI+aL4MkBj1Ke4rzaUFZjKC37Gv1ilw5dptgjmfzz/7hO4uakZff2lsf9V6zp5/49Kd55Sx8UxpvgdgeoQvP5in8XxpH3wruAIYAELmLlxHfoNsFo3rAXAM5j8QYF7Qt0lPKlwgB/mvmMBJNGacngGMBHqAwep87Wp+4XUtKAAD58lS5VpDxE2jVn1p2qwVJJz7SvNmEHowu7bo1yjsW4wABpTHZnHAlK7VoJusgt7Mj4zU/bECMFiZ56z0i9W9mtk5FECAUeC+1o8vKwCDFVqw3xCj8cHMfAYYUF82j8O+k9Q27BQgs/+WvZQ1U1oJHukBBmhFBnfd4Dqp2pf6ctQ6WaGFSmO9kBDno59XzGowmN2z7j14nIqWbeLkC4bbowcYrIxj5KH/Zjlfq3sk1pjp3K4e+fV0ZmLB38u4yQttgIGJ+xFebYDB0ak2wOAeYICd/9WrV2tfQO7cubV7SOtzgEYBpP8R9ACDcKgszD5/Kp8VLVqUNm3aJO+Fk2Npmgg/jACGPwVYDHBDLV++qPujAgwAM8DgV00xQRMC5ow4fPfdd5oZJNbI4Ge4qs9XrlwpfR2owAjADpiI4pA8eXLNXJFZgIHfxRX1BwACMGf48OHaI/hlQLxRwF4NoAJrfSANwBh3woJG+YRUnA0whBRlzeVrAwzm6BRiqcIDwFC3SQ+as2AtecI+89NXv9Opa4/o1PUn9I1gZiSNE5myp45DX37umPg9Qej4P0+gZ69/p9MHl1PqlD94Issg58HSoZCmhjPX5EkTanntO3SS8ggGBYJ6yHF3ONFrMOBddnRYqXxhmj9tkGa3HGrUfAhngEFVYYZd+JgxoiILsbi8oxoNutDSFZudNvSuAIa+gyZRn4GTnNIiHzgx/iFtccl0Ob53CaVLm4zYprzqx4GZwnqns2z2AHkFBWCATwMr9GAHwUwflIvA2idgksCOtjtb8Mz8Vs0I8CHSCBBhyVjWYLDaJ44aBvxrVA9XB0FV4lw1OwSzA6UqtZC+BDxlIgk1Zd8fAKRmTeqnSctDwwaMKT0TRN86bhuYFzs2zNAAH2aiID2cjceLE1MzOaZqwEDSFEww+CNBUAEGo/HJ0rL4btl/Ar6RDDkrScCuU1tfGtCrpcwLf46dPE9Z8laTv29d2ETQYnDHrGBmXVA1GFAXNq1mpp0jx8+l9l2HS0lrmO2CCQcEniMwzmErGuPcFXNv0Ijp1K3P2AB5YL7KV9RXjhk2+WR2fnj+4iUVLNVQ5gkNrk8//UTWC3/gfBomN4wce2uJ7JtgUWD42NnUsftIqpo/JU1vVzxYeUH47uajV3Tq6iNxfU2xo3xDaRNFo5QJogQrX/XlIp0X094zd2nhrMFUuZxrDTz1naDe8zytZwTCHNiaDTukdC3yhum1c0dW0jdff+VUFDsw5Ui9/xOeu4zmPj3AYHW+djW/cJlBARhU++qnDizT7OarTEWsd5ibWJvJzNqiX6PcAQyjJsyjdl0++OAw6w/CCsBgZZ6z2i9W9iZm51DssYwC97V+fKn7y8A0GKzQYte+Y5S/eD1ZFd774QdA8DxCW/X02cuaiSTEs0+PnNnS07Z10zRzfABisJYC9OZ9mX4t3bH7cLDXDasAgxVaqDT2NMBgds+6ffch0wCD1XGs/2bRnwhW90hsJhXA06Gd8+n7yBFlPqpmt2rGST60/3w0FLABBkdX2gCDa4ABZo+iRXNvplj9IGCO6JtvvvEYwAAzQi1bfjjr9e7dm+BHARoG8eN/0LxjgAHXihUrOgESqmYF11XVYOjTpw/17NmTH0lNBGgkcDh69Kg0teQJgAFmmxo3biyzhkNngBeqeSU8GD9+PMEnBAeAM/Xr15fmpWAGatq0aZo5KOQH/wsc4LMiduzY/NOrVxtg8Cr5bRNJ3iU/hQsTSXGSFqCHj5/R0Yl1KGX8oDMLFu+4QL7DNgQgeQqR5+yOJSiNYEZ4IjQY4U8Ltp0nI+adJ/K3kscjQTcwyiDZjACTOwmErdHjgiEJsAABJmr8V07UfEa4O5wYAQwqcxP5Q1ISZm5Um+Z8UMPhvVCZRtK0AJinsGcMyX1Ig4GBAEbj2SMrJIMUdXMFMHA9kObnSsWk1OKTp89pxeptGnP6yK6FkmHIhwiUB3vJY4d2oQnTFmuMAl9h8gKmkSC1tnPPUWQpAw7GXdrXk44z3THsmfnMDHsr9GBH3CgQDjFTJEtEFy/fkIxoSKE2a1SVxgzp7KiQi7/MNICTOoA80P5wV19mXHF9rfaJi2powIpaD1cHQYyxOEkLysM/gISKZQtJZ8gAmMAQ4DCob2tqJ8wMgRZxkhWU/g/ANOJgdEhX7Xnz4ZoZ9ngPjhphQ33V2u3ad6HXNuD8+cp9jN8wmQC70m9++Y1gigQBkrNtm9eS98zYw3irW7MsAVxY579LKwuJYCIM6QFIGY3Pv4UUxrcxs8n8MA7r1Swnx67KRIFZjuxZfqRbtx9ISTskViXx9EwRmdm/f1wxAPHYjAYD0llpJw7u0GYC0wZaBjDdAufq/L3xXOmOuYcxk+THEnIsII9qlR0MadjwB/MI9L5yap1ktJqdH9A3XC/Mg9WqFJemp8Acg6kyBABHiRLEkff2H89SgOf3OZ1KUoU8yYOc+T9iXWk1fivN8D8dII9yuZPRtLbFPCJEMHL5Eeo+Y7c06TZ1bO8AZXkygudp5Il1kYM6P2LMb1g+QWos8XO+qsAx4vSS9q4YwEirBxiszte9B04kvyFTA+yBuMygAAyoV+1G3WneonWSHhCKePb8pWYWEWv7ivkjpT8WK2vL0NGzpA8k9hPkbg7SOz7Wgz+oo1FggMHomRoHx7vZxJxudp6z2i8nTl/UgOjA9mpm51AVlFXbwn0dHIDBypyPsivUaCvXdXwvdWqUkY635y9er629qgaDKowCwQGsSb//8ZamzFgm1xgAD9vXT5d7SP1aCpM+wV03jPYu+nJUelqhBdJ+Fyu7fJ33QJwX7w31Y9esBoPZPaur/kc99BoMVsex/pvltuFqZY8E7VZoLGN/ibHxc+Vicr82ceoSbR9qAwwqdT+uextgcPSnDTC4BhjgZ6BRo0amB/7ChQupatWqwQYYKlSoQMuWLaMqVapIJ8qoQI8ePahv376yLnonzQwwwJyQ6ix6hnBMXbeuwyKE2gjEzZo1S0Zly5ZNOkzm53gHTq05vHz5kiJGjOhkIimoGgxwRB0pUiTOWvqBUE0t4QEcWcMEEgL8VsB/RerUqen8+fMy7uzZs/I3fuh9UUDjA8BFWAg2wODdXrA1GLxL/zAPMPAhJ1707+jizAZBptaGQ9eoUt9V8v3yuZNT3rTx6LjQZJi96YyMSyQcTB4RAEYED2gyzNtylhqN2kQVyhakJbM/SLsFufLBfBH2c2s17CaZ+vqscCiHY9rIkb7THqkOU/WHEzC/U2cu5yQJhhch0VXu5zbaphxxMIMEfwVdeo0mmI8BExkBDJI2nYfSzLmO/pCR4g8OeeNGdBO2+D/4OGANjJ0bZ1Bu4fRPDTCl06hlP6cy8RwHwxkT+wm7uvFk8nMXrkmGOzNm4ED6yy++oDqNu2vMCSTEobRX18aUPWs6Kle1tTx09O3ejLp1aKAx7I2c1DLzWbVvbZYesP/fvttwGj1hvqyr+gcMlHHDuzqZD1Kf8/3y1VuoXtPekg5sHoAPkWbra6VPuFz91ageOAh27jlK80+gvgMtjap1Oko6czz6bvYUP4L0JLSWEGC66PmLV9KkAaR1b513qJbi2ap126lC9baC4VeOpv5rF/31m18pctycsj9f3tuHZDIA+ClfrY2TM23kN0GMuRJF83Aywyv3Mfws9Og3zilN80Y/04iBHTQtk7v3H8nxA7V/DihnpjBVcu3GXek8FfE7/KdTnhyZyGh8QhqZD/xIq5oW2rbzEMFpKw7FHDB2B/ZpRQ3qVNCk8Nm5tU/eLLR1zRROKq+9Bkyg/oOnkOr0lROwQ1j1m+Vn6tVqO1Hfmg26av5FOC/Uu0OrOtLnisrcMzJZBTNT1et1lqaL+H1cARotmDFY+rngeLPzw4lTF8X82FUDXPl99Nm08b2pkI+DUcPx9tVzFPhaONn84+2fdGtBU4oa8YOJOKsl9J23jwYvdPitgR+HRLEi0c5Tt2nN/isyq9YVMpOfr/tv3EyZJ648olyt51GypMLnwdEPKvNm3rWaRgUY9O8ClAVDtFHdipoWoD6Nuo63aFyNRg3u6JQE80hhAfYbOSmGubWGLfpSkwaVadywrvI9K/N1vyGTDecXLlNvronnbBTEptF4jgLIjPkBAYAgTOGMGj9P/uY/mP+H9m9LcCiPYKWu0K5S16jA5iAGxQA0Q0uVmTNcF6MrM1uNnqlxbMLPyjxnpa0AA8zuTVAvs3Oo2ga+577Wjy91XOo1GNgfFK+NyMsKLZB3k9b9JQjF9cAVfnug3aACDIiH9kttsTc+eMQZmISgBgQ7AOAhGK2lwV03jPYuRuXICvz7xywtVBrr9/CsgaAHGLr1HUuDhk8n3vOq5ar3ZvesrvofeU2atoSatRsQ5PlF/82q9cM9yjazR0Lam7fvU6Ua7aRgEn4jQIBhcN82VK5aa6c6Op7afz8WCtgAg6MneQ3D2vdfCmZ8MMBZ8b59jnNktWrVaP78gGd1mB5ic0Rw/gyTQ8E1kQS/DPDPUL58eSnlj35B+QAPYDKoXbt2dPr0h3ULUv4LFixwcs4MvwtNmzZ16lL0NbQgTp48SQAWONSpU0dqPty8edNJewBAAzQGEDyhwYB8VJriNzQa2AE2tCUY+MCzTp060aBBgyTgAOABAWnhNwLaCv3799f8MBQqVIg2b94s04SFPzbA4N1esAEG79I/zAMMC5f5U416Xah0jqS0sFvpIFOr/nB/Wrj9vHQiCWeSHM7fekqZm86WP2cJLYZKeVPwoyBfL9x+Rj81mSUc1salKyfXBTkfT7+IwzkkuW8IKeLkQlIe5pJcSZ8FpWxIdZ06d4meP3tF6YUN+Ngx3WuEPBDmcM4Kx8ZvBYMpYfw4mskDK2WDMXH+0jXhbOmB9H+QIF5smQ9vmDgvlHHv4WP65quvtEMjnkEV+po4ZMYTGgypUvygMYlxQLt0+SYlTRI/gOkJzjOwqxV6gBbHTpwjSN/jUJv0hwSCJubV/AAQPHr8XL773bcfpF0Dq6P+eXD7xGo9kP6UkD5/KxxzZkyf0gnounbjjnAkSBpQpK9rUH5jA40D5S3xP5YYnz8kimfqG2CAAc49wcw/duq8sF/5qzCBlsRwnENVH86lHz58KiQtkjg5hgajHf+hrcLfn6vxCQ2kX3//neLGiiFtv3Ob//zzL7pw6YYYu7cpSpRIlEmAcnoTKZw2JK9W24m6PHn2QjLzI0f6VtJf9T1ipq5gcNy8c5/OC+AwwpdfUiphhi7Wv6bW9O+bnR94brx95wH9JVSM48ePRWlTJtVMYenztX8HnwKsVZREmCk8NcU3WBmmqTeNbjx8RWv6VaACGRNqefWYuYdGLDtMcaJ+S5dnN9Tig3PzfdlR9PavdwQ/DGzqKzj5hdS7+E6g9YW55sT+JU6+ioJaZmjP167qCYlnOJiFo3s46I3yvcO0iZo+pOrKJjsB/jepX1kt0qP3VuY5q221sjcxO4d6tPG6zKzQAq/Cn9cpoa0RW2gIpk2V1Gnt1GUtzS5cv3nPsZ589QWlTJ5Y06DVp9X/9sa6YZUW+jp76ndw96xG9bA6jo3y4DgreyT4XLpw6bo8C6RInkjsv+OHGTve3B776nkK2ACDg6Z8XrYBBucxBifEqqkg9kXgnIpoyJAhkhHO8Y8ePZKCB9GjR+coIRxhzQcDAwx6E0lahrobOHWGaSGYOwosQIsAPhZgdmj69OkukwOggCPlOHEcGtyeAhgAYgAkgDkpdwFpDhw4IHg7X0mH1PAr4e4dVbPBXb6h9cwGGEKL0sbl2ACDMV1CLTas+2DoO1jY2h8wiTpUzkq9azs7V7VCpGS1p9C9p2/o6ARhZklnkzlPm/l07PJDalcpC/Wt88F5j5X89Wm/LTVCHFze029PjtAXn3+mf2z/tilgUyAcUUAFGNhObziqvl1VmwJhjgIr126jikJytHjWH2hpz7JBrt/D57/SDzUn0TcRPqcHS5vT/wkJLQ5nbzyhrM3nyJ9v1rV1esZprF6ztZhDZ4QPp8M7F0hgz+r7oZV+09Z9VLxCM6nRt3vTrNAq9qMu5+mzlxQjcT7ZRviMUTU/P+qG242zKWBTwKbAR0YBG2BwdKgNMBibSBoxYoTUFOBh/+uvv0pmN//mK0z3wIQPB5gYKl26NEWNGpWjnACG4sWLk7+/v3y2Zs0aKlWqlLyfNGkSNWnSRN4zwADTR76+vtKUEGcG4AJpoYnAWgy9evWSoCiugYXXr18TwAOA1dAKgDaEPkAbAJoEqj8DOHXevt1hohhXAA4cVCfPu3btojx53GsM37hxgwYOHEhTp07lLJyu0FzA/8iRI2vxaGu5cuWkjwgtUtykSpVKOteGX4mwFGyAwbu9YQMM3qV/mNdggGNUOGEd3awQ1S/+Y5Co9evvf1HJ7kvl5LtpUGX6VDgSVUOp7sto+4lbEsAAkOGJkKLuVLrz+DVdE/a7E9r2uz1BUjsPmwJeo4ANMHiN9HbBHykFxk1ZRK06DKKGJdPTyCYFgtzKp69+p/UHr1L0yF9TsSyJnfKZufE0NR+7hWAWcW7nkk7PgvqjYp+V5H/4Oq1aNJpKCf8oYS3MXbRWmoGbJPzD3L33iNYvH0dFCwZdOCOstc8b9dl78ATtFQ7fN2zaI82zdWxTlwb2buWNqthl2hSwKWBTwKaAByhgAwwOItoAgzHA4IEh5rEsoE1x9epVSpAggXRqzH3miQLgtwDOkvEfDq1TpEghNOOjeCLrQPOANsWtW7eEFYrbUusD7YMDazjKdhXgePvMmTP0TvgqTJo0KcWLFy9MapzZAIOrHgydeBtgCB06uywlrGswVKndgeCscG7nUoJJ8MFTvMsGWXywUTAKKgiGAcLmwVUoZ5q4FnMwTp69xVw6ff0xHduziNL/GHyzS8al2LE2BWwKhAYFbIAhNKhsl/FfokD/oVOoV/8J1OnnbNSzRk6PNX3f2bt09d4LOn71EU3bcErmqzedFJzC6o8Q5ha3nadZk/tRzaoOybPg5Ofpd6vW7UhLVzjs0MKW/KKZQzxdxH8uv8kCrGna1k+2G/bZ92yeLR0I/+cIYTfYpoBNAZsCHwkFbIDB0ZHMrLZNJH0kA9tuRpiggA0weLcbbIDBu/QP8xoMcLK1Zv1OWtKjLJXI9oPHqPXX3//QkMUHacCCAzLPn/Onomntinks/7xt59PRSw/pwI55lCVjGo/la2dkU8CmQOhT4OiJ8/SnsKGZNVNazU9H6NfCLtGmwMdDgV5+wtH4kCnUo2ZO6lz1g7O54LYQAgMQHOBQMFNC6b/pqy88Y6qw6ZjNNHvTGZoinMrXE86Fw1qA81n4WUqYMA5lEMINzDwIa/UMT/W5d/+xcAR8imIIXy8Z06WkryJ8GZ6qb9fVpoBNAZsCNgV0FLABBgdBeI9gAwy6AWL/tCkQDArYAEMwiOeBV22AwQNEDE4WYV2DoXKt9rR89Vaa36UUlc3lGQ2GnSdvU7vJ2+micMaM0KRMRhrgm4c+//ST4JDS6d0cLefSqWuP6ejuRZRBODy2g00BmwI2BWwK2BSwKeCgQD8BLvQWIENnocHQw4MaDFjfD118QPvP36Wtx27KwuBIGv6XPvvU2TxiUPqiwYiNtGDbOZo5qR/V+jnsaTAEpU32OzYFbArYFLApYFPgv0QBG2Bw9LYNMIR9E0n/pe/yY2mrDTB4tydtgMG79A/zGgwNWvShGXNW0rgWhahu0aD5YGASwxdD28nbaN6WczIqfZIY0vZzlhSxOInHrql8p9GtR6/oyql1lDihZ8wueaxydkY2BWwK2BSwKWBTwIsUGDt5AbXuOIQal8pAwxvnD5Ga3Hr0mrI0m02//P4nbREmEHN4wARipb6raMOha7RiwUgqU+KDk7sQaYCdqU0BmwI2BWwK2BSwKeBxCtgAg4OkNsBgAwwe/7jsDKVfCGgF/dc0g8JK19sAg5d7IqxrMPQeMIH6DZ4SbClHMBjytl0gtRa+ifA5jWpagKr4pKT/+9//QqQHIpUZSTDD9MvDQxQhwhchUoadqU0BmwI2BWwK2BQIjxRYvnoLVa7VgUpmT0KLu5cJchNW7LlMu8/eoQLpE1ApkZc+1Bi0llaKNN2FKaYuHjDFlFNoJ54U2okHhfnDzLb5Qz257d82BWwK2BSwKWBTIMxTwAYYHF1kAww2wBDmP9ZwWEFbg8G7nWYDDN6lf5jXYJi3eB3VbthdOniGo+eghlbjt0qHj3Gifku7RlSjWFFce6gPahn83pW7zyl9o5mUIF4sun7Wn6Ptq00BmwI2BWwK2BSwKSAocPL0JcqUuwolj/c9HZ9UN8g0GbnsCHWfuZvypotPGwZUCpAP+2QY3awg1S+eLsBzqxHRKoyh3/74i57c3EXfR45o9XXL6X/97Xf68osvbN8vlilnv2BTwKYAKPDHH28lIb780hZ2skeETQGmgA0wOChhAww2wMDfhH31HAVsgMFztAxKTjbAEBSqefCdsK7BcPrcZcqQozIlihWJzk6rF6SW/yNUlGJVGifNJPgPrEx5fowXpHzMvrRo+wWqN3wDlSnpQyvmjzT7mp0uFCnw8tUbSpmpDCX9IT7t3jTLcslwpFnNtxNly/IjzZzYz/L7YeWFbTsPUY36XahuzbI0oFfLsFKtEK3Hx9J3IUokO3ObAiFMgX/++Ye+jJqZ3r17R/cWN6dI3wSN+bX79B0q1mWJrO3BsbUobeJo8l4s+7TpyHUCwICwd3QNyiDMIgYnnLnxhLI1n0OJE8WlKyfXBSerQN+dOW8VTZiymI6fukATRnSjiuUKU65CtSha1MhBWrMCLfAjSDBm0nyaOHUJtW9VWzjgLv8RtMgzTZi/ZD217zqcOrapS22a1fRMpuEkl2r1OtOJkxdowYzB/zl/aFt2HKCmrf3ounD6Xql8YVo0c0g46TW7mjYFQp4CNsDgoLENMNgAQ8h/bf+9EmyAwbt9bgMM3qV/mNdgAHmiJcxLz1+8otNTfemH2JEtU+zSneeUsfFM+R7MI7kKnYT5hLYVM7t6bDq+6ZjNNHvTGRrcrw21b1nb9Ht2wtCjAMYTxhX8Y8BPhtWw//Apyl2oNuXL/RNtWzfN6uthJv26jbuoTJVW1Lh+ZRo/vGuYqZenKtK60xBav3E3TRjVjQr5ZJfZfix9h8ZkzvMzASw7tHN+qEhTe6pfPoZ8jMbWx9Cu0GxD/hL1adfeo7SwW2kqnSNpkIsu3nUp7Tp1W77/c/5UFOW7CLRy72W69/SNjKtdJC1NaFk4yPnzi+NWHadOU3dQzZ9L0qxJ/Tna49cXL19T1AR5ZL7ffvM1zZjQh3JkS09xkhWk6NG+pwdXt3u8TH2Gi5b7U4++46lOjTLUrUMD/WOn31eu3abi5ZtSsqQJaf2ycU7PQvNHT7/x5DdkKg3o3ZI6tfENzaLDdFmTpy+lpm39qFeXxtSzc2Ov1dUb61XOgrXo4JHTtGfLbMqRJfgaTF4jXhAKTpqupAQX8GrndvXIr2eLIORiv2JT4OOkgA0wOPrVBhhsgOHj/MK92yobYPAu/W2Awbv0DxcAQ/V6XWjRMn8aWD8ftSyXyTLFWKMgsBe7VstB3ao7mJCBpXX3PEmtyfTg2S90dPei/5zElDu6hKVnNsDg6I2PHWCoUqcDLVu5hZbNHU7lSheQjf6YAIZIcXLSm19+pftXtlGM6FHC0if20dfFaGx99I32cAMHDp9G3fuOo1qF09DEVkWCnPuLN39QS2EGccWeSwHywLrepuJP9NUXnwV4ZjWiVPdltP3ELZo9pT/VqFLS6uum069Zv5PKVWtNOQWosHXtVPr888/o4aOnoQowTJq2hJq1G0AtGlejUYM7uq37mXNXKH2OSpQsSQK6cGy127Qh+RDaaWcviLqkTUFpUwcdsArJOnoj77ACMHhjvdq4dS89efqCihbKRdGiWBdQ8kZ/eaJMFaS09weeoKidx8dGARtgcPSoDTDYAMPH9m2HhfbYAIN3e8EGGLxL/3ABMCxZuYl+rtOJMqeIRTuHV/MyxdwX73/4OlUUJhlSp/yBTh9c7j5xKD+FSYrbdx9KZkXsmA4zEp6swm+//0Fv3/5JkSN9ZzpbHIIgofnpp584vQMA4KsIX1JgNmMfCKYLyowXJ2agNqr/+utvwkYKZZkBGJAGdfvss0+d6oYf7pjUoPP9h0/o008+oZgxogZ410zEn3/+Jeto5f2nz17Sd99+LfvXXRm///5WczweFIDhl19/o0ePn1GcWNED7R/UA+MCAf2pBsSjnZEifqtGB7i3Sgt1HBoxgd31XYDCgxih1kHN4tXrX+jxk+divMYwRTv1XaN7Mwwbd+NYzdNqv6rv8j3G/iPRvlgG495sPZDXs+evhEbGd/J75bw9dUU7sfHTj0d9/oHRw2hs6fOwf7unwLnzV+nH7BXp268+p3uLmgU6h7vPjei+APav3XtBL399S/C39EPsSBTx66CZXtKXdevRK0rl69BWe35nL0X8LuT8OK1YvZUq1WpPtauXFtoLfWVVzAAMZsc2MsS6iXnK1XrtaYAB5WH/ARNPgc35etrz77//fidv9fsFfu7Jq5X5KjjlhvS+DHXTAwwo89adBxQzelRtL+CuDWZpgT0IxqCr/ZiZ9cpVPbB2ot6uxqv6nrt1SE33XthRu3vvEf0tzLTFjR3DcK/H6ZHn8xevKcr3EYO9Lllpi5VvGnXFPBFVAClPn72QgCT2sC/v7eNmOF2tfpPu6IpvE8IO+v5BPMYPNK/cBfTF/QdPKFbMqHJ9dpeWn6E8mMLDvtddeCJo8erVLxQ7VrRA132zY91defaz8EEBG2Bw9JMNMNgAQ/j4YsNXLW2Awbv9ZQMM3qV/uAAYQKIYifOJDfNL2jSoCuVKG9fLVHNdfFW/1bR2/1XyE/bsO7cNGRX9MlVb0eGjZ6hHp0bUtEEVp8rsPXiCKtVoJw/wZw+vlAwbbMLbdhlGM+Y4bFHjBRw6+vdsTk3qV9GYOrDXWqtBNypZLC9NHdvLKd/hY2fTsNGzpemBujXKyme1GnajLdsP0Prl42nhUn+aOnO59Dsxe7Kx6Yg9B45T5ZrtpSR5/jxZaMzE+bTv4ElZlyoVi9DQ/u3If/Me6j1gIl2+ekuWARNE86YPcmJY4pAzasI8GjpqlmTWckVh5gd+BPSMn2Wrtoi6z6Ijx8/Jsgr4ZKVmgm6FSjcKYCIJh7nOvUbR8lVbtbwh+dalfT3KlS0DF2UIMIBx7zdsKg0c9sFkEujcpEFl6tutmdvDK2c8a/5qWr1+B0GCFSGuYEQXyJeVqlcuIa8yUvzh/pg0qgddunqTxk9ZJA/KeJ47R0ZBs4HywMzp3737h/yGTpH9BNrisFe6RD7KLkwG1Gvay5SJJDDm2wqTQ6Ajh4zpUtLksT0JVwQwrODbAsxb2DzuO3iS1haU179HC3r3zztq0qq/NFuAdyD1OnV8byf6It4sLYzGYeEC2WnP/uNaH6IfIkb8RkrXnjxzycm8ldXvCXXTB6M6wAcLfwv4tlq2H6SNa7xf0CcbjRNmqeAHhEMxYWbk5OmLtHn1ZCcJXKbr5599Jk16YXw0atVXax/6E/5AVi4YJbMyO46R2Ey/cv30VzAh+wyaRMMHtqeXL9/QyHFzJYOJTbhYqQfGaH8xRpEnQBi0CXaj8+bMRM3bDaR6tcuL8dOcAEqm+qksffP1VwHMm/Eco5/DwLwYO3mBtM/Oc0vmjKmpoW9F8q1ZzqlZgdED4GGmXFU02qtjKzDQwqkg+4ekgE+JerR77zEa3VQ4YS4Rdk2Y9J69l4YuOUTVq5SgOVP8Qqz3KtRoS/vFuohvACFVisRyrc8jvgMjE0lWxjbyg9+dbn3GaPM45t+ypfJT7y5N6IsvHCYk8xSpQ6fPXpEMQ7yDb3HJ3GGUO3tG/HQKXXqPlt8V9hkISNtOmIZk85BXr9+hRi370M49R7X3UOaYYZ01s3XaAxc3l67cJJgk27xtv0yBtaSR+HaXr95G5y9co81rJtPXX0Wg8VMXyfmjTfOaVEDMr9nyVZfpYcYQdFTDhKmLqd/gyUKyPafmR8nKfKXmhXur60hI7cv09cJvBhi6tK9Pf/31l/i9TOtb7LEmje7ptA7hHSu0mL1gDXXuOUobs3gfY2riyO5yPAS2XiG9GjBmyv3cin4Sc3TFMoVoqNi/YT1HwNiZNKanXBf4HXfrEHxMnRJr/rxpgyhd2mTyFaw10J7CnpbHLR7AR4X6HSDu+q271KrDYNqwaQ9+yj0kxt/A3q0oTuzoMs7dH6ttQV5mv2n4ZylRoZlcJ0uJfXu/QZPlnho+SFaIbwO+FxAw9rGPHdq/rfxt9pt0R1doWB08fJqWzhtOcxeupcXLNklaQuuqfcs6lDtnRmrcup/UIEWhmBeMziwwkTd30TppyhJzHtZTn7yZqZiob4M6FTQwh9d2+AvLI/Lu0muMmKMuy/ZgTEwY2Y18xLlCDWs37KTm7Qdqe2M8q1iuEA0TZ414cWNqSa2Mde0l+ybcU8AGGBxdaAMMNsAQ7j/mMNgAG2DwbqfYAIN36R9uAIYe/cbRAMG4LZMzKS3oWtrLVDMu/uilB5S37QK5IYZKMjbUIRHAXO7YfaRk6h7ZvdCpiFYdB9O4yQs1yUccnrLkrSYZm9i4FymUgx49eqYd1mpULakxQVev20Hlq7eRAMPqRaOd8mXbxiMGdaBWTRyH9iJlG9PWHQclExxSYAi1qpXSDutOGYgfYLIWLdvEKRo0YkYKDgnM/FPvwQg8uGO+9h6YufMXr5e/cWiCNOMuwbxAW+FT4dTBZZqUEh+skRjMekhWqwxy1QcDpKbhRBN1AK1yZs9AlwVjgw9pC2cOpsrlHWY8jKTgYeMY5eFdgBioF8zzIDSoW4EABrgL6Df0HwIYpBEifCHbxfQBDUALBO4PlIV2o20J4sWSh0s8B/1UUxVwdLh42UY80hgt5y9el7/xJzAfDBcv36DUmR2M2B/TJJOmv84IBhQOuAiXT66lHxLFo19/+52+i+UwM6bWjccH+htADNeZ45GHqsZvhRZG47CyYEzfvvOQzgnmE48LlA1zIyfOXHQCGKx8T6inUTCqA38L0+esoIYtHFLI2TL/SPHixaQDh05ph95TB5ZRmlRJZLZps5Yn9MvhXQsoU/pUWm3Aan4AADmESURBVFEqXd+9Oim/pd5+EzWQBgBPxgwC7BGMIivj2Gy/ahXR3QweOYO69h7jNAeAzgAYrNQD2arfNZgUkLBVxwePUUgjxkzsI2sCWqiB5xh8P+ocVrVuR1q6YrNM6pM3C70SYAiPXdUEjBl6ANgoX62N4djCN2sHaxRYsHQD1azflVLEj0LHJtax9nIopf7l9z8pae0p9FpoRuzYMF0wuDKFWMm1G3WXAAPWHcyh0Ibs0LoOYe4wAhjMjm1UeKIA75oLs0cIWEu+/jqCxvgHML1KrPvQLoD2BIQY8P1xHUYO6UhZMqaR76p/MAfMEwxCXk9QT2heNKxbkWCy6Kc8VWVyrEmZMqSS3w0zBqcL3xJ1qpdRswtwf/TEecqa74PmKuY6/nY58dNbu6XkNK+L7IMhQ87KkgnZTwCTXQVzXQ38bNbkflSzainL85WaF+6trCNYk0JqX6avF36r+yD8xr4niQC2sQ6hLgg3zm2k+P8yXa3M3XAgDcEUBIAVcYU26c49R+TYQTmXTqyhbbsOkav1Sr6o+3NWaDalE5pNasC8ffTYOa2+G1dN1AAqd+uQkQ8G1j5D/lhrHjx8qu3z1PXg4NHTVLRME20PAR8j+w6ckL+xzukFAdT68r3VtuA9s98070NRF94n4v3unRpKwQ7+zvBNFsyfjfp0bWrpmzRDV5SHoK8D+p7nMKyLXL8NQiCpSMGc8p3Dx89Sdp8a8h51xJg8fvK8NpeMHtKJmjf6WT7ntZ33lbhmEnueYycuaGPiyul1lDhBXJke4AhMvCFg7vk+ckSn/dJe4ZMDgKqVsS4zs/98NBSwAQZHV9oAgw0wfDQfdRhqiA0weLczbIDBu/QPNwADTLIkSFVESF/9TWv6VaACGRN6mXIBi6/cbxWtP3iN2raopUkKBUwV/BiYGUiUuqjMSN1QQxUZjiFxYMTBB5LvkNCDRgAO5ZtWT9IcwfLBBJnwQS2oAAPygENrMBWiiE08JlWjwAcEPAPzdYCQAIse9XsaPXEedeg2Qr6CQ+Tk0T0ks/qQYHDkKFBTxr+4u0+qQUPaCY5BcbgAoydDuhTyOcZHLuF0GQeabh0bSI0BqDrDkTPCUL+21LZ5LXkPe9GFyzSSBx4VYGjXdRiNGj9PHjhXLx6jqXqPFpoWbTsPlWU+ur5DHkqYfuzk+Y8/3tLXMbLK/C8eX6NJA966+4ASpy4m4988PKgBHzJC9wcOEME4WTF/pNQE4cd8QFaZJMxIQZreXZtQl3b1JaBx+txlypCjsnz19oXNUsIOEqtoL8K2dVMFEyCzvIdmR5XaHeQ9M2/lD4M/vQZMoP6Dp5AKSEHKrlWnwTR+8iLNobnKCMeYmz6xD/2YOpmm8YGscRBdvmCkdLiI9HGSFpRjdv3ycVS0YC5ZuhVaMHMfL+rHITMS3PlgsPI9ycoZ/HFVBzhgTpymuGzfjIl9qXY1BziKb7V6/c4SgAKDY9fGmRKYNAswcBWMTE5YGcdm+5XL01+ZAYH4KhWLSieS8ePGklpRVuoBzau8RerKb2zz2skaIxNaLNCwQeAxahVg4HkNc8aB7fMoZfJEMj8uEz8Y0LFCD6OxJTO2/1imADN7hzb0oaZlAkrJW87Qwy90n7GbRi4/QsUK56J1S8d5OPeA2Zk1kWRlbN+9/4gSpHQA5ItnD6WKZQvJgqGRU0CsqQDWVQECZtCpDNeANXXEuPLBUKBkfQlg+NYqJwH2Tz5x7A3GTJpPbTo51tSLx1e7NCWINSZv0boSOMf8Mm1cb7mGYr0vU6WlJizgCmCAlkILIcEMCe4zh1Zo1VeBRJiPwdxgZb7SMlJurKwjIbkvU6qk3aoAQ59uTeV+AX0B5mrBUg0kCKOa47JCC96fTBNaiKzdCiECaMFgP7Nv2xzK9tOPsi5G65VWSeVGZcpjH7Fr00zZ79A8qNmwqxSWANMYfQpBDnfrENePnTxvF2AHtFcRuG4YZ8uFWTLeD726v58ifPmlBMfApG/VtDoN6ddWloUzSI0GXeTarQeylSZot1bbYuWb5n0oCoPgx1TxfaQVwgpgnLsyqWblmzRDV5QN0KBQ/ux09/5jKiL2miwoNGeqH1UqW1jSrZwA5df576JOQrMbmsYI0BaGJnLrZjVo+ID2Mg5/eH+L88DWNVNkvHp+gHYMtKwBGmCPhf0i9v4sBARt9yQ/lpD7LmhXtm7qADGguZE8QylHfkLrKX/erMH+7mVm9p9wSQEbYHB0mw0w2ABDuPyAw3ilbYDBux1kAwzepX+4ARhApv5DplAvvwmUMWkM2jPKsWH0Mvm04hdtP0/1hvtLJviVU+uFDdRI2rOQuOFDgrp53r3/GPkUqycZuHcubpFMvk8jZZDFz58xkKpWcDC6uT4tOw6SzGGWrueDjdGhiTf8KgOCmarqIYDzNrqqBwQGDJBOZcLrHWMnTVdSHhz2b5tLWX9KSzAfsWrtdkONgOWrt1DlWh2kNPWt85sEcOEABsDA3b1pllOVps1eIcw29NVMJKkO8dYuGUvFi+R2Sg+zPzg0rVo4ikoVz6cxzBlgAJMmXnIHw2bJnKFUQaj1c2Ang8UL55H2ezlef4WpCZh/8uvZUh7I+DmPe5h9GObXTkZzf0DqdKf/DE4qr8ycZ5AJphtwsOvVpTH17NzYKS3MB8C8FTNvnR4qPyAJBoYTDvss+YXHaPe2nQcpQfzYlCdHJicNhu0CAIJ5Gw4ACCERq0ql4RnU6GFea0j/NtSuRW2Z3Aot3I1DIyYwH8q571Cgme/Jnd1vV3WAyQ6YRoKGCcakGuA/JG6ygjKKpUeDCzBYHcdm+1Wtt3qvMiDAlIF0P4LVenA/gfnVvWNDtQgqUbE5bdyyVxujVgGGbD7VJSMS5i9g1kINPAfyt2WFHlxnFbxS87bvzVNg6arNVLV2R+GL4Qs6LrQYYkcNOf8G5mvlSHns8kPK08ahQbd78yzKmTW91SwspzcLMFgZ2936jqVBw6cbaj4y2Awm5Yl9S2R9gwswqNoLPL+phOA11Whvwuk4DwAAty9udrK1DuY11joEVwADgIjYSQvINOePrqLkQgIdwW/oVOrZfzzB3ApAC6vzlczE4I+ZdQSM/ZDclxlUS9NgwPp9aOd8JyGQmfNWUf1mvSXIArDFKi14jwZJc6ztHNjhdpZMaTW6BwVgOL53iWbaCHm/fiPs/Md1SMCf3L9UmhJ0tQ4hvR5ggGkfmGziOR9pOOQr5kswx7V6yRh6IcCX4sL8EMKj6zud9vTqfg/aehCacBVUgMFMW6x807yXQdk7N85wMmFmBDDw94T0Zr5JM3TVr9kw3Tht1gppgpNNNqK8Rcv9qbpvF2mqaf0yB0iLsQf6wNQrNGA57Np3jPIXrydBE56P+PyAueDGOX9NAAjvMFABzQ1oabAGLOaz43sXa2aWkBZ7Z+w3u3VoIM3dQSgLwcy+Xya0/3w0FLABBkdX2gCDDTB8NB91GGqIDTB4tzNsgMG79A9XAANIlT5HJYK0XJsKmam/r2Nj6GUS0t0nbyhHq3n07NVvwqZ6F+nXIKTrBJu3vk16SjMHbD4INorHTlogzSkM6tNaMn+Z6W10mOA8mEkdVIBBlcx2124+IOCQq5p2UqXe9VL+zCzfvn4a5c31k7TvD0a/EVNClU765eEhoR7tR7Pnr6GRgztQy8bVnap27cYdSpa+lAYwwGwSDnYIE0d1D+AvYcKUxVIaD34roC3ABzuVSc1SuMgDNC1RNI/UFkB7WXITzwILp85cpqMnzkoTMRcu3RC2dLfKV9QDMQMMrK2h5ln1X3MwLM0VK0l+qa0BmqMuapi7aC3VadRDY96qz9R7dgaNOBzwqlctQT5CEwJaMqpTP7UvmenD+fBBn4EPjgeYMGTkTKmJARu9ajBDC2buG41DIyawUd/xt6Ca49J/T2q99Peu6sAMbAbx9O9x37BmSXABBqvj2Gy/6uvNv5kBweagON5qPZjZeGDHPE17gfNiaV8GwawADADsPovskIiHlCSbouK8d+w+Is2tMUhqhR5GY4vzta/WKQDpYTC6S+dISgu7OTR9rOfi+Td82i2kwxfvU4sm1WjUoI6eL8AgRzMAg9WxXapyC2lHHmYFK5Z1AJtcNEzEwAwlwttnxyTAHVyAgUELI3AV5dRt0oPmLFhrOO/jOQILDRgJCeA5M6x5reF1kU0kIQ0D7AP7tKKOresiSttHMAhudb6SmRj8MbOOqMzpkNiXGVRLAxiMQFZ13/Tk5i66duOupb1Qh+4jaMTYObJYaBWUL1OQ4GMrlzAxyT49uE7cX6o5RH6mXlWm/J/PjwfYP/G+cPXi0VSyaF5Ng0G/DiFP3newBgOvNQtmDKIqFRyawGrZfD9y/Fxq33W4FA6Alqg+AJRBYO0K1rSTkeIP/PHs2zpHMtDZ3FNgbSleOLel9Yr3MgA47l3e6gQcGQEMVr9JV+s72sh01TPm+RtkZj/TA4I2JSo0l3tjvVDMPaH5sHPvEbp56z7duHVPaodAE1sFPPn8AFNKoKsaWEOH9wi872pUrxJNGNFNTep076nv3ilT+0e4oYANMDi6ygYYbIAh3Hy04aiiNsDg3c6yAQbv0j/cAQw79hymgiUdEq7T2xWjqvk/2Cn3FilLdl9GO07cks6LIc0aGkGVMrsupHnixopBcZMXlIxk1gKA/dicBWpJ6Sp2uqrWjTfsbCbIHcDQvttw6cDVSIOBD+lq3kb3XB60A3Ao4aAypf9+ecJJ2ogPkgAYcgkHk59/72AWXjqxlpIk/iDxhLzgCPfbmNlktncubaFqdTtJXxNQ04aTZDXg8ILDLredD15qGqN7OEmEo1k+2KkAAw50PYRUpOpMG3ng8OfXq0UAZ7L6/Ndv3C39jBw8ctrpEd6H/VojgGHYgHbUppnDjBS/xP4WADCUKeFDX0V3OL67dmYDJRSaBmrYtHWflNLjg5n6TH8PW8uwt6/axUca9Odk4XQxdsxoThoMevv4fCDlgz7nbwQwWKEFM/eNxqERE9io78x8T1xfo6urOsBpMxyTGoEfyIdpwiZLXAEMMGURPZHD3JdKVz3Dxuo4Rh3M9CvSGQVmQOilGK3Uo6/QWmAQgH15qGXNW7yOajfsroFg7gAGOHUs+3NrzY+MysxT89Tfq8CSWXoYjS19vvZv8xSACZ+MwmY+xnrXatmpW/Uc5l8OoZQtx2+l6RtOSWDq2J7FTpplIVSkzNYMwGB1bLOkeWD1Zk2k4AIMcMwLx7+q6R21bGZENmlQmcYN66o+0u5HjJsjzSfCxBJMougDa8W5AxhWrtlGFWu205iVbEYQwMeNs/6SKWtlvsL67yqYWUdCel9mVDdmwKpagpwO5oFYowL7qpOnL2qmgjiN0ZX3QjD313vgRBoofKTpA7QaoHX52Wefykf69Uqfnn8zwKA3bcXPWYiC/We4WoeQntdY7DuyCm0K3kPyPpnz1F+btx8gnZfr4/W/YWIUQAq0h/UBa7WVthTIl03ThNXnpf7m9cpoL8PpjAAGq9+kGbqqvsFQNn/XKsiHeCOAAbTpN2Sy5qsM6RDwbWKfaQQw6M8PSD911nJq3KqftkdgrUc4km/WwOEDBun0wVPfvT5f+3f4oIANMDj6yQYYbIAhfHyx4auWNsDg3f6yAQbv0j/cAQwgFzvTw/3qvhWoYKaEuPVKaDDCnxZsOy/Ue+MKe65zKVqUyKFWDz5kYROdLm1yacMcEmTs3JcPNqiQXjMAcUzHwgVykP+KCeQOYGApQE8ADHoTTGYBBmgw8AHViJms+myAFGalWu2kKjwfhNFmDmwvmQEGBj/wHHZ5/yf+GYW4sWNIvwbuDnYAL3YKyegdwtEhzA+xczuWUjfKl21Y4xlsTVcsXZBSCeeeqN/0OSulY07Vtwcf4tT+4HxVgKFSOWH/9l8zWexrg9PhymrrZgAGpAcz4sz5KxK4gZkBOPlGYC0YtS9VRjjSqAf9HFnSIUoGPcBglRaumPvI3IgJ7KrvAvueHLU1/uuqDpynngGPXMCY+SKKw4QUgy6uAAbV0alKV/4eWCLU6jjm1gTWr5xOf3XFgLBaD24H28RWyxkgGFeQruYx6g5gGDVhHrXrMkwDGBhIRH4AGpMkjq9mrd1DKwfMLA5m6GE0tvh9+xo0Cqxat50qVG8rXx7dtCDVL/FhnghajkF/y2/+ARqwYL/MABKzkJwNrWAGYLA6ttn0CjR5Kpd3+GIwak+WTGkkyB9cgIGZ2ioIr5bHTEBVs0B9jvsZc1dSg+Z9DEEK2Pn/JqbD75E7gAHpoCkGeoGBDq09+BPq272ZNJOCcqzOV3jHVeA531v7MqN6cV8YgTlXrt2mFBkdGkPYJ+4T/nCKlnVI7JvZC3F5b9/+SYeOnaFde47SkhWbNEe9kP5nzUSe53m94nf1V967utJ+KVi6Ie3YdVgzC+RqHUK++n0H18FoP4R5H//BGOgu1hyAJtCeGfqvaUp9PfE7ZfLE4nshrb2cBiYVM6VPpQEMZtqSXuzjUT8EM+uVq70M3jcCGHgcmP0mzdCV/RehTATemwYGMOC7TJGptAQSMLf61ipLP2VIQ4kTxZFaDPAlZgQw6M8PKFMPMPDabLT35z4GU3WrMO8ZlLGOMu0Q/ilgAwyOPrQBBhtgCP9fc9hrgQ0weLdPbIDBu/QPlwADSNaq42BpZ/Pzzz6hxd3LUOGfHI47Q5OcjUdvormbz0pVaDCPswgfAaEZ2JwHDkCZBVMADooH9W1NHVrVkdVQmb1b104hH6G2rgb2Z9CsUVUaM6SzNJ8AMwp6cwTYkCcUTqUhUaQytF0xVdUy1Hs+xOsPCGo93WkwAGDgw6KRPwG2d88gC9tlZWkvtS7MwGGAQTUTwHZ91fTT56yQPhjgxwKOpfUHO0glQlIyR5b0Tg6a4ZTQp7ivdFKpHrTVvHHPTi9hvoLt03Kapm39pIkDIw0GtT84vQowgJHEWiCd29WTTng5Ha78HTHzVn2m3sNMzS+//iZ9OHz9VQTtEfcpImCW6p/3/9B3sbLL5yojHBHcd8xMl4nEHz3AYJUW7sYhHzRVO/n6vuN6BPY9cTqjq6s68GFb/00hD9Ue8l1h3iBWjKhaX2351wEhlwWp+loNHKr+Kl2ZWcIMG6vj2Gy/RojwBVfF6eqKAWG1Hjw2jDQ9eDzzGFWlhPXAaYMWfaQGkTrHsBmq8cO7SpBCbcBOAQL6C/8OkGyFaQ8r9DAaW2re9n3QKMC+c/C2t0CGAQsOkN98B7gwX5hSqerGlErQWun+LV6fVOl/I4ahlbHt27SnNBlYsVwhWjxrqFMFALiPn7JQ+AiKpJkRCi7AoK4NrBXBhcK80/fxckumv95nEafBdcduobFaqqEE/04fXO6k3ciaAEjnDmDAc5ZGhwT/mIkL5F7m6un1lChBHDwmq/OVfMnFn8DWEXW/ExL7MqNqMWPZaB1iM4nQlISmqxVaxI0bg4YJTRW8y/6TuHxm0LPwAeL16xWn1V8ZYED88zt7KeJ3H3yygH5xkhaUYweaqtCcdLUO4X1eW3jfwb/1zGcA/jCfBGfBMNV34eJ1aYYU4POz23uczDQhbd/BkwigSrcODZ18g6BMNVhti5Vv2tVeBuUbzRdWv0kzdA0qwMA+VEDf+1e3yXMU023x8o1UzbdzkAEG3ncZ7f3ZR8RQv7ZUuriP5vTZzL6f62dfPw4K2ACDox9tgMEGGD6OLzpstcIGGLzbHzbA4F36h1uAAWSDk14460WY0LIw1S4SOgz+V7++pfrCofOGQ9foi88/o7VLx0lb9LIiofgHh5sYiX3kQYuL1dv0ZcYnbNNOH99Hs9EKR3apfiorX2MVZ/ZLgEhmWOKeGb64VxnanLeRNgHS6gMfblTmH9KoB+7AAIZBI6ZTtz5jCSAC6s0HT+SRr6iv9JPAZoPYURzKUJ3rQZIxT5G6dPrsZc1EEg6MGXJWklJondr60oBeLfGaDMdOnqcseavJ+1sXNhG0GPQHO2aEoF5nDq1wMqUBx4F79h+nscO6SGd2/2brdGFtElViCwmglVGkTGPZx0EFGNhJNMAUOHj8PnJEWbY6Bph561Qp5UeeInUkSMJ+HfjRrr1HKX+J+tIvAw7hf7x9G2yAwSot3I1DliZVGdf6vuO2mPmeOK3+6qoO+w6dpDyF68jkGBeqlDw7FFZV/pkJqNrKfv7ilRiblSVTDBkZAQxsMszqODbbr658iLhiQFitBzMEMEaP7lmkfdcq/dQxykwY1f7z4eNnKbtPDUlrdY5he8xgrm1bN00z1wGgAt81GEo8rq3Qw2hsycLtP8GmwMDh06h7X4dPgC7CXFL3UDSX1GbiNpqy7qRsw+SxPal+rfLBbo/VDMwCDFbGtmoOBPbaYwpAEwHfao0GXWjpis2alhDimSltZNcez9XATFQwmtkW/MtXbyhhqqJy7Zo6rpeTiUBmIIK5eO/KVlJBazVfzH2J0xSXecye0p9qVCkpH796/Yv0EwB/TAiBAQwqGIH07HMF9whW5yvHW8Z/zawjvF6ExL7MqFbcl3im+q/644+3VFIIlUAbgLXsrNACewk2S3nz3EaKFzemVjz7zildIh+xs18GGHi90hLrbng8IVrvPLrXgAlSAwVrBUzqgTnmah3C+wwoMMDAvhX0e0gG8TEmMZ7uPXxMiVMXQxa0fvk4Klowl7zHHxZmYQEV7YHBjdW2WPmmXe1lUA0jgMHqN2mGrkEFGFSNY9VRNupdqlILuZdX98Ouzg9oq16DQfWtcOrAMs33kgqeod7p0iS3tO9HWXb4eChgAwyOvrQBBhtg+Hi+6rDTEhtg8G5f2ACDd+kfrgEGkI59A+C+Ycn0NKyRD30i1JtDKmw5eoPaTtpO1x+8lJJTC2cPoVzZMoRUcYHmy5J5SFjQJxttWjXJ6R1VShrP4ZQXh3Y4P4bpHkjtLp0zTL4DTYXYSQvIeKhzg0l3RjDh9/0/e3cCJkV1rnH80ygMw7AoiDIQFlnFi/uuicYEXAGXgDcqiuKemKuCRjRGY0w0btG4r+hNMPFJABX3LXivS0S5XAiIG0tAQFFUQERwIeerSrWn256Z7umq+Xqq/5UnUt3TdU7V7/Tp6q636tTfw4Mt+iLrgEGDhN7bHRKso/5APHrEwcG6T3JXD2hgoAc43pr5sNS0rg6ej67S0B+No44dJh07tA9u6hodmPB/IPqBhLrsudt28s9Fy4IrZbQw/wqA3B92ern3dnscGRys1Mu9jz7q4GCYJf1RpEMJ6eSfMRk84f3H/0GkB5z1YOj0/5+bucGzvlS391du/OcfHjYocxm63x5RcdEZ39FBU73yQMff1vbW7f3RiIPk88+/CMYX1rBFJ//gbVSO/+/4Pz4g0U0Nzz1rlPTv21Nef3NB5n0UXQXjh0X+gXAtK/eHflR+7hUMxVpEB2vyBV1RuKLvCz3QccNV4+RVFxh9Z9Dx7gbcuwQHnKP10H8b6k/+a/35+tZh+HFjg3bU9+BpJw0PzhJ+/qUZ8vBjzwVFvDz1Pjc0QHgvGb1S5pQzLw2eP2zI/sGZkY8/9ULQdlF9vuv3Dz1JprohKfTm3cOPGBycfVzM+7jQdo3qzv23vgMQxazHqtVrgjMJo/foke4mtO8tX+GuCno2E6D679FjTxoXDD+m63PCyMPk/Q8+ynjqc37AoGP69xoYHqTUMxrVda07sHb73X8NXLWvPfvIXUEoWIxHvvdWCxc4M8Uj8PtbJsjZ54dn2g/Zs7fbt+8vXbdoE0/heUqZveB9GeP27c/Pfif46x/u/I0cPTzcv+R5eaJPFRowFPPe1v37oGGnBgeT9fNQbzyvB/b1yjv9zNXPp9mvTAoCdN04v//qgfBz/+uErIDUB/Dvf6R9b/TIw4PP25vvuF/OHHt58FLtpxqw6r73jvETg+fqC92j8qMrKfSxXuFXXV0lTz3zUuZzQZ9vKGDQbe+7w5Bg/6yvz9e2/vY2tP/XMuqbGtqPJPm9LN96+QGD/l0d+/ftIc+6YCH63vTaqw/IZu3bBosXY3H8qT+XP/754eC7lw6/tUXHzWTa9NmZNvavUMm3v8q3vv5Bef27fnfdfdeBwVV/jz7xv8Ei/nB69e2Hcr93+N/V9DuV7g+muRM5dD+qk/+d6tdX3SG/cPfV0kmDju7dOgcnffx18lPBc49OvEkO+EE4pFHwRJ7/FLstxfTp3O+hfvX5Agb9ezF9shDXxgYM+l0xuhJFgwT9Xrtw0dIg6Iy+l+r66lXZY848Tp557uVgOCN/365/1yk3YNDnovelfq7plWArPvw4851BvwtOmvC7IJwq5r2u5TKlR4CAIWxLAgYChvT06vLZEgIG27YgYLD1b/YBg/JFZxPp/Na1m8lF7mzHEfv114exTe99tEZ+7YZN0Bs+6rT/vru5mw5eIj26Z980N7YKCyzIP8PXHwbGX/x5N6bu8GPHZB2k1L+fedrRcuWvzhb/oJjeAFHPmI/uG6Cv0wOXOpas3rDx+it/FvzQ0uejG9hOffxu+Y67AXND0zNTX5bB7gCHf0abLuMfnMi9giEaN/pvj90l390rHK9+sRvO4ZjR52cFH1qOHjC+7+7fypadOujDYNKyTz/rsuAHcPSc/qsHo/c/eHTmCobob7qOevDS3379gaLjRJ886sjMlQnRGZH+mZB60OC4Uy74xli8GtbcedMlMuh74dBBUV25//pXikR/0x9Gv7zgDBl61E+DAwHRQfHoLD6/PaJlooOv0Y2D9Xn94abvAb0sPZr0x/VvLz1bDj/6LMk3NnP0Ov1Xh7TQMO/6myf4Twfzuo43uuFnqltVZbWlfyBcXxidHR6dSRgVdOGlN8gV19yVNSZ2MRb1vQ/nzJ0XBBvRD1YdqmP23LeDm5/7bRetSyH9KXqt/29966DvwXN/fq3ogTJ/0vfFRPcjNwoX9G/qfNb5V8pNt/0581I9GHi7u4n2SDdEkm6H7zrxwadk9BmXBM/7wwEU+j4utF0zK5MzE900UoOvC9wN0HOnQtdDl9OzC48/5ULxb3KugZgO6XLl78ZnhWB69cEw1yf88FNDFr366Kjjz/3GZ0y+srVO/WzT4eHUWKdiPPK9t6JgMyiM/5Qs8OAjf3OB2y/lgxUfS02rFnLRsXvJTw4L9wMlF/7vAr76aoNc/ie938JLwTP9+nSX22+8xPTEgeheFP7NjTVw0xMA9L2qZ/xGU6HvbX29fn5oaDP+Dw9Eiwf/6mfHjddemPVZpGe377rvjzL7s3xj1vuFREGxPhcNP6MH9vXgn+6Dc6foBr25z+d7rFdfjLv4+kxAoEGF7rt0GBXdpmiotGi/6A8VGZUXXf2oj1cteynvVRPFfF5F5eb7t5D9SFLfy/KtT3QA9hfjTpNn3H2T/M9NPbj7wP3XS/eunbMWLdRC+6ZezaVXQeROOgzNOT85LvN0XfurzAv+PRMdlNfvKPu673VRIKV/1u9j1115now6Zlhmsfr2Q1HA4AcS+v1u5MkXZO4hFRWk3/N0iNHogJvuD2647U9yzr+Dzuh1anaFe21D4YK+vtht0WUK7dP5vofq8jrpNuqVfrn3fiimTxbimnuzbL3h96+uuC34znzeWSeEK+P++8TTL8jBR/4468QOvQL2P0edl/V9WwP/e2//tVx6xa3y3/dNCZbXe6rpa/P9ftAXRCdm+N9j9SQaHSZVh471J/1Mveqyc6R9u6/D6kLf6345zDd/AQKGsA2jzzv9bKikacqUKTJ06FAZMoSAoZLavam2lYChqaTz10PAkN+lyZ5dNON2Wfnuq9K13wHSrmPvJqs37or0DLxz3QFQHYpGpz0HdAmuaBixb2lBw7ylH8tdj82Umx+aIZ+7oQR0utjdtO4XPzs1mLf+TzSskf7oWr5galZY4K/b+vWfiw6J85Y7e7C2SyfZpu/WmWFI/NfpvH7J0APSS999PxijWMe4LbdJf/gtXLxUXnMHkFtVVQU3RNYx7OuadIzpmbNel9raTjJwQJ86nXR5tZr7xgKZN3+RdHBXPOzszi4v9MCh/qjRH5SLFi9z75cvpJs7423gNn2krjHsc9dXzzrTmyi3dldg7Diwf2Y5bRN9vlPHzTNDW+Qu29BjvR/E3Dfmu+1aLP379ZQ+vbplhsxqaNno78vc+k2fMUd0iAo90NWnV3fp0S2ZkC0uCx2yQoc7qKmuzhxIjrYn999C+1PucoU81gOEc1y4oYFDf9f/dPzvuoYe0qtO9AC2vm/6OuOqqpZ1VqEH2N5b/mGwbW3btM68rpj3cZLtWsx66Pt83oJ33E0e3wluyKzvrXvve8iFKBdnBQzRRupBlLdcP9V+0atn13rfz/qZMX/hkvAzo7plcHNOHe4s31SoRzHvrXz18FzDAkuWLpcxF14dnNmqr+7bdXM59dAd5YQDB0pLdw+mxk4frv5M7n5sltwyZYa8++EnQTGnnPhDueY3Y7PGA29s+U25XDHvbV0vfX/PdvsTff/26NYlM4RI7jrrPmPpsuXB013cvlN/MNU36WfcmrVrpWvnLbP2sfoZNef1ebJ0yXLp3bub9OvdQ1q2bFFfUXn/pvudTTfdpMEwO+/CBT5ZzOdVXUUWuh/Ruiy+l2k76ckk/fr0lG7esEa521OohX5u65Up/5jzlqz//PNgqKQB/Xq5q/XC4Rj9cuvaX/mviQ7KR0Pk6DLTXp0tLataBN/f2rWt8V/e6Pn3V3wUBGibtW/j9h/fzhs6aeF6hZ1+/1r+3grpXLuF7OpuRlzXvjt3ZRq7LcX26dx6G3ocV59sqJ6G/q7rMdNdfbzus/Wy0w7bZK6i0eW0H7m3lvsu8O2Giqnz73qlhLbdRu5/vbfulvc9qQsX+l6vsyL+0OwECBjCJiNgIGBodp23GawwAYNtIxEw2Pqn4goGn/AWd5awjt+sByV06tyhRobt1UcOcDeB3mdgV6lu2fAQFm++86E8O2ORPDptnjzzfwuDcvQ/ww8f7G7odrIM3LZP5jnrmejs74vOP1UuGXe69epQPwLNWoD+VH7Nd8+EB+sMGMpvbVmjpAQmPvS0XH71ncEwKVpHddWmctjefeWgXXvKd7frJh3btWqw6iUfrJbnZuq+fb5Mfv7NzOv33WcXueDck+QH++2ReY4ZewEdLuY/djs8CCteee5PWQFxNJzZ4O/vJY9Nutl+Zb01YD/iYTRiNvegfCOKKJtF0rQtZYPKiiAQgwABQ4hIwEDAEEN3oogcAQKGHJAmfkjA0MTgudWl5QqG3O26+c775e57J8sMd9a6Pw3o3lF6dWkvnTevkbbu7OBvbbKRuzHtF/LBqrWyePkqmbtohbz/8af+IsH4nXpm4x67bJf1vOUDvYHeOnf2mx5w0cm/KbPlelE3As1RgP5Uvq1GwFC+bWOxZvdPelzuvGdSMH68X//Wte2lb5fNpbZjjbRvXeXOdN/YnZX6pXz0yWfyzvur5Q134oDu4/1pyMH7BUPfHeLue8NUngJ6g3u9T4AOgTZq5LDgSiU9m/2Xl4f3m3rywduCe0uVw9qzH4mnFdJ0UD5N2xJP61IKAuUhQMAQtgMBAwFDefTIdK0FAYNtexIw2Pqn7gqGXE69edvDT/yP6Bibf582K/fPeR933qqjfHfvXdz4qnvJYYfuX+dQQnkXbqIn23fZOxh7WKvT8e9PP2lEE9VMNQikT4D+VL5tSsBQvm1juWZ6k/kp7kbpTz37UjCmvN4zoKFJh1bZZ6+dZPD+e7r7dHyv3uFhGiqLvzeNwFvzFsmI48YGIUNujZPvuy6430ru81aP2Y/EI5+mg/Jp2pZ4WpdSECgPAQKGsB0IGAgYyqNHpmstCBhs25OAwdY/9QGDz6tjbOqX/flufO/33FiqOvanjjFc5cZ13Xyzdu5maFu5MWm7u4MO2Te788sol3kNTD51Y4tu079XSeOTlsv2sB4IWArQnyz1669bx8nWg4xbbdlBtu7etf4X89eKFdB7luj9ZfTeQStXrQ7ugdNi002DMb1rO3cK7jmjN41lap4Cet8AvceW3ndlh+37y/buRrutqxseFqspt5b9SDzaa9eukxn/eN3d+6qVbLdt33gKNSolTdtiREi1CCQiQMAQshIwEDAk0sEqvFACBts3AAGDrX9FBQzG1FSPAAIIIIAAAggggAACCCCAAAJGAgQMITwBAwGDURdMdbUEDLbNS8Bg60/AYOxP9QgggAACCCCAAAIIIIAAAgggkLwAAUNoTMBAwJB8b6u8GggYbNucgMHWn4DB2J/qEUAAAQQQQAABBBBAAAEEEEAgeQEChtCYgIGAIfneVnk1EDDYtjkBg60/AYOxP9UjgAACCCCAAAIIIIAAAggggEDyAgQMoTEBAwFD8r2t8mogYLBtcwIGW38CBmN/qkcAAQQQQAABBBBAAAEEEEAAgeQFCBhCYwIGAobke1vl1UDAYNvmBAy2/gQMxv5UjwACCCCAAAIIIIAAAggggAACyQsQMITGBAwEDMn3tsqrgYDBts0JGGz9CRiM/akeAQQQQAABBBBAAAEEEEAAAQSSFyBgCI0JGAgYku9tlVcDAYNtmxMw2PoTMBj7Uz0CCCCAAAIIIIAAAggggAACCCQvQMAQGhMwEDAk39sqrwYCBts2J2Cw9SdgMPanegQQQAABBBBAAAEEEEAAAQQQSF6AgCE0JmAgYEi+t1VeDQQMtm1OwGDrT8Bg7E/1CCCAAAIIIIAAAggggAACCCCQvAABQ2hMwEDAkHxvq7waCBhs25yAwdafgMHYn+oRQAABBBBAAAEEEEAAAQQQQCB5AQKG0JiAgYAh+d5WeTUQMNi2OQGDrT8Bg7E/1SOAAAIIIIAAAggggAACCCCAQPICBAyhMQEDAUPyva3yaiBgsG1zAgZbfwIGY3+qRwABBBBAAAEEEEAAAQQQQACB5AUIGEJjAob4Aobp06cn/8alhkYLdO7cWWpraxu9fDELEjAUoxX/awkY4jctqsRFM26Xle++Kl37HSDtOvYuallejAACCCCAAAIIIIAAAggggAACCDQHAQKGsJUIGOILGLp06SJLly5tDm//ilzHJUuWEDBUSMsTMBg3NAGDcQNQPQIIIIAAAggggAACCCCAAAIIJC5AwBASEzDEHzDsvPPOib9/qaBwgWXLlgXBDwFD4WbN/ZUEDMYtSMBg3ABUjwACCCCAAAIIIIAAAggggAACiQsQMITEBAzxBwxNeSA78Y6SggqiK0uasl0YIsn2jUPAYOvPPRiM/akeAQQQQAABBBBAAAEEEEAAAQSSFyBgCI0JGAgYku9ttjUQMNj6W9ROwGCh7tXJFQweBrMIIIAAAggggAACCCCAAAIIIJBKAQKGsFkJGAgYUtnBvY0iYPAwKmSWgMG4oQkYjBuA6hFAAAEEEEAAAQQQQAABBBBAIHEBAoaQmICBgCHxzmZcAQGDcQMYVE/AYIDuV0nA4GswjwACCCCAAAIIIIAAAggggAACaRQgYAhblYCBgCGN/dvfJgIGX6My5gkYjNuZgMG4AageAQQQQAABBBBAAAEEEEAAAQQSFyBgCIkJGAgYEu9sxhUQMBg3gEH1BAwG6H6VBAy+BvMIIIAAAggggAACCCCAAAIIIJBGAQKGsFUJGAgY0ti//W0iYPA1KmOegMG4nQkYjBuA6hFAAAEEEEAAAQQQQAABBBBAIHEBAoaQmICBgCHxzmZcAQGDcQMYVE/AYIDuV0nA4GswjwACCCCAAAIIIIAAAggggAACaRQgYAhblYCBgCGN/dvfJgIGX6My5gkYjNuZgMG4AageAQQQQAABBBBAAAEEEEAAAQQSFyBgCIkJGAgYEu9sxhUQMBg3gEH1BAwG6H6VBAy+BvMIIIAAAggggAACCCCAAAIIIJBGAQKGsFUJGAgY0ti//W0iYPA1KmOegMG4nQkYjBuA6hFAAAEEEEAAAQQQQAABBBBAIHEBAoaQmICBgCHxzmZcAQGDcQMYVE/AYIDuV0nA4GswjwACCCCAAAIIIIAAAggggAACaRQgYAhblYCBgCGN/dvfJgIGX6My5gkYjNuZgMG4AageAQQQQAABBBBAAAEEEEAAAQQSFyBgCIkJGAgYEu9sxhUQMBg3gEH1BAwG6H6VBAy+BvMIIIAAAggggAACCCCAAAIIIJBGAQKGsFUJGAgY0ti//W0iYPA1KmOegMG4nQkYjBuA6hFAAAEEEEAAAQQQQAABBBBAIHEBAoaQmICBgCHxzmZcAQGDcQMYVE/AYIDuV0nA4GswjwACCCCAAAIIIIAAAggggAACaRQgYAhblYCBgCGN/dvfJgIGX6My5gkYjNuZgMG4AageAQQQQAABBBBAAAEEEEAAAQQSFyBgCIkJGAgYEu9sxhUQMBg3gEH1BAwG6H6VBAy+BvMIIIAAAggggAACCCCAAAIIIJBGAQKGsFUJGAgY0ti//W0iYPA1KmOegMG4nQkYjBuA6hFAAAEEEEAAAQQQQAABBBBAIHEBAoaQmICBgCHxzmZcAQGDcQMYVE/AYIDuV0nA4GswjwACCCCAAAIIIIAAAggggAACaRQgYAhblYCBgCGN/dvfJgIGX6My5gkYjNuZgMG4AageAQQQQAABBBBAAAEEEEAAAQQSFyBgCIkJGAgYEu9sxhUQMBg3gEH1BAwG6H6VBAy+BvMIIIAAAggggAACCCCAAAIIIJBGAQKGsFUJGAgY0ti//W0iYPA1KmOegMG4nQkYjBuA6hFAAAEEEEAAAQQQQAABBBBAIHEBAoaQmICBgCHxzmZcAQGDcQMYVE/AYIDuV0nA4GswjwACCCCAAAIIIIAAAggggAACaRQgYAhblYCBgCGN/dvfJgIGX6My5gkYjNuZgMG4AageAQQQQAABBBBAAAEEEEAAAQQSFyBgCIkJGAgYEu9sxhUQMBg3gEH1BAwG6H6VBAy+BvMIIIAAAggggAACCCCAAAIIIJBGAQKGsFUJGAgY0ti//W0iYPA1KmOegMG4nQkYjBuA6hFAAAEEEEAAAQQQQAABBBBAIHEBAoaQmICBgCHxzmZcAQGDcQMYVE/AYIDuV0nA4GswjwACCCCAAAIIIIAAAggggAACaRQgYAhblYCBgCGN/dvfJgIGX6My5gkYjNuZgMG4AageAQQQQAABBBBAAAEEEEAAAQQSFyBgCIkJGAgYEu9sxhUQMBg3gEH1BAwG6H6VBAy+BvMIIIAAAggggAACCCCAAAIIIJBGAQKGsFUJGAgY0ti//W0iYPA1KmOegMG4nQkYjBuA6hFAAAEEEEAAAQQQQAABBBBAIHEBAoaQmICBgCHxzmZcAQGDcQMYVE/AYIDuV0nA4GswjwACCCCAAAIIIIAAAggggAACaRQgYAhblYCBgCGN/dvfJgIGX6My5gkYjNuZgMG4AageAQQQQAABBBBAAAEEEEAAAQQSFyBgCIkJGAgYEu9sxhUQMBg3gEH1BAwG6H6VBAy+BvMIIIAAAggggAACCCCAAAIIIJBGAQKGsFUJGAgY0ti//W0iYPA1KmOegMG4nQkYjBuA6hFAAAEEEEAAAQQQQAABBBBAIHEBAoaQmICBgCHxzmZcAQGDcQMYVE/AYIDuV0nA4GswjwACCCCAAAIIIIAAAggggAACaRQgYAhblYCBgCGN/dvfJgIGX6My5gkYjNuZgMG4AageAQQQQAABBBBAAAEEEEAAAQQSFyBgCIkJGAgYEu9sxhUQMBg3gEH1BAwG6H6VcQQM66c9KOteecgvNtb56kN+Kt/qsX1sZX65cKZ8+sjvYytPC2pzwrUi1e1iKzNu043cutXoOsY4rX34evnin7NiK3ET18atXFvHOX0y/hzZ8OnK2IpsuetQabHbsNjKkzUfy+p7xsRXniuJ/lI6J/2ldEMtgf4SjyP7l9Id2b+UbqglsH8p3ZH9S+mGWgL7l3gc2b+U7liR+5fS2UxLIGAI+QkYCBhMO2ITVE7A0ATIZVYFAYNxgxAwxNMAfEEv3bEiv6ATMJT+xnElEMjFwigEcqU7EmCXbqglEGDH4Mj+JQZE9i+xILpC2L+ULsn+pXRDLYH9SzyOzbkUAoaw9QgYCBiacz8uZN0JGApRStdrCBiM25OAIZ4GIGAo3ZGAoXRDLYEzTEt35AzT0g21BM4wjceR/UvpjuxfSjfUEti/lO7I/qV0Qy2B/Us8juxfSnesyP1L6WymJRAwhPwEDAQMph2xCSonYGgC5DKrgoDBuEHiCBg2rP5Avlq1ItiSDZ99ImsfvznWreIHbemcG7VuLzWjrim9IK+E2M8A6rmDtDr4TK+G0mc/ccMPbXBnccY1xf6D1g3ftNoN4xTnRH8pXZP+UrqhlkB/icex7A8AsX+JpaHpL7Ewlv+QlfSXWBqa/hILI/0lBsZNKvH3SwxulkUQMIT6BAy2AcMjjzwiJ554YlZXuPrqq2XkyJFZz51++ukyadKkrOcWLFgg1dXVWc+V+uDFF1+Uo446Kihmxx13lIceKn4I9JkzZ8rgwYODMrbffnt58sknC16tNWvWyGmnnRa8/sADD5Rjjjmm4GXreiEBQ10y6X2egMG4beMIGLI2gUvyszga+4AhXxorl70cl+RnezTmEZfkN0btm8vEHshxz5JvIjfiGQK5RqDlLMIZ2TkgjXwY+wFTvo81siWyF+P7WLZHYx/xfayxcl8vx/exry1KmeP7WCl66ViWgCFsRwIG24DhL3/5i4wYMSKrU40aNUrGjx+f9dyWW24py5cvz3pu1apV0qZNm6znSn3w9NNPy6BBg4JidtppJ5k+fXrRRU6bNk123333YLkBAwbInDlzCi7j1ltvFQ1TdBozZoxo2FLqRMBQqmDzW56AwbjNCBjiaYCyP8OUmzzH0tAcAIqFsfzPmKO/xNLQ9JdYGOkvMTBW5BAWBAwxvHO4B0MsiK4QAobSJQkYSjfUEggY4nFszqUQMIStR8BQfgFD165dZfHixZnutXDhQunZs2fmcTSTloBh/vz5QXgyYcIEufHGG6PNI2DISDBTrAABQ7FiMb8+7oAhGCLpsXiHSKra80jZeKtesW35V+/Ok89emhhbeVpQq4N+LBtVtY6tzC9ef0HWz30htvI2blUjVQeeEVt5WtB6Z/iFs4xr2qRzb2mxxxFxFReU85kbruurtZ/EVmaLbfaWTfrvHVt59Jd4KOkv8TjSX0p3ZP9SuqGWwP6ldEf2L6UbagnsX+JxZP9SuiP7l9INtQT2L/E4NudSCBjC1iNgKL+AQVtmyZIlUltbGzRSvqsc9A+5AcOXX34pOmzS3LlzZeXKldK/f//g/zU1NUE5+f6jV0XMmjVLOnToIAMHDpSpU6fWewXDunXr5M0335S3335bevToIdtuu620aNEiq+hir2CI3oNZhbgHXMGQK8LjQgUIGAqVSuh1cQcMCa0mxSKAAAIIIIAAAggggAACCCCAAAKNFiBgCOmig7sbNmxotGVzXHDKlCkydOhQGTKkfAIGHU7otddeCzgnTpwoRxwRnnR59tlny3XXXSf+3/VFfsCgyw0fPjyzvN8melXAGWecIVFbR8vqUEyTJ0/OvLRTp04yduxYOe+884LncodI0qBj9OjRsnr16swyOjNu3Di57LLLZOONNw6eLzZgaNu27TfK1IIIGAJO/tMIgX8BAAD//3YbHy8AAEAASURBVOydB4ATRfvGX+4ox6EUAQWk96ICiood7A1F7BVRsGDDBoqKBVBQQOwIUsTeUET95G8DFaVYERCkK02kCEqHy3+eiTM32UvuklySvcs9832X7M7Ozsz+Zjes88z7vqUCKgmTbwR+/3GkbFrzndRudopUqtbYt36wYRIgARIgARIgARIgARIgARIgARIgARJIFoEVCybLpnWLpE6ba6RyzXbJaqbI11uqVCndx5I2HTdp0iQ566yzpFOnTvL+++8nZJz2339/WbVqlaxcuVJq1aoVVZ1vvfWWXHDBBbos+vLjjz/KihUr5Pbbb5chQ4bo/NatW8vs2bOlZ8+e8uyzz9p6N2/eLHvvvbc+hjL5pTvvvFMeffRRXSQnJ0fat28vs2bNyu8UOfjgg+X777/XZUaOHCnXXnttxPKXX365jB8/Xh+fOXOmHH744Xq7ZcuWMnfu3Ijn4cD27dsFfULq16+fDB06VG+7DHRGnB/xjEucTdnTMjIyBM9USXuuLACfN0op8BQYfBwECgw+wmfTJEACJEACJEACJEACJEACJEACJEACKSFAgSGImQJD0RIYKlWqJC+//LIceuihgon6TZs2SeXKlfVgvfTSS4KJfJOMwHDqqafK5MmTdXbt2rW1MFGlShUZPXq0vPnmm6a4zJ8/X5o1aybvvvuudOnSxeZD4IAgALFj+vTpNt8IDOvXr5cGDRrIP//8o4+dc845ctlll8m0adNk2LBhtvyMGTPksMMO0/2ORWCwFaiNvn37yiOPPKKzKDC4ZLgdCwEKDLHQSkJZCgxJgMoqSYAESIAESIAESIAESIAESIAESIAEihQBCgzB4aDAULQEhtNOO01bKmB0MKGPSfsTTzxRDxYEgubNmwcHTn1CYFizZo00bdrU5n322Wdy/PHH632s4a5Ro4asXbtW7/fp00cGDRoknTt3lokTJ+o8tPfRRx/p7d27d8sxxxxjRQYjMLzxxhty0UUX6TL77ruvttAoXbq03odQAcEC6aabbpInn3ySAoNiQQsGfUv49kGBwTf0wYYpMPg8AGyeBEiABEiABEiABEiABEiABEiABEgg6QQoMAQRU2AoWgLD/fffL+3aBV12TZkyRb7++mu59957BRP7y5cvl/Lly9tnAwLDl19+KWeeeabN27p1a0iZa665RkaNGqWPn3HGGfLBBx9Iq1atZN68eTrv6aeflhtuuMGeD+sJYyVhBIaHH35Y7rnnHl0GLpmOPPJIWx7uj+DSCclYXcTqIslWpjZoweDS4Ha8BCgwxEsuQedRYEgQSFZDAiRAAiRAAiRAAiRAAiRAAiRAAiRQZAlQYAgODQWGoiUwvP3221KuXDk9OJjYnzp1qnZ/BAuCcePGSVZWln2mIDC8+uqrct111+k8IwjYAmrjueeesxYRJh6CGXOU+/zzz6Vjx472FFhAGIsJU9/1118vI0aMsGUibcCSYsGCBbRgUIBowRDpLklNPgWG1HCO2AoFhohoeIAESIAESIAESIAESIAESIAESIAESCBNCFBgCA6kmWwuaSFRi2qQZwScPvbYY+Wrr76Sk046ST755BM9ULA06N69ex6BwWvBsGXLFsnOzrZP6dVXXy1jxozR+yagdZ06dazVAQQII1CgEAIs33HHHbq8ERj69++vgy8jE1YQV155pT7u/ahQoYLA5RItGCgweO+NVO9TYEg1cU97FBg8QLhLAiRAAiRAAiRAAiRAAiRAAiRAAiSQdgQoMASHlAJD0bJggMBw3333yYABA0KeOUzaH3TQQXkEhj///FOaNGliyyLY88knn6z39+zZI7Vq1bIxGO666y4dQNkNCo2AzRMmTLDnw5oBrpmQjMDgxmA46qijtFsmrNBHmjVrlsyePVtvIxA04j9QYKDAoG8IHz8oMPgIH01TYPB5ANg8CZAACZAACZAACZAACZAACZAACaSIwJ5lP8uOHyeHba1c21Mks37rsMfSIZMCQ3AUKTAUPYEBcRJgbeCm7du3612viyTEREAMhg8//FAfR6wGBHKuVq2ajr0ASw2TFi5cKI0bN9aulrp162aydXBmBHdGsGe4YTLJCAwrV66U2rVrm2wdo+Hiiy8WBJ1GzAgEo0Z67bXXdDBoCgwUGOzN4tMGBQafwJtmKTAYEvwmARIgARIgARIgARIgARIgARIggfQmsGvuVNk+ZXzYi8zqcIWUaXVc2GPpkEmBITiKFBiKnsCwZs0aqVmzpn3MMPkPV0g7duzIY8EAgQGBlg844ABbPtwGAkXD1RHS7t275fDDD5cffvghXFGbZwQGZIwePVq7aLIHPRuwrkB9mZmZtGBQbBiDwXODpHiXAkOKgXubo8DgJcJ9EiABEiABEiABEiABEiABEiABEkhPAhQYFkmdNtdI5Zrt0nOAo7gqCgz+CgzvvPOOnHfeeXqkzj77bHnvvff0dqNGjWTJkiV6u2/fvjJw4MA8AsO///4riHuAtGjRIrnkkku0yyKd4XyMHTs2T9yEjRs3aksEY/mA4rBSQEwGYz1x6KGHarHAVIW+9urVy8ZvMPmI8/D4448LxA6k7777TnAuEoSHn3/+WW9H8+G6h+rdu7cMHjw4mtPyLbP//vvLqlWrBJYYcBmVikSBIRWUI7dBgSEym5QcocCQEsxshARIgATSjkAgR2Tn+oBsX50ju7eKlK9ZSrJqZkhG2bS7VF5QoggERPYoS++MciKlgi5cE1Uz6yEBEiABEiABEoiSAAUGCgwUGPwVGKJ8VKMuhpgMcF20adMmadasmUCoKF26dMTzMfE+Z84cQfwEuE8y90OkE2D9sHjxYi1owA1TixYtpGLFipGKF4l8CgxFYhhS2gkKDCnFnbcxCgx5mTCHBEiABEggHwJqknjt57tl4TM7ZYcSGNyUmVVK6l1URmpfWFpKZ5eyh34dsEM2zVWKhEot7yknFQ8onrPLf7y+S35/c5e+jtrnlJF6l5fR2+E+/v5xj8ztvyPcoTx5YFLlkEyZ1mWrBHYHD7d/pbyU3juXYZ6TEpQR2PNfRaqpZE36//3THln45E75Z3HwHmjYtazUvyo8O5Sd0y/IrZT676KjJmQn6EoLqEbdyhDNdEoiiwJ6wcMkQAIkQAIkkHQCFBgoMJgJ5UAg9F0+6Tefzw0gNsFZZ52lV+sjsHIikh8T2Ynod7rX4ce40ILB37uKAoO//Bnk2Wf+bJ4ESIAEihuB5eN3yeLRO/PtduVWmdLmiSzJ+G8O+ccbt8vGX4Iz2W2HZkmVdpn5nl9UDy4dtVOWvhwUGOpdWEYa9YxsrrH+2z3y813BwGwFXU/rh7Ok6lGZ8vlxW2zRY97PljKVkiswbF8TkG8uVOYnKpWrWiopk/m7/wnIl2cG2zAX10AJDA0iCAwbZuyRn3rncjt+atAE3JybrO81k3fLvIeDwkatU0tL87uVmQUTCZAACZAACaQhAQoMFBgoMKSXBUMa/kwV+pIoMBQaYbGrgAKDz0NGCwafB4DNkwAJkEAxIrBtVUC+vTh3srj8fqWk6hGlJVPNxW78eY9snm+WgIu4k8gUGERg3REpHTignOxzaOoFhm2r1XheFBzPskrMOFqJGolOsOT4oVeuYFD3/DJS7ehMqdwmvMhUFASGmqeUlhZ9KTAk+l5gfSRAAiRAAkWDAAUGCgwUGCgwFI1fo+T1ggJD8tgW1ZopMPg8MhQYfB4ANk8CJEACxYjA6g92y6+PBVd5Y0L6yLeytT99fQnKwnrBkB2yUpVBqtgkQ9q9UF5vewUGTC7/uyRHdqwNSIX6KnZDrYzw7nlUnXDDtO2PHIEF997NMqR0hbwT9bu3BMS4+ikDt0Lq/9tW5siujaofjjum3VsDsnVJQNdZoVEpKR+pXfRatYcJ+K3LcyS7XoYqW0oKY8EQzUr8giwYdm4IyBbFLUcZUezdNEPKKquDSCmaay2MwJCjjFi2/p4jW5bmSCmlFVRomCHZtdU4ety9/jVlt/xyf/Ce2aetsmwZnhWpyzo/ksCAa87ZqQZFpcxypXQ7u/8NyKY5OdrSo0KDDCXi6MN5PratyJGtKwKC8uWqldJ9LVMxlJ1rwRBJYNilrDG2/RGQXZsDslfjDF2Xt7HC9BPxKcB0518BKav6mV23lGSWD+2naU/zV8/FNnVdWSr+Ca7fWAyZMuZ7zzbcNypeypocLXSVU8LgXo2Um7LwVZvT+E0CJEACJJCmBCgwUGCgwECBIU1/3uxlUWCwKErMBgUGn4c6EQJD4J91krN5fdgryai6v5TK2ivsMWaSAAmQAAkULwKue6S96mfIoWPLhwgDW5blyNLRQRdCGWqiueX9wVXgrsDQ9OaysvyVXSHxGyq1zJQD+pcLmbD9d1GOzH1gh2xRk6huQru1ziwttdVKeJNmXbXN+vdv+3iWLHxip/yr+lLtiEw5aFCWnpBf/qJyb/RSsG/mPFgVtFQr1asfF7qaHpO8s/vskK2rcttGHzEpu3JSsI5YXSQVRmDYuTEgCx7dIX99YwImBK8gWwkkBz5STok0uTEtMMEdzbXOf2SHrPr4v4APBoj6jsY90LovVXyJgTtUwObghL85HaLTAQ+Wk8pKSEBCTAlvnA7k1z2vjDS+Kbx7qUgCw7Kxu2TJuKBrrobdyqqYHntk/cxcHuHGcsc6xU2JXuuUuypv2r9TGWl6a1ktjkTqZ0vlJqmGcpe0a1NA5g/Kyx9upaodWVqaqHvaBDePp5+Iu7FsbK77Lbev9S8tIw26q37mDrGs/UwJfY/uzMMf8U8aXhtads3/dsuC4XnL4t5pfldZqdw69N532+Y2CZAACZBAehLYNe9L2f7Fi2EvLqtjVynT8tiwx9Ihc8WCybJpHQUGCgwUGNLhec7vGigw5EcnPY9RYPB5XBMhMOycOVF2zAofICf7jJsls35rn6+SzZMACZAACSSCwLqv98jse3Ld3VQ7PFP2O7m0dneDleGRkiswRCpTsbmyeHg+aPGAFddTT811xRTunFYqMDLaRnIFBkw0m4lvIzDMVUGD/5yadzLd1Nv8jnJSq1OwLkxKT790m63DlPF+p0pgwKr26ZeEn6hHn3C9B6t4F3srfkjRXisCb6/+JC+Tmicp90D3RnYPtGrSbpmvJu3zSwcNyJJqx2TK12dtlZ1qct6b6nQpI01uiVFgGKMEBiUSFZSOfDNbstQKfVi0/KBif2yal1dcMHXAXVPjG8tG7GeLPuWk5umlpaD7t5Yq01yVRVoWYz9xzpz7dsjaL/OOBY4hNbhMiQw9grxWTVT8h0XmX+OE0tKyX7Av66epOCB9c5/XYG25n7h3Dn+pvGTtG/nZzS3NLRIgARIggXQhQAsGCgwUGCgwpMvvWaTroMAQiUz65lNg8HlsKTD4PABsngRIgASKEQG4iJmhJt/DTRojHgPiCFQ7urRUba9WRTtzlt4J2uqqTDUV1HjF27us5QEwHPFqeSm/f4ZeoT3noeAkKlZaN+hWRsruU0qWjdtlg0W7E+GuwGBw4rzqx2bqvrgxAOqcW0YqH5Qhq9XqfbOyHROtCKqcoeZlYf3wx4RcSwcICRnq+KoPQq0uYhUY0Ea41PyOsrKfmtRHCuciyZ2whoVAXbVKvWyVUvL767u0lQbOq3pYprR+LEu88Q7yu9btf+bIPwuUlYgSGpDQv9aDy2nO2XWd5fL6aPADLoamnZsrvlRRq9/rKj4BZeix5IWg1QhK4l44/JVs7c5pg7IyWKyOIcFtVtNeqg0lRmXVCM8jogWDZ+K++pFK3FLctiwLyFJHeDDCE65t1jXbdLu4tkOeyVLuhkRWK4Fk2WvB8UX+sf/Lln9U7JC/lAC1XDFF2ueQTKl/RRntGgtCxbRzc8WuJjeU1a661n62R1ZMDJZ341e444W6Curnptl75PubckUAWJDs3SJTVr0f+mx0+LSCQHj75vxc/qi7hooXsemXHPldPUsmtXuuvFRsmSE/37ndWnnAugFWP/8uzJGFT+601jnNbi0n+3cO3n/mfH6TAAmQAAmkNwEKDBQYKDBQYEjvXzkRCgzpPsJ5r48CQ14mKc2hwJBS3GyMBEiABIo9gX9+U+6D7toe1vWNubgKdTL0KmrECUByBQYzGY783Uqw+PLM3MnbdiPUxGgLNfn/0W75+4fgyvMap5WWKmrCF5PYC1T8h1XqGBImt9s+GXS67woMmNxu+3TuquwFQ3Zat0bu6m64EvpKtW2sHQ5Urn2qH1daPu+wRdePj0Zq1Xg9tXocyWvZEKvAoCsJ82FWyeNQOIHh2wu2yrY/g1YALXqrFfVnBCeD4UJq5tXBCXSce9Q72VqAMS6cCrzWDqV1jIlYgjyv/hCueXJXzx/3cQU9aY/2EfPi20ty+9P64SypqkQkdxW9O/Y4J1yKRmCAeISV9ybeg3t/NbyqrNTvWiakXbTT/HYVSFsJMeXUav1V7+3SMT2QX+tMJSApYSlSDIbNc3Nk5bvByXtM2u+vrC+Q3PLY7/hFBe3GyBUYoukn7mkTt6RqOyUUDQ3e03sUyqmn5t6LeDbAeG7/IH+IGke9m61dPKH9X+7ebl1o1e4cdP80s+u2EBGqnnK3VFGJF/8uzpHNvwafL7j9ihRwG/UykQAJkAAJpB8BCgwUGCgwUGBIv1+20CuiwBDKoyTsUWDweZQpMPg8AGyeBEiABIohAUzOb/x+j2AyeP30PXY1tHsp8E9/+PjyUnqvUiECA2Iw1FZWBCa5bnTaPasEhlZBUQITrBtm7Jb1qo2tywPa0sGIATg3ksDQ+JqyUldNpJr0483bZePPwclUiA/lVSBikzapyWNTJ1Z41724jHx1dq7gcdjo8jqYrynvxi2IVWCA6BIuNVL9raYsLZC8AgMCWn9xQu4kM9xIuUGuN6gxMKnNo1k6tkW019ro+rIxCwyLn9kpy98MTra7/E0f3FgGja9T46B4JkNg8MaJcCfpG3QtKw2uKqPGVeRrNZZmfE0fMTFf/djSUlNZCrgBwF3BwBvkGeLWv0pYgzXG3+pe+kdZAXiteMIJDNH0070/m6q4FLVVfAqTEMgZwcaRSqns5SqGyJIxQWsQWF9U+u9ZwXHEPzHxLkyA9eUv75LFo/K6lapyYKa2ZoAbKze2A+phIgESIAESSH8CFBgoMFBgoMCQ7r90FBjSfYTzXh8FhrxMUppDgSGluNkYCZAACaQlgV1/B2SdCkAM//yuz/vWKsByVRVo2V1h3lat0K6iVmqb5E5Kt3tGCQwHZOigut9ftz2scGHOcye4XQsGs3LelHMtAExeuO/aZ5eRmip49KweuavwO36mVqU73mOWKlc/JlB0rAJDPEGeMcE87bxcwSNcv03egQ9lyaJndlhrB5Mf7hvX2vS22AUGN1ZAvQvKSCPlLshN7nG4Z0Lw42QIDA0uDwY+Nm3/NmyndVdkBAYcW/kuAhznWlyY8uYbohKEFqRIAgPEtDn3bbfutMy53u9wAkM0/XTvf4yhN+C4244rpLj53m1YTrR/rbwgODisjTYrF1DhEkSvAweVk2xHdAtXjnkkQAIkQALpRYACAwUGCgwUGNLrVy3v1VBgyMsk3XMoMPg8whQYfB4ANk8CJEACxYjAErUaGv7ekWqoFeAIgOsmrBrHhL5Z3d1ETd7WUZO4sQoMs3urCV1luYAEq4M6ys9/JbXqevvqgPzSL+ivPpLAgIDHrssXt21Miu+tXDCFS+XVpGxptbrduAxCmcPHlpcKDXPLh/i0V31q1DN0gt2td/23KsCumtw1KR6BIbN8KZlyUq4FQ8u+5SQjd4G7qVp/VzooU+Y9sMPGqCjoWhEUepviaa7XjSMQUrGzs/hZZcHwRtCCoXKrTDn42aA7H1PEnSxvothg3PwUGNAvuLZa99UeJYApaxhlgeBNh44sr2MqRBIYVry1S357OtcKoMEVZZXLrgzJUvfLN+fnij/xCgzfdd8mm5VFBJI3HgLGJ7AraMKQVSNDflcWCSbQNdxN1VTuw8Il3DcQ9nRSp2/4TlkaKQEwnLWRCYQerh7mkQAJkAAJpCcBCgwUGCgwUGBIz1+33KuiwJDLoqRsUWDweaQpMPg8AGyeBEiABIoRgcXPqQnm/wLhYpX0YS+Wlwxnjt3rlqblPeWkxsmlYxMYlNsXNw5CmyFZOng0MMFFjAkYHK3A4MZgqK/iKTRUcRVMwsTzTmV9gVTpwAzJViu6vzg+d0Lfdefk9YmfCguGMhA8nBgMLgusrP/z/3br2BTo/74dMmXx87tsvImCrrVC/dgFBsTG+HVwrkUAAiSXzg4Ga976e45MvzzX+sNYr/glMCCA+N8/ByfuYRWw34mlBcwQ2+MnJWCZ1PIudY+qifoQgUEFj25xrwrMoJJrleEyhdD2/Y251xuvwDBXiUJ/fhGMK7KfigHS6qFgu7s2BeSrs3IFjCPfyJZNc/bYGAyVWiqBRwWuNi6ONv+aI4jLgVS+Zimp0CBDfhueK4y0vE+JU+rW367ieSxVQuHqT4JtRiMs6Ur5QQIkQAIkkDYEKDBQYKDAQIEhbX7QIlwIBYYIYNI4mwKDz4NLgcHnAWDzJEACJFCMCGyYpSZn78idnIWLlRqnlBZMhO9UK8VXf7wrxEXPMROzpUzl0BgMBbpI8ggMTZQbHlhLIN7DfBWw2fjUhx/5tk/nDfLstWBY9/UemX1Pbp/hQgdiAvzp//5fPAEMwRGvZ+uJ2V8H7LCTr8hvfG1ZKVutlHa347p/SpXAsEjFPTD9xGRww+5lBfEtEMx5nbKSQNLBhF8uL7CaiOVaXQsG1AMXPXupwNyYoA6Xdm8NyLRzttkxQEyIeirOAibul47ZZV1aweqk/asqALFaYO+XwLBsnFrtPzY4wQ5erfqV0/E3ECj559477DWY+8UVGMATLqRg5TH/4R02ePJ+HUtLs9vLapdDi5RVw78q7oFJHT9X7rSU0YAb5DkaF0lrP9stcx7KFW1wX+3VRAU6VwG1N/wYHN992mZKm+FZ2hpj2rm5ogMCee+nxJCty3PUte6y19RKiSMIWO5av6DeWuco85ecgPzx5m5ZoQJdI2EM2z1f3lwGv0mABEiABEoAAQoMFBgoMFBgSPefOgoM6T7Cea+PAkNeJinNocCQUtxsjARIgASKPQF3wju/i2nRp5x1oeS6KSpQYFAxGOA33kyeR2oD4sbhalIdyY3BYCaM3fPmPbhD1nweXLHt5pttEy8A+1jh/Y2yGigopUpggFXIzCu2hgg33r65vvtjudaAQvLlGaGBkGs6q/e97WB/zce7Zd4juRPi4coY6wUc80tgwDjOuCJXDAnXz72UFUe7UUErnH8W5Misa3ItElAe9/CeLYEQF0nh6kGeEdNiFRgQQBpukv5ZnCtWeNtwLVcQ52T+kMj8cU2HKtdesGxwg3J76zT7cLsFkZCJBEiABEig5BCgwECBgQIDBYZ0/8WjwJDuI5z3+igw5GWS0hwKDCnFzcZIgARIoNgTwITo2k93yxIV8HibmsT1pr0bZejAufscmhvI+cebt8vGn4OrsdsOU0GeD8k95roAMkGe4Tf/FwSn/c83PdpAvU1uKis/9Mq1RjBiRYjA8KSKwdA6t36ci2DJfyjXTotH57qMQT5S0xvLyv4qILFxNYO8LUtzZHaf7SHXh1XkVY/MlIXKogDJDRCsMzwfsLhwXfHEHIPhA2X9sXfQkmDnhoAsHqHc2kwOFUlg0QDXNy7rWK915Xu7ZZFyfWUsQwoSGHCZsGSZ13+HjbVhLh2WAgf2z5KKygrFJDcWBeIGtH4sNG6DKWe+vVYyhptrkeAGcsZ5vz2ugjz/tyK/4ZVlpX63YKCKzfNy5LcndoQNcozYA017lZOsGv9Za6hbea4Sooy7ItQLgQGT778OVAKVsjQwCdyb3V5OWW3kWjEYa4V4+rn734DMH7xT1n6Z2wbaQjsH9C+X537+a8pu+e3JnbJjfejzV0tZ+jTpVVYQgwFJW5Yod0gmbobO/O8DdTe4St37nSkuuFy4TQIkQAIlgQAFBgoMFBgoMKT7bx0FhnQf4bzXR4EhL5OU5lBgSCluNkYCJEACaUMAQsOOtQHZviZHdqk4BmX3KSVZNTOkrJpkdifr471g1L9lSY7sVJOoezUO1htvXeY8WAMgVgD6Xb5WKe0yx40hYcrhG+1vW5GjA0snqn23/ni2d/0TkK3LAoIJ6ex6KtCwmhyPxDqWa4UosXOjmqxW/y+9F/7Cu0hy+ww+sBLYqtwEoQ/ZDVR/qqvzCj7VrSb52+qa4Apqx1p1n25Sk/b6Pi0l5ZTbq3BJs1CCDq4Drr8y/9NDtq0KyLY/ctQ9XkrH6kjGdSLuwhbFc7fqZ4WG6nlSrpoijW9A6XVw97Ttj4B2Q5bdoJSNh+G9LsQPQXyGnetzJKOcuvZ91TXUzlDb3pLcJwESIAESKAkEKDBQYKDAQIEh3X/rKDCk+wjnvT4KDHmZpDSHAkNKcbMxEiABEiABEiABEiABEiABEiABEvCNQM5fy2X30p/Ctl+6QRvJqF4v7LF0yFyxYLJsWkeBgQIDBYZ0eJ7zuwYKDPnRSc9jFBh8HlcKDD4PAJsnARIgARIgARIgARIgARIgARIgARJIOgEKDEHEFBgoMCT9YfO5AQoMPg+AD81TYPAButskBQaXBrdJgARIgARIgARIgARIgARIgARIgATSkQAFhuCoUmCgwJCOz7d7TRQYXBolY5sCg8/jTIHB5wFg8yRAAiRAAiRAAiRAAiRAAiRAAiRAAkknQIEhiJgCAwWGpD9sPjdAgcHnAfCheQoMPkB3m6TA4NLgNgmQAAmQAAmQAAmQAAmQAAmQAAmQQDoSoMAQHFUKDBQY0vH5dq+JAoNLo2RsU2DweZwpMPg8AGyeBEiABEiABEiABEiABEiABEiABEgg6QQoMAQRU2CgwJD0h83nBigw+DwAPjRPgcEH6G6TFBhcGtwmARIgARIgARIgARIgARIgARIgARJIRwIUGIKjSoGBAkM6Pt/uNVFgcGmUjG0KDD6PMwUGnweAzZMACZAACZAACZAACZAACZAACZAACSSdAAWGIGIKDBQYkv6w+dwABQafB8CH5ikw+ADdbZICg0uD2yRAAiRAAiRAAiRAAiRAAiRAAiRAAulIgAJDcFQpMFBgSMfn270mCgwujZKxTYHB53FOhMDg8yWweRIgARIgARIgARIgARIgARIgARIgARLIlwAFhiAeCgwUGPJ9UNLgIAWGNBjEGC+BAkOMwBJdnAJDoomyPhIgARIgARIgARIgARIgARIgARIggaJGgAJDcEQoMFBgKGrPZqL7Q4Eh0USLfn0UGHweIwoMPg8AmycBEiABEiABEiABEiABEiABEiABEkg6AQoMQcQUGCgwJP1h87kBCgw+D4APzVNg8AG62yQFBpcGt0mABEiABEiABEiABEiABEiABEiABNKRAAWG4KhSYKDAkI7Pt3tNFBhcGiVjmwKDz+NMgcHnAWDzJEACJEACJEACJEACJEACJEACJEACSSdAgSGImAIDBYakP2w+N0CBwecB8KF5Cgw+QHebpMDg0uA2CZAACZAACZAACZAACZAACZAACZBAOhKgwBAcVQoMFBjS8fl2r4kCg0ujZGxTYPB5nCkw+DwAbJ4ESIAESIAESIAESIAESIAESIAESCDpBCgwBBFTYKDAkPSHzecGKDD4PAA+NE+BwQfobpMUGFwa3CYBEiABEiABEiABEiABEiABEiABEkhHAhQYgqNKgYECQzo+3+41UWBwaZSMbQoMPo8zBQafB4DNkwAJkAAJkAAJkAAJkAAJkAAJkAAJJJ0ABYYgYgoMFBiS/rD53AAFBp8HwIfmKTD4AN1tkgKDS4PbJEACJEACJEACJEACJEACJEACJEAC6UiAAkNwVCkwUGBIx+fbvSYKDC6NkrFNgcHncabA4PMAsHkSIAESIAESIAESIAESIAESIAESIIGkE6DAEERMgYECQ9IfNp8boMDg8wD40DwFBh+gu01SYHBpcJsESIAESIAESIAESIAESIAESIAESCAdCVBgCI4qBQYKDOn4fLvXRIHBpVEytikw+DzOFBh8HgA2TwIkQAIkQAIkQAIkQAIkQAIkQAIpIrBr7lTZPmV82NayOlwhZVodF/ZYOmRSYAiOIgUGCgzp8Dzndw0UGPKjk57HKDD4PK4UGHweADZPAiRAAiRAAiRAAiRAAiRAAiRAAikiQIFhkdRpc41UrtkuRcSLXjMUGCgwFL27MrE9osCQWJ7FoTYKDD6PEgUGnweAzZNACgj8vekfWb/hb6lXp5aULp2ZghbZBAmQAAmQAAmQQHEnwPeH4j6C7D8JhCdAgYECAwUGCgzhfx3SJ5cCQ/qMZbRXQoEhWlJJKkeBIUlgi0G1W7Zuk5btOsvOnbt0b/eqkC2/zHhHsrLKFdnejxr3jvQb8IzuX88eF8p9fa71va979uRIIBDQ/UjG5P327Tuk2cFn2eucO+tdwVgVlDAp0G/gMzLxgy9kxco/bfGj2reRY49uJ/fc0UPKly+6Y207nKANjBHGCikjo5T6y0hQzcFqzr7oFpn53S96p/99N0r3rl0SWn+kypJ9/0Vql/kkQAIllwDfHxIz9sn+/eb7Q2LGie8PieHIWooWAQoMFBgoMFBgKFq/SonvDQWGxDMt6jVSYPB5hCgw+DwAPjb/9nufyIVd7wzpwXuvDZdOp3cIyStKO8OeHi933jNMd6nXDZfJ0Ifv8L179VqeYifwF//ykdSvWyuhfdq6bbvsXaO9rXPDH19LpYp72f1wG9O/my3nXNRL1v61IdxhnXfowa1kghrvWjWqRyyTTgdeefNDuaLHPfqSrrriHBn11P0JvbyjTrxCps+aresc+sgd0qvnZQmtP1Jlyb7/IrXLfBIggZJLgO8PiRn7ZP9+8/0hMePE94fEcGQtRYsABQYKDBQYKDAUrV+lxPeGAkPimRb1Gikw+DxCFBh8HgAfm8eK6w/+NzWkB+d3OVleH/toSF5R2inqAsPC2R9Iw3q1E4os1gkClG9xyNlW9EBnau+/nxx0QFOZv2CpLFm2wvYP+b9+P1Gyy2fZvHTdcCcIul3eWV54+oGEXmpREBiScf8lFBIrIwESSAsCfH9IzDC6AkMyfr/5/pCYceL7Q2I4spaiRYACAwUGCgyJFxi+++67ovWgl/DenHXWWbJq1SpZuXKl1KqV2EWgkdDCSwIsH42Hi0jlmJ8cAhQYksM16lopMESNKq0K/rV+o9Ro2DHsNa3//SupXGnvsMeQuWHjJpm3YIls+XermriuIc2a1I/o1x//cbvgt2Wyes1fUqNGNWnauF5E9z47duyU3xYtl8VL/9CxAlq1aCRly5YJ6UdBAkM0daBP27bt0PVW3LuClClTWsUn2CSzfvhFTj3xaNve7t17ZNnvq2Tp8hVSpXIlOahVkzz9QeFoJgjW/LlOflXMduzcKa0PbC4196tm2/FuwPXEz3N+U+589sghbVrqw7FYMDzwyHPSf9DzttrHB98pN193qd7HP3RPPf+q3NrnMXv8iUf7yI3XXmz3sQH3SvMXLpWFajyqV6sizZs2lLq1a+RxK4S+ghMSWOJFdeHi32Xtug3STvW9XLmy+hg+lq9YLYsW/S6NGtbJY+XhjgmsM+BqauPfm+XHn3+VRo3qSl11n5mXYNSF69iwcTM2Vb7IPlUq6W18wOUE+o8EV0hVKlfU29FMEERz/+jKwnyEExjQl3+3bNWly5Ypo11SwSUZLEyQWjRrKNWrVtHb3o8/165XY7BM1qhnp5oq07BBbWlQb39vsQLvv2ju48L0E2OxcvVafX/jGuvUqRFRZMvJyVHP00otdFWstJe0PqCZvm/yXBQzSIAEiiwBvj/w/cHcnHx/MCRE+P4gEut7Dt8fcu8fP7YoMFBgMP9thWexJKVJkyYJJp47dUq8wFCSOBana6XAUJxGq3B9pcBQOH6FPpsCQ6ERFssKRo59W67vNUD3Ha5yVqsJcOOn/8WRA+SyC8/Mc12YVLjmpgfl/Q+nhBzDKvhH+98qF557qs3ftWu3DBwyKmSi2xy8p3cPuf+u6yUzM9cHPtwtdL/hAfnn3y2mmP6+6/arpf+9N9iJ7fwEhmjruPuBJ+TRx8fq+sc895C8/9EUeW/S53p/z6af9Pc7Ez+Rq3vm7c8xRx6s4z6c0OFwcSes9Un/fYDH8nmT9R5cFF1/6wBbvykHoWXCq8PVBHMDkyWYfH14yAty/8BnbR42nn+yn1x780M2Lz8XSZgoLrvPwbbsZRedKS8+Hxxnm6k22ne8VAkqc3XW6accI5PefEpvYzJ60LDRefqAg+0PPUheHTdY6tWuqcvi48DDu8i8+Uv0PlgOGjpai0SmwJND7pJzOp0gp5x9rS2HY7AgePKxu6zlxA23PywjXnhTn/b00L7y0f99JR9N/spUI7hHX3rhEWmixAaklavWSt0WJ9vjO9Z/b0Wu2XN/k7ZHXqCP7Vt9H1m96POQSXh7ktpw7/Vo7x/3fHc7nMDw5TffS8fTrtbFupx9ohZWhj013j1N+t19nXoerrN5eHbuG/C0PDZ8nM0zGx2PO0zGjxyo3VpFc/9Fcx+j7nj6ifN+/uU3uejKO0PGHPm4V54aercc3LoFdnX68ef5csU1fUPuAxzA/feyGtuC3H4Fa+EnCZCA3wT4/sD3B74/8P0B78FIfH/w+xc5/vYpMFBgoMCQOIGhXbt28T+MPDPpBN5//31aMCSdctFogAKDz+OQCIFhz8oFsmfl/LBXUqZpeylVeb+wx5jpH4FjT7lSpk0PTqbDXzwmbM3E54kd28vk90aEdA4rzL1ud0IKqJ2P33tOTup4hM6+qFtveWvC/3mL2P377rpWHrj7er2PwM3X3dLfHvNuuJPkkQSGWOpwBQZvWxAYpk77Xo4/PTgp7D1u9hf8OEnx+1Guur6fybLfZlK7IGZ771VBPv1glLRrG7RSyK9ftnK1kZ/AAIuLRgeebotP+2y8tG93kN03GxCU1qxZp3f32ivbTtxfcc098sobH5pieb7R59kqEDisGZBcgSFP4f8ywCNcLIhHHrxFevfqpku5AkOketD2kjkfaWuFWAWGmo2PD9sHiCJdLzlLYrl/IvUvnMAQzb2E+t55ZZh0PvN4XfW9/Z+WR5TQFClh0n7m1Fdl/GuT8r3/omkb93FjZVESTVn0x+0nBIN2x14UqZuC8UJA8v1r7Suu6BPuBLjv+nTSKKm6T64lSrhyzCMBEvCfAN8fggKDdyT4/sD3B+89gX33HTbcceTx/SGUDN8fQnkka48CAwUGCgyJExiS9Zyy3uJHgC6S/B0zCgz+8pdECAw7Z06UHbPeD3sl2WfcLJn1W4c9xkx/CMAFUdM2nWzj8PuLCdsOp15l8/5Y8ElI8F+sar/nweAqdxR6SFkVwLXQ40+/ZCduTz7hSPnfhGdl2oyf5NiTr7R1IaAuVqCPGvuO/KBc3pi07a9ZymJhq54QN5YLnTsdL5ddcIauA3Wb9O0XL8thBx8g4QQGuDfCpHq0dYSbyIfVQRNlVfDp+yPlxjseludGBVfTY3X1jddcpN0AXXr13baNcc/3l5OPP1J+W7xcOp13k81/ZcwjypVNHd3XAY+OtJYAmGS/45augu9hT70ks5ULJCRjPeCdMAeviy84Xb6d+XMeoSY/geGzKTPkZGUtYFJ+ZU0Z8/2NauuYk7qaXbnh2ovk7NM7yh8r10iv3o/aa+x66Vky5tmgRYVXYMAxCCtecan7lV306v3Bw8baemDFgfgPSF6B4aj2beTsMzvKnHmLZPyrk2yfbrvpCnlswG0xWzDM/GGOtiIZPGyMrgv3at87u0vTRvWU5UPpmO4f2xnPRjQTBJhIv+2my5W1SkBuuXOwZdGj27kyYvh92r2Ta4Ey4ZXH5dB2B8hnU6bLldfeZ1ucPf0d5Tapcr73X7T38eUXdcojMBTUT3TklM7XyadfTNd9wvPT59arZNGS3+WJZ1+x/Rz0UC+54+aucmKnHjLlq6BPUlM3RKf+g0ZaBo8NvE1uu/EKey43SIAEih4Bvj/kWkCa0eH7gwjfH4LWtwW9w5p7xvvN9we+P3jviVTsU2CgwECBgQJDKn5rSlobFBj8HXEKDP7yp8DgM38/mnfFAqyGnvXla3oCvVq9Y+1kH1zb3NAjd3Vyk9Zn2uDAg5U7JEwaIn2tVvEfd0pwFTr24arm5t6D5PnRb2FXTj3paPnw7af1Nvz1V6wZtHBAxvQvXpGlKuDwxd366OOYfP9j/ifW1c25l91mXQshRgB8/YYTGN6cMDmmOrwCw2fKiqDDMYfqPuADxxEzAWng/TdroQWT5od3uMS6dxnQ70a5+/buukykGAwus9HPPihXXnq2Lg+3MgcfHXThgwyIORMmfiq39B6sj2Pl1sqFn0qF7PJ6v++DT4qZGEdGfqLB6PETlBur4OQ/yhqXT9guKHW99l55+fUPdDFM8H85eZw95aXXJ4VMcK9b/qWObeAKDE8NuVt69rhQn+PWBbEEY4308adfyxnn3qi3cZ1/r5ymt12BoWXzhjJjyqvWfVL3Gx+QsS+9p8sZUcIryBTkIgknuy6F3CDPsd4/uiNhPqKZIFgy93/WxZQrQMH1EcQtxDepXv84WzsElcsvPlNaNW8sk/43RVat/ksfO+XEI6WRErKQIt1/sdzHXguGgvq5YtWfUq/FKbp9fLiWMo8MfUHufSj4zENAg3DiurOa9917Om4Lznt21Bty0x2PYFPHZzGCk87gBwmQQJEjwPeHUIGB7w/BW9T9N5/vD5kaSrh32EgPNN8fgpa2fH+IdIckJ58CAwUGCgwUGJLz61Kya6XA4O/4U2Dwlz8FBp/5+9E8XB0hmDISVhjfecuVervnbQOtMGCEBxyAT/isarl+Bb/78nVp27q5PgcfJmAytsuXLycnnnWNfDF1JnbFK1QgCB5WbyOVK1dGBj8+xk5GYsL5iMNzrV3mzV9s40KYSepwAoP7HyTR1OEKDK4Aojv13wcEBsRm+FK5S1qkghYbf8OmTEECg5cZfNJXrLiXOV3+77Nv7DZcS338yTQZ/szLOg8r4N55eZg9jqC4jQ86w+7nJzC898Hncu6lt9myW9fODAm0bA+E2Tj02IuthYl7fSiKAM0NW51mz4JggDFxBYZP3n9ejj/ucF1m4GOjpN+AZ/T29T0ukKeH9NXbXlc5RgBxBQZYx9xzZw/b1sQPvpAul95q9yEmIACyO2ldGIEh1vvHdsSzUdAEgRubA6e+8c7HcslVd+laENtjyv+C1hWndekZcn+gAO7rU046Ui485xRl2XF8SPySSAIDzov2PnYFhmj6+dW3P4RYPJlxRJuIA4JA1kiIswLrEVeEhPWISevWbbT3HPI2r/7WCmumDL9JgASKDgG+P+QKDHx/yL0v+f4Q/He6oHfYXGKhW3x/CPLg+0PofZHsPQoMFBgoMFBgSPbvTEmsnwKDv6NOgcFf/hQYfOaf6ubhogj/IWhSw/q1tY907C9YuMy6O8K+WWnsnVxG0FxYG0RK7oSn67M9XHlX1Ah33OSZlevhBIZY63AFhjt7XSmDHuxlmtHfM777RY484fKQPO+OOwHvXi/cTTWsV1u8K7y957v7cIMz7tWJNnj2A32v14GkTZnt23dIhf2CE/fIy09g8FpHGB/7pi7zDQuAlWvW6t1Ke++lV5RX3v8oa8GCoM9Yfe4m9zj6DBdGrsDwxf9Gy7FHHqJPeXT4WLn7/if0dq8bLpOhD9+ht6MRGOBm6qJzc8WM+b8tlVaHnmO7sv73r2TLlm0RBQaXAe5T3K9IkSwYYr1/bEc8GwVNEHhXdSKo9IVd79S1uAIDXE2ce8mtIc+i21SHY9opq6BnJCurnM4Od//hQCz3sSswRNPPV9/6SC7vHhSNYHHyy4wJbhdDtl0hJeRAmJ01S76Q6lWrhDnCLBIgAb8J8P1horZwfPTxsXoo+P4gwveH3IU5+T2f5h02Uhm+P4Qnw/eH8FwSlUuBgQIDBQYKDIn6PWE9uQQoMOSy8GOLAoMf1J02GYPBgVECNvv0e1yGPPFiVFf64D095d7e18jmf7ZIldpH2XPg/71Vi0Z6f/fuPQKfzEj4MW3SqK6073ipXfH/3PB75Zpu5+nj+MBqfLO6uV6dmjLkyRdtnAJMaF+pfPiHSxUqlJdTTzw6rIsk19VMNHW4AoN3tTysMRAQ2MRzgOuaqy7vLIcdcqAKBPy2ZVeQwABLjex9D7OXMn7UQMkqV9buuxtHHN5GMxgz/l2dfX6Xk+X1sY/aIlgBfkTHy+x+fgKDd6zcYNq2ArXh/scsYmSMeup+LTyZGBlekWPJ8hXS5KAzbRUIMnxIm5ZJERi8Vi/uRDw6gNXyq9b8JXWanWT788+a6dalkuuGKRqBIdb7xzbq2XCZInB6r56XhcQ2iGbi3lT575atMvnTacqKZqq2ZvAGyX588J1y83WX6uLhBIZY7+NYBQaXsevqCh3Cs2NcOe1VIVsWqd8HEzQd1hHDFJtIqdNpHaRs2TKRDjOfBEjARwJ8fzg6RGDg+4MI3x9yY21F8/4Z6fHl+0OQDN8fIt0hycmnwECBgQIDBYbk/LqU7FopMPg7/hQY/OVPCwaf+aeyeYgBdZqfFHFltLcvsG747adJgpcPd/X6iCfukx5XnquLfzT5K+l0wU1625S/vEdfee2t/+m8C887VV4dPUhvr1v/t+zXsIPexgf8vM+YOdvGT8AELNzE4EcZCW6JfpkbDIbcoN7+0vHYw8IKDK4P/WjqyE9g+PHn+dLu2NzYEybWAPrjuq6JJDCAl/GN78Zg+L+Jz8sJHYJWCBBYsAI8JycH1co5nU6QF16cIHf1G673MWH717KpOog2Mlx3Q9jPT2DA8Yu69Q4JsuxaFuC4d0WYsVa4qmc/efGVYLB2uHSa9ul4FNdp7MvvSfcbHvhvL7cPybBgwBh+8dEY6wbo2lsekhfGBVfII0Dwj9PezCN6/fzt23JAy8a6f+74RhIY3EDVsd4/FoJnIxETBF98OVNGjAnGL2netIE82LenBAIB+Wn2AhVb40HrTsjtvyswmPsv1vs4VoHBa1UCCwZYMiDddf9weWz4OL2N4NX39b42xNpk7dKpUnWfSvo4LH2Mu7BySoC7VAV4ZyIBEih6BPj+EHwHcf998QoMsf7uYpTD/X4jn+8P8VlA8v2B7w94fpiiI0CBgQIDBQYKDNH9WrBULAQoMMRCK/FlKTAknmlMNdKCISZcxbrw51NnyElnXWuvYc7Md6VFswZ2HxtYNX36uTfYPONrH4Gbn3n+dZv/9NC+OhjzsKfG23gODz9ws/S59SrxrjjvfWs3aXNgMxmjAvV++sV0XceJHdvL5PdGiDdY72UXnSkXn3+qzP9tmTww8DlrSfDqmEFy4bmnhhUYYq0jlgmCbz57SWrWqK6DH9/X/2l7/e7EgjtB0P++G+U0FdgaMSrc1Z6Y6MaxWqquUS++Y90hwWwek7OwAmnZrrOtH37qYc2xUMV/uH/gszYfGwUJDKiraZtOIeecd85J2g0S3Ad98L+p9hhibUBIwKpxr0sdTGKfd/ZJsuz3VTYQL040KxaxXRiBwV357sZgQL3wbX1e5xNlzrxFNjYF8h8dcKvcflNXPelepfbR9v5APIjrul+gyz/+9EsoqlMkgQHccQ9jnLDa343nUNA9aOr2fidCYPDGNsB9f/QRB+vgzzeoGCnTpv+kmy3o/kMhVygr6D6OVWCAn+Q2R55ng55DXLj7ju6yZOkKbeVjLIC++uRFOUKJVYcdd4kVRyBe3XbT5bJdWfkMGjra1nHt1efLs8Pu8WLlPgmQQBEgwPeH4DsI3x/4/sD3h+APknkP4ftDEfiBjrMLFBgoMFBgoMAQ588HT8uHAAWGfOCk4BAFhhRAzq8JCgz50UmvY91vfEDGqkl+pEh+073+/m+98XIZMvD2PEKAlwwmcr//+g09gY6V+Yd3uNROKHrLYt9d0T/mpXelx40Phium87BqHYGlETA2XAwGFIqljvwmCLyuZSJ1yo0rcM4lvaxggPJmUnvrtu3Suv15smTZikjViBuj4va+Q0Im0yOdVJDAgPNGj5+gVrw/FKkKnY8J/p++fUvq161ly/VQq+SNqyab6WzgnLmz3rVxO5IlMDhN2k1wXfzLR9YVkivg2EKeDTMWyPb6D0femOcekq6XnBXT/YPzwqVECAwIDt7htKtk+qzZ4ZqwefN/eF+7I0NGuPtvyS//C3H1ZU/0bJj7ONYJAlTzwcdT5ewLb/HUmLuLWBGfThqlLaAgFLU+ItdVWm6p3K2lcz+WurVr5GZwiwRIoMgQ4PtD8B2E7w/BW5LvD3x/4PtDkfl5jrsjFBgoMFBgoMAQ9w8IT4xIgAJDRDQpOUCBISWYIzdCgSEym3Q6gonLrGrt7CUNvP8mueu2q+2+u3HJ1XfJG29/rLPcCVoEgb7oyt4ye07QbZE5B6vH31FBf/evta/Jkr83/SPX9eof4qoHB1HfWy8PlaPbt7VlsTFh4qdy692PyYqVf4bkY7U8fLbjP2aRhj/7stx+9xC9fdtNV8hjA27T2/iItg53gsB1dWQqwmTrRSr4ruv3HhYAp598jFx1fT9TTIz7pK+n/yg9lPug3xYt18dcZmv+XCf3KssHI+yYk1Hm5RcesW6TkI9V4Q8NHiEDBo80xfT3e68Nl2tufsj2Z+OKaVJx7yCPkIKeHQRU7nrNvXnGC8UuvfAMGfRQLy0IuadBHHriuVfkjr5D3Wy9DauTsSP6h5zT9qgLbP1TJ4+144rYGn3ue1yf546TO9GMMf175TRdxrVguKd3D/ny6+/lq29+sH3AZPVLihcsQEyCyw7cYy5brCxEfIIzzr1RF3PHAq6GXPddKGAEBmxHe/+gbLjkCgwmRoJrkeAGcsb5aO/8K4LxCHB9n33wgq4W8SXgLuuVNz7M0wzEtice62ODaaNApPsvlvt4zq+LpMOpV+n2ou0nCsNF2iVX3WUtSXQF6gPWL7BGMIGokQ/3IbcoSyhjhWHKwqIBMUqMazGTz28SIIGiQYDvD7nvIHx/4PuDeSr5/iD6PZjvD+aOKH7fFBgoMFBgoMBQ/H65in6PKTD4O0YUGPzlzxgMPvMvbs1jkvYPJQIsWLhUd/3Alk2kxn7VIl4G4i7MV2U3rN8kLVVg6Ib197cxFrwnmYDRi5f8IVWrVlbumxpGNZHu1pOIOlAfrA8wQb9duc85pG1LK3C4bbnbmJhfs3a9ILhz+axyeZhs2LhJcVgmmzf/I02b1Jf6dWpF5LBp87/y0y8LZC8V2Bp8Cxv0duPfm2XegiXyx4rVUqd2TcEkfPWqVdzu59lGnAi4Z1qk/qpVryLNmzSwfvPzFE5AhiswvPDMA9Ltss6Ce2fegsXSrHF92W/fqhFbQV9xj1WqtLfUrrmfjd0Q6QRY6fy5boOUUv+rpu6z7PJZtmii7h9bYSE2IE4t+2OVrFmzTiqra9u/1n5qEr522Psm0v0X630cT3cx+Qi3XAuVwLaP4nlQqyYRnxf8fkBEhBgHiyTEmcjv9yOe/vAcEiCBokmA7w/hxyXS77cpzfcHQyL8N98f8nLh+0NeJszJS4ACAwUGCgwUGPL+MjCnsAQoMBSWYOHOp8BQOH6FPpsWDIVGyApIgAQKSSDcBEEhq+TpJEACJEACJEACaU6A7w9pPsC8vKQRoMBAgYECAwWGpP3AlOCKKTD4O/gUGPzlTwsGn/mzeRIgARFOEPAuIAESIAESIAESiJUA3x9iJcbyJBAkkPPXctm99KewOEo3aCMZ1euFPZYOmSsWTJZN6ygwUGCgwJDSXEjaAABAAElEQVQOz3NRuwYKDP6OCAUGf/lTYPCZP5snARKgwMB7gARIgARIgARIIHYCFBhiZ8YzSKCkE6DAELwDKDBQYCjpvwXJuH4KDMmgGn2dFBiiZ5WUknSRlBSsrJQESCAGAotU3I1Va9bqM5o2qke//DGwY1ESIAESIAESKKkE+P5QUkee100C8ROgwBBkR4GBAkP8TxHPjESAAkMkMqnJp8CQGs4RW6HAEBEND5AACZAACZAACZAACZAACZAACZAACaQJAQoMwYGkwECBIU0e6SJ1GRQY/B0OCgz+8qeLJJ/5s3kSIAESIAESIAESIAESIAESIAESIIHkE6DAEGRMgYECQ/KftpLXAgUGf8ecAoO//Ckw+MyfzZMACZAACZAACZAACZAACZAACZAACSSfAAWGIGMKDBQYkv+0lbwWKDD4O+YUGPzln1YCw8Xd+siiJb/7TJTNkwAJkAAJkEByCdzZ60q54JxTkttICa/97ItukVWrg7FhSjgKXj4JkAAJkEAxInDNVedJj67nFqMep7arFBiCvCkwUGBI7ZNXMlqjwODvOFNg8Jd/2gkMb06Y7DNRNk8CJEACJEACySXw2rjBFBiSi1ggMHzwv6lJboXVkwAJkAAJkEBiCYx48j4KDPkgpcAQhEOBgQJDPo8JD8VJgAJDnOASdBoFhgSBjLeadAryDAsGCAyvv/66NG7cOF4kPI8ESIAESIAEiiSBxx57TN544w2hwJD84TECw/vvvy+1atVKfoNsgQRIgARIgAQKQWDkyJGCPwoM+UOkwBDkQ4GBAkP+TwqPxkOAAkM81BJ3DgWGxLGMq6ZECAxxNZyEk1yB4cILL0xCC6ySBEiABEiABPwjcNFFF1FgSBF+V2Do1KlTilplMyRAAiRAAiQQH4Frr72WAkMU6CgwBCFRYKDAEMXjwiIxEqDAECOwBBenwJBgoLFWR4EhVmIsTwIkQAIkQAL+EKDAkDruFBhSx5otkQAJkAAJFJ4ABYboGFJgCHKiwJA4gWHChAnR3Xws5QuB9u3bp8wamQKDL0NsG6XAYFH4s0GBwR/ubJUESIAESIAEYiVAgSFWYvGXp8AQPzueSQIkQAIkkHoCFBiiY06BIciJAkPiBIb9999fVq1aFd0NyFIpJ7By5UoKDCmn7k+DFBj84W5bpcBgUXCDBEiABEiABIo0AQoMqRseCgypY82WSIAESIAECk+AAkN0DCkwBDlRYEi8wNClS5fobkKWSgmB6dOna+GHAkNKcBeJRigw+DwMFBh8HgA2TwIkQAIkQAJREqDAECWoBBSjwJAAiKyCBEiABEggZQQoMESHmgJDkBMFhsQLDKmcyI7ubi/ZpYxlSSrHhS6S/L3nKDD4y18oMPg8AGyeBEiABEiABKIkQIEhSlAJKEaBIQEQWQUJkAAJkEDKCFBgiA41BYYgJwoMFBiie2KKbykKDMV37OLtOQWGeMkl6DwKDAkCyWpIgARIgARIIMkEKDAkGbBTPQUGBwY3SYAESIAEijwBCgzRDREFhiAnCgwUGKJ7YopvKQoMxXfs4u05BYZ4ySXoPAoMCQLJakiABEiABEggyQQoMCQZsFM9BQYHBjdJgARIgASKPAEKDNENEQWGICcKDBQYontiim8pCgzFd+zi7TkFhnjJJeg8CgwJAslqSIAESIAESCDJBCgwJBmwUz0FBgcGN0mABEiABIo8AQoM0Q0RBYYgJwoMFBiie2KKbykKDMV37OLtOQWGeMkl6DwKDAkCyWpIgAQsgR07dkggEJCsrCybx43oCfz999+yfv16qVevnpQuXTr6E1ky7QlQYEjdEFNgSB1rtkQCJEACJFB4AhQYomNIgSHIiQIDBYbonpjiW4oCQ/Edu3h7ToEhXnIJOo8CQ4JAFtFq7rvvPhk5cqTuXdmyZWXmzJlSs2bNkN4++uijMnToUJ3Xu3dvuf322+Wss86SGTNm6LwTTzxRXnnllZBzsPPhhx/KVVddZfPnzp0rY8aMsXWdfvrpMnbsWH38kUcekeHDh9uy7ka1atWkVatWctxxx0m3bt0kOzvbHt66das0aNDA7n/yySdy0EEH2X2z8c0338g555xjduX444+X1157ze7nt9GlSxeZNm1afkVCjoEFmERK4NmnTx99+Oabb5YnnnhCb2PCfc+ePXo7IyND8Od3csfeHa94+oWxuuuuu+SNN96QtWvX6ip27twpZcqUiae6EncORAU8r++9956sWLHCXv9RRx2ln417771Xypcvb/NTtbF7927dFP4jJDMzM65mp0+fLkcccYQ+t2nTprJgwYK46uFJIhQYUncXUGBIHWu2RAIkQAIkUHgCFBiiY0iBIciJAgMFhuiemOJbigJD8R27eHtOgSFecgk6jwJDgkAW0Wpuuukmefrpp23vrr76annhhRfsPjYwqTlgwACdd//998sDDzwggwcP1pPFpuCWLVtCJv6Rf/3118uIESN0kfbt28u3334bUlenTrn/aN99990yaNAgU13E74YNG8qUKVOkTp06ugza3WuvvWz5WbNmSbt27ey+2bjhhhvk2WefNbv6+88//5R99903JC/cztFHHx2TwPD+++8Lri1SwnXiepFuvPFGeeqpp/T2yy+/LJdffrneDjcO+kCKP9yxd8crnm7gvnnwwQdDToUlA4QtpvwJYAL+7LPPtsJMuNKHHnqoFh9q1aoV7nBS8pYtW2YFvtq1a8sff/wRVzsQACGUIFFgiAuhPYkCg0WR9A0KDElHzAZIgARIgAQSSCAmgWHrJtmzcU3Y1jOr1BDJrhT2WDpkUmAIjiIFhty5isLe135MZBe2zyXhfD/GBYtIsbAUf0ypJ0CBIfXMQ1qkwBCCI+12vAIDLvCHH36Qtm3b2mt1J5mNwLBw4UI9EWgKTZ48WU4++WSzq38wa9SoYSdEIWJgkt+ty52w9goMEBKQ/vrrL/nnn39svdiAhcKPP/6oV/hHIzBgErt69ep56nnuuefkuuuuC6k73E7nzp3l888/t4e8/dl7773tMWy8/fbbISxCDqqd0aNHCywDkLp27Sp9+/bV267AAMsPlPM7RRqvePrVsWNHLQ7hXAhOZ5xxhhZa4l31Hk8fiuM5sPxo1qxZiNUCJvNbt24tv/76qyxZssReFvKx+t+18rEHk7CxdOlSMc8qxDqIdvGkn3/+WS644AJ9Kq4VIh1TfAQoMMTHLZ6zKDDEQ43nkAAJkAAJ+EUgFoFh19ypsn3K+LBdzepwhZRpdVzYY+mQSYEhOIoUGCgwpMPznN81UGDIj056HqPA4PO4UmDweQCS3Hw4geGYY46RqVOninmpcCeZjcCAbh1yyCFajMD2bbfdZl0fYR8Thm3atMGmTitXrhSsrHbriiQweCfX4Wseq99dSwsjaEQjMEyaNEm7dDJ9Md/GqsLsR/v9+uuvy8UXX6yLQ1zYvHlzyKmYEN62bZvOq1ixonYBhGuA+6nTTjtNtm/fLug3ElzamMngaAQGnPvbb7/JvHnztO/9li1bSpMmTfK4GYLrIdMH1A83RHCxg5XwVatW1S6nTLu6IxE+Io0XrnHXrl36LDCAEv/7778L3GBhkhv98goHcHOFfiN9/fXXdsW6zlAfOTk5gglrTJpXqlRJT6CDn5vgjmfTpk06y7ADE7iwwv1YuXJlW3zNmjW6PQhMuBe9rr9QEGOH1QPoP64D27Nnz9YT5eDqut+yFf+3sWHDBl3/v//+qy1qMDEeKR4C2sG1o0/g0KhRo6hdYOGZe+ihh2zzcCV2yy236H3098knn5RevXrZ49jHc+0mjP38+fP1vQOxrUWLFlK3bt08fYiVRzQCA+6VxYsX6/ujcePG+n71uv9yxxUMMf5IhbmPoxl/3EvGxdM+++yjf/PQVwib+H0ojokCQ+pGjQJD6lizJRIgARIggcIToMAQHUMKDEFOZi6gpK20NnMH7lxFdHdO5FJ+TGRH7g2PGAJ+jAstGAx9n77VDxqTjwSW//B8YPZHPQIbFr8d2LPpp2L9d0GXU2CHFFATxD4SLVpNKxc9mgm4uH/vvPOO7ajy7W6PqclOm//YY4/ZfLWK2eZjQ7lQssc6dOhgj7l1qX+0bb7yzW/LK4HB5rsbygWMLWPOVZO7Ng/9Vy6S3FP0tloZbcuoOAx2G+XVZGKe8gVlqNgNtg41KZ2nuIqvYI+rGBMBt00UVvEm7HHwR1KT8jbPHYfx48fr4/hQvvcDaM89jm21cjyg3EbZctjo37+/LafcWwWUtYDdxzmo59133w05J9xOpPHCmJp+vPjiiwFlVWL3kY99JTboKl966aWQY+Y8fKuJXF1GWc0ElCiRpxz6rSbHbdc++OADW6Znz54BJTzZfdSBpFbRhzA37SnXOwE1yW/rUpPX9lzwUAKPZmnK4xtjpyxW7DnYUPEjAspdkT3XlMcY4t5wkxI3AnhmTBnzjfYmTJjgFg27rSa/Q85VLrTClnOfDTAzSYlAASVOhNRh+qAm0APKxZEpGoiVh3LjFbZe5COhPnd8TLu4dvBz21YCka0L42RSPPdxtOOPNg4++GDb7hdffGHvY/SvuKYLL7xQX9Nr4wYX63+vi8P7xpmnHadZK4ub4nq7sN8kQAIkQAIliMA111yj/90a8eR9Bb4jbP/micDfDx8S9g/HisO/0/H2cfnMwXr+Y+OqvP9dWYJuF/uOXJKuGdeK9zr8d4uZb0jE9auFlrpOtegyEdWxjgQR8GNclHCn74UEXQKriZEALRjUr5ufiRYMftJPftuuBQOCIKtJZ90oVqHDDVJWVpbcc8898vDDD+t8NVkqatJQb2OlL1Ykm4TyZt+NW4Ag0j169NDFIq2Id10keS0YTP3Dhg3TAaaxb9yxFGTBgJXbVapUMVUIYjQgvgFWqSMhuDQCD8eSCrJgQH2IUREuqd8/HWvCG4Nhv/32s+6k3PMQBPvKK6/U7pK6d+/uHsqzPXHiRGupoSZmpV+/fnnKeDOWL1+uV7J7881+pPFy3R2Zst5v48pKiSQ6OLf3OPbhUgcrzeHuJ1JCPXBRBcsLBA4/88wzwxaFay9YEXjdCbmF1eS2rgtxOmCBEU38B1gHPP7447qacO6K3PqxbaxrsA1LF9wvkRLineB+jJTcGAcogzgm4VbWr169WnNEGcQkgfUFEmJ6wDImUgKPOXPm6HsgVh4IuD5u3Lg8VcPtF/ILugfhWmnRokXaaiBSDIaC6jCNm/u4oPFxxx/nulZYOGbcnyHeBYJpF8dEC4bUjRotGFLHmi2RAAmQAAkUngAtGKJjSAuGICdaMNBFUnRPTPEtRQuG4jt2cfc8RkGCxRNMgBYMCQZaxKpzLRjUBH4A1gPqYdV/Kk6A7q2KEWDzsBrbTe7KaRVEWR9S7oBsedSFFcUmRVoRH40FA6wqTN/wjdXhBVkwwALAnIMV5soNTwAr+k2eu1ra9LGg71gsGEw7aPv444/XVYezYJgxY0bAZXDKKacEvvzyy4CafA9s3LgxxHIB9WAlvxIU7IprtAMrEjBB8q5ax0r8t956K6CCLNtrxzmvvvqqLh/pI9J4uRYMqAd9x4p8NbkZUr+a+NUr/tXEuO6f4fH8888HkIdV7m5dsHyARQSsY9SEr61ryJAhuouuBYOpC+VwHyqBK+S6YdmBerwWFmaFv7tiH3WhnmeeeSbw5ptvhnDF2JmkhDbbJ5yDFfYqaHeI5QPGDglWJaaP+L711lsDKj5HiPUD2lST4qb6PN+ffPJJSB2uNUeewp4M1yoA7eNZR31jxowJYasELH1mrDyUyyVtsWGuEdeCexb5SHi2zDFcu3K7FlBB320ejimBQZd1++o+k7Hex275gsYfDbsWDKavaF8JgLpfxfGDFgyps7SkBUNxfELYZxIgARIouQRowRDdOwItGILPiHk3LmlPDC0YSs6I04Kh5Iy1uVIxG/z2hwAFBn+4p6pVr8AAsz3zMoFvtTI6kJ/AoFZ22/Jm4hYT2aYOM9lqrifShLU7uR7JRZJ3cln5iy9QYDjppJNsX3r37q27oeJD2Dz007jWMX0s6DtWgUGtvg+pMpzAgAKuKyGXASaEDU98Q1QxCRO07jEwQnInWjFhisljk9wJ/YEDB5rssN+RxsutQ1mn2HNV3IeQ/nz11Vf2mDuZC0EFacWKFSHlzeQ0jqmYG/aYmXT23gOY3HcTRBbDA9xM+umnn2w+juM+906ow0WSSV5xAK6GkNz6jQCHfFynaRffKH/99dfbPNe1EUQgTMab8hAdIqVRo0bZcigfS0Kbpo2jjjoq5FSILuYYvlU8ibh4qADTth5M6JuE+pQlg/5TVlJa2MMxiEBuu+b+iEZgiOY+dsenoPFHf9x7EueqOCLILtaJAkN0kwfxuk1wz6PAUKwfFXaeBEiABEocAQoM0b0jUGAIPhrmnb2kPSgUGErOiFNgKDljba6ULpLUL7ufiS6S/KSf/LZdF0lwQaRWGmv3PmrCXzeuXkSlWrVqYV0koYDXhQtcFqFONbmnzzcufvSO+ojkcicaF0lPPPFESDBb9SOhgyXDJYxJcIEE9zdICCwNV08mue5lEGRXTY7qQypmgnZbZMoV9B2LiyQEdf7oo49CqlST4uJ1kYQCkYI8o39qMlvXoawX5LPPPgupr06dOqIm6nXe0KFDdcBt17UMXPDAFY9JauJb1EpyvassGvJ1pRRpvFwXSUoYkcsuu8xUrwMYG7bKr70oMUIfc93RKIFBDjvsMB3sGUHFTVKClNnUgXbh9sgkBFNWE/8hLpLg1scEVva6+IErIRMsGHXAdZFJ2MY1uC6S0GcT1Bk8wdUkJQpoVz5uefStbdu2pogNqo0MBKA+4YQTtDsm7KvJa+u2CPtwCWTc8eBeMC7IcMxNcFnWpUsXm4WA1uXKlbP7+W24vJWQJEootMXhUqh+/fp2HwHIEQjbvb6CeKBsQUGe4QIJ9yv+EPgdAcrdpAQGgTu1aFwkFXQfg6Pb/4LG/+STTw5xkYRn7M4773S7Vyy36SIpdcNGF0mpY82WSIAESIAECk+ALpKiY0gXSUFOdJFEF0nRPTHFtxRdJBXfsYu35xQY4iWXoPMoMCQIZBGtJpzAoFahi1otbCetMakNH/hIbgwGc0lHHHGETJ8+Xe9iMl2toLWTp8pdkuyzzz6maKEEBogdakW3rksFBBYVRDhfgcErSKhV3JKRkaHPx8SmSYjnsGrVKsnMzDRZ+X7HIjAoq4k88RhiFRjAU7nt0X0KV58KYi3KakQfh0AEocgVGLxj5o55IgSGTz/9VE+mG2iIgWAmkgsSGFyW5vxI3yq4smAi3MRgUKvP5fvvv7fFvaKAPRBmAxP3yuImZEIa/vshDCDhfsALh0kQGBDnwJ2UR/wI3DuRkitiRSqDfGVFJE899VTYIpiUx8S/SW6cE5OHb4hp+EOCqIIxqFixon0OleWHvl5d4L8P93g8PAoSGBCHxAiVbrvudiwCQ0H3MWK3uKKQ2453G9fbuXPnEIEhHCPvecVhnwJD6kaJAkPqWLMlEiABEiCBwhOgwBAdQwoMQU4UGCgwRPfEFN9SFBiK79jF23MKDPGSS9B5iRAYds+fJjt/nRa2R1lHnCsZNRqFPZbozIu79ZE3J0zWQVcxacsk2tpAuaLRKIwFA3a8K6cNK+8kH/JxPiatkTDpa1add+qU9x/lSCviC7JgwIRv8+bN7YSpCbybX5BnBA42wZx15/L5wMr44447Lp8SuYfcSXHl6kaUq6bcg2oLk6omyDMm+pWboZDjsQoMbn1Y7a/83IfU51owIBgx2BQXgUH55bcWDrA2McGUQy7wv52zzjpLVAwBKzDAMgIChkkQARCU3CRYVrj7Jh/fRx55pFSvXj0mgQHWA65FBIIjt2rVSle7e/duQdBzJIhYCLLsWnlAGFJxIvRx7wesJmBtEC7h3nLbROBuiELe5Ip8ZqW/a8HgFZJgnQABxKTvvvtOEEzbtQAoSHDJT2D48ccf9W+BqR99QhB5WK2ceOKJ9rlMpMAASx93vAsaf2USGyIw4F489thjTZeL7TcFhtQNHQWG1LFmSyRAAiRAAoUnEJPAMO9L2f7Fi2EbzerYVcq0LP7vTGEvTmVSYAiSocCQdy4j0j1TUL4fE9kF9YnHRS8oxDwTFurhvw1TkTBXAE8c+GNKPQEKDKlnHtJiIgSGnTMnyo5Z74fUa3ayz7hZMuu3NrtJ/abAkBevu5rdFRhQ0p0gNWeGExj++OMPqVu3riliv+Hy59JLL7X72IhHYMBkKCYoIQKYpHzqCwSESALDvHnz7OSvOSe/7+uuu06ee+65/IrYY6kQGFTgXYF7KaRx48ZJt27dbPubNm3Sq9ORsWDBAi28mIOwIIFbpuIiMHjdWK1bt06qVq2qLwcWCcatEdwCwQ3Thx9+GFFgwEmu1QDECExmI6l4C/LKK6+ICvKt9+F2CK61Yp1Qd1f9q0DVAqsaJLdfcIcE10A33HCDvadw36u4GLosPpRvT+0CCtsQO1q0aIHNsMm1YEEB70S4ez/iuFmJj3sG9w4S3AXBRZhJcGGGZ8okFTxasrOzY+YRyUUSnqWePXvq6mE59PXXX+ttrwiUSIEB4kss41+lShUKDOYG4HdcBCgwxIWNJ5EACZAACfhEICaBYe5U2T5lfNieZnW4Qsq0im5hVtgKingmBYbgAFFgoMBQxB/VQnfPD+GHAkOhh61wFZhgDPz2h0Aigjxv++TBwN8PHxL2b+fPLwbcoInJ3L6gyymQCQNqQs4fmEWwVW+QZ7eL3sC4YKcEBreI3UYQWRx3/9SkpT1uNiIFDVar9O25CICL+vDnBm01davJd1NdxCDP6Kcpj+CzaoW5PcdsKKsJWwZtIkBxNCmWIM9qoj9PldEEeUZAWzVBHlAupgJKUAgJCqwmiwPvvPNOAP1AOXOdYGWCObtBnr1j5o65Wtmep39uRqTxUtYDtl3lIsk9JaRPysLAHnMD6pogz2rCPyTQLq4NQcIR8Fq5wbJtKAFI1+MGeUYfvEn50LfnYNxHjhwZwDlnn322zQczBGH2BnlWK/ZtdUr4sOXBV02M62MuO+Q/88wzASU0hFwzxhdp0qRJIXWANcbU7SPqUAKaLh/pwxvIG+ecf/75AYyNshIKaQOMTV+V27KQY3huwMINno26lNCgm46HB/qOOsyfsnzS16NcPtk83JfgqSybApdcconNxzlKLNFtRxPkOZr72GVb0PijYfeeNH3RHSrGH0qQ0oxfGzc4Zf+2JvPf7aJcN4M8F+MHhV0nARIggRJIgEGeGeQ5ltvevN/Hck46lGWQ53QYxeiugUGeo+OUTqVowaB+2f1MtGDwk37y287PggGtuwGBsa8m+eSBBx7AZkh69tln9Yptk3nuuefK22+/bXbtdzQWDLZwmA2sYsfKfhPYN5wFA9w01atXz8aQMHEJvNUhbgRcy5g0ceJEgRuegpK7YjyRLpLgWsrrKscEyYa7lyuuuCLfrhnrBRQqLhYM6CtcDR144IHYjJgQlBhWMq6lgBIYQlwk4WS49UFdsHqJlCZMmKDd9XiDQkfjEshrceFtAzEZ4B7ImFhefvnlOni3t5zZj3RvmuPmG0G6e/ToYXbDfuNehEswN05E9+7dZfTo0WHLIxPn/Prrr9o8NB4eOAcWJyZgNers2rWr3HzzzXnuZRzzJhMLIZogz97fHve3y7iAimX80RfXjZTXMsTb1+KyTxdJqRspWjCkjjVbIgFDAJZw6j90Q1zimWP8LpgALBYRHw3vyeZduuCzWCJdCNCCIbqRpAVDkBMtGGjBEN0TU3xL0YKh+I5d3D1PJ7WkOF4LLRiK46hF32c1SWdXFA8fPjzPiSqQbcjq+Ugr3r0rvlVQ4jx1IUMJDLY9rCo3qW/fvjZf/ViEbGMFNFZZjx8/Xq88N+fgWwkMIWVV0N+ACgQckqfcs7in2G1YNWCVs2lPTQbbY/ltwALGnAPLB29SvuDt8YEDB3oPB1R8BntcTcTa41jN713hrQQGexyr393+mj6o2AUBrFZ304ABA2wb3jFzxzychYVbT6TxUoG/bf0qALh7SshqfndVuLtaHGPkJqxuD2cFA4sGrOA3SYkotl30IVxSwZgDKuCvLWc4gR0YmgQrBnMM364FC+pwjxnLEJw7f/78gIpXEHIcZVWMhYBy62Sq19+o07VYcet88sknA3v27Akpn9+OCvgctl3UqYQ3bSXgPR/1Dx06NE9fcc5JJ50Uck68PEaMGBHyG6EEBt0NXJ97vdhWIkFAxQix+RhzJOW+yebBwsSkeO7jaMcfbbj3pHLXZJot1t+0YIhudWIiLCNKugUDrKjwu4o//DukfNjmeXbw750pM2TIEH0cllcmD//mhUuwtjJl8P3XX3/pfztNnmvJ+PDDD4eUNWXwDUs4WHzBcgvvC27CvlsWv7HhEiys3HJKxAtXLGyeij0Tcq5bT7ht99+ocBXm9/6A33D8xfLvSrg2EpXnjr07XvHUj7HCuwuYmX9X3H+X46mzJJ2zcePGACww8ZwafvjGv8F4/3YtOFPJxdyz4ayMo+1HpPeHaM8vaeVowRDdO8LymYMDsz/qEdi4alZJu0VCrtf8XoRkloAdWjCUgEH+7xJpwVByxtpcKS0Y1C+7n4kWDH7SZ9slkQCCCSthR7BqpFq1ato3vuGAGAJYza8muSUzM1P77kdwZLPCxJQrjt/qR19bnSCuhLm2GjVqxH0pGzZs0JwQs6JZs2Z6ZT98HiYioa+IPYJxQEKA5Pz6ipX1uC7ElUBQZwSBRlyJeJKaKBDEGPn999+1VYeakNcBq/OrS03EyMKFC/UfglsjYLqJdZHfedEew4pS3LO4PytXrqz/cK6aFJS5c+fqe7ht27ZSpkyZaKssdLlkjn+hO5fECmjBkES4nqpLugWDa0UENIjrAmsrN7lWi0pg1BaQauJZlFtEW0xNHof8O4cDrvWkiSHj1qVECh3LBmWVu0MZNGgQNvNNarGCjuVUp04dXQ7tIhaPSbNmzZJ27dqZXfuNeDqw0nQTfu/UZLebFXb76KOPFiVQhD0WLhPxeXBtkRKuE9eLpCaMRbmj09uIeQWLOaRw46APpPgj0njF0w1YzsJSzU34d8eNo+Qe43YuAVjrqgU9snbt2txMz5ZaICHvvfeetb70HE7K7rJly/T7ECrHeyzeqeJJkSwg46mrJJxDC4boRpkWDEFO5r8v8d89JSkpN7faq4H7rlHY6/djpXxh+1wSzvdjXBiDwd87iwKDv/yFAoPPA8DmSYAESIAESCBKAhQYogSVgGIUGG4SZRkQQhKu/iAmmuROMhuBAWInhFGTJk+eLCeffLLZ1e5vINiaCVG0gUl+ty73P/q9AgOEBCQInK77NuRBDIYbO/zHXTQCAyaxIcp660EgexUbCFXmmzp37izKys+W8dYDN3VugmtJl4V7DNtweffoo4/qbLijU6vP9bYrMCgLvnxd4+kTUvARabziabpjx45aHMK5EJzOOOMMLbRgMQJTZAJY3IAFFljcYBIm81u3bq3dI7ruJJGPhRDZ2dmmaFK/ly5dKuZZhVgH0S6epCyP5IILLtCn4loh0jFFJkCBITIb9wgFhiANCgy5ixnc+yOebT8msuPpZ0k7x49xocDg711GgcFf/hQYfObP5kmABEiABEggWgIUGKIlVfhyFBjyCgzHHHOMIJ6JmZRwJ5mNwADybvyT2267TZQrNzsgmDBs06aN3UfsG8S1ceuKJDB4J9fhax6r310hxAga0QgMZhWj7cx/G8aqwptf0H5BMZwwIaxc6+lqKlasqK2+cA3KraCcdtppAgtH9BupfPnydjI4GoEB5/7222/aAg6+95X7KG1N57Usg8Wb6QMmm3EcfvuxEh6Wb61atbLt6o5E+Ig0XrhG5RpHnwWBBf+hDYs8WLthkhv98goHaBOWe0jK7aUo1z5623zAeg4T1ogpVKlSJT2BDn5uUm54BBaNSIYdmMDCBPcjrO9MWrNmjW4PAhPuxZo1a5pD9nvz5s1aDEP/cR1Y4Ys4SJgoh5UirBUjJVjY4Xr+/fdfgUUNJsYjxUNAOyiLPoFDo0aNNLNIdbv5eOYeeughm6Vcocott9yi99Ff5cpQlNtCexz7sExyE8Ye1pq4dyC2tWjRQltP4rrdFCuPaAQG3CuLFy/W90fjxo01V2+77riCIcYfqTD3cTTjj3sJbSPts88++jcPfYWwid+HopooMEQ3MhQYgpzMv+W0YIjuvsmvlB8T2fn1h8eCBPwYF/w7hmeqpD1XReaeU+CZfCTAGAw+wmfTJEACJEACJBADAcZgiM6/MmMwxHBTRSgKn+7qPxby/L3zzjv2DMRpMGXUZKfNf+yxx2w+4iy5yY0z0KFDB3vIrUsJDDZfuVuydSH+TriEGDmmH+ZcNblr83BMuUjKc6paGW3LIJ6CqQPfajIxT/mCMl577TVbR0ExnBCDyW0TdbsxfcAfSU3K2zrd/iFulUnK/U1IrBxTDjENpkyZYorpb8RmMscRB0dZC9h95KPf7777bsg54XYijRfG1NT/4osv5okthBhHSmzQVb700ku2rDnHfCMuBxJiOCHWhsk33+i3mhzXZfCBuB7mWM+ePQNKeLL7qANJiQMhzE15xAZSk/y6DD4Q/8EcAw8l8ITEh8AxjJ2yWLHnYENZ5QQQf8yca74xhrg33KTEjQCeGVPGfKO9CRMmuEXDbiOugTkH35HijLnPBpiZhPgISpwIqcPUpybQA8rFkSkaMw/EVTN1ud/IRwJfd3xMGVw7+LltI0aKOe7GcIrnPo52/NFHN4bTF198Ye9jN74cyhW1xBgM0b0jLGcMBn3rmmerqN3Hye4PYzAkm3DRqZ8xGIrOWKSqJ7RgUL/sfia6SPKTPtsmARIgARIggegJ0IIhelaFLUkLhlwLBjWZKmrSWSPFKnS4QcrKypJ77rlHVBBmna8mS0VNGuptrPTFimSTUN7su3ELRo4cKT169NDFIq2Id10keS0YTP3Dhg2T22+/Xe8adywFWTBg5XaVKlVMFYIYDYhvgFXqSGqyPySWhC2Yz0ZBFgyITYEYFeGS+g8vHWvCG4Nhv/32s+6k3POUQCEquLJ2l9S9e3f3UJ7tiRMnan/TOKAmZqVfv355yngzEA+qbt263my7H2m8XHdHtrBnw7iyUiKJdOvWzXM0uAtLAaw0h7ufSAn1wEUVLC8+/PBDOfPMM8MWhWsvWBF43Qm5hdXktq4LcTpggRFN/AdYBzz++OO6mnDuitz6sW2sa7B98cUXC+6XSAnxTnA/RkpujAOUUcGQw66sX716teaIMohJAusLJMT0gGVMpAQec+bM0fdArDwwpuPGjctTNdx+Ib+gexCulRYtWqStBiLFYCioDtO4uY8LGh93/HGua4WFY8b9GeJdIJ5FUU20YIhuZGjBEORECwa6SIruiSm+pWjBUHzHLt6eU2CIl1yCzqPAkCCQrIYESIAESIAEkkyAAkOSATvVU2DIFRgwgY/JxjFjxmhCiBNw5513RhQYUOiwww7Tk/bYRhBlBHaG6xg3AD0mkU0w5UgT1tEIDGrFt5x77rloSie4vsGkaH5BntXqebniiit0eYgmcOMDsUStzNd5iCMBn/WxpHgEBrSNtj777LOwAgPcJ0HcMYGuTznlFM0d55QrV05PAJvJz+OPP17gkmrPnj3a5ZQRSzBhC/dCmDT3TsxCPLrkkku0mx6IRCa9+uqrehLc7Hu/I42XV2CAqIJ74c033wyZUMfEL1wZQYy69NJLxcQLeP7553UsDUzwIl6FssDQTUNMgIiE2B1wC2SueciQITo/nMCAieHmzZsLrkVZEFhhBfcc7l98w32X4YTYD8oSIo/AgHrAHy6ElNWHLY+xM8GLIUiZmBnoMDjD/RSeHRNvBGP38ccfazdjytJDXxc+br31Vu0WCvckxCAktInnA4zCpU8//VROOukkewiCmXEfZDMjbLiT9iiCgOKYOMe1wMWSYQsBC0KWV2AoiAeeG8RCgYiChPIYH/A2Qg9cMiHh2hHLBPenG/cEAgPcRbl9dZ/JWO9jt3xB449+uQID9pHQPn5njKgazC1anxQYohsPCgxBThQYKDBE98QU31IUGIrv2MXd81SZSrCd8AToIik8F+aSAAmQAAmQQFEjQBdJ0bk/oIukwt+5roskNUkaULESrKsS9dIfUCujA2pC1ebB3Yub1Mpue8y4ZnnrrbdsnppsdYsHIrncicZFkuseB31T/uIDBblIUpOzti+9e/fWfVHxIWwe6jGudUI6ms9OLC6SUL9afR9SWzgXSSjguhJy3UQpwSekv7hmk9QEbcgxMEJy3eKoCVPtrsac47o3GjhwoMkO+x1pvNw6lHWKPVfFfQjpz1dffWWPue5oZsyYofNV4OKQ8ipOgC2vYm7YY7gGJO89oAQBWx4bcNUF5vgDN5N++uknm49juM9dF0nIg4skk5TgEVJeTb7rQ279SoAzxQO4TtMuvlFeiW02z3VtBLdJajLeHlNBwW093o1Ro0bZcqg3loQ2TZ9UvIuQU+HWyhzDtxIF4+KhBCNbj5rQt22gPmXJoP9UPIiAiq+hjykrJ1se7Zr7I5KLpFjvY3d8Chp/dMi9J3GuEiDtNRTlDbpIiu4dgS6SgnexedaL8j2djL7RRVIyqBbNOukiqWiOSzJ7RQsG9cvuZ6IFg3/0sSIIwcKYSIAESIAE0o8AAtcmOtGCIdFEI9dHC4ZQCwasNIZ7H6xIR1ITWVKtWjW7mher342LJBz3unCByyIEmDVWEMbFD8oiRVoRH40FwxNPPBESzFb9h4sOlhzJggGBpbH63CTXvQxWTZvV9H369LGWA6Zsft+xWDAgqPNHH30UUh1WyXtdJKFApCDP6B+sSZBgvQArCDchwLCaqNdZWKkP6wZ3JTdc8MAVj0mwMhkxYoTeffDBB+2Kf3Pc/Y40Xq4FA1bkX3bZZfY0l63yay9mFb+7WlwJDNriAcGeEVTcJKz+NwnvznB7ZBKCKcPSwXWRhHdsE1gZ267LIwTpdVf7w3WRSdjGNbjlcT+YoM7gCa4mwVoGq4Dd8uhb27ZtTREbVBsZsEg44YQTtDsm7KvJa+u2CPtYsW8sCHAvRFotD6uWLl264BSdENAaFi3RJJe3EpJCLC9gWVK/fn1bDSxoEAjbvb6CeKBsQUGeYaGA+xV/CPxuLBpMw0pgELhTi8aCoaD7GBzd/hc0/rCccRkZiy3Tt6L8HYsFg2zdJHs2rgl7OZlVaohkBwNqhy1QzDNpwRAcQFow0IKhmD/KBXafFgwFIkq/AslUL1h3wQRowVAwo2SV8K6QUk93yOod7pMH7wHeA7wHiuc9oFxjJOWfDlowRLc6kRYMhb/9vBYMqFH5MQ8JOqwmte17i9eCAeURLNb8hqnJ9JDV2evXr0cRmyKtiI/GggEr5U07CAiMlJ8Fw/Dhw215nIdV3GoyW/+ZevCNldcIphttisWCwVhNuHXHasHgBqkOV9/5559vr1MJRLopd+W3d8zcMVcCg9u1PNuRxsu1YFBufELOg7WB4YvAuSa5q8WNBYPL0pwT6RvBlV0LBtTnJuX6x7YbqQ6TjwDX3vdz3PcmeS15YHWgxLSQ+hFMOL/krqY37Yb7xnhESl7LC1gAhEuwBAFT/BkrENdKwli2uOe6x+PhgboiWTDgGKxLwl2vmxeLBUNB93Gs448+uvdkOEYoUxRTLBYMifh3srjWQQuG4N1rnrmieC8ns0+0YEgm3aJVNy0YitZ4pKI3tGBQv+x+Jlow+EffrKiCj1b4dWUiARIgARIo/gRWrVqlfW+rSaqEXwwtGBKONGKFtGDIa8EAWN6V0wag14IB+cqVjbZawLaasLOrzjt1yrtqMNKK+IIsGPC8wc++WfVtAu/mF+QZgYON3330Lb+ElfH/z96bgFtRXOvf6wwMIoIGlCF6BWRQUQkqoviJI6hx1igXrySCShyuRj9xxPGTqDFO1xgFvQh/cEAUVCImqPihRoPgEIioKIoTCBgVBxSQw/n3qm01vffZp8/u3r2qqne//TyH7t1Drar3rdPA+u2q2n///cNu8a9FGcHAIwm8JL3/LB9EHcHAo0n0otH8bf8XXnghr7zgCAZejJi1CY5gKPSMR5iwZ7wlMYKB1wngb+vrjeff199Ub2oEw/PPP++PcODRJnoxZV1WcH/00UfTM888449g4JERXL7eeJQBL0quNx5ZEfysz/N+wIAB6t/kwW+88wLBei0E7m/8jUi9cdk8eiA4IoLXK+ndu7e6xQNUap0J/lBdXa1GKwRHeXhgiPr166eLy9vzqAn+Jn2xzZsGLC8mL9zNnhVu++yzD82dO1ed1t/0D347v9BnHp3AI0309uqrr6o1MaLoETaCgddm4HeB3rhOvA4Ir9NxyCGH+L+XUUYwNNWPeaRP0O+m/OfRh0GNuC8OHDhQV9npfaQRDE63RLZyGMGQ0xcjGBr+WyRuz7PxTfm4dc3SczZ84b/rvUS6+smS1q60FYDBshMADPYMCAIGiUSUvZYhMhSAAlAgmwpIv9cBGMz1KwCGTclmXqiWp0jSWzBBqs8VJvn4PC8a+x//8R/6Fn/PU/7wwr7BLQ5g4GQoJygZAujN+2Y3MUBoDDC89dZbfvJXPxO254Vn77777rBb/GsmAINeeJeDTpw4kYYPH+7H//rrr6lNmzbqMy+0y+BFbzwdE0/LlBbAUDiN1b///W9/gXCepkhPa8TTAvE0TMFFngsBA2sQnJ6JYQQns3njf38/8MAD5K0FoD7ztEM8tVbUhDrrriEXL1TNU4jxFqwXT4fEUwOdc845fp/ifs+LVuvN+2atP30qw46ddtpJX2qw90a0qcWz9YXCRHiwP/I93jfxiRey5j7DfYc3ni6IpwjTG09hxr9TeuPFo1u1ahVZj8amSOLfpbPPPlsV740cIp4Ki7dCCJQkYGD4EsX/rbbaCoBBuVK5fwAw5LwFYABgqNzf8lzLABgq3eEi7TMxTAIxGlcAUyQ1ro30FT0EW2oqDen6o3woAAWgABTIV0D6vY4pkjBFUn6Pk/sUnC6HF3kOboXTs3j/vK/3AEPwFv+Ypx/i68EfL2npX9cHjU25E5wiiadu4fL4p9g0M17yXRfX6BRJXE9dl8amQPJGTfj3cExeoLiULTitDz9XuHnfpPbL9RL9hZfrS5kiiacZ8hLk9TzFlAcU8qad4imppk2bVs/1CE5HxFrxu4m3tEyRxIv/Bqep4bbxIuG84DVPg6U99ACQaldwiiQPMKhzwT8uuugi/xn2/Z577lHTKh1zzDH+edbMg8SRp0jiOMHfF67bn//853oPNOT5wP7y9pe//MWPyffydFTsabCOfJ6nGQrbChfy5md4Wiz+XfJGCeXFYC15OifevBENedf494b1Cy6ezWV5oEHdr/9e43P809SUUfxQcIokfoanWuJzf/rTn/zY3C95yileTP3kk0/2z/P9HixRsUtZ5Lnw3RP0grXlLahtU/7z/cG+p+vC513fMEVSaf9GwBRJuZ6sf6dd79dJ1w9TJCWtqLvlYYokd72RqhkPHcFmUQEABnvi63+wAzDY8wCRoQAUgAJJKiD9XgdgKC15kMS80Ecevr9KePF/RLO4BZN0hYCB9eDErk5O8L4wyac140Rr8L4TTjhBX8rblwIYguUUHnvfYlfJYV1osTUY6urq8taQ0OsS6Gf03vtGd16dn3jiCX0pdC8FGF577bW8+nDbvUWyVV0mTZrU4FqhNrz+hd7SAhi4vv/617+abJu3KLFqWlOAwRvRUhRKBbWaPn26Kku/x/W1UhLqvNaBvr/YnpPanEzXG/fXYvfpc431Tf283t97772h5XB5DLu8EQX6EbVneKBjFdvzM9wm3uLowc9wGcGyf/Ob39QX68vBe/QxAwnekgIMUfznuAAM5v6uTeLv66hlADBwL/fmcPHeD/yTtQ2AITuOAzBkx2vd0uy90XTLHdkDMNgzQv+DHYDBngeIDAWgABRIUgHp9zoAg7mkR9YBgzcfv5984EWRCzdeyDaYQNTfFC68r3BR3KlTpxbeoj57U8X48fhb5Xq7/PLL/fM6GaL3/A1oTpRykp2/eR7cOKGo7+M9JzbnzZuXd86bniX4iH/MCztzQlg/P2zYMP9a2IE3JY3/DGtTuAVHMPz+978vvFzvrafgP3/eeef51/nb/IXf8NaAgW/ib78H66vr7a1doL6t7hfkHYwZM8aPUehZ0PNiIyyC5TTmV3Dh7+eeey74SN63+YPfCg8mc9mj4Mbfbi82CoZHNPA3+PXGEEW3m+tQbPvss8/qR4wY4d+n72ftWEO9cV/S13gfHMHCZQSv8Ttfb7yI8m677ZZ3ne/11ljwk/X6Xi4zOGIlWOYdd9xRzzCs1G3BggVF43KZDDKCYEOXyeXfcsstDerKzwwaNCjvmbh6jB07Nu8dwYCBN25fsL18zIDSWyPEP8+e8xaEfTzCRG9x+nGp/nOMYJ/UC07r2C7vMYKhtH8jADDkerH+PXS5T0vUDYBBQlU3ywRgcNMXyVphDQbvzW5zwxoM9tT3/sGu5jTlRZ69/6DYqwgiQwEoAAWgQCIKSL/XsQZDIjaVVEjW12AoSSTcZFQBXkzYAzvE82a3b99ezY2vK8BrCHjf5icvyU01NTVq7n5eHFnPsa3vS+Pe+48o8boLvK6EblvHjh1jN+XLL79UOvGaFbzwdJcuXdQCzLELDDzIdeW1R9gH3jzgQGF15QWkuV3cPl7UuUePHsTrSsTZvvrqK+I1Rj7++GO19omXkFcLVoeVxf//eO+999TP1ltvrdbtaNeuXdgjka7x2grcZ7l/brnlluqHC/j8889p0aJFqg/37duX+P9CpjZJ/021obE4WOS5MWXyz2MNhpwe+u8Hfm9lafOmqaOjjz6avKnkyIMNiTTdxlz/iVS8wgux4QsWebbbqQAY7OpPSQCGDe+8ROvffqloS1rucwJVd9yh6LWkTw4dfglNnT6LeFEzXvjM9U06EeV6+1E/KAAFoEClKSD9XgdgMNdjABjMaY1IUAAKQAEoUL4CAAylaQjAkNMJgAGAobTfmPTeBcCQXu/i1hyAIa5yCT2XBGBIqCplFwPAULaEKAAKQAEoAAXKUACAoQzxHHsUgMExQ1AdKAAFoAAUCFUAgCFUHv8iAENOCgAGAAb/l6JCDwAYKtTYkGYBMISIY+ISAIMJlYvHkE5EFY+Ks1AACkABKCClgPR7HSMYpJxrWC4AQ0NNcAYKQAEoAAXcVQCAoTRvABhyOgEwADCU9huT3rsAGNLrXdyaAzDEVS6h5wAYEhIyRjHSiagYVcIjUAAKQAEoUIYC0u91AIYyzIn4KABDRMFwOxSAAlAAClhVAIChNPkBGHI6ATAAMJT2G5PeuwAY0utd3JoDMMRVLqHnABgSEjJGMdKJqBhVwiNQAApAAShQhgLS73UAhjLMifgoAENEwXA7FIACUAAKWFUAgKE0+QEYcjoBMAAwlPYbk967ABjS613cmgMwxFUuoecAGBISMkYx0omoGFXCI1AACkABKFCGAtLvdQCGMsyJ+CgAQ0TBcDsUgAJQAApYVQCAoTT5ARhyOgEwADCU9huT3rsAGNLrXdyaAzDEVS6h5wAYEhIyRjHSiagYVcIjUAAKQAEoUIYC0u91AIYyzIn4qMuA4dprr43YGtwOBaAAFIACaVCgU6dONHLkyFhVBWAoTTYAhpxOAAx2AcPMmTNpxIgReZ325ptvpmHDhuWdO+uss2j69Ol555YuXUqtWrXKO1fuh5dffpmGDBmiiunbty/NmDEjcpELFiygwYMHq+f69OlDTz/9dJNlrFq1imbPnk1z586lTz75hHr27EkDBgygo446inQfbbKQRm4AYGhEmAo+DcBg2VwABnsGSCei7LUMkaEAFIAC2VRA+r0OwGCuX7kMGHQSyZwaiAQFoAAUgAImFBg3bhwAg7DQAAw5gXXytr6+Xlhxt4r/y1/+QkcffbRKYMdJohdrTZxE9iOPPEInnXRSXnGnnnoqTZgwIe9chw4diJPwwe2bb76hLbbYIniq7ONnn32WBg0apMrZfffd6bXXXotc5rx586h///7quZ133pkWLVoUWsann35K++yzD/G+cDvggAMUWNlqq60KL5X8OY4vJRfeyI3V1dXEv1NZ+71qRA7jpwEYjEueHxCAIV8Pk5+kE1Em24JYUAAKQAEoQCT9XgdgMNfL0gAYrrnmGnOCIBIUgAJQAAqIKbB8+XK65557CIBBTGK/YACGnBQADHZHMBQDDNtuu636Fr/urB9++CF17dpVf/T3lQIYevXqRe+++67frsKDk08+mR544IHC0yV/BmAoWaqKuRGAwbKVAAz2DJBORNlrGSJDASgABbKpgPR7HYDBXL9KA2AoJxFlTklEggJQAApAgaYU0CPTynmv6zLG3nElnfGbE5oKmdnrAAw56wEY3AMM7MyyZcuoc+fOyqRiEIIvFAKGuro64mmT3n77bfr6669pxx13VD+tW7dW5RT7g0dFLFy4kNq1a0e77rorzZkzJ3QEw7p16xQMWLJkCXXp0oV69+5NzZs3zys6yggGrm+3bt3U8zwa4x//+IeCKXfddRdddNFFfrlBPfyTJR4AMJQoVAXdBsBg2UwABnsGSCei7LUMkaEAFIAC2VRA+r0OwGCuXwEwmNMakaAAFIACWVdAwwEABvmeAMCQ0xiAwR3AwNMJvfXWW8qYadOm0fHHH6+OL7jgArr99tspeJ0vBAEDP3fiiSf6z6sHf/rjzjvvpLPPPjtvLQN+lqdieuyxx/xbt9lmGxo1ahRdfPHF6lzhFEkMOk477TT69ttv/Wf44LLLLqMxY8YQTwvEWxTA8PDDDxP/v4a3M888k+6++251vGHDBvrZz37mx+Kpmw4++GB1LeofAAxRFUv//QAMlj0EYLBngHQiyl7LEBkKQAEokE0FpN/rAAzm+hUAgzmtEQkKQAEokHUFABjM9QAAhpzWAAzuAAZe0PiNN95QaxFceOGFxIs988YLJfMoA4YE/M1+vWnAwNf4nrCNRwPcdNNN6paNGzfS3nvvTfPnzw97hIKAgadu4/dTYxsvSj1p0iR1OQpgePLJJ4kXuubtuOOO8xeH5rULOnbs6K87MWvWLP+aujnCHwAMEcSqkFsBGCwbCcBgzwDpRJS9liEyFIACUCCbCki/1wEYzPUrAAZzWiMSFIACUCDrCpgGDBs//4g2LP1nUdlru/6Cqrfevui1SjgJwJBzUQOGSvA0Ths4qe/KIs9cl7Zt29L9999P/fr1UyMBeKqjLbfcUjVt8uTJxIl8vWnAcNhhhxEn4Hnj9RsYTPCiyOPHj6epU6fq2+mdd94hXu+ARy3o0RF8kReZ5kWZeYTC3Llz/fs1YPjiiy/UtEV65AKDgFNOOYVeeukluvXWW/37X3nlFdprr70ijWDwHy444Lqffvrp/lleAJpBQZwNgCGOaul+BoDBsn8ADPYMkE5E2WsZIkMBKAAFsqmA9HsdgMFcvwJgMKc1IkEBKAAFsq6AacDw46Lnae2c3LeOC7VvecCvqVnv/QtPV8xnAIaclQAMbgGGww8/XI1UYHc4oc9J+0MOOUSZxYCA11XQGwOGFStWUM+ePfUpmj17Nh100EHqc+EogEsuuYRuvPFGOvbYY+mJJ55Q93C8p556Sh3ztET77befDxk0YAhOY8TTKPF6CLW1teoZBhV6mqVzzz2X7rjjjrIBw4QJE2jEiBGqfP6Dp3Lic3E3AIa4yqX3OQAGy94BMNgzQDoRZa9liAwFoAAUyKYC0u91AAZzWSsz6QAAQABJREFU/QqAwZzWiAQFoAAUyLoCAAzmegAAgzmtXYz0l7/8hY4++mhybQTD1VdfTXvuuaeSjBdc/vvf/05XXHEFcWL/o48+os0228yXkwHDCy+8QEceeaR/7vvvv8+7Z+TIkXTvvfeq60cccQTxlES8MLNe64HXZzjnnHP853n0hB4loQHD9ddfT6NHj1b38ELMAwYM8O9ftGiRmtKJT+hRF1GmSPIL8g647gwp7rvvPv8014HXX+ARGXE3AIa4yqX3OQAGy94BMNgzQDoRZa9liAwFoAAUyKYC0u91AAZz/QqAwZzWiAQFoAAUyLoCAAzmegAAgzmtXYzkKmB49NFHqUWLFkoyTuw///zzavoj/rf/xIkTqWXLlr6cDBgefPBBtTgyn9RAwL/BO+BFk3ntBt54kWgGAsFRK8899xwdeOCB6jr/wSMg9IgJXd5ZZ51FY8eO9e9p7IBHUixevDjWCIbly5cr2PP666/7xXObGY60bt3aPxfnAIAhjmrpfgaAwbJ/AAz2DJBORNlrGSJDASgABbKpgPR7HYDBXL8CYDCnNSJBASgABbKuAACDuR4AwGBOaxcjuQoYeD2IgQMH0osvvkiDBg2iZ555RsnHIw14TYJCwFA4gmHNmjXUqlUrX/LTTjvNHxGgR2tst912/qgDBhBnnnmmf/8tt9xCo0aNUp81YLjuuuvoqquuUud4FARPWVRs23zzzYmnXIo6guHjjz9Wox9WrVrlF8ujGIYPH+5/LucAgKEc9dL5LACDZd8AGOwZIJ2IstcyRIYCUAAKZFMB6fc6AIO5fgXAYE5rRIICUAAKZF0BAAZzPQCAwZzWLkZyGTBceeWVNGbMmDzZOGm/2267NQAMK1eupB49evj38mLPgwcPVp/r6uqoc+fOpBP3l156Kd1www0UXBSaF2yePn26/zyPZuCpmXjTgCG4BsO+++6rpmWqrq5W98yfP58WLlyojrt27arWf4gCGDZu3KhACo+k4K1bt27E3vBoi6Q2AIaklExPOQAMlr0CYLBngHQiyl7LEBkKQAEokE0FpN/rAAzm+hUAgzmtEQkKQAEokHUFABjM9QAABnNauxjJZcDA6yTwaIPgtnbtWvWxcAQDr4nAazDMnDlTXee1Gngh5/bt26vphbidenvvvfeoe/fuaqql4OgAXveAF3fmxZ55Gia9acDAizpvu+22+rRao2Ho0KHEi07zmhG8GDVvDz30EPH/UaIABoYTffr08ctmwLDTTjv5n/XBhRdemDeVkz5fyh6AoRSVKuseAAbLfgIw2DNAOhFlr2WIDAWgABTIpgLS73UABnP9CoDBnNaIBAWgABTIugIADOZ6AACDOa1djOQyYFixYgV16tTJl42T/zwV0rp16xqMYGDAwOsq7LLLLv79xQ54oWie6oi3DRs2UP/+/Sm43kGxZzRg4Gvjx49XUzQVu4/P8egKLq+mpiYSYLjnnnuI33tNbZMnT6ZTTjmlqduKXgdgKCpLRZ8EYLBsLwCDPQOkE1H2WobIUAAKQIFsKiD9XgdgMNevABjMaY1IUAAKQIGsKwDAYK4HADCY09rFSK4AhmnTptGvfvUrJdExxxxDjz/+uDreYYcd6IMPPlDHl19+Of3+979vABi+++474nUPeFuyZAmdfPLJxFMWFW4TJkxosG7CV199pUYi6JEP/AyPUuA1GfToiX79+ilYoMvjup5//vn++g36PK/zcNtttxHDDt5effVVtaYCHzN4WLBgAR8W3YJrRBS94aeTvJg1j5qIswEwxFEt3c8AMFj2LwnAsH7eE7Ru/oyiLWl1xHlU02XT0KeiNyV0cujwS2jq9Fk0ZcoUGjJkSEKlyhUjnYiSqzlKhgJQAApAgWIKSL/XARiKqS5zDoBBRleUCgWgABSAAg0VAGBoqInUGQAGKWXTUa4rgCFptXhNBp666Ouvv6ZevXoRg4ra2tpGwyxfvpzefPNN4vUTePqkqqqqRu/lCzz64f3331dAg6dh4umM2rRpE/qM7YsADLYdMB8fgMG85nkRARjy5DD6QToRZbQxCAYFoAAUgAIk/V4HYDDXyQAYzGmNSFAACkCBrCsAwGCuBwAwmNPaxUiVChhc1Np2nQAYbDtgPj4Ag3nN8yICMOTJYfSDdCLKaGMQDApAASgABQAYKqgPADBUkJloChSAAlDAcQUAGMwZBMBgTmsXIwEwuOiKTJ0AGGR0dblUAAbL7gAw2DMAgMGe9ogMBaAAFJBQQPq9jhEMEq4VLxOAobguOAsFoAAUgALJKwDAkLymjZUIwNCYMtk4D8CQDZ+5lQAM2fFatxSAQSthaQ/AYEl4L6xEIuq1114j/gcqNncV4MWPwjZeyOmxxx4LuwXXLCpw2GGH0ZgxY0qqwT333EP8g81NBfbYYw8aN25copWTeK8HKwjAEFRD9hiAQVZflA4FoAAUgAKbFABg2KSF9BEAg7TCbpcPwOC2P0nWDoAhSTXTURYAg2WfABjsGSCRiGLAsOeee9prFCKHKsAJzVIAwxVXXBFaDi7aU2D06NGRAAOAnz2vmoo8cuRIAIamRMrwdQCGDJuPpkMBKAAFDCsAwGBOcAAGc1q7GAmAwUVXZOoEwCCjq8ulAjBYdgeAwZ4BkoBB4pu59pSqjMgMfqIABk5kH3fccZXR+Apoxd/+9jdi8BMHMHAim3+wuaGAHukFwOCGH67WAoDBVWdQLygABaBA5SkAwGDOUwAGc1q7GAmAwUVXZOoEwCCjq8ulAjBYdgeAwZ4B0oChqW/K22t59iLrkSVRAUOpU/FkT1HzLeapq8oBDElPxWNegcqJyNNW8X/kARgqx1OJlgAwSKiKMqEAFIACUKCYAgAMxVSROQfAIKNrWkoFYEiLU+XXE4ChfA3TVgIAg2XHABjsGQDAYE9705EBGEwrnnw8AIbkNbVVIgCDLeXTFReAIV1+obZQAApAgTQrAMBgzj0ABnNauxgJgMFFV2TqBMAgo6vLpQIwWHYHgMGeAQAM9rQ3HRmAwbTiyccDYEheU1slAjDYUj5dcQEY0uUXagsFoAAUSLMCAAzm3ANgMKe1i5EAGFx0RaZOAAwyurpcKgCDZXcAGOwZAMBgT3vTkQEYTCuefDwAhuQ1tVUiAIMt5dMVF4AhXX6htlAACkCBNCsAwGDOPQAGc1q7GAmAwUVXZOoEwCCjq8ulAjBYdgeAwZ4BAAz2tDcdGYDBtOLJxwNgSF5TWyUCMNhSPl1xARjS5RdqCwWgABRIswIADObcA2Awp7WLkQAYXHRFpk4ADDK6ulwqAINldwAY7BkAwGBPe9ORARhMK558PACG5DW1VSIAgy3l0xUXgCFdfqG2UAAKQIE0K2AaMKRZq3LrDsBQroLpfh6AId3+Rak9AEMUtSrjXgAGyz4CMNgzAIDBnvamIwMwmFY8+XgADMlraqtEAAZbyqcrLgBDuvxCbaEAFIACaVYAgMGcewAM5rR2MRIAg4uuyNQJgEFGV5dLBWCw7A4Agz0DABjsaW86MgCDacWTjwfAkLymtkoEYLClfLriAjCkyy/UFgpAASiQZgUAGMy5B8BgTmsXIwEwuOiKTJ0AGGR0dblUAAbL7gAw2DMAgMGe9qYjAzCYVjz5eAAMyWtqq0QABlvKpysuAEO6/EJtoQAUgAJpVgCAwZx7AAzmtHYxEgCDi67I1AmAQUZXl0sFYLDsDgCDPQMAGOxpbzoyAINpxZOPB8CQvKa2SgRgsKV8uuICMKTLL9QWCkABKJBmBQAYzLkHwGBOaxcjATC46IpMnQAYZHR1uVQABsvuADDYMwCAwZ72piMDMJhWPPl4AAzJa2qrRAAGW8qnKy4AQ7r8Qm2hABSAAmlWAIDBnHsADOa0djESAIOLrsjUCYBBRleXSwVgsOwOAIM9AwAY7GlvOjIAg2nFk48HwJC8prZKBGCwpXy64gIwpMsv1BYKQAEokGYFABjMuQfAYE5rFyMBMLjoikydABhkdHW5VAAGy+4AMNgzAIDBnvamIwMwmFY8+XgADMlraqtEAAZbyqcrLgBDuvxCbaEAFIACaVYAgMGcewAM5rR2MRIAg4uuyNQJgEFGV5dLBWCw7E4SgKFu2WKqW/ZO0ZY067k3VW3Zoei1pE8OHX4JTZ0+i6ZMmUJDhgxJuvjEywNgSFxSZwsEYHDWmpIrBsBQslTO3wjA4LxFTlQQgMEJG1AJKAAFoEAmFABgMGczAIM5rV2MBMDgoisydQJgkNHV5VIBGCy7kwRgsNwEPzwAA1GURLYvHA7EFYjiS5xEtngDEIDi+CKZyIYl8RWQ9EUCHAdb+p//+Z/08MMP00MT/0AnHXdo8BKOE1YAgCFhQVEcFIACUAAKNKoAAEOj0iR+AYAhcUlTVSAAQ6rsKquyAAxlyZfKhwEYLNsGwGDPAIlEVJREtr2WZy9yFF/iJLKzp6j5FsfxRTKRbV6Byoko6YvEez2oPABDUA3ZYwAGWX1ROhSAAlAACmxSAIBhkxbSRwAM0gq7XT4Ag9v+JFk7AIYk1UxHWQAMln0CYLBngEQiKkoi217Lsxc5ii9xEtnZU9R8i+P4IpnINq9A5USU9EXivR5UHoAhqIbsMQCDrL4oHQpAASgABTYpAMCwSQvpIwAGaYXdLh+AwW1/kqwdAEOSaqajLAAGyz4BMNgzQCIRFSWRba/l2YscxZc4iezsKWq+xXF8kUxkm1egciJK+iLxXg8qD8AQVEP2GIBBVl+UDgWgABSAApsUAGDYpIX0EQCDtMJulw/A4LY/SdYOgCFJNdNRFgCDZZ8AGOwZIJGIipLIttfy7EWO4kucRHb2FDXf4ji+SCayzStQORElfZF4rweVB2AIqiF7DMAgqy9KhwJQAApAgU0KADBs0kL6CIBBWmG3ywdgcNufJGsHwJCkmukoC4DBsk8ADPYMkEhERUlk22t59iJH8SVOIjt7ippvcRxfJBPZ5hWonIiSvki814PKAzAE1ZA9BmCQ1RelQwEoAAWgwCYFABg2aSF9BMAgrbDb5QMwuO1PkrUDYEhSzXSUBcBg2ScABnsGSCSioiSy7bU8e5Gj+BInkZ09Rc23OI4vkols8wpUTkRJXyTe60HlARiCasgeAzDI6ovSoQAUgAJQYJMCAAybtJA+AmCQVtjt8gEY3PYnydoBMCSpZjrKAmCw7BMAgz0DJBJRURLZ9lqevchRfImTyM6eouZbHMcXyUS2eQUqJ6KkLxLv9aDyAAxBNWSPARhk9UXpUAAKQAEosEkBAIZNWkgfATBIK+x2+QAMbvuTZO0AGJJUMx1lATBY9gmAwZ4BEomoKIlsey3PXuQovsRJZGdPUfMtjuOLZCLbvAKVE1HSF4n3elB5AIagGrLHAAyy+qJ0KAAFoAAU2KQAAMMmLaSPABikFXa7fAAGt/1JsnYADEmqmY6yABgs+wTAYM8AiURUlES2vZZnL3IUX+IksrOnqPkWx/FFMpFtXoHKiSjpi8R7Pag8AENQDdljAAZZfVE6FIACUAAKbFIAgGGTFtJHAAzSCrtdPgCD2/4kWTsAhiTVTEdZAAyWfQJgsGeARCIqSiLbXsuzFzmKL3ES2dlT1HyL4/gimcg2r0DlRJT0ReK9HlQegCGohuwxAIOsvmGlX3vttbR8+fKwW3DNogKdO3emq6++2mIN7IX++uuv6eKLL7ZXAURuUoFx48Y1eY+LNwAwmHMFgMGc1i5GAmBw0RWZOgEwyOjqcqkADJbdAWCwZ4BEIipKIttey7MXOYovcRLZ2VPUfIvj+CKZyDavQOVElPRF4r0eVB6AIaiG7DEAg6y+YaUzYLjmmmvCbsE1iwqwN1kGDFtuuaVF9RE6TIG2bdvS6tWrw25x9pppwLDx849ow9J/FtWjtusvqHrr7Yteq4STAAyV4GL8NgAwxNcubU8CMKTNsfLrC8BQvoZllQDAUJZ8ZT0skYiKksguq/J4OJICUXyJk8iOVBncHEuBOL5IJrJjNQIPKQUkfZF4rwdtA2AIqiF7DMAgq29Y6RowcCK7U6dOYbfimkEFPvvsMwV+ABi2JE5k33TTTQbVR6imFOAEPQDDb4n/jTP2jivpjN+cECrZj4uep7VzJhW9p+UBv6Zmvfcveq0STgIwVIKL8dsAwBBfu7Q9CcCQNsfKry8AQ/kallUCAENZ8pX1sEQiKkoiu6zK4+FICkTxJU4iO1JlcHMsBeL4IpnIjtUIPKQUkPRF4r0etA2AIaiG7DEAg6y+YaUHAUNWvykfpo+ta/CFiKdI4hEMaU5k2+o/knErwRfTIxgAGJbQdr8YSVt22lOya6JsBxUAYHDQFKEqATAICetwsQAMls0BYLBngEQiKkoi217Lsxc5ii9xEtnZU9R8i+P4IpnINq9A5USU9EXivR5UHoAhqIbsMQCDrL5hpSORHaaOvWvwBYDBXu8LjwzAkNNHQwqMYAjvLxjBEK5PpV8FYKh0hze1D4BhkxZZOQJgsOw0AIM9AyQSUVES2fZanr3IUXyJk8jOnqLmWxzHF8lEtnkFKieipC8S7/Wg8gAMQTVkjwEYZPUNKx2J7DB17F2DLwAM9npfeGQAhpw+AAzh/URfBWDQSmRzD8CQHd8BGLLjtW4pAINWwtIegMGS8F5YiURUlES2vZZnL3IUX+IksrOnqPkWx/FFMpFtXoHKiSjpi8R7Pag8AENQDdljAAZZfcNKRyI7TB171+ALAIO93hceGYAhpw8AQ3g/0VcBGLQS2dwDMGTHdwCG7HitWwrAoJWwtAdgsCS8F1YiERUlkW2v5dmLHMWXOIns7ClqvsVxfJFMZJtXoHIiSvoi8V4PKg/AEFRD9hiAQVbfsNKRyA5Tx941+ALAYK/3hUcGYMjpA8AQ3k/0VQAGrUQ29wAM2fEdgCE7XuuWAjBoJSztARgsCe+FlUhERUlk22t59iJH8SVOIjt7ippvcRxfJBPZ5hWonIiSvki814PKAzAE1ZA9BmCQ1TesdCSyw9Sxdw2+ADDY633hkQEYcvoAMIT3E30VgEErkc09AEN2fAdgyI7XuqUADFoJS3sABkvCe2ElElFREtn2Wp69yFF8iZPIzp6i5lscxxfJRLZ5BSonoqQvEu/1oPIADEE1ZI8BGGT1DSsdiewwdexdgy8ADPZ6X3hkAIacPgAM4f1EXwVg0Epkcw/AkB3fARiy47VuKQCDVsLSPgnAsH7eE7Ru/oyiLWh1xHlU06VP0WtJnxw6/BKaOn0WTZkyhYYMGZJ08YmXJ5GIipLITrxBKLBRBaL4EieR3WhgXEhMgTi+SCayE2tYBguS9EXivR60CIAhqIbsMQCDrL5hpSORHaaOvWvwBYDBXu8LjwzAkNMHgCG8n+irAAxaiWzuARiy4zsAQ3a81i0FYNBKWNoDMFgS3gsrkYiKksi21/LsRY7iS5xEdvYUNd/iOL5IJrLNK1A5ESV9kXivB5UHYAiqIXsMwCCrb1jpSGSHqWPvGnwBYLDX+8IjAzDk9AFgCO8n+ioAg1Yim3tJwDBt2rRsiupoq88991xavnw5LVu2jDp37mykltXV1VRfX69+jAREkDwFABjy5DD/AYDBvOY6okQiKkoiW9cDe3kFovgSJ5Et3wJEiOOLZCIbjsRXQNIXifd6sKUADEE1ZI8BGGT1DSsdiewwdexdgy8ADPZ6X3hkAIacPgAM4f1EXwVg0Epkcy8JGLKpqPutBmBw36OkagjAkJSSMcsBYIgpXAKPSSSioiSyE2gCiihRgSi+xElkl1gN3FaGAnF8kUxkl9GUzD8q6YvEez1oGABDUA3ZYwAGWX3DSkciO0wde9fgCwCDvd4XHhmAIacPAEN4P9FXARi0EtncSwCGE044IZtipqTVf/rTnzCCISVelVtNAIZyFSzzeQCGMgUs43GJRFSURHYZVcejERWI4kucRHbE6uD2GArE8UUykR2jCXjkJwUkfZF4rweNA2AIqiF7DMAgq29Y6Uhkh6lj7xp8AWCw1/vCIwMw5PQBYAjvJ/oqAINWIpt7CcCQTSXR6mIKYIqkYqqYOwfAYE7ropEAGIrKYuSkRCIqSiLbSCMRRCkQxZc4iWzILK9AHF8kE9nyLa7cCJK+SLzXg04AMATVkD0GYJDVN6x0JLLD1LF3Db4AMNjrfeGRARhy+gAwhPcTfRWAQSuRzT0AQzZ9N9VqAAZTShePA8BQXBdjZwEYjEndIJBEIipKIjtYoZkzZ9KIESOCp+jmm2+mYcOG5Z0766yzaPr06Xnnli5dSq1atco7V+6Hl19+mYYMGaKK6du3L82YMSNykQsWLKDBgwer5/r06UNPP/10yWWsWbOGzjzzTHX/YYcdRv/1X/9V8rPFboziS5xEto4JH3NKrFq1imbPnk1z586lTz75hHr27EkDBgygo446iqqqqrRckfZxfImbyIaPOWtWrlxJEyZMoMWLFxMnD3r06EEHH3yw/3sdycDAzXF9CRTR6KHEez0YDIAhqIbsMQCDrL5hpSORHaaOvWvwBYDBXu8LjwzAkNMHgCG8n+irAAxaiWzuARiy6bupVgMwmFK6eBwAhuK6GDsLwGBM6gaBJBJRURLZwQo98sgjdNJJJwVP0amnnqqSe8GTHTp0IE7eBrdvvvmGtthii+Cpso+fffZZGjRokCpn9913J25X1G3evHnUv39/9djOO+9MixYtKrmIsWPHEsMU3i688EIFW0p+uMiNUXyJk8jWIeEj0aeffkr77LOP2mtd9P6AAw5QgGyrrbbSp0rex/ElbiIbPpKCQwwIv/322wYeMShi0FlbW9vgWikn4vpSStkS7/VgXACGoBqyxwAMsvqGlY5Edpg69q7BFwAGe70vPDIAQ04fAIbwfqKvAjBoJbK5B2DIpu+mWg3AYErp4nEAGIrrYuwsAIMxqRsEkkhERUlkBytULKG57bbbqm9/6/s+/PBD6tq1q/7o7ysFMHzwwQcKnjzwwAN05513+u1LO2DImo+9evWid9991/ev8ODkk08m9jjqZhswZMlHhgoMBRkWNbaNGTOGRo8e3djl0PMADKHy4OJPCgAw2OsKSGTb0z4sMnwBYAjrHzavATDk1AdgKK0XAjCUplOl3gXAUKnOutEuAAa7PgAw2NWfABjsGeA6YGBlli1bRp07d1YiFYMQfKEQMNTV1RFPm/T222+raU123HFH4p/WrVurcor9waMiFi5cSO3ataNdd92V5syZEzqCYd26dSqJvGTJEurSpQv17t2bmjdvnld01BEMjU2dk3bAwKJkxUfud926dVP9gEfV/OMf/1BQ7K677qKLLrrI7x9BPfyTTRzYBgxcvWC9K/n38a9//Sv98pe/VI7wCKbHHnuM2rdvT+eddx6NHz9ene/Xrx/x73icDYAhjmrZewaAwZ7nSGTb0z4sMnwBYAjrHzavATDk1AdgKK0XAjCUplOl3gXAUKnOutEuAAbLPtRjs6rAR6+Pq1/41Bn1X77/aH3d1/+M9fPDM9fWr75+j6I/6xf8n1hlxqnLSccfWu915/opU6ZY1bTU4OvXr1f1bdasWamPNHnfq6++qsrcY489mrw3eMPUqVPVc6yf981h/3jatGn+beeff746H7zO93uAwb/Hm4Yo73m+rn+8UQH1Gzdu9O/lA+8/BPXHHXecfw/fu80229TfdNNN/jkvwZj3DNfVSx7713X5l112Wb0HN/x7X3nlFf8ernNTW7EyuWwPMDT1aJPXo/jifTNb1dv7dnaT5RbekHUf+Xdf9wdvDQ1fHg/m5fUZbwou/1qpB3F8GTdunKrPyJEjSw2j7su6j8Hf/9tvv93X7oUXXvD99UZ0+OejHsT1pZQ4Eu/1YFxvbRqlwUMT/2Ds79Y4fx9XwjNHHr6/0tpbAyhogRPH/E7hdx335UrcrrnmGtU+3mNzRwH4Ul+/evVq1Tfbtm3rjjGoSUX4ksR7XZcx9o4rm/w3wtqX/6fo/935//R8rRL+Hm+sDR/N+4PKf3y1fD5+ezKoAP+7jv8N5U25msHWo8nSCnhfWlX9SzoOyi+uAEYweG83mxtGMNhT39URDDy/+RtvvKGmJwl+e58XSuZRBmeffTbxN8L1pkcw8DW+J2zjb5F7yUN1iwcbaO+996b58+eHPULBNRj0N48be4AXpZ40aZK6HHUEw9q1a4nrxNtVV11Ft9xyizoOaqBOxPgjytRVcb4pr6sU/FZ7Fn188skniRdI5s0DV/5iwN5fP9SxY0d//ZBZs2b517R2Te3j+KL7q/cfPvISgU2F8K9n3UcvgUP8w1unTp2oRYsW6vjGG28kDySq40suuYT4c5wtri+lxJJ4rwfjYg2GoBqyxxjBIKtvWOn4pnyYOvauwReMYLDX+8IjYwRDTp8oIxjqPlxA696YVVTYFn0PpZou4f+nK/pgSk5iBENKjBKqJkYwCAmLYpUCGMFgtyMAMNjVH1MkWdRfIhEVJZEdbHphQtP7Zhbdf//9pKch0f9w52cmT55MnMjXmwYMhx12GHHiljeeL/7mm28mXkyXpzTxvpGtb6d33nmHeJ58nvbk+OOP98/zItO8KDPXZe7cuf55DRi++OILNd2NXvSVE8innHIKvfTSS3Trrbf693sjF2ivvfZS06fEXeT58ssvpxtuuEGVmWbAkHUfdafgPnj66afrjwqe/fznP/c/l3JgEzBk2ccNGzaQNxpFTYn24osvKqt4+it+R/A6DXE2AIY4qmXvGQAGe54jkW1P+7DI8AWAIax/2Lym/5/C/17SX1CwWZ84sTUc4C+k8BdT4my6DG8EA53xmxPiFJGJZwAYMmFzo40EYGhUGlxIQAEAhgRELKeI4gMbcNaUApgiyZTSDeNITKURZSqeYI2CU7LwcEFvhIIa2uX9btd7Cf16nlKGj/nHAwT+MX/mKZK8RXXzzs2ePdsvnqdF4mmP9PPeN4/VtWOOOcY/d/jhh/v383Q23sgG/5qeIik4/Q2Xx/fpLTjN0rnnnqtOR50iSZfFe55uSdc3rVMkwceco/fdd5/vJXt66qmnBq0u+djWFElZ99FbbyXPP/aQ3w/eehQle1d4I6ZIijcdYmNTDVTqeUyRVPibY+4zpuIxp3WUSPAFUyRF6S8m762Eqav09EblTH2nyyhliqRK/bu7lHZhiiSTv53uxcIUSe55Ukk1whRJdt0ku+ERHYDBXh9wGTBoUMHJPG/B5XqdXOXE/g8//JCX8GPA4E1Nk3fu+++/zxP2jDPO8K8fccQR6lpwLQdenyG4eaMk/Ps1YPC+Qe6f4/USDj30UP+H52PXQMAbdaGKAmA4qj7LPq5Zs6Z+xIgRfr/g/sF96csvvwx2tZKP9e9AlLUx4iayC4Ffln1kkMiQpWfPnnle7rvvvnmQsWQjvRvj+lJKDIn3ejAu1mAwB0cAGII9z+wxEtlm9S41GnwBYCi1r5i+D4AhpzgAQ2n/RgBgMP0b6lY8AAa3/Ki02gAw2HUUgMGu/vUADPYMkEhE6URkOYs8czIv+K3h66+/XiXyOUHrzf9d761VkJfoY8AwduxY/5wGAkFlgyMi9ILLGgjw/rnnngvenjdiQpfHi/YGn2nsmBORvAEwZNdH/nY795tgH+G+y6Nx4m42AUPWfx+1Z4Ug8/XXX9eXIu0BGEr7D3gp3wKs5HsAGCL9WiV6MxLZicqZWGHwBYAhsc6UcEEADDlBARhK+/cNAEPCv4ApKw6AIWWGpay6AAx2DcMaDF4GzOaGRZ7tqe/yGgzeX7w0cOBA4jnPBw0aRM8884wSyhtpoOayb9mypS8cr8Hwwgsv0JFHHumf8749Tq1atfI/n3baaeRNVaM+ewCDuPzttttOzYXPJ++++241z7p+gBdYHjVqlProJYqJ15a47rrr1OLLfNIbBUHeVDfqeuEfm2++OXlTLmENhp90zpqPH3/8sVo7ZNWqVX7X4L43fPhw/3OcA5trMGTt95G1Xrx4sbLJGzGi1mzRnnXo0MFfrJvXdjnxxBP1pZL3WIOhZKkyfSPWYLBnP+b6t6d9WGT4gjUYwvqHzWtYgyGnPtZgKK0XYg2G0nSq1LuwBkOlOutGu7AGg2Uf7PINRMcIBnt9wOURDKzKFVdckfcNcO9VUT9v3ryiIxjee++9vHu9xZ59Yb1FWvPWYLj00kvVNZ7iiMvkH15DIbgdcMAB/jU9giG4BgNPj1JXV+c/wvX63//9X/Wj13/ACIajlD5Z8pH7xEEHHeT3nW7dutUvWrTI7yflHNgcwcD1zpKP559/vu8hj47S2wcffOCf5/eGt8C7vhRpjxEMpX3Dr5JHJ5TSNoxgiPRrlejN+KZ8onImVhh8wQiGxDpTwgVhBENOUIxgKO3fNxjBkPAvYMqKwwiGlBmWsupiBINdwzCCwcuS2NwwgsGe+q6PYPCmIyEebRDcvOmR1MfCEQzemghqBMPMmTPVdW+tBrrxxhupffv2dO+99xJ/U0BvHoyg7t2708SJE/O+Ve4tzkz77bcfPfXUU+qavl+PYPCmvSFvrQV9moYNG0ZDhw4lb9Fpuvrqq8mb/kZde+ihh8ibDgcjGH4awZAlHxcuXEh9+vTx+4gHGGinnXbyP+sDb+FuOvDAA/XHkva2RzBkycfHHnuMjj/+eOULv1v+/Oc/E49MuvXWW8mDCv55fifw9agbRjBEVSyb92MEgz3f8U15e9qHRYYvGMEQ1j9sXsMIhpz6GMFQWi/ECIbSdKrUuzCCoVKddaNdGMFg1wcABrv6EwCDPQNcBwwrVqygTp06+QJx8p+nQvLmg6digMH7pjjtsssu/v3FDrxvYaupjviaN7KB+vfvT9486sVu9c9pwMAnxo8fr6Zo8i8WHOy2226qvJqaGgCGnwBDlnzUieOCbtHgo7eIOJ1yyikNzoedsA0YsuSj901E8kYp0VtvvdWoJfwu8BbxbvR62AXdT7xv+pE3miHs1sjXJN7rwUowPH344YfpoYl/oJOOOzR4CccJKwDAkLCgEYpDIjuCWAZvhS8ADAa7W6RQAAw5uQAYSus2AAyl6VSpdwEwVKqzbrQLgMGyD3YHUCB6ElMkbfj02fr1bz1c9GfDyr/XlzIVQRL3nHR8bsodnkonDZtLUyQ9+uij/tQjxxxzjC8fTzHjvSLUz+WXX67OFy7y/N133/n381RJ/fr185/Rz/J+woQJ/n364Msvv6z31lPIu98bpVDv/cXvn+PyghvXle8Jls3H3joP9bzgtN7mz5/v3+OBB326pH1wOpqLL764pGfCboqy+HacqXh07Kz76CWcfc8L+0fw84MPPqglK3kfx5e4U/Fk3Uc2hd8NwWnUtH/eiIV6b5RSyb4VuzGuL8XKKjwn8V4PxhgyZIjq4x5gMPZ3axJ/P6exDEyRFOx5Zo8xFY9ZvUuNBl8wRVKpfcX0fZgiKac4pkjCFEmmf/fSGA9TJKXRtfTUGVMk2fUKIxi8rInNLYkRDDbrH4w9dPglNHX6LPIAA3lJmOAlJ48lvunKiyHvueeetMcee5CX1LbW7pUrV6qpi/gbRb169aIddtiBamtrG63P8uXL6c0336SuXbuq6ZO8F3Oj9/IFHv3w/vvv05IlS9Q0TDwNTps2bUKfsXkxii9xvikv1Tb4uEnZOL5IflN+U82aPkqzj7xgPI+O+vzzz9V7hKe9at68edONDrlD0heJ93qwKRjBEFRD9hgjGGT1DSsd35QPU8feNfiCEQz2el94ZIxgyOmDEQzh/URfxQgGrUQ29xjBkE3fTbUaIxhMKV08DgBDcV2MnQVgMCZ1g0ASiagoiewGFcIJMQWi+BInkS1WcRTsKxDHF8lEtl8xHERWQNIXifd6sIEADEE1ZI8BGGT1DSsdiewwdexdgy8ADPZ6X3hkAIacPgAM4f1EXwVg0Epkcw/AkE3fTbUagMGU0sXjADAU18XYWQAGY1I3CCSRiIqSyG5QIZwQUyCKL3ES2WIVR8G+AnF8kUxk+xXDQWQFJH2ReK8HGwjAEFRD9hiAQVbfsNKRyA5Tx941+ALAYK/3hUcGYMjpA8AQ3k/0VQAGrUQ29wAM2fTdVKsBGEwpXTwOAENxXYydBWAwJnWDQBKJqCiJ7AYVwgkxBaL4EieRLVZxFOwrEMcXyUS2XzEcRFZA0heJ93qwgQAMQTVkjwEYZPUNKx2J7DB17F2DLwAM9npfeGQAhpw+AAzh/URfBWDQSmRzD8CQTd9NtRqAwZTSxeMAMBTXxdhZAAZjUjcIJJGIipLIblAhnBBTIIovcRLZYhVHwb4CcXyRTGT7FcNBZAUkfZF4rwcbCMAQVEP2GIBBVt+w0pHIDlPH3jX4AsBgr/eFRwZgyOkDwBDeT/RVAAatRDb3AAzZ9N1UqwEYTCldPA4AQ3FdjJ0FYDAmdYNAEomoKInsBhXCCTEFovgSJ5EtVnEU7CsQxxfJRLZfMRxEVkDSF4n3erCBAAxBNWSPARhk9Q0rHYnsMHXsXYMvAAz2el94ZACGnD4ADOH9RF8FYNBKZHMvARh+/vOfZ1PMlLR6/vz51LlzZyO1BWAwInOjQQAYGpXGzAUABjM6F4sikYiKksguVieck1Egii9xEtkytUapQQXi+CKZyA7WDcfRFJD0ReK9HmwdAENQDdljAAZZfcNKRyI7TB171+ALAIO93hceGYAhpw8AQ3g/0VcBGLQS2dxLAYbly5dnU9AUtHrZsmUADCnwKYkqAjAkoWIZZQAwlCFemY9KJKKiJLLLrD4ej6BAFF/iJLIjVAW3xlQgji+SieyYzcBjngKSvki814OmATAE1ZA9BmCQ1TesdCSyw9Sxdw2+ADDY633hkQEYcvoAMIT3E30VgEErkc29JGDgRDY2dxTo168fMfgBYHDHE+maADBIK9xE+QAMTQgkeFkiERUlkS3YNBRdoEAUX+IksgvC4aOAAnF8kUxkCzQxM0VK+iLxXg8aA8AQVEP2GIBBVt+w0pHIDlPH3jX4AsBgr/eFRwZgyOkDwBDeT/RVAAatRDb30oDB1FQ82XQvWqt56ioAhmiapf1uAAbLDgIw2DNAIhEVJZFtr+XZixzFlziJ7Owpar7FcXyRTGSbV6ByIkr6IvFeDyoPwBBUQ/YYgEFW37DSkcgOU8feNfgCwGCv94VHBmDI6QPAEN5P9FUABq1ENvcADNnxHYAhO17rlgIwaCUs7QEYLAnvhZVIREVJZNtrefYiR/ElTiI7e4qab3EcXyQT2eYVqJyIkr5IvNeDygMwBNWQPQZgkNU3rHQkssPUsXcNvgAw2Ot94ZEBGHL6ADCE9xN9FYBBK5HNPQBDdnwHYMiO17qlAAxaCUt7AAZLwnthJRJRURLZ9lqevchRfImTyM6eouZbHMcXyUS2eQUqJ6KkLxLv9aDyAAxBNWSPARhk9Q0rHYnsMHXsXYMvAAz2el94ZACGnD5RAEPdhwto3Ruzigrbou+hVNOlT9FrlXASgKESXIzfBgCG+Nql7UkAhrQ5Vn59ARjK17CsEgAYypKvrIclElFREtllVR4PR1Igii9xEtmRKoObYykQxxfJRHasRuAhpYCkLxLv9aBtAAxBNWSPARhk9Q0rHYnsMHXsXYMvAAz2el94ZACGnD5RAMOPi56ntXMmFRW25QG/pma99y96rRJOAjBUgovx2wDAEF+7tD0JwJA2x8qvLwBD+RqWVQIAQ1nylfWwRCIqSiK7rMrj4UgKRPElTiI7UmVwcywF4vgimciO1Qg8pBSQ9EXivR60DYAhqIbsMQCDrL5hpZeTyJ45cyaNGDEir/ibb76Zhg0blnfurLPOounTp+edW7p0KbVq1SrvXLkfXn75ZRoyZIgqpm/fvjRjxozIRS5YsIAGDx6snuvTpw89/fTTTZaxatUqmj17Ns2dO5c++eQT6tmzJw0YMICOOuooqqqqavL5YjeU40ux8tJ4Lm4iG/2yodsTJ06kN998U13YaqutaPTo0Q1vKvFMXF9KLN7IbRoOjBs3jkaOHBkrpi5j7B1X0hm/OSG0DACGJbTdL0bSlp32DNUJFytPAQCGyvO0sRYBMDSmTOWeB2Cw7C0Agz0DJBJRURLZ9lqevchRfImTyM6eouZbHMcXyUS2eQUqJ6KkLxLv9aDyAAxBNWSPARhk9Q0rvZxE9iOPPEInnXRSXvGnnnoqTZgwIe9chw4diJPwwe2bb76hLbbYIniq7ONnn32WBg0apMrZfffdif89EHWbN28e9e/fXz22884706JFi0KL+PTTT2mfffYh3hduBxxwgAIrnNCNupXjS9RYrt4fN5GNfpnvKIOFXXfd1T+5zTbb0MqVK/3PUQ/i+hI1juT9Gg4AMEiqnCsbIxjkNXY5AgCDy+4kWzcAhmT1TENpAAyWXQJgsGeARCIqSiLbXsuzFzmKL3ES2dlT1HyL4/gimcg2r0DlRJT0ReK9HlQegCGohuwxAIOsvmGll5PILpbI3XbbbdW3+HXMDz/8kLp27ao/+vtKAQy9evWid999129X4cHJJ59MDzzwQOHpJj+X40uThafkhriJbPTLTQbX19fT/vvvTy+++KJ/EoCBCIDB7w7iBwAM4hI7HQCAwWl7Eq0cAEOicqaiMAAGyzYBMNgzQCIRFSWRba/l2YscxZc4iezsKWq+xXF8kUxkm1egciJK+iLxXg8qD8AQVEP2GIBBVt+w0stJZBdL5HKsZcuWUefOnVXYxu4pBAx1dXXE0ya9/fbbxInlHXfcUf20bt260erzqIiFCxdSu3bt1De058yZEzqCYd26dQoGLFmyhLp06UK9e/em5s2b55UfZQQD17dbt27qeR6N8Y9//EPBlLvuuosuuugiv9ygHv7JJg7K8aWJolNzOUnAwI0O+lDJ/TJo8P33399gyjIABgCGYB+RPgZgkFbY7fIBGNz2J8naATAkqWY6ygJgsOwTAIM9AyQSUVES2fZanr3IUXyJk8jOnqLmWxzHF8lEtnkFKieipC8S7/Wg8gAMQTVkjwEYZPUNK72cRHYwScvTCb311lsq1LRp0+j4449XxxdccAHdfvvtFLzOF4KAgZ878cQT/efVgz/9ceedd9LZZ5+dt5YBP8tTMT322GP+rZw0HTVqFF188cXqXOEUSVzX0047jb799lv/GT647LLLaMyYMVRdXa3ORwEMDz/8MPF7grczzzyT7r77bnW8YcMG+tnPfubH4qmbDj74YHWt1D/K8aXUGK7flwRgCPa7rPRL7etXX31F22+/veqHvA6K7p8ADAAMuo+Y2AMwmFDZ3RgADO56k3TNABiSVtT98gAYLHsEwGDPAIlEVJREtr2WZy9yFF/iJLKzp6j5FsfxRTKRbV6Byoko6YvEez2oPABDUA3ZYwAGWX3DSi8nkR0EDLyg8RtvvKHWIrjwwguJF3vmjRdK5lEGDAn4m/1604CBr/E9YRuPBrjpppvULRs3bqS9996b5s+fH/YIBQGDfg819gAvSj1p0iR1OQpgePLJJ4kXFObtuOOO8xeH5mlpOnbs6K87MWvWLP+aurmEP8rxpYTiU3FLEoAhi/1Sm/u73/2O7rjjDuJpy3h0T/fu3dUlAAYABt1HTOwBGEyo7G4MAAZ3vUm6ZgAMSSvqfnkADJY9SgIw1H/7b9r4zRdFW1Ld7udU1bLxoeRFH4p5cujwS2jq9Fk0ZcoUGjJkSMxSzD0mkYiKksg211JEiuJLnEQ2FJZXII4vOoE0cuRI4kX7sLmhgKQvEu/1oGoADEE1ZI8BGGT1DSu9nER2IWBo27Yt8ZQs/fr1I07U6wQxx588eXLeVC0aMBx22GHECXjeOBHKYIIXRR4/fjxNnTpVnec/3nnnHeL1DnjUgh4dwed5kWlelJnrMnfuXD6lNg0YvvjiCzVtkR65wCDglFNOoZdeeoluvfVWfTu98sortNdee6l6R1nk2S8gcMB1P/300/0zvAA0/8c7ylaOL1HiuHyv7j/cr1avXl1yVdEvScE+/h3gjX9nuG/rPgjAYAEwvPUCrf3//0/RPtzywN9Qs50HFr1WCScBGCrBxfhtAGCIr13angRgSJtj5dcXgKF8DcsqIQnAsH7eE7Ru/oyi9Wh1xHlU0yX8W2BFH4xxEoCBKEoiO4bEeCSmAlF8iZPIjlktPBZBgTi+SCayI1QdtxYoIOkLAEOB2Cn+CMBgz7xyEtmFidzDDz9cjVTg1nBCn5P2hxxyiGocAwJeV0FvDBhWrFhBPXv21Kdo9uzZdNBBB6nPhaMALrnkErrxxhvp2GOPpSeeeELdw/GeeuopdczTEu23334+ZNCAITiNESdWeR7+2tpa9QyDCj3N0rnnnqu+7R1lBIMqpOCPCRMm0IgRI/yzPJUTn4u6leNL1Fiu3p8UYMhav+RRPvvuu6/6XeDfJ/69Wr58OQBDoKNjkeeAGMKHAAzCAjtePACD4wYlWD0AhgTFTElRAAyWjQJgsGeARCIqSiLbXsuzFzmKL3ES2dlT1HyL4/gimcg2r0DlRJT0ReK9HlQeIxiCasgeAzDI6htWejmJ7ELAcPXVV9Oee+6pwvGULH//+9/piiuuIE7sf/TRR7TZZpv5VWHA8MILL9CRRx7pn/v+++/z7uERaffee6+6fsQRRxBPScQLM+u1Hnh9hnPOOcd/PrigrQYM119/PY0ePVrdwwsxDxgwwL9/0aJFakonPqFHXcQFDFx3hhT33XefXz7Xgddf4BEZUbdyfIkay9X7kwIMWeuX3Ad5vRHeeAqyXXfdFYChoJMDMBQIIvgRgEFQ3BQUDcCQApMSqiIAQ0JCpqgYAAbLZgEw2DNAIhEVJZFtr+XZixzFlziJ7Owpar7FcXyRTGSbV6ByIkr6IvFeDyoPwBBUQ/YYgEFW37DSy0lkFwKGRx99lFq0aKHCcWL/+eefV9Mf8e/SxIkTqWXLln5VGDA8+OCDanFkPqmBgH+Dd8CL0vLaDbzxYr0MBKqqqtRn/uO5556jAw880P/M39TWIyZ0eby47dixY/17GjvgkRSLFy+ONUUSfzuc5/p//fXX/eK5zQxHWreON3VpOb74lUj5QVKAIWv9co899lB9kYGaBnCspV7kmbvFpZdeqn5Xoi4+zs/G9YWfdWUDYDDnBACDOa1djATA4KIrMnUCYJDR1eVSARgsuwPAYM8AiURUlES2vZZnL3IUX+IksrOnqPkWx/FFMpFtXoHKiSjpi8R7Pag8AENQDdljAAZZfcNKLyeRXQgYZsyYQQMHDqQXX3yRBg0aRM8884wKzSMNeE2CQsBQOIJhzZo11KpVK7+6/C1sPSKAE/hc/nbbbeePOuCE6Zlnnunff8stt9CoUaPUZw0YrrvuOrrqqqvUOR4FwVMWFds233xz4ql0oo5g+Pjjj9Xoh1WrVvnFcp2HDx/uf45zUI4vceK5+EzcRHbW+6VeWL0pT7mP6d+Npu4NXo/rS7AM28cADOYcAGAwp7WLkQAYXHRFpk4ADDK6ulwqAINldwAY7BkgkYiKksi21/LsRY7iS5xEdvYUNd/iOL5IJrLNK1A5ESV9kXivB5UHYAiqIXsMwCCrb1jp5SSyiyVyr7zyShozZkxeSE7a77bbbg0Aw8qVK6lHjx7+vbzY8+DBg9Xnuro66ty5M+nEPX/j+oYbbqDgotC8YPP06dP953k0A0/NxJsGDME1GHheeoYa1dXV6p758+erKWT4Q9euXdX6D1EAA891zyCFR1Lw1q1bN+JkCo+2KHcrx5dyY7vyfNxEdtb7JU87tmDBgjwb169f7/8u8QVeUP3iiy9W03rl3VjCh7i+lFC0sVsAGIxJTQAM5rR2MRIAg4uuyNQJgEFGV5dLBWCw7A4Agz0DJBJRURLZ9lqevchRfImTyM6eouZbHMcXyUS2eQUqJ6KkLxLv9aDyAAxBNWSPARhk9Q0rvZxEdrFELq+TwKMNgtvatWvVx8IRDDyFCydDZ86cqa7zWg28kHP79u3V9EKcmNDbe++9R927d1dTLQVHB/C6B7y4My/2zNMw6U0DBl7UmZOpehs2bBgNHTqUeNFpnpufF6Pm7aGHHiL+nY8CGHh+e/62uN4YMOy00076o7+/8MIL86Zy8i+EHJTjS0ixqboUN5Gd9X5ZzGQs8pyvCgBDvh6SnwAYJNV1v2wABvc9SqqGAAxJKZmecgAYLHsFwGDPAIlEVJREtr2WZy9yFF/iJLKzp6j5FsfxRTKRbV6Byoko6YvEez2oPABDUA3ZYwAGWX3DSi8nkV0skbtixQrq1KmTH5KT/zxqYN26dQ1GMDBg4HUVdtllF//+Yge8UDRPdcTbhg0bqH///nnrHRR7RgMGvjZ+/Hg1RVOx+/gcj67g9RNqamoiAQb9fmusXH1+8uTJdMopp+iPJe3L8aWkACm4KUnAkKV+WcxaAIZ8VQAY8vWQ/ATAIKmu+2UDMLjvUVI1BGBISsn0lAPAYNkrAAZ7BkgkoqIksu21PHuRo/gSJ5GdPUXNtziOLzrRM3LkSBo3bpz5SiNiUQUkfZF4rwcbAcAQVEP2GIBBVt+w0stJZE+bNo1+9atfqeKPOeYYevzxx9XxDjvsQB988IE6vvzyy4nf6YWA4bvvviNe94C3JUuW0Mknn0w8ZVHhNmHChAbrJnz11VfEIxH0yAd+hkcp8JoMevREv379FCzQ5XFdzz//fH/9Bn2e13m47bbbiGEHb6+++qpaU4GPGTwUTjXD5/UWXCNCnyu258WsedRElK0cX6LEcfneuIAh6/2ymKdBwMK/K5988kmx20o6F9eXkgo3dBMAgyGhvTAADOa0djESAIOLrsjUCYBBRleXSwVgsOwOAIM9AyQSUVES2fZanr3IUXyJk8jOnqLmWxzHF8lEtnkFKieipC8S7/Wg8gAMQTVkjwEYZPUNK92lRDavycBTF3ECs1evXsSgora2ttHq87ey33zzTbV+Ak+fVFVV1ei9fIFHP7z//vsKaPA0TDydUZs2bUKfsXXRJV9saeBKIhv9Mr8HuOJLfq2ifQJgiKZXOXcDMJSjXvqfBWBIv4eltgCAoVSlKuc+AAbLXgIw2DNAIhEVJZFtr+XZixzFlziJ7Owpar7FcXyRTGSbV6ByIkr6IvFeDyoPwBBUQ/YYgEFW37DSkcgOU8feNfhCCjRtueWW1LZtW1q9erU9MxA5TwEAhpwcGlKMveNKOuM3J+RpVPjhx0XP09o5kwpPq88tD/g1Neu9f9FrlXASgKESXIzfBgCG+Nql7UkAhrQ5Vn59ARjK17CsEgAYypKvrIclElFREtllVR4PR1Igii9xEtmRKoObYykQxxfJRHasRuAhpYCkLxLv9aBtAAxBNWSPARhk9Q0rHYnsMHXsXYMvAAz2el94ZACGnD4ADOH9RF8FYNBKZHMPwJAd3wEYsuO1bikAg1bC0h6AwZLwXliJRFSURLa9lmcvchRf4iSys6eo+RbH8UUykW1egcqJKOmLxHs9qDwAQ1AN2WMABll9w0pHIjtMHXvX4AsAg73eFx4ZgCGnTxTAsPHzj2jD0n8WFba26y+oeuvti16rhJMADJXgYvw2ADDE1y5tTwIwpM2x8usLwFC+hmWVAMBQlnxlPSyRiIqSyC6r8ng4kgJRfImTyI5UGdwcS4E4vkgmsmM1Ag8pBSR9kXivB20DYAiqIXsMwCCrb1jpSGSHqWPvGnwBYLDX+8IjAzDk9IkCGMIVreyrAAyV7W9TrQNgaEqhyrkOwFA5XpbaEgCGUpUSug+AQUjYEoqVSERFSWSXUEXckpACUXyJk8hOqJooJkSBOL5IJrJDqopLTSgg6YvEez3YHACGoBqyxwAMsvqGlY5Edpg69q7BFwAGe70vPDIAQ04fAIbwfqKvAjBoJbK5B2DIju8ADNnxWrcUgEErYWkPwGBJeC+sRCIqSuOYebMAAEAASURBVCLbXsuzFzmKL3ES2dlT1HyL4/gimcg2r0DlRJT0ReK9HlQegCGohuwxAIOsvmGlI5Edpo69a/AFgMFe7wuPDMCQ0weAIbyf6KsADFqJbO4BGLLjOwBDdrzWLQVg0EpY2gMwWBLeCyuRiIqSyLbX8uxFjuJLnER29hQ13+I4vkgmss0rUDkRJX2ReK8HlQdgCKohewzAIKtvWOlIZIepY+8afAFgsNf7wiMDMOT0AWAI7yf6KgCDViKbewCG7PgOwJAdr3VLARi0Epb2AAyWhPfCSiSioiSy7bU8e5Gj+BInkZ09Rc23OI4vkols8wpUTkRJXyTe60HlARiCasgeAzDI6htWOhLZYerYuwZfABjs9b7wyAAMOX0AGML7ib4KwKCVyOYegCE7vgMwZMdr3VIABq2EpT0AgyXhvbASiagoiWx7Lc9e5Ci+xElkZ09R8y2O44tkItu8ApUTUdIXifd6UHkAhqAasscADLL6hpWORHaYOvauwRcABnu9LzwyAENOHwCG8H6irwIwaCWyuQdgyI7vAAzZ8Vq3FIBBK2FpD8BgSXgvrEQiKkoi217Lsxc5ii9xEtnZU9R8i+P4IpnINq9A5USU9EXivR5UHoAhqIbsMQCDrL5hpSORHaaOvWvwBYDBXu8LjwzAkNMHgCG8n+irAAxaiWzuARiy4zsAQ3a81i0FYNBKWNoDMFgS3gsrkYiKksi21/LsRY7iS5xEdvYUNd/iOL5IJrLNK1A5ESV9kXivB5UHYAiqIXsMwCCrb1jpSGSHqWPvGnwBYLDX+8IjAzDk9AFgCO8n+ioAg1Yim3sAhuz4DsCQHa91SwEYtBKW9kkABktVbxB26PBLaOr0WTRlyhQaMmRIg+uunZBIREVJZLumRyXXJ4ovcRLZlaydK22L44tkItsVXdJYD0lfJN7rQY0BGIJqyB4DMMjqG1Y6Etlh6ti7Bl8AGOz1vvDIAAw5fQAYwvuJvgrAoJXI5h6AITu+AzBkx2vdUgAGrYSlPQCDJeG9sBKJqCiJbHstz17kKL7ESWRnT1HzLY7ji2Qi27wClRNR0heJ93pQeQCGoBqyxwAMsvqGlY5Edpg69q7BFwAGe70vPDIAQ04fUcBQXUO0sS7ciJRcBWBIiVFC1QRgEBLWwWIBGBw0RbhKAAzCAjdVPABDUwrJXZdIREVJZMu1DCUXKhDFlziJ7MJ4+Jy8AnF8kUxkJ9/C7JQo6YvEez3oDABDUA3ZYwAGWX3DSkciO0wde9fgCwCDvd4XHhmAIaePKGCoaeEFqSeq+zG3D7fE6asADE7bI145AAZxiZ0JAMDgjBXGKgLAYEzq4oEAGIrrYuKsRCIqSiLbRBsRI6dAFF/iJLKhs7wCcXyRTGTLt7hyI0j6IvFeDzoBwBBUQ/YYgEFW37DSkcgOU8feNfgCwGCv94VHBmDI6SMPGH7yoX5DqkczADCE/z5V+lUAhkp3eFP7ABg2aZGVIwAGy04DMNgzQCIRFUxk22sZIhdTgL3ZY4896NVXXy122T+nE9l8Lza3FGAPR48eTWPGjCmpYjqRDS9LksvoTezlyJEjady4cYnGlXivBysIwBBUQ/YYgEFW37DSkcgOU8feNfgCwGCv94VHBmDI6WMMMKhw6R3NAMAQ/vtU6VcBGCrd4U3tA2DYpEVWjgAYLDsNwGDPAIlElAYM9lqFyGEKRAEMYeXgmj0F4gAGe7VF5DAFABjC1ME1AAZ7fQCJbHvah0WGLwAMYf3D5jUAhpz6JgDDv7/fSFu2rKLa6qpc0BSOZgBgsPnbaj82AIN9D0zVAIDBlNLuxAFgsOwFAIM9A6QAg70WIXIpCjT1bXaGRNjcVqApD3Xt4aVWwt19qV6W2gKJ93owNkYwBNWQPQZgkNU3rHQkssPUsXcNvgAw2Ot94ZEBGHL6mAAMK9bkFnpuXVtNrVv8BBlStjYDAEP471OlXwVgqHSHN7UPgGGTFlk5AmCw7DQAgz0DpBNR9lqGyFAACkCBbCog/V4HYDDXrwAYzGldGAmJ7EJF3PgMXwAY3OiJDWsBwJDTxCRg4Ii1VVWpHM0AwNDwdyhLZwAYsuM2AEN2vNYtBWDQSljaAzBYEt4LK52IstcyRIYCUAAKZFMB6fc6AIO5fgXAYE7rwkhIZBcq4sZn+ALA4EZPbFgLAIacJqYBg3YibaMZABi0c9ncAzBkx3cAhux4rVsKwKCVsLQHYLAkvBdWOhFlr2WIDAWgABTIpgLS73UABnP9CoDBnNaFkZDILlTEjc/wBYDBjZ7YsBYADDlNbAEGjp6m0QwADA1/h7J0xhXAMHPmTBoxYkSe9DfffDMNGzYs79xZZ51F06dPzzu3dOlSatWqVd65cj+8/PLLNGTIEFVM3759acaMGZGLXLBgAQ0ePFg916dPH3r66aebLGPlypU0YcIEWrx4MfG7vEePHnTwwQf75TRZQMgNAAwh4lToJQAGy8YCMNgzQDoRZa9liAwFoAAUyKYC0u91AAZz/QqAwZzWhZGQyC5UxI3P8AWAwY2e2LAWAAw5TWwCBu1KGkYzADBot7K5dwUwPPLII3TSSSflmXDqqaeqZHvwZIcOHWjVqlXBU/TNN9/QFltskXeu3A/PPvssDRo0SBWz++67U5y1BOfNm0f9+/dXZey88860aNGi0GrNnTtXgYRvv/22wX1HHXWUAiu1tbUNrpV6AoChVKUq5z4ABsteAjDYM0A6EWWvZYgMBaAAFMimAtLvdQAGc/0KgMGc1oWRkMguVMSNz/AFgMGNntiwFgAMOU1cAAxcE9dHMwAwNPwdytIZlwHDtttuS5988olvx4cffkhdu3b1P+uDSgAMDBUYQnz66ae6WQ32Y8aModGjRzc4X+oJAIZSlaqc+wAYLHsJwGDPAOlElL2WITIUgAJQIJsKSL/XARjM9SsABnNaF0ZCIrtQETc+wxcABjd6YsNaADDkNLnqqqvo4YcfpmuvOJtOOv7QhkIFzmx4dx6tm/d44MymwxZ7HUu1PffadCJwtGJNXeBT+KGroxkAGMJ9q/SrLgMG1n7ZsmXUuXNnZUOxUQ58oRAw1NXVEU+b9Pbbb6tphnbccUfin9atW6tyiv3BoyIWLlxI7dq1o1133ZXmzJkTOoJh3bp19O6779KSJUuoS5cu1Lt3b2revHle0VFGMPz1r3+lX/7yl+p5HjHx2GOPUfv27em8886j8ePHq/P9+vUjLjPuBsAQV7n0PgfAYNk7AAZ7Bkgnouy1DJGhABSAAtlUQPq9DsBgrl8BMJjTujASEtmFirjxGb4AMLjRExvWAoAhp4lrgIFr5eJoBgCGhr9DWTrjImDgb/K/9dZbyoZp06bR8ccfr44vuOACuv3229U3/fV1vhAEDHz+xBNP9J9XD/70x5133klnn302VVVV+af5WZ6KiRP6ettmm21o1KhRdPHFF6tThVMkMeg47bTTqHAqo8suu4x4lEF1dbV6Lgpg+OMf/+jH4zb+7ne/U2W8+OKLNHDgQHVcOKJDnYzwBwBDBLEq5FYABstGAjDYM0A6EWWvZYgMBaAAFMimAtLvdQAGc/0KgMGc1oWRkMguVMSNz/AFgMGNntiwFgAMRJ95IwuuutIbwTD1Yfr/rjjHG8GQW2i1oVq5Mz++O5/WNzKCobk3gqFZz36NPRrrvEujGQAYYllYMQ+5CBh4vYE33nhDTRd04YUXEi/2zBsvlMyjDBgS3HXXXb4HGjDwNb4nbLvooovopptuUrds3LiR9t57b5o/f37YIxQEDPfccw/x9GuNbbwo9aRJk9TlKIBh9erVxD+8derUiVq0aKGOb7zxRmJwwdsll1xC/DnuBsAQV7n0PgfAYNk7AAZ7Bkgnouy1DJGhABSAAtlUQPq9DsBgrl8BMJjTujASEtmFirjxGb4AMLjRExvWAoDBfcDArrkymgGAoeHvUJbOuAoY2rZtS/fffz/paYH0e429mTx5MnEiX28aMBx22GE0a9YsdZq/7c9gYquttlJTDE2dOlXfTu+88w716tVLjVrQoyP4Ii8yzYsy8wgFXnBZbxowfPHFF2oNCD1y4bjjjqNTTjmFXnrpJbr11lv17fTKK6/QXnvtpaYzirLIsy5gw4YNdOaZZ6opmHgEA2+8iDXXiUd3xN0AGOIql97nABgsewfAYM8A6USUvZYhMhSAAlAgmwpIv9cBGMz1KwAGc1oXRkIiu1ARNz7DFwAGN3piw1roRBwn6PQ3Yhve5fYZvUDzuHHjaOTIkZEr6/oIhmCDbI9mAGAIupG9Y1cBw+GHH65GKrAjnNDnpP0hhxyiDGJAwGsq6I0Bw4oVK6hnz576FM2ePZsOOugg9bm+vp46duxIvM4Cb3okwLHHHktPPPGEOsfxnnrqKXXMCf799tvPhwwaMPCaLvx/D954GiVeH6K2tlZ9ZlChp1k699xz6Y477ogNGNavX++PYFCFe3/wSAueLkqvR6HPR9kDMERRqzLuBWCw7CMAgz0DpBNR9lqGyFAACkCBbCog/V4HYDDXrwAYzGldGAmJ7EJF3PgMXwAY3OiJDWsBwJCOEQxB52yOZgBgCDqRvWNXAcPVV19Ne+65pzKEF1z++9//TldccYVK7H/00Ue02Wab+WYxYHjhhRfoyCOP9M99//33efcwqLz33nvV9SOOOIKefPJJtTCzXsuB12c455xz/Od59IQeJaEBw/XXX0+jR49W9/CIggEDBvj3L1q0SE3pxCf0qIsoUyT5BXkHDDgYWCxevFiNYtDX9t13X7X4tIYa+nypewCGUpWqnPsAGCx7CcBgzwDpRJS9liEyFIACUCCbCki/1wEYzPUrAAZzWhdGQiK7UBE3PsMXAAY3emLDWgAwpA8waBfzRjPUrdOnRfcADKLyOl+4q4Dh0Ucf9b/Fz4n9559/Xk1/xP/2nzhxIrVs2dLXlgHDgw8+qKYV4pMaCPg3eAd33323PyKCpxliIBBc7Pm5556jAw880H+ER0DoERO6vLPOOovGjh3r39PYAY+kYDgQFzAEy505c2YeOHn99depb9++wVtKPgZgKFmqirkRgMGylQAM9gyQTkTZaxkiQwEoAAWyqYD0ex2AwVy/AmAwp3VhJCSyCxVx4zN8AWBwoyc2rAUAQ3oBw2Y1VdSmRbWX+PR8BWBo2LlxJnEFXAUMM2bMoIEDBxKvQTBo0CB65plnVNt5pMHpp5/eADAUjmBYs2YNtWrVytfrtNNOo/vuu0995kWkufztttvOH3XAAILXPdDbLbfcQqNGjVIfNWC47rrr6KqrrlLneBTEqaeeqm/P22+++ebEUy5FAQy///3vFZTggniUBK8RobcOHTr40zvxWhInnniivhRpD8AQSa6KuBmAwbKNSQCGumWLqW7ZO0Vb0qzn3lS1ZYei15I+OXT4JTR1+iyaMmUKDRkyJOniEy9POhGVeIVRIBSAAlAACoQqIP1eB2AIlT/RiwAMicoZqTAksiPJZexm+ALAYKyzRQwEwJA+wMA8gcHCZrV85G31dUQbN+SOhf/ECAZhgR0v3mXAcOWVV9KYMWPyFOSk/W677dYAMKxcuZJ69Ojh38uLPQ8ePFh9rqurU2sX6DUYLr30UrrhhhsouCg0L9g8ffp0/3kezcBTM/GmAUNwDQaeroihRnV1tbpn/vz5tHDhQnXctWtXtf5DFMBwwQUX0O23366e51ESvA4Nb0uXLqVu3bqpY/6DF5QOTs3kXyjhAIChBJEq7BYABsuGJgEY1s97gtbNn1G0Ja2OOI9quvQpei3pkwAMSSuK8qAAFIACUCCKAgAMUdRy+14ABnv+IJFtT/uwyPAFgCGsf9i8BsCQLsCQv/5C/U9wwQMMhjYABkNCOxrGZcDA6yTwaIPgtnbtWvWxcIokXhOB12DgKYV440WYb7zxRmrfvr1ae4Hbqbf33nuPunfvrqZaGj58uD5NvDgzL+7Miz3zNEx604CBF3Xedttt9Wm1RsPQoUOJF53mNSN4MWreHnroIbUYdBTAwAtE87oLvHFb/vznPxOPhLj11lsVVNDnuQ58Pc4GwBBHtXQ/A8Bg2T8ABnsGSCei7LUMkaEAFIAC2VRA+r2OEQzm+hUAgzmtCyMhkV2oiBuf4QsAgxs9sWEtABjSAxjypkQiDy7U/egZ6u0NbgAMBsV2MJTLgGHFihXUqVMnXzVO/vOogXXr1jUYwcBJd15XYZdddvHvL3bAC0XzVEe88WLK/fv3J17XIGzTgIHvGT9+vJqiqbH7eXQFl1dTUxNpiqTVq1cTj4rQi04XK59jjxgxotilks4BMJQkU0XdBMBg2U4ABnsGSCei7LUMkaEAFIAC2VRA+r0OwGCuXwEwmNO6MBIS2YWKuPEZvgAwuNETG9YCgMF9wGBzSqTCHgPAUKhItj67AhimTZtGv/rVr5T4xxxzDD3++OPqeIcddqAPPvhAHV9++eXEaxUUAobvvvtOfdufb1qyZAmdfPLJxFMWFW4TJkxosG7CV199pUYi6JEP/AyPUuA1GfToiX79+ilYoMvjup5//vn++g36PK/zcNttt/kjDF599VXiZ3lj8LBgwQJ9a9E914VHRPD0TsGN4ck999yjRkUEz0c9BmCIqlj67wdgsOwhAIM9A6QTUfZahshQAApAgWwqIP1eB2Aw168AGMxpXRgJiexCRdz4DF8AGNzoiQ1rAcCQ04QXY+U506+94mw66fhDGwoVOLPh3Xm0bl4uoRk4rQ5b7HUs1fbcq/C0+rxiTfSpjGxPiVTYEACGQkWy9dkVwJC06rwmA09dxO9DXjCZQUVtbW2jYZYvX05vvvkm8foJPH1SlVppvdHb1eiH999/XwENnoZpp512ojZt2jT+QIQrvEA1j8b4/PPPVb15DYbmzZtHKKH4rQAMxXWp5LMADJbdBWCwZ4B0IspeyxAZCkABKJBNBaTf6wAM5voVAIM5rQsjIZFdqIgbn+ELAIMbPbFhLQAYcpq4CBhcmBKpsMcAMBQqkq3PlQoYsuViaa0FYChNp0q6C4DBspsADPYMkE5E2WsZIkMBKAAFsqmA9HsdgMFcvwJgMKd1YSQksgsVceMzfAFgcKMnNqwFAENOk9/+9rdqWpGxd1xJZ/zmhIZCBc78uOh5WjtnUuDMpsOWB/yamvXef9MJPqppoT6XOoLBpSmR8htCBMBQqEi2PgMwZMdvAIbseK1bCsCglbC0B2CwJLwXVjoRZa9liAwFoAAUyKYC0u91AAZz/QqAwZzWhZGQyC5UxI3P8AWAwY2e2LAWAAw5TVwBDK5NiVTYYwAYChXJ1mcAhuz4DcCQHa91SwEYtBKW9gAMloT3wkonouy1DJGhABSAAtlUQPq9DsBgrl8BMJjTujASEtmFirjxGb4AMLjRExvWAoAhp4kLgMHFKZEKewwAQ6Ei2foMwJAdvwEYsuO1bikAg1bC0h6AwZLwXljpRJS9liEyFIACUCCbCki/1wEYzPUrAAZzWhdGQiK7UBE3PsMXAAY3emLDWgAw5DSxCRhcnhKpsMcAMBQqkq3PAAzZ8RuAITte65YCMGglLO0BGCwJ74WVTkTZaxkiQwEoAAWyqYD0ex2AwVy/AmAwp3VhJCSyCxVx4zN8AWBwoyc2rAUAQ04TW4DB9SmRCnsMAEOhItn6DMCQHb8BGLLjtW4pAINWwtIegMGS8F5Y6USUvZYhMhSAAlAgmwpIv9cBGMz1KwAGc1oXRtKJ7JEjRxZewmfLCtxzzz10zTXX0NVXX225JnbCBxPZQ4YMsVMJRC2qAPfNtm3b0urVq4ted/2khgPjxo2juO8+XYbJRZ7TMCVSofcADIWKZOszAEN2/AZgyI7XuqUADFoJS3sABkvCe2GlE1H2WobIUAAKQIFsKiD9XgdgMNevABjMaV0YSQOGwvP47IYCAAxbumEEatFAAQCG3xKDllIAw8bPP6INS//ZQEM+Udv1F1S99fb512paqM8r1tSpfZqmRMpvCBEAQ6Ei2foMwJAdvwEYsuO1bikAg1bC0h6AwZLwXljpRJS9liEyFIACUCCbCki/1wEYzPUrAAZzWhdG4gQZNrcViPsNa7db1XTteATDww8/3PSNuMOaAmntm3r0gakRDJENCgCGtE2JVNhWAIZCRbL1GYAhO34DMGTHa91SAAathKU9AIMl4b2w0okoey1DZCgABaBANhWQfq8DMJjrVwAM5rRGJCgABaBA1hVIC2D4eu1GatOimqp4CAPVE9X9mNunyEAAhhSZJVBVAAYBUR0tEoDBUWMEqwXAIChuKUUDMJSiksw90okomVqjVCgABaAAFGhMAen3OgBDY8onfx6AIXlNUSIUgAJQAAoUVyAtgMGvfb03VdLGDf7HNB0AMKTJreTrCsCQvKaulgjA4KozcvUCYJDTtqSSARhKkknkJulElEilUSgUgAJQAAo0qoD0ex2AoVHpE78AwJC4pCgQCkABKAAFGlEgPYDBG7Wg4EJuLYZGmuP0aQAGp+0RrxwAg7jEzgQAYHDGCmMVAWAwJnXxQAAMxXUxcVY6EWWiDYgBBaAAFIACmxSQfq8DMGzSWvoIgEFaYZQPBaAAFIACWgHnAYOuaAXsARgqwMQymgDAUIZ4KXsUgCFlhiVQXQCGBEQspwgAhnLUK+9Z6URUebXD01AACkABKBBVAen3OgBDVEfi3w/AEF87PAkFoAAUgALRFABgiKZXOXcDMJSjXvqfBWBIv4eltgCAoVSlKuc+AAbLXgIw2DNAOhFlr2WIDAWgABTIpgLS73UABnP9CoDBnNaIBAWgABTIugIADOZ6AACDOa1djATA4KIrMnUCYJDR1eVSARgsuwPAYM8A6USUvZYhMhSAAlAgmwpIv9cBGMz1KwAGc1ojEhSAAlAg6woAMJjrAQAM5rR2MRIAg4uuyNQJgEFGV5dLBWCw7E4SgMFyE/zwQ4dfQlOnz6IpU6bQkCFD/POuHkgnolxtN+oFBaAAFKhUBaTf6wAM5noOAIM5rREJCkABKJB1BQAYzPUAAAZzWrsYCYDBRVdk6gTAIKOry6UCMFh2B4DBngHSiSh7LUNkKAAFoEA2FZB+rwMwmOtXAAzmtEYkKAAFoEDWFQBgMNcDABjMae1iJAAGF12RqRMAg4yuLpcKwGDZHQAGewZIJ6LstQyRoQAUgALZVED6vQ7AYK5fATCY0xqRoAAUgAJZVwCAwVwPAGAwp7WLkQAYXHRFpk4ADDK6ulwqAINldwAY7BkgnYiy1zJEhgJQAApkUwHp9zoAg7l+BcBgTmtEggJQAApkXQEABnM9AIDBnNYuRgJgcNEVmToBMMjo6nKpAAyW3QFgsGeAdCLKXsvci7zx25W08dvPiNZ9S9U/24Gq23Z2r5KoERSAAqlXQPq9DsBgrosAMJjTGpGgABSAAllXAIDBXA8AYDCntYuRABhcdEWmTgAMMrq6XCoAg2V3ABjsGSCdiLLXMoci122gDR+8SGvnTaKabXoQ1f1AdV8uoRZ9/4tqux9CVc1aOlRZVAUKQIG0KyD9XgdgMNdDABjMaY1IUAAKQIGsKwDAYK4HADCY09rFSAAMLroiUycABhldXS4VgMGyOwAM9gyQTkTZa5kDket+9EYsrKK1c+6kH999lqiaqFnvX1LVxrX049I5RLU1VLt9f2rR/xyqabs9UU0zByqNKkABKJB2BaTf6wAM5noIAIM5rREJCkABKJB1BQAYzPUAAAZzWrsYCYDBRVdk6gTAIKOry6UCMFh2B4DBngHSiSh7LbMYecN6qvviY/rxrWdp3WuPEK1f7cGDKvWTBxhqiKpqqqmq1RbUbKdjqbbrIT+BhhYWK4/QUAAKpF0B6fc6AIO5HgLAYE5rRIICUAAKZF0BAAZzPQCAwZzWLkYCYHDRFZk6ATDI6OpyqQAMlt0BYLBngHQiyl7LLEX2Ri38+PYcWvfyZNqwcrFXiQ0eRPB2IYCBfrpe0647Nd/pRKrpMoiqajFtkiUHERYKpF4B6fc6AIO5LgLAYE5rRIICUAAKZF0BAAZzPQCAwZzWLkYCYHDRFZk6ATDI6OpyqQAMlt0BYLBngHQiyl7L7ESuX7eGfph5M63/5xNqSiQGC1UtmnuV+dH7XFUwRZJ3rbbWG8aw0QMQ3i3evc3+YyC12GsUVW3Wzk4DENWKAhs3bqT169fTxvp6K/HTHLRF8+ZUU8O/QNi0AtLvdQAGrbT8HoBBXmNEgAJQAApAgZwCAAzmegIAgzmtXYwEwOCiKzJ1AmCQ0dXlUgEYLLsDwGDPAOlElL2W2Ylcv+47+uEvf6D1C2f+NCXSIGqx26G05onLPJBQlw8YWrT0FnoeRvU/rKIf33/yJ8Dw//wEGLa20wBEjaUAwwHemnvJ7jhbvQcWGDIEt++//57+93/vpXbt2tOQ//xPqqn2ptOq8qbawpanAGtS7WmDbZMC0u91AIZNWksfATBIK4zyoQAUgAJQQCsAwKCVkN8DMMhr7HIEAAaX3Um2bgAMyeqZhtIAGCy7BMBgzwDpRJS9ltmJrEYw/PU2qvvsbWp54OnUrNdAtR7DdxOHehVqCBg22/cCarbjkbRhxUJav2AsVW/RkVrsca43gsEuYNi4bi2tfXM+tdypL1W3al2ymJwoX7duHXG/Cm6cAG7RogXV8oiNEjcua+3atVRXV1fSE5xobtWqlfEk/B133EG33HILPf7449S3b9+S6lrKTR988AHdfvttSrcOHTpS27ZtqHPnn9N2221HW2+9NW2++eYKSmy22WbqnlLKxD3ZUED6vQ7AYK4fATCY0xqRoAAUgAJZV8A4YPj+a6r7akVR2Wu26kjUqm3Ra5VwEoChElyM3wYAhvjape1JAIa0OVZ+fQEYytewrBIAGMqSr6yHpRNRZVUuhQ/Xews8b/h0EdX8rDNVt+mgWlD376VUFDDUVlFNe2/dhV1Ootou+/GKz7Txm4+oum1XqmpeelI/aZk2/rCGvn32Mfp+wTza+pyrqHar9iWHWLNmDU2aNIleeukl/xmevqZ169b0i1/8QiXg+/TpQ82aNfOvN3bwxRdf0OTJk+lf//pXY7fknW/Xrh1dd911xpPt119/Pd155500c+bMxAAD68jAYunSD+jQQw+jli1belMoraPVq1fTV199RZ9//m/vZxV98803dNRRR9PAgQNjg5UNGzYoKKQhUGOjAfhdEeYbg6DC0QR8jstrbOQFj9pgkKSnOGI4xaNAGruf68D3NlbHvA6R4Q/S73UABnOdC4DBnNaIBAWgABTIugKmAcOPi56ntXMmFZW95QG/9kZ+71/0WiWcBGCoBBfjtwGAIb52aXsSgCFtjpVfXwCG8jUsqwQAhrLkK+th6URUWZWrkIcbBQzetPFVNd7ULl5CteU+/6832uEY6y2u95LN3zw1hb7+61Rq2Xt3aj/8QqrfWEfrP11KLbr0pOrNNg+t45dffkm/+93v6KmnnvIS30epZDAnjVeuXEmLFy+mNm3a0H//93/T8OHDmwQBxQADT0U0b948+vbbb+mQQw7JS3hXCmDgpPvrr7/uAYsnqUuXLsRApqamViXiGSisWJH7phePCHn99deoe/cedNxxx6nRG6HmBC5yUp89ee211+iTTz6mH35Yq5L2PEqiV68dFShhqKG3r7/+mqZOfZj23ntv2nXX3fRpf88jTZ577jnaYYcdqGfPngoQMAyZM2eOV78dqHfvXRpAA27nCy8873nYnPbZZx81UmXixAkeLNnfL8MP4B1w27lf9eu3pxene/ASjgsUkH6vAzAUCC74EYBBUFwUDQWgABSAAnkKADDkySH6AYBBVF7nCwdgcN6ixCoIwJCYlKkpCIDBslUADPYMkE5E2WuZO5GbBAweaGj+i1O9qZF+a7XSnHRePe0++uZvj1BzDyb87L/OoRbb96BvX/wrffPMY/SzIb+lzXbtF1pHDRj++c9/0qxZs9Q37/kb8t999x0tW7aMrrzySvrwww+JpxU6+uijQ8viBDR/k5/7qN44aX3VVVfRRx99pEY3MLDQG3+rvW3btg0S2fq61D7pEQyff/45PfDAA970R3XUrVs36tq1mxq5wG1u3XpzlcTnKZN4VMgzzz5D//ZGMxxwwAHUo0ePkpv4ySefECfzeUTCbrv1UVMubdjwI3366acewJnvQYHudPrpp/sjCzj22WefRfwPpGuvvZY6deqcF4t9+eMf/0j77rsvHX744cqDjz/+mG644QYPjGykm2++2avvFnnPcL/g8+zhGWecoQDH2LF306pVq+j88y+grbbayr+f72XgwsDi4osvbhDfvxEHSgHp9zoAg7mOBsBgTmtEggJQAApkXQEABnM9AIDBnNYuRgJgcNEVmToBMMjo6nKpAAyW3QFgsGeAdCLKXsvcidwoYGjurUnQdV/a+OU71Kz7oR5gONNapRVceGyiAgzNt+vmTY10NTXftqs3xKKKNnz1b1r7r/nUavd9qaqlt8aBN0UNny+2acDA0xrNnTtXAYbgfTwqYdddd1XfuP+f//mfSGsycDlc/qhRo7ypg5aqKYQYKDS28WgHBhRbbLFFo3GC0/ow0ODphzhxz6MDCjcui8vkhLie0ofvKQYY+PcqzlQ+XJ+77vozbbnlVmqURps2W6iEO6+9wN/y79ixoz+FEI/i+Nvf/kbbbLMNffbZZ3TCCSfkjegorL/+zF7ff/9kevfdd73RJOcquMBwhs+zBuzdlVdeQVdffQ3tueee6jEGDGeddabSZr/9BhL/BzS4oHUOMNxEAwbsS7/85S8VYOBnGCC8++5iGjx4MF144ShdBbVnaMBQgkdNnHHGSFV37h9jxlxH/fv3pxNPPMnXmet6551/omOPPU7BFEyRlCdlgw/S73UAhgaSi50AYBCTFgVDASgABaBAgQIADAWCCH4EYBAUNwVFAzCkwKSEqgjAkJCQKSoGgMGyWQAM9gyQTkTZa5k7kRsFDC1a0mb7X0a12w/w1l74hGq27m2n0t435b+Z/QR9+dDd3toR21CHC/4ve+cBH0XRhvEnlTR66BBCr4J06VURlY4iiBQpIr1KE0GqgDQR6R0RAUH4pAlI70167z2BNNLrN8/gxpRLSLm9u4SZ3y/c3e7szOw7x97u+5/3fSbCPp97vLGEPr6HF4unInPzjnAqXy3efm54HWBgnY8++kg6/GfOnIklS5ZIZ/TYsWOjncmsw/L8+XMsW7YMTH3Uvn17CSteBxjotD579qyMkPjrr79k9APT97Rp00aswO8lU/9o8ODGjRv4/PPP5XaulmdqJzrK2d+CBQvQoEEDCSj27NkDwhCmLaI2AP9atGghUz1R1JmAYc6cOVJ7gmP++eefcfnyZXkOX3zxhazn5uYmj5MbE/iHcGHv3r04f/68PFemD6pRo4YYY0cBHLLEO54whBEMlStXwcGDB1CtajWULFkygdb/20yIMGbMGJEKqYS0a1xnPUFDly6dJdD48stX0EsDDF999ZWMTGnXrp3YXyNaCyEhwDBnzo8yxdPq1asxatQo1KpVO/oYQ4CBfVO/47ff1qJHjx4oW/Ytkb4pCIsXLxInYAXakwBIlcQtoPd1XQGGxO1vzL0KMBjTmqotZQFlAWUBZYHELKAAQ2LWMe4+BRiMa8+01poCDGltxlI+XgUYUm67tHqkAgxmnjkFGMw3AXo7osx3ZpbT8+sAg13x98062NAHt/F84WREBvgj5+DvBVwoaHA8QZfO4PmSqcLNa4U84xbAxuW/9ETaAUkFDMzvP2/ePEycOBFz584FUyqVKRMbsNDRzNQ5dEzTocmIgMQAAx3069atw8iRI2VqIQICd6FhQIf9gQMHQB2BcePGydRMXH1/8eJFGUlRpUoVGRHBVfMVK1aU6ZeoH1GqVClMnTpVQhBuryfSELm6uuLKlStyvIQWHTt2lGmACBnq168vzcC6TG20f/9+EHK0bdtWpnVKLNqCjnW2u379OhGRkAuBgQEICAxEmdJlUKlSJXkemo21V2oxEEhUFxCCDv6jRw6L8XSCs3PiOhkEDMOGDZN6Cox6MFT69++HAgUKiGiRoXK3BhgmTJiIW7duybF27dpVpkxihcQAwyeffCLSYz0WGg17MHDgIJHyqZCEJYYAA9tiOi2miOJ8ETKcPn0K27dvF++/lNoMrKNK4hbQ+7quAEPi9jfmXgUYjGlN1ZaygLKAsoCyQGIWUIAhMesYd58CDMa1Z1prTQGGtDZjKR+vAgwpt11aPVIBBjPPnAIM5psAvR1R5jszy+nZogGDyI//ct9WvFgxC9k7D0TGeh8laLiosFAZ5eC3cwNy9BoNl5rvxav7OsBAYeFatWpJAegpU6bIlf5Vq1aVDm86/7VC5/P06dOxefNmLFy4UKxkLyt3JQYYGLlAXYfatWuLNDsToh3ZdN4TMhBU0Gm9atUq6bAnYCBIIJigVoAGMeiAZ1m/fr1MBdSvXz8ZhZAjRw7pGGd7TOXDekxPRLjA4wcMGCB1C+iYZ1QAne7UiyDcYFuJaSS8itZYKiIXHKX+AdM6MfIiT968eHD/AXLlyikjAWJGG9Dpf/LUSRnBwDH9889Z2AoI06BBw0QhA8dNHYMaNaqjVavkAYYpU6bKc160aJGAC3lldIWtre1rAUOFChVliiNbWzt06tRJpplKCDDQ9jw3AqiCBQvi5MmTYMQEgVHM1FSsp4phC+h9XVeAwbDd9diqAIMeVlVtKgsoCygLKAsYsoACDIasos82BRj0sWtaaVUBhrQyU6kfpwIMqbdhWmtBAQYzz5gCDOabAL0dUeY7M8vp2ZIBQ1RoCHz+WI6XB3Yiz6hZsMvjlqjhAs4chuecMXCp9R5cu34dr25CgIEOcKYkon7CuXPnsHTpUjRs2BB0MjNlkp+fn9RUoMOehXoH3E/dAUY5ODk5ye0JAQa236dPH5FKZ7GMTIjrzKdTfePGjUIHYDC+//576bAmYPjwww/RvHlzCSRiCkZzXHRoU3CZkRTZsmWT/Rv6R0uRtGXLFjAaQivsc/ny5RgxYoSMZChfvry2K94rowK2bdsmNQ+YookAI0gAhqoi7RFtcfbsGWQS0IEwhg59llOnTsl6Jf5Ni+Tp6YGZM2ZIpz+FlhMqqQUMZcuWkREcCxcukhEGTBP1uggGngfnn6mOqNFAHQeOI64GgzZm7tu1axdmzZopIjgqS3FwLbWVVke9JmwBva/rCjAkbHtj71GAwdgWVe0pCygLKAsoCyRkAQUYErKM8bcrwGB8m6alFhVgSEuzlbqxKsCQOvulxaMVYDDzrBkDMIRfPYzQK4cNnolD9dawzl3E4D5jb2zXZRjWbdyJtWvXytQoxm7f2O3p7Ygy9njTYntRwS8RfHghQi9uhl3x+rCKDEbY/SOwK9YQGap9CeuMecx2WpGB/vBeOw8hD+4gZ7/vYJs1R6JjCX10F0/GfoUMhUsj94jp8epqgOHvv/+WmgZ0/HPb8ePHpYYBv2/UN+jQoUN0Lv6tW7eC6XZ+/PFHMJ0OC/UOWrZsKbUPqHeglYQAAwWPCQuYeolpiQwVpiBiyh066RkdQcDQqlUrUP+BK+SpraAV1m3UqJFskxEUiRUChp9++gk8DzrbtcJzZ7TEoEGDpLM85j6tjvYaKNIhMS3Q22+/LUSPM0uwERYWjqoibRML7XZC2NDGxlpGgDDFEwWeswrwkStXLjBd0vp1v0kQ8sUXXZEnT8LfqdQCBop0s43ffvsNe/bsFhBgtgRF06bFF3mmBgPnlICB0IYQhdoSTL3EdFMJAYZQAb7mz58v6h4U3xMrqXGRK1duzVzq9TUW0Pu6rgDDaybAiLsVYDCiMVVTygLKAsoCygKJWsDkgOHyAQTvXWFwTA71O8GudB2D+9LDRgUY0sMspvwcFGBIue3S2pEKMKS1GUv9eBVgSL0NU9WCMQBD6InNCDm5xeA4nD7sBxv3hFcPGzwohRsVYEih4dL7YSIVUdjDs4jyvI6oIC/Y5HtbiDtXF7q11mY986iQYHhvXIqA4/sEMJgBu1z5Ex1P0MVTeDZjJJyr1kWOnqPi1dUAw++//y5z+DOlDx33FCl+9913pahy0aJFYx1HxzN1BqhxwLRJXKlOh7+Hh4dMLcQV/VpJCDAw0kBLj0TdBEOFTng+ONH5Tue1Bhi+++472V/MY/bt2ycd46w/fvz4mLvivTcGYGCju3ZRlDpcpkhiBEN4eAQqVa4c3R+dxmfPnJafGSmxYcN6lH2rHO7euYPr16/LVE8Usk4s2oIHGwMwsB1GVnz77WihN1EEH3/8sYBBs0TapZoyQoFzzjRHMQEDj2GkAwWxc+bMgc6du2CGiLjInDmT0NroATs7O1aR53Ho0EHwO9S7dx8JMpydncTc9VQCz9JCr/9HAYbX2yit1FCAIa3MlOWMM+L5TUQG+Qq1pCjY5CgGK4fMljM4NRJlAWUBi7aAyQHDpf0I3rfSoE0c6nWEXZm6Bvelh40KMKSHWUz5OSjAkHLbpbUjFWBIazOW+vEqwJB6G6aqBQUYUmW+VB2styMqVYNLjweLVe1c2W4lHO8WUcRKdL89m/Fi1Y/IKXQVnN9pkOCwooRWgc/vS+CzZbVIjzQEGes3i1dXAww3b96UKYkIC5g3nymONAdy3IPo8GbaIkYAMLqBMIKr/ZnOaPjw4bGqJwQYvL29ZaolCgivXr061jHaBzq8GcFAPYeYEQyGAAPTDzF1E4WcGZ2QWDEWYKAOAyOfGjZsIKI+vGEt7Fa+/NuxuqZexDkhiO3l9QL79u0VwthlERISIqNBeF41a9aMFYkR6+B/P9DeI0YMl+mcWrduY7B+7969JOigKDMLbffVVz0FAJoKRjBohamdJk+eJG117NhxqX/BFEgJAQYed/XqVQkjuAr+8OEjYr4zxwIM9+7dBTUeqNtAaETQNGvWLJkyi2mzEvoeaWNSr68iXhjlQluFhoYa3SQqgsHoJk2wQQUYEjSN2hHHAlFBfgj9ZwNCL/whVv02RuTzqxA/wLCv0hW2uYSOUYwovTiHqo/KAsoCygLSAgowmO6LoACD6WxtiT0pwGCJs6LPmBRg0MeultyqAgxmnh0FGMw3AQowmM/2ltJzyK0reL5wMmBnj1wDJ8I2e674QxNQJPj6RTxfMg2RgS+Rb8Ji2GT5L7JAO0ADDHRIM4d+UvPmU8i3S5cuEihw9T7Fkenkj6ulkBBg4PeYUQ9HjhzBtWvXQJHkuOXQoUPo2bOn7IMpmhKLYCCwoLPexcVFijQz9VJCxViAQUs7RDHpCAEB8ufPL4SqC8fqlqmUbt26ib927sSJE8dFtMCHEsgQPDDdk6ZVEeugOB/Yj5aaiOmU6IiOWaiH0b17N7Ru3VpEcbSVuxICDIRlTHvEm+SXL/3Qvv1niUYwsDH2z2MYgeHikhGlS5eKBgwhIqKG+hyMdOjWrTukLcS58bvE1EpdunwhwUfMdFYxx67ev7KA3td1BRhM901TgMF0tk6rPUWFBSHi0XkE75+LiGdXALF+IUOVDuL9OUR4XhK/7XawL9sK9hW7wNpe/DZav9LxSavnq8atLKAsoJ8FFGDQz7ZxW1aAIa5F3qzPCjC8OfOtAMObM9famSrAoFnCTK8KMJjJ8KJbvR1R5jsz1XNSLRAlnPM+29fCd9Ny2LsVRfbOg5HBvdh/qx2FEzno2nmh1TAfobevIkf3YXCu/b7B5lMKGJjiaODAgTJP/9mzZ6W4Mx3NTLEUsyQEGFiHq/8JKZjSiNEPMZ3QdMwzddKff/4ptQ5KlCiRKGCg43zo0KEylRK1ITp37hxvLNq4jAUY2N4dke6IqYHc3d2l/oKr6ytNDEYpnDh+DI8fP4ZrDle4OLtg8+Y/UK9efTx79kzqMpQuXTrWOWvji/vKc9u7d69IPbQWBAyVRRomRpmw+Pv7SzsyXRPPW9M9SAgw8BjqXzDi4PffN2DkyFGvBQw8JlgIWFPAmXoZjBLRUiQdPHgAv/76K/r27YeSQrxam0NfX18BHpbI1fj9+w+QWhtsRxXDFtD7uq4Ag2G767FVAQY9rJo+2owKCRAQ4TpCz25B2NVdgt4GiYgFoSUkdGtiAQaxzYp/WfLDvswnsC1QA9YuQtPG6tV1P31YQ52FsoCygDEsoACDMayYtDYUYEiandJrLQUY0uvMxj8vBRji2yS9b1GAwcwzrACD+SZAb0eU+c5M9ZwcC9Dp/GL5DPjtWA9rByc4vl0D9kVKSsgQducq/I/sFr4IW2Rr+yUyfdAWVrav8uXH7SOlgIH9L1u2DP369ZO5/RmJUL260KiIUxIDDGyjSZMmOHr0KMaMGSNS+nwFR0dHuWJ+8uTJmDlzpkzFRAhBh3piEQzsNigoSKYDotN91KhREjIwMoIO9RUrVsDNzU2m8DEmYOA5cKwUev6sw+dyDEeOHMaF8+eEAHQFoXdQONrpTpFn6lPkzJlTpg/iuSa10MG/ZcsWrFmzRogtZxdAo5Bw+gdJLQdGbfB8ixT5Tyvj/v376NXrK5kiqUyZMvG6YdTIlCnfCy2GT/D+++/LMfIYppeiyDMhRtxCuzIVE+e5a9duEi516tRRRkE0b948+jy14whfJk2aiE6dOkugom1Xr/EtoPd1XQGG+DbXa4sCDHpZNm23G+nvhbCTGxFydhMi/Z4AtoQI4pwSAQyMbLDK4AibPOVhX+4Loc8g0iYJpQZVlAWUBZQFNAsowKBZQv9XBRj0t7El96AAgyXPjnHHpgCDce2ZFlpTgMHMs6QAg/kmQG9HlPnOTPWcXAtEidQ1gUd3w3f7OoS/8EBkaLBswsouA+xy50emhs3hXK0+rOwzJNg0U9swvdHDhw/lSvSkpkhig8ePH5caCUxHdOzYsXgOZtZh+4xQoPOaEQ5xUyFReHjIkCEypQ4d7tRzYMol5qLnQ1NnEYmgpRGiU7xbt24y2qFFixZsPl6hU5vREEzhRCjB/hhtQQAwYsQIdOzYEXPnzpVjWbVqldR30BohLGA0wtixY2VUADUSklIIFAhDagjBZP7/ZDRDgQIF5DlwDPzjyn5qMIgu0LhxYzAiI7mF46PuAzUPvL19ZLu5c+eW/Wk20tqkEPeNG9dFeqJichzadu2VaY8ook37aHPCsXMbIUjc9rTj2D+PJSTh640bN1BI6GjETduk1X/w4AEyZcok7a9tU6/xLaD3dV0Bhvg212uLAgx6WTZttxv++AqCNo1DhMe1V4zATvwu2JIgRMSPYOA+G0YDin0SRIgIh8r9YFfiE1FfAYa0/U1I3uj5Wx4qfptVSb4FHMW9qRZVmfyj084RCjCYbq4UYDCdrS2xJz0BQ968eS3xlN/oMTELwaNHj2CquWEWCD7r808V01tAAQbT2zxWjwowxDKHST/o7Ygy6ckks7PrN+/izv3H4qi4F14rlCpeCG758ySzxZRX58X/xOmL8BH572OWrEIAN1vWzHjm8QKVK5RGhjj58mPWNdb7yICXCL55GRFenqLJSFgLrQXH4uVg7Rxf1yBun/w+Xb58Wa68r1q1aoJpheIex8/UXOCDTdeuXcVq+V6GqkiH+5UrV8CUR1wVb2sbP5c00wmdOXNGRijwPVfkV6tWLVbKHTbOSITTp0+jePHiif7YMz0PgQdhAx/OnZ2dUa5cOSHAXF72T0Hru3fvStFkgoeYhaCF9mD/cffFrBfzPUWNV69eJRzzrvIYniM1Fvhgq90kREVFCoHkw8iTJ4+I1OglIzVitqHev9kW0Pu6rgCD6b5fCjCYztZpqafwR5deAQbPG4C9AxxqdEBU2EuEnvtdQIOoWCmSrJwywfHdiUL8eYUAEmcFbBCAoVJv2JX8VNSNnYYwLdngTRsrf/9DQ0PlfQcXGqSksA3C/JiFkaGMWvxYRBvWrVMH1lzEELOCei8tQGeNAgxJ+zJokGL+j6PRvVPrRA8Ku7QfwftWGqzjUK8j7MrUNbgvPWxUgCE9zGLKz0FPwJDyUakj9bSAAgx6Wtey2laAwczzoQCD+SZAb0eU+c7s9T3vOXAcR0+cE07rcAEaHomHNhsJFeyEQ7dxgxrCoR8/FczrW01ZDT7wDR0zA89f+KBwwfxiNfmrh363/Lnlw+Th42cxYVRfZM2SKWUdWPhRdKAzIoF5/zdu3BhP3NnCh2/U4dEBQJBBB4LmUCBQYcom7S8iIlwAhiMioqAI3nuvcbJAjlEHqxqzSAvofV1XgMF0064Ag+lsnZZ6Cn90GcHbfhBqzo5wbDIYNq7uCDmyFMFHFhoEDM6f/CKiDzMi/OY2hN7YAPviLUUEQxuzA4YIfz8EHNmFTO8l7oSMOze8Z+LvIn8jYxb+bjIKMq5+U8w6cd/z/oNpA+O2Fbee9pltJxSVp9Ux9isXS/Tt21cujGCEI6MNjVF4zlxAMWHCeFSqVBlZs2aRUYf58uWXkZOMAtUiURmdmBy7GmN8qg3TW0CDAwsWLJBRxSkZgdaGAgyJW08BhsTtk9736gEYuEpeFcu2gIpgsOz5MdboFGAwliVT2I4CDCk0nBEO09sRZYQh6tZEUHAI+NDm6+ePJas3IVNGF7Rv8wGcHF+FQT95Jlbwi2fXwu5CGNHeTtQNxQsvH7Ei3hmPnjxDZEQkihZ2Ew9f9vLB9PFTTzx99hw5c2RD3tw5pIOYg/f3D5QAwyFDBhR0ywN7ka6HIeqECXZ2tvKY0iUKY+CoaciRPSu6dWwlHpAzyBVkVmJ14Zbt+3Dw6Gl8P2aAjGYIDArG3XuPxMJDaxQplF8CCG+fV5EPWTMzysAKHHt28aDo4GAv+vFGmHBY58n1SjBYN4OmomGu2h80aJCMDqAYc1JX+6eiyzR/aGBggHjYf+VMSfMno07AqBbQ+7quAINRpyvRxhRgSNQ8b+zOSN9niPB+DNv8pUVqJJG2UDiKQ44sMQgYIFIn2bpVg33ZT2GTqywig18A4SGwyVZM2M98a9XDPJ/A+7f5QkPCD7lHzkzWXD579gzjxo0DIwy1wmg/OsRr1qwpNIvelppFSVnpTz2m5cuXi1SB3lpTib4WKVIEI0eOTLSOsXcSgDClIyMYuBDDWIDhyZMnWLx4kbz3evfd9+S9LMEN01F6e3vBw8NT/In7XQF0Bg0ajBw5Un4fybHzt4nzxDSICUUDcIGFoehUzaZsg2kvtUJIQkiU2DHcr0Ug8Fz4OWYbWlvaK8fK/QmNUauXHl81OKAAg/6zqwCD/ja25B70AAyWfL5qbKa1AH/z+PuY1MUTph1d+u9NAQYzz7ECDOabAL0dUeY7s6T37OP7Ej8t+hWZM2XEFx1aICoyCiMn/IirN++JRqJQXECE6ROG4NqNuxg/bYGEDTwmTACG+jUrY1Cvjtj4526sXveneAiLEg85NmjVtBE+bdlYgAgPjJowR6Q+8pcX+Mpvl8Z3w3vhwaOnGDL6B/nw4i+AwZ9rfsTg0dMlmOjbvb1IefNK54A/Cms2bI8GDHwoGj1pLm7L1E6AW95cGD+yDxiNcevOffTp3g7BAoT0+XoS+n/5GeqK8Q3/braIfMiI4QO6Jt0oJqjJh0itbN++Xa6UmjNnDlq3bv1GPtRptlCvygKptYDe13UFGFI7Q0k/XgGGpNvqja6ZGGAQKZGYFsk6U25kqD0Ktnkrm91UYc8ewWfjUgSc2I/MLToia9PPEPrkAaJEdJ59Pncx3vjpD2MOmqkJa9euLVfXN2zYUO7y9/cHV29Sy4eA4euvv0bdunWjF3vEPD7me0OAwU9Ajx07dsj0ibVq1YpZHekFMFCzaufOHTJVZI0aNeDmVlCeJ+8zvbxeCA2lZzL9IuudO/cPPvmkrUwFGcsYr/nAtm7duiX7ePHiuYwY5sKabNmyiTmqIFNXxoyKIPD4669Si9E4AABAAElEQVSdaNiwEfLnzx+vdYKPtWvXol27dtELUajJtWPHdjRt2kymjYx7ELWeDh48CKbtpNDmkyePZYrJOnXqSv2nuPUJrzZsWC/O9xMBVHLG3Z3uPyvAYLopVoDBdLa2xJ4UYLDEWUk/Y1KAwbxzqQCDee0PBRjMNwF6O6LMd2ZJ7zkuYFi08ndcuHwDg3t3QrhInzRxxiJ83Pw9FC9aEN9O/hltBTh4v2FNbNiyC0dPnkenT5uK97vhli8XenT+GMdPncf5y9fFMY3x3dR5yJo5Ewb17iihwuwFv6BZ47qoVrkc+o/4Hg3rvoMGtauiTMmiIoJhKhhVUbViWdgJSMGVdw3rvCPhgRbBQBDi8dwb/Xt+JkHIuGnzUaKoO+rUqIS1G3dgSJ9O2HvwBDZt3Yt6tSqj1UcNMWnmYrRt0RjvibRPllL40Dlp0iSpa0ARYIohv//++xgrBJEp+KuKsoCyQMotoPd1XQGGlM9Nco9UgCG5FntD6ycBMFg5ZhJaDUNhW7iRWY0U4euNF6t+RODpg8jYoBmytOwsAyl8fl+KCJ8XyN5xAGyE9lNiRQMMFStWxKpVq2RVRqRSW4nAYMyYMRIOzJs3T0YyJNYWr5d0onNlu1boFG/Tpo2EGIwYiFm4Uj5TJtOmqzR2BAPvwU6ePIl//vlHgIQn+OCDD+WKfTr4GVHKCIlChQrJ+zFPT09cElpSt4VNevfuLVNQxbRHQu/ZB7Wi/vhjE0qUKCFSXxYH0ywxQuL27Vs4cuQImjdvIfr+IDr10tmzZzFkyGC0aNECnTt3kfVjtn/v3j10794NixYtRsGCr4DI0aNHRUTJCDRq1AiDBw+JNz4CJ85h586dRRqoSiBAmDfvZ5QpU1b031xGU2h9MA3lzJkzZGTM6NHfSg0vbd+b8qoAg+lmWgEG09naEntSgMESZyX9jEkBBvPOpQIM5rW/AgxmtL/ejigznlqSu44LGL7o8y2cnRzRqF51EXUQiT+27UXxIm4CLLyPyTOXYLBw4jMSgY78pb/8gR6d2uDshau4dOWm1G0oVDCfEIkuLEPBew2dhL7dPkWTd2uLvPoRGPzNDwgUoe4jRDTB6Ek/oZ+IMqhW6S35YDtg5FSZrqlsqaIyCsJWAIbmH9TH/sOnoyMYug/4Du+KcfXs8rE8v2lzluPk2UsYNbCbHEu7Nk2wTIypYIG8si2Oc9vuQ5g96WvxQOySZJvoXZHh5+vWrcPu3btlyH/RokXlw1/hwoX17lq1ryyQ7i2g93VdAQbTfYUUYDCdrdN0T4kABivHjEIotQXCb++EQ7V+AjC8a7ZTjRJOfI+fxiLw+F5kbNQcWdt0h03GzCJyIQIhNy8LoepQOJQsJyM+re3sExynBhjeeecdbNq0KVY9OrY3bNiAPn36YPXq1UKn6L1kR0Vev34djRs3RoMGDbBkyZJY7cf9QIc54QahQ0IpmXhN1lLyEBbwj872uPUZtcroCabmcXFxiXa8GwIMrKulE0puKp+nT59Kx3/GjJnw6NFDlCpVCrRp6dJlQGjDVFPaeM+dOydSbb5AmLhvI8D5+ONX959x7RD3Mx35s2bNRIUKFSRI0NIiaePeu3evSE21DBMnTpIwg8efOXNGAgbmqO7R40sweiRmhIMGGBYuXAR3d3fZJUHFuHHfSV2ML774Ah991FRu1/7hXBIwdOlCwFBZ3m8TfKxZ8wsGDhyE4sWLy6oc19atW/Hrr2sEZJj1xi52MTVgQKCvSPf2VJuuWK82WYXWiFPmWNvS0wcFGNLTbCb/XBRgSL7N1BFJt4ACDEm3lR41FWDQw6rJaFNFMCTDWEauqrcjysjD1aW5uIChQ89RyJvLVTjy36EEgyzZs2ZGFqFvEAswHBKAYfUfGPjV58gnUhVdvX4bV66Jvxt3kF98ZlTBd1PnY2DPDni3fnX5UDNI6Cz4+Prj26+/xOjJc9GvR/tYgCF3Tlf07tYWjkKkkMXa2gq//r4jGjB07T8WTd+rI1I5tZT7Z81fjSNCqHrad4Owat3/ZFQDgcO0ceLzb38K/YcA8dDliHEjektoIQ+ykH/oBODDKh+Q+aCtCQlayPDUMJQF0qwF9L6uK8Bguq+GAgyms3Wa7ikxwOCcGS6fbUJUaCAQESJSJcVPPWOKc48KF9pTi6fC/9BOuNR+H9k+7QmbzNnidR108SSezR6NfN8thF1et3j7uSExwMD9XJ1PZ3OPHj3Qvn17jBgxQqTdaSid4zEd1qx74cIFLFy4EK1atUL9+vW5Ca8DDIQAu3btEo7ombhy5YoEIgQCXbp0QadOnaTDXHP6U99hwoQJOH78OFauXIkZM2bIex86timwzEKHP0HJ9OnTpf4Bnd2M5uS1duDAgXJVPjUYCDOGDRuG/fv3i1X8i2S0AdP+sM5nn32WJPFp3nNt3LhRak48ePAAV69eweefd5RRpHHvwziOU6dOIkTAhXz58uN//9uCNq3byOgQOfBE/rl06aKAM0tFxEF3CTDiVmXECOeGc0SQw0LAQBtUq1ZNaj/069dP2CFX9KEJAYYffpgmbbVv335hiwEoWrRYNFSKCxjYGIHNokULpd2/+Wa0tC+jKpims2XLVjK1VnSnb9gbkwOGN8y+MU9XAYaY1njz3ivA8ObNuSnPWAEGU1o7fl8KMMS3iUm3KMBgUnPH6kxvR1Ssziz0Q1zAMH7qAnj7+uGbIT2EMLIrTpy5KCIYCoIizoYAA1MPXbt+B62bvYuSxQth/5FT2LB5lwQLY6bMAyMSRg3qjoePn0ngUF2kRyJwMAQY8uXJKaGDoxCaZokQOg9rNmyLBgyTZiwW4s5WGCkiFrjyrf+IKXDNlgWTv+0vUyT9tmkHyoj+vh36pYxk2CwEojt92gztWr8faxWYhU6FGpaygLKAESyg93VdAQYjTFISm1CAIYmGetOrJQEwWNk5m9VKgScP4NmsUXAqVw3Zuw+DbTbDgsF+O9bj+crZcKneEK49hsM6g2O8cScVMDClT9euXdGkSRO5In/ZsmWxBJLpbJ86dSooaEsooaVoTAww0Mk/efJkrF+/XqTZKSMd81ydz1Xx+/btk6l75s6dG70yftasWZgyZQp69uyJP//8U6blYT93794Vq/cngk5+ikYzXRF1AuhsZ5Qn0wVdvXpViDAvRq5cuaTI86lTp2T7hBkUs2aUAVfdnz59GvPnz5caVvGMFWMDnfrUIyAcyZs3j4QMNkLvokqVKihXrlw8EWcuBGEKIpb8BQrgidC4IAxhWqO4MCJGN/Itz4dAhZEkhqJTCS+6dv1CpKL6WLbHgwgYZs+ehQEDBgpb/U+MMZ8ENpqAc2KAYdas2QLSbJT3uh07dorWaDAEGNgX9To4L5UrV8aHH36I335bKyONmUqJi17e1KIAg+lmXgEG09naEntSgMESZyX9jEkBBvPOpQIM5rW/SpFkRvvr7Ygy46kluWsChvnL1guRZxfpjPcRcIHCyCEipN0hgz38Xvrj677MBesMpiSi/kHFcqVwQICEFWv/h26ftxIA4AyOn76AEgJE3H/0TEKFHp1a49rNu5g6exmcXRyFnkMEMok2fhg/BC+8fDBOCEb36ipE8yqUkdENQ8fMECLPOeU2R4dXIs8EDL9t2onDx89iwqg+8PL2lWCCoRWRkREQmtKYKLYXE/0eOHoaC5ZvQKe2TaXewoEjZzBj7goJSiqLPlRRFlAWeDMsoPd1XQEG032PFGAwna3TdE8WDhiihCPfY84YIeq8D3nHL4JD0dIJmjsyOAhPxvdBpL8fcvQZC4di8e9fEgMMWoqk4cOHy1X+jFxgSkY68X/66ScJG7TOHz58KB3/dODHTIWUGGDYvHmzjBjgynv+UbCYhfn7t23bBvbLFfhMz8RCwPDdd9/JVf/sgxCBD/6EG3Scf/vtt1KHavTo0RI+ODq+Aircz/HlyZNHRkgwgoEOeLbfsmVL6QSnk54RFB07dpRwhP0nVmi3adOmonr1GjLKwtfXF+5CayFS3Gt6eDyTUITaBlr0BX9LGGVhI8bJtEX8/M/ZMzI9EUWatTRKhvokIKE+RkoAw4QJE2Ukx/TpP8j0mfXqvYosSQwwMG2St7e3TLvUoEFDGY1COycEGDhmAhv2wfl69sxD9sXIEu38DZ1Xet+mAIPpZlgBBtPZ2hJ7UoDBEmcl/YxJAQbzzqUCDOa1vwIMZrS/3o4oM55akrsOE0LOt+48gK2dLdzd8oLaB888X+Ci0FSgbkJhoalAB76vnz+u3bgr3rsha5ZM8BRiy3cfPEaxwm5iJZc9zl28JsFBNpFOiRoMTKnEcvP2fdy6+1AIydmhfNniyJYlMwICg4Rmwy0ULVwArM+HxPOXrosw7QwoWqhAdF5ebn/w6KnIf+uDsqWLCvFnWzx64iFTMfEBiNERuUWUBYuv30vcuHUfJYq5I6OLM7x9/HD73kMRfcHPTrKO+kdZQFkg/VtA7+u6Agym+w4pwGA6W6fpniwcMFC8+enUIRA3O8g7cYmIxLRJ1Ny+W3+Fz6YVQvC5H1zqfBCvbkKAgSv0Dxw4gKFDh0qHOIFCAbHyntECrVu3RtOmTWWKIc2Jv337djAND+tRc0ErCQEGts/V/s7OzjKCgWAiZqEWA1MYLV26VK78Z4QDAQNFpxklwWtnzMJrdbFixdCsWTN8//33CaY40jQYCDGYxofOfq2wDQpSEz4wGiKxwuiF/fv3CRHlllLcmILYFSpURE5xHhR7ZsREqZIlo1M8+fv7y9RO1GTI7uoqF8OcOH5Mnlv//v1FZEKRBLtLDWCgLgPBytatf4rIgnXSNpzHxAADhZ8zZ86MnTt3yAiNMWPGImvWrIkCBkKcJUsWg9CoU6fO8juiRUskeGLpfIcCDKabYAUYTGdrS+xJAQZLnJX0MyYFGMw7lwowmNf+CjCY0f56O6LMeGqqa2UBZQFlgTfSAnpf1xVgMN3XSgEG09k6rfcU7nENwX/PQsSjM8hQuT0inp1D5MtHyFDpc9iXE45ta1uznWLYo3t4NnMk7EXkQk6hc/W6EnTpDJ4JIJH1kx7I/GFspzyP1QCDq3B6dxYpbejYf/LkCfbs2YPbt2+jbNmyEhpwZToXYwQEBAgh4HE4dOiQTNtTpEgReQzz/TNd0IoVK+Dm9p/eQ0KAwcPDQwKLr7/+GuPHjzd4GkyD1LZtW9k/NRkIGKZNm4Zr165J4eaYBxGGMBqB+6kZkVDRAANTJ1GwOHduIX77b2HExieffCJTH3F8iRUvLy+RlmkCvvyyp6x2/vx5VK32DmhHFu4/IPQdqlWrCtrIy+sFTon0S0xVxLRMPIcd27cJyFJVQhFuS6ikFjDkz59fRupOmjRZRErYon//ASLK4JnQdOgmNDNiizxTg4GAIXv27DKSZOrUKWRZMjqE35WYIs8xxxsYGCDb9fT0EFEd1dG7d594cxSz/pvwXgEG082yAgyms7Ul9qQAgyXOSvoZkwIM5p1LBRjMa38FGMxof70dUWY8NdW1soCygLLAG2kBva/rCjCY7mulAIPpbJ0eeooSqRPDLm1FVIifiBYQKXgK14JNdq4ytzLr6YV7eeDZtK9hZWePvN/NF8OxTnQ8/ns248Wan5G9Q1+41P8oXl0NMHBFP1e6sxAk0PFO537z5s1jOeG5f8eOHejVqxd+/vlnGa1w//59dOjQQebfp0hyTE2BhACDBi/mzZsntQHYbtzCqIAaNWpg0qRJMj2QBhhu3LgRL0KBAtBMeUTNBkZYJFSMBRjY/rx5P6N06TLSbpcvX0alylWkY17rm1ELe/bsQulSpWVEAEWwcwsbMzXSkydPZXokjpVRHIkVYwAGtk9wRJ0M6j4QevTs+WWigIHHMJJj0KCBQnT7C6n/wAiVLl06o1KlytwtC2HNypUrcOLECQkW1qxZI/Qv6gsx8EYydZVW7017VYDBdDOuAIPpbG2JPSnAYImzkn7GpACDeedSAQbz2l8BBjPaX29HlBlPTXWtLKAsoCzwRlpA7+u6Agym+1opwGA6W6ernqIiX53Oaxz5pjrnKJHG59mMEQg6dwz5pq6Cff5CCXbNuk+/H4xwj4dw7fUtHEu9Ha+uBhjq168vUwYRLlAPgE5vPlQbKgQKBAz58uWTkGH37t1gJMLMmTOlsHLMYxICDNREKFq0qIQHgwYNinlI9PsjR45IgPFq1fx/EQyGAANT81CEesaMGVJHIbqROG+MCRhoB6YeqlOnroz2qCic7kwlFLPQ+c5IBkYwUAyZoseMVuD2OnVqo5SAD68r586dkw78r77qJW0Wtz5TgNLp/+mn7aRQNvdrIs9MkcQIBhampfrrr50iQuMQGjVqJGw/UUYruLu7y/20d8wIBrlR/MN0UL/8sloCpJ07d4q+ukQDBkZ9UJSbgGHgwEEoUaIE/v57jxTp7tatu0FRaq3d9P6qAIPpZlgBBtPZ2hJ7UoDBEmcl/YxJAQbzzqUCDOa1v1EAQ/jVwwi9ctjgmThUbw3r3AnnCTV4UAo3tusyDOs27sTatWvlKqoUNmOyw/R2RJnsRFRHygIGLUA17AhERYbLV/FG6l2wqswBLfJAWzFthHgVWwy2oDYqC6Q1C+h9XVeAwXTfCAUYTGdr1ZO+FvDftxWeCyfDuea7QlthAGwyZo7foXA6v9y/Fc8XT4NT+WrI0ftbWDvFT8OjAYYmTZpIvYP4DcXfQqfyDz/8gMWLF0vnMnURmFKI2ghaiiDtqIQAAx39hQsXRq1atWSqJQcHB+2Q6FdGLIwaNUqKI7/11lvRKZIMAQauzqdOA8HH2LFjE1w5b0zAQDusXLlSpHrKj3Ah7lyxYqV4aYF8fLxx6dJl/Pm/LWAaIcIIOvoJGRgdkhSdAp7v/Pnz0K5dO+nYJwSKWWj7Pn16Y/DgIVJkmfsMAQZup3jzihXLxXx5ijRXB8X4V8lICu5LCDBQX2Hp0iWyzYCAQAwY0D8aMNy5c1uMbb4U3G7VqrWMfqHg9bJlS0FbEzrEjGhhP29KUYDBdDOtAIPpbG2JPSnAYImzkn7GpACDeedSAQbz2t8ogMHMpxDdvQIM0aZQb5QFzGSBKESFBSMiwBNRvo8R4fsI4T73ERnwHJHBvkB4KOkCrByzwMYlB2yzusMqa0HxKsS1nUQeYgkbzDR01a2ygBEsoACDEYxoIU0owGAhE6GGkWoLRAYHwuOn7xB0/gQy1v0AWVp0gm32nNHtRoYEI+DQTnhvXC5+r/2QZ9RsZChWNnp/zDcpAQw8/uTJk2jfvr3UbViyZImMYKBDNa7zOyHAwDYIJlatWiVSDc2LJQzNfYwO4Ep5RlNQi4GO+MRSJHEVP4Wnnz59KturXLlyrLFwP8dmTMDAcdIOx44dQ6nSpYXTvZJwpjvIxRfUIjh+/DhChaZFrlw5hWPfB3fu3EFJIfx848Z19OjxpUybxDZeV5hqadGihRLiECLEhDjPnz+XuhOMiCBYoTgzS0KAgXbgnBMQ/fPPWSn8/LoIBrbHiBOmV7p69QomT54sAQOBCQEFx8foES16g32w/pgx38o0SfyexP1esM30XhRgMN0MK8BgOltbYk8KMFjirKSfMSnAYN65VIDBvPZXgMGM9tfbEWXGU1Ndv4EWiAoLQuij0wh/cgnhz28iwvuOSEMd8lpLWNk5wS5XKdjlLgc793dgk4k5nWOvtnttI6qCsoCFWEDv67qKYDDdRCvAYDpbq570t0CUWAX/ZEI/BF86BVvXPHCqWhd2OfIgMtBfgIdjCL5+Edb2GZB71I9wKFEuwQGlFDBwFf7HH3+MTZs2yXz+27ZtQ7FixeL1kxhg8PHxkREMdD4zDVLdunVlWqZ79+6JvP+DpOOeq+oLFiwo200MMLDCqVOnZBqfAgUKSAd6vXr1pLOfgsqMNOjXrx+yZMmCbt26yRRFqRF51k6UQIMr+GvXqSNX8b944YU9u3fJSAFCDools7x48UI4569KQWymoypXLuE50dqO+UqQQBCzX6RbKlOmNHLkyCGFpHluxYsXl7AmS5b/0jP9888/mD17thCiniiEpfPGbEraZPv27Zg+/QcRmbAs2r4EJQQPixYtioYF2oGEBgcPHpAaF8OGDRfRGhVFZMYlqUPRs+dXUgxcq6u9UpOBkGHOnJ8MpnbS6qXXVwUYTDezCjCYztaW2JMCDJY4K+lnTAowmHcuFWAwr/0VYDCj/fV2RJnx1FTXb5AFoiLCEPrgFEKu70KE1y1EBvkIgct/c1Anww5Wtg6wzVEcDm+1gn1ekfdZRTMkw3qqqqVYQO/rugIMpptpBRhMZ2vVk2ksEBHwEi93rof/0b8R4ectIg5FVKHQTbB2dEaGgsWRpVVnZChcMtHB3L17Fy1btgSd3tQvSE5h5AKdqJ07d5bpkgwdSzFnrmCvWbOmcGhPj1WFTmvqC3DlPYWMKTLNSAU606kbwAgHHseHexY6vunMP3ToEBwdHWO1xQ+8XtNxPm3aNBBscDW/jY0NmLKHotW///67dLYPHix0KUTan3HjxiFnzv8iP5jyiOdDx/j58+fjtW9oQ2BgoEwHdPPmLalJQfBSWkQzMMqA58I/joEw5eTJE8Jxn02CGaZISm5h24wMoPAy+6UNaCc3N7doG2ltMlLj0aOHIv1RIdm/tl175flzbpimimNkodA326Y2hmZzrT5fad9nz54KaOIq++YYvLy85LzxHOMWzi/TOxH4GJqvuPXT22cFGEw3owowmM7WltiTAgyWOCvpZ0wKMJh3LhVgMK/9FWAwo/31dkSZ8dRU12+IBZj6KOjMLwi5e0Q4KoLEWQvdhdQUsSrR2l4IGtbsI6IZqqemJXWssoBZLKD3dV0BBtNNqwIMprO1Xj2d+ucyXnh5x2ve2dkJZUoUQdYsmeLt02uDl7cvLl65iSDhyP2vWKFkMXc8fOyBDPZ2qFyhzH+79HonnLhhnk8Qevc6Iv18YGVvD9s8bgIwFBPvM7y2VzqJuXKdDvHkrqpnVED37t2xY8cOCSgMdRYQECBTBSXWPlf3Hz16VKZFolOaoKF69eryNWabTDFEpzgjEww5tFmXxzO9Es/J09NTOsoZRcD26OjmfsIDOuup2RBTH4D7Tp8+LaMP3n333ZhdJ/ieUILggFAiV67cwpHuIPuMjBQpJkV7jM5gX35+vjKdEmFOvXr1DTrwE+xE7UiTFlCAwXTTpgCD6WxtiT0pwGCJs5J+xqQAg3nnUgEG89pfAQYz2l9vR5QZT011nd4tIISbwzyvI/DofIR73TH+2drYIVO9YbBzqyI1G4zfgXFbpFOAq/K44pDOEWdnZ5lWgfmFDa3qS07vbJvXChbmln4T8xInx17mrqv3dV0BBtPNsAIMprO1Xj39umEbbt19iJcvA3D9zn0UzJcbrtmzyr8P36uNAuKzqcqFyzcw4+dVyJzJBdmzZXmVCFBkA2xUpxpWr9+GLJldMGFUX1MNx+T9hAhtgQ4dOsh8/nSu8/fsTS0ECPzj/QFfGT3AaICgoEDxFyyjDXx9fWRKIYKLYsWKq9/+N+DLogCD6SZZAQbT2doSe1KAwRJnJf2MSQEG886lAgzmtb8CDGa0v96OKDOemuo6HVsgKjQQobcPIvDMqlfCzTqdq7VjVrjUF5AhtwlWdKbiHAgA/v77b5kqgmkY/Pz84OTkhDoiv/KQIUMSXTmZlG4JLJhWgjcrnTp1QsaMGZNymKpjJgvofV1XgMF0E6sAg+lsrVdP/gGBCBOpXa7duIufFq3Fp63eR8133oaNuJ6GhITi8VNPZHJxhnvBfNKB6+3jh2CxPXNGF9wUQIIwIF+eXCIljI10BN+59wj+AUECTOSS0Q/aanPP596yrZyuWZEvby55Oj5+L2UfvHZ7eHghLCIc0+Ysx2etP0D1quWl1BDVhmxEOsAhY2YgqwAM348ZKI997uWD+w+eCOiQEQXd8iIiPAIvRAREpozOcHZyFE7oEDGOQDkGju3xEw+5Et6UERnJnTNCBQKGoUOHyiiG5B7/ptVnpAPBAyMmtJREb5oN3rTzVYDBdDOuAIPpbG2JPSnAYImzkn7GpACDeedSAQbz2l8BBjPaX29HlBlPTXWdTi0QFeKP4CtbEXRho0iJFKjvWYo0Afb5q8C5Zi9YO70SPdS3w5S1ztQJTGHw6NEjCRWYMoI5ovfs2SNFDZcuXZqkNBIEFUw9QaBAiKDlH6YgJNtkegc6aJiyQSusyz+KUNqLNBdxC9sk8OA+rT1/f384OAi9i39zKMc9Rn1OnQX0vq4rwJC6+UnO0QowJMdall330tWbmPrjcnRu1xz1alXGiTMXMWfRr/B84S3AQRQa16+OAT0/w6JVG7H3wAnh1LWDx3MfmbaoX4/24pgqmDh9EQ6d+EfAXis4iVz2owZ2RYVyJbFjz2EsXrkRgWKFfmREJFp+2AA9u3yMlWv/J/cFiOt6Dtfs6Nv9U/wwdyW6dWiJOjUqC6AhbCb+CQsNQ78RUyRgmDS6Pw4cOY35y9bDWwAKlno1Kgkw0gRzF6+VKZQ+afEedv59BDt2H8ag3p/DLX8etOk8GB0+/hAtP2ooj7GUf0JDQyUcp6N8+PDhoFPn4sWLCpRbygSpcViUBRRgMN10KMBgOltbYk8KMFjirKSfMSnAYN65VIDBvPZXgMGM9tfbEWXGU1Ndp0MLUGMh+PKfCLq4CVEhrxwfep+mVYaMcK7SBRmKNxJd0RtjeaV3795YsGABKleujF9//VUKQBIKNGjQQOZlpihkvnz5hKjjMplzmqs3KSb5ww8/gHmkKSL58uVLrFq1Kvo9UyxR5LJx48aYOXMm5s2bJ1fXcvXnwIEDZXsTJ07EkSNHZPqkHDlyoE+fPrK+h4cHRo8eLaECrzEXLlwQAovZ5crRq1evSvDBCAvWoRimKsa1gN7XdQUYjDtfibWmAENi1klb+2IChvJvFceUWUuF0z8rOnzSFFev3xEO/XXCWd8Rp89dxo5dhzB+VB8RvZARC5avh4vQa3i/UU0MHzdbwoF6Natg3ea/4Ohgj6aN62HgN9PQsHZVNPugPg4KOLBszWZ83a8Lbt6+j01b/0b3jq1Rungh+ImIA0YwFCvsBvcCeaQB8+TKgcpvl8HIiXMkYOj3ZQdMnrlY6DIUwqet3xdtPMCseavxcfP3ZEQFdRzGDu8lxr8E1JdgP04ih//4HxZi3IheKFemuMVMDK+F7733HsqWLSu1DU6ePCl/dxiJp1L9Wcw0qYFYkAUUYDDdZCjAYDpbW2JPegAGPgeqYrkW2LJlC/LmzWuSASrAYBIzJ9iJAgwJmsY0O+6fXQjfp6eQv0RjZHYtappOdeqlXZdhWLdxJ9auXYu2bdvq1IvxmtXbEWW8kaqW3ngLUAfg3nG8PDQbUaH+JjVHhkJ14PROd1g7ZjFpv0npjKsyCQEOHDiAWbNmoX///tGHMVKAuZWZ3uCnn37C2LFj8cknn8h0R6dOnUKPHj1k1ANhxPbt2+U1K1euXChZsiT++usveRPEFEtMu0QIwcJczGxn/vz5WL16tYxqIFxgG8xnzXYZqfDRRx/hypUr0ZCB4+B+Ri1wRSlTL9SuXRu82cqcOXP0mNWb1FtA7+u6Agypn6OktqAAQ1ItZfn1YgKG4kULov/w71G2dDGUKOour4cbtuzCZyICgKmJjhz/B6vmT5IpiJas2iQiGbzQVUQdzJr/C+xEOqKK5UuhYIG8qPBWCVy6dgs/L1mHIX06oWypouL6GoaWnw/AR43rCgCRQUZKzJgwRESNZcC5i9cwVQCGvLlzyD9azS1fHhHNUBHfTJorAUPXz1tj3LT56NutHapWekumdxr67QwBJPKikEjjtHbjDsye/LWos0Be/yuWKyXTMB07fQHzp38jrvmWo2vA6DqC8Hv37snfHqYN/Pjjj5Epk+mEtS3/m6lGqCzwnwUUYPjPFnq/U4BBbwtbdvt6AAYuJnv8+LFln/gbPDpmGlCA4c34AijAYOZ5VoDBfBOgtyPKfGemek5vFogMeA6/v8Ygwvu+yU/NJlM+uNQdCNscJUze9+s65M1KixYtpGN/8+bNaNasmcFDpk+fLsEAnStMmRQTMPBmlHCCMKFr164yEoGaDpcvX5ZRDKVLl46VIunhw4fo0qUL7t69i6NHj0ogQegwe/ZsCRamTZuGpk2byugJAgRCi8KFCyN37twy0oJAokqVKihTpgzWrVsHtq+K8Syg93VdAQbjzdXrWlKA4XUWSjv7YwKGYkXcMGDEVFSpVBYlihRElDgNGxtrFBfv9x46GRswrBaAwdMLQ/t2BvUZLl+7Lf5uyeiEejUrS32ExSKt0tA+nVGqRGGEC62EZu37okmj2sjo4oQTZy9ixvjYgKGLSNNUu3pFuYrfSqRbYlolLUVS1w6tMEGkYuor0jJVfru0aC8cI8b9KIFE44Y1MXbKPNQRx95/+ESAiUoCYFzC02eeKCkiJAb16mhxE8LxM00fV/O5uLio1HwWN0NqQJZkAQUYTDcbCjCYztaW2JOegIHPeKpYjgX4bM5nbQUYLGdO9B6JAgx6W/g17SvA8BoD6bhbb0eUjkNXTb9JFoiKRODxpQi6vNk8Z21ljYy1B8K+SF2Zr9o8gzDcq5eXFz788EMcO3ZMggM6/rVy584dmfqIDn5GGxACaICBqSIYwcAbHkYf7Nq1S4IFHkO9BGoufPDBB/j888+lXkJMDQbqMPTr10+CgzNnzsjuNmzYILcx9RHbImBgmiVGc7m7u0u4wFemcipUqJDUbCBYYEontq2K8Syg93VdAQbjzdXrWlKA4XUWSjv7YwKGMqWKYPTEn2Q6oY6fNoWDiDI7ceaC/Lx6/VaDgKFcmWI4fOysSKPUCdmzZRHRDKvBdEVMUdR32GSRaukjNHu/Lk7/c0VGIFC34ZEQXjYEGHqIKIU6NStFpwkKCaEGw/cygqG3iFwYN3UBGtSpitbNGok0ej4YMHKq1FZoIz7z/T0h/sx9TNU04+eVuCxSPE0UKZ2qiYgHVZQFlAXSrgUUYDDd3CnAYDpbW2JPegIGUzqyLdG2ljYmLbLElPOiUiSZ91ugAIN57a80GMxof70dUWY8NdV1OrJAuNcd+G0drr+ocyI2c67YAQ5vtRLLTC0n/QOHy1RDTMe2ceNGtGnTBsuXLwed/EyPRDhAbQVqIxAcjBkzRkIDOv3379+Pnj17goCCgIHaCLt375arK27fvi33sw0eQy2GihUrRos8EyqwTQo1Mw0S0x5Rv2Hw4MESOjBqgYCBQtFr1qyRgIEhoQULFsTChQvlZ6ZFUoAhkS9cKnbpfV1XgCEVk5PMQxVgSKbBLLg6Iw9mCmd8h7ZNUbPa29i9T0Dh1X+IlHIOiIgMF+mMHCQs2CciGI6dOo8lP36HgMAgLP91Mzyfe+OLz1rg28k/y2u+qwAMDx97oG2r99G8ST2s/X07/ti2V6SbcxHXdF+ZeumbwT2wYctfUifh+zEDBMSwx4XLNwQQWCXaao5a77yKYKDJmFZp8OgfkEVoPnz7dU/8uXM/1mzYLq7hziJqwldqNgwWYCNXzuxYuGIDtu85gnHDe4qUTMWkpgN1H9YunQZnJ0cLngE1NGUBZYHXWcDUgCHS8x7C7/xjcFi2hd6GdY6CBvelh40KMKSHWUz5OSjAkHLbpbUjFWBIazOW+vEqwJB6G6aqBRXBkCrzpepgvR1RqRqcOlhZ4F8LBByag+Drf5nVHg7FGwsdhm6wsnUw6zgMdU6HPiEDIw+oscD0QwQITD/EaIFNmzbJVEZff/21hA8UbabGAqMamIuaKY/Gjx8v6zNFEsWh58yZgxUrVkhowGgFCocRZhBOMCKCkRLnz5+XURHcx/o7duzAsGHD0K1btyQDBrb31ltq1auheU3pNr2v6wowpHRmkn+cAgzJt5mlHuH30h/Xb96DmxBXzumaTaYyunX3Ae7cfQRbOxuULlFEpiG6c/chXojIhMoVysg6d+8/RojQrWH6pMCgYJw9fwVBwaHInyeHTIlEwBsUHCKFop94eEpIwGPthebNPXGsl0irVK5scdiIFEG+fi/lGNzd8kmBac1W1MihPoOdaIu6EAQOHOv9R0/gIqBBeaH1QMFpFqZDunPvsdSByCCgxZ17j6RuRBXRpyrKAsoCadsCpgYMYZf2I3jfSoNGc6jXEXZlRORwOi0KMKTTiU3iaSnAkERDpYNqCjCkg0lM5ikowJBMgxm7ugIMxrZo0tvT2xGV9JGomsoChi0QGeQD3y2DEBngabiCibbaF6oFl1r9YGVneSs0o4QANoUsR48eHZ3ygmZheCQd+IxsuHbtGlq2bCkjDridws90KjGSQBN5bteunYx44H7uy5MnjwQHFJEuUaKEjG4oVaoU5s2bh+vXr6Nv375SsNnKygocQ9myZXH48GE8f/5cAQYTfS8NdaP3dV0BBkNW12ebAgz62FW1qiygLKAsoCwQ3wIKMMS3iV5bFGDQy7Jpo10FGNLGPBljlAowGMOKaasNBRjMPF8KMJhvAvR2RJnvzFTP5rRApHA2ewWE4alfMP555AdP/xCRAiIKdkJMMrtzBpTLlwm5M2VAFkc72NtaJzrU4Gs7EXhiiUiPFJRovSTttLaR1aysbREVHiI88LavDouKgPCQJ9qEvbsADLUtEzBoA6cOA/UPnj17JqMMPvvsMymurO0nZNi6dasU7axWrZoUYab4ZceOHcFrAbUVtm3bJnJuv5CaCU2aNJHREAQON2/exB9//CFFMqnN4ObmhkOHDmHPnj2yneLFi6N169ayX6ZlYjSDnVhB27BhQ5kqidES1GSoU6eOjJqg0HTWrFnRqFEjZMuWTRuiejWCBfS+rivAYIRJSmITCjAk0VCqmrKAsoCygLJAqi2gAEOqTZjkBhRgSLKp0mVFBRjS5bQaPCkFGAyaJV1vVIDBzNOrAIP5JkBvR5T5zix5PdPJeu7cOekoNXQkHawUoqVT1dLKqVOnpFBvzHFxvHTacrU5nbjMqX/x4sWYVWK9Z8qbt99+W65qj7UjBR8CQiNw+r4Pdl31wNVn/giNiO+4t7exhls2R1R1y4IqBbPAPZuTWG1vFb83Ie4ccGwRgq9tByIFBEhOEVoJVuIvKjRQggS+h429bME2e2GEPT4Ha2dXKdps7ZgV4R7XxL74Y9W6zFC0AZyr97TICAZtjOpVWYAW0Pu6rgCD6b5nCjCYztaqJ2UBZQFlgTfdAgowmO4boACD6WxtiT0pwGCJs6LPmBRg0MeultyqAgxmnh0FGMw3AXo7osx3Zsnr+dKlSxg6dCjOnj1r8EDmtv/+++/BFDKWVnr06AHepMQsGmDgCvQhQ4bIlecU602ocBX5jBkzkCNHjoSqJGl7oIALOy57YMvFp/AODHvtMdYitU5hV0fUL+aKeuLPJcO/EQX/HhkZ7Av/fdMFDDA8L1oHQaJf36Aw5MrkIFIEia0CJNjlLgPrjLkReu8Y7HKJ3NACukSFBiDy5VPY5i2HSL9niBLCmlbWIorCrSoCji8SECNcazLeq+NbreFYsb2AFq8gRbwKaoOygIVYQO/rugIMpptoBRhMZ2vVk7JAii0gIiB5PyHvIUREZJRYHCHXK4gbEisrEaUpoiWtbMT9jdWrKMoU96MOVBbQ2QIKMOhs4BjNK8AQwxhv4FsFGN6cSVeA4c2Za+1MFWDQLGGmVwUYzGR40a3ejijznVnyer5//z6WLVsmV/oz9/zmzZsRGBiIWrVqoWDBgqCIYqdOnWR6l+S1rH9tivquX79eivBWqFBBdvjo0SOZKocpapiKhnn2f/31V7nvwoULUpyX+3gsC6MXKO5Lwd+UlkiRAunwbS8sPnoPPkEJO+oNtZ9RgIW6RbOjTYW8yOokIg3+LeHe9xBwcDbCn9/QNsV6DQ6LwLqjj7D1zBOQKyzrVRmO9uIBntDAvQZsXYsi+Mr/kKFIfYTc2A2bjHlEqqVgOJRvjUiv+wh7KqI6bDOAEQ1B59cnmibJuXovOJR4T7StHASxJkF9sDgL6H1dV4DBdFOuAIPpbK16UhZIlgUERIgMeSn1oSJ9HiHC5yHC/R4iMtALUUG+4n5CQAZbezBC0iZTHthmLQSrrAVgmzk/rB3EvRbBgyrKAhZmAQUYTDchCjCYztaW2JMCDJY4K/qMSQEGfexqya0qwGDm2VGAwXwToLcjynxnlvKeQ0ND8dZbb+Hhw4dYtWoVWrVqJRsLCAjAxo0b5TZPT0+Zm57RAcWKFZMiuZMnT4aNjQ2qV6+OFStWSLFciuMOGjQIjo6OUgz3t99+k/seP34sj6NTn3UouBseHo6///4bCxYskADA3d1d9t2lSxc4ODjg+PHjmD9/PooUKSLz43NsHM9XX30lAcO4ceMwatQoKezr4+ODfv36ybEOHjwYU6ZMkWPjiXCcEyZMkDDhyZMnKTdUnCMZsTBl1w1cEWmRUlIICJq/lRufVs4HR7tXTvwwj6sIODRHPLjfN9jk3xc90W72CXz1XmFcfuiLpV9VhouDWClo7ySgQVH5YB8VHgob18KIeHEb1k5MhXRdfC4qnALPYZO1oNh+U4CI4gIwbBB9GE6RxKiFjPWHwU5EOqiiLGDpFtD7uq4Ag+m+AQowmM7WqidlgaRZIAoR/p4Ie3ga4c+uIlzcQ0T4PBCHGr5/iNmmtVM2EV35FuzyVYRdwWqwtneOuVu9VxYwuwUUYDDdFCjAYDpbW2JPCjBY4qzoMyYFGPSxqyW3qgCDmWdHAQbzTYDejijznVnKezYEGLiNTvlp06ZJTQMnJyfcu3cPZcuWlZEPBAQ1atQAtRxy5swpgYCHh4eMgmDqJTr1f/zxR4wdO1YK3bIOAQahwpo1a8BURoww6N27twitj5JtEBJQLJcAgemNtm/fLlM0ETawf84dhXf79OkjAcPXX3+N/v37i3B8KynS++2338pIjKlTp2LgwIESPNAqegGGsw98MW7HdVDgOaWFMgxf1XLHe6VyyibCRHoj/2MLJAyI22a40HZo/+MJ3PEIwLhPSmP/5ef4tk0pOGV4BSdsshSAtUsumV7J2slVvM8hQQXFna1tHRAZFigARDZEhfjBNkdxEc1wKcEUSdzvXKufWIFYMO4w1GdlAYuzgN7XdQUYTDflCjCYztaqJ2WB11kgKiwIIdd3IeTOIQkVmHYxKWAhVrviHs06Q2aZqtG5cidxb/LqfidWHfVBWcBMFlCAwXSGV4DBdLa2xJ4UYLDEWdFnTAow6GNXS25VAQYzz44xAEPoic0IObnF4Jk4fdgPNu7lDe4z9sZ2XYZh3cadWLt2Ldq2bWvs5o3ent6OKKMP2AQNGgIMTDnE9EPUYmCUQJkyZeTr6dOnQUf+hx9+KNMnBQUFyVRKdMDNnTsXmzZtkumHdu3ahb59+0qY0LJlS/Ts2RM89uTJk2jTpo1MVVS1alXcuHEDvXr1wscffyyjGBghQZFmRkQQSFADImPGjPjggw9kpET37t3RuXNnCRhKly6NkiVLSgsxwuLEiROy70WLFsmIDM10egGGhYfuYqvQX0htyeEi9C6alYareOWDfMDxxSKtUWC8Zk/f9sYHk4+gTmlX9G1cGOfv+6FHo0KwtzV22gErZCjWCM5Vv4BVBpd44zDXBgKuPXv2xOqeaa+Y0qt8+fIyLVasncn8wDRhhF0EZwRiehVG8xw9elT+32ratKle3bxR7ep9XVeAwXRfJwUYTGdr1ZOyQGIWCPe8hoDD8xDhex9REa/XmEqsLblPpEhiVGXGd8fANluh11ZXFZQFTGEBBRhMYeVXfSjAYDpbW2JPCjBY4qzoMyYFGPSxqyW3qgCDmWdHAQbzTYDejijznVnKezYEGPbu3YsGDRoge/bsMjUSNQ2oZXD58mV88cUXUr+AkIFaDdRDqFevnoxYICAoXLgwjh07htGjR8sURy4uLhIEUFCZ0ILHMyqBPz6lSpUCgcA777yD4OBgfPTRR9i3bx+WL18uIxA+++wzeQzTIxFysGgaDGzP1dUV1JAgEPH39wfFm9ke0y1pRS/A8O3Wqzj3yE/rJlWvn1bMi3aV8yP40hYEnFxmMLJgwIpzWH/kEXoKqJAvuyPyi79G5XKCwtEsjKSIEFEOdgaAAx3noeGRch/rRwj9CB5lLUIouI+fbWys5bYoOxc4V+kMR+ov/Nt2REREdMqpkJAQ6Rxn5Igpy5YtW+KJjhMw5M2bV0InQqzUaGpUrlxZ2mLOnDkSMuh1bvy/NWDAAGTJkgX79+/Xq5s3ql29r+sKMJju66QAg+lsrXpSFjBkgajQQKHh9LdIo7gOkUHehqqkahujLV3qDBJpHYtE32OkqkGdD+bvi5eXF7y9vcH7Hy6C4R/vbVN7HyTvzUTEMNON8n5aFdNbQAEG09lcAQbT2doSe1KAwRJnRZ8xKcCgj10tuVUFGMw8OwowmG8C9HZEme/MUt6zIcBAZ27z5s2RK1cuMFKAKZG0Ur9+fRm9wDRH3P7XX3+hXLlyUkuBOgju7u4SRjx9+hQLFy6UuggUleaDGKMRqNFAnQWuPKdTl3UIHvjg1r59e6mzQAFqRk8QMNStW1duy5YtmxyCBhgIMxglwXL9+nV5LFM27dixQwIPuUP8oxdgGPrHJVwX6YqMUXJlzIC5n7yFCCHQHHCKgCEiVrN+QWGoO+YA/ISYdP8mRRAugECbd/LBzdUJTJ2075In1h97iBtPA1A6f0bM6FhORjYQOhy++hw/br8tAUSlwllQpYiIENl/H183K45ieVyw+uB9/H78MZZ9VQmZnWwx90ggWvQch3dq1sWSJUvA7wLnoEOHDpgxY4ZMQ0VnPtNTmbJQf6N169YSQjFaig/k586dw88//yy/cwRdJUqUkEPSHALUAjEEHegoYLouAjRr61cRIIyG4cP+4sWLUbt2bYOnxu8XQVbu3Lmjj4tZkULpTPXFCIi4zgJqmvD/GiN5mAaMgIERPaqk3gJ6X9cVYEj9HCW1BQUYkmopVU9ZwPgWiAz0RvCFTQi+vkNEUgYZvwO2KCIZ7PNXhtM7PWCTMZc+fRipVd6Xrl69Wt7f8n6Dv+G8b+B9KO8/ed+QGsjw/Plzec9BHTQukIl5r22kU1DNvMYCCjC8xkBG3K0AgxGNmQabUoAhDU5aCoesAEMKDZeGD1OAwcyTpwCD+SZAb0eU+c4s5T0bAgxXrlyRegs1a9aUqY/ouGUEAzUQihcvDq4cpxOWD0NMh8SHI4o1a4Bh586dEgoQLIwYMUKKQlMbgboLVapUwaFDh2T0AR3A8+bNkymXfH19JXCg85f1qMfAFEl0bjP1EleMsWiAIabI87Nnz2SapYMHD8p+W7RoEf3QpxdgMGYEg4OIOpjYtBQKeO4TEQxLRTqC0FgTuuXUE/Rddg42Imjg05r5Ub1YNjStlBdR4vP2M0+xYPdtjG9bBptOPMbcnbexqm9lNCibAysFSJiy+boQhXZH57rumPTHNazYdx9ZnW1xcfq7Qs8hED0WnsGDF4H4a1Rt/HXBA0d83bBu3Tppf4IfpsGaNGkSzp49K1Ne8TPnnqvvU/NgHesEk/BBAwxc/c80XdTlYOQKnb/8zvzxxx8ShlEYnPu54pDA4L333pMpt+j05zkwJRcFxAko+F2kVghBF6NpWJ/AiyCB9fg95/eWETb8HjMVHEvRokWlTZjiaOnSpSDsYhtMf3Tt2jUQhjHNFyNyODaOeeXKlVKPhDomTPeUP39+BRiSMO9JqaL3dV0BhqTMgnHqKMBgHDuqVpQFkmuByIDnCDy5AiF3D8Zb5JDctl5X30roQjmWbwuHMs1gZWv/uupm279161Z07NhRLhjgvQQXF+zevTs6mnf8+PHyXvZ1A+S9Be9xubCBEcG8t2BhVDCjczuL1J+zZs2KTvXIyFzWZ+FiBEP3Wows5cIF3kezPfbBBRDOzs4GF0DIxtQ/8SygAEM8k+i2QQEG3UybJhpWgCFNTJNRBqkAg1HMmKYaUYDBzNOlAIP5JkBvR5T5zizlPRsCDHxoYQQDHfZ0JjNCgQ5SPsDQ2VyrVi35lxhgoAgzncLvvvuu/CNU4MMaoQGdsiNHjsTs2bNRoEABmY7p1KlTckU6nbJcKU8ncFIBAzUYuMKe0RR07H7++efRD2R6AYb5B+9i+5XUazBw5uxFeqIBDQqjCi4h4OhCRAb7xJrQKX9cx+ztN2VUwtCmxdC1gTsc7G2k4HPLH46hRZW8GPRRUQxZeQGn7nhj3YBquPHEHyN/vYR82Rzxx9DqCAwNx8Tfr2LJ3ntoX6sAfuxSHmfv+KDd7BNwtLdGpcJZsf28FwhuOHeMXmCUAh3xdLZzpR5X93MeqctBEXBTFg0wMKLm7bfflg/81PBgFAC/J9OnT5cP19RjYORN9erVpS4Ho1t4PjyHrl27SjDCfXxg53eMkTg8V27jwz8f9H/77Tc8ePBApvmi7gfFyqkxwjpMyfT333/LKAWCMKY84kpGAjACC65IPHPmDKgxQqHymTNnyrFxTIzaIaQjGCGQUBEMxvkG6X1dV4DBOPOUlFYUYEiKlVQdZQHjWiAy0Evce8xH6P3jQsM50riNJ9AaUyVlbPQNbDLlTaCG+TdXqlQJly5dkr/xQ4cOlWmR+LvdqlUrGcnARQe8N+HiBUZ2ctEBAcT3338v71N++OEHuaCA9yc8juDAzc0tOk3iwIED5SId3hv06NFDLtLhogQukrh69ao0AO+RCTK40IdtcxEF78m5EIiLFXjPQx0zLvLR9NMYKVyoUCHzGzANjEABBtNNkgIMprO1JfakAIMlzoo+Y1KAQR+7WnKrCjCYeXYUYDDfBOjtiDLfmaW8ZzpVuUKLTnrqJtSpU0eCBDr8CQD++ecfuYqdK7cYGdCpUyf5mQ9TTGNEByp1FyiSyxVYefLkwXKhoXDgwAEZncDIB4aZM18tIx3o7K1YsaJ8MGJ9Oqw9PDxkKhvuZ/oYRk7QcUvHLp3JhAQ8noUPXtxHZzHHQkcxV5zzQY7bmT6HbdAxzsLwdo6Hx3OVu7HKUeHIn7L7hrBV6lu0E1oIX1R3Q6NsTxF4+GdE+D2ObjRSpEPqsegstpx8jFolXbFEpDLK6vxq9dvIXy/i18MP0bW+O3JkyoCLD33RXEQ2VCiUBSPXXsQfJ55gdudyIuqhAB55BckoiENXXmBhjwpoUTUvHr4IwtT/XYOzvS0oIn3Hx1pCJa7m79atm4RKnHemIeKDMSNUNJFiOtNNWTTAwPHwxoWpjV68eCHFwPm95PeQ0RWMQOB3gt8lwiY+kDPVFiNf+L0gCOB3jqmMtLp0HFSrVg1PnjyR30OeI6MSJk6cKCMSCBn4/4HRDlxNSCDB79ovv/wiv7sEMRwD2yP0oFA0QQgf+AliGNHDVFMNGzaUtqSzgJFACjAY5xuk93VdAQbjzFNSWlGAISlWUnWUBYxngajwUBG5sAzB13YY1H8yXk9xW7KCyzs9kaH0B3F3WMRnRtEy2oD3t/wt5+83C++FuAiHhZGU/O3n/e6UKVMkZKBmGLXGeC+9bds28P6CUIAggNEFhw8flve4vMfgH6MeeV/ChTG8B+cCHi5E4DYu6iFwIIA4cuSIvJfgMbx/YRpGLhBiYQQDP/MeiPdGvO8gvOA9uiqJW0ABhsTtY8y9CjAY05ppry0FGNLenKV0xAowpNRyafc4BRjMPHcKMJhvAvR2RJnvzFLeMx9gbt++DdqGPwjUSWDhQxShA9McacJ2fNBiODYfau7cuSMduXzwYSQDH4Kou8AHnSJFisgHo4cPH8oVVazP47gSiyu5tbz3PIYrsAgIuJ+rw/mnQQMezwcyppPRgAG3sT5FnvmnjZVpktgeQQL7YRssXFHO9RPf5gAAQABJREFUP/ZJp66xisfLEIwWQs9P/UJS3SQBQ3sh8tysQAD8D/6ICK870W36BwsANPcUDlx+ji/qFcTUz9+S+3wDw9Bk0mHcfx6I1kKPoaWIYiiS2wX5RcTChfu+6DDnJELCIrFpyDsoXSAT/jr/DD0XnpVREH8Oq4HCuZ2xWaRUeiCOLyF0GwaL6Id8RcpKgW6uguPDLiNYGK0wfPhw+fDMdEGEQ/v27ZOr/fkd0b4v0QPW6Y0GGAjDGI1AoXCCAD5IM2KAabQYNdC7d28ZCcOx8XvL7xcjHBo3bow+ffrggw8+kKmOuJ/C4nzld4xQhQ/6WmFUAoHCiRMnZFQDIyHc3d2jv5tMX0B7sHC1IMdEIMfIB/6fYHuM9uFKRkZK8P8A/28wyoagg2mUFGDQrJ26V72v6wowpG5+knO0AgzJsZaqqyyQSguIaIXQ2wcRcGKxEHSOHTmZypaTdLhdnnLI9L6Ihvz3fi1JB5mo0sWLF+VCBS2Cl4DAUGHUQkKAgbCBi3HoWPvpp59k5CIXGxBQMIqX96YxUyRxoQ6jKLlIYc2aNfJ+lqmZGEXx3XffyftnAgbe+/LehFGUvJ/gfRkjK3kfxHskOs25MIeQQpXELaAAQ+L2MeZeBRiMac2015YCDGlvzlI6YgUYUmq5tHucAgxmnjsFGMw3AXo7osx3ZqpnU1sgJDwSa089wsbzT1LdtY0ADB0EYGhexAr++2ci3ONydJsvhcBzx7mncfDKc7SulhcLelSEt38oJmy6io3HHiMgJALNq+bBTCHs7OJgCw/fYHT++TRO3fIWos12EjAUyumMLvNOiygFH2TPaI8NA6vh+C0vHLnuheFC7HnB7ruYv+s26jd8FxTY5gp7rv6naDHTWvEBmw53RoDwoZgr47i/WbNmKFasWPRY9XyjAYbBgwfLB21CAUa+8GGcqYg2bNggAQFTPFEMmisB+fDNcyFg4FgJHxgRw6gGwjCeKx/0tQgGAguuHuTDPNvmAztXKDJahlCLD//sl8cw1zFBAvUqCBhoH6ZBIGBgCgQCBkZV8CGfehUEcgQUFKNmXUIwBRiM843R+7quAINx5ikprSjAkBQrqTrKAsaxQIS/p4ia/Amhj84Yp8FktmJlbYssn66AtUOmZB6pf3Xtt5zpIRm1SKc/C6N+tUU5TEPEqNmYgIHpRHnPwAgGOtSmTZsmUyxywY6rq6tcCNGmTRsZVckFEDEBA+8XGI1LaDBnzhzZH++52AbbZJpGAob69evLe48///wT7du3B9tjlAUjLTlO1uUCCC5kUCVxCyjAkLh9jLlXAQZjWjPttaUAQ9qbs5SOWAGGlFou7R6nAIOZ504BBvNNgN6OKPOdmerZHBa49vQlfjxwBw99glPVPSMYOlUrgKZlXAVgmIGQO4dEe69yLzEF07DVF7Dy4H04Cd2FjnXdcE9EHTQokxPXH7/Eor/vwtbGGrVLZoebiF44fP0FejUuguV77+LMXV/kypwBGR1tMeCDogIk3MGlBy9RKKcTSomohZ++qIBM+Upi0MrzWLV5n4w6ITBo0KCBTCPEFXxHjx6VegN84GWqHzrgs2fPLh9qmXbIVEUDDMxDTODBiBZGFTD3MZ31dP7zwZ4P5UyJxBQFhBGnT5+WD/KEIjwH1ieEYEQBtT6Y9mC5SKHFh3JG8zAdFMEEt1EngWmQmJaLuZaZI5nbCA4IDNgn8yQnBBgIH5gGjG0RbDAqhNtoU+Z2VoDBON8eva/rCjAYZ56S0ooCDEmxkqqjLGAcC4Tc+Bv+R+YCEa9S7Rin1eS1kqnJZNjlKZu8g0xQmyCB9xaMdORCBUYiMA0R4QLvJRhFMFak8aTDf8+ePTKlItNI8jef2+rWrSsBA53+jPrkgg3ejzDygBG7XBjB6N+YgIGLFBj5yHsFQg1G3nJhBNMvMkqSkQpsu1GjRvI+hYBBSwHJexe2TwihAEPSvyAKMCTdVqmtqQBDai2Yto9XgCFtz19yRq8AQ3KslT7qKsBg5nlUgMF8E6C3I8p8Z6Z6NocFGMXwvwtPsfHcEwSERqR4CBR5HtigMGoUyobgCxsReHYNosL/S710XQg2j//9Cp76hMDN1REda7uhRonsCBf6DN+tv4Kz93xgLVIMlM6XEb0FXJDCziL90epD94WAs42EC2VEmiQChn2XPFGnlKsQhS4Op1xF4Vjxc2w//QCzRHofPuzyYZb6Ckw9xFX+XE1HoECRY4b2M10VnfV8iDVl2bVrlxxbzD6ZcogP+XyYpsjh+fPn5XumHWCkAVN1EQQwdRLTEzBdwaJFi2R0AgEFU2YRHtARwNRJBAzUXeBDPZ0JdDCMGTNGrgLkykICF6Zdoj3Y34ABA2SEBx0CTEvAVFK0GVcuFi0qgI5YUcjUXd988w2oRULnBDVK2AbTjf3+++8xT0e9T6EF9L6uK8CQwolJwWEKMKTAaOoQZYGUWED83vltG4GwZ5dScrTRjnGu/hUcSlmmDgMjB4YNGybvJejI5/0PFx0wfSQ1wBglwN99LiJo0qQJ+FuhpVZklAGd/rzH4L0JtzMlI++tqAPFyEZGNBAw8Fi2RV2pevXqyfsHpqdkKkjqljH9J+8n+FuXFMBADS1qQqgIhtd/TRVgeL2NjFVDAQZjWTJttqMAQ9qct5SMWgGGlFgtbR+jAIOZ508BBvNNgN6OKPOdmerZXBbwCgjF6pMPsffGC0Qy3CAFxcHWGuM/KoniOV0Q/uIWXu4ah8hAr1gtUXPBS6RGopgzUyFpJVRAjqcigoJplnKKfXaiLRZufy50Ipwz2MpUSdz2Migc1HTIkdkBjvnLw6H8J7DP/RYixEMvtTYoaMgHUjrWqTGQKVMmqRvAY5kSiA+/1NDgQ6+pC6EBHfwxC8fBmxjqdmgaHVwteOvWLQkBKLTMKAemOeJDPAUPuf/u3bvyfdmyZeWDPI/lqkIWRkjwvPmZKxcJBAgieAydBLQDUx1RRJr6E9SroCAjHQ8EGoQHjFBgn4y04Bi54pGREwQ4TKnAFEuEDVyNqErqLaD3dV0BhtTPUVJbUIAhqZZS9ZQFUmeBCL/H8Pn9KxEsGZm6hlJ5tOPb7eBUsX0qW9HncP7eM3KRUY2arhd7qlChAhYvXixfCQsIEyIiImQd3j/w/onbmGKSAIKggftZuJCBCzR+++03eX/Fewfuo1g0F0BQa4GRmJpWGaEEdR64IIG6DEkBDN27d5f6TwowSJMn+o8CDImax6g7FWAwqjnTXGMKMKS5KUvxgBVgSLHp0uyBCjD8n73zAI+iWhvwR0JIaKFJUxBQERVQLIgoiA17QRS5YAf1evVir1exIfbeG4r+FuyCFQELiqLYUECa9F6kI4Ek/PMd3HWTbDaT7PS84wM7O3PmlPcbw+S8c87xOXQIBv8C4HZHlH8to2Q/CSy3OvJf+n6+jLXWNajI1rBmNXm0V3sz2kB/2V//9WOSN2NURbIq85oq1lzHObseKdm7dpfM3O3LTE8CCASdgNs/1xEM3t0BCAbvWFNS+AhsKdgqK9bnyR/WNIlTrSkaV2/aYhqRUzXTTJG4e+Pa0qBmltSpnmVeOkjVwvXjnpC8aR+nSmL/XGbW36KiikhhvlSpmi1bddol885F6hcvgiwYFMC6devMekzffPONecFA12TQdRcaNWoU56PndFSDLqqsI0D1RQZ9OeH44483LxTo6E998UBlgb7EoGtC6VpNuulxvVZHPepISh2ZqVMfjR8/3sgIfVFB/w3SFyFmzJghWpZO3aRTWeqLITo9k46I7Nq1q1k0Whd61jWgOnXqZF5qiFeSnaQEEAxJsbhyEMHgCtbQZIpgCE2o0q4ogiFthKHLAMHgc8gQDP4FwO2OKP9aRsl+E9i0xZouadISefPnRaJTJ5Vn69GuiZx74LZfNvW6wr9WydpPb5UCazSDY5u1mGK1HfaWnA69pWr9VlIls5pjWZMRBPwk4PbPdQSDd9FFMHjH2ouSFi1aJD/99JPpWE1Wno6a22uvvaRu3brJTvt2TN9c13qvXr26SB2081c7jnX0m246qk47eUvbtNNYR9I5sf1pjWL8csYK+cL6M98atVhgTZFYfKtpTYnYpnFN6dSivnRoliuNa2cXefM+nr6wQFa9c7EUrl0YP2R3p0qWjmDMkK1b/tomEqyXIjKq17EeXKw3+HPqSMHq+ZJZr7kUrl9hvcTQVPKXT0+ZdY2O50r19j1TpuEkBNwi4LVgcKsdYcgXwRCGKLlXRwSDe2yDljOCIWgRcb8+CAb3GacsAcGQEo+rJ93uiHK18mQeeAI6RdIs683CDyzRMHnxOrMuQ771xqGulVD496LN1vt9UtWa4z8rs4rkZGXKPs3qyL+7tLC+b5vaKNZI/eU9b9ooa8HnL6Vw3VLzNuDWAuttReuX+L9fC4wlLflZJcMSCNYbhZZEyMiuLVUbt5Wc3Y6SzAa7SJWMzJLpOQKBEBNw++c6gsG7mwPB4B1rL0oaPny46MK3mzdbb7Mn2bSjXue579ChQ5Kz/h3Safb++9//yi+//FKkEioYmjSxXgg491yz7o7Ok69T3ZS26cK8AwcOLO207eMrrOkRX5owX76xRkluSSIWimek0y7uuUOuHLV7I9nL+iz+fGGmR3rrQuuykpIiMa+1ltTYaK0v1aTutmkRq1SrKdVaHmieQ/JXzJDMutaLEVWryVbrGaVw8wbJ3K61FFqCIaN2Y0teLJHsVl1lww9DrWK2TQ+UmHdsv9Zh/5Pslt6u6RQrm08IIBi8uwcQDN6xDmJJCIYgRsWdOiEY3OEa5FwRDD5HB8HgXwDc7ojyr2WUHDQCS9bmybRl62Wp9bnSWqdh05YC86t8lrVWwnbWlEhNrXUQdm1cS5pY6yboAs2lbSoV8v+cJQWr5lq/uC+w3gpcZt4c3Jqv0w8kjJTQLDKyzBuFVWrUk6p1mktGnWaS1Xg30U4BNghElYDbP9cRDN7dOQgG71h7UZKuZaOL0+q6PgsWLJBx48aZaV90ihmd075Fixams17XpgnSNm3aNDnzzDNlwoQJ0qtXLzPCQtuga+noWjw68mLs2LFmGhud3ka3kSNHyrJly8xUNzrHvm465Y22NZ3tL+vZ4cXx82X0tOW25EKsLH2u0GeN0/bZXo5o09DiHTsjkjfve1k/etA/B4rtzV2+UYaNmy9fTFkhO1kjIh7vv00AVcmqLjl79rIEQ77kL50kVbJqWs8ns63njWbWoU1Svc3RkjdrrBEMWxb8KNV2PED+mvSulXtpIqOK1D3lKcmss32xGvAVAt4QQDB4w1lLQTB4xzqIJSEYghgVd+qEYHCHa5BzRTD4HB0Eg38BcLsjyr+WUXLQCejCfrolLhRYkTpvtX6xF+sX+a2b/yoqGDQza97jKtVqbBu9IAm9CRUpiGsgEBICbv9cRzB4dyMgGLxj7XVJuuCtvvmvC9ouXbrULHSv/y5qp/wjjzxi5pLXhW179uwp55xzjmy33Xby+++/y/33329GOOhc+NpBUbt2bZPPKaecYuar/+uvv+S+++6Tjz76SHRfF+C99tprRefK123jxo1m8VwdTaGSQ8//5z//MXPW67z2uniuznmvC+mqNFAp8sILL8QFg9YhlpeeO++88+S3336Tl156yaSJcdQ58fX6/v37G6kSO57u5/jZq+TZb+bKCutFhYps1jsNcvURu0jnVvXjTwWbpn8qG75+NGl2G/IK5K73psmQz+bIq5d0lOtf/U2+HXyYSZtRczup2mBnSyxYIxqqVpet+XmSUaO+NVphkRRssKZEUlFgjbLMrNfCGoE5Uqq1Olg2pRAM+vJDvb4vWyMrqyatCwch4DYBBIPbhP/JH8HwD4vKuIdgqDxRRzBUnljHWopgiJHw6RPB4BN4q1i3O6L8axklQwACEKicBNz+uY5g8O6+QjB4x9rrkpIJhpkzZ8qAAQOMXNDRDJs2bZLFixdLv379zNRJuljuEUccIdWrV5f69esboaBrHug0RY8++qhZRFf//9RFdHXR3JycHJk9e7YZYfDdd98ZGaFTFA0ZMsSMQlA5oXJDpzp69tlnjczQaY7uuOMOs0CvTuWkab744ou4YFBpoGspqAzR+lxzzTWiUyh9+umncuihh8YxuiEYdG2n135YIMN/W1LqGIB4BVLs6MjJQcftJrs3rW1SbZwwVP767e2kV0yev1YufPZnaVovR+4+vZ0MfH2KvDyg47a01jCIrCZ7Wm9KWM/TS3+31nNqaUmGTVK4YaU1oNJa3FmnYKySaY2krC6FG1dI1aZ7Sv7i36yXIZJPkZS986FSq9sVSevBQQh4QQDB4AXlbWUgGLxjHcSSEAxBjIo7dUIwuMM1yLkiGHyODoLBvwC43RHlX8soGQIQgEDlJOD2z3UEg3f3FYLBO9Zel1RcMOhovqFDh8qll15qOvBVGMyfP19uvfVWWblypYwaNUr+/PNPIxiaNWtmOvbbt28vF198sUyfPl1uvvlmOf3002X//feXDRs2mDUTDj74YNFyVCJcccUVRljoqAkVBw899JDsvPPO8vLLL8tzzz1nRjLolE0xwdC8eXM57bTTpE2bNqL5xKZI6t69u+Tm5pppnrTcGTNmiI6e0Lqr0IhtbggGnWLx8a9my8SFa2PFVPhzb2u9pysP21lq51SVdZ/dJZvnjCuRV6G1vsOQz+fKjcMmy82n7i7NG+TI3OV/yX+P2blE2rQPWGtF1ep6uWTvckjaWTmVgd4nP/74oxk5k5hnrVq1ZNdddxW9/3TUS0U3XTRc7+tGjRpJp06ditw/Fc2ztOt0AXIVYq1bt5aDDjqotGSV/jiCwbtbAMHgHesgloRgCGJU3KkTgsEdrkHOFcHgc3QQDP4FwO2OKP9aRskQgAAEKicBt3+uIxi8u68QDN6x9rqk4oJBO3Nvv/12ufPOO6Vly5amw1WnONJO0UWLFhkRoCMVdASDrmXw+OOPm3Q6RZGOSPjf//5nFpBWGaAjIZo2bWo6U+vWrStHHXWU6P+3b775pmgH4oknnihvvfWWmZZJRydccMEFMmfOHFmxYoXcfffdZgSDigsdyaAyIXENBpUS1apVM9Mv6egKrbcuAK3TOiVubgiGOSs3yn1j/pD5q60pEdPc6lbPkn8f1EIO3Km+rP3oetmyZFKJHPOsERO9HvxO5izbIPed2V7enbBIrj1xV2nZ6J91nAosCVFojeYovnC0ZlZQWGj9EXPO8kfW/laz9oOuB6HyouDv66wPqZLbTOoceaO1fsMOph6F1rUqnfSPjhbRNS90pImX29q1a+WWW24pMsWV1kdH0LS07lGVX3ovVnSbMmWK9OjRQw444AB54IEHzDRgFc2rrOtUqN10001GlOn/O2zJCSAYknNx4yiCwQ2q4ckTwRCeWKVbUwRDugTDdz2CweeYIRj8C4DbHVH+tYySIQABCFROAm7/XEcweHdfIRi8Y+11ScUFg8oEHYXw4IMPmjUOtAM3cdNOfB0hoIJBRxZoh37jxo3NCIYnnnjCCAbt8NVO23vuuccssqzCQN8wr1evnjz88MOyatUqIwPOOussIyWqVq1qFmZWwaDrKOhICV3jQcWCiobLLrvMyIREwfDZZ5+ZERba4f3iiy+aqZu0nnqt2yMYZq3YIHePmilL1uUloqnQfoY1TdKxuzeSfp13lA0fX2dNcTSlRD4LLKFxwA1fyD471ZND225npmW6/NjWRhKs3bhF/m/sPPn+j1Uyx1oEus9BzeW8w1tKVStflQevfzNf3v5+sZEPZx28o1nvYcQPi+XmXrtL9WqZcsubv8uMxetl5A1d5MdZf8oPm1rLhdffI3XrNZDBgwfLxIkT5corr5SGDRuaNTQ0ltpBrvH3alPBoOJKO+RvvPFG2XPPPWX9+vVmCi5d40Ml0scffxyvjq7vofeBrheiEiJx07VGdASO3iM6AkJFhd5zKr8OPPBAefLJJ01bE6/RfZUrep2uR6KyTK8rvq1Zs8asLaIjIRJHVKik0bVKVIjpiKDrrrvOTDemI3bYkhNAMCTn4sZRBIMbVMOTJ4IhPLFKt6YIhnQJhu96BIPPMXNCMBQsnCYFC6cmbUnWrgdIlbqNk55z+mCfc6+VN94ZKcOGDZPevXs7nb3j+bndEeV4hckQAhCAAARSEnD75zqCISV+R08iGBzFGajMigsG7UjVt6x1QWZddFnf6NY1GH7++Wez8LOOTJg8ebLpYNbnSxUGxQXDSSedZEYmaCeu5vHLL7/IVVddZaSDSgVduFmnOmrVqpV88803phNYRzKovMjOzpYlS5bEp0i69957zYgI7ZxNFAyJizzrWg96rU7lpKMZdIRFbAv6CAat577N68hFB7eS7K8Hy+aFP8WqHv+89/3pcvd706XjLvVk58Y1ZGDP3aVx3RxRuXDPiOmiIxxusYTB2U/8IH+u3yKP9dvLjG647uVJMvLXpfLAWe2t62rJhc/9LJPmrZWD2tSXt6/qLK+Pm28EQ1bVKjJm4MEyePgM6dL7Kmt0yYVmuqt27dqZaXx0dIp2jOs0RRpTHU2g94dXW6Jg0Pv18MMPNx3+7777rpEfKsH0PtLYqxx75ZVXJC8vz9xLOiWXrs9Rs2ZNIyG043rhwoWms//UU081gkun14oJhscee0xee+01GTRokJnmS0WXCgK9b1W26H3YpUsXM52XioSrr75adKHyjh07yieffGJG0uj/IyredOomLUun7tK1R7bffnszpdOXX36JYCjj5kEwlAHIwdMIBgdhhjArBEMIg1bBKiMYKgguxJchGHwOnhOCwecmxItHMMRRsAMBCEAAAj4QQDD4AN2lIhEMLoENQLbFBYOOJhgzZoxcdNFFMnfuXDOlkUoH7cxVkaDptSNf32BPJRi0U1XfdlcRqPPka6fv1KlTzdRLOh2NrvGgCzLrWg3akT169GgjFrTjWjuJY2sw2BEMI0eONPWdNWuWKUPXa4htbgiGJboGw9hZ8uuidbFi0vpsY3X+X9y1pWw3dahsmvrPm/ixTHV6pC8mLZfddqgtg/+1h3TZbdsohne/Wyh3j5hhpk1q1zxXetz7jezWLFduPmV3efGLufLYyFnyr4Oayf3WtEqzrOmVzn3SWsdgwTq5rdcectHRO8k9w6fLIx/PtORDTWvx6Ooyd1Mds8h2t27djNRRqaDTBumb/RozZRlbyFu/e7UlCga977SjXqWXCg9dXPy+++4z8df1Ny6//HLp0KGD7L777uaeVSGiI2v03tV2qcBSGaAd/7p2h8qHPn36GMGw7777SteuXc1oid12283kq5LguOOOM3JMJYSW++2335p0r7/+uhEcOhJBJcd+++1nRJwuaK73sN7jeu33339vRgPpiIlff/3VrE2iC6brlGJsyQkgGJJzceMogsENquHJE8EQnlilW1MEQ7oEw3c9gsHnmCEY/AuA2x1R/rWMkiEAAQhUTgJu/1xnBIN39xWCwTvWXpekb37regv6lraOBNCpXXSKGRUKL730kminvU4Lo5JAF2bWDlNdqFanLdIOZ/3UqY90pINeoyMTNJ12uuoo2gULFpi8dVoZ7ageOHCgmYJm7NixZjoanUpJp7vRt8G1A1jz01+Cn3nmGfm///s/03Hcq1cvs06Dio3bbrvNSARdFLpFixYGl3ba6nRMKkR0Kp1jjjkmjlE7eXUExfHHH2/eNo+fSGNn4+YCefn7BfLRlKVmuqI0sjKXtqhfXS7q2kpaLBslGyc8XyS7/IKtsvsVn5pRCgOO3ln0T441tdHGvALp99QPsnDlJjn3kBayzJIei1dvkv6HtpTN+YVyyQsTjVQYdWMXab9jHfl04lK56uXfZOnqPPn85q6yhyUiPpu0TJ4aNUsOtoTFC1/Ok327Hm0643VqIWWrI0l0MW3twNd1M3TqK10EWd/w19EnXm2JgkHlUWxxb723dCFxlVLXX3+9fP3116aee+yxh0lz1113mVENOppAp0HS0Qgqvl544QUz4kHvd73X9J5WeaBrS+iCzzpN2LPPPmvu5TfeeMPcg9pWHd2wbNkyMzpBy/7pp5/MCJ7nn3/enDv//PPNGiV6/w8YMEB69uxp7kWdXkr/39AFy3UaMB3xgGBIffcgGFLzcfIsgsFJmuHLC8EQvphVtMYIhoqSC+91CAafY4dg8C8AbndE+dcySoYABCBQOQm4/XMdweDdfYVg8I611yVp574u3qyjFFQixOaW17UNdMFl7VDVY7pYs/5yqp2w2qmrnfna0atvtOuoB+2M1jnqtXNaZYHOda9Tz+goBs1bp6hp3bq11K5d2zRRj2knrV6nHbp16tQxb4Hrp25arl6rb57Xr1/f1EEXctY3zzW9LvKsb6Prpm+Vaxv0s0GDBuYac8L6S+upwkQFh7bBic2quoyetlyGfjdf1uflp53l9nVyzAiG1n/9Iuu/vLdIfiusdR52u2yUbFe7mgzqvYf06tzMnJ+1dIMce+c4yaqaIRce0Uo6tKwruzerLbnWotFPW9JAp06qX6ua/Hj3YfLX5kK5671pMuSzOdKkbrZMuPMwayqlzfLC53OMfPjLEiaX/d/vcvkVVxqBox3nOnJB46hTYKlkOPLII83b+TqyRYWDrjegYilxrYEiFXfwS6JgeOqpp6Rz587m/lKhoGt06Hm912IjZP744w9zb+j9qLFXwaBrNagsUEmikkvvb71f9N5WiaaCQadY0k3vZ13vQWWAXqujafTeU+mi97Wu76DXvvfee/L222/Lq6++auScTt2ka4Pop3LSEQ0XXnihuVdVgmm+uq6IriOhEoIRDKXfJAiG0tk4fQbB4DTRcOWHYAhXvNKpLYIhHXrhvBbB4HPcEAz+BcDtjij/WkbJEIAABConAbd/riMYvLuvEAzesaakcBBYvGaTPPzFLPl96fq0KxwTDG0yF8raD68pkt8yq5w9rhgtdWtkyU2n7iZndWshC1b+Jec//ZP8Nm+NtXizyINn7Smndt7BLOA8xhqVcMNrk2W2teDz9vVy5Me7DpNvp/0pt739u0y2pkfaq0UdGXbZ/vL06NnSsHa29D6wmZx477cyY9kW0/GuUxDpehbjx4830/roNEAqofTNf327X0emNGvWTLQTX6cWKr6IcpHKO/QlUTDotFoqPHQbNWqUWeND66IjFHQkzgcffGA68XUdEF33Q9dFUEmgMkI79HVkga4xou3TdT9Ukpx44olGMKgA0PUTdDovnUZJR2p89dVXZlTNXnvtZUbU6Egf5aD/vqlwUPmgjHQ0xCGHHCKff/65GdmjgkFFQ9++fc3IBR0ppCMZtHyVFwiG1DcHgiE1HyfPIhicpBm+vBAM4YtZRWuMYKgoufBeh2DwOXYIBv8C4HZHlH8to2QIQAAClZOA2z/XEQze3VcIBu9YU1J4CLw3cbEM+9EaUWFNSZTO1txasPlia5Hn3barJn++erpIfl48u0LLIBx885cybdF66dCqjpnOaPzMP+WaE3eVIdYIhJG/LJPa1atKj45NZUv+Vlnz1xbZY4dcefHLubJ0TZ4036667LtTXdm1SW2TfvWGLdLWWq+hp5X+/CN3kRo7HSy7n3G/NZJkqegaAbr2gHbO61v+2qn+zjvvGJlwxhlnmBEq2gGvHevaea9TEXmxJQoG7cTXkSg6ikVHWui0WbpOgo4cOOecc0wHv05lpGm0c19HKugoDB0xowuO60gWXXdBRYDKiosvvth09scWedapvlQM6OgaXcBZp0/Sf2t0dMN1111nRjKoZNF/33Sxc11roTTBoIyUl47E0TUrdISQTv2lo3aYIin1nYNgSM3HybMIBidphi8vBEP4YlbRGiMYKkouvNchGHyOHYLBvwC43RHlX8soGQIQgEDlJOD2z3UEg3f3FYLBO9aUFB4C6zblyyPWKIbv561Oq9KxRZ5b1K8h68bcIZvnflskv3HTVpopj3RthQ7Wegr/7r6TtGhUQ6bMXysPfTRT5q7YKNWsqZK6tGkg/Q5tKZkZVeShD2fKd3/8aRZwvvzYXcSaC8haEHq6LF21Sfp2aSa9u+4s1VvsLzX27y/3PjbETPeji23rIsk6DZauJ6DrLWhnfF5ennmLX6en0nU0tKNep6jyatMRFNrxr+tuxDad2kinyNJ1DXTaIV3UWddW0Omb1q1bZ6bK0pEWKiC0Tbogua7HoWt/6BRfuvizjlJ4+umnjQDQEQW6yPMtt9wi48aNM3JBeQwaNMiMftCFonXaLt102i5d90FHPgwePNhMv/Tkk08aXt99951Zu+H000+XK664wiyYrnkuX75catSoYaYZU1mhI0K03mzJCSAYknNx4yiCwQ2q4ckTwRCeWKVbUwRDugTDdz2CweeYIRj8C4DbHVH+tYySIQABCFROAm7/XEcweHdfIRi8Y01J4SIw709rjv+xs60phjZUuOL7NK9jFnluaK2ZkDdvgqz/bLBIYUE8P2sWJFlhLeKsgqFhbraRCXpSj6/buEVWWusp1MyuKo3qbFuTQs9tsNaGWGaNYGhijY6obi0KrZtOt6TrRzRpWE+yd+omOe1Plszc7c1b/toRr2su6FoZKhR0GiCd0kcX+NZN38LXjnvt0NephLzcdN0DXSdBBUdsU8Gg63noYtM6KkG/60iHH374wYiAli1byo477ijTp083oxlUiOhoBhUMuoaCrh+iAkXXDFHhMHHiRLMOSGwdEp3SSNuu4kLX9dA1FHSdBx0Jocd0pIduM2fONPJAZYTWQ7npouLakaN1U1b6Xadx0rVEVHpoO3SEhS5YzZacAIIhORc3jiIY3KAanjwRDOGJVbo1RTCkSzB81yMYfI4ZgsG/ALjdEeVfyygZAhCAQOUk4PbPdQSDd/cVgsE71pQUPgIqF4Z8O7dC6zFkWB3jR+7WUM47cEfJysyQrQVbZO0n1iLESye7AiKjVkOp0b6XZLU8QDKq13OlDDKFQLoEEAzpErR/PYLBPqsopkQwRDGqyduEYEjOJcpHEQw+RxfB4F8A3O6I8q9llAwBCECgchJw++c6gsG7+wrB4B1rSgofAR0VsHjtJnnlhwXy7exVUqArL9vccnOqynmdW0i31g3iV2xZ8pusH/uQFK7fNiVP/EQaO1WyakhOm6Mku91JkplTVyRj26iGNLLkUgi4RgDB4BraEhkjGEogqVQHEAyVJ9wIhsoT61hLEQwxEj59Ihh8Am8V63ZHlH8to2QIQAAClZOA2z/XEQze3VcIBu9YU1J4CWwpKJTv566Wj6cslfnWWgc6pVF+of7R6Yy2SQcdsZBlrZFQ1fqj0xodvXsjOWnPJmbdhHjLLWNRsHah5E39RDYv+EEKN60RKciXrYVbikydFE9fZKeKkQdVMrNE/2TUbCjVduwk1XY5VDJrNTZrMRRJzhcIBJAAgsG7oCAYvGMdxJIQDEGMijt1QjC4wzXIuSIYfI4OgsG/ALjdEeVfyygZAhCAQOUk4PbPdQSDd/cVgsE71pQUfgI6omHWyg0yy1p8eam1dsLqv7ZIXn6BNad/FaleNVMa1c6WJtZaCnvukCu1LMmQatu6ZZNsWTJJCtcukoI1C6Vw458i+dZaCgWbty3CELvY8gpVMq01GLKqS0atRpJZZwepWm9HyWzYRqowWiFGic+QEEAweBcoBIN3rINYUlAEw4cffij9+vUrgui+++6TM888s8ix//znP/LOO+8UOTZ79mypUaNGkWPpftF1eHr37m2y2XvvvWXEiBHlzlLX9jnyyCPNdXvttZd8+umn5cpj6NChMmnSJHNNvXr15IYbbijX9cUTIxiKE4n+dwSDzzFGMPgXALc7ovxrGSVDAAIQqJwE3P65jmDw7r5CMHjHmpKiSaDQsg5V9D9LBFR8s8ZBWGs0iCUdtm75q0Q2VapVtwRDDUsopJYWJS7kAAQCRsBrwVC4fK7kz/4lKYWqrTpIRsMWSc9F4SCCIQpRrHgbgiIY3nzzTTnttNOKNOScc86RF154ocixxo0by7JlRacPXLt2rdSuXbtIunS/jB49Wrp3726y2WeffeTHH38sd5bff/+9dOrUyVy3xx57yOTJ9tdWUrHQvn37eJmNGjWSpUuXxr9XZAfBUBFq4b4GweBz/BAM/gXA7Y4o/1pGyRCAAAQqJwG3f64jGLy7rxAM3rGmJAhAAAKVnYDXgmHL5C9l0xcvJcWec8hZktW2W9JzUTiIYIhCFCvehiALhmbNmsn8+fPjjZszZ460atUq/j22EzXBsNV6IaFbt27y1VdfxZooCIY4CnbKQQDBUA5YbiRFMLhB1V6ebndE2asFqSAAAQhAwCkCbv9cRzA4Famy80EwlM2IFBCAAAQg4AwBBIMzHO3kgmCwQym6aYIsGJT6woULZfvttzcBSDbKQU8UFwwFBQWi0yb9/vvvsmbNGtltt93Mn1q1apl8kv2loyJ+/fVXadCggRk58MUXX6QcwZCXlyfTp0+XmTNnSsuWLaVt27ZSrVq1IllXdATDyy+/XGJqKARDEbR8sUkAwWATlFvJEAxukS07X7c7osquASkgAAEIQMBJAm7/XEcwOBmt1HkhGFLz4SwEIAABCDhHAMHgHMuyckIwlEUo2ueDKBh0OqEpU6YY8G+//bb07NnT7F9++eXy0EMPSeJ5PZEoGPS6Xr16xa83F/7912OPPSYXXXSRNVXhP3MV6rU6FdO7774bT6qd+VdddZVcc8015ljxKZJUdPTv31/WrVsXv0Z3rr/+ern99tut9ZYyzPGKCIZVq1ZJixYtTN663sSTTz5p8kIwGAz8VU4CCIZyAnM6OYLBaaL283O7I8p+TUgJAQhAAAJOEHD75zqCwYko2csDwWCPE6kgAAEIQCB9AgiG9BnazQHBYJdUNNMFUTCccMIJ8vPPP8uCBQvkyiuvFF3sWTddKFlHGagkeOKJJ+IBiQkGPadpUm1XX3213HPPPSZJYWGhHHDAATJhwoRUl0iiYHjmmWdEfz6Vtumi1C+9tG26tYoIhksvvVQeeeQR0emhdBTFLrvsYopCMJRGnOOpCCAYUtHx4ByCwQPIpRThdkdUKcVyGAIQgAAEXCLg9s91BINLgUuSLYIhCRQOQQACEICAKwQQDK5gTZopgiEplkpzMKiCoU6dOqJTBXXs2FG0o16nOqpbt66Jy//93/8VmUIoJhiOPvpoGTlypEmjHfQqJurVqydDhgyRN954Ix7TqVOnSps2bcyohdjoCD2pi0zrosw6QmH8+PHx9DHBsHLlSrMGRGzkwsknnyxnnHGGjBs3Th544IF4+u+++072339/U+/yLPKsUkXL0k1HVGgeujCzbggGg4G/ykkAwVBOYE4nRzA4TdR+fm53RNmvCSkhAAEIQMAJAm7/XEcwOBEle3kgGOxxIhUEIAABCKRPAMGQPkO7OSAY7JKKZrqgCoZjjjnGjFRQ6tqhr532RxxxhAmCCgJdVyG2qWBYsmSJ7LrrrrFDMmbMGDnssMPMd100uUmTJqLrLOh27bXXyl133SU9evSQ4cOHm2Na3kcffWT28/PzpWvXrnHJEBMMr7/+uujvHrpph7+uD1G1alXzXUVFbJqlAQMGmFEI5RnBoKMpDjroIFOm1lvrv2jRIgSDoctfFSWAYKgoOYeuQzA4BLIC2SR2RM2ZM6cCOXAJBCAAAQgEjYC+eZOVlSWbN292vGoIBseRlpohgqFUNJyAAAQgAAGHCSAYHAaaIjsEQwo4leBUUAXDzTffLPvtt5+JgE4V9PXXX8uNN95oOvbnzp0r1atXj0dHBcPYsWPl+OOPjx/buHFjkTQXXHCBPPvss+b8cccdJx988IFZmDm21oOuz3DxxRfHr09caDkmGO644w654YYbTJratWvLgQceGE8/efJkM6WTHoiNuiiPYHj++efNug56vU711L59ewSDwmBLiwCCIS186V+MYEifYUVziAmGil7PdRCAAAQgEEwCCIZgxqU8tUIwlIcWaSEAAQhAIB0CCIZ06JXvWgRD+XhFLXVQBcNbb70l2dnZBrd27H/55Zdm+iN9uWjo0KGSk5MTD4UKhldffVUuvPBCcywmBOIJrB1dLFnXbtBNF4lWIZC42PNnn30mhx56qDmvf+kIgtiIiVh+uujyU089FU9T2o6OpJg2bVq5pkjad9995aeffhIVFzHRodNCxRZ51rKuu+46U6fDDz+8tKJTHteXvnRUhI682H777VOmdeqkLnitI0j0D5v3BBAM3jMvUqITgmHz98Mlb8KIIvnGvtQ47hLJbJl64ZlY2nQ/+5x7rbzxzkgZNmyY9O7dO93sXL9eBUPLli1dL4cCIAABCEDAewL6MOv0xggGp4mWnh+CoXQ2nIEABCAAAWcJIBic5ZkqNwRDKjrRPxdUwTBixAg5+OCD5auvvpLu3bvLqFGjTDB0pMF5551XQjAUH8GwYcMGqVGjRjyA/fv3Fx0loJsuIq35N2/ePD7qQDvyY4JC09x///1y1VVX6W58kedBgwbJTTfdZI7pKIhzzjnH7Bf/q2bNmqJTLpVnBENsAevieRX/fuutt8brUPxcWd8RDGURit55BIPPMUUw+BwAiocABCAAAQjYJIBgsAnKgWQIBgcgkgUEIAABCNgigGCwhcmRRAgGRzCGNpMgC4aBAwfK7bffXoStdtrvueeeJQTD0qVLpXXr1vG0utjzkUceab4XFBSYN/ZjazDoSIA777xTEheF1gWb33nnnfj1OppBp2bSLTaCIXENBl0vQaWGvqGv24QJE8zURrrfqlUrs/5DeQSDTu80ceJEvTy+6fSysTrrQV24+pprrhFd46EiG4KhItTCfQ2Cwef4IRh8DgDFQwACEIAABGwSQDDYBOVAMgSDAxDJAgIQgAAEbBFAMNjC5EgiBIMjGEObSZAFg66ToKMNErdNmzaZr8WnSNKphbST/sMPPzTndRFmXch5u+22M2svaDtj24wZM2SXXXYxUy2de+65scOm414Xd9bFnnUaptgWEww6Gls7+WPbmWeeKX369BFddFrXjNDFqHV77bXXzGLQ5REMsTwTP1nkOZEG+xUhgGCoCDUHr0EwOAiTrCAAAQhAAAIuEkAwuAi3WNYIhmJA+AoBCEAAAq4RQDC4hrZExgiGEkgq1YEgC4YlS5ZI06ZN4/HQzn8dNZCXl1diBIMKBl1XoV27dvH0yXZ0oWid6ki3/Px86dSpk1n7IFna2LGYYNDvQ4YMMVM0xc4V/9TRFbqWQmZmZrmmSCqej35HMCSjwrHyEEAwlIeWC2kRDC5AJUsIQAACEICACwQQDC5ALSVLBEMpYDgMAQhAAAKOE0AwOI601AwRDKWiqRQngiIY3n77bTn11FMN85NOOknee+89s7/zzjvLrFmzzP7//vc/GTx4cAnBsH79etF1D3SbOXOm9O3b10xZZA4k/PXCCy+UWDdh1apVoiMRYiMfNLmOUtA1GWKjJzp27GhkQSwrretll10WX78hdlzXeXjwwQfNQs167IcffhC9VjcVD8WnQDInUvyVKFi0TvPnz0+RuuxTTJFUNqOopUAw+BxRBIPPAaB4CEAAAhCAgE0CCAaboBxIhmBwACJZQAACEICALQIIBluYHEmEYHAEY2gzCYpgcBqgrsmgUxetWbNG2rRpIyoqqlatWmoxOlpg0qRJZv0EnT6pSpUqpabVEzr64Y8//jBCQ6dh2n333SU3NzflNX6fRDD4HQHvy0cweM+8SIkIhiI4+AIBCEAAAhAILAEEg3ehQTB4x5qSIAABCFR2AggG7+4ABIN3rINYUlQFQxBZ+10nBIPfEfC+fASD98yLlIhgKIKDLxCAAAQgAIHAEkAweBcaBIN3rCkJAhCAQGUngGDw7g5AMHjHOoglIRiCGBV36oRgcIdrkHNFMPgcHQSDzwGgeAhAAAIQgIBNAggGm6AcSIZgcAAiWUAAAhCAgC0CCAZbmBxJhGBwBGNoM0EwhDZ05a44gqHcyEJ/AYLB5xAiGHwOAMVDAAIQgAAEbBJAMNgE5UAyBIMDEMkCAhCAAARsEUAw2MLkSCIEgyMYQ5sJgiG0oSt3xREM5UYW+gsQDD6HEMHgcwAoHgIQgAAEIGCTAILBJigHkiEYHIBIFhCAAAQgYIsAgsEWJkcSIRgcwRjaTBAMoQ1duSuOYCg3stBfgGDwOYQIBp8DQPEQgAAEIAABmwQQDDZBOZAMweAARLKAAAQgAAFbBLwWDAVzJkrezyOT1i1776Mks+VeSc9F4SCCIQpRrHgbEAwVZxe2KxEMYYtY+vVFMKTPMK0cEAxp4eNiCEAAAhCAgGcEEAyeoZYwCIZbbrnFOyCUBAEIQAACrhFYtGiRPPPMM/L000/LBRdcUKFyYpLiqUcGyvlnn1KhPCrDRQiGyhDl0tuIYCidTdTOIBiiFtGy24NgKJuRqykQDK7iJXMIQAACEICAYwQQDI6hLDOjMAiGMhtBAghAAAIQCBUBBIP74UIwuM84yCUgGIIcHWfrhmBwlmcYckMw+BwlBIPPAaB4CEAAAhCAgE0CCAaboBxIFmTBcOuttzrQQrKAAAQgAIGgEWjatCkjGFwOCoLBZcABzx7BEPAAOVg9BIODMEOSFYLB50AhGHwOAMVDAAIQgAAEbBJAMNgE5UCyIAsGB5pHFhCAAAQgEDECTJFkL6AIBnucopoKwRDVyJZsF4KhJJOoH0Ew+BxhBIPPAaB4CEAAAhCAgE0CCAaboBxIhmBwACJZQAACEICAZwQQDPZQIxjscYpqKgRDVCNbsl0IhpJMon4EweBzhBEMPgeA4iEAAQhAAAI2CSAYbIJyIBmCwQGIZAEBCEAAAp4RQDDYQ41gsMcpqqkQDFGNbMl2IRhKMon6EQSDzxFGMPgcAIqHAAQgAAEI2CSAYLAJyoFkCAYHIJIFBCAAAQh4RgDBYA81gsEep6imQjBENbIl24VgKMkk6kcQDD5HGMHgcwAoHgIQgAAEIGCTAILBJigHkiEYHIBIFhCAAAQg4BkBBIM91AgGe5yimgrBENXIlmwXgqEkk6gfQTD4HGEnBMPWdSukcO3KpC3JaLCDVMmplfSc0wf7nHutvPHOSBk2bJj07t3b6ezJDwIQgAAEIOArAQSDd/gRDN6xpiQIQAACEEifAILBHkMEgz1OUU2FYIhqZEu2C8FQkknUjyAYfI6wE4LB5ybEi0cwxFGwA4FyEehxZ3JBGMukT5fq0rtrjdjXEp/pXn/jK2tl0rwtJfKNHWi3Y5bcfnpu7CufEKi0BBAM3oUeweAda0qKDoHJc7fIDa+uTdmgwX1zpW2LrFLTuP1MUdYzTakV4wQEAk4AwWAvQAgGe5yimgrBENXIlmwXgqEkk6gfQTD4HGEEg88BoHgIBIBAur/Mp3s9giEANwFVCAUBBIN3YUIweMeakqJDAMEQnVjSkvARQDDYixmCwR6nqKZCMEQ1siXbhWAoySTqRxAMPkcYweBzACgeAgEgkK4gSPd6BEMAbgKqEAoCCAbvwoRg8I41JUWHAIIhOrGkJeEjgGCwFzMEgz1OUU2FYIhqZEu2C8FQkknUjyAYfI4wgsHnAFA8BAJAIF1BkO71CIYA3ARUIRQEEAzehQnB4B1rSooOAQRDdGJJS8JHAMFgL2YIBnucopoKwRDVyJZsF4KhJJOoH0Ew+BxhBIPPAaB4CASAQLqCIN3rEQwBuAmoQigIIBi8CxOCwTvWlBQdAgiG6MSSloSPAILBXswQDPY4RTUVgiGqkS3ZLgRDSSZRP4Jg8DnCCAafA0DxEAgAgXQFQbrXIxgCcBNQhVAQQDB4FyYEg3esKSk6BBAM0YklLQkfAQSDvZghGOxximoqBENUI1uyXQiGkkyifgTB4HOEEQw+B4DiIRAAAukKgnSvRzAE4CagCqEggGDwLkwIBu9YU1J0CCAYohNLWhI+AggGezFDMNjjFNVUCIaoRrZkuxAMJZlE/QiCwecIIxh8DgDFQyAABNIVBOlej2AIwE1AFUJBAMHgXZgQDN6xpqToEEAwRCeWtCR8BBAM9mKGYLDHKaqpEAxRjWzJdiEYSjKJ+hEEg88RRjD4HACKh4CPBOYszZdJ8/LludEbUtai/Y5Z0s76U9r22tcbSztljpd1/We/bZKlawpLzaNxnQzp372mtG2eJTVzqpSajhMQiDoBBIN3EUYweMeakqJBQJ8pPv81T4b/sCllg45ony0N62SWmsbtZwp9Jjlh/xyeKUqNACfCSgDBYC9yCAZ7nKKaCsEQ1ciWbBeCoSSTqB9BMPgcYQSDzwGgeAj4QOC76Zvl0Q/Wy/q8rT6Unl6R2jEw4Lha0qhuRnoZcTUEQkgAweBd0BAM3rGmpHAT+MySCs9bLyqE8ZnigF2rmWcKXl4I9z1I7bcRQDDYuxMQDPY4RTUVgiGqkS3ZLgRDSSZRP4Jg8DnCCAafA0DxEPCYwGOWWBj9W57HpTpbXK3sKnL9KbWlbYvSR1U4WyK5QSAYBBAM3sUBweAda0oKL4G73l4n462XFsK88UwR5uhR90QCCIZEGqXvIxhKZ1MZziAYKkOUt7URwVB5Yh1rKYIhRsKnTwSDT+ApFgI+EHj9q43y2td/+VCy80Vqh8AD/eoyksF5tOQYYAIIBu+Cg2DwjjUlhZNA1J4pnr6oHtMwhvNWpNZ/EyiPYCiYM1Hyfh6ZlF323kdJZsu9kp6LwkEEQxSiWPE2IBgqzi5sVyIYwhax9OuLYEifYVo5IBjSwsfFEAgNgWWrC+WCJ1eFpr52KqrTJQ06PddOUtJAIBIEEAzehRHB4B1rSgofATuLOYetVTpd0nXW6Eg2CISVQHkEw5bJX8qmL15K2tScQ86SrLbdkp6LwkEEQxSiWPE2IBgqzi5sVyIYwhax9OuLYEifYVo5IBjSwsfFEAgNgSi9aZgI/ZXL6/PGYSIQ9iNNAMHgXXgRDN6xpqTwEXh+1AYZUcZizuFrlch71zcIY7WpMwQMAQSDvRsBwWCPU1RTIRiiGtmS7UIwlGQS9SMIBp8jjGDwOQAUDwGPCNz4ylqZNG+LR6V5V4yuxdDJeuuQDQKVgQCCwbsoIxi8Y01J4SMQ1WeKwX1zWd8pfLcjNf6bAILB3q2AYLDHKaqpEAxRjWzJdiEYSjKJ+hEEg88RRjD4HACKh4ADBBatLJAnP9mQMqfZS/Nlfd7WlGnCeLJxnQxpVCczZdWZRiklHk6GiACCwbtgIRi8Y01JwSIwb3mBPPtp6meK3yL4woJGoaxnipxqVeSGXkyjFKw7ltrECCAYYiRSfyIYUvOJ+lkEQ9Qj/E/7EAz/sKgsewgGnyONYPA5ABQPAQcI/LE4X64cusaBnKKZBVMeRDOulbFVCAbvoo5g8I41JQWLwNT5+XLdyzxTJItKzewq8soV9ZOd4hgEfCeAYLAXAgSDPU5RTYVgiGpkS7YLwVCSSdSPIBh8jjCCwecAUDwEHCDghGBolJsht59eRxrVzXCgRs5k8f6ETTJkdOq3KO2UhGCwQ4k0YSCAYPAuSggG71hTUrAIOCEYdmqUaS2YnBuoZwon1qJCMATrXqU2RQkgGIryKO0bgqE0MpXjOIKhcsRZW4lgqDyxjrUUwRAj4dMngsEn8BQLAQcJOCEYTtwvR/p1r+lgrZzJqsedK9POCMGQNkIyCAgBBIN3gUAweMeakoJFwAnB0KdLdendtUagGrZh01Y5/cE/06oTgiEtfFzsMgEEgz3ACAZ7nKKaCsEQ1ciWbBeCoSSTqB9BMPgcYScEw9Z1K6RwbfJOwIwGO0iVnFqetLLPudfKG++MlGHDhknv3r09KZNCIBAEAk4IhiB2BihbBEMQ7jDqEBQCCAbvIoFg8I41JQWLQFQFg1JO9/fdYP4AAEAASURBVJkCwRCse5XaFCVQLsEwZaxs+vzFohn8/S3n0LMla4+Dk56LwkEEQxSiWPE2IBgqzi5sVyIYwhax9OuLYEifYVo5OCEYNn8/XPImjEhajxrHXSKZLfdKes7pgwgGp4mSX1gIBFUwfDd9s9Sy5itu2yKrwijT7QzQghnBUGH8XBgwAggG7wKCYPCONSUFi0BQBcNnv+aZRZj9fKZAMATrXqU2RQmUSzBM/lI2ffFS0Qz+/pZzyFmS1bZb0nNROIhgiEIUK94GBEPF2YXtSgRD2CKWfn0RDOkzTCsHBENa+LgYAoEgEETBsGx1oVzx/Gq5/pTaCIZA3CVUIgoEEAzeRRHB4B1rSgoWgSAKhjlL8+XGV9b6/kyBYAjWvUptihJAMBTlUdo3BENpZCrHcQRD5YizthLBUHliHWspgiFGwqdPBINP4CkWAg4SCKJguGLIGpm1LF8G981FMDgYa7Kq3AQQDN7FH8HgHWtKChaBoAkGXTthoCUXgvBMgWAI1r1KbYoSQDAU5VHaNwRDaWQqx3EEQ+WIs7YSwVB5Yh1rKYIhRsKnTwSDT+ApFgIOEli0skCe+GRDyhwnzduS8ryTazA8P2qDjPhhkynPbcHQKDdDGtXNTNm220/PTXmekxAICwEEg3eRQjB4x5qSgkVg3vICeebT4DxTPPbBehn9W56B5PczRU5WFbnxtNrBChi1gcDfBBAM9m4FBIM9TlFNhWCIamRLtgvBUJJJ1I8gGHyOMILB5wBQPAQ8IlDWWgZOCQZdd+HOt9fFW+V2Z0C69S4sLJTvv/9e5s2bJ2vXrhX9XrVqValZs6bss88+0rp163hbdGfjxo3y8ccfFzkW+5KTkyP169eXDh06SPXq1WOH5ZtvvpHFixeb75pnq1at4ud0Z+zYsbJ8+XJzrGPHjrLjjjsWOc8XCMQIIBhiJNz/RDC4z5gSwkvAq2cKXXfhkQ/Xx0EF+ZmC54l4mNjxiQCCwR54BIM9TlFNhWCIamRLtgvBUJJJ1I8gGHyOMILB5wBQPAQ8IuBFZ0Bs3YX1eVvjrQpyZ8Dq1avlgw8+MNIgXuFiOyoMevbsKRkZGebMmjVr5PXXXy+WqujXKlWqyOGHHy477bSTOfHmm2/KqlWrzP5+++1nxEXiFcOGDTNyQ4917txZ2rdvn3iafQjECSAY4ihc30EwuI6YAkJMwItniti6C2F4puB5IsQ3c4SqjmCwF0wEgz1OUU2FYIhqZEu2C8FQkknUjyAYfI4wgsHnAFA8BDwi4EVnQGzdhcQmBVUw6JuGL774omzZ8s/UUbm5uVK7dm0jA3SkQmxr0aKFHHXUUeZrccEQEw9bt24V/RPb9PjZZ58tWVlZgmCIUeEzXQIIhnQJ2r8ewWCfFSkrHwG3nykS111IpBvEZwqeJxIjxL6fBBAM9ugjGOxximoqBENUI1uyXQiGkkyifgTB4HOEEQw+B4DiIeARAbc7AxLXXUhsUhA7A7R+3377rfz222+mqioDjjzyyCJTE/3yyy9m6qRYW8455xypVq2aJAoGve68884zSbSDYc6cOTJ69OjYJXLsscdKs2bNEAxxIuykSwDBkC5B+9cjGOyzImXlI+D2M0XiuguJdIP4TMHzRGKE2PeTAILBHn0Egz1OUU3lpmDYd999o4otlO3SKYoXLVokCxculO23396TNmj/QPEXDz0pmEIMAQSDzzcCgsHnAFA8BDwi4GZnQPF1FxKbFMTOAK3fc889Z9Zb0H1dM2H//ffX3SKbTp+k6zLodsghh5gHk9IEQ+zCl19+OT7lUqdOnWSvvfZCMMTg8Jk2AQRD2ghtZ4BgsI2KhJWQgJvPFMXXXUjEG8RnCp4nEiPEvp8EEAz26CMY7HGKaio3BUNUmYW9XQiGsEfQfv0RDPZZuZISweAKVjKFQOAIuNUZkGzdhcTGB7EzQKc/UhEQ2/r162cWdo59T/WZSjDoKIbnn38+Li4OPPBAadeuHYIhFVDOlYsAgqFcuNJKjGBICx8XR5yAW88UydZdSEQZtGcKnicSo8O+3wQQDPYigGCwxymqqdwQDD/++GNUcUWiXU2bNmUEQyQiWXYjEAxlM3I1BYLBVbxkDoHAEHCrMyDZuguJjQ5aZ4DWbd68efLJJ5+YamZmZkr//v3jVV6yZImZ6ih+4O+dnXfeWRo2bFhkiiRdzLljx44mhXYyzJo1Kz56QQ/26tVL6tWrh2D4myEf6RNAMKTP0G4OCAa7pEhXGQm48UxR2roLiXyD9kzB80RidNj3mwCCwV4EEAz2OEU1lRuCIaqsaFf5CTBFUvmZOXkFgsFJmhXIC8FQAWhcAoGAEVi1vlA+/TkvZa1e+/qfRYuTJezTpbr07loj2alSj5W27kLiBW53BrTfMUvaWX9Sbb27Vi9yeurUqTJ27FhzLDs72yzGHEswfvx4+fXXX2Nf45/77bef7LPPPkUEQ/xkkh0VCyoYdGOR5ySAOFQhAgiGCmGr0EUIhgph46IIEFixtlDGTPT+maK0dRcSkfr9TJFVVaRn53+eKXieSIwO+34TKI9gKFw+V/Jn/5K0ylVbdZCMhi2SnovCQQRDFKJY8TYgGCrOjivLJoBgKJuRmykQDG7StZE3gsEGJJJAIOAE/licL1cOXZNWLcsrGFKtu5BYkSPaZ0vDOpmJh+L7x3fMkZo5VeLfk+2U9ZZksmuKH3vv+gZFDi1btkzee+89c0xHIZx//vnx8+kKBs1vp512kq5du5pFoTXjt956S/78809ThkoKlRWJ26uvvirr1683hzp37izt27dPPM0+BOIEEAxxFK7vIBhcR0wBASUwdX6+XPeyt88UqdZdSMTk9zNFzewq8soV9eNV4nkijoKdABAoj2AIQHV9qwKCwTf0gSgYwRCIMES2EggGf0OLYPCXvyAYfA4AxUPAAQJeC4ay1l2w06QT98uRft1rlpnUDcFQUFAgQ4YMiZfdo0cPadSokfm+devW+BoKb7/9tqxevdocTzaCQR8gTjrpJHNexULNmjWlevV/3myMFTB8+HBZunSp+ary4YgjjoidMp9aF62Tboceeqi0bt3a7PMXBIoTQDAUJ+LedwSDe2zJOdgEvBYMZa27YIeW3Zck0n2mKC4YeJ6wEx3SeEUAwWCPNILBHqeopkIwRDWywWgXgsHfOCAY/OWPYPCZP8VDwAkCXguGstZdKKtNOzXKlEGn1ylz9ILmk25ngOZRfASDHkscNVCrVi057bTTiiz0vGjRIvnggw80qdlKEwznnXdeLEmpn6NHjzbrM2gCLatv377xtDpyQesS20444QTRhajYIJCMAIIhGRV3jiEY3OFKrsEn4KVgsLPuQlnE9Jnigf51y0pmzqf7TFFcMGimPE/YQk8iDwggGOxBRjDY4xTVVAiGqEY2GO1CMPgbBwSDv/wRDD7zp3gIOEHAS8FgZ92FVG3SX84Hn54rLRtbExnb2NLtDNAikgkGHVGgIwtiW1ZWlrRt21bq1KljRhtMmzZNdDRDbEtHMMyZM0c+/fTTWFZmtMQuu+wimzZtkkmTJsnmzZvNOX0g0QWndTQEGwSSEUAwJKPizjEEgztcyTX4BLwUDHbWXUhFTJ8pHuxXVxrVzUiVLH4u3WeKZIKB54k4XnZ8JoBgsBcABIM9TlFN9dFHH8lxxx0nxx57rHz44YdRbSbt8oGA9h3o7/P6u3xhYaEPNaBIBIPP9wBTJPkcAIqHgAMEvBIMdtddSNWk/kfUlBOstRfsbul2Bmg5yQSDHh83bpxMnjxZd5Nu+oAQezhIRzBo5iNGjJAlS5YkLSd2sEuXLrLHHnvEvvIJgRIEEAwlkLh2AMHgGloyDjgBrwSD3XUXUuG65Lhactie2amSFDmX7jNFMsGgBfA8UQQzX3wigGCwBx7BYI9TVFONHTtWunXrZtbL0302CDhFQKdWrlevnuTm5sqaNemtZeVUnSpbPggGnyOOYPA5ABQPAQcIeCUYUlV18twtcsOra1MlkU6tq8n1p9ZOmab4yXQ7AzS/0gSDnps1a5Z8/vnn8TUQ9Jhu+mBwzDHHiA6j3bhxo3Ts2FH23ntv87Dw+uuvmzQqIOxMkWQSW3999tln8scffxQZGaHnqlatKocccohZHDqWlk8IJCOAYEhGxZ1jCAZ3uJJr8Al4JRhSkbDzTHF4u2wZcEKtVNmUOJfuM0VpgkEL4nmiBG4OeEwAwWAPOILBHqeoptJR6rvttpvoaPIZM2ZEtZm0ywcCU6ZMMTMitGnTRqZOnepDDSgSweDzPYBg8DkAFA8BBwiEQTDoL+XPXFTP1roLiUjS7QzQvFIJhlhZ69atk8WLF0t2drbssMMORdZjiKVx6lPfaNDRDFpWkyZNJCfH/ogOp+pAPuEkgGDwLm4IBu9YU1KwCIRBMDTKzZAHrXUXauaUb0rBdJ8pUgmGWBR5noiR4NNrAggGe8QRDPY4RTWVTk2rv4PptmXLFld/54sqQ9qVnMC7774rPXv2ZPqt5Hg8OYpg8ARz6YUgGEpnwxkIhIVAGATD4L650rZFVrmRptsZoAXaEQzlrhgXQMAHAggG76AjGLxjTUnBIhAGwfBQvzq213JKpJvuM4UdwZBYHvsQ8JIAgsEebQSDPU5RTtWuXTszTe748eOlU6dOUW4qbfOQwE033SSDBg2Sa6+9Vu666y4PS6aoGAEEQ4yET59OCAafql6i2D7nXitvvDNShg0bJr179y5xngMQqMwEyvqluk+X6tK7a40KI0o1ncGJ++VIv+41K5S32/WuUKW4CAI+EUAweAceweAda0oKHwG3/21O9UyRzvOK2/UOXySpcZQIIBjsRRPBYI9TlFOdf/758txzz8k999wjV199dZSbSts8JHDYYYeZqZd1JEOPHj08LJmiYgQQDDESPn0iGHwCT7EQ8JiA279Ul9YZsFOjTHnAmsagopvb9a5ovbgOAn4QQDB4Rx3B4B1rSgofAbf/bS7tmaLdjlly++m5FQbmdr0rXDEuhIADBBAM9iAiGOxxinIqfSG1T58+oh3CY8aMiXJTaZtHBP78809p0KCBKW3VqlVSt27F+z88qnIki0Ew+BxWBIPPAaB4CHhEwO1fqpN1BuhUAg/2qyuN6mZUuJVu17vCFeNCCPhAAMHgHXQEg3esKSl8BNz+t5lnivDdE9TYfwIIBnsxQDDY4xTlVLpWTm7uNlk9e/ZsadmyZZSbS9s8IPDkk0/KRRddxPoLHrBOVQSCIRUdD84hGDyATBEQCAABPzoDLjmulhy257ZFtCqKwO16V7ReXAcBPwggGLyjjmDwjjUlhY+A2/82JxMM159SWzrtWi0tWG7XO63KcTEE0iSAYLAHEMFgj1PUU51xxhnyyiuvyMCBA+W2226LenNpn8sEDjjgAPnuu+/kpZdekjPPPNPl0si+NAIIhtLIeHQcweARaIqBgM8E3P6lunhnQKfW1eT6U2un3Wq36512BckAAh4SQDB4BxvB4B1rSgofAbf/bS7+TJHOWk6JdN2ud2JZ7EPAawIIBnvEEQz2OEU91WeffSaHH364mdZm4cKFkp2d3ktxUedF+0on8P7778uJJ54o22+/vei9xOYfAQSDf+xNyQgGnwNA8RDwiIDbv1QndgY0ys2QB611F2rmVEm7dW7XO+0KkgEEPCSAYPAONoLBO9aUFD4Cbv/bnPhMoWs5DTq9Ds8U4btNqLHHBBAM9oAjGOxxqgypjjzySBk1ahSjGCpDsF1sY5cuXWTcuHFy9913yzXXXONiSWRdFgEEQ1mEXD6PYHAZMNlDICAEvOwMeKhfHWnZuKojLXe73o5Ukkwg4BEBBINHoK1iEAzesaak8BFw+9/mRMHAM0X47g9q7A8BBIM97ggGe5wqQ6rYKAZt66RJk6Rt27aVodm00UECjz32mAwYMEBatWols2bNcjBnsqoIAQRDRag5eA2CwUGYZAUBnwisWl8on/6cl7L0177emPJ8ny7VpXfXGinTpDoZ6wxIN5/iZZTVidF+xyxpZ/1JtfXuWj3Vac5BIDQEEAzehQrB4B1rSgoWgRVrC2XMxGA8U/Q/oqac0DHHMUDpPlNkWe9O9OzMM4VjASEjRwkgGOzhRDDY41RZUl1wwQXy7LPPyhFHHGFGM1SWdtPO9AlMmTJF9t57b9m8ebO8+uqr0qdPn/QzJYe0CCAY0sKX/sUIhvQZkgME/Cbwx+J8uXLomrSqka4YUMHw2td/ye2n56ZVj+IXl9UZUDx9su/vXd8g2WGOQSB0BBAM3oUMweAda0oKFoGp8/Plupd5pkgWlZrZVeSVK+onO8UxCPhOAMFgLwQIBnucKkuqtWvXSocOHWT27Nly9dVXyz333FNZmk470yRw4IEHyrfffivnnHOOvPDCC2nmxuVOEEAwOEExjTwQDGnA41IIBIRAEATDnKX5UiM7QxrVzXCUCoLBUZxkFnICCAbvAohg8I41JQWLQBAEgz5TNKyT6ci6C4l0032mQDAk0mQ/aAQQDPYigmCwx6kypdJ1GHQ9Bt0efPBBueyyyypT82lrBQj07t1b3njjDdlzzz1l/PjxUr06oxsrgNHxSxAMjiMtX4YIhvLxIjUEgkggCILBLS7pdgZovRjB4FZ0yNdrAggG74gjGLxjTUnBIhAEweAWkXSfKRAMbkWGfJ0ggGCwRxHBYI9TZUv19NNPy4UXXmia/fDDD8sll1xS2RDQXpsEzjzzTHn55Zelfv368vnnnxvJYPNSkrlMAMHgMuCyskcwlEWI8xAIPgEEQ+oYIRhS8+FseAggGLyLFYLBO9aUFCwCCIbS44FgKJ0NZ/wngGCwFwMEgz1OlTHVfffdZ6ZJ0rbfeOONMmjQoMqIgTaXQmDVqlWicuHDDz+UWrVqmc+DDz64lNQc9oMAgsEP6gllIhgSYLALgZASQDCkDhyCITUfzoaHAILBu1ghGLxjTUnBIoBgKD0eCIbS2XDGfwLlEQxbJn8pm754KWmlcw45S7Ladkt6LgoHEQxRiKJ7bXjsscdkwIABpoAePXrIo48+Ks2aNXOvQHIOBYGRI0fKxRdfLH/88Yc0b95cXn/9dencuXMo6l6ZKolg8DnaCAafA0DxEHCAgBOCoVPranL9qbUdqI1zWSxbXSgXPLkq7QwRDGkjJIOAEEAweBcIBIN3rCkpWAScEAxBfKbQdR0uez69xasRDMG6V6lNUQIIhqI8SvuGYCiNDMdjBEaMGCH9+/eXFStWSG5urtx+++1x6RBLw2flIKD3wC233CKPP/64afDhhx8uQ4YMkRYtWlQOACFrJYLB54AhGHwOAMVDwAECTggGrcYBu1aTVo2qOlCj9LPYuKlQxs/YLEvXFKadGYIhbYRkEBACCAbvAoFg8I41JQWLgBOCQVsUxWcKBEOw7lVqU5QAgqEoj9K+IRhKI8PxRAILFiyQyy+/XN566y1zuF27dkYyXHDBBYnJ2I8ogXnz5slTTz0luh7Hxo0bTStvvvlmIxsi2uRINAvB4HMYEQw+B4DiIeAAAacEgwNVCWQWCIZAhoVKVYAAgqEC0Cp4CYKhguC4LPQEnBIMoQeRpAEIhiRQOBQYAggGe6FAMNjjRKptBHQqHB3BMGnSJHOgXr160qtXLznuuOPksMMOM3PxwyoaBObOnSujR4+W4cOHy/vvvx9v1MknnywDBw6UvffeO36MnWASQDD4HBcEg88BoHgIeETgiiGrZdayAo9K866Y/kfUlBM65nhXICVBwEcCCAbv4CMYvGNNSeEjcMHjq2TZ2vRHGAat5defUls6WaM52SAQRgIIBntRQzDY40SqogReeuklefbZZ+Xrr78uckJHNrRp08as06DyoWrVYMwGUKSSfElKYNOmTWYaLBULU6ZMEf1M3Pr27Sv6c5WFnBOpBHsfweBzfBAMPgeA4iHgEYHnR22QET9s8qg074p5qF8dadmYBznviFOSnwQQDN7RRzB4x5qSwkfg0ffXy5hJeeGreBk1fuY/9aRR3YwyUnEaAsEkgGCwFxcEgz1OpEpOYOLEieYN91GjRpWQDcmv4GhYCKgg6tatmxx99NGioxYaNWoUlqpTz78JIBh8vhUQDD4HgOIh4BGBz37Nk0c+XO9Rad4Uw1QF3nCmlOAQQDB4FwsEg3esKSl8BN6fsEmGjN4QvoqnqHGj3Ax55uJ6KVJwCgLBJoBgsBcfBIM9TqQqm8DmzZvl119/lZkzZ8rixYtlzZo1kp+fX/aFpAgEgezsbKlfv740b97cjELRkShs4SaAYPA5fggGnwNA8RDwkMCNr6yVSfO2eFiiu0UxlYG7fMk9eAQQDN7FBMHgHWtKCieBqE29OLhvrrRtkRXOYFBrCFgEEAz2bgMEgz1OpIIABCAQNgIIBp8j5oRgKFg4TQoWTk3akqxdD5AqdRsnPef0wT7nXitvvDNShg0bJr1793Y6e/KDQOgJLFtdKJc/v1o25G0NfVsOb5ctA06oFfp20AAIlIcAgqE8tNJLi2BIjx9XR59AlJ4pTtwvR/p1rxn9oNHCSBNAMNgLL4LBHidSQQACEAgbAQSDzxFzQjBs/n645E0YkbQlNY67RDJb7pX0nNMHEQxOEyW/KBLQDgGdKinMIxlY2DmKdyZtskMAwWCHkjNpEAzOcCSXaBMI+zOFTrX4r6415ISOOdEOFK2rFAQQDPbCjGCwx4lUEIAABMJGAMHgc8QQDD4HgOIh4BMBXZNhztJ8IxpmLSvwqRb2itUOgFbWQs7td6wqh7bPYQFGe9hIFUECCAbvgopg8I41JYWfQNieKdrtmCU7Nc7kmSL8tx4tSCCAYEiAkWIXwZACDqcgAAEIhJgAgsHn4CEYfA4AxUMgAAR63LkyZS36dKkuva03/Erb0r2+rLUhtCPg9tNzSyue4xCoNAQQDN6FGsHgHWtKig6ByXO3yA2vrk3ZoLLWOnD7maKsZ5qUleckBAJMAMFgLzgIBnucSAUBCEAgbAQQDD5HDMHgcwAoHgIBIJDuL/PpXo9gCMBNQBVCQQDB4F2YEAzesaak6BBAMEQnlrQkfAQQDPZihmCwx4lUEIAABMJGAMHgc8QQDD4HgOIhEAAC6QqCdK9HMATgJqAKoSCAYPAuTAgG71hTUnQIIBiiE0taEj4CCAZ7MUMw2ONEKghAAAJhI4Bg8DliCAafA0DxEAgAgXQFQbrXIxgCcBNQhVAQQDB4FyYEg3esKSk6BBAM0YklLQkfAQSDvZghGOxxIhUEIACBsBFAMPgcMQSDzwGgeAgEgEC6giDd6xEMAbgJqEIoCCAYvAsTgsE71pQUHQIIhujEkpaEjwCCwV7MEAz2OJEKAhCAQNgIIBh8jhiCwecAUDwEAkAgXUGQ7vUIhgDcBFQhFAQQDN6FCcHgHWtKig4BBEN0YklLwkcAwWAvZggGe5xIBQEIQCBsBBAMPkcMweBzACgeAgEgkK4gSPd6BEMAbgKqEAoCCAbvwoRg8I41JUWHAIIhOrGkJeEjgGCwFzMEgz1OpIIABCAQNgIIBp8jhmDwOQAUD4EAEEhXEKR7PYIhADcBVQgFAQSDd2FCMHjHmpKiQwDBEJ1Y0pLwEUAw2IsZgsEeJ1JBAAIQCBsBBIPPEUMw+BwAiodAAAj8sTg/ZS3q186QerUySk2T7vWLVhbIX5u3lpp/9WpVZPsGmaWe5wQEKgsBBIN3kUYweMeakqJDQP8t13/TU23677n+u17a5vYzRVnPNKXVi+MQCDoBBIO9CCEY7HEiFQQgAIGwEUAw+BwxBIPPAaB4CEAAAhCAgE0CCAaboBxIhmBwACJZQAACEICAZwTKIxgKl8+V/Nm/JK1b1VYdJKNhi6TnonAQwRCFKNIGCEAAAiUJIBhKMvH0CILBU9wUBgEIQAACEKgwAQRDhdGV+0IEQ7mRcQEEIAABCPhIoDyCwcdq+l40gsH3EFABCEAAAq4QQDC4gtV+pggG+6xICQEIQAACEPCTAILBO/oIBu9YUxIEIAABCKRPAMFgjyGCwR4nUkEAAhAIGwEEg88RQzD4HACKhwAEIAABCNgkgGCwCcqBZAgGByCSBQQgAAEIeEYAwWAPNYLBHidSQQACEAgbAQSDzxFDMPgcAIqHAAQgAAEI2CSAYLAJyoFkCAYHIJIFBCAAAQh4RgDBYA81gsEeJ1JBAAIQCBsBBIPPEUMw+BwAiocABCAAAQjYJIBgsAnKgWQIBgcgkgUEIAABCHhGAMFgDzWCwR4nUkEAAhAIGwEEg88Ri6Jg2HfffX2mSvEQgAAEIAABdwj8+OOP8trQu+W0k49ypwByNQRigoFnCm4ICEAAAhAICwF9RnjqkYFy/tmnhKXKntcTweA5cgqEAAQg4AkBBIMnmEsvJIqCofTWcgYCEIAABCAQfgIIBvdjGBMM7pdECRCAAAQgAAHnCCAYUrNEMKTmw1kIQAACYSWAYPA5clESDD/98rvPNCkeAhCAAAQg4A2BfTrs7k1BlbQUnikqaeBpNgQgAIEIEOAZofQgIhhKZ8MZCEAAAmEmgGDwOXpOCAafm0DxEIAABCAAAQhAAAIQgAAEIAABCEAgJQEEQ0o8nIQABCAQWgIIBp9Dh2DwOQAUDwEIQAACEIAABCAAAQhAAAIQgIDrBBAMriOmAAhAAAK+EEAw+IL9n0IRDP+wYA8CEIAABCAAAQhAAAIQgAAEIACBaBJAMEQzrrQKAhCAAILB53sAweBzACgeAhCAAAQgAAEIQAACEIAABCAAAdcJIBhcR0wBEIAABHwhgGDwBfs/hSIY/mHBHgQgAAEIQAACEIAABCAAAQhAAALRJIBgiGZcaRUEIAABBIPP9wCCwecAUDwEIAABCEAAAhCAAAQgAAEIQAACrhNAMLiOmAIgAAEI+EIAweAL9n8KRTD8w4I9CEAAAhCAAAQgAAEIQAACEIAABKJJAMEQzbjSKghAAAIIBp/vAQSDzwGgeAhAAAIQgAAEIAABCEAAAhCAAARcJ4BgcB0xBUAAAhDwhQCCwRfs/xSKYPiHBXsQgAAEIAABCEAAAhCAAAQgAAEIRJMAgiGacaVVEIAABBAMPt8DCAafA0DxEIAABCAAAQhAAAIQgAAEIAABCLhOAMHgOmIKgAAEIOALAQSDL9j/KRTB8A8L9iAAAQhAAAIQgAAEIAABCEAAAhCIJgEEQzTjSqsgAAEIIBh8vgcQDD4HgOIhAAEIQAACEIAABCAAAQhAAAJeEdi4RgpWLUlaWma9JiI16iQ9F4WDCIYoRJE2QAACEChJAMFQkomnRxAMnuKmMAhAAAIQgAAEIAABCEAAAhCAgG8Etkz+UjZ98VLS8nMOOUuy2nZLei4KBxEMUYgibYAABCBQkgCCoSQTT48gGDzFTWEQgAAEIAABCEAAAhCAAAQgAAHfCCAYZkrzDhdI3ab7+RYDCoYABCAAAWcJIBic5Vnu3BAM5UbGBRCAAAQgAAEIQAACEIAABCAAgVASQDAgGEJ541JpCEAAAikIIBhSwPHiFILBC8qUAQEIQAACEIAABCAAAQhAAAIQ8J8AggHB4P9dSA0gAAEIOEsAweAsz3LnhmAoNzIugAAEIAABCEAAAhCAAAQgAAEIhJIAggHBEMobl0pDAAIQSEEAwZACjhenEAxeUKYMCEAAAhCAAAQgAAEIQAACEICA/wQQDAgG/+9CagABCEDAWQIIBmd5ljs3BEO5kXEBBCAAAQhAAAIQgAAEIAABCEAglAQQDAiGUN64VBoCEIBACgIIhhRwvDiFYPCCMmVAAAIQgAAEIAABCEAAAhCAAAT8J4BgQDD4fxdSAwhAAALOEkAwOMuz3Lk5IRjyp46Tzb+PS1p2TudTJKPJzknPcRACEIAABCAAAQhAAAIQgAAEIAAB7whsmTJWNn3+YtICcw49W7L2ODjpuSgcXDBtpKxZgWCIQixpAwQgAIFEAgiGRBo+7DshGDZ/P1zyJoxIWvsax10imS33SnqOgxCAAAQgAAEIQAACEIAABCAAAQh4R4ARDAgG7+42SoIABCDgDQEEgzecSy0FwVAqGk5AAAIQgAAEIAABCEAAAhCAAAQiRQDBgGCI1A1NYyAAAQhYBBAMPt8GCAafA0DxEIAABCAAAQhAAAIQgAAEIAABjwggGBAMHt1qFAMBCEDAMwIIBs9QJy8IwZCcC0chAAEIQAACEIAABCAAAQhAAAJRI4BgQDBE7Z6mPRCAAAQQDD7fAwgGnwNA8RCAAAQgAAEIQAACEIAABCAAAY8IIBgQDB7dahQDAQhAwDMCCAbPUCcvCMGQnAtHIQABCEAAAhCAAAQgAAEIQAACUSOAYEAwRO2epj0QgAAEEAw+3wMIBp8DQPEQgAAEIAABCEAAAhCAAAQgAAGPCCAYEAwe3WoUAwEIQMAzAggGz1AnLwjBkJwLRyEAAQhAAAIQgAAEIAABCEAAAlEjgGBAMETtnqY9EIAABBAMPt8DCAafA0DxEIAABCAAAQhAAAIQgAAEIAABjwggGBAMHt1qFAMBCEDAMwIIBs9QJy8IwZCcC0chAAEIQAACEIAABCAAAQhAAAJRI4BgQDBE7Z6mPRCAAAQQDD7fAwgGnwNA8RCAAAQgAAEIOE5g8dIV8t2EX+W3KTNl1uz5ssT6vnbdBiksKJTsnGxpUL+ONN+hiezWuqXs3WF36bRfe8frQIYQgAAEIACBIBJAMCAYgnhfUicIQAAC6RBAMKRDz4FrEQwOQCQLCEAAAhCAAAR8JzBt+hx5/Z1P5P2Pv5Sffvm9XPXJrV1Tju5+kJx84hFy2slHlutaEkMAAhCAAARCRWDjGilYtSRplTPrNRGpUSfpuSgcXDBtpKxZgWCIQixpAwQgAIFEAgiGRBo+7CMYfIBOkRCAAAQgAAEIOEZgzJffyWNPvSYjPvoinmf17KpyYNsdZK+dGkvrHepJ0wa1pE6NapKZmSF/5eXLyrV/yZyla+T3eStlwrTFMtX6jG1Nm2wnF/Y/TS67+AypVbNG7DCfEIAABCAAAQiEnACCIeQBpPoQgAAESiGAYCgFjFeHEQxekaYcCEAAAhCAAAScJDBl6h9y8+An5J0RY0y2mRlVpO8RbaXnQbvKkfu1KldRc5askffHz5Q3v5wqP07f9lZn/Xp1ZOB1F8glF55errxIDAEIQAACEIBAMAkgGIIZF2oFAQhAIF0CCIZ0CaZ5PYIhTYBcDgEIQAACEICA5wTuf/QluebGB0y5NbKz5LJTOspFJ+4t9WrnpF2Xz36aK48M/1FG/TDb5HVI147y8D3XSrs9dkk7bzKAAAQgAAEIQMA/AggG/9hTMgQgAAE3CSAY3KRrI28Egw1IJIEABCAAAQhAIBAENm/eImdfeKO88fZIU59zjm4vN5/ZRRrVdX4qo3e+mi43DR0rs63RDdWqZcmQJ26Vvr2ODQSHdCuxYuVq6Xrk2fFsxn46VBo2qBf/nrizfsNG6XzYGZKfX2AOj/nwOdm+ScPEJGntr16zTnbf9yRpvfOOMnbk0HLn9fPEqdK337VywP57ygtPDir39V5e8M33E6X/f26SLgfuI88+erOXRftS1l9/5ck+XU4zZf/+43DH6vDIU6/Ik8++IVdderb0P6unY/mWJ6Pb73lGHn9mmDz72M1y/NHdynOpL2mDwMyXhlMoBIoRQDAUA8JXCEAAAhEhgGDwOZAIBp8DQPEQgAAEIAABCNgisGjJMjn1jKvkuwm/Sp1a2fLYf7tLz65tbF1b0US6XsNlT46Wl0dNNlncPehyueqSfzrmK5qv39ctWrJcmrfpHq+GdpL2O/Pk+PfEnbfeGyW9z746fmjW5I+lRbOm8e/p7vy5ao00bNlNdmrZTGZM/KDc2WmnfdfuZ8shXfeTMR88V+7rvbxg1OffytE9/iNHHHqAjHzvKS+Ldr2sYW9/LANve1zOOeMkueHq8015Gzb+JblNO5v9/NU/S5UqVRypx02DH5fB9zwrd9xyiVx7eT9H8ixvJldbI6gesEZSvfbC3XJaz6NKvXzGH/Pk2J4Xya6tW8qHbz1Wajq3TwSBmdttJH8I2CGAYLBDiTQQgAAEwkcAweBzzBAMPgeA4iEAAQhAAAIQKJPA4qXLrU66i+XXSdNlr50bydBrjpNdm9Uv8zqnEtz/5vfWaIavTHaDbx4g113R36msfcmnuGA4tNv+MnrEM0nr0vucq+Wtd0fFzyEY4ijKvaPcx3wx3hoB0kgOP6RTua8P8gVPPfeGXHzlHTLgwr7y0N3XmKrqqJfX3vrIiIUzeh/vWPV11Mqk32dIh/a7Sfu2rR3LtzwZ2RUMv0226nlgL9l1lxbi5CiO8tRV0waBWXnrTHoIuEEAweAGVfKEAAQg4D8BBIPPMUAw+BwAiocABCAAAQhAICWBgoIC6XZMP/n2u4nSue0O8uZNPaRerfTXWkhZaJKTz3/8qwx4bFtH++MP3CAX9u+VJFU4DiUKhtq1asq69Rtk4fTR0qTxdkUasHbdBqnX7KAix0oTDIuXrpC8vM3SfIcmkpmZUeSa4l+2bMk3nc5Vq2aKnREMmkbrmZVVtXhWUtERDMtXrpI1a9bL9k0bSo3qpd9Pa9aul2XL/7Ta1VhycrJLlF/8wNatW2XR4uWGZVkcil+r31O1NTG9Tl21dNlK2aFpI1v1SrxW9wsLC2XegiVm+i+7U14pC72uXt3c4tlJMsFQIlEpBzQWyabo0vY13K6eZGSkvp9KydbWYbu8NTOd8ql69W33gJOCQf+/0VhoW+vWqW2r3sUTxaYw0/+n3N7KwyydulTkHk2nPK6tHAQQDJUjzrQSAhCofAQQDD7H3AnBkD91nGz+fVzSluR0PkUymuyc9FxYD85dsFhmWsOdFy1eJqutX0z1l+Rq1i+8devWtn5JbSStd9pRmjdrEtbmUW8IQAACEIBAoAicef7/5NU3PpL2OzWUj+44Teo7sJBzRRv4xIif5eqnPzOXj3r/GTns4P0rmpWv1yUKhn5nnSzPv/SuPGFJk38Xkyavv/2Jtb7BddKn1zHy2psfmzonCgbtAHzoiZfl3oeGmk74WKMuPO80uePmS6RObq3YIfOp0y3d9/BQmfDTZCMMDj+0k1x8fm/pfuK/S0yRpB3o1938kLz93uh43kd37yLXX9Vfuhywdzzf8gqG9z/6Qv571Z2yYOHSeB6nntxd7rv9yiLPbzqd0SVX3SXTZ86Np9OpjR67/39mvYjYQe1o/mTU19Zb+9fKO++PkXeG/1Pf/5x/mtwz6Iq4wNC6/nvArXJQ573lqYcGxrIQu23VCzSPK669xzCMZbDPXrvL04/eJPpZ1qYy6Yrr7zMxj6VVeXP7Tf+V/5zXOy6HZs6aLyf3uVT226etnHpSd7nXittX3/xkLtG38Z965CbpdtC+5vvBR51jjS6aYUSVHmjUsL688X/3yX4d2sr+h/QxaX777h3zef+jL1r3wIty562XysJFy2Toy8Nl1pwFJv79zz5Zrr70XLn5jidkyIvvmLhr3Xr1PNLcnzHB9Pizw4zQuPy/Z5qpvTSmF1xym8m/tL8mfvuWqZeeLw/vVavXymUW70/HfGPqs8duO8m/Tj3GiKCHHn855RRJ19/ysFkrQpnrplyutKZYi02zpoz/fcmt8sVXP5jz+peyfeS+66T7odumloqfKGVn2ow58fppkhOPO0T+3e9UeXv4GJny+x/y6YinpWaN6pLI7HDrPj7gkNNNjjqtmLYpcXvi2ddl0N1Py9HdD4qva1IeZol56f5J/7pUvv/hNxl47b/lIuv/98Tt6/E/S68zrjRiZdL375r7z+49qv+PnnX+DXL8Md1KrGkSu890Gq1zz+iRWCT7lZwAgqGS3wA0HwIQiCwBBIPPoXVCMPjcBNeLX2jN+Tzigy9ktPUQO278L7J8xaoyy2zcqIH55bH7YQfIScceKvqdDQIQgAAEIACB8hF49OnX5LJr7paaOVnyxQN9ZY8WRd+wL19uzqT+33NfysPv/mA6mX/55s0KvT3uTE0qnkuiYPjo7cfl2FMulq7WwsNffPx8kUxPOeMKee/9z+S91x6SHn0uM+cSBcNZF9wgr7z+oTmunf/69vSXVmepdhDqmgoTx78V71x/esibctEVg03aZtZogKbWaAkVDbEtcQ2GlX+ukS7dzzKd+9rBrB3y062OVO2I1i1x3vvyCIbYW/aah3bk1q9XR8Zba3ropp3zX496UbKzq8mQl96RCwZs67A+oOOe0rx5EzOCJiYltLO63R67mOv+de418uY7n5p9/UvzVbESa9tBB3QwXPUt/GRrMJSnrVOnz5a2HbetlbFnu11l7712k9+sjv2fJv5uyp/+y/uyc6vm8boU39G47N+tb5zrUd0PlKVLV8bFwRn/Ol5efPp2c9mkKTNlr86nFslCp9L64cfJcZHwyXtPmo7wXmddZTqQlY/Gq+3uO8uD91wjbXfbucQaDLG1AGIZa3rdYp3wyi8mdfSeiMX8uiv7y+CbBpi0sTxiazC8O2KMnGstnl18i+WpxxfNGGOex8vDOzGt5qH3gtZHR7TEtsR7MXYs9nn3g8/Ly8M+kClTZ5lDev3Zp58oF5x7qpmyaL+D/2WOa5v33XsPmWwJAZ0GTjddVP6c008y+6X99cPPU6TTIX3jp/Uejt0LsYMr5o41I06KM9v7oNNMWYMG/lf+d9V5seTmM3Zu6NOD5Mx/nSCJHMr6/7FIRn9/0c7+a2580Pw/NmHsa0WSXGr9fH/M+jmvXJ5/4jZzH9i9R4d/8Ln0PP1yIxiGD3u4SL6x9j5w19Vy6X+2yZQiCfhSaQkgGCpt6Gk4BCAQcQIIBp8DjGAoPQAfjfxKnrPenhr+4edFEjWuV9PM+7xDw9rWFA3ZkpWZKZut6RtWrdskC5evk2kL/pTlqzcWueaUk46Q8845RY48zN7bSEUu5gsEIAABCECgEhLQjrzd9z1JdOqPIVcdK/86tOy3s73CdNR1r8vXvy2Qiy/4lzxy73VeFetYOYmCYf2S76TpLoeZjr15v38qO2zfyJSzes06abBjV7OvaWo12bZmQEwwfPn1D3LYceeZDuXPPxpiOrs1sU5r08VadFnjd8M158ttN1wcnwZJz987+Aq54r9n6a7o/PRHnvRv02GbKBiu/N99om+Ha+f88NcfiU/J8/CTr8gV191rylw663MjA+wKhhUrV8suex5n2nn/nVfJZRedYeqgb5G32fsEsz/Ketu7w567yU7tjjXpnn/yNjm774nmnN6Hp593nVmPQuv15ScvmGmeEgXDGy/dK6dYb/vrph29HQ/e9vb+q8/fJb1POTqpYChPW/XN/tvvfkYSRYBOyXTptXfL408Pk7IWIde30m+540nT0Tty+FNGsGhdYwx1///ZOwv4KI4vjj+8QClQoLgWKVDc3aW4Q3F3CO7uEopr8OLuTnF3KO7uVqA4//+8SWezt9lL7i6XXHL5TT/l9mZnR76zl717v5n3lGigFxjYcL1n6xwpFn39+o3qNe9FS1dskWIK70xgYUmJN/oYDGZBnpXhl9vieWBjO7tBayZ2diihhncmcKBodsWkjNM5xE6Kw7sW8mWk6lACg8w0/KPn379nS+rXo6UsYQ/vAcOn0uAR0+UK/w1CiOPg5nwfdO83Vt6fXKFfAgOftxaDoVi5pnLnAu8g4t0syp3WhGkLqWN373v80sm1vtyWcZ2ceN4LlW4kFz/VrFaaZk4aIOeHP38Va7bXBC5rAgPvUmgndvLw7gW1u4Tr1YtYr+4fkJ81e5hxHcbE7p+Spy8ts6+e3UApkiaSx8wydtKC8rO2be10GZfEnnsUAoORNN7bQgACgy2UUAYEQAAEQh4BCAwunjMIDL4ngFeXDfOcSXv3n9BOVsyXikpnT0EFMyamZPGia/nWDm48eEV7z96lTceu08bD17ViHNCvV9emVDh/Di3PHQ7YTRQbEqJGjUyJEsR1hyFhDCAAAiAAAi4moFbH1yqajmZ1/s3FvbFs/tS1x5TfY4HMPLBzPuXOntGyQDB/pxcYPr04Se26DifeYTDRs6fmwmTR8k1Ur2kvql+7vHSTEi56ZjkqJTCo3Q3NGlW1cPfDhVau3U416ncl3qlw+8JWUsIAG+b3bp0r61H/8GKOFsK9jRIY2CUNGx05rV82kcqU8hY5VHkWnXiFO++qKF+msGYcL1wgO7G7F2uJV0nzamle+X9y/1IpDqiybLD2mrNSGrXDhA0jXSOpvqsy/MpxJhKlLi6zbp7fQkmES0wlMFQqX5RWLvhDX5wGjphGg4ZPI3bBtHTuaF8Cg71j5SDKbMhng7/abcEN8nxy8OikSRJQwbzebossOiLesEE6fAxv11ILZw+nWlUtP1Ptu42QIoWaT73AcHL/MsqUIbVWpT42B+/i4UDL9goMxvlS9xuvkH96a48Wb0NvnOZ7lQ3x/gkMPE9ZxQp93mnA7BfPHinjONjDm3fkxElWSBq/dwh3aEV07tA4ZkIyYTDn+h0RGDjgstq9oO4jDa44UPe42TypcqoO5nXn0jb6IZr3ThA+rxdXrAkMLEQkSFVMVnfh+BpKkyqZPB462ov6DZlMjepVkqKFPcz482gtKUFFL+7tPXiCivzWRLqOuntpu5xbe+5RCAzWaCPfLwIQGPyig3MgAAIgEHIJQGBw8dxBYPCZgLdv/6VOYsUc+3zlFO/HqNSqfFZqWCoDxY4e2aegnUePXryjudvO0dR1J+nZ6/fy6lbCN/Efw7vKwHp2VhesivMX++FjZmqrpLhz7F+WhZTfSuanOjXK+urvlh375cosXydEBgdPTJ0qKZUXvlRrVy9jEdSP/fuyeyr2MWztxzNvke81cAKlE1vzjT/yzdpz1zz+wVZTGHZ4Dni7eUhLQ0bNoMkzlpDXpP5UrnQhq91nVxs85ysXjaW8OTNZLeeKE7wSc7/wVf3nzOGUXbg9YENA7cbdKXfOjJo/Y2v9KlutLd24eVee9s8PMxu2zpy9LMt6Du1MZUt7G+Ws1W1vfqlKLaULhVMHllldRelXnWwk4aRWuPpV1pZztZv0oFOnL9IiYSxi1yBI7kvg8PGzlK+Y9yr3C7ObUtK4/ov7QU2j1yzhKmnVcapUrgitXDg2qJsPUHtGgWHvgeNUvHxz6QLmwI75sm72m75h8x7NyG8UGPwygup3BfDuhzadh9K8heto7Miu1L6lpbuS6+LvXerM5TWBgV0L5S7iXWbquD6aoVkNeMqMpdKAyjEDenZuarPAoAzoHGeC401YS6qcMrQby/FuDzYs79zgRYUL5NAEhlFDOlLndg0sivNu2PI12klRg/+OGl0k2TvWDVv2iNXpHrINNizXqVWWiog+8Pcus8DL+s7o59zMqD1v0TpqLNwMKVdZeoFBGfb19fHuDDZkr106Xj6r7RUYunZoSCMGervd4noVG3YjpO5BzufA0j8mzs+H9P7pMfnd2S+BgQMxFy/fTLq+YjGJhRiOQcDJHt41qpSS9yVf9+XVKQtBivMat+4n72lHBAaORVKzgY8Ax/XpU6NWfWn+ovU0oFcrGbdAf04dKxHPTLTjMjES5pPiiDWBgcuozzjHw+jWoRFnaeLGX2JXEsfYsIcZfx6tJXV/6XeicGyLidMWkboX7L1HITBYo418vwhAYPCLDs6BAAiAQMglAIHBxXMHgcF7AnbtOyaC7g0i/pHLqU+dvNS9Vm5h4A7jXcAJ/378/JVGLjks/+fq0qZJQTOEsTxvLu8VgU5oIsiq4B9vnXt7ytWOqlHeYn333mP5Y0blebSuQ6NF0ES17ZvzVcBIVcbaKwds49WJYcJ4z4H6oWS2mlHVMXfhWmrSur/2Q17lh7ZX9SOdGRp90oYEFhww84+J8/1dFahWjRpXFgaHMbJhfseuw6R+oCv3E8YVm2Z9VUY7PlenZlmaP8PbZ7mxrP6HOJ/7c+YwKcwZywXkvfLDrHebYmt9+tWyH54d92UktLUefbl8xetLo9E+YTAKbqKSvp84DjgBFdjZo0p2GtbEutAY8JYcr+G5WDTwc/3p9Fm4+Ti6e5H0oe54bUF7pf7vBxuP+fOqXJVcP7dJxhDg92zEfnT9L7kAQC8wJIoflyL+mFV2+vKp9ZQyhaXf/3/ff6Bo8XLL83cvb6fajbpLP//zvYb6WnzAfvL5Ga92MCjjq39Eegq/8UOE/3hb/76yeMvBmFm4bdPM2/e9WRu/VWktA/rq3SPpy6m/Q0vnjaZqlUpoAsPKhX8Isamovqj0qZ8xd1W5+OLhtb80IzoHi966ZhrZO1aufOGyjdRrwASLINWczzs9povAywnixeG3vpIS7XghCPfFmNR3BzUPSmAwutBR16lnsPLTb6/A4DmsM3VsU09V54uNOmGPwMD3MRv+2TjP9+7ZIyvlLhNVlz28S4g5YhdgZjtZuL7egybSiDGz/P2uYuYiiQNm9+g3Tos7oPqnXpWAwkHCJ3n2UtkWr39Mmk9de/9B7GLJa2J/i3P8Jmm6UvIe8Utg4EUa1ep11r43nz1/hbLkrSHHfPPvzXKhjz3M+PNoLel3QvAuKP4bkihNcSnWHd+7RC4asPce9Utg6NJ7DI2d9CchBoO1GQm9+RAYQu/cY+QgAALuTQACg4vnFwIDkTJK81QUypSEPFsUCdQgkmeuP6HO0/+iQ+fvy9kPDKNgYN9Wyj8qt9NP+LXt6tFQ+n399u0b/X3xmgwIOVC4BOA0eUwvail2bKikBAYOaHdw558qm74I/7s3bt6jNRv+olFj58h8vSECAoOGyt8DZSSAwOAvqkAr4CyBgTv45tFhLUiqvsNThZuMtsJdhkqB8bckuAkMvAOKdzJJ1xWxYqqh49XNCDx8/FS4ofH2Y39xdjNKEvcHh0f4+cs32nb8Jl29/1II1kQpE8SkAhkS0Q9RIzlcp/7CrjN20ZS1J6lVs5rCENhTfypYHxsFBl4IoFYTswuT2LFiUIPmfTQ3KTwYvcDAvujVc1kJqfoBHzl+jvIW8zYef3x+gqrX70zrNu4mJQroyyoXOMqwrZ5hXIbdT4UR/5kldsnI8SJsFRhqNuwq4yeY9YEN0/w/L2r4Xew241gAA3u3pj7dmls0zT7jI8XydkGkhE5laDf7G7z/8CkqVKqRjFVw8cRaX0Z0e8eqOsN9PXfhqhRtmCsL2pzU7gNVTv+qBAPOM3uuqFgHJYvlpc2rppAqb83AXrxCc9q15yjt3jKbCuTJareLJKPhV7FQ4ovquz0Cg+eEedS9r/duIjU/qh5+VW3wsX/31uOnz7UYGi/u7peiG1+nUssOg6VbLUd2MKiA59YWHSgxTL+zQLWrXmf/uZqatR1oKlLwQiAVM8UvgYHLqfgrLBT+uWS9jPExqE8b6S6M27KHmYrfovpofFWfFRb5MmVIY/HZ4LLqnuNjW+5RvwQGtTvDeJ9x3UihmwAEhtA9/xg9CICA+xKAwODiuQ3tAsNkryXSzy5PQ9tK2Whks8JBNiMdpuwkr42nZXu84qxpgypB1nZAGrp5+74Mksh1zJw8gBrVrWRanfI7zH6Cj+1drJVRAoO1FXFcUPl11q/KUoaMwNjBwEEI791/LFcY+uVigAUUNoREjBjB6gpBbaD/HTwSfoBjC0MoB0DUp7fv/pUrw6JE/k6fbfMxB6vkOhInjGexQ4QrUD8G9QIDrxwLJwKS6330mjX26dNnGYwzXtzYZqd95TGTx8JVRHyT8i9evpYrCCNECO/rOmMG/8iNHNnb4OfoDoYPHz5K/9hJhOFLv2vG2JYz3rP/aR57jOjRTKtzpsCw4s8xVLlCMV/tsNuwA4e9/4bwSTPjFuezcYbdeSQWvtDZDZlfiY1nbLhSc+afwODXfcz1KF/KZjsY/LqH/eqWillqAABAAElEQVSjf+d4NbRo2t973Vo9/s2t8ToeJ/uy5lXBYcOGNZ7GewcITBCBfDuKQL4c/2hRrwoO1OB9ydV7L6hy/1V089FrizoSxo5Gy/tVokw/ewcztjhp5xteNJC3/Z8yWC77jQ8pyUxgUMZwfm7Hixeb2L3PJhHYtlTxfHJYRoFBreTXB9BV41ffr3gxARvWe/QfR6PHzSW9exRVdtXaHUKA6KLtYNC7V1L+/VVZfp01f5WMwcAxBNhVmq0Cg1oVbtaHFh6DaObcVTIANQe3HjrKSwaYNsaLUH7vuR/3ruyQzz5lNO3eqTEN69+eT2lJGbzV9xn1fFZGdHvHygs8+O8uByxWbn+4MVUvH7NLKvU85fcq6QMum+38U9+92rQQgctH9bAw9hoN7FxXwlTF5a5V3qHCuybs3cFgNPyqMSg2qt+2CgzKHRVfp3ZVqDrUqz28kyVNoMUC0X8OuC7+u887Uy5cuuHQDgY1Vq7r9YOD9H3UKHwoE3+3+DFxAclWHzRcnVevu/YelW7N+Pv02cMrtR2/fF7tBOBjvwQGPt+2yzCa6rWM2MXXhKmL5Pfha2c3UvKkCfk02cPMP9eFysUXu3XKke1XGSh7xKAOcqESt2XvParm3OgmiueHY2Twd3vjfcbtIIVuAhAYQvf8Y/QgAALuSwACg4vnNjQLDHOEO52mwp0Op0ENC1Dn6jmDfDaGLTpEQxcelO0umDWcfq9mGXAvyDtkQ4MqwCD7teUtzdYMuWxYr1qnE30VOxPWLBmv+W+3RWAYMHwqDR4xnfQ/MgNDYOAfhs3aDpAuV9TQ2bAybnR3yqdzXcUGy049PWn2/NWqmDScs//nVk1ragzYF3HZqm2oepWSMo4Ej4ENwCp4Hv/gmTh9kfwhxwEqObGho3njatS4XmWtbr8O2Ictb6tng7FKHFhy6tg+0sDJeeqHKwsMVYRhetK0xdJPMp9jY4+XEIby587Cb7XEO3nWbtwlV5hyJq9YZJ/OHEeDX1ViAwKLR7zC9dWrN3L7ORs7lLsFPmZD0so1O7Q+8mrznl2a+GqTRQ9eMbtt50FZln8k1xKfARYmxk1e4O+PdmXUWTR7BC1avln6Clf95OCEHOeEBRWev19zViEWT9aJgKHMXJ941abn+HkyfsHMSQP0pyyO2eXH+CkLaPO2/Zphnz8HRQrmoBaNq2sBEvkiZwgM7P+bg47y/bRkziiLvtwVQhf/eGajNsd14BWsRoGB74P2XUZIQ5y6mD9Tk8SuolQ/J1FZ8pXdInTvO07OBWfwKti2LX4nDvZ49u8rpHeRZOt9zOXMBAZb7mGLzune1G3ak86cu0wLZo6QAUf3HTpJNep1kausC+bLSj37T5D95Uv4Xp8ytrdFYE5dVRaH9sxt5dod6PDRs7R782xiNxcrVm+X9xi746hSsZj8LEaKFNGifryxj0DJii1EwNojNKdrGapROK19F/9X+t2Hz5SlxRy6/+wNZU4Zl1qWy0zfRQxP40TMhNMiQPMvSWLRkUn1KbxYuR/QlEcIDGeF0LB59VQqWTRPQKsLkuvNBIavX79pLku4E3xPP7m5W4sXZRQYRvwxi3oPnCg/a4d3LdRWeLORsHDpxvK5o9zg7DlwgoqWaSLHpg8YzH+fC4oV/vx3Ru1gYKEzS77q0nhrNNqfOH2BchaqLeu5fXEr8S4GWwUGvS/5M4dW0K/pUsp69AbUo3sW0QcRwLdgyYbyHMeP4WeTSuo7ELsj4gUPnNSziHkd37dEcxd178FjuQKen9cqWK96PqvvN/aOVQnLxlXze/Yfl+58uA/P7+zTvpeofqtX9WziwN2zJg/URNHLV29RuuzeC0Z4Lvk5qV9Nzs+D8aO6q2qo/7ApcqU7z9mV0+ulcVutyldBwbmw3mCs4hgoocdo+DWyUY3ZIjCcv3hdGvz5GuXPX12vf7WXtwpMzM/hheK5o77zspuq+s2843gY50LfHh8rjvy8vi9EKRaiWcRKlq60fHZwvCn9d0D1PZnn8v7VHRZCkr5u/q6U4tcyso55M4ZQ3Zrl5GnmxTFM1PdM/wQGvRjBFRQplJN2rJuhNWUvM+1CkwMOjh03RRHZZ3XaGA/EnntUxW/huh5c3Ulxf4olq50wTYjU3UfLY+N9JjPxT6gmAIEhVE8/Bg8CIODGBCAwuHhyQ6vAoFb9MH5XiQtq6vUig9l2blUuuLzyjxb+kc4BEjlQor1J/XCyZQeDPhCjswUGvaGBf/SxoVi5GOAfdScPLqUUSRPJH0FszOAfapxfqkReevz4uXRLwGOvW6sczZs+RGJQRg6uTy8AKP/UygjBhfkH3GthoGdRglO7lrVp3Mhu8tjaP/oftLytPpHYvbBbxA/hFVr8I//yqXXyh6v6ka6vh1d3Xb1+R+uX/gfdpOmLyaPbSFmcRQle+bhn33GtrDI2cIGRY2dL388sQHC7nHi8LDA8f/Ga8peor7HKlycLXRFGixu37sly+h/h+rJ8koM6cjk9N315WYHhHz1PPsUGkahRIwsmx2VJntNjexbL3SPK3UC3jo1o+ADvAJmqOhXzwNqKR1VOrfLj+6BM6QL0+vVbOnDolLxHmMHVMxu0VYjqB7JyHaLuDWvuEFQb/Kr6s27pBKpQ03s17Kv7B+T9p8qp1cEd29aTsWOMAgOv8G3ebpAszmwTJ45Hh46c0eZMb1w7evJvKlm+hRwHjy3jr6mEse+qhQFALzDouft1H5sJDLbew2qcxle1Ylr9rVT3OvebDZX8mi1LWjpx6qLW/6tnN8jPsrEu/Xt75lb1QbXJoiQn9VlWq3/19ePYdgIfhRAY9aeccoXwvaVtKOb3ju3ymrruFHUR7gh/TR6Hto+sqblEev3uIyWoMUl26MD4ulJ8sL135iX7ztlHf6w4St3F35dhhr8v5le4PtdMYOBeqR1kfGwMcmwUGNh4nDJjWfl3mwW92jXK8GW0Svh1Z8HA+HdRrZDnz07DuhWlG6aFSzdqhlAlMHAdekGiSsXilEcIqbfvPCR+XnHq0bkJDe3XTh7b8/e1QYs+tGDJBvm3okGdCuK59YoWC4GaU4WyhWmVCNbNbpJ4RwXvrOC+tmxanWL9GIP2i7/3HPSa0xERcyN7lnTyWP83kcfM/eXdidwOP9NYsN2x3ks+i9TfLCUwcAX2jHXOgjXUtM0A2S4b0n9JnZwuXbkpgw1zW/79/dHvwOA+8AICNlRzAG6+nvu+fL6nrF8ZxuUb8Q+Xz5UjA3EdvHKcE7sZyp09ozzWj4NFhq4ejShpkvj0Q3xv0S0wBQb1d5k7wvei2W69yUJw5tg9+n76d2/pBS2eR3YfxTt59QtO/Puuoo9Hwt+xmogFJXyvTfFaSu26DJfseFEEfy/m75q8sIDTROFyrbVwveZXUrtGuAwv5ogS5TvavvOQ9vzjfP8EBn5Wc5B19V3NuFiB67CHGZf3K6nnLZfRfw7UNfbco9z3BKmKyXuXv5cy33Pibw8v7lEJAoMigVdFAAKDIoFXEAABEHAvAhAYXDyfoVFgYPcX2fLXlF+kg9otkrXp9piyg2ZuPCN+XPxMJ/cv1dyTWCvvynxl6N+yZiqVKOKzUpO/5PPqdbP0XaRI2piUwMA/APdtm6cV//T5Mx0/cZ7Wb9mj/XDbuHISlS6eX5ZR7TrDRRL3tVDpRvIHCP/IW/HnH3JVGq+orygMuvzDhGNL9BcuCFS8CTYibl07TbrB4A4pgwYfKxb6PDZue4nV8BnECklezaz8xLKx4tBfC0SQ7+R8KSmXFHzMKyezZfY2WPB7Y1I/oPWuqdi1EK9oZOOm+qGvDBh8/e/Vf6ORgzpKP9U8P3mK1pWrQlVwSi6To+Dv8no2rFQsV4SzZFLtDRZB+3qJYJqclMDAxzWrlZYGHuWSqHMvT7nzgMWMtcI4rtxNjRfuTjoJdyc89sc3dkkeapcK/6DeIFxwsD9vXiXXvd9YWQfX79+Pdr1RR80BX8c/TouIlbJscFZBOtV9xz9Ab53forkS0K96NLqA4LpU4pWoSdOWkm/1xnZ28xM3RWGZr/xQ8xtnCAx7ts6hMcKfNIsH+vni+tXcsKF95B+zpdFLGQX0qxrV+Pka5lunaQ+54p7naM+WOZKDWhHLKzRnTxkkDWNcR5XaHTUhTY3ZnvuYP2fGHQyq3/7dw9xfs6SuNwoMXJZ38nCgyx9jRperQ/m+ZoOJf/eRvXOr+sBtciD68mUK8yHNmLOCWnUYoq3Clpn4x24C+8WulEJi9TvvOmABwNGUz2OB3Kkwv3s5qlowjUU1aw9cpbtP/6GyuVNS8njRLc458mbL0RtUdeBqskVAdKT+wLiGdxkmTF1cVs1BntXKbH3shG1rp1vsYFMCg16g5t1UdZr0sDDocaXMYtHskdqKYs5jQyt/Rtjwrk8sxPLuBr3AwOd5FwvvWtILz/wcYb/0zRpW1VwPqhXYxpXX+jbU8efPX+QuO94lp0/swmj0kE6a2zvuK4stbMDVJ36GrBTPSiUu8Dn1LGIBe+6CtRb9ZQPqnGmDNbeKPCbeoaPiHKi6bR0ru8/h4LXjpyxUl2qvLJjwDjX/XC/y947qdTtb9JMr4YUOowZ31HasKIGBv68VEvOpDN9cludh3Khu1LBORX4rE7spzFHod/kdgzP4uZxP7JZUwb6VwND/v90PvCOCd0aoZI0Nf2+PmcjbTZdyt6fqUO51MuSqorWr6jO+6u9nW3lzHbwoqfLvHS2M9tUql5Df13r2H+/r+Wxsl9/3HDBeiy+mYoDwM9Jr7kr5mTBe49+CB315DsLM/VACAX+v4u99tRv3kH1WsQyMzPR1qN1InPfPw0OmuybsYaav23h84MhpbYeQNReQtt6jXDfvwCxVsaXF/czfZ/j7NO/4Nd5nxv7gfegjAIEh9M05RgwCIBA6CEBgcPE8h0aBoVm7AcKAvYYKZ05CG4favwI/sKasWNfFdPjCA2rdvCZNHN0zsJoJUL38I5+DwXFif6/p0/6s1ad+iGoZugMO5tamWS2Zowy9utOmh/xDmY2dKjlTYNCvjnp8Y7dcRana4VV5Tdr0p4L5s0m3NMpAqtwbqHL82r7bCJo8fYm2ylMvMOiNzVxW7fwwW0Gv6uHV6J5DO3Nx05QqUzlvYczgqoDH8/fFq5QzWwbppkcvMBiN5myYGDvpT+rbowUN6NlKtsM/fNloMbRfe81gwyeGjJpB/YdOIX2/9AKD3m8wizOxkxaU9ZmJQGpVPhtkZYDeZIXkD1+jH2jePs+uf/he888wrIw6XTwayB/TsvH//lGrVNVqTBZXoifIK8/qV56yC6B+QyZLsWTRrBH6KiyO2QjN3FKlSGIRtJwLKRcKemOzswSG+2KXCBsJ2Hi+csEfsk+37z2kFOl/k6uD2d1CZSEE8KpaJTCo3Q1sCLt9YavFOB4Ko2Ki/4yKbCR8+uyF5m5ErXJUF7ALMTbacFICgz33sZnAYOs9rPpgfFXGfaPAwMaum+c3a6IWX6d8vvfp3pwG9mptrEp7b+/cqj4M6NWK+nZvodWj/wxYM9BohXFglcC0WcupTaehVL/krzTVw1vUs1rYyol/xC6F+P/tUni+2oM40PO5G0/o3ccvlDrRj5Q0AEGjzZp8+Pwtpaw/Xa5yZ5dCoS3x8+PW3Qd0Qbipifzdd5ROfDcwi8ujuHAsozNnL1ECEaA5Q7pUmkFbnde/smu7i5dv0vUbdyiWCDydTewa0Pur15e155h3X3CQZA4gnVL8XY/1o7nQxPFVzl+8JsWRX1KnkH7plRij2lPPorVLx1PZUgWJ3Q3duHmPUiRPJHcYqHL+vdozVv5bfuLUeRljh3dNpPo5KSVLksC/JrTz3Bb386pYMZ8g4U+UVowt+g/fa+f5QH2v4wUTpw4sk8/so8f/pkjfRZTzZizP17CbrQcPn/ChXNgQnOPS2MObF3OcOX+ZXjx/TZlF3A+OOWFv4nvp3fv3lCh+XIt7nhdDnL90nR7cf0IpUyahNCmTyYUY9tbPrpE4fhILTPpdE19f+6zmt7dOY3l7mBmvVe+VWyN+butdsKnz6tWWe1SV5e8bt+48IN6ZxbEjHJkfVRde3Z8ABAb3n2OMEARAIHQSgMDg4nkPbQLD1h0HqUxVb0PTialia7nwwRxc0qmrjyl/B+8Vdbs2z6KCebMFl65p/dD/YFGr5dXJG7fvUc16XdVb+apchuhXD+kFBuVaRF3ErnnSiO3+pcQW9GqVSqhs+crCBhudzYzXqiDHEWgi4mpwvfrA0uq8euXVXjUbdPW1WlKdV696FxL6FZvqPPuSb9yqn3R/wP7YlcDAP/aVn10uy8aXCDGzyss6tKmr+X1W9ezae4zYTYR/qy/1rit4RSEbz4sWzEn5hSsivc93JTAoo4Bqh1+V8bll0xo0Wax01Kcz567Q8VN/0+27D6VBh91DcDITGPQ+lrmM3uXU1HF9tB0rfI7TlBlL5S4JjltRo0opuR2f89WKRj5WqXHrftJdg60Cg9k9oeZGz6BRq740f9F6EQ+iKQ0RuzI4qVWPZnWo/uhfecfC0ePn5A9ZdpNw5NhZbeVuYAgMmYRhh8U1TuxXm4NKqx0hys90xVoeFgKDEqyM7k3UONRnaecGL7p7/xE1bNFXumtYvWicKqK9qrIsMMQXQV/tuY/NBAZb72GtA4YDZdw3CgzsBurAjvkWpZU/cLN73aLgf29snVvVB/2uGVWfEkKVIKPy8Wo7Ad7FxDFRBjYoQF1qOBYbiWMs8A4GDubcpmJW6jXL262N6kWFvKloTMuilCCWpUFVnXfkNV61ifTm/SfNHYkjdeCakElALzCUK10oZA7CpNdGgcGkCLJcSIBdTf6as7IUK9gdJH/3VEm50jLulFHnXfnae9BEGjFmlsVCF1f2J7S2/e3pbfpy01x8Cp88M4WNk9Rt0UBgcNupxcBAAARCOQEIDC6+AUKbwFDot8a0/+BJ6lsvH/WoldvF9H0333/efvJcJrbPF8tDm1dN9V0gGOQkTVdK+nE3umwx65paxW0mMPgVg8GsLrUCXh/Izlhu7OQ/qUuvMaY+XfVlPYXbme59x1qsCtefV8fK7QL/aFNBjNU5flWGfOXWQQkMRjcZeqFCf73xmGMIcLwDa4ld3LBroeGeM30VYTcDvPuBV6+pfumDUKoLlM9fvdF145a9NEzUeVgYy/WJx82ijpnAMLB3a+rTrblWXIk2WoaVAzbulxAuI4qWbSoDSRtX2PNl6senrQLDwZ1/Uq7sGSxaVIKHfu4UFzVfvCqWf5z7t4pOVax2dKj3/MrX8upDToEhMHAwbmW8UjsU1C6CQ7sWUM6sv5JRYPitSmsZrFnvHkl28L9/lIGcP8MXL9+gAcOmErsHYfdCxqTaYoN5mLBhKHEaS+HPWJ7fq/vYTGCw9R42q5fzVN+NAoPZvc6uJ1p6DJY7ToximrF+e+ZW9UEfm0TVB4FBkXD8tWHLPvTn4g00o2NpqlPcMiC7rbXuOXOHyvRablG8bgnvuhZsPy/zs6WORztH/04Rwoe1KOfom0zNZ9O1+y/pwvE1FgHfHa0P14UcAupvNO9ggMAQcubNHXqaJV8NGe+EF9Y0rFeRfha7ZniXycDh0+Tw9G6hXD1edjnKMXbUd1h9UGZX9y00tv/5/B76sNtyYYbi8F3h+hQhvfuIpWpc6hUCgyKBVxAAARBwLwIQGFw8n6FJYNggfPtXrOlBCWJ/T1fmtRCb4oNf+vfjZ0rdYAa9fPOBtq0Tvo8L5Qp2nSxfo50M7se+/RfMHG61f2xcVIHXnCEwFBbi0D4hDrVqVoMmeVquvFedqN+8t9wJwLEB/HJ3M1X4VG7beZhmCFXXq1feccBBHtk/f6Y81WS28mGryvDrmInzqFufsZovZWsCAxug1Sr0+V5DpTsGfT3qmI3VLLz4l9iN0JET52Qg5mWrtmp+h5XLFmVI52B3a5eMt6jOKDCcO3+VMuetLsswt2oVikv3FmyEnzV/teTUqV196ZuaCykXSUaBQbXJZXh3C7udMEuJEsSlx0+fy7gPfN7owonzWnYYLH092yowmIldajcLxxrYu3UuVytjELAbJ56P43uX0JYd+6nPoEnSBzTfo34lfawMdnNVvHBu+jlFYkqSKB793ri7jGugNy4pcS0gQZ45BgMLDKtFwNRq9TrL4IVjR3alVBnLSXFGxZIwCgzK2GWcI8UgUizv3VFspD8uAjx37D7a6mdBCYosMPzwQ1S77mMzgUEx9u8eVuWMr8q4bxQYzO51WwUGe+dW9cEsZgoEBuOM2f9eBQJe3LsC8U4DR9LGw9epxuA18tIqBdLQNOFqKWrkCPL93zefUq623kaVP3uUpyoFUjvShK9r8osdE6fEzomjIvgvu/FBCj0E1N9c/TPAHUaPHQzBfxavXr9DNURAcg6qbky8K5HjjAWXpJ6P3B+OFdJK7KRFch0BCAzXKHHm5hQjfnbXTQJaBgEQAAEQcCoBCAxOxWl/ZaFJYFBGuMGNClKnajnshxVEVwxdeJCGLTok3ciwgTW4pZnzVlGL9oNkty6fWi+M5YlNu6i2Z/NJZwgMyrUKuwbiFaIsAOgTr4zmXQ4c5G700E7UqW19/WmLY70x3GjgXrR8E9Vr2ot4RfSSuaPoh/h55LXGWAGcqQxhbVrUogmjemgukow7GLiscjXDK6l594A+7d53jDZv30+5RAwFdntklp4+fynchsyVW+A7t2tgUaTP4ElyRViBvFmJXTWp8ZkZXY0Cw4RpC6WBmeMibFwxyaLe1sIPOruZsWUHw7UbdylNlvLy+tMHl1OG9JaGwVnzV9EV4eu5VtXfKFnSBFq8hk0iwHOp4t4ugPhiNkpnzF1Viia2CgwqILe+88rNkjGWh7qPOAbFSuEqi+MMGN196etRxzUbdpUiwkgRALNLe0v+Kq5AYO1g0LsmY3Fj1Ng51KNzExlgm/un/rapHQ79hk6moaO8RHBNH3FFjUMff+SeiN9w6vRFYtGQk/GzoN95o1z+2HMfGwWGV/+8sfkeVv01virjvjMFBnvnVvUBAoNxdpzznoOLr924i5b2qUjl8qR0qNLDF+5Tsa5L5LV/z2xCyePHsKiHz3GZnrXzUJ86eS3OOfqmYMeFdOLKI7kLjXfxIIUeAmzk5Wd0WuHiMWaMH9xm4Bx34NS5SyLmRWTKmN45QpzbwAlmA+H4DrwIh3edcowIdq8YNUrkYNVLDhL9r4h9kvaXn63+dghWHXbzzkBggMDg5rc4hgcCIBAKCUBgcPGkhxaBgQMKJheBYzndW9KGYkb7zmHyvELx0+evptdHixKR0iWNbXrO1sz7z97IXQxc3hiA2NY6ArMcB/DLV7ye9LnP7mc2CgOxPpYCGxWXr95GzdsN0lzHOENgULsDeGxs8B49pJMmMnz+/IXqNe9Fy1dtk0O/dHKdCHiYxCoGXsGeOE1J2b9p4/tSs4ZVZdkPHz4SbzdnQ/gfI7qSR6s6pFaic8yBWZMHkgpYyMER02WvJK9TrlJUH80EBuUXn42+OzfM1GIUcGDYnIVqS2HEL4O63sjMK9cTi5XzKvG2c3ZzwyvVeMWaPQKD2oWhj1XA9R4RcQZKVWwpGdkiMLDAkyVfdWmw796pMQ3r3151j06cvqAFEr59cSvxLgYVGLl6lZK0cOYIUkEzFy7bSPWb9ZbX+sWDC6hVo3ysd0tyV3zeM+SqKvuudhBwGU7HT12gXIVre78R//JOjSun12v3knbCcFC2WlvaIkQgPQsuohfcAktg4HbU7hw+5sQxRtTnzigwHDhymgqWbCjLnTuyymJXTBuxc2ea2MGjXAq9ePmaUvxaRrLSryjkzzHvJJk5d5WsRwkM9tzHRoHh85cvFC2et2s6/+5h2ajJP8q470yBwd65VX2AwGAyQU7IqtOkBy1ZsYXmdC1LNQr/4lCNtx//Q+kae8lrHy1vR/xs1qeeM/fQhNXHqWmZTDS+jbmoqy9vy3H21vPo4u1ndObQCl9xdmy5HmVAAARAAARAIKgIQGCAwBBU9xraAQEQAIGgIgCBIahIW2kntAgMk6YvJo9uI6mycIWwQLhEcDQJmxt9X26M1ctzp0sofDrXsnre1hNVB6ymLcdukNek/tS4XmVbLwuychy8OUfB37X22DjNLiHY0M8BcNlAz6la5RJy1bczBAauT80jHydKGFcGV+Ygd4eOnJEGUs73zyjNZTjpd1jUqVlWrjqct3CdrIfrPnVgGf0YMzrpV3wXF7EDihXORWyU5bK8Uox3HCyf7ynr9Etg4H7+nMHbkMurWyuVL0rvhaAxY/YKWQ8LD39tnEXhw4eTdZn906BFH1qwZIPcxcDBouPEjklHT/wt3Qlx+WXzR1PViiXsEhhUrAK+no3O3I8TYlW7CvDM+bxrZLAIisyBt625SOJyew6coKJlmvCh5JInZ0a6feehnDfO06+614sOvPOCAxFy0OTZwi2TSv7NpV5gYPdSLLBEixZV9p3nxlpwQxXPg9sZNqA9de/YWDVp9XXclAXUuaf3PHO8AnaNxDtPdu87rl3DO0Z6dmlCubNn1IQpJXD4dW9oFfx3oPqnXCRxtnLxxsdGUcQoMHCZ6sJlAs8hc2nZtDrF+jEG7T90SgaD5vNHhBuX7P+5cVGBvzmfXZ/x7hMe17adBzlLJiUw2HMfGwUGjg9i6z2s2jW+KuO+MwUGe+dW9QECg3F2nPNeiVijmheRAZodqfWTEDxjVhwnLzVzg8QukthV0oimhaldZW+XYY60o78mWZ2p9PTVv8Q7g+LHDdhCA329OAYBEAABEAABZxOAwACBwdn3FOoDARAAAVcTgMDg4hkILQKDMsB5dSpNtYs57rrg8ct3lKLuNIoTIwrlEWKCMaVNGov61fVx92I8b+v7mRvPkMeUHcHWTRKPg92ndOvzBy1evtnXsNjdzvCB7SlSRLGjQ6zyn+jZk1o3qynLqWDAxhXzviqxksEiw2jhKuje/ccWJdjoOrBPa6pdvYxFvl9vFizdQA2a97EowsbuKWP7WKz6Zh/t1et2lkKAvnC7lrVplHCZEzGit29vFRS6SKGctGPdDH1RecxuhBqIOBHGYMq8ip9dLPGOEL/Ss+ev5Kr9XXuO+iqmdwvF29BLVmyh7WjQF+bV67yKXR/LQrlJ0pdj10IDe7WmCjXbS9++alcGs+/Rb5wUHHqJgM3GxG3XbdrTghUbuYcP9JA7RfQCyq69R6ny7x01cYjrYlEqW+Z01LP/eDKLraBvr7ZY6bxUrHSeOq4PDR3tZXFPsKF8hghaHCWy791KI/6YRb0HTpRV3b28nRLEi6Ov1vSYd2hw8Fn9/c7j6t+rJeXJlYkq1+ogxzyoTxvq3bUZqUDLu7fMpgJ5spJ/94a+USUwKCM6n+PdNVHjesdkYfdOA3q20i6pXLsDrdu4mxbOHi5dUPEJ3vHC7qB4vvWJxbOVC8dq4gKfYyFg/NSFmoCiyvM9xS4XuG49J1vvY73A8PH5CSme2XoPqz4YX5VxX7m18uteZ7dcvJNKf68b6+P39s6t6gPH8cgiXFHok/IxreelP49j/wl4TphH3fuOpdYVs9JoITI4mjpM3UleG05ToUxJaM2gKhTxP/H2tNiFmE/ES+B0dHIDSp8s4GLA2/efKG61iRQpUkT694nvv8+OjgHXgQAIgAAIgEBgEIDAAIEhMO4r1AkCIAACriQAgcGV9EXboUVg4MCu7Irm0tzmlDhONIepH730kIp0XkSthOHDMwCGD/86cPnuC8racg4lTPAT8crh4Jz+efOOLl29SffuPqLUqZJR2jQpNHc3gdlv9jd76uxFihA+gjTy8W4DRxIbFy9evkEvX/9DyZIklCvTzer59OkzsVukq2J3RoKEPwlfyyko+g/fmxX1M48DSN+4dZ8uiADSkaNEkrzYZZCtiY22vEOEgzN/+vxZukpKl+ZnsULdsfGrdh89fkbnLlylqFGjUJYMv1DkyJHkKW6P83+K/SPFs3FVLrO6ePkmXb9xh2LFiiF3t3wv6jVL7OP5zPnL9OL5a+k32BZjv1k9nMfz8+DhE8qUIY3cfWKt3LxF66hxq35SzFg6d7S1Yqb53MZ1IRSxi6p0wo+wcu3EBv3LV25RqpRJhL9q87GaVhjImfw5OX/xmhQcfhH3bPKkCbU+G5vmv5Enxe4VFoGyZEpLP4jdINZSQO7jwLqHrfXV1vyQNre2jiukldu4Za8UNgtnTkIbh3oHoHdkDA+fv6WU9afLS39NHodKZktO7z5+punrT8m8FuWz0B8tizpSta9rDl94IGI+LKbMGX+hE/uW+DqPDBAAAR8CLJhz+u477+8ZPmdwBAIgEFQEIDBAYAiqew3tgAAIgEBQEYDAEFSkrbQTGgSG6zfvUurM5Slh7Gh0ZV5zKyRsy1666yI19txE41sXp6ZlM9l2kYOlEtWaTC/ffBDuFrYLdwv+r7B2sBlcBgKhjkDuInVkDJFta6dLl1ehDgAGDALBmMDDx08pUeoSFPW7CPRkpU8sF0e6fPXeC2o4ehPxrgV96i0CO3ermYvChwurz3b4ePyq49Rr1h5q2rAKTR/fz+F6cGHoIMC7Bqd6LaMuHg2oSf0qoWPQYpQcH6p1h6Ey5hTv3FwyZ1SoGTsGCgLBjQAEBggMwe2eRH9AAARAIKAEIDAElGAAr3eGwPDp6Fr6eGydaU+ilG1P4ZIFriHetGFd5tYdB6hM1TZUJHNS2jC0mu6M/YfDFh2ioQsP0vJ+lSh29Ch0+e5zii5WofPqyBQJYthfoR9XFO2ymI5cfEC7N88SsQac4yPaj+ZwCgTcmsBDsUNjxpwVdEvEeZi/aD1xHIxDfy3wN7izW0PB4EAgmBLIlKca/X3hGm0aVkO4OEocoF5y7KRbj17RxTvPpXvDXxLH8hX0OUANiIsr9V9F24/fpPleQ6lOjbIBrQ7XuzmBfkMn09BRXjbHAHIXHKkylZPiAo9HH5PJXcaHcYBASCIAgQECQ0i6X9FXEAABELCFAAQGWygFYpnQIDDMXrCGmrUZQPVK/ErTOpQKEM0mYzbRkr8umtbRoFQGGt6kEEWP6pwt3/VHbqCVey8Lv+ojhF/10qZtIhMEQMA2AmysZKMlJ46dsGXtVBmM2barUQoEQCAoCfToP45Gj5tLrSoId4QtHI/DEBR9fvb6PSWtPUU29fjGboot3MIhgYBfBE6duUR/X7xKmYU7wgzpU/lV1G3OsQs+dlfK6cHVnRT3p1huMzYMBARCIgEIDBAYQuJ9iz6DAAiAgF8EIDD4RScIzoUGgWHs5D+pS68x1K5SNhrRrHCAqBbsuJBOXHkk62j8W0ZKlyQ2nbv1lOZtPSfzyuVOSUv6VBSrogPUjLy4/eQdNGvTGZryR29q0cRxP9QB7wlqAIGQT4BjJOz467CMe8F+0uPEihnyB4URgICbEjh28m/KXaQuxfohMt1Z3DpYj3LimhPUw2s3lfutEK1dMj5Y9xWdcy4Bjjf0/OVr4cbS/0DhHLfm8dMXNpV9+vwlvX79lhLEj0NRIn9ntdNc54uX/8gYTGEC8MXz9T9vieuKGeMHq22pE2/f/Uthw4b1s1+qLL9yfKfY4nn7TIwpYeriUuB/df+Avoh2/PHjJ7pz7xHFiR2TYkT3P16af0yZo9mznmMTcRs8Dr8Sl+N+cPB2W9Kbt++Id0z5Fb+I67F1frnsC3F/8aKICBHC81skEHAaAQgMEBicdjOhIhAAARAIJgQgMLh4IkKDwDBy3Gzq1X8Cda6ekwY1LBAg4u0mbadHL95RH+G/OdPPP2l1bT9xiyr1WynfbxpWXbh0SKKdc/Sgy/S/aOq6UzRuVHdq1+J3R6vBdSAAAiAAAiAQ4ggUKNmQDh45TRPaFKcmZVzratEveFlbzhHuEl/QsvmeVLVicb+K4lwIJ1CwVEPilfi8s3Tg8Gm0Zv1fckQ/xfmR+vVoSS3FYhBl6J82cxkNHDGNxgzvQq9evaGxk/4kNs4/vPYXTfZaQny+Y9t61LheZY3K+k27qW2X4XTvvk/MkGqVS5DnkM6UOFE8rdyN2/fIo+tI2rR1n8xjA3SFsoVp+AAPSpjA57updoHu4NqNu1T5dw/KLtwEVqtYgkaPn0v7Dp6UJVKnTErTJvSjQvks3XL+T1jNJ05fJONGXLl2W5ZlN4PNG1ez6P/JMxeprHBJyvEVygvBbfCI6XTg8GkZa2LV2p2ae6R0v6Sg0iXy0+ghnWRd3KcW7QfS7n3HtZ5yXyZ49qASRfJoeX4xHTNxHnmOn0fDB3rQ/QdPaO6CtbK9FMkSUZMGlamrRyPqP2wKzZq3ip4IoYeZcT95EY/eeM9sOT7Ghs17SI21cIHsVCh/djlffB2nfYdOUo16XahRvUpUMF9W6il+Z5z9+4o8x32fMrY3FSmYU75X/9g6v3yf8C6ulWt2yL7y9cyrZ5cmlD93FlUdXkEgQAQgMEBgCNANhItBAARAIBgSgMDg4kkJDQKDM3cw+DVd5fusoL9O3aYhjQpSx2o5/Cpq0znsYLAJEwqBAAiAAAi4IYGFyzZS/Wa9KWXCmHRmRuNgOcK5Yvdimwnb6Nd0KenMoRXBso/olPMIxEiYj3iVOhuZ+TVf7sz08NEzzXDep3tzGtjLe8fNyLFiccuACZQoYVxNMGAhggUGsxgMbDxv03mY7CwbqH+MGZ0OHzsr32fNlJb2b58nV9IfPn6WSldsJdtn43nqVMnowKFT8j3Xv23tdD/dLundBSoyRQrlpOMnzss6OG/LmqkWhv1ajbrR8lXbZHEu+1oIJiwmcGrXsjaNG9lNHh88eoYKlGhA3A824qvEXNZt3K0Z4HPnyEjFi+aWrNhdVPaCtWRRHne2LOno/MXrWtlZUwZSwzoV5XlbmKo2lRDA88SJ61aCAXO7ceuezNfHgvjw4SPlKVZPts1l8ufNQteF+MEiCScWcVYvGiePOWB16UqttHuB28uWJS2dOHVR43j17AZKkTSRLG/r/D5/8Zryl6gv+8p15suTha5cvaX1d/GckVSjSsDcvcoO4Z9QTwACAwSGUP8hAAAQAAG3IwCBwcVTGhoEBmfGYPBruhqM2kgr9lyinrXzyB0OfpW15RxiMNhCCWVAAARAAATclUCuInXo+MnzNLBBAepSw3I1sKvH/PHzV8rYbBbde/qG5kwbRPV/r+DqLqH9QCagBAZu5vKp9ZQyhXcAcr3xWBmVlTGcy9asVpqG9mtHSRLFp3DhwvoSGJ49f0UpM5aVhmne8dChdV2+jHhlf5os5eXx9nXTxSr6HNIYzyvlPVrXoVGDO1H48OHo8+cvVLdZT1qxeru/rrr0AgMLF3u2zpHujr5+/Ub1mveipSu2SGP8uSOrZN1rN+yiKnU6SkP6ob8WUNo0yWV/9h8+RYVKNZLHR/csomyZ05ESGDgz46+pyWvSAMogxDd2McSukthFkhJZ5IXin2LlmsqdC43rV6Zp4/pKPnxuwrSF1LH7aNnupZNrKZ5wQ2ULU76WGTZvVI2+fv1KzdoN1MQR3jHSu2sz6QqKdzx06zOWeCfG4V0L+TJaunIL1W7cQ+bt3jybvvvOO6aaEhO4zL9Pjsrx6PMqlS9KXhP7S1Ho1es3lKPg71IQUGKArfNbtFAu6tzLk8ZNXiDFq7VLJ2huq8ZPXUidenjzeHxjl81um7jPSCBgRgACAwQGs/sCeSAAAiAQkglAYHDx7IUGgWHrjgNURmzZLpI5KW0Y6h3k1RHsGw9fp+5euyhbmvg0r1tZiyo+fPoigjxOpbfvP9HsLmWoZpG0FucdeVO0y2I6cvEB7d48iwrktdyu7kh9uAYEQAAEQAAEQhKBDVv2UsWa7WWXj05uQOmT+e/rPqjG11m4MZwm3BgWLpiDdq73Cqpm0Y4LCSiBYaJnT2rdrKZFT9h9Eq90nzymF7VsWsPCGP76wUH6PmoUrbxxB8Ok6YvJo9tIaZQ/uX+p5maJL+CyXnNWSsN4qp+TyO+znG8MKP7g0VNKnKYEn5K7JNiQb5b0AsPJ/csoU4bUWrF/3ryjmInyyfenDy6XOyFyC5HvmBD5unVsJF0waYXFQftuI2jy9CXSdZDn0M4WAsPuLbOpQJ6sWnEzgUG/e+Hm+S1CgPFxA8UXps1WUa7kXzh7ONWq+ptNTNmd0c4NM7V2Fy3fRPWa9pJCxdNbezR3SBzrIXn60rLcpxcnpbCx58AJsdNiF5UpWYCKFc6l1cExnKLFyy3fP7m5R8a8UAID7zK4eX6zJgRwIRWkXu1osXV+69QsqwXCXr9sIpUpZenWVfFYs3gclS9TWPYH/4CAowQgMEBgcPTewXUgAAIgEFwJQGBw8cyEBoHh+s27lDpzeUoYOxpdmdfcYeJX772gzC3myOv/8vydcqVNoNU1Z8tZajtxu3x/bX4Lih/re+2coweJak6ml28/0L0r20VQwDiOVoPrQMCpBHhVIfsn7uLRgJrUr+LUulEZCIAACBgJNBe+2WfNW0150yek7aO8XakYywT1+2Vit2IjsWuR0+FdC8SK51+DugtozwUElMBwcOeflCt7Bose9BwwnkaNnUOd2tWXsQXUavv6tcvTnKmDLcoaBQZlqG8hYjhwTABrSbn8ZLdLA3q18lWsaZsBMo93JVjz1a8XGJRhXV8Rr75n90drl46XhvYIMb1Fgg5t6kpXYPqyu/Yeo4VLNxK7TdqxboYmMLC4cf/KDosgymYCw4o126lmg67SjdTtC1v1VcvjRq360vxF6+VY+3ZvoQkMfjHt2qEhjRjYQatLCQHslunAjvlaPge2/jFxfvn+/dNjFDFiBO0c/244ffYS3brzgG7ffUibt+7XXBQZBQZjvVzJ9FnLqXWnoVJoYsHJ1vllIYcFHU5Tx/XRxBCZIf6ZMmOpnJsh/dpSz85NVTZeQcAhAhAYIDA4dOPgIhAAARAIxgQgMLh4ckKDwMCIYyctKAPzXZrbnBLHieYw9XojNtCqfZfl9XWKp6fk8WPQ+VtPafU+78BuznLjwAEjOXAkB+u7c9Hb763DncaFIOBEAkbDiBOrtqjq6vU7VKZKa+lfeuOKSRbn8AYEQCD0EHj//iPlKPQ7Xbx8gxqWykCT25d06eCPX35IJbovpU/CRdKoIR2pc7sGLu0PGg86Akpg4DgKxh0CM+asoFYdhlCDOhVo9pRBmjF8YO/W1Keb5eIW43O0bLW2tGX7fhnUuE0z6yJa2y7DpMDv34iNMRT05ZXAwIGW2Q2SMal4C3OnDxar+HNruyKM5fTvlZsh5SLJuIuAy5oJDBxguke/cRozfZ18rDi1alaDJnn2somp57DO1LFNPa0qJTAUL5Kbtq6ZpuWbCQzs3qh91xFSNNEKigN9HA2jwMC7DHi3gT55zV1JLT0GawKDrfOrBBd9XWbHPbs0pSF925qdQh4I2EwAAgMEBptvFhQEARAAgRBCAAKDiycqtAgMFWt50IbNe8irU2mqXSy9w9T//fiZ+szZR9PXn/JVx7QOpaheCeesYpy58Qx5TNkhA7mxD1ckEAguBNilwd8Xr1LmDL/4GUgyoP09d160kbe69AV98cTagFaH60EABEIwgSPHz1FB4e/9y5cv5FElOw1rUsglo7lw6xlV6r+K7j97Q43qVaKZwsc8UughoASGG8IlTlIRT0Gfho+ZSX0GTaLBwvDbSxiA1Q4GWwSGmg27yvgJZobj//3vf8T/hwkThvoOmUzDPWdK//yjhUsiayltmhT0Q7SopqeVwMBGc7NdA8UrNKdde44SuzjKnCEN8Zg5zfcaKmJOJDGtk90EsWBhr8CgVvqbCRLckDLMDx/oQd06NLKJ6R8jupJHK+9dAFyHPQKDmkMWj3gnCgdY/lkEe477UyzJgQNGs5slDsCt6i33WyFau2Q8N6Ulo8Bg6/zu2H1YBo7mig7snE9hxH9mKVGCuHIBktk55IGArQQgMEBgsPVeQTkQAAEQCCkEIDC4eKZCi8Cg/J9WLpCaFvTwDpgXEPTP/3lPl+88p3/+/UQ/J4hByeLFoAjhwwakSotrqw5YTVuO3RAB8vpT43qVLc7hDQgEdwKfPn2m12/eUpxYMa129du3b8Q+o8OHCyeDNxoL2iIwsNHlwcOnFD9ebAtXDMa69O852CIbXvTuEPTn1fHT5y/p9eu3lCB+HBkAU+UbX3kcL17+I30yswEICQRAIHAIrNnwF1Wt00lW3qpCFvJsUTRwGrJSK8dEqjt8PT14/pbKC6PiGoNR0cplyHYjAkpg2LhyEpUu7u1eRw1PGcOXzhtN1SqVsMkYPmxAe+resbG2Ul/tBFB18msLj0E0c+4qGj20k3jOxKDGrfrJeALP7+zTAiJzuS9fvtKgkdPo48dPIl5Dc38FBr7mxd39FP0HH5ee7/59TwlTFZfBpu9e3k4J4sWh+CmL0pOnL7TYEnydSrv3HaPNYudFrmwZqErF4nYLDMpIz/UZ41Tws/XHxAVkX5bNH01VK9rG1FGBIWzYsHK3M4sIbNzPnT2jGibdFfEakv0Xr8G4g8EWgUHtxPBvfiuUKaIF9VYxMLROiINZ81fJmBQcjyJLpl/0p3AMAnYTgMAAgcHumwYXgAAIgEAwJwCBwcUTFFoEBn0wt3tL2lDMaN+5mLz15nllZOoGM2QBYxA/61fhDAgEDYHJXkto2sxlMqgji18c/JADDyZNHJ9mCz/TLTsMlqsfuTe8CrB7p8bUoXVdrXPs7mSop5dchakyefUju0AY1LuN9DnMvqw5zgP/0OfE9XRu34C6iP857dl/nP5csoE2igCwbPjg64sUykG/lchPzRpW1QJkjpk4jzzHz6Np4/rS5Wu3aPKMJXTv/mNZR4G8WWnBrOHEKwH1af2m3dS2y3CtHJ+rVrkEeQ7pTIl1AShv3L5HHl1H0qat++Tl3IcKZQvLIJjs2gwJBEDA+QSWr9lGtRp0kxWXy5OSprQrSbGiR3Z+Q4YaF+28QC3HbaGv3/5HFURw1dUiyCpS6COgBAY2FO/aNJsiR44kIegN5dfObqTkSRPaJTDofe+fObRCi3Vw7cZdzeB8dM8iih07JqVI/5ts0yhy8LO5fZcRlEKsuL96ZoPVyVE7GLhA2xa/0/hR3bWy/YdNoSEjZ8g6rpxeL5+lKn5AvtyZZfDkCBHCy/IvX/1DOQvVlrEJeKdtjSql7BYY2CVRsnSl5bPeuKBm6cotVLtxD/l8v391B0WNEtkmpgERGCLFyibHxvEnypX23iX1+fMXKfLMW7hOnnNEYLB1fjP9moay5KtOFy7dkN+dhvX3DnDPDZ84fUHy5uPbF7f6+u7C+UggYA8BCAwQGOy5X1AWBEAABEICAQgMLp6l0CIwMGblJmlwo4LUqVoOF5O33vzQhQdp2KJDcI9kHRHOuJCAWomnVl7yiscf4ueRPWIjO4sCbLy//+CJFhRRrejkQhz4kN0icNliRXJR+PDhpGsIPtesUVUpBrBriQVCQOAf2Zw4iCL7tW7eqBodPfk35SniLVhwfsqfk9BJ8cNblWVjCRtNOKm+qn6xSwgWQg4cPi3Pp06ZlPTul1g4adN5mHaO3SAcPnZWvs+aKS3t3z6PIkWKSIePn6XSFVvJsbIxJ3WqZHTg0Cn5nsWQbWunB6r7KNkh/AMCoZQAuxGp36w3PX7ynBLE+p5GNitMVQqkCRQar999FG4R99Lszd5/B5o2rELTx/cLlLZQafAnoAQG7im7BGL/+2/e/iufaZzHuww6ta3PhzYZw9VzlMs3aNFHPvf4ecXPu+cvXtHi5Zv5lBSvVy0cKw3+Q0d7UT/hKokTP+uSJolP7EJsxertMm/TyslUqri3WyOZYfhHLzDwKY5NkCtHBmL3h0ow16/gf/7iNf2coYx8vrGwUql8UXr/4SPNmL1CCvwsPPy1cZZ8ltvrIonbn+K1lNoJUZ8Tux1jrleu3SavOStl3kTPntS6WU15bIvbKUcFBt7VqH4n8HOc+8LiArtX5f6oxMGueY4vXL4u3RnZsoOBr7V1fvccOEFFyzSRzfGukDw5M9LtOw+Jd2Jz6tG5CQ3t104e4x8QAAHHCNy7vJVeP4PA4Bg9XAUCIAACwZcABAYXz01oEhg2bNlDFWt6UILY39OVuS3EDzUXwzdpnmM88O6Fl28+0LZ106lYoVwmpZAFAq4joIz2yjCiFxjYYL9iwR+UPu3P0md0vWa9pIGkZdMa0r3CB2GUiBrX+56+dHIdpRLiAKfb9x5qqzLfPDosXRJZc5HUo/84Gj1uLvGP/DHDumggVL+KFMpJO9Z57wBSeVxoQK9W1LNzU2kEOXv+CmXJW0Ney0HUeccBu05KmbGsNKKMGd5F23WhX0G6XXwmC+XPQdkL1qKzf18hj9Z1aNTgTrJONkTUbdZTGnnMDA5aR3EAAiAQYAL8N6O1CKi7ZfsBWVf5vCmpS7WclD2NpV/8gDQ0ee1JGrX0MD17/V5WM3ZkN2rfsnZAqsS1IZyAEhg4zkLfwZMsRsPG/j+Gd9XcFqkAxiomg76w2ikwYlAH6urRUJ7iZwg/38ZNXqAvSo3rV6bRQzpRjOjRZD67DpoojM2deoy2KJfx19Q0QsQq8Etc4AuUwMDP60IFsmuGfD7H4sa4Ud2oYZ2K/FZL/Bxs0Ly3JrirE9WrlKQJo3poAa9ZfM9XrL7YUejzHFZlebchu1syxn5gV4ccs4ADZBsTB5quV8vHraktTPWLDLi+nbuPUMmKLahksby0edUUrYl/3ryjmIm8hZgPz47L3ZP3HjymyrU60MkzF7Vy3N85UwfR9Zv3ZOBmPrFr8yz6/OmLrJd3Lq5eZLmjiV0ZNW83SO7M5ODUnGydXy7Lfa7btKcUcPg9J54bjkXBuzR5YQYSCICA4wQgMDjODleCAAiAQHAmAIHBxbMTmgQGRl3ot8a0/+BJ6lsvH/WoldvF9H0333/efvJcdoRKFM1DW1ZP9V0AOSDgYgLKaG8mMOzc4EWFC/jsDuLVkOVrtJM7EA7smC9jLiROU0KOQPlUVsPZsmM/PX32ksqULCjjGVgTGOYsWCMNJLyi8efkidXlpFb9sZHl1IFlMl/1lXdU7N48WyvLBzkK/i6NCLzboFjhXHJ1oEe3kcTXn9y/VHOzxGW5Hl5N2btrMymKlKnahrPJ6MKMY0qo8T289pdmdJGF8Q8IgIDTCYyfupD6DZ5Mb9/9K+suk+tnqlcsPVXIl8qhtq4/eEVLd12kOdvO0oNnb2Udv5XMT8MGeFDG9I7V6VBHcFGwJKAEBg70ywbfE2cu0BthqE6fNqWMV+CMTrNof+7CVRngl4Mqx/oxumm1bCDnck8eP6f4CeJQjiy/auKG6QX/ZSqBQT0redfh0eN/U6TvIlKGdKksYjLo62Fh48at+3Th4nWKHCUScSBpo4tBfXl7j7kf5y9dpwf3n1DKlEkoTcpkcsegvfUEtDzHsuBFCI8ePaP06VNaBPNmkYT//yV1coeN/LbOL8eyunj5Jl2/cYdixYpB2bKko++jRgno8HA9CICAIACBAbcBCIAACLgnAQgMLp7X0CYwbN1xkMpUbS2pH5/akNImieXiGfBp/tTVx5S/g/fKtV2bZlHBfN6+YH1K4AgEXE9AGe3NBIZ/Hh6SfpJVL5UhQ+8TOku+GnL1P5dhw3/Z0gWlKMEuiMKF8wmUbk1gUHWzC6bd+4/RrdsP6Obt+3LnABsolNGEy6m+9u7WTMZ3UNfya61G3Wj5qm2kfEcrP9MtmlSnKX/01he1OB47+U/q0muMXIXJuyKMqWmbATJrz9Y5lD93FuNpvAcBEHAyAQ7IPmrsHJowdZEIdPtF1h7j+0hUPFtyypcuIWVOGZdSJYzpK/bSNxFP4fbj13ThM9tpBAAAG/9JREFU9jM6eukR7T57h45ffqj1jl2wdWpXTwaX1TJxEKoJ6AUGdqEXEpN6LuuflSFxHOgzCIAACDhKAAKDo+RwHQiAAAgEbwIQGFw8P6FNYGDczdoNpNnzV1PhzElo49DqLp4Bn+aLdV1Mhy88oNbNa9LE0T19TuAIBIIRAWW0NxMYvr72jm2gustxETLkqmIRdPLR42fUV/iP5s+gPrHP46H92xEHjuZkTWBg48jgUdM1f9OqDnZjwAGc9UYT1VfPYZ2pY5t6qqh8rd2kBy1dsUUTGMpWayvcreynCZ49qE2zWhZl9W/adhkmA1Dr88yOt6yZSiWKeMemMDuPPBAAAecSePHyNc1ZsJY4OOyJUxd8VR4lUgSKFiWiEDLD0IdPX+nFP96uj/QFOWhvVeH3vG6tcvj86sHgWBKAwIAbAQRAAARCPgEIDCF/DjECEAABEDAjAIHBjEoQ5oVGgYG3s2ctUJNuCH+qbStlkwEigxC5aVMdpuwgr41nRHC7n6V7lggRwpuWQyYIuJqAMto7KjCo/vNug917j9GufcdknAZ2O8BJuVkyExjev/9Iv2SrIIUEXl3cuH4lyi7cQqRInlDuYuC4CmYCgzHoI7djFBhqNuwqRYueXZrSEOFfW5/YRzT/H0YEbmFxZLjnTOLAlqOHdtYXszhm9xE/RItqkYc3IAACQUOAg7Lu2nNUBmQ/9/dV4T/9LrFLGWNKnCiedHeSNXNaKpAnq/TTrt9JZSyP96GbAASG0D3/GD0IgIB7EIDA4B7ziFGAAAiAgJEABAYjkSB+HxoFBka8a+9RKl6+uaQ9qGEB6lw9ZxCT92lu2KJDNHThQZmxb/s8ypszk89JHIFAMCMQEIGBA0CuXrdT3OOZqWK5ItrIvn79RkXKNKYDh0/LYMx9u7cw3cHAgRc5dgL7vn5wbacMBq0q4VXLtRv3cFhgUOPKkTU9Hd61UFUrX1t4DKKZc1cJQaGT8Icdgxq36if78PzOPgu3Tuy7edDIafTx4ycRr6E5BAYLingDAq4lwHEa3rz9l76JvzeRIkUkdnETNmwY13YKrYcoAsfFzphPnz9TrmwZLP72h6RBsFB/6twl4c8/sogrkjokdR19BQEQAAGnEIDA4BSMqAQEQAAEgh0BCAwunpLQKjAw9jkL11LT1v3lDLhKZNCLCwtmDaffq/3m4jsCzYOA3wSUId6RHQxK2EudMimdO7LKIkhiYRGAfZ8IwD7RsydxAGflJ5pdJ92/skMYAsPSkePnKG8xb1dH+iDK7HapfPV2MmizozsYjp08T7mL1JGDP3NoBf2aLqU8vnbjLqXJUl4eH92ziGLHjkkp0nt/TjeunESli+fXgE32WkLtu4ywcAmlncQBCIAACIAACIAACIAACICASwlAYHApfjQOAiAAAoFGAAJDoKG1rWJnCAxf71+mr/cvmTYYIXVuChMjrum54JCpDILclzbCXdKoZoWDrFsdpu4krw3ePuunT+hHTRtUCbK20RAIOEogIAIDr5zMmLsq3bh1j9jFUe2aZSiM+G/7rkO0buNu2aVrZzdS8qQJ6d/3HyhavNwyr9xvhaiJiM1QrEguSpiquFiF7B3MuVqlEnTrzgMZrJnzVBoxqAN1blefBgyfSkNHeZEtLpL42gYt+tCCJRvk7oQGdSrQ8xevpPsmPlehbGFatXCsdJM0dLQX9ROukji1bfE7JU0SX4ofK1Zvl3mbVk6mUsXzyWP8AwIgAAIgAAIgAAIgAAIgEDwIQGAIHvOAXoAACICAswlAYHA2UTvrc4bAYGeTwa74XLGTocl/OxkKZUpCni2KULqksQOtn2euP6HO0/+iQ+fvyzb+nDmMalcvE2jtoWIQcCaB/sOm0JCRM4iN+F09GloIAcYgz5eu3KT0OSpbrOg/deYS1W/eizgAtD5xkOaZkwdYBFbtOWA8jRo7RxZTsRH27D9OtRp2IxWzgU9yPIR5M4bSoBHTaP6i9bL8x+cnZDBo7uv4Ud2lECBP/PdP3aY9pXiwdN5oYqGC0+fPX6hH/3E0bvKC/0p5vzSuX5lGD+lEMaJHkxnfvn2jidMXU6ceoy3K8e6JEQM9IC5YUMEbEAABEAABEAABEAABEAgeBCAwBI95QC9AAARAwNkEIDA4m6id9UFg8AbGgWZbtB9E14U7FE596uSl7rVyO9U/88fPX2nk0sM0cvFh2QYHgZ0xsR/lzZVZvsc/IBBaCLAhn10g3bn7kD5/+UJJxA6ADGlTUeTIkXwhePzkOb17/54SxY9LESNGkOd5t8KZv6/Qxw+fiIOzxozxg3YdB3MV8ZgpZYrEWp69B+/+fU/nLlyVuytSpkgi4i5EN62Cg8ZyuSePn1P8BHEohwg4jQCxpqiQCQIgAAIgAAIgAAIgAAIuJwCBweVTgA6AAAiAQKAQgMAQKFhtrxQCgw+rtyL4Y6denjRr3iqZGe/HqNSqfFZqWCoDxY4e2aegnUePXryjudvO0dR1J+nZ6/fy6lZNa9CY4V0p0n8GUzurRHEQAAEQAAEQAAEQAAEQAAEQAAEQAAE7CEBgsAMWioIACIBACCIAgcHFkwWBwfcEsD/4YZ4zae/+E9rJivlSUensKahgxsSULJ75amatsDi48eAV7T17lzYdu04bD1/XThUrnIt6dW1KhfPn0PJwAAIgAAIgAAIgAAIgAAIgAAIgAAIgELgEIDAELl/UDgIgAAKuIgCBwVXk/2sXAoP1Cdi0dR/NFLsZ1m7cZVEobsyolDrRj5QwTjSK+X0kihAuHH36+pVevvlA95++ocv3XtDTV/9aXFO1YnFq2rAqlSyaxyIfb0AABEAABEAABEAABEAABEAABEAABAKfAASGwGeMFkAABEDAFQQgMLiCuq5NCAw6GFYO7z96Qus27KYdYmfDgcOn6emzl1ZK+mTH/SkW5cuThUoUzU0VyxQhfo8EAiAAAiAAAiAAAiAAAiAAAiAAAiDgGgIQGFzDHa2CAAiAQGATgMAQ2IT9qR8Cgz+ATE7fvveQrl2/Qw8ePqFXr98SB6yNGCE8xYgRjRLE/4lSiaCwiRPFM7kSWSAAAiAAAiAAAiAAAiAAAiAAAiAAAq4gAIHBFdTRJgiAAAgEPgEIDIHP2M8WIDD4iQcnQQAEQAAEQAAEQAAEQAAEQAAEQAAE3IAABAY3mEQMAQRAAARMCEBgMIESlFkQGIKSNtoCARAAARAAARAAARAAARAAARAAARBwBQEIDK6gjjZBAARAIPAJQGAIfMZ+tgCBwU88OAkCIAACIAACIAACIAACIAACIAACIOAGBCAwuMEkYgggAAIgYEIAAoMJlKDMgsAQlLTRFgiAAAiAAAiAAAiAAAiAAAiAAAiAgCsIQGBwBXW0CQIgAAKBTwACQ+Az9rMFCAx+4sFJEAABEAABEAABEAABEAABEAABEHAbAt+e3qYvN0+bjid88swUNk5S03PukAmBwR1mEWMAARAAAd8EIDD4ZhKkORAYghQ3GgMBEAABEAABEAABEAABEAABEAABlxH4fH4Pfdg937T97wrXpwjpC5mec4dMCAzuMIsYAwiAAAj4JgCBwTeTIM2BwBCkuNEYCIAACIAACIAACIAACIAACIAACLiMAASGa5Q4c3OKET+7y+YADYMACIAACDiXAAQG5/K0uzYIDHYjwwUgAAIgAAIgAAIgAAIgAAIgAAIgECIJQGCAwBAib1x0GgRAAAT8IACBwQ84QXEKAkNQUEYbIAACIAACIAACIAACIAACIAACIOB6AhAYIDC4/i5ED0AABEDAuQQgMDiXp921QWCwGxkuAAEQAAEQAAEQAAEQAAEQAAEQAIEQSQACAwSGEHnjotMgAAIg4AcBCAx+wAmKUxAYgoIy2gABEAABEAABEAABEAABEAABEAAB1xOAwACBwfV3IXoAAiAAAs4lAIHBuTztrg0Cg93IcAEIgAAIgAAIgAAIgAAIgAAIgAAIhEgCEBggMITIGxedBgEQAAE/CEBg8ANOUJyCwBAUlNEGCIAACIAACIAACIAACIAACIAACLieAAQGCAyuvwvRAxAAARBwLgEIDM7laXdtzhAYPh1dSx+PrTNtO0rZ9hQuWSbTc8gEARAAARAAARAAARAAARAAARAAARAIOgIQGCAwBN3dhpZAAARAIGgIQGAIGs5WW4HAYBUNToAACIAACIAACIAACIAACIAACICAWxGAwACBwa1uaAwGBEAABAQBCAwuvg0gMLh4AtA8CIAACIAACIAACIAACIAACIAACAQRAQgMEBiC6FZDMyAAAiAQZAQgMAQZavOGIDCYc0EuCIAACIAACIAACIAACIAACIAACLgbAQgMEBjc7Z7GeEAABEAAAoOL7wEIDC6eADQPAiAAAiAAAiAAAiAAAiAAAiAAAkFEAAIDBIYgutXQDAiAAAgEGQEIDEGG2rwhCAzmXJALAiAAAiAAAiAAAiAAAiAAAiAAAu5GAAIDBAZ3u6cxHhAAARCAwODiewACg4snAM2DAAiAAAiAAAiAAAiAAAiAAAiAQBARgMAAgSGIbjU0AwIgAAJBRgACQ5ChNm8IAoM5F+SCAAiAAAiAAAiAAAiAAAiAAAiAgLsRgMAAgcHd7mmMBwRAAAQgMLj4HghsgSFCqlwUNkZci1GGT56ZwsZJapEXoDfvXtGn83sCVIXx4oi/FiaKEt2Y7fD7b09v05ebpx2+3uzCiDkrmmU7nPf11hn6+uSWw9cbLwwTNQZFSF/ImB2g9/xl+H9ivp2Vwv2UjMIly+Ss6mQ9n46udWp9+Lw4Byc+LwHniM9LwBlyDXi+BJwjni8BZ8g14PniHI54vgScI54vAWfINeD5EnCOoeX5AoEBAkPAPy2oAQRAAASCFwEIDC6ej8AWGMyG913h+k41PH8TRvF3ywebNeVwXtQa/Zwqgvj1Jc7RTkZrM8vRS02v+7hrHn26sNf0nCOZ4YSIFEVwdGb6d9kg+irEGmeliEIAiSTuR2emN5ObOLM6wufFOTjxeQk4R3xeAs6Qa8DzJeAc8XwJOEOuAc8X53DE8yXgHPF8CThDrgHPl4BzDC3PF79+mzr72RDwWXFuDfcub6XXzyAwOJcqagMBEAAB1xOAwODiOYDAYD4B+IJuzsWe3NDyBd3IBAKDkYj97/360WN/bd5XwADkKDmf62AA8mERkCM8XwJCz/taPF8CzpBrcLYRCQs+nDMvWPDhHI74PhZwjvg+FnCGXENwXCDl19w6+9ngHIrOqwUCg/NYoiYQAAEQCE4EIDC4eDZcIjAUaUAR0hV02sgD5wdtf7GDIYnT+vhZ7Az4IHYIODM53WC6e75TXU3BAOSc2f4OnxengMTnJeAYQ4TAgM9LwCda1IDPS8Ax4vMScIZcQ9Qa+D4WUJL4PhZQgt7X4/uYczji+RJwjs54vrBr3I+ntpp2JlKWUk53IWvakIsyITC4CDyaBQEQAIFAJgCBIZAB+1e9SwQGuEjyb1psOu/0L+hwkWQTd/8KYcWcf4T8P+/Xqir/rzYvgc+LORd7cp3xg9bYHj4vRiL2v8fnxX5mZlcExxWmxn7i82IkYv97fF7sZ2Z2BT4vZlTsywucBVJw8WrfLPguHVoFOd8k3DcHAoP7zi1GBgIgELoJQGBw8fxDYDCfALiwMOdiT25o/YIOA5A9d4l5WRiAzLnYmwsDkL3EfJeHAcg3E0dy4PLFEWq+r8HzxTcTe3PwfLGXmHl5PF/MudiTi+eLPbSsl8XzxTobnDEnAIHBnAtyQQAEQCCkE4DA4OIZhMBgPgEQGMy52JMLgcEeWtbLOtsPKn7QWmdtzxn8oLWHlvWyMJhaZ2PrGRhMbSXldzkYTP3mY8tZPF9soeR/GTxf/GdkSwk8X2yh5HcZPF/85mPr2ZDwfLF1LO5QDgKDO8wixgACIAACvglAYPDNJEhznCEw/O/NM/r2z3Ob+x0uZjyiKNFtLu9fwf99/kDfntz2r5hd58P+lIzCRIhk1zV+Fv73NX19+cjPIvaeDJcwjb2X+Fn+f68e07d3r/wsY8/JMBG/E3Esktpzib9lvz29Tf/79MHfcrYWCBs1BoWJEdfW4jaV+3r/sk3lbC2Ez4utpPwuh8+L33xsOYvPiy2U/C+D54v/jPwrgeeLf4RsO4/ni22c/CuF54t/hPw/j+eL/4xsKYHniy2U/C4TWp8vflNxr7MQGNxrPjEaEAABEFAEIDAoEi56dYbA4KKuo1kQAAEQAAEQAAEQAAEQAAEQAAEQAAEQsIkABAabMKEQCIAACIQ4AhAYXDxlEBhcPAFoHgRAAARAAARAAARAAARAAARAAARAINAJQGAIdMRoAARAAARcQgACg0uw+zQKgcGHBY5AAARAAARAAARAAARAAARAAARAAATckwAEBvecV4wKBEAABCAwuPgegMDg4glA8yAAAiAAAiAAAiAAAiAAAiAAAiAAAoFOAAJDoCNGAyAAAiDgEgIQGFyC3adRCAw+LHAEAiAAAiAAAiAAAiAAAiAAAiAAAiDgngQgMLjnvGJUIAACIACBwcX3AAQGF08AmgcBEAABEAABEAABEAABEAABEAABEAh0AhAYAh0xGgABEAABlxCAwOAS7D6NQmDwYYEjEAABEAABEAABEAABEAABEAABEAAB9yQAgcE95xWjAgEQAAEIDC6+ByAwuHgC0DwIgAAIgAAIgAAIgAAIgAAIgAAIgECgE4DAEOiI0QAIgAAIuIQABAaXYPdpFAKDDwscgQAIgAAIgAAIgAAIgAAIgAAIgAAIuCcBCAzuOa8YFQiAAAhAYHDxPQCBwcUTgOZBAARAAARAAARAAARAAARAAARAAAQCnQAEhkBHjAZAAARAwCUEIDC4BLtPoxAYfFjgCARAAARAAARAAARAAARAAARAAARAwD0JQGBwz3nFqEAABEAAAoOL7wEIDC6eADQPAiAAAiAAAiAAAiAAAiAAAiAAAiAQ6AQgMAQ6YjQAAiAAAi4hAIHBJdh9GoXA4MMCRyAAAiAAAiAAAiAAAiAAAiAAAiAAAu5JAAKDe84rRgUCIAACEBhcfA9AYHDxBKB5EAABEAABEAABEAABEAABEAABEACBQCcAgSHQEaMBEAABEHAJAQgMLsHu0ygEBh8WOAIBEAABEAABEAABEAABEAABEAABEHBPAhAY3HNeMSoQAAEQgMDg4nsAAoOLJwDNgwAIgAAIgAAIgAAIgAAIgAAIgAAIBDoBCAyBjhgNgAAIgIBLCEBgcAl2n0YhMPiwwBEIgAAIgAAIgAAIgAAIgAAIgAAIgIB7EoDA4J7zilGBAAiAAAQGF98DEBhcPAFoHgRAAARAAARAAARAAARAAARAAARAINAJQGAIdMRoAARAAARcQgACg0uw+zQKgcGHBY5AAARAAARAAARAAARAAARAAARAAATckwAEBvecV4wKBEAABCAwuPgegMDg4glA8yAAAiAAAiAAAiAAAiAAAiAAAiAAAoFOAAJDoCNGAyAAAiDgEgIQGFyC3adRCAw+LHAEAiAAAiAAAiAAAiAAAiAAAiAAAiDgngQgMLjnvGJUIAACIACBwcX3AAQGF08AmgcBEAABEAABEAABEAABEAABEAABEAh0AhAYAh0xGgABEAABlxCAwOAS7D6NQmDwYYEjEAABEAABEAABEAABEAABEAABEAAB9yQAgcE95xWjAgEQAAEIDC6+B/QCQ7iw4V3cGzQPAiAAAiAAAiAAAiAAAiAAAiAAAiAAAs4n8OrpZXr97BolztycYsTP7vwGUCMIgAAIgIBLCEBgcAl2n0aVwOCTgyMQAAEQAAEQAAEQAAEQAAEQAAEQAAEQcE8CEBjcc14xKhAAgdBLAAKDi+eeBYb/ffvk4l6geRAAARAAARAAARAAARAAARAAARAAARAIfALRE+TGDobAx4wWQAAEQCDICEBgCDLUaAgEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAE3IcABAb3mUuMBARAAARAAARAAARAAARAAARAAARAAARAAARAAARAAASCjAAEhiBDjYZAAARAAARAAARAAARAAARAAARAAARAAARAAARAAARAwH0IQGBwn7nESEAABEAABEAABEAABEAABEAABEAABEAABEAABEAABEAgyAhAYAgy1GgIBEAABEAABEAABEAABEAABEAABEAABEAABEAABEAABNyHAAQG95lLjAQEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAEgowABIYgQ42GQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQMB9CEBgcJ+5xEhAAARAAARAAARAAARAAARAAARAAARAAARAAARAAARAIMgIQGAIMtRoCARAAARAAARAAARAAARAAARAAARAAARAAARAAARAAATchwAEBveZS4wEBEAABEAABEAABEAABEAABEAABEAABEAABEAABEDg/+3ZMQ0AAADCMP+ucbGDVAGhnBDIBBwMGbUgAgQIECBAgAABAgQIECBAgAABAgQIECDwI+Bg+NlSEwIECBAgQIAAAQIECBAgQIAAAQIECBAgkAk4GDJqQQQIECBAgAABAgQIECBAgAABAgQIECBA4EfAwfCzpSYECBAgQIAAAQIECBAgQIAAAQIECBAgQCATcDBk1IIIECBAgAABAgQIECBAgAABAgQIECBAgMCPgIPhZ0tNCBAgQIAAAQIECBAgQIAAAQIECBAgQIBAJuBgyKgFESBAgAABAgQIECBAgAABAgQIECBAgACBHwEHw8+WmhAgQIAAAQIECBAgQIAAAQIECBAgQIAAgUzAwZBRCyJAgAABAgQIECBAgAABAgQIECBAgAABAj8CDoafLTUhQIAAAQIECBAgQIAAAQIECBAgQIAAAQKZgIMhoxZEgAABAgQIECBAgAABAgQIECBAgAABAgR+BBwMP1tqQoAAAQIECBAgQIAAAQIECBAgQIAAAQIEMgEHQ0YtiAABAgQIECBAgAABAgQIECBAgAABAgQI/Ag4GH621IQAAQIECBAgQIAAAQIECBAgQIAAAQIECGQCDoaMWhABAgQIECBAgAABAgQIECBAgAABAgQIEPgRcDD8bKkJAQIECBAgQIAAAQIECBAgQIAAAQIECBDIBBwMGbUgAgQIECBAgAABAgQIECBAgAABAgQIECDwI+Bg+NlSEwIECBAgQIAAAQIECBAgQIAAAQIECBAgkAk4GDJqQQQIECBAgAABAgQIECBAgAABAgQIECBA4EfAwfCzpSYECBAgQIAAAQIECBAgQIAAAQIECBAgQCATcDBk1IIIECBAgAABAgQIECBAgAABAgQIECBAgMCPgIPhZ0tNCBAgQIAAAQIECBAgQIAAAQIECBAgQIBAJuBgyKgFESBAgAABAgQIECBAgAABAgQIECBAgACBHwEHw8+WmhAgQIAAAQIECBAgQIAAAQIECBAgQIAAgUzAwZBRCyJAgAABAgQIECBAgAABAgQIECBAgAABAj8CDoafLTUhQIAAAQIECBAgQIAAAQIECBAgQIAAAQKZwABT4SyKdSfwRQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "display.Image(\"images/mme-gpu.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271e7d9",
   "metadata": {},
   "source": [
    "## How it works?\n",
    "\n",
    "1. SageMaker routes traffic to the right instance behind the endpoint where the target model is loaded. SageMaker takes care of model management behind the endpoint, loads model to the container's memory and unloads the model based on the endpoint's traffic pattern.\n",
    "2. Dynamically loads models from Amazon Simple Storage Service(S3) to the instanceâ€™s storage volume. If the invoked models is not available on instance storage volume, the model is downloaded onto instance storage volume. If the instance storage volume reaches capacity, SageMaker deletes any unused models from the storage volume.\n",
    "3. SageMaker loads the model to NVIDIA Triton containerâ€™s memory on GPU accelerated instance and serve the inference request. If the model is already loaded in the container memory, the subsequents requests are served faster as SageMaker does not need to download and load it again.\n",
    "4. SageMaker takes care of traffic shaping to the MME endpoint, SageMaker continues to routes traffics to the instance where the model is loaded. If the instance resources reach capacity due to high utilization,  SageMaker unloads least used models from the container to free up resource to load more frequently used models.\n",
    "5. SageMaker MME can horizontally scale using auto-scaling policy, provision additional GPU compute instances based on metrics such as GPU utilization, memory utilization etc to serve spiky traffic to MME endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2fbaf5",
   "metadata": {},
   "source": [
    "In this notebook, we will show you how to use the new features Amazon SageMaker MME with GPU with a computer vision use case. For demonstration purpose, we will use a ResNet-50 convolutional neural network pre-trained model  that can classify images into 1000 categories. We will -\n",
    "\n",
    "*  Show how to use NVIDIA Triton inference container on SageMaker MME, leverage different model frameworks such and PyTorch and TensorRT. \n",
    "*  Walk you through steps to convert ResNet-50 models to optimized TensorRT engine format and deploy it with SageMaker MME. \n",
    "* Show how to get insights into instance and invocation metrics using Amazon CloudWatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516f97c",
   "metadata": {},
   "source": [
    "### Installs\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server. Update SageMaker, boto3, awscli etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ac0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker\n",
    "!pip install nvidia-pyindex --quiet\n",
    "!pip install tritonclient[http] --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22828c11",
   "metadata": {},
   "source": [
    "### Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddec97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "# variables\n",
    "s3_client = boto3.client(\"s3\")\n",
    "auto_scaling_client = boto3.client(\"application-autoscaling\")\n",
    "sample_image_name = \"shiba_inu_dog.jpg\"\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# sagemaker variables\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"resnet-mme-gpu\"\n",
    "\n",
    "# endpoint variables\n",
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "\n",
    "# account mapping for SageMaker MME Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8223f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mme_triton_image_uri = \"850464037171.dkr.ecr.ap-south-1.amazonaws.com/tritonserver:22.07-py3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c7b0d",
   "metadata": {},
   "source": [
    "### Creating Model Artifacts\n",
    "\n",
    "This section presents overview of steps to prepare ResNet-50 pre-trained model to be deployed on SageMaker MME using Triton Inference server model configurations. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Note </strong>\n",
    "We are demonstrating deployment with 2 models. However, customers can prepare and 100s of models. The models may or may not share the same framework.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeeb1cc",
   "metadata": {},
   "source": [
    "#### Prepare PyTorch Model \n",
    "\n",
    "`generate_model_pytorch.sh` file in the `workspace` directory contains scripts to generate a PyTorch model. First, we load a pre-trained ResNet50 model using torchvision models package. We save the model as model.pt file in TorchScript optimized and serialized format. TorchScript needs an example inputs to do a model forward pass, so we pass one instance of a RGB image with 3 color channels of dimension 224X224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77ffc96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "== PyTorch ==\n",
      "=============\n",
      "\n",
      "NVIDIA Release 22.07 (build 40241807)\n",
      "PyTorch Version 1.13.0a0+08820cb\n",
      "\n",
      "Container image Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "Copyright (c) 2014-2022 Facebook Inc.\n",
      "Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n",
      "Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n",
      "Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n",
      "Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n",
      "Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n",
      "Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n",
      "Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n",
      "Copyright (c) 2015      Google Inc.\n",
      "Copyright (c) 2015      Yangqing Jia\n",
      "Copyright (c) 2013-2016 The Caffe contributors\n",
      "All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "NOTE: CUDA Forward Compatibility mode ENABLED.\n",
      "  Using CUDA 11.7 driver version 515.48.08 with kernel driver version 510.47.03.\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for PyTorch.  NVIDIA recommends the use of the following flags:\n",
      "   docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "Using cuda device\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 278MB/s]\n",
      "Saved model.pt\n"
     ]
    }
   ],
   "source": [
    "!docker run --gpus=all --rm -it \\\n",
    "            -v `pwd`/workspace:/workspace nvcr.io/nvidia/pytorch:22.07-py3 \\\n",
    "            /bin/bash generate_model_pytorch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32082d09",
   "metadata": {},
   "source": [
    "#### PyTorch Model Respository\n",
    "\n",
    "The model repository contains model to serve, in our case it will be the model.pt and configuration file with input/output specifications and metadata.\n",
    "\n",
    "```\n",
    "resnet\n",
    "â”œâ”€â”€ 1\n",
    "â”‚   â””â”€â”€ model.pt\n",
    "â””â”€â”€ config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153e96d",
   "metadata": {},
   "source": [
    "#### PyTorch Model configuration\n",
    "\n",
    "Model configuration file config.pbtxt must specify name of the model(resnet), the platform and backend properties (pytorch_libtorch), max_batch_size(128) and the input and output tensors along with the data type(TYPE_FP32) information. Additionally, you can specify instance_group and dynamic_batching properties to achieve high performance inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202d179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-pt/resnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7934c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing triton-serve-pt/resnet/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile triton-serve-pt/resnet/config.pbtxt\n",
    "name: \"resnet\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 128\n",
    "input {\n",
    "  name: \"INPUT__0\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: 3\n",
    "  dims: 224\n",
    "  dims: 224\n",
    "}\n",
    "output {\n",
    "  name: \"OUTPUT__0\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd4aca9",
   "metadata": {},
   "source": [
    "#### Prepare TensorRT Model\n",
    "\n",
    "1. We export the pre-trained ResNet model into an ONNX file, which runs the model once to trace its execution and then export the traced model to the specified file. It is one of the better options in terms model conversion and deployment when converting using ONNX.\n",
    "\n",
    "2. We use `trtexec` to automatically convert ONNX model to TensorRT plan. As ONNX is framework agnostic it works with models in TF, PyTorch and more. You will export the weights of your model from the framework and load them into your TensorRT network.\n",
    "\n",
    "\n",
    "In this step, we load pre-trained ResNet50 model from torch and convert to onnx representation using torch onnx exporter. Once onnx model is created, we use TensorRT trtexec command to create the model plan to be hosted with Triton. The script for exporting this model can be found [here](./workspace/generate_model_trt.sh). This is run as part of the `generate_model_trt.sh` script from the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fdd3c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "== PyTorch ==\n",
      "=============\n",
      "\n",
      "NVIDIA Release 22.07 (build 40241807)\n",
      "PyTorch Version 1.13.0a0+08820cb\n",
      "\n",
      "Container image Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "Copyright (c) 2014-2022 Facebook Inc.\n",
      "Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n",
      "Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n",
      "Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n",
      "Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n",
      "Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n",
      "Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n",
      "Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n",
      "Copyright (c) 2015      Google Inc.\n",
      "Copyright (c) 2015      Yangqing Jia\n",
      "Copyright (c) 2013-2016 The Caffe contributors\n",
      "All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "NOTE: CUDA Forward Compatibility mode ENABLED.\n",
      "  Using CUDA 11.7 driver version 515.48.08 with kernel driver version 510.47.03.\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\n",
      "\n",
      "NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n",
      "   insufficient for PyTorch.  NVIDIA recommends the use of the following flags:\n",
      "   docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 ...\n",
      "\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 189MB/s]\n",
      "Saved model.onnx\n",
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8401] # trtexec --onnx=model.onnx --saveEngine=model.plan --explicitBatch --minShapes=input:1x3x224x224 --optShapes=input:128x3x224x224 --maxShapes=input:128x3x224x224 --fp16 --verbose\n",
      "[10/18/2022-15:53:38] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[10/18/2022-15:53:38] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[10/18/2022-15:53:38] [I] === Model Options ===\n",
      "[10/18/2022-15:53:38] [I] Format: ONNX\n",
      "[10/18/2022-15:53:38] [I] Model: model.onnx\n",
      "[10/18/2022-15:53:38] [I] Output:\n",
      "[10/18/2022-15:53:38] [I] === Build Options ===\n",
      "[10/18/2022-15:53:38] [I] Max batch: explicit batch\n",
      "[10/18/2022-15:53:38] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[10/18/2022-15:53:38] [I] minTiming: 1\n",
      "[10/18/2022-15:53:38] [I] avgTiming: 8\n",
      "[10/18/2022-15:53:38] [I] Precision: FP32+FP16\n",
      "[10/18/2022-15:53:38] [I] LayerPrecisions: \n",
      "[10/18/2022-15:53:38] [I] Calibration: \n",
      "[10/18/2022-15:53:38] [I] Refit: Disabled\n",
      "[10/18/2022-15:53:38] [I] Sparsity: Disabled\n",
      "[10/18/2022-15:53:38] [I] Safe mode: Disabled\n",
      "[10/18/2022-15:53:38] [I] DirectIO mode: Disabled\n",
      "[10/18/2022-15:53:38] [I] Restricted mode: Disabled\n",
      "[10/18/2022-15:53:38] [I] Build only: Disabled\n",
      "[10/18/2022-15:53:38] [I] Save engine: model.plan\n",
      "[10/18/2022-15:53:38] [I] Load engine: \n",
      "[10/18/2022-15:53:38] [I] Profiling verbosity: 0\n",
      "[10/18/2022-15:53:38] [I] Tactic sources: Using default tactic sources\n",
      "[10/18/2022-15:53:38] [I] timingCacheMode: local\n",
      "[10/18/2022-15:53:38] [I] timingCacheFile: \n",
      "[10/18/2022-15:53:38] [I] Input(s)s format: fp32:CHW\n",
      "[10/18/2022-15:53:38] [I] Output(s)s format: fp32:CHW\n",
      "[10/18/2022-15:53:38] [I] Input build shape: input=1x3x224x224+128x3x224x224+128x3x224x224\n",
      "[10/18/2022-15:53:38] [I] Input calibration shapes: model\n",
      "[10/18/2022-15:53:38] [I] === System Options ===\n",
      "[10/18/2022-15:53:38] [I] Device: 0\n",
      "[10/18/2022-15:53:38] [I] DLACore: \n",
      "[10/18/2022-15:53:38] [I] Plugins:\n",
      "[10/18/2022-15:53:38] [I] === Inference Options ===\n",
      "[10/18/2022-15:53:38] [I] Batch: Explicit\n",
      "[10/18/2022-15:53:38] [I] Input inference shape: input=128x3x224x224\n",
      "[10/18/2022-15:53:38] [I] Iterations: 10\n",
      "[10/18/2022-15:53:38] [I] Duration: 3s (+ 200ms warm up)\n",
      "[10/18/2022-15:53:38] [I] Sleep time: 0ms\n",
      "[10/18/2022-15:53:38] [I] Idle time: 0ms\n",
      "[10/18/2022-15:53:38] [I] Streams: 1\n",
      "[10/18/2022-15:53:38] [I] ExposeDMA: Disabled\n",
      "[10/18/2022-15:53:38] [I] Data transfers: Enabled\n",
      "[10/18/2022-15:53:38] [I] Spin-wait: Disabled\n",
      "[10/18/2022-15:53:38] [I] Multithreading: Disabled\n",
      "[10/18/2022-15:53:38] [I] CUDA Graph: Disabled\n",
      "[10/18/2022-15:53:38] [I] Separate profiling: Disabled\n",
      "[10/18/2022-15:53:38] [I] Time Deserialize: Disabled\n",
      "[10/18/2022-15:53:38] [I] Time Refit: Disabled\n",
      "[10/18/2022-15:53:38] [I] Inputs:\n",
      "[10/18/2022-15:53:38] [I] === Reporting Options ===\n",
      "[10/18/2022-15:53:38] [I] Verbose: Enabled\n",
      "[10/18/2022-15:53:38] [I] Averages: 10 inferences\n",
      "[10/18/2022-15:53:38] [I] Percentile: 99\n",
      "[10/18/2022-15:53:38] [I] Dump refittable layers:Disabled\n",
      "[10/18/2022-15:53:38] [I] Dump output: Disabled\n",
      "[10/18/2022-15:53:38] [I] Profile: Disabled\n",
      "[10/18/2022-15:53:38] [I] Export timing to JSON file: \n",
      "[10/18/2022-15:53:38] [I] Export output to JSON file: \n",
      "[10/18/2022-15:53:38] [I] Export profile to JSON file: \n",
      "[10/18/2022-15:53:38] [I] \n",
      "[10/18/2022-15:53:38] [I] === Device Information ===\n",
      "[10/18/2022-15:53:38] [I] Selected Device: Tesla T4\n",
      "[10/18/2022-15:53:38] [I] Compute Capability: 7.5\n",
      "[10/18/2022-15:53:38] [I] SMs: 40\n",
      "[10/18/2022-15:53:38] [I] Compute Clock Rate: 1.59 GHz\n",
      "[10/18/2022-15:53:38] [I] Device Global Memory: 14910 MiB\n",
      "[10/18/2022-15:53:38] [I] Shared Memory per SM: 64 KiB\n",
      "[10/18/2022-15:53:38] [I] Memory Bus Width: 256 bits (ECC enabled)\n",
      "[10/18/2022-15:53:38] [I] Memory Clock Rate: 5.001 GHz\n",
      "[10/18/2022-15:53:38] [I] \n",
      "[10/18/2022-15:53:38] [I] TensorRT version: 8.4.1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::BatchTilePlugin_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::CoordConvAC version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::CropAndResize version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::CropAndResizeDynamic version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::DecodeBbox3DPlugin version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::EfficientNMS_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::FlattenConcat_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::GenerateDetection_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::GridAnchor_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::GridAnchorRect_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::LReLU_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::MultilevelProposeROI_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::NMS_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::NMSDynamic_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::Normalize_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::PillarScatterPlugin version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::PriorBox_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::Proposal version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::ProposalDynamic version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::Region_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::Reorg_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::RPROI_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::ScatterND version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::Split version 1\n",
      "[10/18/2022-15:53:38] [V] [TRT] Registered plugin creator - ::VoxelGeneratorPlugin version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:38] [I] [TRT] [MemUsageChange] Init CUDA: CPU +311, GPU +0, now: CPU 319, GPU 241 (MiB)\n",
      "[10/18/2022-15:53:39] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +207, GPU +68, now: CPU 545, GPU 309 (MiB)\n",
      "[10/18/2022-15:53:39] [I] Start parsing network model\n",
      "[10/18/2022-15:53:40] [I] [TRT] ----------------------------------------------------------------\n",
      "[10/18/2022-15:53:40] [I] [TRT] Input filename:   model.onnx\n",
      "[10/18/2022-15:53:40] [I] [TRT] ONNX IR version:  0.0.6\n",
      "[10/18/2022-15:53:40] [I] [TRT] Opset version:    11\n",
      "[10/18/2022-15:53:40] [I] [TRT] Producer name:    pytorch\n",
      "[10/18/2022-15:53:40] [I] [TRT] Producer version: 1.13.0\n",
      "[10/18/2022-15:53:40] [I] [TRT] Domain:           \n",
      "[10/18/2022-15:53:40] [I] [TRT] Model version:    0\n",
      "[10/18/2022-15:53:40] [I] [TRT] Doc string:       \n",
      "[10/18/2022-15:53:40] [I] [TRT] ----------------------------------------------------------------\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::BatchTilePlugin_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::BatchedNMS_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::CoordConvAC version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::CropAndResize version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::CropAndResizeDynamic version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::DecodeBbox3DPlugin version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::DetectionLayer_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::EfficientNMS_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::FlattenConcat_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::GenerateDetection_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::GridAnchor_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::GridAnchorRect_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::LReLU_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::NMS_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::NMSDynamic_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::Normalize_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::PillarScatterPlugin version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::PriorBox_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::ProposalLayer_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::Proposal version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::ProposalDynamic version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::PyramidROIAlign_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::Region_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::Reorg_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::ResizeNearest_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::RPROI_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::ScatterND version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::SpecialSlice_TRT version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::Split version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Plugin creator already registered - ::VoxelGeneratorPlugin version 1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Adding network input: input with dtype: float32, dimensions: (-1, 3, 224, 224)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input for ONNX tensor: input\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: fc.weight\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: fc.bias\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_497\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_498\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_500\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_501\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_503\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_504\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_506\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_507\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_509\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_510\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_512\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_513\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_515\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_516\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_518\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_519\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_521\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_522\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_524\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_525\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_527\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_528\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_530\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_531\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_533\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_534\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_536\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_537\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_539\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_540\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_542\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_543\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_545\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_546\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_548\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_549\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_551\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_552\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_554\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_555\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_557\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_558\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_560\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_561\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_563\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_564\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_566\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_567\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_569\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_570\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_572\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_573\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_575\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_576\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_578\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_579\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_581\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_582\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_584\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_585\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_587\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_588\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_590\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_591\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_593\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_594\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_596\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_597\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_599\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_600\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_602\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_603\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_605\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_606\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_608\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_609\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_611\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_612\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_614\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_615\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_617\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_618\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_620\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_621\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_623\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_624\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_626\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_627\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_629\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_630\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_632\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_633\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_635\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_636\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_638\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_639\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_641\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_642\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_644\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_645\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_647\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_648\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_650\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_651\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_653\n",
      "[10/18/2022-15:53:40] [V] [TRT] Importing initializer: onnx::Conv_654\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_0 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_497\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_498\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_0 [Conv] inputs: [input -> (-1, 3, 224, 224)[FLOAT]], [onnx::Conv_497 -> (64, 3, 7, 7)[FLOAT]], [onnx::Conv_498 -> (64)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 3, 224, 224)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_0 for ONNX node: Conv_0\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (7, 7), strides: (2, 2), prepadding: (3, 3), postpadding: (3, 3), dilations: (1, 1), numOutputs: 64\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 64, 112, 112)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.4 for ONNX tensor: input.4\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_0 [Conv] outputs: [input.4 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_1 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.4\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_1 [Relu] inputs: [input.4 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_1 for ONNX node: Relu_1\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::MaxPool_323 for ONNX tensor: onnx::MaxPool_323\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_1 [Relu] outputs: [onnx::MaxPool_323 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: MaxPool_2 [MaxPool]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::MaxPool_323\n",
      "[10/18/2022-15:53:40] [V] [TRT] MaxPool_2 [MaxPool] inputs: [onnx::MaxPool_323 -> (-1, 64, 112, 112)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: MaxPool_2 for ONNX node: MaxPool_2\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.8 for ONNX tensor: input.8\n",
      "[10/18/2022-15:53:40] [V] [TRT] MaxPool_2 [MaxPool] outputs: [input.8 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_3 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.8\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_500\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_501\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_3 [Conv] inputs: [input.8 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_500 -> (64, 64, 1, 1)[FLOAT]], [onnx::Conv_501 -> (64)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_3 for ONNX node: Conv_3\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.16 for ONNX tensor: input.16\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_3 [Conv] outputs: [input.16 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_4 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.16\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_4 [Relu] inputs: [input.16 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_4 for ONNX node: Relu_4\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_327 for ONNX tensor: onnx::Conv_327\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_4 [Relu] outputs: [onnx::Conv_327 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_5 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_327\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_503\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_504\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_5 [Conv] inputs: [onnx::Conv_327 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_503 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_504 -> (64)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_5 for ONNX node: Conv_5\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.24 for ONNX tensor: input.24\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_5 [Conv] outputs: [input.24 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_6 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.24\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_6 [Relu] inputs: [input.24 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_6 for ONNX node: Relu_6\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_330 for ONNX tensor: onnx::Conv_330\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_6 [Relu] outputs: [onnx::Conv_330 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_7 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_330\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_506\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_507\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_7 [Conv] inputs: [onnx::Conv_330 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_506 -> (256, 64, 1, 1)[FLOAT]], [onnx::Conv_507 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_7 for ONNX node: Conv_7\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_505 for ONNX tensor: onnx::Add_505\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_7 [Conv] outputs: [onnx::Add_505 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_8 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.8\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_509\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_510\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_8 [Conv] inputs: [input.8 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_509 -> (256, 64, 1, 1)[FLOAT]], [onnx::Conv_510 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_8 for ONNX node: Conv_8\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_508 for ONNX tensor: onnx::Add_508\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_8 [Conv] outputs: [onnx::Add_508 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_9 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_505\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_508\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_9 [Add] inputs: [onnx::Add_505 -> (-1, 256, 56, 56)[FLOAT]], [onnx::Add_508 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_9 for ONNX node: Add_9\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_335 for ONNX tensor: onnx::Relu_335\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_9 [Add] outputs: [onnx::Relu_335 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_10 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_335\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_10 [Relu] inputs: [onnx::Relu_335 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_10 for ONNX node: Relu_10\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.36 for ONNX tensor: input.36\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_10 [Relu] outputs: [input.36 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_11 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.36\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_512\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_513\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_11 [Conv] inputs: [input.36 -> (-1, 256, 56, 56)[FLOAT]], [onnx::Conv_512 -> (64, 256, 1, 1)[FLOAT]], [onnx::Conv_513 -> (64)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_11 for ONNX node: Conv_11\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.44 for ONNX tensor: input.44\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_11 [Conv] outputs: [input.44 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_12 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.44\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_12 [Relu] inputs: [input.44 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_12 for ONNX node: Relu_12\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_339 for ONNX tensor: onnx::Conv_339\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_12 [Relu] outputs: [onnx::Conv_339 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_13 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_339\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_515\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_516\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_13 [Conv] inputs: [onnx::Conv_339 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_515 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_516 -> (64)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_13 for ONNX node: Conv_13\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.52 for ONNX tensor: input.52\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_13 [Conv] outputs: [input.52 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_14 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.52\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_14 [Relu] inputs: [input.52 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_14 for ONNX node: Relu_14\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_342 for ONNX tensor: onnx::Conv_342\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_14 [Relu] outputs: [onnx::Conv_342 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_15 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_342\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_518\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_519\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_15 [Conv] inputs: [onnx::Conv_342 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_518 -> (256, 64, 1, 1)[FLOAT]], [onnx::Conv_519 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_15 for ONNX node: Conv_15\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_517 for ONNX tensor: onnx::Add_517\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_15 [Conv] outputs: [onnx::Add_517 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_16 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_517\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.36\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_16 [Add] inputs: [onnx::Add_517 -> (-1, 256, 56, 56)[FLOAT]], [input.36 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_16 for ONNX node: Add_16\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_345 for ONNX tensor: onnx::Relu_345\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_16 [Add] outputs: [onnx::Relu_345 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_17 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_345\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_17 [Relu] inputs: [onnx::Relu_345 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_17 for ONNX node: Relu_17\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.60 for ONNX tensor: input.60\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_17 [Relu] outputs: [input.60 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_18 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.60\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_521\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_522\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_18 [Conv] inputs: [input.60 -> (-1, 256, 56, 56)[FLOAT]], [onnx::Conv_521 -> (64, 256, 1, 1)[FLOAT]], [onnx::Conv_522 -> (64)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_18 for ONNX node: Conv_18\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.68 for ONNX tensor: input.68\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_18 [Conv] outputs: [input.68 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_19 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.68\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_19 [Relu] inputs: [input.68 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_19 for ONNX node: Relu_19\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_349 for ONNX tensor: onnx::Conv_349\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_19 [Relu] outputs: [onnx::Conv_349 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_20 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_349\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_524\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_525\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_20 [Conv] inputs: [onnx::Conv_349 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_524 -> (64, 64, 3, 3)[FLOAT]], [onnx::Conv_525 -> (64)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_20 for ONNX node: Conv_20\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.76 for ONNX tensor: input.76\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_20 [Conv] outputs: [input.76 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_21 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.76\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_21 [Relu] inputs: [input.76 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_21 for ONNX node: Relu_21\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_352 for ONNX tensor: onnx::Conv_352\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_21 [Relu] outputs: [onnx::Conv_352 -> (-1, 64, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_22 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_352\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_527\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_528\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_22 [Conv] inputs: [onnx::Conv_352 -> (-1, 64, 56, 56)[FLOAT]], [onnx::Conv_527 -> (256, 64, 1, 1)[FLOAT]], [onnx::Conv_528 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 64, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_22 for ONNX node: Conv_22\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_526 for ONNX tensor: onnx::Add_526\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_22 [Conv] outputs: [onnx::Add_526 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_23 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_526\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.60\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_23 [Add] inputs: [onnx::Add_526 -> (-1, 256, 56, 56)[FLOAT]], [input.60 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_23 for ONNX node: Add_23\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_355 for ONNX tensor: onnx::Relu_355\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_23 [Add] outputs: [onnx::Relu_355 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_24 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_355\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_24 [Relu] inputs: [onnx::Relu_355 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_24 for ONNX node: Relu_24\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.84 for ONNX tensor: input.84\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_24 [Relu] outputs: [input.84 -> (-1, 256, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_25 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.84\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_530\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_531\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_25 [Conv] inputs: [input.84 -> (-1, 256, 56, 56)[FLOAT]], [onnx::Conv_530 -> (128, 256, 1, 1)[FLOAT]], [onnx::Conv_531 -> (128)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_25 for ONNX node: Conv_25\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 128, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.92 for ONNX tensor: input.92\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_25 [Conv] outputs: [input.92 -> (-1, 128, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_26 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.92\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_26 [Relu] inputs: [input.92 -> (-1, 128, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_26 for ONNX node: Relu_26\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_359 for ONNX tensor: onnx::Conv_359\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_26 [Relu] outputs: [onnx::Conv_359 -> (-1, 128, 56, 56)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_27 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_359\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_533\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_534\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_27 [Conv] inputs: [onnx::Conv_359 -> (-1, 128, 56, 56)[FLOAT]], [onnx::Conv_533 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_534 -> (128)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 128, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_27 for ONNX node: Conv_27\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.100 for ONNX tensor: input.100\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_27 [Conv] outputs: [input.100 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_28 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.100\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_28 [Relu] inputs: [input.100 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_28 for ONNX node: Relu_28\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_362 for ONNX tensor: onnx::Conv_362\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_28 [Relu] outputs: [onnx::Conv_362 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_29 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_362\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_536\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_537\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_29 [Conv] inputs: [onnx::Conv_362 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_536 -> (512, 128, 1, 1)[FLOAT]], [onnx::Conv_537 -> (512)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_29 for ONNX node: Conv_29\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 512, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_535 for ONNX tensor: onnx::Add_535\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_29 [Conv] outputs: [onnx::Add_535 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_30 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.84\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_539\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_540\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_30 [Conv] inputs: [input.84 -> (-1, 256, 56, 56)[FLOAT]], [onnx::Conv_539 -> (512, 256, 1, 1)[FLOAT]], [onnx::Conv_540 -> (512)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 56, 56)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_30 for ONNX node: Conv_30\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (2, 2), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 512, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_538 for ONNX tensor: onnx::Add_538\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_30 [Conv] outputs: [onnx::Add_538 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_31 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_535\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_538\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_31 [Add] inputs: [onnx::Add_535 -> (-1, 512, 28, 28)[FLOAT]], [onnx::Add_538 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_31 for ONNX node: Add_31\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_367 for ONNX tensor: onnx::Relu_367\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_31 [Add] outputs: [onnx::Relu_367 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_32 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_367\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_32 [Relu] inputs: [onnx::Relu_367 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_32 for ONNX node: Relu_32\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.112 for ONNX tensor: input.112\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_32 [Relu] outputs: [input.112 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_33 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.112\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_542\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_543\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_33 [Conv] inputs: [input.112 -> (-1, 512, 28, 28)[FLOAT]], [onnx::Conv_542 -> (128, 512, 1, 1)[FLOAT]], [onnx::Conv_543 -> (128)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 512, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_33 for ONNX node: Conv_33\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.120 for ONNX tensor: input.120\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_33 [Conv] outputs: [input.120 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_34 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.120\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_34 [Relu] inputs: [input.120 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_34 for ONNX node: Relu_34\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_371 for ONNX tensor: onnx::Conv_371\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_34 [Relu] outputs: [onnx::Conv_371 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_35 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_371\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_545\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_546\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_35 [Conv] inputs: [onnx::Conv_371 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_545 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_546 -> (128)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_35 for ONNX node: Conv_35\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.128 for ONNX tensor: input.128\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_35 [Conv] outputs: [input.128 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_36 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.128\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_36 [Relu] inputs: [input.128 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_36 for ONNX node: Relu_36\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_374 for ONNX tensor: onnx::Conv_374\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_36 [Relu] outputs: [onnx::Conv_374 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_37 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_374\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_548\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_549\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_37 [Conv] inputs: [onnx::Conv_374 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_548 -> (512, 128, 1, 1)[FLOAT]], [onnx::Conv_549 -> (512)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_37 for ONNX node: Conv_37\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 512, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_547 for ONNX tensor: onnx::Add_547\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_37 [Conv] outputs: [onnx::Add_547 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_38 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_547\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.112\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_38 [Add] inputs: [onnx::Add_547 -> (-1, 512, 28, 28)[FLOAT]], [input.112 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_38 for ONNX node: Add_38\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_377 for ONNX tensor: onnx::Relu_377\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_38 [Add] outputs: [onnx::Relu_377 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_39 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_377\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_39 [Relu] inputs: [onnx::Relu_377 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_39 for ONNX node: Relu_39\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.136 for ONNX tensor: input.136\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_39 [Relu] outputs: [input.136 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_40 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.136\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_551\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_552\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_40 [Conv] inputs: [input.136 -> (-1, 512, 28, 28)[FLOAT]], [onnx::Conv_551 -> (128, 512, 1, 1)[FLOAT]], [onnx::Conv_552 -> (128)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 512, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_40 for ONNX node: Conv_40\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.144 for ONNX tensor: input.144\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_40 [Conv] outputs: [input.144 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_41 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.144\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_41 [Relu] inputs: [input.144 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_41 for ONNX node: Relu_41\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_381 for ONNX tensor: onnx::Conv_381\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_41 [Relu] outputs: [onnx::Conv_381 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_42 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_381\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_554\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_555\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_42 [Conv] inputs: [onnx::Conv_381 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_554 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_555 -> (128)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_42 for ONNX node: Conv_42\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.152 for ONNX tensor: input.152\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_42 [Conv] outputs: [input.152 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_43 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.152\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_43 [Relu] inputs: [input.152 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_43 for ONNX node: Relu_43\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_384 for ONNX tensor: onnx::Conv_384\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_43 [Relu] outputs: [onnx::Conv_384 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_44 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_384\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_557\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_558\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_44 [Conv] inputs: [onnx::Conv_384 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_557 -> (512, 128, 1, 1)[FLOAT]], [onnx::Conv_558 -> (512)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_44 for ONNX node: Conv_44\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 512, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_556 for ONNX tensor: onnx::Add_556\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_44 [Conv] outputs: [onnx::Add_556 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_45 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_556\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.136\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_45 [Add] inputs: [onnx::Add_556 -> (-1, 512, 28, 28)[FLOAT]], [input.136 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_45 for ONNX node: Add_45\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_387 for ONNX tensor: onnx::Relu_387\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_45 [Add] outputs: [onnx::Relu_387 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_46 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_387\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_46 [Relu] inputs: [onnx::Relu_387 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_46 for ONNX node: Relu_46\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.160 for ONNX tensor: input.160\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_46 [Relu] outputs: [input.160 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_47 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.160\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_560\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_561\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_47 [Conv] inputs: [input.160 -> (-1, 512, 28, 28)[FLOAT]], [onnx::Conv_560 -> (128, 512, 1, 1)[FLOAT]], [onnx::Conv_561 -> (128)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 512, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_47 for ONNX node: Conv_47\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.168 for ONNX tensor: input.168\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_47 [Conv] outputs: [input.168 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_48 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.168\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_48 [Relu] inputs: [input.168 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_48 for ONNX node: Relu_48\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_391 for ONNX tensor: onnx::Conv_391\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_48 [Relu] outputs: [onnx::Conv_391 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_49 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_391\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_563\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_564\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_49 [Conv] inputs: [onnx::Conv_391 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_563 -> (128, 128, 3, 3)[FLOAT]], [onnx::Conv_564 -> (128)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_49 for ONNX node: Conv_49\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.176 for ONNX tensor: input.176\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_49 [Conv] outputs: [input.176 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_50 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.176\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_50 [Relu] inputs: [input.176 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_50 for ONNX node: Relu_50\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_394 for ONNX tensor: onnx::Conv_394\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_50 [Relu] outputs: [onnx::Conv_394 -> (-1, 128, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_51 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_394\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_566\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_567\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_51 [Conv] inputs: [onnx::Conv_394 -> (-1, 128, 28, 28)[FLOAT]], [onnx::Conv_566 -> (512, 128, 1, 1)[FLOAT]], [onnx::Conv_567 -> (512)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 128, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_51 for ONNX node: Conv_51\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 512, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_565 for ONNX tensor: onnx::Add_565\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_51 [Conv] outputs: [onnx::Add_565 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_52 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_565\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.160\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_52 [Add] inputs: [onnx::Add_565 -> (-1, 512, 28, 28)[FLOAT]], [input.160 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_52 for ONNX node: Add_52\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_397 for ONNX tensor: onnx::Relu_397\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_52 [Add] outputs: [onnx::Relu_397 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_53 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_397\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_53 [Relu] inputs: [onnx::Relu_397 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_53 for ONNX node: Relu_53\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.184 for ONNX tensor: input.184\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_53 [Relu] outputs: [input.184 -> (-1, 512, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_54 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.184\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_569\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_570\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_54 [Conv] inputs: [input.184 -> (-1, 512, 28, 28)[FLOAT]], [onnx::Conv_569 -> (256, 512, 1, 1)[FLOAT]], [onnx::Conv_570 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 512, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_54 for ONNX node: Conv_54\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.192 for ONNX tensor: input.192\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_54 [Conv] outputs: [input.192 -> (-1, 256, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_55 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.192\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_55 [Relu] inputs: [input.192 -> (-1, 256, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_55 for ONNX node: Relu_55\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_401 for ONNX tensor: onnx::Conv_401\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_55 [Relu] outputs: [onnx::Conv_401 -> (-1, 256, 28, 28)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_56 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_401\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_572\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_573\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_56 [Conv] inputs: [onnx::Conv_401 -> (-1, 256, 28, 28)[FLOAT]], [onnx::Conv_572 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_573 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_56 for ONNX node: Conv_56\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.200 for ONNX tensor: input.200\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_56 [Conv] outputs: [input.200 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_57 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.200\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_57 [Relu] inputs: [input.200 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_57 for ONNX node: Relu_57\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_404 for ONNX tensor: onnx::Conv_404\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_57 [Relu] outputs: [onnx::Conv_404 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_58 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_404\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_575\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_576\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_58 [Conv] inputs: [onnx::Conv_404 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_575 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_576 -> (1024)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_58 for ONNX node: Conv_58\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_574 for ONNX tensor: onnx::Add_574\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_58 [Conv] outputs: [onnx::Add_574 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_59 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.184\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_578\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_579\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_59 [Conv] inputs: [input.184 -> (-1, 512, 28, 28)[FLOAT]], [onnx::Conv_578 -> (1024, 512, 1, 1)[FLOAT]], [onnx::Conv_579 -> (1024)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 512, 28, 28)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_59 for ONNX node: Conv_59\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (2, 2), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_577 for ONNX tensor: onnx::Add_577\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_59 [Conv] outputs: [onnx::Add_577 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_60 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_574\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_577\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_60 [Add] inputs: [onnx::Add_574 -> (-1, 1024, 14, 14)[FLOAT]], [onnx::Add_577 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_60 for ONNX node: Add_60\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_409 for ONNX tensor: onnx::Relu_409\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_60 [Add] outputs: [onnx::Relu_409 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_61 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_409\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_61 [Relu] inputs: [onnx::Relu_409 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_61 for ONNX node: Relu_61\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.212 for ONNX tensor: input.212\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_61 [Relu] outputs: [input.212 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_62 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.212\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_581\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_582\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_62 [Conv] inputs: [input.212 -> (-1, 1024, 14, 14)[FLOAT]], [onnx::Conv_581 -> (256, 1024, 1, 1)[FLOAT]], [onnx::Conv_582 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_62 for ONNX node: Conv_62\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.220 for ONNX tensor: input.220\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_62 [Conv] outputs: [input.220 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_63 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.220\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_63 [Relu] inputs: [input.220 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_63 for ONNX node: Relu_63\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_413 for ONNX tensor: onnx::Conv_413\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_63 [Relu] outputs: [onnx::Conv_413 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_64 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_413\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_584\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_585\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_64 [Conv] inputs: [onnx::Conv_413 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_584 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_585 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_64 for ONNX node: Conv_64\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.228 for ONNX tensor: input.228\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_64 [Conv] outputs: [input.228 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_65 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.228\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_65 [Relu] inputs: [input.228 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_65 for ONNX node: Relu_65\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_416 for ONNX tensor: onnx::Conv_416\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_65 [Relu] outputs: [onnx::Conv_416 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_66 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_416\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_587\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_588\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_66 [Conv] inputs: [onnx::Conv_416 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_587 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_588 -> (1024)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_66 for ONNX node: Conv_66\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_586 for ONNX tensor: onnx::Add_586\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_66 [Conv] outputs: [onnx::Add_586 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_67 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_586\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.212\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_67 [Add] inputs: [onnx::Add_586 -> (-1, 1024, 14, 14)[FLOAT]], [input.212 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_67 for ONNX node: Add_67\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_419 for ONNX tensor: onnx::Relu_419\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_67 [Add] outputs: [onnx::Relu_419 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_68 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_419\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_68 [Relu] inputs: [onnx::Relu_419 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_68 for ONNX node: Relu_68\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.236 for ONNX tensor: input.236\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_68 [Relu] outputs: [input.236 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_69 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.236\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_590\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_591\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_69 [Conv] inputs: [input.236 -> (-1, 1024, 14, 14)[FLOAT]], [onnx::Conv_590 -> (256, 1024, 1, 1)[FLOAT]], [onnx::Conv_591 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_69 for ONNX node: Conv_69\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.244 for ONNX tensor: input.244\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_69 [Conv] outputs: [input.244 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_70 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.244\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_70 [Relu] inputs: [input.244 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_70 for ONNX node: Relu_70\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_423 for ONNX tensor: onnx::Conv_423\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_70 [Relu] outputs: [onnx::Conv_423 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_71 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_423\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_593\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_594\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_71 [Conv] inputs: [onnx::Conv_423 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_593 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_594 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_71 for ONNX node: Conv_71\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.252 for ONNX tensor: input.252\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_71 [Conv] outputs: [input.252 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_72 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.252\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_72 [Relu] inputs: [input.252 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_72 for ONNX node: Relu_72\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_426 for ONNX tensor: onnx::Conv_426\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_72 [Relu] outputs: [onnx::Conv_426 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_73 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_426\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_596\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_597\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_73 [Conv] inputs: [onnx::Conv_426 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_596 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_597 -> (1024)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_73 for ONNX node: Conv_73\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_595 for ONNX tensor: onnx::Add_595\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_73 [Conv] outputs: [onnx::Add_595 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_74 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_595\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.236\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_74 [Add] inputs: [onnx::Add_595 -> (-1, 1024, 14, 14)[FLOAT]], [input.236 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_74 for ONNX node: Add_74\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_429 for ONNX tensor: onnx::Relu_429\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_74 [Add] outputs: [onnx::Relu_429 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_75 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_429\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_75 [Relu] inputs: [onnx::Relu_429 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_75 for ONNX node: Relu_75\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.260 for ONNX tensor: input.260\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_75 [Relu] outputs: [input.260 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_76 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.260\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_599\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_600\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_76 [Conv] inputs: [input.260 -> (-1, 1024, 14, 14)[FLOAT]], [onnx::Conv_599 -> (256, 1024, 1, 1)[FLOAT]], [onnx::Conv_600 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_76 for ONNX node: Conv_76\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.268 for ONNX tensor: input.268\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_76 [Conv] outputs: [input.268 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_77 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.268\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_77 [Relu] inputs: [input.268 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_77 for ONNX node: Relu_77\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_433 for ONNX tensor: onnx::Conv_433\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_77 [Relu] outputs: [onnx::Conv_433 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_78 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_433\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_602\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_603\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_78 [Conv] inputs: [onnx::Conv_433 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_602 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_603 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_78 for ONNX node: Conv_78\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.276 for ONNX tensor: input.276\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_78 [Conv] outputs: [input.276 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_79 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.276\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_79 [Relu] inputs: [input.276 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_79 for ONNX node: Relu_79\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_436 for ONNX tensor: onnx::Conv_436\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_79 [Relu] outputs: [onnx::Conv_436 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_80 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_436\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_605\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_606\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_80 [Conv] inputs: [onnx::Conv_436 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_605 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_606 -> (1024)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_80 for ONNX node: Conv_80\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_604 for ONNX tensor: onnx::Add_604\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_80 [Conv] outputs: [onnx::Add_604 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_81 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_604\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.260\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_81 [Add] inputs: [onnx::Add_604 -> (-1, 1024, 14, 14)[FLOAT]], [input.260 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_81 for ONNX node: Add_81\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_439 for ONNX tensor: onnx::Relu_439\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_81 [Add] outputs: [onnx::Relu_439 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_82 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_439\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_82 [Relu] inputs: [onnx::Relu_439 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_82 for ONNX node: Relu_82\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.284 for ONNX tensor: input.284\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_82 [Relu] outputs: [input.284 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_83 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.284\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_608\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_609\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_83 [Conv] inputs: [input.284 -> (-1, 1024, 14, 14)[FLOAT]], [onnx::Conv_608 -> (256, 1024, 1, 1)[FLOAT]], [onnx::Conv_609 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_83 for ONNX node: Conv_83\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.292 for ONNX tensor: input.292\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_83 [Conv] outputs: [input.292 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_84 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.292\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_84 [Relu] inputs: [input.292 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_84 for ONNX node: Relu_84\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_443 for ONNX tensor: onnx::Conv_443\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_84 [Relu] outputs: [onnx::Conv_443 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_85 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_443\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_611\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_612\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_85 [Conv] inputs: [onnx::Conv_443 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_611 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_612 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_85 for ONNX node: Conv_85\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.300 for ONNX tensor: input.300\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_85 [Conv] outputs: [input.300 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_86 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.300\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_86 [Relu] inputs: [input.300 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_86 for ONNX node: Relu_86\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_446 for ONNX tensor: onnx::Conv_446\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_86 [Relu] outputs: [onnx::Conv_446 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_87 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_446\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_614\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_615\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_87 [Conv] inputs: [onnx::Conv_446 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_614 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_615 -> (1024)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_87 for ONNX node: Conv_87\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_613 for ONNX tensor: onnx::Add_613\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_87 [Conv] outputs: [onnx::Add_613 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_88 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_613\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.284\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_88 [Add] inputs: [onnx::Add_613 -> (-1, 1024, 14, 14)[FLOAT]], [input.284 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_88 for ONNX node: Add_88\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_449 for ONNX tensor: onnx::Relu_449\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_88 [Add] outputs: [onnx::Relu_449 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_89 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_449\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_89 [Relu] inputs: [onnx::Relu_449 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_89 for ONNX node: Relu_89\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.308 for ONNX tensor: input.308\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_89 [Relu] outputs: [input.308 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_90 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.308\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_617\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_618\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_90 [Conv] inputs: [input.308 -> (-1, 1024, 14, 14)[FLOAT]], [onnx::Conv_617 -> (256, 1024, 1, 1)[FLOAT]], [onnx::Conv_618 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_90 for ONNX node: Conv_90\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.316 for ONNX tensor: input.316\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_90 [Conv] outputs: [input.316 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_91 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.316\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_91 [Relu] inputs: [input.316 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_91 for ONNX node: Relu_91\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_453 for ONNX tensor: onnx::Conv_453\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_91 [Relu] outputs: [onnx::Conv_453 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_92 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_453\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_620\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_621\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_92 [Conv] inputs: [onnx::Conv_453 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_620 -> (256, 256, 3, 3)[FLOAT]], [onnx::Conv_621 -> (256)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_92 for ONNX node: Conv_92\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.324 for ONNX tensor: input.324\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_92 [Conv] outputs: [input.324 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_93 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.324\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_93 [Relu] inputs: [input.324 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_93 for ONNX node: Relu_93\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_456 for ONNX tensor: onnx::Conv_456\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_93 [Relu] outputs: [onnx::Conv_456 -> (-1, 256, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_94 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_456\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_623\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_624\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_94 [Conv] inputs: [onnx::Conv_456 -> (-1, 256, 14, 14)[FLOAT]], [onnx::Conv_623 -> (1024, 256, 1, 1)[FLOAT]], [onnx::Conv_624 -> (1024)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 256, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_94 for ONNX node: Conv_94\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_622 for ONNX tensor: onnx::Add_622\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_94 [Conv] outputs: [onnx::Add_622 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_95 [Add]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_622\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.308\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_95 [Add] inputs: [onnx::Add_622 -> (-1, 1024, 14, 14)[FLOAT]], [input.308 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_95 for ONNX node: Add_95\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_459 for ONNX tensor: onnx::Relu_459\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_95 [Add] outputs: [onnx::Relu_459 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_96 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_459\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_96 [Relu] inputs: [onnx::Relu_459 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_96 for ONNX node: Relu_96\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.332 for ONNX tensor: input.332\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_96 [Relu] outputs: [input.332 -> (-1, 1024, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_97 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.332\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_626\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_627\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_97 [Conv] inputs: [input.332 -> (-1, 1024, 14, 14)[FLOAT]], [onnx::Conv_626 -> (512, 1024, 1, 1)[FLOAT]], [onnx::Conv_627 -> (512)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 1024, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_97 for ONNX node: Conv_97\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 512, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.340 for ONNX tensor: input.340\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_97 [Conv] outputs: [input.340 -> (-1, 512, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_98 [Relu]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.340\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_98 [Relu] inputs: [input.340 -> (-1, 512, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_98 for ONNX node: Relu_98\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_463 for ONNX tensor: onnx::Conv_463\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_98 [Relu] outputs: [onnx::Conv_463 -> (-1, 512, 14, 14)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_99 [Conv]\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_463\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_629\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_630\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_99 [Conv] inputs: [onnx::Conv_463 -> (-1, 512, 14, 14)[FLOAT]], [onnx::Conv_629 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_630 -> (512)[FLOAT]], \n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 512, 14, 14)\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_99 for ONNX node: Conv_99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 512, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.348 for ONNX tensor: input.348\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_99 [Conv] outputs: [input.348 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_100 [Relu]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.348\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_100 [Relu] inputs: [input.348 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_100 for ONNX node: Relu_100\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_466 for ONNX tensor: onnx::Conv_466\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_100 [Relu] outputs: [onnx::Conv_466 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_101 [Conv]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_466\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_632\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_633\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_101 [Conv] inputs: [onnx::Conv_466 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_632 -> (2048, 512, 1, 1)[FLOAT]], [onnx::Conv_633 -> (2048)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 512, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_101 for ONNX node: Conv_101\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 2048, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_631 for ONNX tensor: onnx::Add_631\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_101 [Conv] outputs: [onnx::Add_631 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_102 [Conv]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.332\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_635\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_636\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_102 [Conv] inputs: [input.332 -> (-1, 1024, 14, 14)[FLOAT]], [onnx::Conv_635 -> (2048, 1024, 1, 1)[FLOAT]], [onnx::Conv_636 -> (2048)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 1024, 14, 14)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_102 for ONNX node: Conv_102\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (2, 2), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 2048, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_634 for ONNX tensor: onnx::Add_634\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_102 [Conv] outputs: [onnx::Add_634 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_103 [Add]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_631\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_634\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_103 [Add] inputs: [onnx::Add_631 -> (-1, 2048, 7, 7)[FLOAT]], [onnx::Add_634 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_103 for ONNX node: Add_103\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_471 for ONNX tensor: onnx::Relu_471\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_103 [Add] outputs: [onnx::Relu_471 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_104 [Relu]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_471\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_104 [Relu] inputs: [onnx::Relu_471 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_104 for ONNX node: Relu_104\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.360 for ONNX tensor: input.360\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_104 [Relu] outputs: [input.360 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_105 [Conv]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.360\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_638\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_639\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_105 [Conv] inputs: [input.360 -> (-1, 2048, 7, 7)[FLOAT]], [onnx::Conv_638 -> (512, 2048, 1, 1)[FLOAT]], [onnx::Conv_639 -> (512)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 2048, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_105 for ONNX node: Conv_105\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 512, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.368 for ONNX tensor: input.368\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_105 [Conv] outputs: [input.368 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_106 [Relu]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.368\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_106 [Relu] inputs: [input.368 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_106 for ONNX node: Relu_106\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_475 for ONNX tensor: onnx::Conv_475\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_106 [Relu] outputs: [onnx::Conv_475 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_107 [Conv]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_475\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_641\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_642\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_107 [Conv] inputs: [onnx::Conv_475 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_641 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_642 -> (512)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 512, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_107 for ONNX node: Conv_107\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 512, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.376 for ONNX tensor: input.376\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_107 [Conv] outputs: [input.376 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_108 [Relu]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.376\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_108 [Relu] inputs: [input.376 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_108 for ONNX node: Relu_108\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_478 for ONNX tensor: onnx::Conv_478\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_108 [Relu] outputs: [onnx::Conv_478 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_109 [Conv]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_478\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_644\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_645\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_109 [Conv] inputs: [onnx::Conv_478 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_644 -> (2048, 512, 1, 1)[FLOAT]], [onnx::Conv_645 -> (2048)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 512, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_109 for ONNX node: Conv_109\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 2048, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_643 for ONNX tensor: onnx::Add_643\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_109 [Conv] outputs: [onnx::Add_643 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_110 [Add]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_643\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.360\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_110 [Add] inputs: [onnx::Add_643 -> (-1, 2048, 7, 7)[FLOAT]], [input.360 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_110 for ONNX node: Add_110\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_481 for ONNX tensor: onnx::Relu_481\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_110 [Add] outputs: [onnx::Relu_481 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_111 [Relu]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_481\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_111 [Relu] inputs: [onnx::Relu_481 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_111 for ONNX node: Relu_111\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.384 for ONNX tensor: input.384\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_111 [Relu] outputs: [input.384 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_112 [Conv]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.384\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_647\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_648\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_112 [Conv] inputs: [input.384 -> (-1, 2048, 7, 7)[FLOAT]], [onnx::Conv_647 -> (512, 2048, 1, 1)[FLOAT]], [onnx::Conv_648 -> (512)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 2048, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_112 for ONNX node: Conv_112\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 512, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.392 for ONNX tensor: input.392\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_112 [Conv] outputs: [input.392 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_113 [Relu]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.392\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_113 [Relu] inputs: [input.392 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_113 for ONNX node: Relu_113\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_485 for ONNX tensor: onnx::Conv_485\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_113 [Relu] outputs: [onnx::Conv_485 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_114 [Conv]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_485\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_650\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_651\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_114 [Conv] inputs: [onnx::Conv_485 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_650 -> (512, 512, 3, 3)[FLOAT]], [onnx::Conv_651 -> (512)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 512, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_114 for ONNX node: Conv_114\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 512, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.400 for ONNX tensor: input.400\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_114 [Conv] outputs: [input.400 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_115 [Relu]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.400\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_115 [Relu] inputs: [input.400 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_115 for ONNX node: Relu_115\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Conv_488 for ONNX tensor: onnx::Conv_488\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_115 [Relu] outputs: [onnx::Conv_488 -> (-1, 512, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Conv_116 [Conv]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_488\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_653\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Conv_654\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_116 [Conv] inputs: [onnx::Conv_488 -> (-1, 512, 7, 7)[FLOAT]], [onnx::Conv_653 -> (2048, 512, 1, 1)[FLOAT]], [onnx::Conv_654 -> (2048)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution input dimensions: (-1, 512, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Conv_116 for ONNX node: Conv_116\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convolution output dimensions: (-1, 2048, 7, 7)\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Add_652 for ONNX tensor: onnx::Add_652\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Conv_116 [Conv] outputs: [onnx::Add_652 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Add_117 [Add]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Add_652\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.384\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_117 [Add] inputs: [onnx::Add_652 -> (-1, 2048, 7, 7)[FLOAT]], [input.384 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Add_117 for ONNX node: Add_117\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Relu_491 for ONNX tensor: onnx::Relu_491\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Add_117 [Add] outputs: [onnx::Relu_491 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Relu_118 [Relu]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Relu_491\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_118 [Relu] inputs: [onnx::Relu_491 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Relu_118 for ONNX node: Relu_118\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: input.408 for ONNX tensor: input.408\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Relu_118 [Relu] outputs: [input.408 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: GlobalAveragePool_119 [GlobalAveragePool]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: input.408\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] GlobalAveragePool_119 [GlobalAveragePool] inputs: [input.408 -> (-1, 2048, 7, 7)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] GlobalAveragePool operators are implemented via Reduce layers rather than Pooling layers\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: GlobalAveragePool_119 for ONNX node: GlobalAveragePool_119\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Flatten_493 for ONNX tensor: onnx::Flatten_493\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] GlobalAveragePool_119 [GlobalAveragePool] outputs: [onnx::Flatten_493 -> (-1, 2048, 1, 1)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Flatten_120 [Flatten]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Flatten_493\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Flatten_120 [Flatten] inputs: [onnx::Flatten_493 -> (-1, 2048, 1, 1)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [W] [TRT] parsers/onnx/onnx2trt_utils.cpp:367: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Flatten_120 for ONNX node: Flatten_120\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: onnx::Gemm_494 for ONNX tensor: onnx::Gemm_494\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Flatten_120 [Flatten] outputs: [onnx::Gemm_494 -> (-1, 2048)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Parsing node: Gemm_121 [Gemm]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: onnx::Gemm_494\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: fc.weight\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Searching for input: fc.bias\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Gemm_121 [Gemm] inputs: [onnx::Gemm_494 -> (-1, 2048)[FLOAT]], [fc.weight -> (1000, 2048)[FLOAT]], [fc.bias -> (1000)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: fc.weight for ONNX node: fc.weight\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Using opA: 0 opB: 1\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: Gemm_121 for ONNX node: Gemm_121\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering layer: fc.bias for ONNX node: fc.bias\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Registering tensor: output_1 for ONNX tensor: output\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Gemm_121 [Gemm] outputs: [output -> (-1, 1000)[FLOAT]], \r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Marking output_1 as output: output\r\n",
      "[10/18/2022-15:53:40] [I] Finish parsing network model\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Applying generic optimizations to the graph for inference.\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Original: 126 layers\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] After dead-layer removal: 126 layers\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConstShuffleFusion on fc.bias\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConstShuffleFusion: Fusing fc.bias with (Unnamed Layer* 129) [Shuffle]\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] After Myelin optimization: 125 layers\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: MatMulToConvTransform on Gemm_121\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Convert layer type of Gemm_121 from MATRIX_MULTIPLY to CONVOLUTION\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ShuffleShuffleFusion on Flatten_120\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ShuffleShuffleFusion: Fusing Flatten_120 with reshape_before_Gemm_121\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ShuffleErasure on Flatten_120 + reshape_before_Gemm_121\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Removing Flatten_120 + reshape_before_Gemm_121\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReshapeBiasAddFusion on Gemm_121\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Applying ScaleNodes fusions.\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] After scale fusion: 122 layers\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_0\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_0 with Relu_1\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_3\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_3 with Relu_4\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_8\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_8 with Add_9\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_5\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_5 with Relu_6\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_8 + Add_9\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_8 + Add_9 with Relu_10\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_11\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_11 with Relu_12\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_13\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_13 with Relu_14\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_15\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_15 with Add_16\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_15 + Add_16\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_15 + Add_16 with Relu_17\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_18\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_18 with Relu_19\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_20\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_20 with Relu_21\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_22\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_22 with Add_23\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_22 + Add_23\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_22 + Add_23 with Relu_24\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_25\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_25 with Relu_26\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_30\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_30 with Add_31\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_27\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_27 with Relu_28\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_30 + Add_31\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_30 + Add_31 with Relu_32\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_33\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_33 with Relu_34\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_35\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_35 with Relu_36\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_37\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_37 with Add_38\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_37 + Add_38\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_37 + Add_38 with Relu_39\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_40\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_40 with Relu_41\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_42\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_42 with Relu_43\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_44\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_44 with Add_45\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_44 + Add_45\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_44 + Add_45 with Relu_46\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_47\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_47 with Relu_48\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_49\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_49 with Relu_50\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_51\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_51 with Add_52\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_51 + Add_52\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_51 + Add_52 with Relu_53\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_54\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_54 with Relu_55\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_59\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_59 with Add_60\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_56\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_56 with Relu_57\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_59 + Add_60\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_59 + Add_60 with Relu_61\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_62\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_62 with Relu_63\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_64\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_64 with Relu_65\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_66\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_66 with Add_67\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_66 + Add_67\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_66 + Add_67 with Relu_68\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_69\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_69 with Relu_70\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_71\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_71 with Relu_72\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_73\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_73 with Add_74\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_73 + Add_74\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_73 + Add_74 with Relu_75\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_76\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_76 with Relu_77\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_78\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_78 with Relu_79\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_80\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_80 with Add_81\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_80 + Add_81\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_80 + Add_81 with Relu_82\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_83\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_83 with Relu_84\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_85\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_85 with Relu_86\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_87\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_87 with Add_88\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_87 + Add_88\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_87 + Add_88 with Relu_89\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_90\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_90 with Relu_91\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_92\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_92 with Relu_93\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_94\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_94 with Add_95\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_94 + Add_95\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_94 + Add_95 with Relu_96\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_97\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_97 with Relu_98\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_102\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_102 with Add_103\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_99\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_99 with Relu_100\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_102 + Add_103\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_102 + Add_103 with Relu_104\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_105\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_105 with Relu_106\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_107\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_107 with Relu_108\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_109\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_109 with Add_110\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_109 + Add_110\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_109 + Add_110 with Relu_111\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_112\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_112 with Relu_113\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_114\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_114 with Relu_115\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvEltwiseSumFusion on Conv_116\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_116 with Add_117\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ConvReluFusion on Conv_116 + Add_117\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] ConvReluFusion: Fusing Conv_116 + Add_117 with Relu_118\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Running: ReduceToPoolingFusion on GlobalAveragePool_119\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] Swap the layer type of GlobalAveragePool_119 from REDUCE to POOLING\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] After dupe layer removal: 57 layers\r\n",
      "[10/18/2022-15:53:40] [V] [TRT] After final dead-layer removal: 57 layers\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:40] [V] [TRT] After tensor merging: 57 layers\n",
      "[10/18/2022-15:53:41] [V] [TRT] After vertical fusions: 57 layers\n",
      "[10/18/2022-15:53:41] [V] [TRT] After dupe layer removal: 57 layers\n",
      "[10/18/2022-15:53:41] [V] [TRT] After final dead-layer removal: 57 layers\n",
      "[10/18/2022-15:53:41] [V] [TRT] After tensor merging: 57 layers\n",
      "[10/18/2022-15:53:41] [V] [TRT] After slice removal: 57 layers\n",
      "[10/18/2022-15:53:41] [V] [TRT] After concat removal: 57 layers\n",
      "[10/18/2022-15:53:41] [V] [TRT] Trying to split Reshape and strided tensor\n",
      "[10/18/2022-15:53:41] [V] [TRT] Graph construction and optimization completed in 1.16377 seconds.\n",
      "[10/18/2022-15:53:41] [V] [TRT] Using cublasLt as a tactic source\n",
      "[10/18/2022-15:53:41] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +499, GPU +216, now: CPU 1158, GPU 533 (MiB)\n",
      "[10/18/2022-15:53:41] [V] [TRT] Using cuDNN as a tactic source\n",
      "[10/18/2022-15:53:42] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +116, GPU +52, now: CPU 1274, GPU 585 (MiB)\n",
      "[10/18/2022-15:53:42] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[10/18/2022-15:53:42] [V] [TRT] Constructing optimization profile number 0 [1/1].\n",
      "[10/18/2022-15:53:42] [V] [TRT] Reserving memory for host IO tensors. Host: 0 bytes\n",
      "[10/18/2022-15:53:42] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:42] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Float(150528,1,672,3) ***************\n",
      "[10/18/2022-15:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.655653\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.662537\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.655858\n",
      "[10/18/2022-15:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.655653\n",
      "[10/18/2022-15:53:42] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Half(150528,50176,224,1) ***************\n",
      "[10/18/2022-15:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.453166\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.577664\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.592018\n",
      "[10/18/2022-15:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.453166\n",
      "[10/18/2022-15:53:42] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Half(100352,50176:2,224,1) ***************\n",
      "[10/18/2022-15:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.939707\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.707954\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.582277\n",
      "[10/18/2022-15:53:42] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.582277\n",
      "[10/18/2022-15:53:42] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Half(50176,1:4,224,1) ***************\n",
      "[10/18/2022-15:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.17499\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.509888\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.543154\n",
      "[10/18/2022-15:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.509888\n",
      "[10/18/2022-15:53:42] [V] [TRT] *************** Autotuning Reformat: Float(150528,50176,224,1) -> Half(50176,1:8,224,1) ***************\n",
      "[10/18/2022-15:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.04183\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.760658\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.769797\n",
      "[10/18/2022-15:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.760658\n",
      "[10/18/2022-15:53:42] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:42] [V] [TRT] *************** Autotuning Reformat: Float(802816,12544,112,1) -> Half(802816,12544,112,1) ***************\n",
      "[10/18/2022-15:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.4142\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.06228\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.12722\n",
      "[10/18/2022-15:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.4142\n",
      "[10/18/2022-15:53:42] [V] [TRT] *************** Autotuning Reformat: Float(802816,12544,112,1) -> Half(401408,12544:2,112,1) ***************\n",
      "[10/18/2022-15:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.21492\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.44545\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.5954\n",
      "[10/18/2022-15:53:42] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.5954\n",
      "[10/18/2022-15:53:42] [V] [TRT] *************** Autotuning Reformat: Float(802816,12544,112,1) -> Half(100352,1:8,896,8) ***************\n",
      "[10/18/2022-15:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.39103\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.46813\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.46907\n",
      "[10/18/2022-15:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.46813\n",
      "[10/18/2022-15:53:42] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Float(802816,12544,112,1) ***************\n",
      "[10/18/2022-15:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.27351\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.87577\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.06837\n",
      "[10/18/2022-15:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 5.87577\n",
      "[10/18/2022-15:53:42] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Half(802816,12544,112,1) ***************\n",
      "[10/18/2022-15:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.02607\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.37658\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.95882\n",
      "[10/18/2022-15:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.37658\n",
      "[10/18/2022-15:53:42] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Half(401408,12544:2,112,1) ***************\n",
      "[10/18/2022-15:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.33713\n",
      "[10/18/2022-15:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.34117\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.42417\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.34117\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,7168,64) -> Half(100352,1:8,896,8) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.18563\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.426\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.14737\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.426\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Half(802816,12544,112,1) -> Float(802816,12544,112,1) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.59103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.48773\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.09536\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.59103\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Half(802816,12544,112,1) -> Half(401408,12544:2,112,1) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.62338\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.64834\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.34279\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.34279\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Half(802816,12544,112,1) -> Half(100352,1:8,896,8) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.38042\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.58516\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.93029\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.93029\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Half(401408,12544:2,112,1) -> Float(802816,12544,112,1) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.08517\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.25341\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.5907\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.5907\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Half(401408,12544:2,112,1) -> Half(802816,12544,112,1) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.57764\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 6.83739\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.1899\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.1899\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Half(401408,12544:2,112,1) -> Half(100352,1:8,896,8) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.37368\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.95923\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.83647\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.83647\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,896,8) -> Float(802816,12544,112,1) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.71444\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.16241\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.91756\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.91756\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,896,8) -> Half(802816,12544,112,1) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.85007\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.41677\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.65167\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.65167\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,896,8) -> Half(401408,12544:2,112,1) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::MaxPool_323 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.54598\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.63168\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.65171\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.65171\n",
      "[10/18/2022-15:53:43] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.14953\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.22761\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.10957\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.10957\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.602414\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.774802\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.784969\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.602414\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.807442\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.86256\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.626103\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.626103\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.05649\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.617033\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.620411\n",
      "[10/18/2022-15:53:43] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.617033\n",
      "[10/18/2022-15:53:43] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:43] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.650971\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.901705\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.00621\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.650971\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.11337\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.787282\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.09248\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.787282\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.687918\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.67173\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.568352\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.568352\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.05634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.656823\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.467237\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.467237\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.765074\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.02978\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.644187\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.644187\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.897262\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.808658\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.894702\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.808658\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.672622\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.65917\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.555639\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.555639\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.844594\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.473966\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.459109\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.459109\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.29109\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.996594\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.711886\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.711886\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.789065\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.735799\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.775607\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.735799\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.26186\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.26844\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.419872\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.419872\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.8) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.31423\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.66085\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.418798\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.418798\n",
      "[10/18/2022-15:53:44] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.12331\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.15205\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.12318\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.12318\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.602706\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.774437\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.784969\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.602706\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.811502\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.862624\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.626446\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.626446\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.05618\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.618203\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.621157\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.618203\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.51435\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.47105\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.49693\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.47105\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.38591\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.841243\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.38507\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.841243\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.45813\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.841938\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.47596\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.841938\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.778533\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.609527\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.778121\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.609527\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.650962\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.90363\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.00615\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.650962\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.1192\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.78155\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.10974\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.78155\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.693166\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.671159\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.578075\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.578075\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.07812\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.657701\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.473472\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.473472\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.784841\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.03662\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.647154\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.647154\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.919049\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.804407\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.928411\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.804407\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.700005\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.74955\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.57173\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.57173\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.850048\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.473362\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.458505\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.458505\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.29083\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.996704\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.710665\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.710665\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.782642\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.738994\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.761563\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.738994\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.25526\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.25441\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.419794\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.419794\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(input.8 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.34945\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.659817\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.41819\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.41819\n",
      "[10/18/2022-15:53:44] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.57557\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.84139\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.30403\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.84139\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.49622\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.08363\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.12237\n",
      "[10/18/2022-15:53:44] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.49622\n",
      "[10/18/2022-15:53:44] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.24288\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.49076\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.54629\n",
      "[10/18/2022-15:53:45] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.54629\n",
      "[10/18/2022-15:53:45] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.2756\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.49212\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.59812\n",
      "[10/18/2022-15:53:45] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.49212\n",
      "[10/18/2022-15:53:45] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.74858\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.90324\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.47601\n",
      "[10/18/2022-15:53:45] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 5.90324\n",
      "[10/18/2022-15:53:45] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.37456\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.72326\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.28506\n",
      "[10/18/2022-15:53:45] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.72326\n",
      "[10/18/2022-15:53:45] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.70154\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.45371\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.73565\n",
      "[10/18/2022-15:53:45] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.45371\n",
      "[10/18/2022-15:53:45] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.18211\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.48422\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.14979\n",
      "[10/18/2022-15:53:45] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.48422\n",
      "[10/18/2022-15:53:45] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.59662\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.58364\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.08723\n",
      "[10/18/2022-15:53:45] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.59662\n",
      "[10/18/2022-15:53:45] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.51262\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.26545\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.98255\n",
      "[10/18/2022-15:53:45] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.26545\n",
      "[10/18/2022-15:53:45] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.05467\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.78507\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.38327\n",
      "[10/18/2022-15:53:45] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.38327\n",
      "[10/18/2022-15:53:45] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.62789\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.79416\n",
      "[10/18/2022-15:53:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.99621\n",
      "[10/18/2022-15:53:45] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.99621\n",
      "[10/18/2022-15:53:45] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.1091\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.13515\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.57246\n",
      "[10/18/2022-15:53:46] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.57246\n",
      "[10/18/2022-15:53:46] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.93858\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.34916\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.02279\n",
      "[10/18/2022-15:53:46] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.34916\n",
      "[10/18/2022-15:53:46] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.94902\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 6.40722\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.2192\n",
      "[10/18/2022-15:53:46] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.2192\n",
      "[10/18/2022-15:53:46] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.43248\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.06024\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.9851\n",
      "[10/18/2022-15:53:46] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.9851\n",
      "[10/18/2022-15:53:46] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.74293\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.07993\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.01024\n",
      "[10/18/2022-15:53:46] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.01024\n",
      "[10/18/2022-15:53:46] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.8943\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.10633\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.01312\n",
      "[10/18/2022-15:53:46] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.8943\n",
      "[10/18/2022-15:53:46] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.31716\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.21134\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.78167\n",
      "[10/18/2022-15:53:46] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.78167\n",
      "[10/18/2022-15:53:46] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_505 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.53774\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.71388\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.80285\n",
      "[10/18/2022-15:53:46] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.80285\n",
      "[10/18/2022-15:53:46] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:46] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.52654\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.71776\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.33193\n",
      "[10/18/2022-15:53:46] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.71776\n",
      "[10/18/2022-15:53:46] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.49385\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.08343\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.12161\n",
      "[10/18/2022-15:53:46] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.49385\n",
      "[10/18/2022-15:53:46] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.23456\n",
      "[10/18/2022-15:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.49268\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.54546\n",
      "[10/18/2022-15:53:47] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.54546\n",
      "[10/18/2022-15:53:47] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.28372\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.4953\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.59803\n",
      "[10/18/2022-15:53:47] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.4953\n",
      "[10/18/2022-15:53:47] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.74465\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.97631\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.492\n",
      "[10/18/2022-15:53:47] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 5.97631\n",
      "[10/18/2022-15:53:47] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.41857\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.72211\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.28557\n",
      "[10/18/2022-15:53:47] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.72211\n",
      "[10/18/2022-15:53:47] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.72185\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.45601\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.73372\n",
      "[10/18/2022-15:53:47] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.45601\n",
      "[10/18/2022-15:53:47] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.18221\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.48363\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.15467\n",
      "[10/18/2022-15:53:47] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.48363\n",
      "[10/18/2022-15:53:47] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.5954\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.58747\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.0876\n",
      "[10/18/2022-15:53:47] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.5954\n",
      "[10/18/2022-15:53:47] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.51814\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.27387\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.01175\n",
      "[10/18/2022-15:53:47] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.27387\n",
      "[10/18/2022-15:53:47] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.05114\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.78497\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.38432\n",
      "[10/18/2022-15:53:47] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.38432\n",
      "[10/18/2022-15:53:47] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.61093\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.79808\n",
      "[10/18/2022-15:53:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.0055\n",
      "[10/18/2022-15:53:47] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.0055\n",
      "[10/18/2022-15:53:47] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:47] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.13871\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.13391\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.57158\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.57158\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.88231\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.39233\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.88184\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.39233\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.62232\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 6.6689\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.35555\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.35555\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.62763\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.16283\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.00784\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.00784\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.8181\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.08166\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.00705\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.00705\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.90514\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.10226\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.03373\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.90514\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.3191\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.21912\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.78023\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.78023\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.36) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.58694\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.71635\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.80224\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.80224\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,3584,64) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,3136:2,56,1) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,448,8) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(401408,3136,56,1) -> Float(401408,1,7168,128) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.65974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.38949\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.57052\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.38949\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(401408,3136,56,1) -> Half(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.20514\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.54475\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.56346\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.20514\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(401408,3136,56,1) -> Half(200704,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.62448\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.72528\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.26511\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.26511\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(401408,3136,56,1) -> Half(50176,1:8,896,16) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.50455\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.24517\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.25732\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.24517\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,7168,128) -> Float(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.23954\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.93029\n",
      "[10/18/2022-15:53:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.05732\n",
      "[10/18/2022-15:53:48] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.93029\n",
      "[10/18/2022-15:53:48] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,7168,128) -> Half(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:53:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.92831\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.72558\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.96112\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.72558\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,7168,128) -> Half(200704,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.11747\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.68189\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.1696\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.68189\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,7168,128) -> Half(50176,1:8,896,16) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.57907\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.21973\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.58258\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.21973\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136,56,1) -> Float(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.29932\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.79724\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.02854\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.29932\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136,56,1) -> Float(401408,1,7168,128) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.29642\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.62047\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.38404\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.62047\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136,56,1) -> Half(200704,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.44917\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.37479\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.24084\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.24084\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136,56,1) -> Half(50176,1:8,896,16) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.37764\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.35934\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.0044\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.0044\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136:2,56,1) -> Float(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.67877\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.11434\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.30094\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.30094\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136:2,56,1) -> Float(401408,1,7168,128) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.96198\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.65018\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.96309\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.65018\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136:2,56,1) -> Half(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.3888\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.14203\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.13225\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.13225\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(200704,3136:2,56,1) -> Half(50176,1:8,896,16) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.75417\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.968105\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.933179\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.933179\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,896,16) -> Float(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.69019\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.00833\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.4436\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.4436\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,896,16) -> Float(401408,1,7168,128) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.53203\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.52279\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.52278\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.52278\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,896,16) -> Half(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.52784\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.07198\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.852626\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.852626\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,896,16) -> Half(200704,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_359 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.73528\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.32365\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.860201\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.860201\n",
      "[10/18/2022-15:53:49] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.622464\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.585733\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.616782\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.585733\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.306094\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.397934\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.396434\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.306094\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.417234\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.442034\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.322359\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.322359\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.595049\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.318729\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.319941\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.318729\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.633417\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.725504\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.633326\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.633326\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.598569\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.445019\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.598601\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.445019\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.624517\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.426235\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.619483\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.426235\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.390295\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.308686\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.390226\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.308686\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.326043\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.462697\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.502345\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.326043\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.523118\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.391461\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.524325\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.391461\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.36304\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.360434\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.295301\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.295301\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.492398\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.35669\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.242153\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.242153\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.403168\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.55413\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.330313\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.330313\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.466743\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.407845\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.467529\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.407845\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.354505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.715726\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.289061\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.289061\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.425161\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.242386\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.232192\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.232192\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.632567\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.536539\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.351241\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.351241\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.407008\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.380635\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.397792\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.380635\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.580841\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.528155\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.214834\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.214834\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_362 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.607877\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.335195\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.215872\n",
      "[10/18/2022-15:53:49] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.215872\n",
      "[10/18/2022-15:53:49] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(802816,1,14336,256) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(802816,3136,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(401408,3136:2,56,1) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Half(100352,1:8,1792,32) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:49] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.60974\n",
      "[10/18/2022-15:53:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.40922\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.5798\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.40922\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.20277\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.58018\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.56382\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.20277\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.63694\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.78824\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.28096\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.28096\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.50071\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.28939\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.28493\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.28493\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.65563\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.87177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.62544\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.62544\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.4424\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.77816\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.54656\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.77816\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.79241\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.69795\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.82039\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.69795\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.76655\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.22881\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.70481\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.22881\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.32213\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.83355\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.02181\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.32213\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.0615\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.6307\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.07742\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.6307\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.48023\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.46961\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.21494\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.21494\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.00381\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.49557\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.987689\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.987689\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.64486\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.19674\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.30989\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.30989\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.83074\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.67617\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.86403\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.67617\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.42417\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.94268\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.17455\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.17455\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.71514\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.994455\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.952027\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.952027\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.72121\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.16931\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.48861\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.48861\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.52025\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.56461\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.50733\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.50733\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.26507\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.18112\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.913408\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.913408\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_535 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.53473\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.35916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.916677\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.916677\n",
      "[10/18/2022-15:53:50] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.76159\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.51307\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.61247\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.51307\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.2051\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.58076\n",
      "[10/18/2022-15:53:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.56297\n",
      "[10/18/2022-15:53:50] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.2051\n",
      "[10/18/2022-15:53:50] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.65317\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.78732\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.28757\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.28757\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.50598\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.28907\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.28485\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.28485\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.65061\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.8865\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.6094\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.6094\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.42589\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.7784\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.50823\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.7784\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.75741\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.69697\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.82741\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.69697\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.80955\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.22734\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.73378\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.22734\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.33264\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.83293\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.01904\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.33264\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.06\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.63463\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.08545\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.63463\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.47895\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.47072\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.23941\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.23941\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.02108\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.49683\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.987941\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.987941\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.63168\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.19635\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.30955\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.30955\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.83029\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.67296\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.8818\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.67296\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.43889\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.00239\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.18924\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.18924\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.70614\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.990062\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.955451\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.955451\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.71567\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.16999\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.48365\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.48365\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.5319\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.56409\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.51953\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.51953\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.24525\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.15664\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.913408\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.913408\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.112) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.52755\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.36038\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.915808\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.915808\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,3584,128) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(100352,784,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,784:2,28,1) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,448,16) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(200704,784,28,1) -> Float(200704,1,7168,256) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.26338\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.21569\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.19472\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.19472\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(200704,784,28,1) -> Half(200704,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.601166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.792283\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.785856\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.601166\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(200704,784,28,1) -> Half(100352,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.814811\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.880347\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.635182\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.635182\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(200704,784,28,1) -> Half(25088,1:8,896,32) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.15595\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.640288\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.639383\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.639383\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,7168,256) -> Float(200704,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.31135\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.47536\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.31201\n",
      "[10/18/2022-15:53:51] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.31135\n",
      "[10/18/2022-15:53:51] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,7168,256) -> Half(200704,784,28,1) ***************\n",
      "[10/18/2022-15:53:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.2013\n",
      "[10/18/2022-15:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.8912\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.2124\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.8912\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,7168,256) -> Half(100352,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.29579\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.860562\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.34282\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.860562\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,7168,256) -> Half(25088,1:8,896,32) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.866665\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.622885\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.867118\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.622885\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,784,28,1) -> Float(200704,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.667973\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.919835\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.998697\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.667973\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,784,28,1) -> Float(200704,1,7168,256) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.04964\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.792869\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.02774\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.792869\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,784,28,1) -> Half(100352,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.722619\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.723223\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.596462\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.596462\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,784,28,1) -> Half(25088,1:8,896,32) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.983305\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.727045\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.48907\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.48907\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(100352,784:2,28,1) -> Float(200704,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.830501\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.09758\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.659913\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.659913\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(100352,784:2,28,1) -> Float(200704,1,7168,256) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.92917\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.826793\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.923721\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.826793\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(100352,784:2,28,1) -> Half(200704,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.706798\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.44538\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.588407\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.588407\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(100352,784:2,28,1) -> Half(25088,1:8,896,32) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.857463\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.492517\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.471625\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.471625\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,896,32) -> Float(200704,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.26626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.07662\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.722304\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.722304\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,896,32) -> Float(200704,1,7168,256) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.80176\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.776133\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.768\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.768\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,896,32) -> Half(200704,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.12404\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.01058\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.43925\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.43925\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,896,32) -> Half(100352,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_401 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.18462\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.670866\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.441682\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.441682\n",
      "[10/18/2022-15:53:52] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.275269\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.265755\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.278299\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.265755\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.153915\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.12651\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.201458\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.153915\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.210318\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.232009\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.161257\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.161257\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.266706\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.164466\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.160366\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.160366\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.305179\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.351803\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.304859\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.304859\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.292064\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.227858\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.292279\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.227858\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.312174\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.227547\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.31392\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.227547\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.199845\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.159127\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.199863\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.159127\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.164987\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.16602\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.251447\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.164987\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.250354\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.18667\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.249691\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.18667\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.185198\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.182944\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.14816\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.14816\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.235611\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.183639\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.129499\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.129499\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.203525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.302459\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.166953\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.166953\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.22085\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.189669\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.220306\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.189669\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.179346\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.426674\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.147383\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.147383\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.206094\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.130235\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.117029\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.117029\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.286427\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.288768\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.196805\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.196805\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.199963\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.192439\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.199886\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.192439\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.281454\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.350208\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.134155\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.134155\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_404 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.285842\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.178519\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.120265\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.120265\n",
      "[10/18/2022-15:53:52] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(401408,1,14336,512) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(401408,784,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,784:2,28,1) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(50176,1:8,1792,64) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.26931\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.04567\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.26427\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.04567\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.603282\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 12.5207\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.784809\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.603282\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.83675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.931826\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.635849\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.635849\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.19864\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.666171\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.625285\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.625285\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.25177\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.35732\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.25\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.25\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.17965\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.889545\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.2387\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.889545\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.4083\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.878688\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.43767\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.878688\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.885129\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.610889\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.868553\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.610889\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.674094\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 12.7328\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.00645\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.674094\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.07676\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.767982\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.07781\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.767982\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.714117\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.720018\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.590702\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.590702\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.02107\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.749568\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.510775\n",
      "[10/18/2022-15:53:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.510775\n",
      "[10/18/2022-15:53:52] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.821179\n",
      "[10/18/2022-15:53:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.18944\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.664448\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.664448\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.903913\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.77579\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.895291\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.77579\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.714007\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.7408\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.594789\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.594789\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.820599\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.522478\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.460795\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.460795\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.17964\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.13615\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.787333\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.787333\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.818537\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.76651\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.802523\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.76651\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.13723\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.4219\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.536105\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.536105\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_574 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.19896\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.697915\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.481472\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.481472\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.30379\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.09\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.29053\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.09\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.605381\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 12.3916\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.784809\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.605381\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.82907\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.933349\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.633157\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.633157\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.18774\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.666551\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.624933\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.624933\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.26311\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.34139\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.26517\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.26311\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.20103\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.889957\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.22436\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.889957\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.3589\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.878432\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.38645\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.878432\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.850839\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.610912\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.850693\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.610912\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.668526\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 12.7303\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.00588\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.668526\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.07809\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.766181\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.07813\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.766181\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.714729\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.72128\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.595282\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.595282\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.03014\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.751566\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.510761\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.510761\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.821253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.19213\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.662245\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.662245\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.896718\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.773051\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.889125\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.773051\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.707223\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.71593\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.589531\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.589531\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.820955\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.523319\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.4608\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.4608\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.19484\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.13423\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.784155\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.784155\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.828855\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.765682\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.803401\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.765682\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.13635\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.42339\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.536274\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.536274\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.212) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.19527\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.698066\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.481376\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.481376\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(50176,1,3584,256) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(50176,196,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,196:2,14,1) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(6272,1:8,448,32) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(100352,196,14,1) -> Float(100352,1,7168,512) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.578528\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.516603\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.578981\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.516603\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(100352,196,14,1) -> Half(100352,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.302734\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 6.0551\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.395913\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.302734\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(100352,196,14,1) -> Half(50176,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.416955\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.461701\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.319474\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.319474\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(100352,196,14,1) -> Half(12544,1:8,896,64) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.56293\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.329138\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.314656\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.314656\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,7168,512) -> Float(100352,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.608101\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.680521\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.616782\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.608101\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,7168,512) -> Half(100352,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.590153\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.448169\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.597138\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.448169\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,7168,512) -> Half(50176,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.640261\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.442971\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.640366\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.442971\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,7168,512) -> Half(12544,1:8,896,64) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.416791\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.309541\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.435621\n",
      "[10/18/2022-15:53:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.309541\n",
      "[10/18/2022-15:53:53] [V] [TRT] *************** Autotuning Reformat: Half(100352,196,14,1) -> Float(100352,196,14,1) ***************\n",
      "[10/18/2022-15:53:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.338798\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 6.58925\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.502213\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.338798\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,196,14,1) -> Float(100352,1,7168,512) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.519218\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.37483\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.518107\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.37483\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,196,14,1) -> Half(50176,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.359799\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.359616\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.288261\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.288261\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,196,14,1) -> Half(12544,1:8,896,64) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.477477\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.368055\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.250761\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.250761\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,196:2,14,1) -> Float(100352,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.406089\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.598368\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.332645\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.332645\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,196:2,14,1) -> Float(100352,1,7168,512) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.445111\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.381728\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.443602\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.381728\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,196:2,14,1) -> Half(100352,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.355579\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.844827\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.291067\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.291067\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,196:2,14,1) -> Half(12544,1:8,896,64) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.407845\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.261559\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.231712\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.231712\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,896,64) -> Float(100352,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.577627\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.571159\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.394414\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.394414\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,896,64) -> Float(100352,1,7168,512) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.402016\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.387657\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.396439\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.387657\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,896,64) -> Half(100352,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.559383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.690761\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.268709\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.268709\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,896,64) -> Half(50176,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_463 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.575058\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.349038\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.241166\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.241166\n",
      "[10/18/2022-15:53:54] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.142535\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.141618\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.142811\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.141618\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0784686\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.76684\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.104251\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0784686\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.103081\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.118144\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0814537\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0814537\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.132992\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0827063\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0817943\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0817943\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.137259\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.177298\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.137026\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.137026\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.133705\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.109733\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.133705\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.109733\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.137474\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.118197\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.137365\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.118197\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0993074\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0814834\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0993371\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0814834\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0832297\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.75632\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.126542\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0832297\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.124642\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0910903\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.124985\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0910903\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0902331\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.113211\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0734103\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0734103\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.117289\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.116585\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0643718\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0643718\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.10085\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.148224\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0835566\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0835566\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.107081\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0939954\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.107026\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0939954\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0888571\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.188343\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0730514\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0730514\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.102825\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0709013\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0710857\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0709013\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.139051\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.139913\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.103136\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.103136\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.10208\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0913371\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.102139\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0913371\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.136953\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.157691\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.07368\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.07368\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Conv_466 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.138446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0945394\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0635718\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0635718\n",
      "[10/18/2022-15:53:54] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(200704,1,14336,1024) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(200704,196,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,196:2,14,1) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(25088,1:8,1792,128) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.640233\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.482939\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.635479\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.482939\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.303387\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 7.01806\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.396091\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.303387\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.404549\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.452183\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.317495\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.317495\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.59611\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.318318\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.310482\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.310482\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.545664\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.679136\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.545902\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.545664\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.539141\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.422245\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.539159\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.422245\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.553079\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.450853\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.553001\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.450853\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.396114\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.312174\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.396133\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.312174\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.327973\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 7.20181\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.502949\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.327973\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.47221\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.36144\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.47264\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.36144\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.353097\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.438199\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.287205\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.287205\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.437477\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.454322\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.247712\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.247712\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.396123\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.569051\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.332562\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.332562\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.460809\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.357815\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.460818\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.357815\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.357815\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.741632\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.291666\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.291666\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.414318\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.272064\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.283209\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.272064\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.555557\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.548622\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.405714\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.405714\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.402875\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.359936\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.396384\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.359936\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.536288\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.602039\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.287273\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.287273\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Add_631 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.548279\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.359538\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.247808\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.247808\n",
      "[10/18/2022-15:53:54] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.640233\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.479525\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.640768\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.479525\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.303776\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 7.08286\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.396073\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.303776\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.408407\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.452105\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.319899\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.319899\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.598318\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.318542\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.310743\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.310743\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.545888\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.673349\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.543319\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.543319\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.529239\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.422455\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.529106\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.422455\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.543013\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.450711\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.543374\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.450711\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.389998\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.31227\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.39024\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.31227\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.326363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 7.27473\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.502057\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.326363\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.477038\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.360448\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.476942\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.360448\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.358331\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.442199\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.28672\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.28672\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.437691\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.454071\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.24805\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.24805\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.396114\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.571369\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.332407\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.332407\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.461408\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.357271\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.461248\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.357271\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.360782\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.776777\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.302574\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.302574\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.430665\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.284933\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.28848\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.284933\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.600064\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.537573\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.403726\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.403726\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.433975\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.356937\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.421888\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.356937\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.562615\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.631515\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.287077\n",
      "[10/18/2022-15:53:54] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.287077\n",
      "[10/18/2022-15:53:54] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> input.360) (Reformat)\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.576073\n",
      "[10/18/2022-15:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.363173\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.247584\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.247584\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,49,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(25088,1,3584,512) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(25088,49,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,49:2,7,1) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(3136,1:8,448,64) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(100352,1,14336,2048) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(100352,49,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(50176,49:2,7,1) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(12544,1:8,1792,256) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,1,1) -> Float(2048,1,2048,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00862091\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0085526\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0086279\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.0085526\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,1,1) -> Half(2048,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00597029\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00876827\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00965486\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00597029\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,1,1) -> Half(1024,1:2,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00885028\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.105214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00567894\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00567894\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,1,1) -> Half(256,1:8,256,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00864161\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00860262\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00587904\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00587904\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,2048,2048) -> Float(2048,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00752289\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00865452\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00752986\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00752289\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,2048,2048) -> Half(2048,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00769179\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0084714\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00723086\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00723086\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,2048,2048) -> Half(1024,1:2,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0078732\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0994674\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0078643\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0078643\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(2048,1,2048,2048) -> Half(256,1:8,256,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0080894\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00870131\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00810438\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0080894\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(2048,1,1,1) -> Float(2048,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00513731\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00882071\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00789919\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00513731\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(2048,1,1,1) -> Float(2048,1,2048,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.007264\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00853082\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00722217\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00722217\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(2048,1,1,1) -> Half(1024,1:2,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00702716\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.09624\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0050054\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0050054\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(2048,1,1,1) -> Half(256,1:8,256,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00720206\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00876289\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00525959\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00525959\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(1024,1:2,1,1) -> Float(2048,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00767687\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0952503\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00495779\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00495779\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(1024,1:2,1,1) -> Float(2048,1,2048,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0077837\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00869997\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00779862\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0077837\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(1024,1:2,1,1) -> Half(2048,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00704588\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0955749\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00483276\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00483276\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(1024,1:2,1,1) -> Half(256,1:8,256,256) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00803835\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00860854\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.073856\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00803835\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(256,1:8,256,256) -> Float(2048,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00759868\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0942331\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00422252\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00422252\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(256,1:8,256,256) -> Float(2048,1,2048,2048) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00775026\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0086556\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00773726\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00773726\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(256,1:8,256,256) -> Half(2048,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00687064\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0951749\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0040254\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0040254\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(256,1:8,256,256) -> Half(1024,1:2,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(onnx::Flatten_493 -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00761143\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0951909\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0731154\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00761143\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1,1) -> Half(1000,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(Gemm_121_out_tensor -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00344098\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00868491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00509777\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00344098\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1000,1000) -> Float(1000,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(Gemm_121_out_tensor -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0044541\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00861311\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00445825\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0044541\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Float(1000,1,1000,1000) -> Half(1000,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(Gemm_121_out_tensor -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00453\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0085198\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00453671\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00453\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(1000,1,1,1) -> Float(1000,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(Gemm_121_out_tensor -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00352033\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0086158\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00466433\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00352033\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(500,1:2,1,1) -> Float(1000,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(Gemm_121_out_tensor -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00485501\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0482682\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00344457\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00344457\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(500,1:2,1,1) -> Half(1000,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(Gemm_121_out_tensor -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00455771\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0486202\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00337701\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00337701\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(125,1:8,125,125) -> Float(1000,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(Gemm_121_out_tensor -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00484343\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0484968\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00324216\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00324216\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(125,1:8,125,125) -> Half(1000,1,1,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(Gemm_121_out_tensor -> <out>) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00456443\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0489189\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00326026\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00326026\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing reformatting costs\n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning Reformat: Half(1000,1) -> Float(1000,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> output) (Reformat)\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00352446\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.00871798\n",
      "[10/18/2022-15:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0035132\n",
      "[10/18/2022-15:53:55] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0035132\n",
      "[10/18/2022-15:53:55] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:53:55] [V] [TRT] *************** Autotuning format combination: Float(150528,50176,224,1) -> Float(802816,12544,112,1) ***************\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:53:55] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (FusedConvActConvolution)\n",
      "[10/18/2022-15:53:55] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:53:55] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (CudnnConvolution)\n",
      "[10/18/2022-15:53:56] [V] [TRT] Tactic: 0x0000000000000000 Time: 19.4625\n",
      "[10/18/2022-15:53:56] [V] [TRT] Tactic: 0x0000000000000001 Time: 12.149\n",
      "[10/18/2022-15:53:56] [V] [TRT] Tactic: 0x0000000000000002 Time: 24.87\n",
      "[10/18/2022-15:53:56] [V] [TRT] Tactic: 0x0000000000000005 Time: 48.4546\n",
      "[10/18/2022-15:53:56] [V] [TRT] Tactic: 0x0000000000000038 Time: 19.9832\n",
      "[10/18/2022-15:53:57] [V] [TRT] Tactic: 0x0000000000000039 Time: 12.1672\n",
      "[10/18/2022-15:53:57] [V] [TRT] Tactic: 0x000000000000003a Time: 24.8962\n",
      "[10/18/2022-15:53:57] [V] [TRT] Tactic: 0x000000000000003d Time: 48.4627\n",
      "[10/18/2022-15:53:57] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 12.149\n",
      "[10/18/2022-15:53:57] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (CaskConvolution)\n",
      "[10/18/2022-15:53:57] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:53:57] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 10.296\n",
      "[10/18/2022-15:53:57] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x27728c886a448c5a\n",
      "[10/18/2022-15:53:57] [V] [TRT] Tactic: 0x27728c886a448c5a Time: 6.02464\n",
      "[10/18/2022-15:53:57] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x597d29027694c20b\n",
      "[10/18/2022-15:53:57] [V] [TRT] Tactic: 0x597d29027694c20b Time: 12.5698\n",
      "[10/18/2022-15:53:57] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x8d5c64a52fab02c9\n",
      "[10/18/2022-15:53:57] [V] [TRT] Tactic: 0x8d5c64a52fab02c9 Time: 10.2607\n",
      "[10/18/2022-15:53:57] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:53:58] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 6.35266\n",
      "[10/18/2022-15:53:58] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:53:58] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 8.21062\n",
      "[10/18/2022-15:53:58] [V] [TRT] Fastest Tactic: 0x27728c886a448c5a Time: 6.02464\n",
      "[10/18/2022-15:53:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x27728c886a448c5a\n",
      "[10/18/2022-15:53:58] [V] [TRT] *************** Autotuning format combination: Float(150528,1,672,3) -> Float(802816,1,7168,64) ***************\n",
      "[10/18/2022-15:53:58] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (CaskConvolution)\n",
      "[10/18/2022-15:53:58] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:53:58] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 24.1731\n",
      "[10/18/2022-15:53:58] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca84742beb9f9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:53:58] [V] [TRT] Tactic: 0xca84742beb9f9767 Time: 24.273\n",
      "[10/18/2022-15:53:58] [V] [TRT] Fastest Tactic: 0x0bf55a7b77a6ff98 Time: 24.1731\n",
      "[10/18/2022-15:53:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:53:58] [V] [TRT] *************** Autotuning format combination: Half(150528,50176,224,1) -> Half(802816,12544,112,1) ***************\n",
      "[10/18/2022-15:53:58] [W] [TRT] Weights [name=Conv_0 + Relu_1.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:53:58] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:53:58] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:53:58] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:53:58] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (CudnnConvolution)\n",
      "[10/18/2022-15:53:58] [V] [TRT] Tactic: 0x0000000000000000 Time: 16.8521\n",
      "[10/18/2022-15:53:58] [V] [TRT] Tactic: 0x0000000000000001 Time: 9.76155\n",
      "[10/18/2022-15:53:58] [V] [TRT] Tactic: 0x0000000000000002 Time: 20.119\n",
      "[10/18/2022-15:53:59] [V] [TRT] Tactic: 0x0000000000000005 Time: 44.2956\n",
      "[10/18/2022-15:53:59] [V] [TRT] Tactic: 0x0000000000000038 Time: 17.3825\n",
      "[10/18/2022-15:53:59] [V] [TRT] Tactic: 0x000000000000003a Time: 20.3393\n",
      "[10/18/2022-15:53:59] [V] [TRT] Tactic: 0x000000000000003d Time: 44.2562\n",
      "[10/18/2022-15:53:59] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 9.76155\n",
      "[10/18/2022-15:53:59] [W] [TRT] Weights [name=Conv_0 + Relu_1.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:53:59] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:53:59] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:53:59] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:53:59] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (CaskConvolution)\n",
      "[10/18/2022-15:53:59] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:53:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:53:59] [V] [TRT] *************** Autotuning format combination: Half(100352,50176:2,224,1) -> Half(401408,12544:2,112,1) ***************\n",
      "[10/18/2022-15:53:59] [W] [TRT] Weights [name=Conv_0 + Relu_1.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:53:59] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:53:59] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:53:59] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:53:59] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (FusedConvActConvolution)\n",
      "[10/18/2022-15:53:59] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:53:59] [W] [TRT] Weights [name=Conv_0 + Relu_1.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:53:59] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:53:59] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:53:59] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:53:59] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (CaskConvolution)\n",
      "[10/18/2022-15:53:59] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x0fe4a9cce7ed878b\n",
      "[10/18/2022-15:53:59] [V] [TRT] Tactic: 0x0fe4a9cce7ed878b Time: 3.55736\n",
      "[10/18/2022-15:53:59] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 4.96405\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 4.92907\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x4092cbc840fbea35\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0x4092cbc840fbea35 Time: 4.68246\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0x446c8c788145836a Time: 5.27301\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 5.18582\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0x98a00f59a4b141f0\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0x98a00f59a4b141f0 Time: 5.1969\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xcbe3f30275b04323\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0xcbe3f30275b04323 Time: 9.03813\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 8.15302\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xd7d66d5d03a72c4e\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0xd7d66d5d03a72c4e Time: 4.541\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 9.11067\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xfc994367fd14b2d9\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0xfc994367fd14b2d9 Time: 8.30301\n",
      "[10/18/2022-15:54:00] [V] [TRT] Fastest Tactic: 0x0fe4a9cce7ed878b Time: 3.55736\n",
      "[10/18/2022-15:54:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0fe4a9cce7ed878b\n",
      "[10/18/2022-15:54:00] [V] [TRT] *************** Autotuning format combination: Half(50176,1:4,224,1) -> Half(100352,1:8,896,8) ***************\n",
      "[10/18/2022-15:54:00] [W] [TRT] Weights [name=Conv_0 + Relu_1.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:00] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:00] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (CaskConvolution)\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_first_layer_filter7x7_fwd Tactic: 0x8c03a77858cdae89\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0x8c03a77858cdae89 Time: 2.62997\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: trt_turing_cutlass_image_network_first_layer_hmma_fprop_f16f16f32_nhwc_nhwc_k64r7s7c4_stride2x2 Tactic: 0xe2222883a6602489\n",
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0xe2222883a6602489 Time: 1.98887\n",
      "[10/18/2022-15:54:00] [V] [TRT] Fastest Tactic: 0xe2222883a6602489 Time: 1.98887\n",
      "[10/18/2022-15:54:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xe2222883a6602489\n",
      "[10/18/2022-15:54:00] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,224,1) -> Float(802816,12544,112,1) ***************\n",
      "[10/18/2022-15:54:00] [W] [TRT] Weights [name=Conv_0 + Relu_1.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:00] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:00] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (CaskConvolution)\n",
      "[10/18/2022-15:54:00] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:00] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,224,1) -> Half(100352,1:8,896,8) ***************\n",
      "[10/18/2022-15:54:00] [W] [TRT] Weights [name=Conv_0 + Relu_1.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:00] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:00] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:00] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:00] [W] [TRT] Weights [name=Conv_0 + Relu_1.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:00] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:00] [V] [TRT] --------------- Timing Runner: Conv_0 + Relu_1 (CaskConvolution)\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x0559d1d2893a8768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:00] [V] [TRT] Tactic: 0x0559d1d2893a8768 Time: 21.2589\n",
      "[10/18/2022-15:54:00] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x0b906efbde4dc01a\n",
      "[10/18/2022-15:54:01] [V] [TRT] Tactic: 0x0b906efbde4dc01a Time: 48.2303\n",
      "[10/18/2022-15:54:01] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x0e0f7f10867063ba\n",
      "[10/18/2022-15:54:01] [V] [TRT] Tactic: 0x0e0f7f10867063ba Time: 8.30669\n",
      "[10/18/2022-15:54:01] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x0ecf8dc91198fd5e\n",
      "[10/18/2022-15:54:01] [V] [TRT] Tactic: 0x0ecf8dc91198fd5e Time: 14.3591\n",
      "[10/18/2022-15:54:01] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0x159236c6c22f62ce\n",
      "[10/18/2022-15:54:02] [V] [TRT] Tactic: 0x159236c6c22f62ce Time: 90.0021\n",
      "[10/18/2022-15:54:02] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0x1c23f4a19fbcb518\n",
      "[10/18/2022-15:54:02] [V] [TRT] Tactic: 0x1c23f4a19fbcb518 Time: 57.3278\n",
      "[10/18/2022-15:54:02] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x33fc6102b341eb5d\n",
      "[10/18/2022-15:54:02] [V] [TRT] Tactic: 0x33fc6102b341eb5d Time: 45.6513\n",
      "[10/18/2022-15:54:02] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x348653930e0a64e2\n",
      "[10/18/2022-15:54:03] [V] [TRT] Tactic: 0x348653930e0a64e2 Time: 28.1054\n",
      "[10/18/2022-15:54:03] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x350e898a5a20ad00\n",
      "[10/18/2022-15:54:03] [V] [TRT] Tactic: 0x350e898a5a20ad00 Time: 15.7726\n",
      "[10/18/2022-15:54:03] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x4e34a65090c3b86f\n",
      "[10/18/2022-15:54:03] [V] [TRT] Tactic: 0x4e34a65090c3b86f Time: 26.9105\n",
      "[10/18/2022-15:54:03] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x5128cdf162fe56b6\n",
      "[10/18/2022-15:54:03] [V] [TRT] Tactic: 0x5128cdf162fe56b6 Time: 23.3387\n",
      "[10/18/2022-15:54:03] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:54:03] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 23.169\n",
      "[10/18/2022-15:54:03] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x5bec1fbd955eb827\n",
      "[10/18/2022-15:54:03] [V] [TRT] Tactic: 0x5bec1fbd955eb827 Time: 12.489\n",
      "[10/18/2022-15:54:03] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:54:04] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 25.0928\n",
      "[10/18/2022-15:54:04] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x62bb371b230a886d\n",
      "[10/18/2022-15:54:04] [V] [TRT] Tactic: 0x62bb371b230a886d Time: 15.1524\n",
      "[10/18/2022-15:54:04] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:54:04] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 13.3059\n",
      "[10/18/2022-15:54:04] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x69a5b2ac9c5bac16\n",
      "[10/18/2022-15:54:04] [V] [TRT] Tactic: 0x69a5b2ac9c5bac16 Time: 10.9025\n",
      "[10/18/2022-15:54:04] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6b44e6396887bed9\n",
      "[10/18/2022-15:54:04] [V] [TRT] Tactic: 0x6b44e6396887bed9 Time: 19.7823\n",
      "[10/18/2022-15:54:04] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:54:04] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 26.7118\n",
      "[10/18/2022-15:54:04] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x75585ae3e9dedb93\n",
      "[10/18/2022-15:54:05] [V] [TRT] Tactic: 0x75585ae3e9dedb93 Time: 31.4565\n",
      "[10/18/2022-15:54:05] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x784dcede905d06c0\n",
      "[10/18/2022-15:54:05] [V] [TRT] Tactic: 0x784dcede905d06c0 Time: 24.3519\n",
      "[10/18/2022-15:54:05] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:54:05] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 8.90383\n",
      "[10/18/2022-15:54:05] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x8781623566dac7f0\n",
      "[10/18/2022-15:54:05] [V] [TRT] Tactic: 0x8781623566dac7f0 Time: 9.12735\n",
      "[10/18/2022-15:54:05] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: trt_turing_cutlass_image_network_first_layer_hmma_fprop_f16f16f32_nhwc_nhwc_k64r7s7c8_stride2x2 Tactic: 0x89d2643fb1fde16d\n",
      "[10/18/2022-15:54:05] [V] [TRT] Tactic: 0x89d2643fb1fde16d Time: 6.30107\n",
      "[10/18/2022-15:54:05] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x9650edb797f919f3\n",
      "[10/18/2022-15:54:05] [V] [TRT] Tactic: 0x9650edb797f919f3 Time: 14.3114\n",
      "[10/18/2022-15:54:05] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0x969b1abbb567ac47\n",
      "[10/18/2022-15:54:05] [V] [TRT] Tactic: 0x969b1abbb567ac47 Time: 23.3528\n",
      "[10/18/2022-15:54:05] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa13cdf70a9d99d45\n",
      "[10/18/2022-15:54:05] [V] [TRT] Tactic: 0xa13cdf70a9d99d45 Time: 15.9395\n",
      "[10/18/2022-15:54:05] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa2dad76f719680b5\n",
      "[10/18/2022-15:54:06] [V] [TRT] Tactic: 0xa2dad76f719680b5 Time: 13.2551\n",
      "[10/18/2022-15:54:06] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0xa3e778b253a14ca9\n",
      "[10/18/2022-15:54:06] [V] [TRT] Tactic: 0xa3e778b253a14ca9 Time: 42.0598\n",
      "[10/18/2022-15:54:06] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:54:06] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 15.6803\n",
      "[10/18/2022-15:54:06] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb0bf64026e546f4d\n",
      "[10/18/2022-15:54:06] [V] [TRT] Tactic: 0xb0bf64026e546f4d Time: 30.3381\n",
      "[10/18/2022-15:54:06] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xb26e93bd0702f504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:07] [V] [TRT] Tactic: 0xb26e93bd0702f504 Time: 35.4319\n",
      "[10/18/2022-15:54:07] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xbb3d6545e4864f26\n",
      "[10/18/2022-15:54:07] [V] [TRT] Tactic: 0xbb3d6545e4864f26 Time: 25.6725\n",
      "[10/18/2022-15:54:07] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xc6e0905d983b4a62\n",
      "[10/18/2022-15:54:07] [V] [TRT] Tactic: 0xc6e0905d983b4a62 Time: 17.4466\n",
      "[10/18/2022-15:54:07] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:54:07] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 26.0251\n",
      "[10/18/2022-15:54:07] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xe84b9aaa289245c0\n",
      "[10/18/2022-15:54:07] [V] [TRT] Tactic: 0xe84b9aaa289245c0 Time: 27.5418\n",
      "[10/18/2022-15:54:07] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 27.0522\n",
      "[10/18/2022-15:54:08] [V] [TRT] Fastest Tactic: 0x89d2643fb1fde16d Time: 6.30107\n",
      "[10/18/2022-15:54:08] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x89d2643fb1fde16d\n",
      "[10/18/2022-15:54:08] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:08] [V] [TRT] *************** Autotuning format combination: Float(802816,12544,112,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:08] [V] [TRT] --------------- Timing Runner: MaxPool_2 (TiledPooling)\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000000101 Time: 8.20729\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000010101 Time: 5.83418\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000020101 Time: 4.93666\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000030101 Time: 4.53856\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000040101 Time: 3.5409\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000050101 Time: 6.16798\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000060101 Time: 4.07896\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000070101 Time: 8.29093\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000080101 Time: 5.80135\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000090101 Time: 4.67968\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x00000000000a0101 Time: 2.74424\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x00000000000b0101 Time: 2.42547\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x00000000000c0101 Time: 3.81225\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x00000000000d0101 Time: 3.29934\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x00000000000e0101 Time: 8.47045\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x00000000000f0101 Time: 5.87717\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000100101 Time: 4.6633\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000110101 Time: 2.56931\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000120101 Time: 2.34009\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000130101 Time: 3.54921\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000140101 Time: 3.23467\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000150101 Time: 8.57792\n",
      "[10/18/2022-15:54:08] [V] [TRT] Tactic: 0x0000000000160101 Time: 5.96778\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000170101 Time: 4.67867\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000180101 Time: 2.51147\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000190101 Time: 2.28724\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x00000000001a0101 Time: 3.47199\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x00000000001b0101 Time: 3.22205\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x00000000001c0101 Time: 8.61556\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x00000000001d0101 Time: 6.00626\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x00000000001e0101 Time: 4.67961\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x00000000001f0101 Time: 2.47977\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000200101 Time: 2.28282\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000210101 Time: 3.43654\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000220101 Time: 3.21387\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000230101 Time: 8.65051\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000240101 Time: 6.01675\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000250101 Time: 4.73564\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000260101 Time: 2.46023\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000270101 Time: 2.26245\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000280101 Time: 3.41457\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000290101 Time: 3.22147\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x00000000006a0101 Time: 2.10504\n",
      "[10/18/2022-15:54:09] [V] [TRT] Fastest Tactic: 0x00000000006a0101 Time: 2.10504\n",
      "[10/18/2022-15:54:09] [V] [TRT] --------------- Timing Runner: MaxPool_2 (CudnnPooling)\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0xffffffffffffffff Time: 2.58741\n",
      "[10/18/2022-15:54:09] [V] [TRT] Fastest Tactic: 0xffffffffffffffff Time: 2.58741\n",
      "[10/18/2022-15:54:09] [V] [TRT] --------------- Timing Runner: MaxPool_2 (CaskPooling)\n",
      "[10/18/2022-15:54:09] [V] [TRT] MaxPool_2 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: 0xb59f9cfb90407c92\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0xb59f9cfb90407c92 Time: 2.61372\n",
      "[10/18/2022-15:54:09] [V] [TRT] Fastest Tactic: 0xb59f9cfb90407c92 Time: 2.61372\n",
      "[10/18/2022-15:54:09] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: TiledPooling Tactic: 0x00000000006a0101\n",
      "[10/18/2022-15:54:09] [V] [TRT] *************** Autotuning format combination: Half(802816,12544,112,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:09] [V] [TRT] --------------- Timing Runner: MaxPool_2 (TiledPooling)\n",
      "[10/18/2022-15:54:09] [V] [TRT] TiledPooling has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:09] [V] [TRT] --------------- Timing Runner: MaxPool_2 (CudnnPooling)\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0xffffffffffffffff Time: 1.78937\n",
      "[10/18/2022-15:54:09] [V] [TRT] Fastest Tactic: 0xffffffffffffffff Time: 1.78937\n",
      "[10/18/2022-15:54:09] [V] [TRT] --------------- Timing Runner: MaxPool_2 (CaskPooling)\n",
      "[10/18/2022-15:54:09] [V] [TRT] MaxPool_2 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP16FP32NCHW_Max Tactic: 0xfd3a631f5c3e741e\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0xfd3a631f5c3e741e Time: 2.11446\n",
      "[10/18/2022-15:54:09] [V] [TRT] Fastest Tactic: 0xfd3a631f5c3e741e Time: 2.11446\n",
      "[10/18/2022-15:54:09] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnPooling Tactic: 0xffffffffffffffff\n",
      "[10/18/2022-15:54:09] [V] [TRT] *************** Autotuning format combination: Half(401408,12544:2,112,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:09] [V] [TRT] --------------- Timing Runner: MaxPool_2 (TiledPooling)\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000000101 Time: 4.09675\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000010101 Time: 2.91292\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000020101 Time: 2.5638\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000030101 Time: 2.43656\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000040101 Time: 1.9336\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000050101 Time: 3.20299\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000060101 Time: 2.16618\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000070101 Time: 4.14181\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000080101 Time: 2.8988\n",
      "[10/18/2022-15:54:09] [V] [TRT] Tactic: 0x0000000000090101 Time: 2.345\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000000a0101 Time: 1.44443\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000000b0101 Time: 1.25283\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000000c0101 Time: 1.95847\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000000d0101 Time: 1.66715\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000000e0101 Time: 4.23709\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000000f0101 Time: 2.93352\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000100101 Time: 2.33341\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000110101 Time: 1.32474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000120101 Time: 1.17752\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000130101 Time: 1.79506\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000140101 Time: 1.63254\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000150101 Time: 4.27764\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000160101 Time: 2.9841\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000170101 Time: 2.3389\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000180101 Time: 1.2759\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000190101 Time: 1.16095\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000001a0101 Time: 1.75237\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000001b0101 Time: 1.61498\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000001c0101 Time: 4.29754\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000001d0101 Time: 2.99841\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000001e0101 Time: 2.33806\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000001f0101 Time: 1.26462\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000200101 Time: 1.15446\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000210101 Time: 1.73433\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000220101 Time: 1.62143\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000230101 Time: 4.31422\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000240101 Time: 3.00763\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000250101 Time: 2.36661\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000260101 Time: 1.2446\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000270101 Time: 1.15446\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000280101 Time: 1.72453\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000290101 Time: 1.62292\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x00000000006a0101 Time: 1.06298\n",
      "[10/18/2022-15:54:10] [V] [TRT] Fastest Tactic: 0x00000000006a0101 Time: 1.06298\n",
      "[10/18/2022-15:54:10] [V] [TRT] --------------- Timing Runner: MaxPool_2 (CudaPooling)\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0xfffffffffffffffd Time: 1.03961\n",
      "[10/18/2022-15:54:10] [V] [TRT] Fastest Tactic: 0xfffffffffffffffd Time: 1.03961\n",
      "[10/18/2022-15:54:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: 0xfffffffffffffffd\n",
      "[10/18/2022-15:54:10] [V] [TRT] *************** Autotuning format combination: Half(100352,1:8,896,8) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:54:10] [V] [TRT] --------------- Timing Runner: MaxPool_2 (TiledPooling)\n",
      "[10/18/2022-15:54:10] [V] [TRT] TiledPooling has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:10] [V] [TRT] --------------- Timing Runner: MaxPool_2 (CudaPooling)\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0xfffffffffffffffe Time: 1.0711\n",
      "[10/18/2022-15:54:10] [V] [TRT] Fastest Tactic: 0xfffffffffffffffe Time: 1.0711\n",
      "[10/18/2022-15:54:10] [V] [TRT] --------------- Timing Runner: MaxPool_2 (CudnnPooling)\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0xffffffffffffffff Time: 1.01503\n",
      "[10/18/2022-15:54:10] [V] [TRT] Fastest Tactic: 0xffffffffffffffff Time: 1.01503\n",
      "[10/18/2022-15:54:10] [V] [TRT] --------------- Timing Runner: MaxPool_2 (CaskPooling)\n",
      "[10/18/2022-15:54:10] [V] [TRT] MaxPool_2 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP16FP32NHWC_Max_CAlign8 Tactic: 0x6613a704fbea2e33\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x6613a704fbea2e33 Time: 1.58824\n",
      "[10/18/2022-15:54:10] [V] [TRT] MaxPool_2 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP16FP32NHWC_Max_CAlign4 Tactic: 0xea62e389fb7bcc66\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0xea62e389fb7bcc66 Time: 2.48248\n",
      "[10/18/2022-15:54:10] [V] [TRT] Fastest Tactic: 0x6613a704fbea2e33 Time: 1.58824\n",
      "[10/18/2022-15:54:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnPooling Tactic: 0xffffffffffffffff\n",
      "[10/18/2022-15:54:10] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:10] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:10] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:10] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:10] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:10] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:10] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CudnnConvolution)\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.40424\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.65448\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.40525\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000000004 Time: 9.4273\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000000005 Time: 6.14137\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000000038 Time: 3.40085\n",
      "[10/18/2022-15:54:10] [V] [TRT] Tactic: 0x0000000000000039 Time: 2.64452\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x000000000000003a Time: 4.39893\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x000000000000003c Time: 9.43596\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x000000000000003d Time: 6.15334\n",
      "[10/18/2022-15:54:11] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 2.64452\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CublasConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CaskConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 1.14814\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 1.18926\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 0.940869\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 1.29155\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 1.14979\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 0.876302\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 0.88496\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 0.876219\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 2.85196\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 1.18405\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 1.07528\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 0.931369\n",
      "[10/18/2022-15:54:11] [V] [TRT] Fastest Tactic: 0x90238daf8750ddb0 Time: 0.876219\n",
      "[10/18/2022-15:54:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:11] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CublasConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CaskConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 1.34464\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 0.944841\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 1.3267\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 1.56358\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 1.05677\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 1.29902\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 1.38887\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 0.952709\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 1.66741\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 1.34531\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 2.20163\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 1.34407\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 1.06906\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 1.37617\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xf92663d88255134b Time: 0.976251\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 1.68267\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 2.13038\n",
      "[10/18/2022-15:54:11] [V] [TRT] Fastest Tactic: 0x48f8d75aa348d22f Time: 0.944841\n",
      "[10/18/2022-15:54:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:54:11] [V] [TRT] *************** Autotuning format combination: Half(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CudnnConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.67791\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x0000000000000001 Time: 1.5228\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x0000000000000002 Time: 3.1627\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x0000000000000004 Time: 8.17614\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x0000000000000005 Time: 5.15228\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x0000000000000038 Time: 2.77203\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x000000000000003a Time: 3.28853\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x000000000000003c Time: 8.17963\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x000000000000003d Time: 5.13807\n",
      "[10/18/2022-15:54:11] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 1.5228\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CublasConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CaskConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:54:11] [V] [TRT] *************** Autotuning format combination: Half(100352,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CaskConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:11] [V] [TRT] *************** Autotuning format combination: Half(100352,3136:2,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CublasConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CaskConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 0.477957\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 0.479333\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 0.478418\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x446c8c788145836a Time: 0.489751\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 0.492722\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 0.476965\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 0.886784\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x97afba3735828021 Time: 0.490176\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 0.477243\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 0.498802\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 0.995634\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 0.492064\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 0.473097\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 1.05179\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 0.761184\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 0.761271\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 0.509504\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:54:11] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 1.05752\n",
      "[10/18/2022-15:54:11] [V] [TRT] Fastest Tactic: 0xcc68d30459859090 Time: 0.473097\n",
      "[10/18/2022-15:54:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:54:11] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CublasConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CaskConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:11] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CublasConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:11] [V] [TRT] --------------- Timing Runner: Conv_3 + Relu_4 (CaskConvolution)\n",
      "[10/18/2022-15:54:11] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 0.440864\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 0.417851\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.455214\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 0.558226\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.452955\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.488887\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 1.12546\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x249110624ee04937 Time: 0.44523\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 1.00088\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 0.495447\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.428032\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 0.487858\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 1.12749\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 0.643374\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 0.434258\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 0.446144\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 0.461495\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 0.441198\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 0.455241\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 0.65803\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 0.47483\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 0.604439\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 0.517056\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 0.487657\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 0.436727\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 0.555177\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 0.493065\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 0.665623\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 0.482821\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 0.846697\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 0.534528\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 0.46165\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 0.499419\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 0.474501\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.449975\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x83b35618df65874c Time: 0.470162\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 0.487959\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 0.631621\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 1.4365\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.502674\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 0.429591\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 1.15218\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 0.605394\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 0.442162\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 0.616014\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 0.454656\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.451255\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.450871\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 0.454016\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.43435\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 0.562025\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 0.454363\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.46464\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 0.475095\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 0.603296\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.464896\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.895547\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 0.553838\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.425806\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 0.417792\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 0.443995\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 0.452613\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.498336\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 0.469893\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 0.489088\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 1.40098\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 0.490162\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 0.434528\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.50608\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 0.488329\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 0.486839\n",
      "[10/18/2022-15:54:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.425755\n",
      "[10/18/2022-15:54:12] [V] [TRT] Fastest Tactic: 0xd8c128ae16cb4132 Time: 0.417792\n",
      "[10/18/2022-15:54:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:54:12] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:12] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:12] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:12] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:12] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:12] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:12] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 13.5595\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x0000000000000001 Time: 6.60626\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x0000000000000002 Time: 20.5838\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x0000000000000004 Time: 9.4593\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x0000000000000005 Time: 6.78889\n",
      "[10/18/2022-15:54:12] [V] [TRT] Tactic: 0x0000000000000006 Time: 5.8944\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x0000000000000038 Time: 13.9857\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x0000000000000039 Time: 6.43563\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x000000000000003a Time: 19.4424\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x000000000000003c Time: 9.43046\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x000000000000003d Time: 6.77977\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x000000000000003e Time: 5.86957\n",
      "[10/18/2022-15:54:13] [V] [TRT] Fastest Tactic: 0x000000000000003e Time: 5.86957\n",
      "[10/18/2022-15:54:13] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (CaskConvolution)\n",
      "[10/18/2022-15:54:13] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 9.44913\n",
      "[10/18/2022-15:54:13] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x268494f0a1c83de3\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x268494f0a1c83de3 Time: 4.34353\n",
      "[10/18/2022-15:54:13] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x27728c886a448c5a\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x27728c886a448c5a Time: 5.31973\n",
      "[10/18/2022-15:54:13] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 9.68411\n",
      "[10/18/2022-15:54:13] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x597d29027694c20b\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x597d29027694c20b Time: 9.47263\n",
      "[10/18/2022-15:54:13] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 4.99345\n",
      "[10/18/2022-15:54:13] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x62b2ffd9a5c0cfb5\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x62b2ffd9a5c0cfb5 Time: 7.74312\n",
      "[10/18/2022-15:54:13] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x8d5c64a52fab02c9\n",
      "[10/18/2022-15:54:13] [V] [TRT] Tactic: 0x8d5c64a52fab02c9 Time: 8.81662\n",
      "[10/18/2022-15:54:13] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 5.34233\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x93a1176336e5b9f6\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0x93a1176336e5b9f6 Time: 7.06438\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x994f5b723e2d80da\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0x994f5b723e2d80da Time: 6.85083\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x128x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xac49795b871b0d29\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0xac49795b871b0d29 Time: 11.9066\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb6717e61503d5e9b\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0xb6717e61503d5e9b Time: 11.1477\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 6.64778\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 6.22749\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd1338f4b38d341e2\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0xd1338f4b38d341e2 Time: 12.0109\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x256x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd2aa21bfe2167c0c\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0xd2aa21bfe2167c0c Time: 19.9221\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xe40b38338a3a7d7e\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0xe40b38338a3a7d7e Time: 7.0578\n",
      "[10/18/2022-15:54:14] [V] [TRT] Fastest Tactic: 0x268494f0a1c83de3 Time: 4.34353\n",
      "[10/18/2022-15:54:14] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x268494f0a1c83de3\n",
      "[10/18/2022-15:54:14] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:54:14] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (CaskConvolution)\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x0447933cc2be855a\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0x0447933cc2be855a Time: 11.22\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 9.12134\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x0e2033f7517a807f\n",
      "[10/18/2022-15:54:14] [V] [TRT] Tactic: 0x0e2033f7517a807f Time: 9.20912\n",
      "[10/18/2022-15:54:14] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x2595402367cdee5c\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0x2595402367cdee5c Time: 7.52272\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3eba442f4c9c4f50\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0x3eba442f4c9c4f50 Time: 6.39076\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x43334a9c8840c773\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0x43334a9c8840c773 Time: 5.92486\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 5.44728\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 9.55893\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 5.99401\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x66e3239eee98201e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0x66e3239eee98201e Time: 10.934\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 9.43109\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 5.13203\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 9.51603\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x963db12d24e61b80\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0x963db12d24e61b80 Time: 5.91468\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb132670a7750e065\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0xb132670a7750e065 Time: 8.28766\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca84742beb9f9767\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0xca84742beb9f9767 Time: 9.60485\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xd2b62ec40baf8ee4\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0xd2b62ec40baf8ee4 Time: 5.076\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xde4165142218dab8\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0xde4165142218dab8 Time: 11.2517\n",
      "[10/18/2022-15:54:15] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:54:15] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 6.12168\n",
      "[10/18/2022-15:54:15] [V] [TRT] Fastest Tactic: 0xd2b62ec40baf8ee4 Time: 5.076\n",
      "[10/18/2022-15:54:15] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd2b62ec40baf8ee4\n",
      "[10/18/2022-15:54:15] [V] [TRT] *************** Autotuning format combination: Half(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:15] [W] [TRT] Weights [name=Conv_5 + Relu_6.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:15] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:15] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:15] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:15] [W] [TRT] Weights [name=Conv_5 + Relu_6.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:15] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:15] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:15] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:16] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (CudnnConvolution)\n",
      "[10/18/2022-15:54:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.5686\n",
      "[10/18/2022-15:54:16] [V] [TRT] Tactic: 0x0000000000000001 Time: 6.2768\n",
      "[10/18/2022-15:54:16] [V] [TRT] Tactic: 0x0000000000000002 Time: 19.1587\n",
      "[10/18/2022-15:54:16] [V] [TRT] Tactic: 0x0000000000000004 Time: 8.2157\n",
      "[10/18/2022-15:54:16] [V] [TRT] Tactic: 0x0000000000000005 Time: 5.54563\n",
      "[10/18/2022-15:54:16] [V] [TRT] Tactic: 0x0000000000000006 Time: 6.6031\n",
      "[10/18/2022-15:54:16] [V] [TRT] Tactic: 0x0000000000000038 Time: 15.0025\n",
      "[10/18/2022-15:54:16] [V] [TRT] Tactic: 0x000000000000003a Time: 18.805\n",
      "[10/18/2022-15:54:16] [V] [TRT] Tactic: 0x000000000000003c Time: 8.19901\n",
      "[10/18/2022-15:54:16] [V] [TRT] Tactic: 0x000000000000003d Time: 5.62652\n",
      "[10/18/2022-15:54:16] [V] [TRT] Tactic: 0x000000000000003e Time: 6.78502\n",
      "[10/18/2022-15:54:16] [V] [TRT] Fastest Tactic: 0x0000000000000005 Time: 5.54563\n",
      "[10/18/2022-15:54:16] [W] [TRT] Weights [name=Conv_5 + Relu_6.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:16] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:16] [W] [TRT] Weights [name=Conv_5 + Relu_6.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:16] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:16] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (CaskConvolution)\n",
      "[10/18/2022-15:54:16] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:16] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000005\n",
      "[10/18/2022-15:54:16] [V] [TRT] *************** Autotuning format combination: Half(100352,3136:2,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:16] [W] [TRT] Weights [name=Conv_5 + Relu_6.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:16] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:16] [W] [TRT] Weights [name=Conv_5 + Relu_6.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:16] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:16] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:16] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:16] [W] [TRT] Weights [name=Conv_5 + Relu_6.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:16] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:16] [W] [TRT] Weights [name=Conv_5 + Relu_6.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:16] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:16] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:17] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (CaskConvolution)\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x0fe4a9cce7ed878b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x0fe4a9cce7ed878b Time: 2.48366\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.63635\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 3.04887\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 3.00354\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x4092cbc840fbea35\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x4092cbc840fbea35 Time: 2.97069\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x446c8c788145836a Time: 3.24228\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 3.21562\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 2.97925\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 5.76512\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0x98a00f59a4b141f0\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x98a00f59a4b141f0 Time: 2.97205\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 2.9791\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 5.83037\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xcbe3f30275b04323\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0xcbe3f30275b04323 Time: 5.34691\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 5.10891\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0xd46b3ee2b59f893c\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0xd46b3ee2b59f893c Time: 2.4289\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xd7d66d5d03a72c4e\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0xd7d66d5d03a72c4e Time: 3.01035\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 3.14154\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 5.94618\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xfc994367fd14b2d9\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0xfc994367fd14b2d9 Time: 5.33153\n",
      "[10/18/2022-15:54:17] [V] [TRT] Fastest Tactic: 0xd46b3ee2b59f893c Time: 2.4289\n",
      "[10/18/2022-15:54:17] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd46b3ee2b59f893c\n",
      "[10/18/2022-15:54:17] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:17] [W] [TRT] Weights [name=Conv_5 + Relu_6.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:17] [W] [TRT] Weights [name=Conv_5 + Relu_6.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:17] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (CaskConvolution)\n",
      "[10/18/2022-15:54:17] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:17] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:54:17] [W] [TRT] Weights [name=Conv_5 + Relu_6.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:17] [W] [TRT] Weights [name=Conv_5 + Relu_6.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:17] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:17] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:17] [W] [TRT] Weights [name=Conv_5 + Relu_6.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:17] [W] [TRT] Weights [name=Conv_5 + Relu_6.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:17] [V] [TRT] --------------- Timing Runner: Conv_5 + Relu_6 (CaskConvolution)\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x00a425145e84482b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x00a425145e84482b Time: 4.61177\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x03512591e8ea2977\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x03512591e8ea2977 Time: 1.65825\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x0559d1d2893a8768\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x0559d1d2893a8768 Time: 2.37775\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x095000b22a78f234\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x095000b22a78f234 Time: 1.31218\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x0b906efbde4dc01a\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x0b906efbde4dc01a Time: 3.34517\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x0c0088d5808566d2\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x0c0088d5808566d2 Time: 1.70427\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x0caa5410b61e6cc5\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x0caa5410b61e6cc5 Time: 5.25491\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x0e0f7f10867063ba\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x0e0f7f10867063ba Time: 1.05531\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x0e131ddbafdfe235\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x0e131ddbafdfe235 Time: 2.39704\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x0ecf8dc91198fd5e\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x0ecf8dc91198fd5e Time: 1.57412\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0x159236c6c22f62ce\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x159236c6c22f62ce Time: 5.4634\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x15ecbd82c22a023f\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x15ecbd82c22a023f Time: 1.4911\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x18ef97651ad5379a\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x18ef97651ad5379a Time: 1.87114\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x1981adfb6b6fd8b9\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x1981adfb6b6fd8b9 Time: 2.81168\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x1b099f7ac29a2a6a\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x1b099f7ac29a2a6a Time: 1.60747\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x1b9cb8d78519a728\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x1b9cb8d78519a728 Time: 2.01604\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0x1c23f4a19fbcb518\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x1c23f4a19fbcb518 Time: 3.63459\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 1.03598\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x1de724868edf11b0\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x1de724868edf11b0 Time: 1.19662\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:54:17] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 1.62297\n",
      "[10/18/2022-15:54:17] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x30150d05024bc911\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x30150d05024bc911 Time: 1.05747\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x32789ed2e6c7b43b\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x32789ed2e6c7b43b Time: 1.63591\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x33fc6102b341eb5d\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x33fc6102b341eb5d Time: 5.37745\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 2.85918\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x348653930e0a64e2\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x348653930e0a64e2 Time: 2.87504\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x350e898a5a20ad00\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x350e898a5a20ad00 Time: 1.09978\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x36662b4d547eefc7\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x36662b4d547eefc7 Time: 2.47584\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x490a097d77573bff\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x490a097d77573bff Time: 1.53541\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x4c6a6da741444412\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x4c6a6da741444412 Time: 3.1928\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x4e34a65090c3b86f\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x4e34a65090c3b86f Time: 1.48244\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x504f864880743a14\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x504f864880743a14 Time: 2.35545\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x5128cdf162fe56b6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x5128cdf162fe56b6 Time: 2.51553\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 2.61844\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r3s3 Tactic: 0x5252dc6c9c5f3aff\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x5252dc6c9c5f3aff Time: 1.78368\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x54b287be85c1522c\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x54b287be85c1522c Time: 5.28272\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x55fb34a08663e5ae\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x55fb34a08663e5ae Time: 1.6937\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x56c66ffbce24b635\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x56c66ffbce24b635 Time: 1.57505\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x58eea09dffe038fd\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x58eea09dffe038fd Time: 1.3707\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x5bec1fbd955eb827\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x5bec1fbd955eb827 Time: 1.56219\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.72508\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x62bb371b230a886d\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x62bb371b230a886d Time: 1.86954\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.64952\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x69a5b2ac9c5bac16\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x69a5b2ac9c5bac16 Time: 1.35444\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6b44e6396887bed9\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x6b44e6396887bed9 Time: 2.27482\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x6cde8847e8cd796b\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x6cde8847e8cd796b Time: 2.63984\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x6cee4d9c86b4cdd5\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x6cee4d9c86b4cdd5 Time: 1.38913\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.6346\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x721049a39aae27ff\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x721049a39aae27ff Time: 2.41693\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 3.06849\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x75585ae3e9dedb93\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x75585ae3e9dedb93 Time: 1.92974\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x784dcede905d06c0\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x784dcede905d06c0 Time: 1.42661\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 1.09217\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x86903737887c556d\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x86903737887c556d Time: 1.42014\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x8781623566dac7f0\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x8781623566dac7f0 Time: 1.09536\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x8b86a8bb857fff79\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x8b86a8bb857fff79 Time: 2.01116\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x8d73ddfc444be692\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x8d73ddfc444be692 Time: 1.83624\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x9650edb797f919f3\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x9650edb797f919f3 Time: 1.81327\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0x969b1abbb567ac47\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x969b1abbb567ac47 Time: 2.66656\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0x9a0f43b4d1dc46d4\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0x9a0f43b4d1dc46d4 Time: 2.30199\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa13cdf70a9d99d45\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xa13cdf70a9d99d45 Time: 2.01744\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa2dad76f719680b5\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xa2dad76f719680b5 Time: 1.82244\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0xa3e778b253a14ca9\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xa3e778b253a14ca9 Time: 2.60325\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0xa5f0bcb42cb01fc7\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xa5f0bcb42cb01fc7 Time: 3.77784\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r3s3 Tactic: 0xa84824f86c61d2d8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xa84824f86c61d2d8 Time: 3.51486\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 1.10333\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xab9c5449bde6902c\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xab9c5449bde6902c Time: 1.4877\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xac4736b5b00e1531\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xac4736b5b00e1531 Time: 2.53072\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 1.09656\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb0bf64026e546f4d\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xb0bf64026e546f4d Time: 2.1896\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xb26e93bd0702f504\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xb26e93bd0702f504 Time: 1.97126\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xb307bc772518d3d7\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xb307bc772518d3d7 Time: 2.0968\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb7dc3705357cc965\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xb7dc3705357cc965 Time: 1.07394\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xbb3d6545e4864f26\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xbb3d6545e4864f26 Time: 1.65669\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xbfc71f913e286527\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xbfc71f913e286527 Time: 1.72437\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 2.71949\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xc684285f13ba11d0\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xc684285f13ba11d0 Time: 1.65754\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xc6e0905d983b4a62\n",
      "[10/18/2022-15:54:18] [V] [TRT] Tactic: 0xc6e0905d983b4a62 Time: 2.1874\n",
      "[10/18/2022-15:54:18] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0xc8ee1e4cdf0d8f84\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xc8ee1e4cdf0d8f84 Time: 5.43835\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xcb7b50f35a87094b\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xcb7b50f35a87094b Time: 1.76888\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd076fab92f5706c9\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xd076fab92f5706c9 Time: 1.15199\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd297ae2cdb8b1406\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xd297ae2cdb8b1406 Time: 2.18297\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.67707\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 1.65506\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xd825f95894186a22\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xd825f95894186a22 Time: 2.62949\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xd9d1d89fceeca81a\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xd9d1d89fceeca81a Time: 3.14584\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xdb70c5e9779254fb\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xdb70c5e9779254fb Time: 2.05832\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 1.60587\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xe84b9aaa289245c0\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xe84b9aaa289245c0 Time: 3.25632\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xe9fa7b19132889a8\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xe9fa7b19132889a8 Time: 2.93537\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0xf1d5fc0783e71536\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xf1d5fc0783e71536 Time: 2.5837\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xf368aae1fb20baa1\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xf368aae1fb20baa1 Time: 3.14205\n",
      "[10/18/2022-15:54:19] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 1.43811\n",
      "[10/18/2022-15:54:19] [V] [TRT] Fastest Tactic: 0x1dcf9babce3d9b3b Time: 1.03598\n",
      "[10/18/2022-15:54:19] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:54:19] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:19] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:19] [V] [TRT] --------------- Timing Runner: Conv_7 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:19] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:19] [V] [TRT] --------------- Timing Runner: Conv_7 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:19] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:19] [V] [TRT] --------------- Timing Runner: Conv_7 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0x0000000000000000 Time: 9.082\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0x0000000000000001 Time: 7.05473\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0x0000000000000002 Time: 9.57621\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0x0000000000000004 Time: 26.9545\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0x0000000000000005 Time: 16.3661\n",
      "[10/18/2022-15:54:19] [V] [TRT] Tactic: 0x0000000000000038 Time: 9.30567\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x0000000000000039 Time: 7.04153\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x000000000000003a Time: 9.56382\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x000000000000003c Time: 27.1474\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x000000000000003d Time: 16.3896\n",
      "[10/18/2022-15:54:20] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 7.04153\n",
      "[10/18/2022-15:54:20] [V] [TRT] --------------- Timing Runner: Conv_7 (CublasConvolution)\n",
      "[10/18/2022-15:54:20] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:20] [V] [TRT] --------------- Timing Runner: Conv_7 (CaskConvolution)\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 2.80994\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 2.94206\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 3.66747\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 3.41633\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 5.10688\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 3.29448\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 3.25808\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 3.33053\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 6.06126\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 4.23812\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 4.36933\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 3.77673\n",
      "[10/18/2022-15:54:20] [V] [TRT] Fastest Tactic: 0x18597bd4a7d0164d Time: 2.80994\n",
      "[10/18/2022-15:54:20] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:20] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:54:20] [V] [TRT] --------------- Timing Runner: Conv_7 (CublasConvolution)\n",
      "[10/18/2022-15:54:20] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:20] [V] [TRT] --------------- Timing Runner: Conv_7 (CaskConvolution)\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 3.29033\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:54:20] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 3.47275\n",
      "[10/18/2022-15:54:20] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 3.29904\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 6.67611\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 4.04279\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 3.27329\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 3.31045\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 3.78997\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 6.48894\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 2.75133\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 5.20012\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 2.90071\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 4.75497\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 5.44442\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0xf92663d88255134b Time: 3.40865\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 6.51335\n",
      "[10/18/2022-15:54:21] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 4.80133\n",
      "[10/18/2022-15:54:21] [V] [TRT] Fastest Tactic: 0x946eca69f99ddcb4 Time: 2.75133\n",
      "[10/18/2022-15:54:21] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:21] [V] [TRT] *************** Autotuning format combination: Half(200704,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:21] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:21] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:21] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:21] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:21] [V] [TRT] --------------- Timing Runner: Conv_7 (CudnnConvolution)\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.7625\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x0000000000000001 Time: 4.75399\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x0000000000000002 Time: 7.78126\n",
      "[10/18/2022-15:54:21] [V] [TRT] Tactic: 0x0000000000000004 Time: 24.1474\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x0000000000000005 Time: 14.4862\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.14578\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x000000000000003a Time: 7.79733\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x000000000000003c Time: 24.1201\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x000000000000003d Time: 14.862\n",
      "[10/18/2022-15:54:22] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 4.75399\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [V] [TRT] --------------- Timing Runner: Conv_7 (CublasConvolution)\n",
      "[10/18/2022-15:54:22] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [V] [TRT] --------------- Timing Runner: Conv_7 (CaskConvolution)\n",
      "[10/18/2022-15:54:22] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:22] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:54:22] [V] [TRT] *************** Autotuning format combination: Half(100352,3136:2,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [V] [TRT] --------------- Timing Runner: Conv_7 (CaskConvolution)\n",
      "[10/18/2022-15:54:22] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:22] [V] [TRT] *************** Autotuning format combination: Half(100352,3136:2,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [V] [TRT] --------------- Timing Runner: Conv_7 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:22] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [V] [TRT] --------------- Timing Runner: Conv_7 (CublasConvolution)\n",
      "[10/18/2022-15:54:22] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [V] [TRT] --------------- Timing Runner: Conv_7 (CaskConvolution)\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 1.40576\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 1.37917\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 1.63844\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x446c8c788145836a Time: 2.5907\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 2.61823\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 1.95935\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.61251\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x97afba3735828021 Time: 2.01438\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 1.84498\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 2.15719\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.37399\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 1.99504\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 1.75045\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.35246\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 1.65665\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 1.68872\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 2.14687\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.32501\n",
      "[10/18/2022-15:54:22] [V] [TRT] Fastest Tactic: 0x21904dd9d0cd407e Time: 1.37917\n",
      "[10/18/2022-15:54:22] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:22] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [V] [TRT] --------------- Timing Runner: Conv_7 (CublasConvolution)\n",
      "[10/18/2022-15:54:22] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [V] [TRT] --------------- Timing Runner: Conv_7 (CaskConvolution)\n",
      "[10/18/2022-15:54:22] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:22] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [V] [TRT] --------------- Timing Runner: Conv_7 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:22] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [V] [TRT] --------------- Timing Runner: Conv_7 (CublasConvolution)\n",
      "[10/18/2022-15:54:22] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:22] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:22] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:22] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:22] [V] [TRT] --------------- Timing Runner: Conv_7 (CaskConvolution)\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 1.3745\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 1.68287\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 1.39353\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 1.66475\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 1.43067\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 1.23348\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 1.39537\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x249110624ee04937 Time: 1.22324\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 1.19445\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 1.18023\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:54:22] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 1.16292\n",
      "[10/18/2022-15:54:22] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 1.16841\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 1.51637\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.62638\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 1.42314\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 1.21183\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 1.44139\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 1.62113\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 1.38854\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 1.69516\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 1.71066\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.52333\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 1.23601\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.80344\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 1.13217\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.79094\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.84293\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.53925\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 1.26437\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 1.20117\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 1.13313\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 1.31456\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 1.20048\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 2.31965\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 1.43986\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x83b35618df65874c Time: 1.65692\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 1.49699\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 1.47102\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 1.78525\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 1.23682\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 1.41312\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 1.28194\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 1.45517\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 1.83647\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 1.42073\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 1.25222\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 1.60533\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 1.57872\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 1.22498\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 1.15349\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 1.2824\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 1.25055\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 1.32688\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 1.52866\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.55004\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 1.12444\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 1.17058\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.75659\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 1.15925\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 1.75591\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 1.85877\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 1.28002\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 1.29517\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 1.48267\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 1.81126\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 1.75952\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 1.38646\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 1.146\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 1.21269\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 2.19745\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 1.40588\n",
      "[10/18/2022-15:54:23] [V] [TRT] Conv_7 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 1.15279\n",
      "[10/18/2022-15:54:23] [V] [TRT] Fastest Tactic: 0xc399fdbffdc34032 Time: 1.12444\n",
      "[10/18/2022-15:54:23] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:54:23] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:23] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:23] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:23] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:23] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:23] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:23] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CudnnConvolution)\n",
      "[10/18/2022-15:54:23] [V] [TRT] Tactic: 0x0000000000000000 Time: 17.4509\n",
      "[10/18/2022-15:54:24] [V] [TRT] Tactic: 0x0000000000000001 Time: 15.2919\n",
      "[10/18/2022-15:54:24] [V] [TRT] Tactic: 0x0000000000000002 Time: 17.6495\n",
      "[10/18/2022-15:54:24] [V] [TRT] Tactic: 0x0000000000000004 Time: 34.8788\n",
      "[10/18/2022-15:54:24] [V] [TRT] Tactic: 0x0000000000000005 Time: 24.6986\n",
      "[10/18/2022-15:54:24] [V] [TRT] Tactic: 0x0000000000000038 Time: 17.3804\n",
      "[10/18/2022-15:54:24] [V] [TRT] Tactic: 0x0000000000000039 Time: 15.2688\n",
      "[10/18/2022-15:54:25] [V] [TRT] Tactic: 0x000000000000003a Time: 17.6395\n",
      "[10/18/2022-15:54:25] [V] [TRT] Tactic: 0x000000000000003c Time: 34.8806\n",
      "[10/18/2022-15:54:25] [V] [TRT] Tactic: 0x000000000000003d Time: 24.6877\n",
      "[10/18/2022-15:54:25] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 15.2688\n",
      "[10/18/2022-15:54:25] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CublasConvolution)\n",
      "[10/18/2022-15:54:25] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:25] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CaskConvolution)\n",
      "[10/18/2022-15:54:25] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:25] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 4.18456\n",
      "[10/18/2022-15:54:25] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:25] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.17382\n",
      "[10/18/2022-15:54:25] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:54:25] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 4.45875\n",
      "[10/18/2022-15:54:25] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:54:25] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 4.28851\n",
      "[10/18/2022-15:54:25] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:54:25] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 5.68507\n",
      "[10/18/2022-15:54:25] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:25] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 4.03781\n",
      "[10/18/2022-15:54:25] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:54:25] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 4.09721\n",
      "[10/18/2022-15:54:25] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:25] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 4.04144\n",
      "[10/18/2022-15:54:25] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 9.5744\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 5.13588\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 4.94061\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 4.31953\n",
      "[10/18/2022-15:54:26] [V] [TRT] Fastest Tactic: 0x5e7d1125e7896624 Time: 4.03781\n",
      "[10/18/2022-15:54:26] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:26] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(802816,1,14336,256) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:54:26] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CublasConvolution)\n",
      "[10/18/2022-15:54:26] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:26] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CaskConvolution)\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 4.17324\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 4.45822\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 4.15609\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 9.20944\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 4.46917\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 4.2449\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 4.18096\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 4.50911\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 8.99757\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.17184\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 6.97968\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 4.15331\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 4.9114\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 7.20847\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0xf92663d88255134b Time: 4.29495\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 9.00066\n",
      "[10/18/2022-15:54:26] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:54:26] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 6.53195\n",
      "[10/18/2022-15:54:26] [V] [TRT] Fastest Tactic: 0xb90177ab6d659acd Time: 4.15331\n",
      "[10/18/2022-15:54:26] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:54:26] [V] [TRT] *************** Autotuning format combination: Half(200704,3136,56,1), Half(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:26] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:26] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:26] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:26] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:26] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:26] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:26] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:26] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CudnnConvolution)\n",
      "[10/18/2022-15:54:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 11.3345\n",
      "[10/18/2022-15:54:27] [V] [TRT] Tactic: 0x0000000000000001 Time: 9.33621\n",
      "[10/18/2022-15:54:27] [V] [TRT] Tactic: 0x0000000000000002 Time: 11.9991\n",
      "[10/18/2022-15:54:27] [V] [TRT] Tactic: 0x0000000000000004 Time: 28.462\n",
      "[10/18/2022-15:54:27] [V] [TRT] Tactic: 0x0000000000000005 Time: 18.7176\n",
      "[10/18/2022-15:54:27] [V] [TRT] Tactic: 0x0000000000000038 Time: 11.5058\n",
      "[10/18/2022-15:54:27] [V] [TRT] Tactic: 0x000000000000003a Time: 12.2304\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x000000000000003c Time: 28.4821\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x000000000000003d Time: 18.7355\n",
      "[10/18/2022-15:54:28] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 9.33621\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CublasConvolution)\n",
      "[10/18/2022-15:54:28] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CaskConvolution)\n",
      "[10/18/2022-15:54:28] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:54:28] [V] [TRT] *************** Autotuning format combination: Half(100352,3136:2,56,1), Half(401408,3136:2,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:28] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CublasConvolution)\n",
      "[10/18/2022-15:54:28] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CaskConvolution)\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.20042\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.21141\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 2.28703\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x446c8c788145836a Time: 2.37648\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 2.53738\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 2.28027\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.77783\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x97afba3735828021 Time: 2.29083\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 2.16937\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 2.51348\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.82734\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 2.35023\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 2.17913\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.6035\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 2.03167\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 2.03513\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 2.37989\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.68231\n",
      "[10/18/2022-15:54:28] [V] [TRT] Fastest Tactic: 0xdb5acaea7b0746d5 Time: 2.03167\n",
      "[10/18/2022-15:54:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:54:28] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8), Float(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CublasConvolution)\n",
      "[10/18/2022-15:54:28] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CaskConvolution)\n",
      "[10/18/2022-15:54:28] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:28] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8), Half(100352,1:8,1792,32) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:28] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CublasConvolution)\n",
      "[10/18/2022-15:54:28] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:28] [V] [TRT] --------------- Timing Runner: Conv_8 + Add_9 + Relu_10 (CaskConvolution)\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 2.07931\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 2.03935\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 2.07702\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 2.42981\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 2.06438\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 1.92732\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 1.87831\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x249110624ee04937 Time: 1.98685\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 1.98219\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 2.00733\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 1.95774\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 1.95881\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 1.87663\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 2.02167\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 1.98293\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 1.97517\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 2.05966\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 2.07982\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:54:28] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 2.02215\n",
      "[10/18/2022-15:54:28] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 2.04595\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 2.12228\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 2.30927\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 1.95957\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 2.23337\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 1.98308\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 2.40523\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 2.20882\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 2.0242\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 2.21459\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 1.97273\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 2.17041\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 2.09979\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 1.96181\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 2.20821\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 2.05377\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x83b35618df65874c Time: 2.12963\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 2.26041\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 2.66737\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 1.8944\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 2.32658\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 1.98743\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 2.46431\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 2.27672\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 2.08811\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 2.62967\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 2.07333\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 2.03659\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 2.07373\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 2.02881\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 1.94355\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 2.19346\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 2.0724\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 2.00024\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 2.43334\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 2.31038\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 2.40744\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 2.07302\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 2.30395\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 1.96377\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 2.03303\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 2.10915\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 2.02721\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 1.97806\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 2.10939\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 2.24195\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 1.91882\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 2.16968\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 1.97791\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 1.91894\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:54:29] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 2.19859\n",
      "[10/18/2022-15:54:29] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:54:30] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 2.10154\n",
      "[10/18/2022-15:54:30] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:30] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 1.94353\n",
      "[10/18/2022-15:54:30] [V] [TRT] Fastest Tactic: 0x2c6739dc8daca583 Time: 1.87663\n",
      "[10/18/2022-15:54:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:54:30] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:30] [V] [TRT] *************** Autotuning format combination: Float(802816,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:30] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:30] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:30] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:30] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:30] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CudnnConvolution)\n",
      "[10/18/2022-15:54:30] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.53749\n",
      "[10/18/2022-15:54:30] [V] [TRT] Tactic: 0x0000000000000001 Time: 4.13885\n",
      "[10/18/2022-15:54:30] [V] [TRT] Tactic: 0x0000000000000002 Time: 11.0866\n",
      "[10/18/2022-15:54:30] [V] [TRT] Tactic: 0x0000000000000004 Time: 24.054\n",
      "[10/18/2022-15:54:30] [V] [TRT] Tactic: 0x0000000000000005 Time: 14.0496\n",
      "[10/18/2022-15:54:30] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.17579\n",
      "[10/18/2022-15:54:30] [V] [TRT] Tactic: 0x0000000000000039 Time: 4.17069\n",
      "[10/18/2022-15:54:30] [V] [TRT] Tactic: 0x000000000000003a Time: 11.0829\n",
      "[10/18/2022-15:54:30] [V] [TRT] Tactic: 0x000000000000003c Time: 24.063\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x000000000000003d Time: 14.0193\n",
      "[10/18/2022-15:54:31] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 4.13885\n",
      "[10/18/2022-15:54:31] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CublasConvolution)\n",
      "[10/18/2022-15:54:31] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:31] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CaskConvolution)\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 4.30491\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.78111\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 2.94544\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 5.21158\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 3.688\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 2.80473\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 2.67457\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 2.84207\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 7.20896\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 3.28219\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 3.49344\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 3.18186\n",
      "[10/18/2022-15:54:31] [V] [TRT] Fastest Tactic: 0x7e29bdfccd92c42c Time: 2.67457\n",
      "[10/18/2022-15:54:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:54:31] [V] [TRT] *************** Autotuning format combination: Float(802816,1,14336,256) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:54:31] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CublasConvolution)\n",
      "[10/18/2022-15:54:31] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:31] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CaskConvolution)\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 5.26432\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 3.27768\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 5.23321\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 3.825\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 3.98189\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 5.08021\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 4.95474\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 3.26915\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 3.76066\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.90059\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 5.83399\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:54:31] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 5.25321\n",
      "[10/18/2022-15:54:31] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 4.20787\n",
      "[10/18/2022-15:54:32] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 3.53427\n",
      "[10/18/2022-15:54:32] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0xf92663d88255134b Time: 3.28558\n",
      "[10/18/2022-15:54:32] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 3.89963\n",
      "[10/18/2022-15:54:32] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 5.66656\n",
      "[10/18/2022-15:54:32] [V] [TRT] Fastest Tactic: 0x852b455de4263ff7 Time: 3.26915\n",
      "[10/18/2022-15:54:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:32] [V] [TRT] *************** Autotuning format combination: Half(802816,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:32] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:32] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:32] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CudnnConvolution)\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.33387\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0x0000000000000001 Time: 3.16296\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0x0000000000000002 Time: 10.1633\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0x0000000000000004 Time: 22.2659\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0x0000000000000005 Time: 12.983\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.82807\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0x000000000000003a Time: 10.0916\n",
      "[10/18/2022-15:54:32] [V] [TRT] Tactic: 0x000000000000003c Time: 22.2412\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x000000000000003d Time: 13.1835\n",
      "[10/18/2022-15:54:33] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 3.16296\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CublasConvolution)\n",
      "[10/18/2022-15:54:33] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CaskConvolution)\n",
      "[10/18/2022-15:54:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:54:33] [V] [TRT] *************** Autotuning format combination: Half(401408,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CaskConvolution)\n",
      "[10/18/2022-15:54:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:33] [V] [TRT] *************** Autotuning format combination: Half(401408,3136:2,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:33] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CublasConvolution)\n",
      "[10/18/2022-15:54:33] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CaskConvolution)\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 1.15559\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 1.1381\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 1.20656\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x446c8c788145836a Time: 1.45297\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 1.82738\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 1.71694\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 3.521\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x97afba3735828021 Time: 1.5001\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 1.38768\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 1.51515\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 3.13783\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 1.47421\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 1.37684\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.95982\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 2.47223\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 2.45253\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 1.45204\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.87431\n",
      "[10/18/2022-15:54:33] [V] [TRT] Fastest Tactic: 0x21904dd9d0cd407e Time: 1.1381\n",
      "[10/18/2022-15:54:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:33] [V] [TRT] *************** Autotuning format combination: Half(100352,1:8,1792,32) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CublasConvolution)\n",
      "[10/18/2022-15:54:33] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CaskConvolution)\n",
      "[10/18/2022-15:54:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:33] [V] [TRT] *************** Autotuning format combination: Half(100352,1:8,1792,32) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:33] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CublasConvolution)\n",
      "[10/18/2022-15:54:33] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:33] [V] [TRT] --------------- Timing Runner: Conv_11 + Relu_12 (CaskConvolution)\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 1.36672\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 1.13425\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 1.15119\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 1.39134\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 1.18987\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 1.16151\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 2.01146\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x249110624ee04937 Time: 1.36356\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 2.66479\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 1.44706\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 1.14135\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 1.45129\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 2.17062\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.7016\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 1.14138\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 1.36496\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 1.19455\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 1.36777\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 1.35976\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 1.69467\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 1.37282\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.5371\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 1.50733\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.16245\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 1.13836\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.39332\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.16147\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.66916\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 1.36735\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 1.70135\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 1.30392\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 1.35843\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 1.49878\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 1.2642\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 1.18813\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x83b35618df65874c Time: 1.38211\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 1.39237\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 1.76859\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 3.04099\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 1.38579\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 1.14247\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:54:33] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 2.74588\n",
      "[10/18/2022-15:54:33] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 1.50016\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 1.36896\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 1.52192\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 1.37184\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 1.15734\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 1.15982\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 1.36133\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 1.15068\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 1.46914\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 1.39116\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 1.36578\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 1.3971\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.49643\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 1.3002\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 1.75426\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.38909\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 1.14191\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 1.13417\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 1.36894\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 1.36488\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 1.16765\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 1.36251\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 1.15915\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 3.03684\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 1.37466\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 1.13904\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 1.15659\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 1.23968\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 1.38213\n",
      "[10/18/2022-15:54:34] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 1.14032\n",
      "[10/18/2022-15:54:34] [V] [TRT] Fastest Tactic: 0xd8c128ae16cb4132 Time: 1.13417\n",
      "[10/18/2022-15:54:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:54:34] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_13 + Relu_14.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(100352,3136:2,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_13 + Relu_14.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_13 + Relu_14.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_13 + Relu_14 (CaskConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_13 + Relu_14.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(802816,1,14336,256) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(200704,3136,56,1), Half(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(100352,3136:2,56,1), Half(401408,3136:2,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8), Float(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_15 + Add_16 + Relu_17 (CublasConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_15 + Add_16 + Relu_17 (CaskConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8), Half(100352,1:8,1792,32) -> Half(100352,1:8,1792,32) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Float(802816,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Float(802816,1,14336,256) -> Float(200704,1,3584,64) ***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(802816,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_18 + Relu_19.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(401408,3136:2,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_18 + Relu_19.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_18 + Relu_19 (CaskConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(401408,3136:2,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_18 + Relu_19.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(100352,1:8,1792,32) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_18 + Relu_19.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_18 + Relu_19 (CublasConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_18 + Relu_19.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_18 + Relu_19 (CaskConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(100352,1:8,1792,32) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_18 + Relu_19.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64) -> Float(200704,1,3584,64) ***************\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(200704,3136,56,1) -> Half(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_20 + Relu_21.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(100352,3136:2,56,1) -> Half(100352,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_20 + Relu_21.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8) -> Float(200704,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_20 + Relu_21.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_20 + Relu_21 (CaskConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8) -> Half(25088,1:8,448,8) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_20 + Relu_21.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Float(200704,3136,56,1), Float(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Float(200704,1,3584,64), Float(802816,1,14336,256) -> Float(802816,1,14336,256) ***************\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(200704,3136,56,1), Half(802816,3136,56,1) -> Half(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(100352,3136:2,56,1), Half(401408,3136:2,56,1) -> Half(401408,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8), Float(802816,3136,56,1) -> Float(802816,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_22 + Add_23 + Relu_24 (CublasConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_22 + Add_23 + Relu_24 (CaskConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,448,8), Half(100352,1:8,1792,32) -> Half(100352,1:8,1792,32) ***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:34] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:34] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:34] [V] [TRT] *************** Autotuning format combination: Float(802816,3136,56,1) -> Float(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:34] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CudnnConvolution)\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 10.8989\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0x0000000000000001 Time: 8.01247\n",
      "[10/18/2022-15:54:34] [V] [TRT] Tactic: 0x0000000000000002 Time: 14.9436\n",
      "[10/18/2022-15:54:35] [V] [TRT] Tactic: 0x0000000000000004 Time: 38.0428\n",
      "[10/18/2022-15:54:35] [V] [TRT] Tactic: 0x0000000000000005 Time: 22.7108\n",
      "[10/18/2022-15:54:35] [V] [TRT] Tactic: 0x0000000000000038 Time: 11.1588\n",
      "[10/18/2022-15:54:35] [V] [TRT] Tactic: 0x0000000000000039 Time: 8.02235\n",
      "[10/18/2022-15:54:35] [V] [TRT] Tactic: 0x000000000000003a Time: 15.1062\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x000000000000003c Time: 38.1299\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x000000000000003d Time: 22.3503\n",
      "[10/18/2022-15:54:36] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 8.01247\n",
      "[10/18/2022-15:54:36] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CublasConvolution)\n",
      "[10/18/2022-15:54:36] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:36] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CaskConvolution)\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 4.55993\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.96494\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 6.089\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 5.28357\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 7.55974\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 5.45881\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 5.18186\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 5.48364\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 7.67598\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 7.11885\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 6.84524\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 6.19638\n",
      "[10/18/2022-15:54:36] [V] [TRT] Fastest Tactic: 0x18597bd4a7d0164d Time: 4.55993\n",
      "[10/18/2022-15:54:36] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:36] [V] [TRT] *************** Autotuning format combination: Float(802816,1,14336,256) -> Float(401408,1,7168,128) ***************\n",
      "[10/18/2022-15:54:36] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CublasConvolution)\n",
      "[10/18/2022-15:54:36] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:36] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CaskConvolution)\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 5.42555\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 5.95177\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:54:36] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 5.71603\n",
      "[10/18/2022-15:54:36] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 7.45911\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 7.82241\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 5.28558\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 5.41647\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 5.92603\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 7.15018\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.98309\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 6.54518\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 5.64897\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 8.40207\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 6.41719\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0xf92663d88255134b Time: 5.5966\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 7.38081\n",
      "[10/18/2022-15:54:37] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 6.11123\n",
      "[10/18/2022-15:54:37] [V] [TRT] Fastest Tactic: 0x946eca69f99ddcb4 Time: 4.98309\n",
      "[10/18/2022-15:54:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:37] [V] [TRT] *************** Autotuning format combination: Half(802816,3136,56,1) -> Half(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:54:37] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:37] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CudnnConvolution)\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 9.25533\n",
      "[10/18/2022-15:54:37] [V] [TRT] Tactic: 0x0000000000000001 Time: 6.56576\n",
      "[10/18/2022-15:54:38] [V] [TRT] Tactic: 0x0000000000000002 Time: 12.142\n",
      "[10/18/2022-15:54:38] [V] [TRT] Tactic: 0x0000000000000004 Time: 35.2488\n",
      "[10/18/2022-15:54:38] [V] [TRT] Tactic: 0x0000000000000005 Time: 20.8904\n",
      "[10/18/2022-15:54:38] [V] [TRT] Tactic: 0x0000000000000038 Time: 9.67738\n",
      "[10/18/2022-15:54:38] [V] [TRT] Tactic: 0x000000000000003a Time: 11.9422\n",
      "[10/18/2022-15:54:38] [V] [TRT] Tactic: 0x000000000000003c Time: 35.3432\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x000000000000003d Time: 20.6242\n",
      "[10/18/2022-15:54:39] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 6.56576\n",
      "[10/18/2022-15:54:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:39] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:39] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CublasConvolution)\n",
      "[10/18/2022-15:54:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:39] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:39] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CaskConvolution)\n",
      "[10/18/2022-15:54:39] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:54:39] [V] [TRT] *************** Autotuning format combination: Half(401408,3136:2,56,1) -> Half(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:54:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:39] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:39] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CaskConvolution)\n",
      "[10/18/2022-15:54:39] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:39] [V] [TRT] *************** Autotuning format combination: Half(401408,3136:2,56,1) -> Half(200704,3136:2,56,1) ***************\n",
      "[10/18/2022-15:54:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:39] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:39] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:39] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:39] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:39] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CublasConvolution)\n",
      "[10/18/2022-15:54:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:39] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:39] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CaskConvolution)\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.1168\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.59011\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 3.21183\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x446c8c788145836a Time: 3.51022\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 3.4578\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 3.01435\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 3.19038\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x97afba3735828021 Time: 2.88534\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 2.85374\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 3.18288\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 3.15626\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 2.91466\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 2.82946\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 3.10799\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 2.61809\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 2.66619\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 3.09191\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 3.19773\n",
      "[10/18/2022-15:54:39] [V] [TRT] Fastest Tactic: 0x16eafdbc5869b184 Time: 2.1168\n",
      "[10/18/2022-15:54:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:54:39] [V] [TRT] *************** Autotuning format combination: Half(100352,1:8,1792,32) -> Float(401408,3136,56,1) ***************\n",
      "[10/18/2022-15:54:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:39] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:39] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CublasConvolution)\n",
      "[10/18/2022-15:54:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:39] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:39] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CaskConvolution)\n",
      "[10/18/2022-15:54:39] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:39] [V] [TRT] *************** Autotuning format combination: Half(100352,1:8,1792,32) -> Half(50176,1:8,896,16) ***************\n",
      "[10/18/2022-15:54:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:39] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:39] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:39] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:39] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CublasConvolution)\n",
      "[10/18/2022-15:54:39] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:39] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:39] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:39] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:39] [V] [TRT] --------------- Timing Runner: Conv_25 + Relu_26 (CaskConvolution)\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 2.01379\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 1.93156\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 1.65177\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 2.07541\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 1.39434\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 1.38303\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 2.28333\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x249110624ee04937 Time: 1.59621\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 2.88827\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 1.59426\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 1.35953\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 1.57725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 2.32224\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 2.07678\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 1.8959\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 1.59861\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 1.39505\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 2.17147\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 1.83374\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 2.06869\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 2.22738\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.79672\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:54:39] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 1.75552\n",
      "[10/18/2022-15:54:39] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 2.00997\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 1.54917\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 2.13376\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.92544\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 2.0108\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 1.6288\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 1.8537\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 1.48375\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 1.76976\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 1.82466\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 2.94268\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 1.41844\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x83b35618df65874c Time: 2.26753\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 1.69954\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 1.98011\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 3.2689\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 1.79585\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 1.85832\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 2.94747\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 1.79376\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 2.42042\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 1.81711\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 1.57752\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 1.69749\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 1.68272\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 1.75017\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 1.36472\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 1.70554\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 1.59366\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 1.58047\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 1.63991\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.81798\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 1.4431\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 2.07316\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 2.10075\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 1.35566\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 2.06592\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 2.61768\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 1.57872\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 1.37947\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 1.84315\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 1.92011\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 3.19253\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 1.64472\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 1.4294\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 1.37916\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 2.67937\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 1.61031\n",
      "[10/18/2022-15:54:40] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 1.35344\n",
      "[10/18/2022-15:54:40] [V] [TRT] Fastest Tactic: 0xfcd06da0f3c31fd1 Time: 1.35344\n",
      "[10/18/2022-15:54:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:40] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:40] [V] [TRT] *************** Autotuning format combination: Float(401408,3136,56,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:54:40] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:40] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:40] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:40] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CudnnConvolution)\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x0000000000000000 Time: 8.49792\n",
      "[10/18/2022-15:54:40] [V] [TRT] Tactic: 0x0000000000000001 Time: 6.11248\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x0000000000000002 Time: 13.0761\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x0000000000000005 Time: 12.557\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x0000000000000038 Time: 8.97773\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x0000000000000039 Time: 5.94917\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x000000000000003a Time: 13.2298\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x000000000000003d Time: 12.4858\n",
      "[10/18/2022-15:54:41] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 5.94917\n",
      "[10/18/2022-15:54:41] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)\n",
      "[10/18/2022-15:54:41] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.87888\n",
      "[10/18/2022-15:54:41] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x27728c886a448c5a\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x27728c886a448c5a Time: 5.29685\n",
      "[10/18/2022-15:54:41] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 5.27198\n",
      "[10/18/2022-15:54:41] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x597d29027694c20b\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x597d29027694c20b Time: 5.26323\n",
      "[10/18/2022-15:54:41] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 5.48177\n",
      "[10/18/2022-15:54:41] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x62b2ffd9a5c0cfb5\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x62b2ffd9a5c0cfb5 Time: 8.11508\n",
      "[10/18/2022-15:54:41] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x8d5c64a52fab02c9\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x8d5c64a52fab02c9 Time: 8.89516\n",
      "[10/18/2022-15:54:41] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 5.46012\n",
      "[10/18/2022-15:54:41] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x93a1176336e5b9f6\n",
      "[10/18/2022-15:54:41] [V] [TRT] Tactic: 0x93a1176336e5b9f6 Time: 6.87723\n",
      "[10/18/2022-15:54:41] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x994f5b723e2d80da\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0x994f5b723e2d80da Time: 6.66156\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x128x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xac49795b871b0d29\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0xac49795b871b0d29 Time: 6.0299\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb6717e61503d5e9b\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0xb6717e61503d5e9b Time: 6.30899\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 7.5605\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 6.90957\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd1338f4b38d341e2\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0xd1338f4b38d341e2 Time: 6.82915\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x256x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd2aa21bfe2167c0c\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0xd2aa21bfe2167c0c Time: 10.6767\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xe40b38338a3a7d7e\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0xe40b38338a3a7d7e Time: 7.41727\n",
      "[10/18/2022-15:54:42] [V] [TRT] Fastest Tactic: 0x195431d38ba5af88 Time: 4.87888\n",
      "[10/18/2022-15:54:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:42] [V] [TRT] *************** Autotuning format combination: Float(401408,1,7168,128) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:54:42] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x0447933cc2be855a\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0x0447933cc2be855a Time: 5.61763\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 5.63436\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x0e2033f7517a807f\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0x0e2033f7517a807f Time: 5.80845\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x2595402367cdee5c\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0x2595402367cdee5c Time: 8.60027\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3eba442f4c9c4f50\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0x3eba442f4c9c4f50 Time: 6.65451\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x43334a9c8840c773\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0x43334a9c8840c773 Time: 6.7543\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 5.58988\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 5.73808\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:54:42] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 6.39817\n",
      "[10/18/2022-15:54:42] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x66e3239eee98201e\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0x66e3239eee98201e Time: 6.41798\n",
      "[10/18/2022-15:54:43] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 5.75722\n",
      "[10/18/2022-15:54:43] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 5.75049\n",
      "[10/18/2022-15:54:43] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 5.55456\n",
      "[10/18/2022-15:54:43] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x963db12d24e61b80\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0x963db12d24e61b80 Time: 6.72003\n",
      "[10/18/2022-15:54:43] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb132670a7750e065\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0xb132670a7750e065 Time: 8.65485\n",
      "[10/18/2022-15:54:43] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca84742beb9f9767\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0xca84742beb9f9767 Time: 5.75624\n",
      "[10/18/2022-15:54:43] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xd2b62ec40baf8ee4\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0xd2b62ec40baf8ee4 Time: 5.76453\n",
      "[10/18/2022-15:54:43] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xde4165142218dab8\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0xde4165142218dab8 Time: 6.71248\n",
      "[10/18/2022-15:54:43] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 6.45387\n",
      "[10/18/2022-15:54:43] [V] [TRT] Fastest Tactic: 0x946eca69f99ddcb4 Time: 5.55456\n",
      "[10/18/2022-15:54:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:43] [V] [TRT] *************** Autotuning format combination: Half(401408,3136,56,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:54:43] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:43] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:43] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:43] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CudnnConvolution)\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.80908\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0x0000000000000001 Time: 6.42607\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0x0000000000000002 Time: 10.9413\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0x0000000000000005 Time: 11.9105\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0x0000000000000038 Time: 8.55069\n",
      "[10/18/2022-15:54:43] [V] [TRT] Tactic: 0x000000000000003a Time: 10.5025\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x000000000000003d Time: 11.8796\n",
      "[10/18/2022-15:54:44] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 6.42607\n",
      "[10/18/2022-15:54:44] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:44] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:44] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:44] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)\n",
      "[10/18/2022-15:54:44] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:54:44] [V] [TRT] *************** Autotuning format combination: Half(200704,3136:2,56,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:54:44] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:44] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:44] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:44] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:44] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:44] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:44] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:44] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:44] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x0fe4a9cce7ed878b\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x0fe4a9cce7ed878b Time: 2.63119\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.70249\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.91195\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 2.94124\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x4092cbc840fbea35\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x4092cbc840fbea35 Time: 2.95175\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x446c8c788145836a Time: 3.47517\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 3.39362\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 2.92357\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.90293\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0x98a00f59a4b141f0\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x98a00f59a4b141f0 Time: 3.3831\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 3.46939\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 3.05465\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xcbe3f30275b04323\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0xcbe3f30275b04323 Time: 3.00853\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.81483\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xd7d66d5d03a72c4e\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0xd7d66d5d03a72c4e Time: 3.37013\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 3.3292\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.89238\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xfc994367fd14b2d9\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0xfc994367fd14b2d9 Time: 2.87136\n",
      "[10/18/2022-15:54:44] [V] [TRT] Fastest Tactic: 0x0fe4a9cce7ed878b Time: 2.63119\n",
      "[10/18/2022-15:54:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0fe4a9cce7ed878b\n",
      "[10/18/2022-15:54:44] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,896,16) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:54:44] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:44] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:44] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:44] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)\n",
      "[10/18/2022-15:54:44] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:44] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,896,16) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:54:44] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:44] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:44] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:44] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:44] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:44] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:44] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:44] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:44] [V] [TRT] --------------- Timing Runner: Conv_27 + Relu_28 (CaskConvolution)\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x00a425145e84482b\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x00a425145e84482b Time: 2.41582\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x03512591e8ea2977\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x03512591e8ea2977 Time: 1.70132\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x0559d1d2893a8768\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x0559d1d2893a8768 Time: 2.44752\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x095000b22a78f234\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x095000b22a78f234 Time: 1.36456\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x0b906efbde4dc01a\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x0b906efbde4dc01a Time: 1.76813\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x0c0088d5808566d2\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x0c0088d5808566d2 Time: 1.06903\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x0caa5410b61e6cc5\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x0caa5410b61e6cc5 Time: 2.71271\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x0e0f7f10867063ba\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x0e0f7f10867063ba Time: 1.22586\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x0e131ddbafdfe235\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x0e131ddbafdfe235 Time: 1.36572\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x0ecf8dc91198fd5e\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x0ecf8dc91198fd5e Time: 1.18652\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0x159236c6c22f62ce\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x159236c6c22f62ce Time: 2.82797\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x15ecbd82c22a023f\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x15ecbd82c22a023f Time: 1.18349\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x18ef97651ad5379a\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x18ef97651ad5379a Time: 1.97779\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x1981adfb6b6fd8b9\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x1981adfb6b6fd8b9 Time: 1.43825\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x1b099f7ac29a2a6a\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x1b099f7ac29a2a6a Time: 1.69942\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x1b9cb8d78519a728\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x1b9cb8d78519a728 Time: 2.19633\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0x1c23f4a19fbcb518\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x1c23f4a19fbcb518 Time: 1.79001\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 1.36871\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x1de724868edf11b0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x1de724868edf11b0 Time: 1.34731\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.976841\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x30150d05024bc911\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x30150d05024bc911 Time: 1.20453\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x32789ed2e6c7b43b\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x32789ed2e6c7b43b Time: 1.19332\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x33fc6102b341eb5d\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x33fc6102b341eb5d Time: 2.90933\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.68786\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x348653930e0a64e2\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x348653930e0a64e2 Time: 1.72552\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x350e898a5a20ad00\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x350e898a5a20ad00 Time: 1.05475\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x36662b4d547eefc7\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x36662b4d547eefc7 Time: 1.36133\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x490a097d77573bff\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x490a097d77573bff Time: 1.01263\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x4c6a6da741444412\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x4c6a6da741444412 Time: 1.62619\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x4e34a65090c3b86f\n",
      "[10/18/2022-15:54:44] [V] [TRT] Tactic: 0x4e34a65090c3b86f Time: 0.956416\n",
      "[10/18/2022-15:54:44] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x504f864880743a14\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x504f864880743a14 Time: 2.69216\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x5128cdf162fe56b6\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x5128cdf162fe56b6 Time: 1.4615\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.51145\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r3s3 Tactic: 0x5252dc6c9c5f3aff\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x5252dc6c9c5f3aff Time: 1.89235\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x54b287be85c1522c\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x54b287be85c1522c Time: 2.78618\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x55fb34a08663e5ae\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x55fb34a08663e5ae Time: 1.77565\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x56c66ffbce24b635\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x56c66ffbce24b635 Time: 1.64103\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x58eea09dffe038fd\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x58eea09dffe038fd Time: 1.37572\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x5bec1fbd955eb827\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x5bec1fbd955eb827 Time: 1.62082\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.6552\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x62bb371b230a886d\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x62bb371b230a886d Time: 1.87619\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.67354\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x69a5b2ac9c5bac16\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x69a5b2ac9c5bac16 Time: 1.41122\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6b44e6396887bed9\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x6b44e6396887bed9 Time: 1.37056\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x6cde8847e8cd796b\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x6cde8847e8cd796b Time: 1.54669\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x6cee4d9c86b4cdd5\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x6cee4d9c86b4cdd5 Time: 1.55311\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.68814\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x721049a39aae27ff\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x721049a39aae27ff Time: 2.48336\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.64835\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x75585ae3e9dedb93\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x75585ae3e9dedb93 Time: 1.97899\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x784dcede905d06c0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x784dcede905d06c0 Time: 1.45483\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 1.36421\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x86903737887c556d\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x86903737887c556d Time: 1.47399\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x8781623566dac7f0\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x8781623566dac7f0 Time: 1.36865\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x8b86a8bb857fff79\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x8b86a8bb857fff79 Time: 2.09072\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x8d73ddfc444be692\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x8d73ddfc444be692 Time: 1.08275\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x9650edb797f919f3\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x9650edb797f919f3 Time: 1.10501\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0x969b1abbb567ac47\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x969b1abbb567ac47 Time: 1.47224\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0x9a0f43b4d1dc46d4\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x9a0f43b4d1dc46d4 Time: 2.45834\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa13cdf70a9d99d45\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xa13cdf70a9d99d45 Time: 2.13935\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa2dad76f719680b5\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xa2dad76f719680b5 Time: 1.86268\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0xa3e778b253a14ca9\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xa3e778b253a14ca9 Time: 2.57234\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0xa5f0bcb42cb01fc7\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xa5f0bcb42cb01fc7 Time: 1.72997\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r3s3 Tactic: 0xa84824f86c61d2d8\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xa84824f86c61d2d8 Time: 1.70198\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 1.05876\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xab9c5449bde6902c\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xab9c5449bde6902c Time: 0.997147\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xac4736b5b00e1531\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xac4736b5b00e1531 Time: 1.35775\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 1.0747\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb0bf64026e546f4d\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xb0bf64026e546f4d Time: 1.19223\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xb26e93bd0702f504\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xb26e93bd0702f504 Time: 2.12893\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xb307bc772518d3d7\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xb307bc772518d3d7 Time: 1.35546\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb7dc3705357cc965\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xb7dc3705357cc965 Time: 1.21199\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xbb3d6545e4864f26\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xbb3d6545e4864f26 Time: 1.00683\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xbfc71f913e286527\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xbfc71f913e286527 Time: 1.87987\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.57943\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xc684285f13ba11d0\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xc684285f13ba11d0 Time: 1.81305\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xc6e0905d983b4a62\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xc6e0905d983b4a62 Time: 2.34582\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0xc8ee1e4cdf0d8f84\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xc8ee1e4cdf0d8f84 Time: 2.74867\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xcb7b50f35a87094b\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xcb7b50f35a87094b Time: 1.86244\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd076fab92f5706c9\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xd076fab92f5706c9 Time: 1.29872\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd297ae2cdb8b1406\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xd297ae2cdb8b1406 Time: 1.30339\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.66227\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.955877\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xd825f95894186a22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xd825f95894186a22 Time: 2.8369\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xd9d1d89fceeca81a\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xd9d1d89fceeca81a Time: 1.82243\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xdb70c5e9779254fb\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xdb70c5e9779254fb Time: 2.20547\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.978272\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xe84b9aaa289245c0\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xe84b9aaa289245c0 Time: 1.59143\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xe9fa7b19132889a8\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xe9fa7b19132889a8 Time: 1.741\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0xf1d5fc0783e71536\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xf1d5fc0783e71536 Time: 1.42892\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xf368aae1fb20baa1\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xf368aae1fb20baa1 Time: 1.60792\n",
      "[10/18/2022-15:54:45] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.940617\n",
      "[10/18/2022-15:54:45] [V] [TRT] Fastest Tactic: 0xfcd06da0f3c31fd1 Time: 0.940617\n",
      "[10/18/2022-15:54:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:45] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:45] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:54:45] [V] [TRT] --------------- Timing Runner: Conv_29 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:45] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:45] [V] [TRT] --------------- Timing Runner: Conv_29 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:45] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:45] [V] [TRT] --------------- Timing Runner: Conv_29 (CudnnConvolution)\n",
      "[10/18/2022-15:54:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.78616\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x0000000000000001 Time: 4.39694\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x0000000000000002 Time: 6.86026\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x0000000000000004 Time: 13.3694\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x0000000000000005 Time: 10.1809\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x0000000000000038 Time: 5.98302\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x0000000000000039 Time: 4.40507\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x000000000000003a Time: 6.70722\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x000000000000003c Time: 13.3796\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x000000000000003d Time: 10.1689\n",
      "[10/18/2022-15:54:46] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 4.39694\n",
      "[10/18/2022-15:54:46] [V] [TRT] --------------- Timing Runner: Conv_29 (CublasConvolution)\n",
      "[10/18/2022-15:54:46] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:46] [V] [TRT] --------------- Timing Runner: Conv_29 (CaskConvolution)\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 2.43331\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 2.47682\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 3.02651\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 2.84528\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 4.05648\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 2.73847\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 2.75531\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 3.11965\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 4.71186\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 3.77095\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 3.60018\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 3.21042\n",
      "[10/18/2022-15:54:46] [V] [TRT] Fastest Tactic: 0x18597bd4a7d0164d Time: 2.43331\n",
      "[10/18/2022-15:54:46] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:46] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:54:46] [V] [TRT] --------------- Timing Runner: Conv_29 (CublasConvolution)\n",
      "[10/18/2022-15:54:46] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:46] [V] [TRT] --------------- Timing Runner: Conv_29 (CaskConvolution)\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:54:46] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 2.72589\n",
      "[10/18/2022-15:54:46] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 2.84222\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 2.68841\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 4.50007\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 3.74433\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 2.57593\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 2.69333\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 2.94876\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 4.46858\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 2.46755\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 3.12993\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 2.73963\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 4.08557\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 3.22366\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0xf92663d88255134b Time: 2.76867\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 4.66008\n",
      "[10/18/2022-15:54:47] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 2.96755\n",
      "[10/18/2022-15:54:47] [V] [TRT] Fastest Tactic: 0x946eca69f99ddcb4 Time: 2.46755\n",
      "[10/18/2022-15:54:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:47] [V] [TRT] *************** Autotuning format combination: Half(100352,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:54:47] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:47] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:47] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:47] [V] [TRT] --------------- Timing Runner: Conv_29 (CudnnConvolution)\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.87371\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x0000000000000001 Time: 3.19349\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x0000000000000002 Time: 5.81691\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x0000000000000004 Time: 12.4132\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x0000000000000005 Time: 9.05248\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x0000000000000038 Time: 5.26426\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x000000000000003a Time: 5.59733\n",
      "[10/18/2022-15:54:47] [V] [TRT] Tactic: 0x000000000000003c Time: 12.4387\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x000000000000003d Time: 9.04689\n",
      "[10/18/2022-15:54:48] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 3.19349\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_29 (CublasConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_29 (CaskConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:54:48] [V] [TRT] *************** Autotuning format combination: Half(50176,784:2,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_29 (CaskConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:48] [V] [TRT] *************** Autotuning format combination: Half(50176,784:2,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_29 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_29 (CublasConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_29 (CaskConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 1.22853\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 1.16684\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 1.29243\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x446c8c788145836a Time: 1.61982\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 1.8391\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 1.63089\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 1.91673\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x97afba3735828021 Time: 1.65689\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 1.4709\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 1.66775\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 1.768\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 1.55254\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 1.44443\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 1.76187\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 1.42416\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 1.40645\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 1.58578\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 1.77649\n",
      "[10/18/2022-15:54:48] [V] [TRT] Fastest Tactic: 0x21904dd9d0cd407e Time: 1.16684\n",
      "[10/18/2022-15:54:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:48] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_29 (CublasConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_29 (CaskConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:48] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_29 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_29 (CublasConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:48] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:48] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_29 (CaskConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 1.18491\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 1.16643\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.877088\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 1.11751\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.650231\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.757851\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 0.909582\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x249110624ee04937 Time: 0.705344\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 0.878007\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 0.897865\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.615077\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 0.892987\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 1.03487\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.21744\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 1.15079\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 0.775689\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 0.70901\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 1.18247\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 1.03254\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 1.21347\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 1.27912\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 0.993175\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 0.984928\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.25046\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 0.855374\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.16538\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.18783\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.08315\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 0.82517\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 0.779118\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 0.841189\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 0.931282\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 0.934176\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 1.52519\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.7312\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x83b35618df65874c Time: 1.25056\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 1.09714\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 1.02788\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 1.13864\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.841147\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 1.10228\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 0.912238\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 0.953701\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 1.40021\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 0.962267\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 0.73344\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.988891\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.977815\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 0.9368\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.654071\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 0.907264\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 0.740937\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.761591\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 0.945591\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 0.988489\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.739145\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.831662\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.21243\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.615461\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 1.28614\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 1.43448\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 0.736229\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.757664\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 1.0256\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 1.21265\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 1.11834\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 0.934089\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 0.859643\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.746249\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 1.57403\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 0.854789\n",
      "[10/18/2022-15:54:48] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:48] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.619589\n",
      "[10/18/2022-15:54:48] [V] [TRT] Fastest Tactic: 0x2a3615ad33745f0b Time: 0.615077\n",
      "[10/18/2022-15:54:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:54:48] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:48] [V] [TRT] *************** Autotuning format combination: Float(802816,3136,56,1), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:48] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:48] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (CudnnConvolution)\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 12.1644\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x0000000000000001 Time: 11.0884\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x0000000000000002 Time: 14.0092\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x0000000000000038 Time: 12.3079\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x0000000000000039 Time: 11.0838\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x000000000000003a Time: 14.0418\n",
      "[10/18/2022-15:54:49] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 11.0838\n",
      "[10/18/2022-15:54:49] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (CaskConvolution)\n",
      "[10/18/2022-15:54:49] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 4.98101\n",
      "[10/18/2022-15:54:49] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 5.47055\n",
      "[10/18/2022-15:54:49] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 7.3535\n",
      "[10/18/2022-15:54:49] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 5.83747\n",
      "[10/18/2022-15:54:49] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 6.00207\n",
      "[10/18/2022-15:54:49] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 6.02505\n",
      "[10/18/2022-15:54:49] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x8d5c64a52fab02c9\n",
      "[10/18/2022-15:54:49] [V] [TRT] Tactic: 0x8d5c64a52fab02c9 Time: 9.17582\n",
      "[10/18/2022-15:54:49] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 5.76456\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x994f5b723e2d80da\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0x994f5b723e2d80da Time: 9.34498\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 7.65092\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x128x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xac49795b871b0d29\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0xac49795b871b0d29 Time: 8.49674\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 7.32223\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 7.35437\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd1338f4b38d341e2\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0xd1338f4b38d341e2 Time: 9.04806\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x256x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd2aa21bfe2167c0c\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0xd2aa21bfe2167c0c Time: 7.53576\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xe40b38338a3a7d7e\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0xe40b38338a3a7d7e Time: 7.50476\n",
      "[10/18/2022-15:54:50] [V] [TRT] Fastest Tactic: 0x18597bd4a7d0164d Time: 4.98101\n",
      "[10/18/2022-15:54:50] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:50] [V] [TRT] *************** Autotuning format combination: Float(802816,1,14336,256), Float(401408,1,14336,512) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:54:50] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (CaskConvolution)\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x0447933cc2be855a\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0x0447933cc2be855a Time: 7.5874\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 4.92043\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x2595402367cdee5c\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0x2595402367cdee5c Time: 7.87215\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 5.49767\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 5.40948\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 8.04064\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:54:50] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 7.54725\n",
      "[10/18/2022-15:54:50] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 5.17949\n",
      "[10/18/2022-15:54:51] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 5.05888\n",
      "[10/18/2022-15:54:51] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 5.44709\n",
      "[10/18/2022-15:54:51] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 8.16333\n",
      "[10/18/2022-15:54:51] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.84555\n",
      "[10/18/2022-15:54:51] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x963db12d24e61b80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0x963db12d24e61b80 Time: 6.25723\n",
      "[10/18/2022-15:54:51] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 6.56004\n",
      "[10/18/2022-15:54:51] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 5.60792\n",
      "[10/18/2022-15:54:51] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xde4165142218dab8\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0xde4165142218dab8 Time: 6.33329\n",
      "[10/18/2022-15:54:51] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0xf92663d88255134b Time: 5.51937\n",
      "[10/18/2022-15:54:51] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 8.16712\n",
      "[10/18/2022-15:54:51] [V] [TRT] Fastest Tactic: 0x946eca69f99ddcb4 Time: 4.84555\n",
      "[10/18/2022-15:54:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:51] [V] [TRT] *************** Autotuning format combination: Half(802816,3136,56,1), Half(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:54:51] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:51] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:51] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:51] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (CudnnConvolution)\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 10.8032\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0x0000000000000001 Time: 14.3044\n",
      "[10/18/2022-15:54:51] [V] [TRT] Tactic: 0x0000000000000002 Time: 11.6258\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x0000000000000038 Time: 11.2408\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x000000000000003a Time: 11.6057\n",
      "[10/18/2022-15:54:52] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 10.8032\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (CaskConvolution)\n",
      "[10/18/2022-15:54:52] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000000\n",
      "[10/18/2022-15:54:52] [V] [TRT] *************** Autotuning format combination: Half(401408,3136:2,56,1), Half(200704,784:2,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:52] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (CaskConvolution)\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.3693\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.57604\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 3.29874\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x446c8c788145836a Time: 3.93509\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 3.82303\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 3.15319\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 3.42917\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x97afba3735828021 Time: 3.4544\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 3.16351\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 3.68385\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 3.36662\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 3.45176\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 3.15091\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 3.6103\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 2.92616\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 2.87685\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 3.62053\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 3.3946\n",
      "[10/18/2022-15:54:52] [V] [TRT] Fastest Tactic: 0x16eafdbc5869b184 Time: 2.3693\n",
      "[10/18/2022-15:54:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:54:52] [V] [TRT] *************** Autotuning format combination: Half(100352,1:8,1792,32), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (CaskConvolution)\n",
      "[10/18/2022-15:54:52] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:52] [V] [TRT] *************** Autotuning format combination: Half(100352,1:8,1792,32), Half(50176,1:8,1792,64) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:52] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:52] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:52] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:52] [V] [TRT] --------------- Timing Runner: Conv_30 + Add_31 + Relu_32 (CaskConvolution)\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x0559d1d2893a8768\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x0559d1d2893a8768 Time: 2.50939\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x0b906efbde4dc01a\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x0b906efbde4dc01a Time: 2.24116\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x0e0f7f10867063ba\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x0e0f7f10867063ba Time: 1.43882\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x0ecf8dc91198fd5e\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x0ecf8dc91198fd5e Time: 1.28431\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 1.4621\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0x159236c6c22f62ce\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x159236c6c22f62ce Time: 1.82889\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x15ecbd82c22a023f\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x15ecbd82c22a023f Time: 1.30717\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x18ef97651ad5379a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x18ef97651ad5379a Time: 2.17936\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x1b099f7ac29a2a6a\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x1b099f7ac29a2a6a Time: 1.96374\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x1b9cb8d78519a728\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x1b9cb8d78519a728 Time: 2.39583\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0x1c23f4a19fbcb518\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x1c23f4a19fbcb518 Time: 1.70043\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 2.22097\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 1.31504\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 1.5562\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 1.52893\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 1.50299\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 1.17258\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x33fc6102b341eb5d\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x33fc6102b341eb5d Time: 2.08411\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 2.05587\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x348653930e0a64e2\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x348653930e0a64e2 Time: 1.69691\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x36662b4d547eefc7\n",
      "[10/18/2022-15:54:52] [V] [TRT] Tactic: 0x36662b4d547eefc7 Time: 1.9816\n",
      "[10/18/2022-15:54:52] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 1.36919\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 1.30784\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 2.20423\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 1.89682\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 2.1908\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x5128cdf162fe56b6\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x5128cdf162fe56b6 Time: 1.64767\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 2.2843\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 2.12358\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 1.72944\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x54b287be85c1522c\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x54b287be85c1522c Time: 2.18323\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x55fb34a08663e5ae\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x55fb34a08663e5ae Time: 2.02575\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x58eea09dffe038fd\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x58eea09dffe038fd Time: 1.44733\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x5bec1fbd955eb827\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x5bec1fbd955eb827 Time: 1.76914\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 2.25927\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x62bb371b230a886d\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x62bb371b230a886d Time: 1.93523\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 2.21115\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x69a5b2ac9c5bac16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x69a5b2ac9c5bac16 Time: 1.44245\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6b44e6396887bed9\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x6b44e6396887bed9 Time: 1.43536\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x6cde8847e8cd796b\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x6cde8847e8cd796b Time: 2.21189\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 2.0523\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 2.0116\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x75585ae3e9dedb93\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x75585ae3e9dedb93 Time: 2.1929\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x784dcede905d06c0\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x784dcede905d06c0 Time: 1.59451\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 1.37918\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 2.05277\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 2.17673\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 1.79565\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x86903737887c556d\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x86903737887c556d Time: 2.03967\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 1.64308\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 1.87977\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 2.07501\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x8b86a8bb857fff79\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x8b86a8bb857fff79 Time: 1.91917\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 2.00746\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x8d73ddfc444be692\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x8d73ddfc444be692 Time: 1.63343\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 2.05475\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x9650edb797f919f3\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x9650edb797f919f3 Time: 1.61945\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0x969b1abbb567ac47\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x969b1abbb567ac47 Time: 2.07514\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 1.33324\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0x9a0f43b4d1dc46d4\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0x9a0f43b4d1dc46d4 Time: 2.51616\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa13cdf70a9d99d45\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xa13cdf70a9d99d45 Time: 2.3236\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa2dad76f719680b5\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xa2dad76f719680b5 Time: 2.17104\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0xa3e778b253a14ca9\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xa3e778b253a14ca9 Time: 2.95617\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0xa5f0bcb42cb01fc7\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xa5f0bcb42cb01fc7 Time: 1.62431\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 1.59217\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xab9c5449bde6902c\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xab9c5449bde6902c Time: 1.58295\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 1.55702\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb0bf64026e546f4d\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xb0bf64026e546f4d Time: 1.85661\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xb26e93bd0702f504\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xb26e93bd0702f504 Time: 1.96018\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb7dc3705357cc965\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xb7dc3705357cc965 Time: 1.68032\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 1.2853\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 1.47979\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xbfc71f913e286527\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xbfc71f913e286527 Time: 1.8571\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 2.11388\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 1.52224\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xc6e0905d983b4a62\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xc6e0905d983b4a62 Time: 2.38783\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 1.61279\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0xc8ee1e4cdf0d8f84\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xc8ee1e4cdf0d8f84 Time: 1.79434\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd076fab92f5706c9\n",
      "[10/18/2022-15:54:53] [V] [TRT] Tactic: 0xd076fab92f5706c9 Time: 1.37919\n",
      "[10/18/2022-15:54:53] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd297ae2cdb8b1406\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xd297ae2cdb8b1406 Time: 1.35872\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 2.3089\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 1.20201\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xd825f95894186a22\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xd825f95894186a22 Time: 2.70533\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 2.15477\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 2.68469\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 1.58863\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 1.88141\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 2.07716\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 1.52475\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xe84b9aaa289245c0\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xe84b9aaa289245c0 Time: 1.61032\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xe9fa7b19132889a8\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xe9fa7b19132889a8 Time: 1.81118\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 1.57199\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0xf1d5fc0783e71536\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xf1d5fc0783e71536 Time: 2.18185\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xf368aae1fb20baa1\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xf368aae1fb20baa1 Time: 1.59017\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 2.49328\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 1.2063\n",
      "[10/18/2022-15:54:54] [V] [TRT] Fastest Tactic: 0x2a3615ad33745f0b Time: 1.17258\n",
      "[10/18/2022-15:54:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:54:54] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:54] [V] [TRT] *************** Autotuning format combination: Float(401408,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:54:54] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:54] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:54] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:54] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:54] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CudnnConvolution)\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.53836\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0x0000000000000001 Time: 3.05913\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0x0000000000000002 Time: 8.30965\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0x0000000000000004 Time: 11.9397\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0x0000000000000005 Time: 9.50192\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0x0000000000000038 Time: 5.89581\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0x0000000000000039 Time: 3.1272\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0x000000000000003a Time: 8.15761\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0x000000000000003c Time: 12.033\n",
      "[10/18/2022-15:54:54] [V] [TRT] Tactic: 0x000000000000003d Time: 9.49862\n",
      "[10/18/2022-15:54:54] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 3.05913\n",
      "[10/18/2022-15:54:54] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CublasConvolution)\n",
      "[10/18/2022-15:54:54] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:54] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CaskConvolution)\n",
      "[10/18/2022-15:54:54] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 2.22795\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 2.23629\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 2.75203\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 2.52343\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 3.55893\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 2.58933\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 2.44589\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 2.61094\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 3.30742\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 3.53211\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 3.38907\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 2.97195\n",
      "[10/18/2022-15:54:55] [V] [TRT] Fastest Tactic: 0x18597bd4a7d0164d Time: 2.22795\n",
      "[10/18/2022-15:54:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:54:55] [V] [TRT] *************** Autotuning format combination: Float(401408,1,14336,512) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:54:55] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CublasConvolution)\n",
      "[10/18/2022-15:54:55] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:55] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CaskConvolution)\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 2.73126\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 2.7197\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 2.70517\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 3.27002\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 3.87625\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 2.44752\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 2.52582\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 2.67347\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 3.27605\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 2.53461\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 3.02628\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 2.68376\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 3.96668\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 3.07407\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0xf92663d88255134b Time: 2.60495\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 3.35407\n",
      "[10/18/2022-15:54:55] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 3.04801\n",
      "[10/18/2022-15:54:55] [V] [TRT] Fastest Tactic: 0x810bd80d0531c0a0 Time: 2.44752\n",
      "[10/18/2022-15:54:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:54:55] [V] [TRT] *************** Autotuning format combination: Half(401408,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:54:55] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:55] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:55] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:55] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:55] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:55] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:55] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CudnnConvolution)\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.81994\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.42717\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x0000000000000002 Time: 5.03144\n",
      "[10/18/2022-15:54:55] [V] [TRT] Tactic: 0x0000000000000004 Time: 11.4896\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x0000000000000005 Time: 8.93425\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x0000000000000038 Time: 4.09077\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x000000000000003a Time: 5.26921\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x000000000000003c Time: 11.3216\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x000000000000003d Time: 8.7789\n",
      "[10/18/2022-15:54:56] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 2.42717\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CublasConvolution)\n",
      "[10/18/2022-15:54:56] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CaskConvolution)\n",
      "[10/18/2022-15:54:56] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:54:56] [V] [TRT] *************** Autotuning format combination: Half(200704,784:2,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CaskConvolution)\n",
      "[10/18/2022-15:54:56] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:56] [V] [TRT] *************** Autotuning format combination: Half(200704,784:2,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:56] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CublasConvolution)\n",
      "[10/18/2022-15:54:56] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CaskConvolution)\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 1.05083\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 1.04565\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 1.14191\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x446c8c788145836a Time: 1.31712\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 1.57209\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 1.45029\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 1.57474\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x97afba3735828021 Time: 1.52612\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 1.41372\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 1.50864\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 1.42747\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 1.44244\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 1.34378\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 1.41117\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 1.31215\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 1.29328\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 1.40551\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 1.41401\n",
      "[10/18/2022-15:54:56] [V] [TRT] Fastest Tactic: 0x21904dd9d0cd407e Time: 1.04565\n",
      "[10/18/2022-15:54:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:54:56] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,1792,64) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CublasConvolution)\n",
      "[10/18/2022-15:54:56] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CaskConvolution)\n",
      "[10/18/2022-15:54:56] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:56] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,1792,64) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:56] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CublasConvolution)\n",
      "[10/18/2022-15:54:56] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:56] [V] [TRT] --------------- Timing Runner: Conv_33 + Relu_34 (CaskConvolution)\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 0.957147\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 0.861321\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.686336\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 0.927127\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.662295\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.669403\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 0.874171\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x249110624ee04937 Time: 0.822789\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 1.24899\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 0.858277\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.644521\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 0.846958\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 0.897358\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 0.958299\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 0.910395\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 0.829339\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 0.677303\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 1.02857\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 0.943959\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 1.00441\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 1.12734\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 0.905952\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 0.909001\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 0.852229\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 0.710048\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 0.988658\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 0.851557\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 0.975246\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 0.843154\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 0.918729\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 0.721824\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 0.892256\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 0.864841\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 1.19728\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.677792\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x83b35618df65874c Time: 1.05823\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 0.861294\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 0.77136\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 1.41647\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.874203\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 0.931529\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 1.33263\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 0.876837\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 1.17827\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 0.879762\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 0.81899\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.707694\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.696617\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 0.858176\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.653499\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 0.871589\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 0.818418\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.822688\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 0.835323\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 0.878459\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.65595\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.927159\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:54:56] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 0.953838\n",
      "[10/18/2022-15:54:56] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.646597\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 0.903429\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 1.21451\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 0.823488\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.680247\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 0.903945\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 0.82885\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 1.36396\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 0.848457\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 0.66677\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.673851\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 1.1991\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 0.827392\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.648603\n",
      "[10/18/2022-15:54:57] [V] [TRT] Fastest Tactic: 0x2a3615ad33745f0b Time: 0.644521\n",
      "[10/18/2022-15:54:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:54:57] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:54:57] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:54:57] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:54:57] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:57] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (FusedConvActConvolution)\n",
      "[10/18/2022-15:54:57] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:54:57] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (CudnnConvolution)\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.94658\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.37414\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x0000000000000002 Time: 12.0276\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x0000000000000004 Time: 4.2417\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x0000000000000005 Time: 4.13564\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x0000000000000006 Time: 5.06773\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x0000000000000038 Time: 8.74379\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x0000000000000039 Time: 5.34528\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x000000000000003a Time: 12.0367\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x000000000000003c Time: 4.22856\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x000000000000003d Time: 4.10208\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x000000000000003e Time: 5.09143\n",
      "[10/18/2022-15:54:57] [V] [TRT] Fastest Tactic: 0x000000000000003d Time: 4.10208\n",
      "[10/18/2022-15:54:57] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (CaskConvolution)\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.8552\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x268494f0a1c83de3\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x268494f0a1c83de3 Time: 4.43113\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x27728c886a448c5a\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x27728c886a448c5a Time: 5.11632\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:54:57] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 4.90656\n",
      "[10/18/2022-15:54:57] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x597d29027694c20b\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x597d29027694c20b Time: 5.05866\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 5.00385\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x62b2ffd9a5c0cfb5\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x62b2ffd9a5c0cfb5 Time: 7.20811\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x8d5c64a52fab02c9\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x8d5c64a52fab02c9 Time: 8.34942\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 5.11535\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x93a1176336e5b9f6\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x93a1176336e5b9f6 Time: 6.25578\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x994f5b723e2d80da\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x994f5b723e2d80da Time: 6.29826\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x128x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xac49795b871b0d29\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0xac49795b871b0d29 Time: 5.70278\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb6717e61503d5e9b\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0xb6717e61503d5e9b Time: 5.93405\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 6.47995\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 5.98324\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd1338f4b38d341e2\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0xd1338f4b38d341e2 Time: 6.49099\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x256x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd2aa21bfe2167c0c\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0xd2aa21bfe2167c0c Time: 10.3802\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xe40b38338a3a7d7e\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0xe40b38338a3a7d7e Time: 6.75545\n",
      "[10/18/2022-15:54:58] [V] [TRT] Fastest Tactic: 0x268494f0a1c83de3 Time: 4.43113\n",
      "[10/18/2022-15:54:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x000000000000003d\n",
      "[10/18/2022-15:54:58] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:54:58] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (CaskConvolution)\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x0447933cc2be855a\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x0447933cc2be855a Time: 5.35449\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 4.62053\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x0e2033f7517a807f\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x0e2033f7517a807f Time: 4.88623\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x2595402367cdee5c\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x2595402367cdee5c Time: 7.67093\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3eba442f4c9c4f50\n",
      "[10/18/2022-15:54:58] [V] [TRT] Tactic: 0x3eba442f4c9c4f50 Time: 5.50035\n",
      "[10/18/2022-15:54:58] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x43334a9c8840c773\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x43334a9c8840c773 Time: 5.74106\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 5.05403\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 4.86841\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 5.71109\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x66e3239eee98201e\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x66e3239eee98201e Time: 5.70251\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 4.84267\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 5.04956\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.81836\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x963db12d24e61b80\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x963db12d24e61b80 Time: 5.79624\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb132670a7750e065\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0xb132670a7750e065 Time: 7.66032\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca84742beb9f9767\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0xca84742beb9f9767 Time: 4.89145\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xd2b62ec40baf8ee4\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0xd2b62ec40baf8ee4 Time: 4.93726\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xde4165142218dab8\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0xde4165142218dab8 Time: 6.08222\n",
      "[10/18/2022-15:54:59] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 5.77563\n",
      "[10/18/2022-15:54:59] [V] [TRT] Fastest Tactic: 0x0bf55a7b77a6ff98 Time: 4.62053\n",
      "[10/18/2022-15:54:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:54:59] [V] [TRT] *************** Autotuning format combination: Half(100352,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:54:59] [W] [TRT] Weights [name=Conv_35 + Relu_36.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:54:59] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:54:59] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:54:59] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (CudnnConvolution)\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.50299\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.78676\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x0000000000000002 Time: 10.1754\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x0000000000000004 Time: 3.83886\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x0000000000000005 Time: 3.68379\n",
      "[10/18/2022-15:54:59] [V] [TRT] Tactic: 0x0000000000000006 Time: 5.45938\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.97701\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x000000000000003a Time: 9.94096\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x000000000000003c Time: 3.83415\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x000000000000003d Time: 3.72506\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x000000000000003e Time: 5.38176\n",
      "[10/18/2022-15:55:00] [V] [TRT] Fastest Tactic: 0x0000000000000005 Time: 3.68379\n",
      "[10/18/2022-15:55:00] [W] [TRT] Weights [name=Conv_35 + Relu_36.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:00] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (CaskConvolution)\n",
      "[10/18/2022-15:55:00] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000005\n",
      "[10/18/2022-15:55:00] [V] [TRT] *************** Autotuning format combination: Half(50176,784:2,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:55:00] [W] [TRT] Weights [name=Conv_35 + Relu_36.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:00] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:00] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:00] [W] [TRT] Weights [name=Conv_35 + Relu_36.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:00] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (CaskConvolution)\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x0fe4a9cce7ed878b\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x0fe4a9cce7ed878b Time: 2.47482\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.49189\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.65224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 2.69029\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x4092cbc840fbea35\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x4092cbc840fbea35 Time: 2.72647\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x446c8c788145836a Time: 3.00471\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 3.0539\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 2.79419\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.75897\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0x98a00f59a4b141f0\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x98a00f59a4b141f0 Time: 3.06965\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 2.91987\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.79315\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xcbe3f30275b04323\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0xcbe3f30275b04323 Time: 2.88848\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.75222\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0xd46b3ee2b59f893c\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0xd46b3ee2b59f893c Time: 2.43651\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xd7d66d5d03a72c4e\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0xd7d66d5d03a72c4e Time: 3.00837\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 2.93361\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.83783\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xfc994367fd14b2d9\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0xfc994367fd14b2d9 Time: 2.82565\n",
      "[10/18/2022-15:55:00] [V] [TRT] Fastest Tactic: 0xd46b3ee2b59f893c Time: 2.43651\n",
      "[10/18/2022-15:55:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd46b3ee2b59f893c\n",
      "[10/18/2022-15:55:00] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:00] [W] [TRT] Weights [name=Conv_35 + Relu_36.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:00] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (CaskConvolution)\n",
      "[10/18/2022-15:55:00] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:00] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:55:00] [W] [TRT] Weights [name=Conv_35 + Relu_36.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:00] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:00] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:00] [W] [TRT] Weights [name=Conv_35 + Relu_36.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:00] [V] [TRT] --------------- Timing Runner: Conv_35 + Relu_36 (CaskConvolution)\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x00a425145e84482b\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x00a425145e84482b Time: 2.39837\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x03512591e8ea2977\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x03512591e8ea2977 Time: 1.6422\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x0559d1d2893a8768\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x0559d1d2893a8768 Time: 2.27222\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x095000b22a78f234\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x095000b22a78f234 Time: 1.21973\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x0b906efbde4dc01a\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x0b906efbde4dc01a Time: 1.48241\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x0c0088d5808566d2\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x0c0088d5808566d2 Time: 0.833806\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x0caa5410b61e6cc5\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x0caa5410b61e6cc5 Time: 2.55104\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x0e0f7f10867063ba\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x0e0f7f10867063ba Time: 1.00407\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x0e131ddbafdfe235\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x0e131ddbafdfe235 Time: 1.22752\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x0ecf8dc91198fd5e\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x0ecf8dc91198fd5e Time: 0.836681\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0x159236c6c22f62ce\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x159236c6c22f62ce Time: 2.50044\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x15ecbd82c22a023f\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x15ecbd82c22a023f Time: 0.812178\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x18ef97651ad5379a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x18ef97651ad5379a Time: 1.78737\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x1981adfb6b6fd8b9\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x1981adfb6b6fd8b9 Time: 1.35471\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x1b099f7ac29a2a6a\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x1b099f7ac29a2a6a Time: 1.55565\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x1b9cb8d78519a728\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x1b9cb8d78519a728 Time: 1.94414\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0x1c23f4a19fbcb518\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x1c23f4a19fbcb518 Time: 1.58267\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.956489\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x1de724868edf11b0\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x1de724868edf11b0 Time: 1.17639\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.782309\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x30150d05024bc911\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x30150d05024bc911 Time: 0.961682\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x32789ed2e6c7b43b\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x32789ed2e6c7b43b Time: 0.844535\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x33fc6102b341eb5d\n",
      "[10/18/2022-15:55:00] [V] [TRT] Tactic: 0x33fc6102b341eb5d Time: 2.73057\n",
      "[10/18/2022-15:55:00] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.50031\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x348653930e0a64e2\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x348653930e0a64e2 Time: 1.64507\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x350e898a5a20ad00\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x350e898a5a20ad00 Time: 0.894688\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x36662b4d547eefc7\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x36662b4d547eefc7 Time: 1.24504\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x490a097d77573bff\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x490a097d77573bff Time: 0.798135\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x4c6a6da741444412\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x4c6a6da741444412 Time: 1.46656\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x4e34a65090c3b86f\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x4e34a65090c3b86f Time: 0.764919\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x504f864880743a14\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x504f864880743a14 Time: 2.37399\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x5128cdf162fe56b6\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x5128cdf162fe56b6 Time: 1.26917\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.30439\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r3s3 Tactic: 0x5252dc6c9c5f3aff\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x5252dc6c9c5f3aff Time: 1.66971\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x54b287be85c1522c\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x54b287be85c1522c Time: 2.50828\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x55fb34a08663e5ae\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x55fb34a08663e5ae Time: 1.61729\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x56c66ffbce24b635\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x56c66ffbce24b635 Time: 1.50975\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x58eea09dffe038fd\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x58eea09dffe038fd Time: 1.25709\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x5bec1fbd955eb827\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x5bec1fbd955eb827 Time: 1.392\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.4852\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x62bb371b230a886d\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x62bb371b230a886d Time: 1.7031\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.45537\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x69a5b2ac9c5bac16\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x69a5b2ac9c5bac16 Time: 1.26069\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6b44e6396887bed9\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x6b44e6396887bed9 Time: 1.22446\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x6cde8847e8cd796b\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x6cde8847e8cd796b Time: 1.27111\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x6cee4d9c86b4cdd5\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x6cee4d9c86b4cdd5 Time: 1.30984\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.47251\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x721049a39aae27ff\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x721049a39aae27ff Time: 2.2675\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.48409\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x75585ae3e9dedb93\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x75585ae3e9dedb93 Time: 1.78498\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x784dcede905d06c0\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x784dcede905d06c0 Time: 1.33473\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.985765\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x86903737887c556d\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x86903737887c556d Time: 1.29908\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x8781623566dac7f0\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x8781623566dac7f0 Time: 0.991808\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x8b86a8bb857fff79\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x8b86a8bb857fff79 Time: 1.85529\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x8d73ddfc444be692\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x8d73ddfc444be692 Time: 0.844645\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x9650edb797f919f3\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x9650edb797f919f3 Time: 0.863433\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0x969b1abbb567ac47\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x969b1abbb567ac47 Time: 1.32155\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0x9a0f43b4d1dc46d4\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x9a0f43b4d1dc46d4 Time: 2.23732\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa13cdf70a9d99d45\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xa13cdf70a9d99d45 Time: 1.94637\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa2dad76f719680b5\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xa2dad76f719680b5 Time: 1.67451\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0xa3e778b253a14ca9\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xa3e778b253a14ca9 Time: 2.36707\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0xa5f0bcb42cb01fc7\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xa5f0bcb42cb01fc7 Time: 1.58568\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r3s3 Tactic: 0xa84824f86c61d2d8\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xa84824f86c61d2d8 Time: 1.56085\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.896283\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xab9c5449bde6902c\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xab9c5449bde6902c Time: 0.786432\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xac4736b5b00e1531\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xac4736b5b00e1531 Time: 1.27274\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.901179\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb0bf64026e546f4d\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xb0bf64026e546f4d Time: 0.968411\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xb26e93bd0702f504\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xb26e93bd0702f504 Time: 1.95262\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xb307bc772518d3d7\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xb307bc772518d3d7 Time: 1.20317\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb7dc3705357cc965\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xb7dc3705357cc965 Time: 0.961426\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xbb3d6545e4864f26\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xbb3d6545e4864f26 Time: 0.822789\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xbfc71f913e286527\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xbfc71f913e286527 Time: 1.7291\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.43659\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xc684285f13ba11d0\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xc684285f13ba11d0 Time: 1.71405\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xc6e0905d983b4a62\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xc6e0905d983b4a62 Time: 2.15723\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0xc8ee1e4cdf0d8f84\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xc8ee1e4cdf0d8f84 Time: 2.567\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xcb7b50f35a87094b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xcb7b50f35a87094b Time: 1.74373\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd076fab92f5706c9\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xd076fab92f5706c9 Time: 1.15148\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd297ae2cdb8b1406\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xd297ae2cdb8b1406 Time: 1.15714\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.49299\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.781243\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xd825f95894186a22\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xd825f95894186a22 Time: 2.49063\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xd9d1d89fceeca81a\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xd9d1d89fceeca81a Time: 1.63489\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xdb70c5e9779254fb\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xdb70c5e9779254fb Time: 1.97091\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.794469\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xe84b9aaa289245c0\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xe84b9aaa289245c0 Time: 1.51611\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xe9fa7b19132889a8\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xe9fa7b19132889a8 Time: 1.60628\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0xf1d5fc0783e71536\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xf1d5fc0783e71536 Time: 1.32034\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xf368aae1fb20baa1\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xf368aae1fb20baa1 Time: 1.55245\n",
      "[10/18/2022-15:55:01] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.753435\n",
      "[10/18/2022-15:55:01] [V] [TRT] Fastest Tactic: 0xfcd06da0f3c31fd1 Time: 0.753435\n",
      "[10/18/2022-15:55:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:01] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:01] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:55:01] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:01] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:01] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:01] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:01] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CudnnConvolution)\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 9.96385\n",
      "[10/18/2022-15:55:01] [V] [TRT] Tactic: 0x0000000000000001 Time: 8.50309\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x0000000000000002 Time: 10.7156\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x0000000000000004 Time: 17.1538\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x0000000000000005 Time: 14.0379\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x0000000000000038 Time: 9.97259\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x0000000000000039 Time: 8.50593\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x000000000000003a Time: 10.7086\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x000000000000003c Time: 17.3882\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x000000000000003d Time: 14.1151\n",
      "[10/18/2022-15:55:02] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 8.50309\n",
      "[10/18/2022-15:55:02] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CublasConvolution)\n",
      "[10/18/2022-15:55:02] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:02] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CaskConvolution)\n",
      "[10/18/2022-15:55:02] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 2.89207\n",
      "[10/18/2022-15:55:02] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 2.96052\n",
      "[10/18/2022-15:55:02] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 3.52578\n",
      "[10/18/2022-15:55:02] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 3.23172\n",
      "[10/18/2022-15:55:02] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:55:02] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 4.6871\n",
      "[10/18/2022-15:55:02] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 3.04385\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 2.9303\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 3.04187\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 6.39724\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 3.60483\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 3.94212\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 3.63791\n",
      "[10/18/2022-15:55:03] [V] [TRT] Fastest Tactic: 0x18597bd4a7d0164d Time: 2.89207\n",
      "[10/18/2022-15:55:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:03] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(401408,1,14336,512) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:55:03] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CublasConvolution)\n",
      "[10/18/2022-15:55:03] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:03] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CaskConvolution)\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 3.03232\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 3.02883\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 3.1279\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 5.86341\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 3.82093\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 2.95047\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 3.01524\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 3.21001\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 5.96283\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 2.89752\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 4.4329\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 2.91579\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 4.03778\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 3.77651\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0xf92663d88255134b Time: 3.10523\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 5.94771\n",
      "[10/18/2022-15:55:03] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 4.07403\n",
      "[10/18/2022-15:55:03] [V] [TRT] Fastest Tactic: 0x946eca69f99ddcb4 Time: 2.89752\n",
      "[10/18/2022-15:55:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:03] [V] [TRT] *************** Autotuning format combination: Half(100352,784,28,1), Half(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:55:03] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:03] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:03] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:03] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:03] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:03] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:03] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:03] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CudnnConvolution)\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.28581\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.70618\n",
      "[10/18/2022-15:55:03] [V] [TRT] Tactic: 0x0000000000000002 Time: 7.89342\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x0000000000000004 Time: 14.6179\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x0000000000000005 Time: 11.2871\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.45095\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x000000000000003a Time: 7.76894\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x000000000000003c Time: 14.6295\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x000000000000003d Time: 11.3731\n",
      "[10/18/2022-15:55:04] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 5.70618\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CublasConvolution)\n",
      "[10/18/2022-15:55:04] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CaskConvolution)\n",
      "[10/18/2022-15:55:04] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:04] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:55:04] [V] [TRT] *************** Autotuning format combination: Half(50176,784:2,28,1), Half(200704,784:2,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:04] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CublasConvolution)\n",
      "[10/18/2022-15:55:04] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CaskConvolution)\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 1.24401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 1.25572\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 1.48554\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x446c8c788145836a Time: 1.97456\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 2.0947\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 1.82746\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.15943\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x97afba3735828021 Time: 1.94615\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 1.7474\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 1.98511\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.01121\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 1.81163\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 1.61323\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 1.98305\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 1.57608\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 1.5698\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 1.81971\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 1.98113\n",
      "[10/18/2022-15:55:04] [V] [TRT] Fastest Tactic: 0x16eafdbc5869b184 Time: 1.24401\n",
      "[10/18/2022-15:55:04] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:04] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CublasConvolution)\n",
      "[10/18/2022-15:55:04] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CaskConvolution)\n",
      "[10/18/2022-15:55:04] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:04] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16), Half(50176,1:8,1792,64) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:04] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CublasConvolution)\n",
      "[10/18/2022-15:55:04] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:04] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:04] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:04] [V] [TRT] --------------- Timing Runner: Conv_37 + Add_38 + Relu_39 (CaskConvolution)\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 1.33927\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 1.24601\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 1.11943\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 1.45204\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 1.20572\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 1.23839\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 1.03109\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x249110624ee04937 Time: 1.01994\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 1.4399\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 1.1135\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 1.01888\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 1.11597\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 1.03746\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.30194\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:55:04] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 1.07871\n",
      "[10/18/2022-15:55:04] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 1.01839\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 1.18491\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 1.23202\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 1.09695\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 1.34853\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 1.33731\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.5047\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 1.08441\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.45762\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 1.01811\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.48506\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.39107\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.31504\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 1.20533\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 1.20598\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 1.27541\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 1.15653\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 1.03512\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 1.52489\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 1.16952\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x83b35618df65874c Time: 1.297\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 1.47305\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 1.59327\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 1.19603\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 1.29016\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 1.07097\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 1.49745\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 1.43334\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 1.33618\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 1.55587\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 1.06071\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 1.12523\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 1.15827\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 1.08133\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 1.02168\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 1.2154\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 1.05765\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 1.10124\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 1.23064\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.47397\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 1.29346\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 1.24452\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.38987\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 1.03551\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 1.19931\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 1.40821\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 1.10358\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 1.25277\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 1.22018\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 1.43756\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 1.23495\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 1.37775\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 1.01214\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 1.24435\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 1.51414\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 1.20731\n",
      "[10/18/2022-15:55:05] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 1.02722\n",
      "[10/18/2022-15:55:05] [V] [TRT] Fastest Tactic: 0xe67db95e0c20b618 Time: 1.01214\n",
      "[10/18/2022-15:55:05] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:55:05] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(401408,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(401408,1,14336,512) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(401408,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_40 + Relu_41.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(200704,784:2,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_40 + Relu_41.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_40 + Relu_41 (CaskConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(200704,784:2,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_40 + Relu_41.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,1792,64) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_40 + Relu_41.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_40 + Relu_41 (CublasConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_40 + Relu_41.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_40 + Relu_41 (CaskConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,1792,64) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_40 + Relu_41.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(100352,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_42 + Relu_43.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_42 + Relu_43.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(50176,784:2,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_42 + Relu_43.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_42 + Relu_43.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_42 + Relu_43.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_42 + Relu_43.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_42 + Relu_43 (CaskConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_42 + Relu_43.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_42 + Relu_43.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(401408,1,14336,512) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(100352,784,28,1), Half(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_44 + Add_45 + Relu_46.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(50176,784:2,28,1), Half(200704,784:2,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_44 + Add_45 + Relu_46.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_44 + Add_45 + Relu_46.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_44 + Add_45 + Relu_46 (CublasConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_44 + Add_45 + Relu_46.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_44 + Add_45 + Relu_46 (CaskConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16), Half(50176,1:8,1792,64) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_44 + Add_45 + Relu_46.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(401408,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(401408,1,14336,512) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(401408,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_47 + Relu_48.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(200704,784:2,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_47 + Relu_48.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_47 + Relu_48 (CaskConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(200704,784:2,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_47 + Relu_48.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,1792,64) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_47 + Relu_48.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_47 + Relu_48 (CublasConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_47 + Relu_48.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_47 + Relu_48 (CaskConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,1792,64) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_47 + Relu_48.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:05] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128) -> Float(100352,1,3584,128) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(100352,784,28,1) -> Half(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_49 + Relu_50.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(50176,784:2,28,1) -> Half(50176,784:2,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_49 + Relu_50.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16) -> Float(100352,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_49 + Relu_50.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_49 + Relu_50 (CaskConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16) -> Half(12544,1:8,448,16) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_49 + Relu_50.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(100352,784,28,1), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(100352,1,3584,128), Float(401408,1,14336,512) -> Float(401408,1,14336,512) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(100352,784,28,1), Half(401408,784,28,1) -> Half(401408,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_51 + Add_52 + Relu_53.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(50176,784:2,28,1), Half(200704,784:2,28,1) -> Half(200704,784:2,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_51 + Add_52 + Relu_53.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16), Float(401408,784,28,1) -> Float(401408,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_51 + Add_52 + Relu_53.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_51 + Add_52 + Relu_53 (CublasConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_51 + Add_52 + Relu_53.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_51 + Add_52 + Relu_53 (CaskConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,448,16), Half(50176,1:8,1792,64) -> Half(50176,1:8,1792,64) ***************\n",
      "[10/18/2022-15:55:05] [W] [TRT] Weights [name=Conv_51 + Add_52 + Relu_53.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:05] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:05] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:05] [V] [TRT] *************** Autotuning format combination: Float(401408,784,28,1) -> Float(200704,784,28,1) ***************\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:05] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CudnnConvolution)\n",
      "[10/18/2022-15:55:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.89507\n",
      "[10/18/2022-15:55:06] [V] [TRT] Tactic: 0x0000000000000001 Time: 6.44368\n",
      "[10/18/2022-15:55:06] [V] [TRT] Tactic: 0x0000000000000002 Time: 9.92437\n",
      "[10/18/2022-15:55:06] [V] [TRT] Tactic: 0x0000000000000004 Time: 22.2103\n",
      "[10/18/2022-15:55:06] [V] [TRT] Tactic: 0x0000000000000005 Time: 16.4001\n",
      "[10/18/2022-15:55:06] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.53847\n",
      "[10/18/2022-15:55:06] [V] [TRT] Tactic: 0x0000000000000039 Time: 6.18581\n",
      "[10/18/2022-15:55:06] [V] [TRT] Tactic: 0x000000000000003a Time: 9.76924\n",
      "[10/18/2022-15:55:06] [V] [TRT] Tactic: 0x000000000000003c Time: 22.2346\n",
      "[10/18/2022-15:55:06] [V] [TRT] Tactic: 0x000000000000003d Time: 16.4159\n",
      "[10/18/2022-15:55:06] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 6.18581\n",
      "[10/18/2022-15:55:06] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CublasConvolution)\n",
      "[10/18/2022-15:55:06] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:06] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CaskConvolution)\n",
      "[10/18/2022-15:55:06] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 4.3108\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.49568\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 5.61683\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 4.83216\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 6.92235\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 5.01176\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 4.76979\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 4.99821\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 6.31868\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 6.7859\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 6.41989\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 5.80418\n",
      "[10/18/2022-15:55:07] [V] [TRT] Fastest Tactic: 0x18597bd4a7d0164d Time: 4.3108\n",
      "[10/18/2022-15:55:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:07] [V] [TRT] *************** Autotuning format combination: Float(401408,1,14336,512) -> Float(200704,1,7168,256) ***************\n",
      "[10/18/2022-15:55:07] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CublasConvolution)\n",
      "[10/18/2022-15:55:07] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:07] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CaskConvolution)\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 5.08822\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 5.13841\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 4.95752\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 6.05076\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 7.19233\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 4.7221\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 4.82789\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 5.14599\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 5.92369\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:07] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.52298\n",
      "[10/18/2022-15:55:07] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 5.91259\n",
      "[10/18/2022-15:55:08] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 5.00362\n",
      "[10/18/2022-15:55:08] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 7.65718\n",
      "[10/18/2022-15:55:08] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 5.74025\n",
      "[10/18/2022-15:55:08] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0xf92663d88255134b Time: 4.90922\n",
      "[10/18/2022-15:55:08] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 6.0148\n",
      "[10/18/2022-15:55:08] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 5.57446\n",
      "[10/18/2022-15:55:08] [V] [TRT] Fastest Tactic: 0x946eca69f99ddcb4 Time: 4.52298\n",
      "[10/18/2022-15:55:08] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:08] [V] [TRT] *************** Autotuning format combination: Half(401408,784,28,1) -> Half(200704,784,28,1) ***************\n",
      "[10/18/2022-15:55:08] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:08] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:08] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:08] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.86141\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.57078\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0x0000000000000002 Time: 9.47494\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0x0000000000000004 Time: 21.3561\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0x0000000000000005 Time: 15.2313\n",
      "[10/18/2022-15:55:08] [V] [TRT] Tactic: 0x0000000000000038 Time: 8.34353\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x000000000000003a Time: 9.41308\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x000000000000003c Time: 20.3646\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x000000000000003d Time: 15.8207\n",
      "[10/18/2022-15:55:09] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 5.57078\n",
      "[10/18/2022-15:55:09] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:09] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:09] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:09] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CublasConvolution)\n",
      "[10/18/2022-15:55:09] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:09] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:09] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:09] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:09] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CaskConvolution)\n",
      "[10/18/2022-15:55:09] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:09] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:55:09] [V] [TRT] *************** Autotuning format combination: Half(200704,784:2,28,1) -> Half(200704,784,28,1) ***************\n",
      "[10/18/2022-15:55:09] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:09] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:09] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:09] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CaskConvolution)\n",
      "[10/18/2022-15:55:09] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:09] [V] [TRT] *************** Autotuning format combination: Half(200704,784:2,28,1) -> Half(100352,784:2,28,1) ***************\n",
      "[10/18/2022-15:55:09] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:09] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:09] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:09] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:09] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:09] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:09] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:09] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:09] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CublasConvolution)\n",
      "[10/18/2022-15:55:09] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:09] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:09] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:09] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:09] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CaskConvolution)\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.21068\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.22972\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 2.50949\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x446c8c788145836a Time: 2.94467\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 3.07491\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 2.87188\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.8778\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x97afba3735828021 Time: 2.72397\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 2.57904\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 2.74988\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.85842\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 2.76532\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 2.60897\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.72333\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 2.48101\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 2.44802\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 2.72472\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.70151\n",
      "[10/18/2022-15:55:09] [V] [TRT] Fastest Tactic: 0x16eafdbc5869b184 Time: 2.21068\n",
      "[10/18/2022-15:55:09] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:09] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,1792,64) -> Float(200704,784,28,1) ***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:09] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:09] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:09] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:09] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CublasConvolution)\n",
      "[10/18/2022-15:55:09] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:09] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:09] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:09] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:09] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CaskConvolution)\n",
      "[10/18/2022-15:55:09] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:09] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,1792,64) -> Half(25088,1:8,896,32) ***************\n",
      "[10/18/2022-15:55:09] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:09] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:09] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:09] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:09] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:09] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:09] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:09] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:09] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CublasConvolution)\n",
      "[10/18/2022-15:55:09] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:09] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:09] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:09] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:09] [V] [TRT] --------------- Timing Runner: Conv_54 + Relu_55 (CaskConvolution)\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 1.84379\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 1.62391\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 1.05326\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 1.66273\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 1.02245\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.92405\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 1.20742\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x249110624ee04937 Time: 1.26566\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 1.4596\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 1.35122\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.880073\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 1.32782\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 1.25272\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.79909\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 1.91576\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 1.2599\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 1.09243\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 2.03985\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:55:09] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 1.74302\n",
      "[10/18/2022-15:55:09] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 1.75497\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 2.06884\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.42607\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 1.61672\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.61269\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 1.38992\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.73344\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.57518\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.70558\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 1.13836\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 1.10821\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 1.26655\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 1.58486\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 1.63196\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 2.4836\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 1.08954\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x83b35618df65874c Time: 2.05441\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 1.37836\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 1.36213\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 1.43925\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 1.1594\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 1.84905\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 1.36482\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 1.402\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 2.39205\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 1.37127\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 1.22997\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 1.14548\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 1.12824\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 1.68638\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.931287\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 1.39835\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 1.26976\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 1.00197\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 1.14424\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.536\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.965979\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 1.13231\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.75799\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.880453\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 1.87854\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 2.49498\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 1.00645\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.94213\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 1.63764\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 1.59188\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 1.40756\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 1.34064\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 1.33061\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.930171\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 2.43447\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 1.14148\n",
      "[10/18/2022-15:55:10] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.873015\n",
      "[10/18/2022-15:55:10] [V] [TRT] Fastest Tactic: 0xfcd06da0f3c31fd1 Time: 0.873015\n",
      "[10/18/2022-15:55:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:10] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:10] [V] [TRT] *************** Autotuning format combination: Float(200704,784,28,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:10] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:10] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:10] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:10] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:10] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (CudnnConvolution)\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.99586\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.36079\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x0000000000000002 Time: 8.45448\n",
      "[10/18/2022-15:55:10] [V] [TRT] Tactic: 0x0000000000000005 Time: 10.2628\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x0000000000000038 Time: 6.51716\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x0000000000000039 Time: 5.29503\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x000000000000003a Time: 8.42021\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x000000000000003d Time: 10.3368\n",
      "[10/18/2022-15:55:11] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 5.29503\n",
      "[10/18/2022-15:55:11] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (CaskConvolution)\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.72353\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x27728c886a448c5a\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x27728c886a448c5a Time: 5.11269\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 5.2618\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x597d29027694c20b\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x597d29027694c20b Time: 5.11971\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 5.22064\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x62b2ffd9a5c0cfb5\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x62b2ffd9a5c0cfb5 Time: 8.04325\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x8d5c64a52fab02c9\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x8d5c64a52fab02c9 Time: 8.69727\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 5.64164\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x93a1176336e5b9f6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x93a1176336e5b9f6 Time: 6.65872\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x994f5b723e2d80da\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0x994f5b723e2d80da Time: 6.39585\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x128x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xac49795b871b0d29\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0xac49795b871b0d29 Time: 5.72591\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb6717e61503d5e9b\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0xb6717e61503d5e9b Time: 6.06646\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 7.7704\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:11] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 7.1608\n",
      "[10/18/2022-15:55:11] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd1338f4b38d341e2\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0xd1338f4b38d341e2 Time: 6.63781\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x256x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd2aa21bfe2167c0c\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0xd2aa21bfe2167c0c Time: 5.36559\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xe40b38338a3a7d7e\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0xe40b38338a3a7d7e Time: 7.71657\n",
      "[10/18/2022-15:55:12] [V] [TRT] Fastest Tactic: 0x195431d38ba5af88 Time: 4.72353\n",
      "[10/18/2022-15:55:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:12] [V] [TRT] *************** Autotuning format combination: Float(200704,1,7168,256) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:55:12] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (CaskConvolution)\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x0447933cc2be855a\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x0447933cc2be855a Time: 5.01441\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 4.84715\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x0e2033f7517a807f\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x0e2033f7517a807f Time: 5.0629\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x2595402367cdee5c\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x2595402367cdee5c Time: 7.9714\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3eba442f4c9c4f50\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x3eba442f4c9c4f50 Time: 5.59137\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x43334a9c8840c773\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x43334a9c8840c773 Time: 6.00883\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 5.42626\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 5.05589\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 5.62727\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x66e3239eee98201e\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x66e3239eee98201e Time: 6.05536\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 5.07386\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 5.1832\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 5.06353\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x963db12d24e61b80\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0x963db12d24e61b80 Time: 6.17765\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb132670a7750e065\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0xb132670a7750e065 Time: 8.04279\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca84742beb9f9767\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0xca84742beb9f9767 Time: 5.01148\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xd2b62ec40baf8ee4\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0xd2b62ec40baf8ee4 Time: 5.36252\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xde4165142218dab8\n",
      "[10/18/2022-15:55:12] [V] [TRT] Tactic: 0xde4165142218dab8 Time: 6.39468\n",
      "[10/18/2022-15:55:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 5.74731\n",
      "[10/18/2022-15:55:13] [V] [TRT] Fastest Tactic: 0x0bf55a7b77a6ff98 Time: 4.84715\n",
      "[10/18/2022-15:55:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:13] [V] [TRT] *************** Autotuning format combination: Half(200704,784,28,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:13] [W] [TRT] Weights [name=Conv_56 + Relu_57.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:13] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:13] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.54431\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x0000000000000001 Time: 6.19872\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x0000000000000002 Time: 9.6683\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x0000000000000005 Time: 10.0343\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x0000000000000038 Time: 8.35312\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x000000000000003a Time: 9.16597\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x000000000000003d Time: 9.99235\n",
      "[10/18/2022-15:55:13] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 6.19872\n",
      "[10/18/2022-15:55:13] [W] [TRT] Weights [name=Conv_56 + Relu_57.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:13] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:13] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (CaskConvolution)\n",
      "[10/18/2022-15:55:13] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:55:13] [V] [TRT] *************** Autotuning format combination: Half(100352,784:2,28,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:13] [W] [TRT] Weights [name=Conv_56 + Relu_57.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:13] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:13] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:13] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:13] [W] [TRT] Weights [name=Conv_56 + Relu_57.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:13] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:13] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (CaskConvolution)\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x0fe4a9cce7ed878b\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x0fe4a9cce7ed878b Time: 2.44129\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.52694\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.77811\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 2.78611\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x4092cbc840fbea35\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x4092cbc840fbea35 Time: 2.79258\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x446c8c788145836a Time: 3.45158\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 3.41157\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 2.82004\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.7315\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0x98a00f59a4b141f0\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0x98a00f59a4b141f0 Time: 3.39851\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 3.26246\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.70219\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xcbe3f30275b04323\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0xcbe3f30275b04323 Time: 2.68485\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.58194\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xd7d66d5d03a72c4e\n",
      "[10/18/2022-15:55:13] [V] [TRT] Tactic: 0xd7d66d5d03a72c4e Time: 3.23021\n",
      "[10/18/2022-15:55:13] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 3.32917\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.71627\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xfc994367fd14b2d9\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xfc994367fd14b2d9 Time: 2.68265\n",
      "[10/18/2022-15:55:14] [V] [TRT] Fastest Tactic: 0x0fe4a9cce7ed878b Time: 2.44129\n",
      "[10/18/2022-15:55:14] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0fe4a9cce7ed878b\n",
      "[10/18/2022-15:55:14] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,896,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:14] [W] [TRT] Weights [name=Conv_56 + Relu_57.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:14] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:14] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:14] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:14] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (CaskConvolution)\n",
      "[10/18/2022-15:55:14] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:14] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,896,32) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:55:14] [W] [TRT] Weights [name=Conv_56 + Relu_57.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:14] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:14] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:14] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:14] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:14] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:14] [W] [TRT] Weights [name=Conv_56 + Relu_57.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:14] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:14] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:14] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:14] [V] [TRT] --------------- Timing Runner: Conv_56 + Relu_57 (CaskConvolution)\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x00a425145e84482b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x00a425145e84482b Time: 1.22262\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x03512591e8ea2977\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x03512591e8ea2977 Time: 1.62465\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x0559d1d2893a8768\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x0559d1d2893a8768 Time: 2.31965\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x095000b22a78f234\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x095000b22a78f234 Time: 1.27411\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x0b906efbde4dc01a\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x0b906efbde4dc01a Time: 1.44326\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x0c0088d5808566d2\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x0c0088d5808566d2 Time: 0.783506\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x0caa5410b61e6cc5\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x0caa5410b61e6cc5 Time: 1.27622\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x0e0f7f10867063ba\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x0e0f7f10867063ba Time: 1.08786\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x0e131ddbafdfe235\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x0e131ddbafdfe235 Time: 1.28255\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x0ecf8dc91198fd5e\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x0ecf8dc91198fd5e Time: 0.919561\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0x159236c6c22f62ce\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x159236c6c22f62ce Time: 1.31202\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x15ecbd82c22a023f\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x15ecbd82c22a023f Time: 0.918002\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x18ef97651ad5379a\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x18ef97651ad5379a Time: 1.94835\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x1981adfb6b6fd8b9\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x1981adfb6b6fd8b9 Time: 1.39083\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x1b099f7ac29a2a6a\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x1b099f7ac29a2a6a Time: 1.66032\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x1b9cb8d78519a728\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x1b9cb8d78519a728 Time: 2.07462\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0x1c23f4a19fbcb518\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x1c23f4a19fbcb518 Time: 1.02459\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 1.03652\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x1de724868edf11b0\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x1de724868edf11b0 Time: 1.2487\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.785861\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x30150d05024bc911\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x30150d05024bc911 Time: 1.05223\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x32789ed2e6c7b43b\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x32789ed2e6c7b43b Time: 0.938418\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x33fc6102b341eb5d\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x33fc6102b341eb5d Time: 1.42162\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.66535\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x348653930e0a64e2\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x348653930e0a64e2 Time: 1.81321\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x350e898a5a20ad00\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x350e898a5a20ad00 Time: 0.914578\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x36662b4d547eefc7\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x36662b4d547eefc7 Time: 1.34117\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x490a097d77573bff\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x490a097d77573bff Time: 0.855227\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x4c6a6da741444412\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x4c6a6da741444412 Time: 0.815488\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x4e34a65090c3b86f\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x4e34a65090c3b86f Time: 0.794917\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x504f864880743a14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x504f864880743a14 Time: 2.50326\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x5128cdf162fe56b6\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x5128cdf162fe56b6 Time: 1.3292\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.34615\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r3s3 Tactic: 0x5252dc6c9c5f3aff\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x5252dc6c9c5f3aff Time: 1.68867\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x54b287be85c1522c\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x54b287be85c1522c Time: 1.28816\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x55fb34a08663e5ae\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x55fb34a08663e5ae Time: 1.68316\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x56c66ffbce24b635\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x56c66ffbce24b635 Time: 1.56761\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x58eea09dffe038fd\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x58eea09dffe038fd Time: 1.2996\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x5bec1fbd955eb827\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x5bec1fbd955eb827 Time: 1.4664\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.47818\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x62bb371b230a886d\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x62bb371b230a886d Time: 1.797\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.51604\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x69a5b2ac9c5bac16\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x69a5b2ac9c5bac16 Time: 1.30633\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6b44e6396887bed9\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x6b44e6396887bed9 Time: 1.25753\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x6cde8847e8cd796b\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x6cde8847e8cd796b Time: 1.39845\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x6cee4d9c86b4cdd5\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x6cee4d9c86b4cdd5 Time: 1.46037\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.54028\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x721049a39aae27ff\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x721049a39aae27ff Time: 2.35768\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.50976\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x75585ae3e9dedb93\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x75585ae3e9dedb93 Time: 1.82403\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x784dcede905d06c0\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x784dcede905d06c0 Time: 1.37248\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 1.01698\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x86903737887c556d\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x86903737887c556d Time: 1.34223\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x8781623566dac7f0\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x8781623566dac7f0 Time: 1.05823\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x8b86a8bb857fff79\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x8b86a8bb857fff79 Time: 1.99579\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x8d73ddfc444be692\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x8d73ddfc444be692 Time: 0.823296\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x9650edb797f919f3\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x9650edb797f919f3 Time: 0.871282\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0x969b1abbb567ac47\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x969b1abbb567ac47 Time: 1.35739\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0x9a0f43b4d1dc46d4\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0x9a0f43b4d1dc46d4 Time: 2.30816\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa13cdf70a9d99d45\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xa13cdf70a9d99d45 Time: 2.01465\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa2dad76f719680b5\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xa2dad76f719680b5 Time: 1.73173\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0xa3e778b253a14ca9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xa3e778b253a14ca9 Time: 2.35756\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0xa5f0bcb42cb01fc7\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xa5f0bcb42cb01fc7 Time: 0.982226\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r3s3 Tactic: 0xa84824f86c61d2d8\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xa84824f86c61d2d8 Time: 0.967273\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.881225\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xab9c5449bde6902c\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xab9c5449bde6902c Time: 0.822766\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xac4736b5b00e1531\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xac4736b5b00e1531 Time: 1.33495\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.905765\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb0bf64026e546f4d\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xb0bf64026e546f4d Time: 1.07988\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xb26e93bd0702f504\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xb26e93bd0702f504 Time: 2.11095\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xb307bc772518d3d7\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xb307bc772518d3d7 Time: 1.28351\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb7dc3705357cc965\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xb7dc3705357cc965 Time: 1.06789\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xbb3d6545e4864f26\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xbb3d6545e4864f26 Time: 0.811881\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xbfc71f913e286527\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xbfc71f913e286527 Time: 1.81227\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:14] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.44535\n",
      "[10/18/2022-15:55:14] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xc684285f13ba11d0\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xc684285f13ba11d0 Time: 1.71973\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xc6e0905d983b4a62\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xc6e0905d983b4a62 Time: 2.20103\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0xc8ee1e4cdf0d8f84\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xc8ee1e4cdf0d8f84 Time: 1.31511\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xcb7b50f35a87094b\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xcb7b50f35a87094b Time: 1.83325\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd076fab92f5706c9\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xd076fab92f5706c9 Time: 1.22355\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd297ae2cdb8b1406\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xd297ae2cdb8b1406 Time: 1.24124\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.55756\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.79755\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xd825f95894186a22\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xd825f95894186a22 Time: 2.55969\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xd9d1d89fceeca81a\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xd9d1d89fceeca81a Time: 1.72987\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xdb70c5e9779254fb\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xdb70c5e9779254fb Time: 2.04773\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.757605\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xe84b9aaa289245c0\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xe84b9aaa289245c0 Time: 0.840247\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xe9fa7b19132889a8\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xe9fa7b19132889a8 Time: 1.68725\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0xf1d5fc0783e71536\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xf1d5fc0783e71536 Time: 1.31714\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xf368aae1fb20baa1\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xf368aae1fb20baa1 Time: 0.80155\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.772087\n",
      "[10/18/2022-15:55:15] [V] [TRT] Fastest Tactic: 0xdc1c841ef1cd3e8e Time: 0.757605\n",
      "[10/18/2022-15:55:15] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:15] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:15] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:15] [V] [TRT] --------------- Timing Runner: Conv_58 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:15] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:15] [V] [TRT] --------------- Timing Runner: Conv_58 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:15] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:15] [V] [TRT] --------------- Timing Runner: Conv_58 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.11852\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x0000000000000001 Time: 3.32241\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.72048\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x0000000000000004 Time: 11.4033\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x0000000000000005 Time: 14.0691\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x0000000000000038 Time: 4.43116\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x0000000000000039 Time: 3.33411\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x000000000000003a Time: 4.83078\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x000000000000003c Time: 11.4548\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x000000000000003d Time: 14.121\n",
      "[10/18/2022-15:55:15] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 3.32241\n",
      "[10/18/2022-15:55:15] [V] [TRT] --------------- Timing Runner: Conv_58 (CublasConvolution)\n",
      "[10/18/2022-15:55:15] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:15] [V] [TRT] --------------- Timing Runner: Conv_58 (CaskConvolution)\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 2.47088\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 2.4171\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 2.72588\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:15] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 2.57201\n",
      "[10/18/2022-15:55:15] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 3.66115\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 2.51553\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 2.33999\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 2.45672\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 3.98658\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 3.47458\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 3.29023\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 3.02723\n",
      "[10/18/2022-15:55:16] [V] [TRT] Fastest Tactic: 0x7e29bdfccd92c42c Time: 2.33999\n",
      "[10/18/2022-15:55:16] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:55:16] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:55:16] [V] [TRT] --------------- Timing Runner: Conv_58 (CublasConvolution)\n",
      "[10/18/2022-15:55:16] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:16] [V] [TRT] --------------- Timing Runner: Conv_58 (CaskConvolution)\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 2.50214\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 2.45936\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 2.49525\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 3.3359\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 3.53995\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 2.34667\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 2.37845\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 2.53134\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 3.38639\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 2.35758\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 2.78328\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 2.49821\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 3.64373\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 2.79962\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0xf92663d88255134b Time: 2.38576\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 3.37686\n",
      "[10/18/2022-15:55:16] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 2.73552\n",
      "[10/18/2022-15:55:16] [V] [TRT] Fastest Tactic: 0x810bd80d0531c0a0 Time: 2.34667\n",
      "[10/18/2022-15:55:16] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:16] [V] [TRT] *************** Autotuning format combination: Half(50176,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:16] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:16] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:16] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:16] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:16] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:16] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:16] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:16] [V] [TRT] --------------- Timing Runner: Conv_58 (CudnnConvolution)\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.92524\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.76508\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.47777\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x0000000000000004 Time: 10.7472\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x0000000000000005 Time: 13.5139\n",
      "[10/18/2022-15:55:16] [V] [TRT] Tactic: 0x0000000000000038 Time: 4.39348\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x000000000000003a Time: 4.47782\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x000000000000003c Time: 10.8613\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x000000000000003d Time: 13.5205\n",
      "[10/18/2022-15:55:17] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 2.76508\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_58 (CublasConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_58 (CaskConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:17] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:55:17] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_58 (CaskConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:17] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_58 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_58 (CublasConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_58 (CaskConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 1.05636\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 1.06127\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 1.05298\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x446c8c788145836a Time: 1.38864\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 1.46589\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 1.47209\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 1.60009\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x97afba3735828021 Time: 1.51172\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 1.37317\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 1.46512\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 1.47769\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 1.37931\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 1.30135\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 1.44917\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 1.27852\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 1.26342\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 1.38442\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 1.40384\n",
      "[10/18/2022-15:55:17] [V] [TRT] Fastest Tactic: 0x3bee4a098b4f8914 Time: 1.05298\n",
      "[10/18/2022-15:55:17] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:17] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_58 (CublasConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_58 (CaskConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:17] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_58 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_58 (CublasConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:17] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:17] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_58 (CaskConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 0.853723\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 0.874496\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.588005\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 0.815762\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.478062\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.50795\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 0.640686\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x249110624ee04937 Time: 0.582775\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 0.676073\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 0.66011\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.423351\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 0.665906\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 0.675191\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 0.906107\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 0.952544\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 0.631474\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 0.523214\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 0.942638\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 0.804229\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 0.878363\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 1.01493\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 0.779429\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 0.804955\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 0.885787\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 0.681106\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 0.890587\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 0.869175\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 0.869234\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 0.541545\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 0.56133\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 0.678528\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 0.781787\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 0.779118\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 1.22486\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.534053\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x83b35618df65874c Time: 1.00118\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 0.722139\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 0.759031\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 0.81755\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.578633\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 0.940078\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 0.741051\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 0.782066\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 1.20919\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 0.797431\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 0.609426\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.660585\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.646162\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 0.790235\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.470089\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 0.724114\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 0.621184\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.466062\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 0.562245\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 0.795049\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.528978\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.558167\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 0.932137\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.455648\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 1.03015\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 1.27384\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 0.484293\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.543845\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 0.847694\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 0.908434\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 0.823003\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 0.706921\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 0.690144\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.536251\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 1.22885\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 0.540064\n",
      "[10/18/2022-15:55:17] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:17] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.434176\n",
      "[10/18/2022-15:55:17] [V] [TRT] Fastest Tactic: 0x2a3615ad33745f0b Time: 0.423351\n",
      "[10/18/2022-15:55:17] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:17] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:17] [V] [TRT] *************** Autotuning format combination: Float(401408,784,28,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:17] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:17] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x0000000000000000 Time: 9.01719\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x0000000000000001 Time: 8.63435\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x0000000000000002 Time: 9.60703\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x0000000000000038 Time: 9.10794\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x0000000000000039 Time: 8.58777\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x000000000000003a Time: 9.79588\n",
      "[10/18/2022-15:55:18] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 8.58777\n",
      "[10/18/2022-15:55:18] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (CaskConvolution)\n",
      "[10/18/2022-15:55:18] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 4.775\n",
      "[10/18/2022-15:55:18] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 5.04052\n",
      "[10/18/2022-15:55:18] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 7.48396\n",
      "[10/18/2022-15:55:18] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 5.75691\n",
      "[10/18/2022-15:55:18] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 5.6183\n",
      "[10/18/2022-15:55:18] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 5.75722\n",
      "[10/18/2022-15:55:18] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x8d5c64a52fab02c9\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x8d5c64a52fab02c9 Time: 8.9816\n",
      "[10/18/2022-15:55:18] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 5.95354\n",
      "[10/18/2022-15:55:18] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x994f5b723e2d80da\n",
      "[10/18/2022-15:55:18] [V] [TRT] Tactic: 0x994f5b723e2d80da Time: 7.82976\n",
      "[10/18/2022-15:55:18] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 7.7551\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x128x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xac49795b871b0d29\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0xac49795b871b0d29 Time: 7.04863\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 7.86571\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 7.91081\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd1338f4b38d341e2\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0xd1338f4b38d341e2 Time: 7.5934\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x256x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd2aa21bfe2167c0c\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0xd2aa21bfe2167c0c Time: 6.31786\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xe40b38338a3a7d7e\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0xe40b38338a3a7d7e Time: 7.59866\n",
      "[10/18/2022-15:55:19] [V] [TRT] Fastest Tactic: 0x18597bd4a7d0164d Time: 4.775\n",
      "[10/18/2022-15:55:19] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:19] [V] [TRT] *************** Autotuning format combination: Float(401408,1,14336,512), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:55:19] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (CaskConvolution)\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x0447933cc2be855a\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x0447933cc2be855a Time: 5.94888\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 4.50274\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x2595402367cdee5c\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x2595402367cdee5c Time: 7.28624\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 4.83733\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 4.97443\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 6.4632\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 7.03189\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 4.61092\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 4.74595\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 5.02879\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 6.50384\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:19] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.61707\n",
      "[10/18/2022-15:55:19] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x963db12d24e61b80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x963db12d24e61b80 Time: 5.88518\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 5.94125\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 4.9559\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xde4165142218dab8\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0xde4165142218dab8 Time: 5.83153\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0xf92663d88255134b Time: 4.91038\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 6.56969\n",
      "[10/18/2022-15:55:20] [V] [TRT] Fastest Tactic: 0x0bf55a7b77a6ff98 Time: 4.50274\n",
      "[10/18/2022-15:55:20] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:20] [V] [TRT] *************** Autotuning format combination: Half(401408,784,28,1), Half(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:20] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:20] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:20] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (CudnnConvolution)\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x0000000000000000 Time: 8.89128\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x0000000000000001 Time: 16.0918\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x0000000000000002 Time: 9.13251\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x0000000000000038 Time: 9.38551\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x000000000000003a Time: 9.40205\n",
      "[10/18/2022-15:55:20] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 8.89128\n",
      "[10/18/2022-15:55:20] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:20] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:20] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (CaskConvolution)\n",
      "[10/18/2022-15:55:20] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:20] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000000\n",
      "[10/18/2022-15:55:20] [V] [TRT] *************** Autotuning format combination: Half(200704,784:2,28,1), Half(100352,196:2,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:20] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:20] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:20] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:20] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:20] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:20] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:20] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:20] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:20] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (CaskConvolution)\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.22803\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.53835\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 3.0763\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x446c8c788145836a Time: 3.69084\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 3.64251\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 2.93042\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.91649\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:55:20] [V] [TRT] Tactic: 0x97afba3735828021 Time: 3.26267\n",
      "[10/18/2022-15:55:20] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 2.80659\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 3.28779\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.9927\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 3.32917\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 2.81922\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.95463\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 2.72121\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 2.69897\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 3.18405\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 3.03311\n",
      "[10/18/2022-15:55:21] [V] [TRT] Fastest Tactic: 0x16eafdbc5869b184 Time: 2.22803\n",
      "[10/18/2022-15:55:21] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:21] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,1792,64), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:21] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:21] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:21] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:21] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:21] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (CaskConvolution)\n",
      "[10/18/2022-15:55:21] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:21] [V] [TRT] *************** Autotuning format combination: Half(50176,1:8,1792,64), Half(25088,1:8,1792,128) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:55:21] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:21] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:21] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:21] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:21] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:21] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:21] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:21] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:21] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:21] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:21] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:21] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:21] [V] [TRT] --------------- Timing Runner: Conv_59 + Add_60 + Relu_61 (CaskConvolution)\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x0559d1d2893a8768\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x0559d1d2893a8768 Time: 2.29618\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x0b906efbde4dc01a\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x0b906efbde4dc01a Time: 1.77451\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x0e0f7f10867063ba\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x0e0f7f10867063ba Time: 1.0014\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x0ecf8dc91198fd5e\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x0ecf8dc91198fd5e Time: 0.906341\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 1.08335\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0x159236c6c22f62ce\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x159236c6c22f62ce Time: 1.38539\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x15ecbd82c22a023f\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x15ecbd82c22a023f Time: 0.911333\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x18ef97651ad5379a\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x18ef97651ad5379a Time: 1.85762\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x1b099f7ac29a2a6a\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x1b099f7ac29a2a6a Time: 1.64132\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x1b9cb8d78519a728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x1b9cb8d78519a728 Time: 2.03766\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0x1c23f4a19fbcb518\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x1c23f4a19fbcb518 Time: 1.25484\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 1.77883\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 1.06791\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 1.01531\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 1.18751\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 1.28734\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.874807\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x33fc6102b341eb5d\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x33fc6102b341eb5d Time: 1.5366\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.68789\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x348653930e0a64e2\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x348653930e0a64e2 Time: 1.53982\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x36662b4d547eefc7\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x36662b4d547eefc7 Time: 1.39113\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 1.1934\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 1.09405\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 1.89097\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 1.62377\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 1.71909\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x5128cdf162fe56b6\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x5128cdf162fe56b6 Time: 1.30803\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 1.9768\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.52236\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 1.53967\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x54b287be85c1522c\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x54b287be85c1522c Time: 1.59805\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x55fb34a08663e5ae\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x55fb34a08663e5ae Time: 1.70853\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x58eea09dffe038fd\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x58eea09dffe038fd Time: 1.25417\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x5bec1fbd955eb827\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x5bec1fbd955eb827 Time: 1.41094\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.70657\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x62bb371b230a886d\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x62bb371b230a886d Time: 1.75397\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.82267\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x69a5b2ac9c5bac16\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x69a5b2ac9c5bac16 Time: 1.27799\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6b44e6396887bed9\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x6b44e6396887bed9 Time: 1.24037\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x6cde8847e8cd796b\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x6cde8847e8cd796b Time: 1.53332\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.58617\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.67926\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x75585ae3e9dedb93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x75585ae3e9dedb93 Time: 1.85186\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x784dcede905d06c0\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x784dcede905d06c0 Time: 1.33794\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 1.07515\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 1.4113\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 1.60037\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 1.49659\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x86903737887c556d\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x86903737887c556d Time: 1.50791\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 1.08626\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 1.77846\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 1.53842\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x8b86a8bb857fff79\n",
      "[10/18/2022-15:55:21] [V] [TRT] Tactic: 0x8b86a8bb857fff79 Time: 1.75114\n",
      "[10/18/2022-15:55:21] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 1.51253\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x8d73ddfc444be692\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x8d73ddfc444be692 Time: 1.08759\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 1.55368\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x9650edb797f919f3\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x9650edb797f919f3 Time: 1.14616\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0x969b1abbb567ac47\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x969b1abbb567ac47 Time: 1.51817\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 1.10694\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0x9a0f43b4d1dc46d4\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x9a0f43b4d1dc46d4 Time: 2.16386\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa13cdf70a9d99d45\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xa13cdf70a9d99d45 Time: 1.94133\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa2dad76f719680b5\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xa2dad76f719680b5 Time: 1.76799\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0xa3e778b253a14ca9\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xa3e778b253a14ca9 Time: 2.51246\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0xa5f0bcb42cb01fc7\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xa5f0bcb42cb01fc7 Time: 1.24994\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 1.19572\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xab9c5449bde6902c\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xab9c5449bde6902c Time: 0.945618\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 1.17429\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb0bf64026e546f4d\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xb0bf64026e546f4d Time: 1.4114\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xb26e93bd0702f504\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xb26e93bd0702f504 Time: 1.85089\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb7dc3705357cc965\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xb7dc3705357cc965 Time: 1.05952\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.901733\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 1.02486\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xbfc71f913e286527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xbfc71f913e286527 Time: 1.62563\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.60153\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.95189\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xc6e0905d983b4a62\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xc6e0905d983b4a62 Time: 2.18814\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 1.10905\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0xc8ee1e4cdf0d8f84\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xc8ee1e4cdf0d8f84 Time: 1.49921\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd076fab92f5706c9\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xd076fab92f5706c9 Time: 1.20744\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd297ae2cdb8b1406\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xd297ae2cdb8b1406 Time: 1.21006\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.7683\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.879735\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xd825f95894186a22\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xd825f95894186a22 Time: 2.32733\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 1.75952\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 2.32916\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 1.02254\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 1.57229\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 1.61259\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 1.2796\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xe84b9aaa289245c0\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xe84b9aaa289245c0 Time: 1.09146\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xe9fa7b19132889a8\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xe9fa7b19132889a8 Time: 1.62895\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 1.03991\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0xf1d5fc0783e71536\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xf1d5fc0783e71536 Time: 1.60827\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xf368aae1fb20baa1\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xf368aae1fb20baa1 Time: 1.1362\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 2.23285\n",
      "[10/18/2022-15:55:22] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.886711\n",
      "[10/18/2022-15:55:22] [V] [TRT] Fastest Tactic: 0x2a3615ad33745f0b Time: 0.874807\n",
      "[10/18/2022-15:55:22] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:22] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:22] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:22] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:22] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:22] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:22] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:22] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CudnnConvolution)\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.92708\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.65568\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.36981\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x0000000000000004 Time: 10.5171\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x0000000000000005 Time: 14.672\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x0000000000000038 Time: 3.49828\n",
      "[10/18/2022-15:55:22] [V] [TRT] Tactic: 0x0000000000000039 Time: 2.68839\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x000000000000003a Time: 4.51487\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x000000000000003c Time: 10.7193\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x000000000000003d Time: 14.5262\n",
      "[10/18/2022-15:55:23] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 2.65568\n",
      "[10/18/2022-15:55:23] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CublasConvolution)\n",
      "[10/18/2022-15:55:23] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:23] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CaskConvolution)\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 2.22211\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 2.17639\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 2.60831\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 2.29006\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 3.33769\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 2.51512\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 2.40084\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 2.59743\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 2.98795\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 3.3821\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 3.25716\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 3.06937\n",
      "[10/18/2022-15:55:23] [V] [TRT] Fastest Tactic: 0x195431d38ba5af88 Time: 2.17639\n",
      "[10/18/2022-15:55:23] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:23] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:55:23] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CublasConvolution)\n",
      "[10/18/2022-15:55:23] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:23] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CaskConvolution)\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 2.38636\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 2.48494\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 2.41184\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 2.86688\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 3.69153\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 2.28469\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 2.27177\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 2.39675\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 2.64543\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 2.24374\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 2.7224\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 2.38874\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 3.74249\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 2.86131\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0xf92663d88255134b Time: 2.36777\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 2.75193\n",
      "[10/18/2022-15:55:23] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 2.74474\n",
      "[10/18/2022-15:55:23] [V] [TRT] Fastest Tactic: 0x946eca69f99ddcb4 Time: 2.24374\n",
      "[10/18/2022-15:55:23] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:23] [V] [TRT] *************** Autotuning format combination: Half(200704,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:23] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:23] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:23] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:23] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:23] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CudnnConvolution)\n",
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.51121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:23] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.30295\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.40006\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x0000000000000004 Time: 10.4947\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x0000000000000005 Time: 13.4969\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x0000000000000038 Time: 4.06682\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x000000000000003a Time: 4.48863\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x000000000000003c Time: 10.1922\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x000000000000003d Time: 13.7377\n",
      "[10/18/2022-15:55:24] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 2.30295\n",
      "[10/18/2022-15:55:24] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:24] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CublasConvolution)\n",
      "[10/18/2022-15:55:24] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:24] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:24] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CaskConvolution)\n",
      "[10/18/2022-15:55:24] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:24] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:55:24] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:24] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:24] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CaskConvolution)\n",
      "[10/18/2022-15:55:24] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:24] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:24] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:24] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:24] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:24] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:24] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CublasConvolution)\n",
      "[10/18/2022-15:55:24] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:24] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:24] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CaskConvolution)\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 1.14805\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 1.024\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 1.02456\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x446c8c788145836a Time: 1.26032\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 1.33098\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 1.27965\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 1.3351\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x97afba3735828021 Time: 1.43009\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 1.3395\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 1.45055\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 1.3124\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 1.36357\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 1.26624\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 1.28262\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 1.24099\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 1.21271\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 1.34813\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 1.22183\n",
      "[10/18/2022-15:55:24] [V] [TRT] Fastest Tactic: 0x21904dd9d0cd407e Time: 1.024\n",
      "[10/18/2022-15:55:24] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:24] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:24] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:24] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CublasConvolution)\n",
      "[10/18/2022-15:55:24] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:24] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:24] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CaskConvolution)\n",
      "[10/18/2022-15:55:24] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:24] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:55:24] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:24] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:24] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:24] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:24] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CublasConvolution)\n",
      "[10/18/2022-15:55:24] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:24] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:24] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:24] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:24] [V] [TRT] --------------- Timing Runner: Conv_62 + Relu_63 (CaskConvolution)\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 0.897902\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 0.710368\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.43349\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 0.678501\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.455557\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.39029\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 0.482075\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x249110624ee04937 Time: 0.554958\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 0.583785\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 0.624791\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.403387\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 0.625486\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 0.557641\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 0.885568\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 0.987954\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 0.655945\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 0.517001\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 0.945733\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 0.832777\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 0.813787\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 1.02047\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 0.680599\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 0.811621\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 0.721381\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 0.637403\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 0.780873\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 0.700046\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 0.778304\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 0.485934\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 0.485394\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 0.602112\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 0.750153\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 0.763671\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 1.04905\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.481202\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x83b35618df65874c Time: 0.949824\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 0.644992\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 0.604142\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:55:24] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 0.635186\n",
      "[10/18/2022-15:55:24] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.487561\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 0.885637\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 0.63488\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 0.656823\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 1.15503\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 0.640731\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 0.609019\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.486985\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.47723\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 0.79285\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.420576\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 0.637221\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 0.60821\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.497179\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 0.53835\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 0.666354\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.423246\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.49152\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 0.763026\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.401701\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 0.838665\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 1.16301\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 0.496914\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.412398\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 0.776754\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 0.705093\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 0.650382\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 0.65653\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 0.634624\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.411063\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 1.08944\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 0.53365\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.403218\n",
      "[10/18/2022-15:55:25] [V] [TRT] Fastest Tactic: 0x21739cdb4c6113ed Time: 0.39029\n",
      "[10/18/2022-15:55:25] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:25] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:25] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:25] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:25] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:25] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:25] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:25] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (CudnnConvolution)\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.69138\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.42554\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x0000000000000002 Time: 7.98281\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x0000000000000004 Time: 3.33034\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x0000000000000005 Time: 9.7991\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x0000000000000006 Time: 4.57413\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x0000000000000038 Time: 6.2879\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x0000000000000039 Time: 5.05851\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x000000000000003a Time: 8.22459\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x000000000000003c Time: 3.27467\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x000000000000003d Time: 9.88031\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x000000000000003e Time: 4.63995\n",
      "[10/18/2022-15:55:25] [V] [TRT] Fastest Tactic: 0x000000000000003c Time: 3.27467\n",
      "[10/18/2022-15:55:25] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (CaskConvolution)\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.6635\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x268494f0a1c83de3\n",
      "[10/18/2022-15:55:25] [V] [TRT] Tactic: 0x268494f0a1c83de3 Time: 4.00748\n",
      "[10/18/2022-15:55:25] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x27728c886a448c5a\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x27728c886a448c5a Time: 5.03571\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 4.874\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x597d29027694c20b\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x597d29027694c20b Time: 4.98801\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 4.87563\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x62b2ffd9a5c0cfb5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x62b2ffd9a5c0cfb5 Time: 7.03547\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x8d5c64a52fab02c9\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x8d5c64a52fab02c9 Time: 8.19229\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 5.08289\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x93a1176336e5b9f6\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x93a1176336e5b9f6 Time: 5.90478\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x994f5b723e2d80da\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x994f5b723e2d80da Time: 5.99634\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x128x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xac49795b871b0d29\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0xac49795b871b0d29 Time: 5.43852\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb6717e61503d5e9b\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0xb6717e61503d5e9b Time: 5.55652\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 6.68999\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 5.75923\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd1338f4b38d341e2\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0xd1338f4b38d341e2 Time: 5.98182\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x256x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd2aa21bfe2167c0c\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0xd2aa21bfe2167c0c Time: 5.25456\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xe40b38338a3a7d7e\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0xe40b38338a3a7d7e Time: 6.78181\n",
      "[10/18/2022-15:55:26] [V] [TRT] Fastest Tactic: 0x268494f0a1c83de3 Time: 4.00748\n",
      "[10/18/2022-15:55:26] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x000000000000003c\n",
      "[10/18/2022-15:55:26] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:55:26] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (CaskConvolution)\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x0447933cc2be855a\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x0447933cc2be855a Time: 4.6812\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 4.53105\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x0e2033f7517a807f\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x0e2033f7517a807f Time: 4.80214\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x2595402367cdee5c\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x2595402367cdee5c Time: 7.44135\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3eba442f4c9c4f50\n",
      "[10/18/2022-15:55:26] [V] [TRT] Tactic: 0x3eba442f4c9c4f50 Time: 5.19225\n",
      "[10/18/2022-15:55:26] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x43334a9c8840c773\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x43334a9c8840c773 Time: 5.55496\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 5.10011\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 4.76111\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 5.41259\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x66e3239eee98201e\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x66e3239eee98201e Time: 5.69691\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 4.80934\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 4.99211\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.64779\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x963db12d24e61b80\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x963db12d24e61b80 Time: 5.71262\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb132670a7750e065\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0xb132670a7750e065 Time: 7.48779\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca84742beb9f9767\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0xca84742beb9f9767 Time: 4.68612\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xd2b62ec40baf8ee4\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0xd2b62ec40baf8ee4 Time: 4.9383\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xde4165142218dab8\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0xde4165142218dab8 Time: 5.77825\n",
      "[10/18/2022-15:55:27] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 5.41378\n",
      "[10/18/2022-15:55:27] [V] [TRT] Fastest Tactic: 0x0bf55a7b77a6ff98 Time: 4.53105\n",
      "[10/18/2022-15:55:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:27] [V] [TRT] *************** Autotuning format combination: Half(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:27] [W] [TRT] Weights [name=Conv_64 + Relu_65.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:27] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:27] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:27] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:27] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (CudnnConvolution)\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.18061\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.43598\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x0000000000000002 Time: 9.22537\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x0000000000000004 Time: 3.04474\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x0000000000000005 Time: 9.58168\n",
      "[10/18/2022-15:55:27] [V] [TRT] Tactic: 0x0000000000000006 Time: 4.69305\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.63678\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x000000000000003a Time: 8.84381\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x000000000000003c Time: 3.02428\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x000000000000003d Time: 9.79786\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x000000000000003e Time: 4.91403\n",
      "[10/18/2022-15:55:28] [V] [TRT] Fastest Tactic: 0x000000000000003c Time: 3.02428\n",
      "[10/18/2022-15:55:28] [W] [TRT] Weights [name=Conv_64 + Relu_65.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:28] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (CaskConvolution)\n",
      "[10/18/2022-15:55:28] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x000000000000003c\n",
      "[10/18/2022-15:55:28] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:28] [W] [TRT] Weights [name=Conv_64 + Relu_65.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:28] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:28] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:28] [W] [TRT] Weights [name=Conv_64 + Relu_65.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:28] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (CaskConvolution)\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x0fe4a9cce7ed878b\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x0fe4a9cce7ed878b Time: 2.27085\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.45374\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.68523\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 2.6968\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x4092cbc840fbea35\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x4092cbc840fbea35 Time: 2.72459\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x446c8c788145836a Time: 2.98569\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 2.92626\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 2.68232\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.62799\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0x98a00f59a4b141f0\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x98a00f59a4b141f0 Time: 3.04969\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 2.76773\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.52869\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xcbe3f30275b04323\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0xcbe3f30275b04323 Time: 2.66957\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.57525\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0xd46b3ee2b59f893c\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0xd46b3ee2b59f893c Time: 2.06413\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xd7d66d5d03a72c4e\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0xd7d66d5d03a72c4e Time: 2.89328\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 2.76977\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.65998\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xfc994367fd14b2d9\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0xfc994367fd14b2d9 Time: 2.63227\n",
      "[10/18/2022-15:55:28] [V] [TRT] Fastest Tactic: 0xd46b3ee2b59f893c Time: 2.06413\n",
      "[10/18/2022-15:55:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd46b3ee2b59f893c\n",
      "[10/18/2022-15:55:28] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:28] [W] [TRT] Weights [name=Conv_64 + Relu_65.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:28] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (CaskConvolution)\n",
      "[10/18/2022-15:55:28] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:28] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:55:28] [W] [TRT] Weights [name=Conv_64 + Relu_65.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:28] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:28] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:28] [W] [TRT] Weights [name=Conv_64 + Relu_65.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:28] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:28] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:28] [V] [TRT] --------------- Timing Runner: Conv_64 + Relu_65 (CaskConvolution)\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x00a425145e84482b\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x00a425145e84482b Time: 1.15683\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x03512591e8ea2977\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x03512591e8ea2977 Time: 1.5992\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x0559d1d2893a8768\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x0559d1d2893a8768 Time: 2.22688\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x095000b22a78f234\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x095000b22a78f234 Time: 1.16335\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x0b906efbde4dc01a\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x0b906efbde4dc01a Time: 1.3248\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x0c0088d5808566d2\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x0c0088d5808566d2 Time: 0.70075\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x0caa5410b61e6cc5\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x0caa5410b61e6cc5 Time: 1.1749\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x0e0f7f10867063ba\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x0e0f7f10867063ba Time: 0.922071\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x0e131ddbafdfe235\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x0e131ddbafdfe235 Time: 1.17163\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x0ecf8dc91198fd5e\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x0ecf8dc91198fd5e Time: 0.791429\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0x159236c6c22f62ce\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x159236c6c22f62ce Time: 1.20617\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x15ecbd82c22a023f\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x15ecbd82c22a023f Time: 0.756942\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x18ef97651ad5379a\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x18ef97651ad5379a Time: 1.7707\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x1981adfb6b6fd8b9\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x1981adfb6b6fd8b9 Time: 1.25536\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x1b099f7ac29a2a6a\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x1b099f7ac29a2a6a Time: 1.49967\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x1b9cb8d78519a728\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x1b9cb8d78519a728 Time: 1.87867\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0x1c23f4a19fbcb518\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x1c23f4a19fbcb518 Time: 0.915218\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.874789\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x1de724868edf11b0\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x1de724868edf11b0 Time: 1.10336\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.664736\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x30150d05024bc911\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x30150d05024bc911 Time: 0.859593\n",
      "[10/18/2022-15:55:28] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x32789ed2e6c7b43b\n",
      "[10/18/2022-15:55:28] [V] [TRT] Tactic: 0x32789ed2e6c7b43b Time: 0.762437\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x33fc6102b341eb5d\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x33fc6102b341eb5d Time: 1.24272\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.43067\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x348653930e0a64e2\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x348653930e0a64e2 Time: 1.63225\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x350e898a5a20ad00\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x350e898a5a20ad00 Time: 0.815982\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x36662b4d547eefc7\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x36662b4d547eefc7 Time: 1.24602\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x490a097d77573bff\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x490a097d77573bff Time: 0.742816\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x4c6a6da741444412\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x4c6a6da741444412 Time: 0.725792\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x4e34a65090c3b86f\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x4e34a65090c3b86f Time: 0.712672\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x504f864880743a14\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x504f864880743a14 Time: 2.36863\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x5128cdf162fe56b6\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x5128cdf162fe56b6 Time: 1.24992\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.24987\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r3s3 Tactic: 0x5252dc6c9c5f3aff\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x5252dc6c9c5f3aff Time: 1.55443\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x54b287be85c1522c\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x54b287be85c1522c Time: 1.22257\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x55fb34a08663e5ae\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x55fb34a08663e5ae Time: 1.53509\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x56c66ffbce24b635\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x56c66ffbce24b635 Time: 1.4432\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x58eea09dffe038fd\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x58eea09dffe038fd Time: 1.20673\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x5bec1fbd955eb827\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x5bec1fbd955eb827 Time: 1.30896\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.37711\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x62bb371b230a886d\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x62bb371b230a886d Time: 1.62007\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.34548\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x69a5b2ac9c5bac16\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x69a5b2ac9c5bac16 Time: 1.18589\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6b44e6396887bed9\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x6b44e6396887bed9 Time: 1.17777\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x6cde8847e8cd796b\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x6cde8847e8cd796b Time: 1.20066\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x6cee4d9c86b4cdd5\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x6cee4d9c86b4cdd5 Time: 1.23143\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.35744\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x721049a39aae27ff\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x721049a39aae27ff Time: 2.092\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.4051\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x75585ae3e9dedb93\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x75585ae3e9dedb93 Time: 1.66912\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x784dcede905d06c0\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x784dcede905d06c0 Time: 1.26157\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.893655\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x86903737887c556d\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x86903737887c556d Time: 1.21595\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x8781623566dac7f0\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x8781623566dac7f0 Time: 0.899973\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x8b86a8bb857fff79\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x8b86a8bb857fff79 Time: 1.77099\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x8d73ddfc444be692\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x8d73ddfc444be692 Time: 0.72821\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x9650edb797f919f3\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x9650edb797f919f3 Time: 0.74448\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0x969b1abbb567ac47\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x969b1abbb567ac47 Time: 1.19641\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0x9a0f43b4d1dc46d4\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x9a0f43b4d1dc46d4 Time: 2.0709\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa13cdf70a9d99d45\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xa13cdf70a9d99d45 Time: 1.83532\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa2dad76f719680b5\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xa2dad76f719680b5 Time: 1.56748\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0xa3e778b253a14ca9\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xa3e778b253a14ca9 Time: 2.18512\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0xa5f0bcb42cb01fc7\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xa5f0bcb42cb01fc7 Time: 0.881477\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r3s3 Tactic: 0xa84824f86c61d2d8\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xa84824f86c61d2d8 Time: 0.865111\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.792658\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xab9c5449bde6902c\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xab9c5449bde6902c Time: 0.721243\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xac4736b5b00e1531\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xac4736b5b00e1531 Time: 1.19619\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.792576\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb0bf64026e546f4d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xb0bf64026e546f4d Time: 0.846926\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xb26e93bd0702f504\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xb26e93bd0702f504 Time: 1.88353\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xb307bc772518d3d7\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xb307bc772518d3d7 Time: 1.15365\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb7dc3705357cc965\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xb7dc3705357cc965 Time: 0.874921\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xbb3d6545e4864f26\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xbb3d6545e4864f26 Time: 0.685787\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xbfc71f913e286527\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xbfc71f913e286527 Time: 1.58711\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.24693\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xc684285f13ba11d0\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xc684285f13ba11d0 Time: 1.54554\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xc6e0905d983b4a62\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xc6e0905d983b4a62 Time: 1.98477\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0xc8ee1e4cdf0d8f84\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xc8ee1e4cdf0d8f84 Time: 1.20748\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xcb7b50f35a87094b\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xcb7b50f35a87094b Time: 1.66127\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd076fab92f5706c9\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xd076fab92f5706c9 Time: 1.10669\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd297ae2cdb8b1406\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xd297ae2cdb8b1406 Time: 1.11996\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.3923\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.726162\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xd825f95894186a22\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xd825f95894186a22 Time: 2.40506\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xd9d1d89fceeca81a\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xd9d1d89fceeca81a Time: 1.61559\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xdb70c5e9779254fb\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xdb70c5e9779254fb Time: 1.89791\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.689591\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xe84b9aaa289245c0\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xe84b9aaa289245c0 Time: 0.737806\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xe9fa7b19132889a8\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xe9fa7b19132889a8 Time: 1.55648\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0xf1d5fc0783e71536\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xf1d5fc0783e71536 Time: 1.21516\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xf368aae1fb20baa1\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xf368aae1fb20baa1 Time: 0.717385\n",
      "[10/18/2022-15:55:29] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.699662\n",
      "[10/18/2022-15:55:29] [V] [TRT] Fastest Tactic: 0x21739cdb4c6113ed Time: 0.664736\n",
      "[10/18/2022-15:55:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:29] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:29] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:29] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:29] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:29] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:29] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:29] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CudnnConvolution)\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.25994\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.36027\n",
      "[10/18/2022-15:55:29] [V] [TRT] Tactic: 0x0000000000000002 Time: 6.61824\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x0000000000000004 Time: 13.2851\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x0000000000000005 Time: 16.1494\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x0000000000000038 Time: 6.25068\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x0000000000000039 Time: 5.36768\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x000000000000003a Time: 6.66277\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x000000000000003c Time: 13.2415\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x000000000000003d Time: 15.9925\n",
      "[10/18/2022-15:55:30] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 5.36027\n",
      "[10/18/2022-15:55:30] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CublasConvolution)\n",
      "[10/18/2022-15:55:30] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:30] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CaskConvolution)\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 2.68284\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 2.66674\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 2.7426\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 2.9563\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 4.05077\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 2.83385\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 2.70599\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 2.84508\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 4.62036\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 3.42338\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 3.48757\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 3.15939\n",
      "[10/18/2022-15:55:30] [V] [TRT] Fastest Tactic: 0x195431d38ba5af88 Time: 2.66674\n",
      "[10/18/2022-15:55:30] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:30] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:55:30] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CublasConvolution)\n",
      "[10/18/2022-15:55:30] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:30] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CaskConvolution)\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 2.84701\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:30] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 2.766\n",
      "[10/18/2022-15:55:30] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 2.66991\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 4.08861\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 3.61467\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 2.57712\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 2.51416\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 2.75234\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 4.11605\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 2.52035\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 2.91263\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 2.66657\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 3.91373\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 3.03537\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0xf92663d88255134b Time: 2.56794\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 4.12919\n",
      "[10/18/2022-15:55:31] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 2.8608\n",
      "[10/18/2022-15:55:31] [V] [TRT] Fastest Tactic: 0x849891f3d1d80c55 Time: 2.51416\n",
      "[10/18/2022-15:55:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:55:31] [V] [TRT] *************** Autotuning format combination: Half(50176,196,14,1), Half(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:31] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:31] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:31] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:31] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:31] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:31] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:31] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:31] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.55254\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x0000000000000001 Time: 3.96547\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x0000000000000002 Time: 5.73346\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x0000000000000004 Time: 11.9589\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x0000000000000005 Time: 14.5706\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x0000000000000038 Time: 5.5744\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x000000000000003a Time: 5.58284\n",
      "[10/18/2022-15:55:31] [V] [TRT] Tactic: 0x000000000000003c Time: 12.0522\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x000000000000003d Time: 14.7562\n",
      "[10/18/2022-15:55:32] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 3.96547\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CublasConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CaskConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1), Half(100352,196:2,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CublasConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CaskConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 1.21392\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 1.11797\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 1.15435\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x446c8c788145836a Time: 1.41229\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 1.67271\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 1.5281\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 1.69925\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x97afba3735828021 Time: 1.6056\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 1.45763\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 1.58252\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 1.59864\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 1.48016\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 1.35052\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 1.59071\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 1.40259\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 1.34822\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 1.51084\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 1.50768\n",
      "[10/18/2022-15:55:32] [V] [TRT] Fastest Tactic: 0x21904dd9d0cd407e Time: 1.11797\n",
      "[10/18/2022-15:55:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CublasConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CaskConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32), Half(25088,1:8,1792,128) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CublasConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_66 + Add_67 + Relu_68 (CaskConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 0.923246\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 0.939566\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.703977\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 0.991186\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.628571\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.727561\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 0.708905\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x249110624ee04937 Time: 0.606336\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 0.950857\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 0.765367\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.55344\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 0.765303\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 0.736402\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.01115\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 0.952027\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 0.64507\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 0.617774\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 0.985966\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 0.831374\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 1.01552\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 1.07214\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 0.995931\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 0.835369\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 0.999451\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 0.722359\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.06248\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 0.998688\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.01769\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 0.760763\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 0.726427\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 0.846222\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 0.820078\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 0.781189\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 1.2483\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.636635\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x83b35618df65874c Time: 1.05126\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 0.968421\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 1.04828\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 0.881234\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.804864\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 0.945582\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 1.02085\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 0.978917\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 1.18031\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 1.01733\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 0.617056\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.740201\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.735387\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 0.785262\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.572466\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 0.966629\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 0.646053\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.584933\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 0.750263\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.0205\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.772384\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.75776\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.04213\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.593106\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 0.991822\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 1.21728\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 0.585294\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.759397\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 0.891173\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 1.01142\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 0.893225\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 0.932667\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 0.726523\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.750153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 1.27825\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 0.721481\n",
      "[10/18/2022-15:55:32] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:32] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.578679\n",
      "[10/18/2022-15:55:32] [V] [TRT] Fastest Tactic: 0x2a3615ad33745f0b Time: 0.55344\n",
      "[10/18/2022-15:55:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:32] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(200704,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_69 + Relu_70.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_69 + Relu_70.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_69 + Relu_70 (CaskConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_69 + Relu_70.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_69 + Relu_70.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_69 + Relu_70 (CublasConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_69 + Relu_70.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_69 + Relu_70 (CaskConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_69 + Relu_70.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_71 + Relu_72.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_71 + Relu_72.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_71 + Relu_72.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] --------------- Timing Runner: Conv_71 + Relu_72 (CaskConvolution)\n",
      "[10/18/2022-15:55:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_71 + Relu_72.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(50176,196,14,1), Half(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1), Half(100352,196:2,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:32] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:32] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:32] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_73 + Add_74 + Relu_75 (CublasConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_73 + Add_74 + Relu_75 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32), Half(25088,1:8,1792,128) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(200704,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_76 + Relu_77.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_76 + Relu_77.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_76 + Relu_77 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_76 + Relu_77.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_76 + Relu_77.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_76 + Relu_77 (CublasConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_76 + Relu_77.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_76 + Relu_77 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_76 + Relu_77.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_78 + Relu_79.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_78 + Relu_79.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_78 + Relu_79.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_78 + Relu_79 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_78 + Relu_79.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(50176,196,14,1), Half(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1), Half(100352,196:2,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_80 + Add_81 + Relu_82 (CublasConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_80 + Add_81 + Relu_82 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32), Half(25088,1:8,1792,128) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(200704,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_83 + Relu_84.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_83 + Relu_84.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_83 + Relu_84 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_83 + Relu_84.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Float(50176,196,14,1) ***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_83 + Relu_84.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_83 + Relu_84 (CublasConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_83 + Relu_84.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_83 + Relu_84 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_83 + Relu_84.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_85 + Relu_86.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_85 + Relu_86.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_85 + Relu_86.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_85 + Relu_86 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_85 + Relu_86.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(50176,196,14,1), Half(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1), Half(100352,196:2,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_87 + Add_88 + Relu_89 (CublasConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_87 + Add_88 + Relu_89 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32), Half(25088,1:8,1792,128) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:33] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(200704,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_90 + Relu_91.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_90 + Relu_91.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_90 + Relu_91 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_90 + Relu_91.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_90 + Relu_91.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_90 + Relu_91 (CublasConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_90 + Relu_91.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_90 + Relu_91 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_90 + Relu_91.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256) -> Float(50176,1,3584,256) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(50176,196,14,1) -> Half(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_92 + Relu_93.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1) -> Half(25088,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_92 + Relu_93.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Float(50176,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_92 + Relu_93.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_92 + Relu_93 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32) -> Half(6272,1:8,448,32) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_92 + Relu_93.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,196,14,1), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(50176,1,3584,256), Float(200704,1,14336,1024) -> Float(200704,1,14336,1024) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(50176,196,14,1), Half(200704,196,14,1) -> Half(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(25088,196:2,14,1), Half(100352,196:2,14,1) -> Half(100352,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32), Float(200704,196,14,1) -> Float(200704,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_94 + Add_95 + Relu_96 (CublasConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_94 + Add_95 + Relu_96 (CaskConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Half(6272,1:8,448,32), Half(25088,1:8,1792,128) -> Half(25088,1:8,1792,128) ***************\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:33] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:33] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:33] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:33] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:33] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1) -> Float(100352,196,14,1) ***************\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:33] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CudnnConvolution)\n",
      "[10/18/2022-15:55:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.87292\n",
      "[10/18/2022-15:55:33] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.68876\n",
      "[10/18/2022-15:55:33] [V] [TRT] Tactic: 0x0000000000000002 Time: 7.97133\n",
      "[10/18/2022-15:55:33] [V] [TRT] Tactic: 0x0000000000000004 Time: 20.7336\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x0000000000000005 Time: 27.0483\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x0000000000000038 Time: 6.71276\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x0000000000000039 Time: 5.38966\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x000000000000003a Time: 8.02691\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x000000000000003c Time: 22.1857\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x000000000000003d Time: 27.0287\n",
      "[10/18/2022-15:55:34] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 5.38966\n",
      "[10/18/2022-15:55:34] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CublasConvolution)\n",
      "[10/18/2022-15:55:34] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:34] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CaskConvolution)\n",
      "[10/18/2022-15:55:34] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 4.32256\n",
      "[10/18/2022-15:55:34] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.35997\n",
      "[10/18/2022-15:55:34] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 5.33621\n",
      "[10/18/2022-15:55:34] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 4.72444\n",
      "[10/18/2022-15:55:34] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 6.45936\n",
      "[10/18/2022-15:55:34] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 4.80946\n",
      "[10/18/2022-15:55:34] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 4.64744\n",
      "[10/18/2022-15:55:34] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:34] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 4.96762\n",
      "[10/18/2022-15:55:34] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 5.87103\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 6.63142\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 6.0612\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 5.73316\n",
      "[10/18/2022-15:55:35] [V] [TRT] Fastest Tactic: 0x18597bd4a7d0164d Time: 4.32256\n",
      "[10/18/2022-15:55:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:35] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024) -> Float(100352,1,7168,512) ***************\n",
      "[10/18/2022-15:55:35] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CublasConvolution)\n",
      "[10/18/2022-15:55:35] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:35] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CaskConvolution)\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 4.56403\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 4.68778\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 4.64278\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 5.30817\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 6.87171\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 4.3757\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 4.45087\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 4.72332\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 5.32212\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.47196\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 5.47226\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 4.72183\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 7.16289\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 5.38185\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0xf92663d88255134b Time: 4.51906\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 5.33863\n",
      "[10/18/2022-15:55:35] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:55:35] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 5.35903\n",
      "[10/18/2022-15:55:35] [V] [TRT] Fastest Tactic: 0x810bd80d0531c0a0 Time: 4.3757\n",
      "[10/18/2022-15:55:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:35] [V] [TRT] *************** Autotuning format combination: Half(200704,196,14,1) -> Half(100352,196,14,1) ***************\n",
      "[10/18/2022-15:55:35] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:35] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:35] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:35] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CudnnConvolution)\n",
      "[10/18/2022-15:55:36] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.1642\n",
      "[10/18/2022-15:55:36] [V] [TRT] Tactic: 0x0000000000000001 Time: 4.66099\n",
      "[10/18/2022-15:55:36] [V] [TRT] Tactic: 0x0000000000000002 Time: 8.29045\n",
      "[10/18/2022-15:55:36] [V] [TRT] Tactic: 0x0000000000000004 Time: 20.0878\n",
      "[10/18/2022-15:55:36] [V] [TRT] Tactic: 0x0000000000000005 Time: 25.6844\n",
      "[10/18/2022-15:55:36] [V] [TRT] Tactic: 0x0000000000000038 Time: 8.13048\n",
      "[10/18/2022-15:55:36] [V] [TRT] Tactic: 0x000000000000003a Time: 8.16026\n",
      "[10/18/2022-15:55:36] [V] [TRT] Tactic: 0x000000000000003c Time: 20.9479\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x000000000000003d Time: 25.9839\n",
      "[10/18/2022-15:55:37] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 4.66099\n",
      "[10/18/2022-15:55:37] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:37] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CublasConvolution)\n",
      "[10/18/2022-15:55:37] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:37] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:37] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CaskConvolution)\n",
      "[10/18/2022-15:55:37] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:55:37] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(100352,196,14,1) ***************\n",
      "[10/18/2022-15:55:37] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:37] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CaskConvolution)\n",
      "[10/18/2022-15:55:37] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:37] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1) -> Half(50176,196:2,14,1) ***************\n",
      "[10/18/2022-15:55:37] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:37] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:37] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:37] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:37] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CublasConvolution)\n",
      "[10/18/2022-15:55:37] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:37] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:37] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CaskConvolution)\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.05453\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.13285\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 2.45497\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x446c8c788145836a Time: 2.84965\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 2.82631\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 2.53336\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.50883\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x97afba3735828021 Time: 2.52632\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 2.3568\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 2.69739\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.62232\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 2.63611\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 2.46593\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.53232\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 2.38775\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 2.32887\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 2.53691\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.44241\n",
      "[10/18/2022-15:55:37] [V] [TRT] Fastest Tactic: 0x16eafdbc5869b184 Time: 2.05453\n",
      "[10/18/2022-15:55:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:37] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Float(100352,196,14,1) ***************\n",
      "[10/18/2022-15:55:37] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:37] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CublasConvolution)\n",
      "[10/18/2022-15:55:37] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:37] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:37] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CaskConvolution)\n",
      "[10/18/2022-15:55:37] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:37] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128) -> Half(12544,1:8,896,64) ***************\n",
      "[10/18/2022-15:55:37] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:37] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:37] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:37] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CublasConvolution)\n",
      "[10/18/2022-15:55:37] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:37] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:37] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:37] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:37] [V] [TRT] --------------- Timing Runner: Conv_97 + Relu_98 (CaskConvolution)\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 1.5104\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 1.37442\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.786336\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 1.32926\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.890834\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.713289\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 1.0175\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x249110624ee04937 Time: 1.13655\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 1.21556\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 1.20094\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.725257\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 1.19628\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 1.04813\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.56381\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 1.7382\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 1.1357\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 0.890066\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 1.70159\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 1.49937\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 1.46682\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 1.85627\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.21299\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 1.48044\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.32389\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 1.18817\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.43797\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.32242\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.4716\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 0.816306\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 0.82827\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 1.15975\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 1.45715\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 1.49889\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 2.11585\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.904055\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x83b35618df65874c Time: 1.84639\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 1.18935\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 1.16385\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 1.24554\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.841435\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 1.69748\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 1.20129\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 1.21417\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:55:37] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 2.17869\n",
      "[10/18/2022-15:55:37] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 1.21217\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 1.10796\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.878382\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.857248\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 1.51786\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.762551\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 1.20526\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 1.13033\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.813897\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 0.933047\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.2484\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.788832\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.861038\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.49738\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.745271\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 1.70101\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 2.32159\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 0.824645\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.739159\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 1.51163\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 1.37329\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 1.28121\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 1.18833\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 1.20393\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.715282\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 2.08505\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 0.918085\n",
      "[10/18/2022-15:55:38] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.722798\n",
      "[10/18/2022-15:55:38] [V] [TRT] Fastest Tactic: 0x21739cdb4c6113ed Time: 0.713289\n",
      "[10/18/2022-15:55:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:38] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:38] [V] [TRT] *************** Autotuning format combination: Float(100352,196,14,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:55:38] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:38] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:38] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:38] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:38] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (CudnnConvolution)\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.67618\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.35665\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0x0000000000000002 Time: 7.0264\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0x0000000000000005 Time: 34.4887\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0x0000000000000038 Time: 6.29499\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0x0000000000000039 Time: 4.79614\n",
      "[10/18/2022-15:55:38] [V] [TRT] Tactic: 0x000000000000003a Time: 7.1585\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0x000000000000003d Time: 34.5235\n",
      "[10/18/2022-15:55:39] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 4.79614\n",
      "[10/18/2022-15:55:39] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (CaskConvolution)\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.67178\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x27728c886a448c5a\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0x27728c886a448c5a Time: 4.98454\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 5.31915\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x597d29027694c20b\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0x597d29027694c20b Time: 4.96084\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 4.99328\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x62b2ffd9a5c0cfb5\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0x62b2ffd9a5c0cfb5 Time: 8.30054\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x8d5c64a52fab02c9\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0x8d5c64a52fab02c9 Time: 9.83363\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 5.70213\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x93a1176336e5b9f6\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0x93a1176336e5b9f6 Time: 6.49661\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x994f5b723e2d80da\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0x994f5b723e2d80da Time: 6.37933\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x128x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xac49795b871b0d29\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0xac49795b871b0d29 Time: 5.70558\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb6717e61503d5e9b\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0xb6717e61503d5e9b Time: 5.80725\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:39] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 8.35925\n",
      "[10/18/2022-15:55:39] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 7.50245\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd1338f4b38d341e2\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0xd1338f4b38d341e2 Time: 6.38021\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x256x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd2aa21bfe2167c0c\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0xd2aa21bfe2167c0c Time: 5.79346\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xe40b38338a3a7d7e\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0xe40b38338a3a7d7e Time: 7.74055\n",
      "[10/18/2022-15:55:40] [V] [TRT] Fastest Tactic: 0x195431d38ba5af88 Time: 4.67178\n",
      "[10/18/2022-15:55:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:40] [V] [TRT] *************** Autotuning format combination: Float(100352,1,7168,512) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:55:40] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (CaskConvolution)\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x0447933cc2be855a\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x0447933cc2be855a Time: 5.50117\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 4.52848\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x0e2033f7517a807f\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x0e2033f7517a807f Time: 4.94222\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x2595402367cdee5c\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x2595402367cdee5c Time: 8.70488\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3eba442f4c9c4f50\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x3eba442f4c9c4f50 Time: 6.68474\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x43334a9c8840c773\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x43334a9c8840c773 Time: 5.93627\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 5.25045\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 4.78648\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 5.46909\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x66e3239eee98201e\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x66e3239eee98201e Time: 6.06644\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 5.15272\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 5.13601\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.85459\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x963db12d24e61b80\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0x963db12d24e61b80 Time: 5.9884\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb132670a7750e065\n",
      "[10/18/2022-15:55:40] [V] [TRT] Tactic: 0xb132670a7750e065 Time: 9.42724\n",
      "[10/18/2022-15:55:40] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca84742beb9f9767\n",
      "[10/18/2022-15:55:41] [V] [TRT] Tactic: 0xca84742beb9f9767 Time: 5.63548\n",
      "[10/18/2022-15:55:41] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xd2b62ec40baf8ee4\n",
      "[10/18/2022-15:55:41] [V] [TRT] Tactic: 0xd2b62ec40baf8ee4 Time: 5.06943\n",
      "[10/18/2022-15:55:41] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xde4165142218dab8\n",
      "[10/18/2022-15:55:41] [V] [TRT] Tactic: 0xde4165142218dab8 Time: 5.94008\n",
      "[10/18/2022-15:55:41] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:41] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 5.89555\n",
      "[10/18/2022-15:55:41] [V] [TRT] Fastest Tactic: 0x0bf55a7b77a6ff98 Time: 4.52848\n",
      "[10/18/2022-15:55:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:41] [V] [TRT] *************** Autotuning format combination: Half(100352,196,14,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:55:41] [W] [TRT] Weights [name=Conv_99 + Relu_100.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:41] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:41] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:41] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:41] [W] [TRT] Weights [name=Conv_99 + Relu_100.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:41] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:41] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:41] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (CudnnConvolution)\n",
      "[10/18/2022-15:55:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.49954\n",
      "[10/18/2022-15:55:41] [V] [TRT] Tactic: 0x0000000000000001 Time: 6.20628\n",
      "[10/18/2022-15:55:41] [V] [TRT] Tactic: 0x0000000000000002 Time: 9.21526\n",
      "[10/18/2022-15:55:41] [V] [TRT] Tactic: 0x0000000000000005 Time: 34.2484\n",
      "[10/18/2022-15:55:41] [V] [TRT] Tactic: 0x0000000000000038 Time: 8.31252\n",
      "[10/18/2022-15:55:41] [V] [TRT] Tactic: 0x000000000000003a Time: 8.4376\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x000000000000003d Time: 34.3704\n",
      "[10/18/2022-15:55:42] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 6.20628\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (CaskConvolution)\n",
      "[10/18/2022-15:55:42] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:55:42] [V] [TRT] *************** Autotuning format combination: Half(50176,196:2,14,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:42] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (CaskConvolution)\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x0fe4a9cce7ed878b\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x0fe4a9cce7ed878b Time: 2.23587\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.65542\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.72743\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 2.6074\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x4092cbc840fbea35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x4092cbc840fbea35 Time: 2.77717\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x446c8c788145836a Time: 3.48864\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 3.51627\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 2.64121\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.70227\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0x98a00f59a4b141f0\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x98a00f59a4b141f0 Time: 3.44795\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 3.64781\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.70836\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xcbe3f30275b04323\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0xcbe3f30275b04323 Time: 2.60212\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.33677\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xd7d66d5d03a72c4e\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0xd7d66d5d03a72c4e Time: 3.24725\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 3.399\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.83755\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xfc994367fd14b2d9\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0xfc994367fd14b2d9 Time: 2.64293\n",
      "[10/18/2022-15:55:42] [V] [TRT] Fastest Tactic: 0x0fe4a9cce7ed878b Time: 2.23587\n",
      "[10/18/2022-15:55:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0fe4a9cce7ed878b\n",
      "[10/18/2022-15:55:42] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,896,64) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (CaskConvolution)\n",
      "[10/18/2022-15:55:42] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:42] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,896,64) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:42] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [W] [TRT] Weights [name=Conv_99 + Relu_100.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:42] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:42] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:42] [V] [TRT] --------------- Timing Runner: Conv_99 + Relu_100 (CaskConvolution)\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x00a425145e84482b\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x00a425145e84482b Time: 1.21323\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x03512591e8ea2977\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x03512591e8ea2977 Time: 1.50054\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x0559d1d2893a8768\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x0559d1d2893a8768 Time: 2.08845\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x095000b22a78f234\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x095000b22a78f234 Time: 1.21417\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x0b906efbde4dc01a\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x0b906efbde4dc01a Time: 1.4775\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x0c0088d5808566d2\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x0c0088d5808566d2 Time: 0.901321\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x0caa5410b61e6cc5\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x0caa5410b61e6cc5 Time: 1.30555\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x0e0f7f10867063ba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x0e0f7f10867063ba Time: 1.0177\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x0e131ddbafdfe235\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x0e131ddbafdfe235 Time: 1.21183\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x0ecf8dc91198fd5e\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x0ecf8dc91198fd5e Time: 0.833275\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0x159236c6c22f62ce\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x159236c6c22f62ce Time: 1.21135\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x15ecbd82c22a023f\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x15ecbd82c22a023f Time: 0.823584\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x18ef97651ad5379a\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x18ef97651ad5379a Time: 1.81867\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x1981adfb6b6fd8b9\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x1981adfb6b6fd8b9 Time: 1.55383\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x1b099f7ac29a2a6a\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x1b099f7ac29a2a6a Time: 1.52753\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x1b9cb8d78519a728\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x1b9cb8d78519a728 Time: 1.92102\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0x1c23f4a19fbcb518\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x1c23f4a19fbcb518 Time: 0.937966\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.91136\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x1de724868edf11b0\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x1de724868edf11b0 Time: 1.15381\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:42] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.797618\n",
      "[10/18/2022-15:55:42] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x30150d05024bc911\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x30150d05024bc911 Time: 0.913829\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x32789ed2e6c7b43b\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x32789ed2e6c7b43b Time: 0.804311\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x33fc6102b341eb5d\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x33fc6102b341eb5d Time: 1.45057\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.41546\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x348653930e0a64e2\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x348653930e0a64e2 Time: 1.58716\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x350e898a5a20ad00\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x350e898a5a20ad00 Time: 0.771218\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x36662b4d547eefc7\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x36662b4d547eefc7 Time: 1.2307\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x490a097d77573bff\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x490a097d77573bff Time: 0.766482\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x4c6a6da741444412\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x4c6a6da741444412 Time: 0.833243\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x4e34a65090c3b86f\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x4e34a65090c3b86f Time: 0.709605\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x504f864880743a14\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x504f864880743a14 Time: 2.3157\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x5128cdf162fe56b6\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x5128cdf162fe56b6 Time: 1.26038\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.49976\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r3s3 Tactic: 0x5252dc6c9c5f3aff\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x5252dc6c9c5f3aff Time: 1.60966\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x54b287be85c1522c\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x54b287be85c1522c Time: 1.50068\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x55fb34a08663e5ae\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x55fb34a08663e5ae Time: 1.63021\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x56c66ffbce24b635\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x56c66ffbce24b635 Time: 1.4938\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x58eea09dffe038fd\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x58eea09dffe038fd Time: 1.27895\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x5bec1fbd955eb827\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x5bec1fbd955eb827 Time: 1.44992\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.44881\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x62bb371b230a886d\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x62bb371b230a886d Time: 1.70139\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.40709\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x69a5b2ac9c5bac16\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x69a5b2ac9c5bac16 Time: 1.23315\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6b44e6396887bed9\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x6b44e6396887bed9 Time: 1.2097\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x6cde8847e8cd796b\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x6cde8847e8cd796b Time: 1.31784\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x6cee4d9c86b4cdd5\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x6cee4d9c86b4cdd5 Time: 1.3289\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.36917\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x721049a39aae27ff\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x721049a39aae27ff Time: 2.18931\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.53015\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x75585ae3e9dedb93\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x75585ae3e9dedb93 Time: 1.80782\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x784dcede905d06c0\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x784dcede905d06c0 Time: 1.33077\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.976101\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x86903737887c556d\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x86903737887c556d Time: 1.27677\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x8781623566dac7f0\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x8781623566dac7f0 Time: 0.935986\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x8b86a8bb857fff79\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x8b86a8bb857fff79 Time: 1.83046\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x8d73ddfc444be692\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x8d73ddfc444be692 Time: 0.834999\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x9650edb797f919f3\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x9650edb797f919f3 Time: 0.875241\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0x969b1abbb567ac47\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x969b1abbb567ac47 Time: 1.42106\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0x9a0f43b4d1dc46d4\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0x9a0f43b4d1dc46d4 Time: 2.05768\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa13cdf70a9d99d45\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xa13cdf70a9d99d45 Time: 1.90839\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa2dad76f719680b5\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xa2dad76f719680b5 Time: 1.75972\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0xa3e778b253a14ca9\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xa3e778b253a14ca9 Time: 2.4538\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0xa5f0bcb42cb01fc7\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xa5f0bcb42cb01fc7 Time: 1.0447\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r3s3 Tactic: 0xa84824f86c61d2d8\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xa84824f86c61d2d8 Time: 1.02634\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.857202\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xab9c5449bde6902c\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xab9c5449bde6902c Time: 0.820448\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xac4736b5b00e1531\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xac4736b5b00e1531 Time: 1.31949\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.821472\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb0bf64026e546f4d\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xb0bf64026e546f4d Time: 0.913477\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xb26e93bd0702f504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xb26e93bd0702f504 Time: 1.90053\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xb307bc772518d3d7\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xb307bc772518d3d7 Time: 1.17354\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb7dc3705357cc965\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xb7dc3705357cc965 Time: 0.935982\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xbb3d6545e4864f26\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xbb3d6545e4864f26 Time: 0.819954\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xbfc71f913e286527\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xbfc71f913e286527 Time: 1.57257\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.44008\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xc684285f13ba11d0\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xc684285f13ba11d0 Time: 1.51318\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xc6e0905d983b4a62\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xc6e0905d983b4a62 Time: 1.96549\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0xc8ee1e4cdf0d8f84\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xc8ee1e4cdf0d8f84 Time: 1.23675\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xcb7b50f35a87094b\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xcb7b50f35a87094b Time: 1.73758\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd076fab92f5706c9\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xd076fab92f5706c9 Time: 1.13822\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd297ae2cdb8b1406\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xd297ae2cdb8b1406 Time: 1.22038\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.39878\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.714519\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xd825f95894186a22\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xd825f95894186a22 Time: 2.32203\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xd9d1d89fceeca81a\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xd9d1d89fceeca81a Time: 1.59977\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xdb70c5e9779254fb\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xdb70c5e9779254fb Time: 1.90903\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.798409\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xe84b9aaa289245c0\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xe84b9aaa289245c0 Time: 0.885138\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xe9fa7b19132889a8\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xe9fa7b19132889a8 Time: 1.60367\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0xf1d5fc0783e71536\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xf1d5fc0783e71536 Time: 1.48709\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xf368aae1fb20baa1\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xf368aae1fb20baa1 Time: 0.843223\n",
      "[10/18/2022-15:55:43] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:43] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.71051\n",
      "[10/18/2022-15:55:43] [V] [TRT] Fastest Tactic: 0x4e34a65090c3b86f Time: 0.709605\n",
      "[10/18/2022-15:55:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x4e34a65090c3b86f\n",
      "[10/18/2022-15:55:43] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:43] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:55:43] [V] [TRT] --------------- Timing Runner: Conv_101 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:43] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:43] [V] [TRT] --------------- Timing Runner: Conv_101 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:43] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:43] [V] [TRT] --------------- Timing Runner: Conv_101 (CudnnConvolution)\n",
      "[10/18/2022-15:55:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.24655\n",
      "[10/18/2022-15:55:44] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.46486\n",
      "[10/18/2022-15:55:44] [V] [TRT] Tactic: 0x0000000000000002 Time: 3.86385\n",
      "[10/18/2022-15:55:44] [V] [TRT] Tactic: 0x0000000000000004 Time: 46.9638\n",
      "[10/18/2022-15:55:44] [V] [TRT] Tactic: 0x0000000000000005 Time: 24.4284\n",
      "[10/18/2022-15:55:44] [V] [TRT] Tactic: 0x0000000000000038 Time: 3.67958\n",
      "[10/18/2022-15:55:44] [V] [TRT] Tactic: 0x0000000000000039 Time: 2.64543\n",
      "[10/18/2022-15:55:44] [V] [TRT] Tactic: 0x000000000000003a Time: 3.80515\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x000000000000003c Time: 47.5448\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x000000000000003d Time: 24.454\n",
      "[10/18/2022-15:55:45] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 2.46486\n",
      "[10/18/2022-15:55:45] [V] [TRT] --------------- Timing Runner: Conv_101 (CublasConvolution)\n",
      "[10/18/2022-15:55:45] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:45] [V] [TRT] --------------- Timing Runner: Conv_101 (CaskConvolution)\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 2.23985\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 2.18481\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 2.52912\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 2.38976\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 3.7753\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 2.63548\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 2.47512\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 2.59988\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 3.25105\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 3.31454\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 3.63892\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 3.03279\n",
      "[10/18/2022-15:55:45] [V] [TRT] Fastest Tactic: 0x195431d38ba5af88 Time: 2.18481\n",
      "[10/18/2022-15:55:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:45] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:55:45] [V] [TRT] --------------- Timing Runner: Conv_101 (CublasConvolution)\n",
      "[10/18/2022-15:55:45] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:45] [V] [TRT] --------------- Timing Runner: Conv_101 (CaskConvolution)\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 2.36801\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 2.33282\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 2.25705\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 2.79962\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 3.4702\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 2.45234\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 2.42747\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 2.4458\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 2.79829\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 2.16305\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 2.61228\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 2.45466\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:55:45] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 3.94153\n",
      "[10/18/2022-15:55:45] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:55:46] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 2.95553\n",
      "[10/18/2022-15:55:46] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:55:46] [V] [TRT] Tactic: 0xf92663d88255134b Time: 2.45877\n",
      "[10/18/2022-15:55:46] [V] [TRT] Conv_101 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:46] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 3.09688\n",
      "[10/18/2022-15:55:46] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:55:46] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 2.80393\n",
      "[10/18/2022-15:55:46] [V] [TRT] Fastest Tactic: 0x946eca69f99ddcb4 Time: 2.16305\n",
      "[10/18/2022-15:55:46] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:46] [V] [TRT] *************** Autotuning format combination: Half(25088,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:55:46] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:46] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:46] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:46] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:46] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:46] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:46] [V] [TRT] --------------- Timing Runner: Conv_101 (CudnnConvolution)\n",
      "[10/18/2022-15:55:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.53823\n",
      "[10/18/2022-15:55:46] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.19227\n",
      "[10/18/2022-15:55:46] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.10683\n",
      "[10/18/2022-15:55:46] [V] [TRT] Tactic: 0x0000000000000004 Time: 45.8569\n",
      "[10/18/2022-15:55:46] [V] [TRT] Tactic: 0x0000000000000005 Time: 23.4745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:46] [V] [TRT] Tactic: 0x0000000000000038 Time: 4.06736\n",
      "[10/18/2022-15:55:46] [V] [TRT] Tactic: 0x000000000000003a Time: 4.09211\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x000000000000003c Time: 46.719\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x000000000000003d Time: 23.4797\n",
      "[10/18/2022-15:55:47] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 2.19227\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [V] [TRT] --------------- Timing Runner: Conv_101 (CublasConvolution)\n",
      "[10/18/2022-15:55:47] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [V] [TRT] --------------- Timing Runner: Conv_101 (CaskConvolution)\n",
      "[10/18/2022-15:55:47] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:55:47] [V] [TRT] *************** Autotuning format combination: Half(12544,49:2,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [V] [TRT] --------------- Timing Runner: Conv_101 (CaskConvolution)\n",
      "[10/18/2022-15:55:47] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:47] [V] [TRT] *************** Autotuning format combination: Half(12544,49:2,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [V] [TRT] --------------- Timing Runner: Conv_101 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:47] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [V] [TRT] --------------- Timing Runner: Conv_101 (CublasConvolution)\n",
      "[10/18/2022-15:55:47] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [V] [TRT] --------------- Timing Runner: Conv_101 (CaskConvolution)\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 0.985925\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 0.969929\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 0.961975\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x446c8c788145836a Time: 1.25016\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 1.29374\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 1.18699\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 1.36699\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x97afba3735828021 Time: 1.3745\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 1.2213\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 1.35224\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 1.28701\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 1.28029\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 1.19955\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 1.26997\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 1.16885\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 1.15024\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 1.26015\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 1.27056\n",
      "[10/18/2022-15:55:47] [V] [TRT] Fastest Tactic: 0x3bee4a098b4f8914 Time: 0.961975\n",
      "[10/18/2022-15:55:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:47] [V] [TRT] *************** Autotuning format combination: Half(3136,1:8,448,64) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [V] [TRT] --------------- Timing Runner: Conv_101 (CublasConvolution)\n",
      "[10/18/2022-15:55:47] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [V] [TRT] --------------- Timing Runner: Conv_101 (CaskConvolution)\n",
      "[10/18/2022-15:55:47] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:47] [V] [TRT] *************** Autotuning format combination: Half(3136,1:8,448,64) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [V] [TRT] --------------- Timing Runner: Conv_101 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:47] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [V] [TRT] --------------- Timing Runner: Conv_101 (CublasConvolution)\n",
      "[10/18/2022-15:55:47] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:47] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:47] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:47] [V] [TRT] --------------- Timing Runner: Conv_101 (CaskConvolution)\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 0.76219\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 0.70245\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.444942\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 0.659131\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.397038\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.389111\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 0.517536\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x249110624ee04937 Time: 0.504023\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 0.605038\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 0.564727\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.361033\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 0.562615\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 0.553545\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 0.752786\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 0.840722\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 0.547776\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 0.449934\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 0.829778\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 0.726002\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 0.724992\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 0.901358\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 0.637669\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 0.712096\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 0.695163\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 0.575433\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 0.722651\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 0.68197\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 0.708032\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 0.426295\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 0.428878\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 0.559817\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 0.686725\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 0.696736\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 1.03453\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.455534\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:55:47] [V] [TRT] Tactic: 0x83b35618df65874c Time: 0.900325\n",
      "[10/18/2022-15:55:47] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 0.590007\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 0.623109\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 0.676686\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.446171\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 0.820663\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 0.630222\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 0.642478\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 1.05886\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 0.650085\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 0.532453\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.497678\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.485349\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 0.710395\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.377294\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 0.617591\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 0.534821\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.391744\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 0.460215\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 0.650985\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.391424\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.466624\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 0.754057\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.371895\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 0.813358\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 1.08398\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 0.395264\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.420654\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 0.729458\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 0.71056\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 0.674021\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 0.588069\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 0.59509\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.412864\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 1.03922\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 0.451685\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.370619\n",
      "[10/18/2022-15:55:48] [V] [TRT] Fastest Tactic: 0x2a3615ad33745f0b Time: 0.361033\n",
      "[10/18/2022-15:55:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:48] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:48] [V] [TRT] *************** Autotuning format combination: Float(200704,196,14,1), Float(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:55:48] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:48] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:48] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:48] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:48] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (CudnnConvolution)\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.08115\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x0000000000000001 Time: 8.37066\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x0000000000000002 Time: 7.75753\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.49116\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x0000000000000039 Time: 8.36933\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x000000000000003a Time: 7.70193\n",
      "[10/18/2022-15:55:48] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 7.08115\n",
      "[10/18/2022-15:55:48] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (CaskConvolution)\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 4.78135\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.98304\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 7.81098\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 5.74884\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 5.34056\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:55:48] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 5.7347\n",
      "[10/18/2022-15:55:48] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x8d5c64a52fab02c9\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0x8d5c64a52fab02c9 Time: 9.54836\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 6.09398\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x994f5b723e2d80da\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0x994f5b723e2d80da Time: 6.62615\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 8.43859\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x128x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xac49795b871b0d29\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0xac49795b871b0d29 Time: 6.45818\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 8.19514\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 8.6724\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd1338f4b38d341e2\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0xd1338f4b38d341e2 Time: 6.68697\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x256x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd2aa21bfe2167c0c\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0xd2aa21bfe2167c0c Time: 5.33178\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xe40b38338a3a7d7e\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0xe40b38338a3a7d7e Time: 8.2745\n",
      "[10/18/2022-15:55:49] [V] [TRT] Fastest Tactic: 0x18597bd4a7d0164d Time: 4.78135\n",
      "[10/18/2022-15:55:49] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:49] [V] [TRT] *************** Autotuning format combination: Float(200704,1,14336,1024), Float(100352,1,14336,2048) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:55:49] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (CaskConvolution)\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x0447933cc2be855a\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0x0447933cc2be855a Time: 5.50397\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 4.36458\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x2595402367cdee5c\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0x2595402367cdee5c Time: 8.14548\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 5.43361\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 4.95764\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:55:49] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 5.67531\n",
      "[10/18/2022-15:55:49] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 7.84988\n",
      "[10/18/2022-15:55:50] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 5.35701\n",
      "[10/18/2022-15:55:50] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 4.59371\n",
      "[10/18/2022-15:55:50] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 4.6862\n",
      "[10/18/2022-15:55:50] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 5.61478\n",
      "[10/18/2022-15:55:50] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.48155\n",
      "[10/18/2022-15:55:50] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x963db12d24e61b80\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x963db12d24e61b80 Time: 5.83743\n",
      "[10/18/2022-15:55:50] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 6.47973\n",
      "[10/18/2022-15:55:50] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 5.30523\n",
      "[10/18/2022-15:55:50] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xde4165142218dab8\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0xde4165142218dab8 Time: 5.87176\n",
      "[10/18/2022-15:55:50] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0xf92663d88255134b Time: 5.33772\n",
      "[10/18/2022-15:55:50] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 5.88624\n",
      "[10/18/2022-15:55:50] [V] [TRT] Fastest Tactic: 0x0bf55a7b77a6ff98 Time: 4.36458\n",
      "[10/18/2022-15:55:50] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:50] [V] [TRT] *************** Autotuning format combination: Half(200704,196,14,1), Half(100352,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:55:50] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:50] [W] [TRT]  - Subnormal FP16 values detected. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:50] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:50] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:50] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:50] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:50] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (CudnnConvolution)\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.9974\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x0000000000000001 Time: 14.9032\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x0000000000000002 Time: 7.92431\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x0000000000000038 Time: 8.72035\n",
      "[10/18/2022-15:55:50] [V] [TRT] Tactic: 0x000000000000003a Time: 8.24198\n",
      "[10/18/2022-15:55:50] [V] [TRT] Fastest Tactic: 0x0000000000000002 Time: 7.92431\n",
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (CaskConvolution)\n",
      "[10/18/2022-15:55:51] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000002\n",
      "[10/18/2022-15:55:51] [V] [TRT] *************** Autotuning format combination: Half(100352,196:2,14,1), Half(50176,49:2,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:51] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (CaskConvolution)\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.10037\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.54112\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 2.83034\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x446c8c788145836a Time: 3.56761\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 3.47429\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 3.03045\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.70453\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x97afba3735828021 Time: 3.35056\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 2.64067\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 3.63537\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.632\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 3.4064\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 2.74203\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.79472\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 2.54459\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 2.35117\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 3.20508\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.73083\n",
      "[10/18/2022-15:55:51] [V] [TRT] Fastest Tactic: 0x16eafdbc5869b184 Time: 2.10037\n",
      "[10/18/2022-15:55:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:51] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128), Float(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (CaskConvolution)\n",
      "[10/18/2022-15:55:51] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:51] [V] [TRT] *************** Autotuning format combination: Half(25088,1:8,1792,128), Half(12544,1:8,1792,256) -> Half(12544,1:8,1792,256) ***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:51] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:51] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:51] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:51] [V] [TRT] --------------- Timing Runner: Conv_102 + Add_103 + Relu_104 (CaskConvolution)\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x0559d1d2893a8768\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x0559d1d2893a8768 Time: 1.99621\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x0b906efbde4dc01a\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x0b906efbde4dc01a Time: 1.34301\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x0e0f7f10867063ba\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x0e0f7f10867063ba Time: 1.03141\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x0ecf8dc91198fd5e\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x0ecf8dc91198fd5e Time: 0.951237\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.898597\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0x159236c6c22f62ce\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x159236c6c22f62ce Time: 1.33226\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x15ecbd82c22a023f\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x15ecbd82c22a023f Time: 0.966007\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x18ef97651ad5379a\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x18ef97651ad5379a Time: 1.88589\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x1b099f7ac29a2a6a\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x1b099f7ac29a2a6a Time: 1.74921\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x1b9cb8d78519a728\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x1b9cb8d78519a728 Time: 2.09017\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0x1c23f4a19fbcb518\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x1c23f4a19fbcb518 Time: 1.11159\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 1.55853\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.928914\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.761344\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 1.02463\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 1.14978\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.751616\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x33fc6102b341eb5d\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x33fc6102b341eb5d Time: 1.2962\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.53498\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x348653930e0a64e2\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x348653930e0a64e2 Time: 1.56069\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x36662b4d547eefc7\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x36662b4d547eefc7 Time: 1.15883\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 1.34794\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 1.03817\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 2.01998\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 1.7174\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 1.66768\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x5128cdf162fe56b6\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x5128cdf162fe56b6 Time: 1.27586\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 1.98685\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:51] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.31641\n",
      "[10/18/2022-15:55:51] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 1.56782\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x54b287be85c1522c\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x54b287be85c1522c Time: 1.4035\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x55fb34a08663e5ae\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x55fb34a08663e5ae Time: 1.91517\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x58eea09dffe038fd\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x58eea09dffe038fd Time: 1.50269\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x5bec1fbd955eb827\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x5bec1fbd955eb827 Time: 1.45865\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.61741\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x62bb371b230a886d\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x62bb371b230a886d Time: 1.76383\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.60241\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x69a5b2ac9c5bac16\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x69a5b2ac9c5bac16 Time: 1.42307\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6b44e6396887bed9\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x6b44e6396887bed9 Time: 1.40332\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x6cde8847e8cd796b\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x6cde8847e8cd796b Time: 1.27675\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.49\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.57722\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x75585ae3e9dedb93\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x75585ae3e9dedb93 Time: 1.85196\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x784dcede905d06c0\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x784dcede905d06c0 Time: 1.42034\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.977481\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 1.26379\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 1.19545\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 1.33793\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x86903737887c556d\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x86903737887c556d Time: 1.28053\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.874217\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 1.7219\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 1.24487\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x8b86a8bb857fff79\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x8b86a8bb857fff79 Time: 1.74837\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 1.36768\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x8d73ddfc444be692\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x8d73ddfc444be692 Time: 0.934473\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 1.34366\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x9650edb797f919f3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x9650edb797f919f3 Time: 0.919173\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0x969b1abbb567ac47\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x969b1abbb567ac47 Time: 1.27278\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 1.38056\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0x9a0f43b4d1dc46d4\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x9a0f43b4d1dc46d4 Time: 2.09599\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa13cdf70a9d99d45\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xa13cdf70a9d99d45 Time: 1.97361\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa2dad76f719680b5\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xa2dad76f719680b5 Time: 1.8026\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0xa3e778b253a14ca9\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xa3e778b253a14ca9 Time: 2.48805\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0xa5f0bcb42cb01fc7\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xa5f0bcb42cb01fc7 Time: 1.17936\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 1.04711\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xab9c5449bde6902c\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xab9c5449bde6902c Time: 0.888247\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.989509\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb0bf64026e546f4d\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xb0bf64026e546f4d Time: 1.14462\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xb26e93bd0702f504\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xb26e93bd0702f504 Time: 1.88416\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb7dc3705357cc965\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xb7dc3705357cc965 Time: 1.12996\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.984078\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 1.10987\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xbfc71f913e286527\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xbfc71f913e286527 Time: 1.68638\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.37889\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.845755\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xc6e0905d983b4a62\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xc6e0905d983b4a62 Time: 2.11441\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.904841\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0xc8ee1e4cdf0d8f84\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xc8ee1e4cdf0d8f84 Time: 1.29872\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd076fab92f5706c9\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xd076fab92f5706c9 Time: 1.40052\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd297ae2cdb8b1406\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xd297ae2cdb8b1406 Time: 1.34669\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.64052\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.804425\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xd825f95894186a22\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xd825f95894186a22 Time: 2.33825\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 1.89037\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 2.58394\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.854309\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 1.64074\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 1.4527\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 1.50613\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xe84b9aaa289245c0\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xe84b9aaa289245c0 Time: 0.964315\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xe9fa7b19132889a8\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xe9fa7b19132889a8 Time: 1.64068\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.804978\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0xf1d5fc0783e71536\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xf1d5fc0783e71536 Time: 1.33965\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xf368aae1fb20baa1\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xf368aae1fb20baa1 Time: 0.922528\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 2.09275\n",
      "[10/18/2022-15:55:52] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.749979\n",
      "[10/18/2022-15:55:52] [V] [TRT] Fastest Tactic: 0xfcd06da0f3c31fd1 Time: 0.749979\n",
      "[10/18/2022-15:55:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:52] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:52] [V] [TRT] *************** Autotuning format combination: Float(100352,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:55:52] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:52] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:52] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:52] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:52] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CudnnConvolution)\n",
      "[10/18/2022-15:55:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.67345\n",
      "[10/18/2022-15:55:53] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.44894\n",
      "[10/18/2022-15:55:53] [V] [TRT] Tactic: 0x0000000000000002 Time: 3.68633\n",
      "[10/18/2022-15:55:53] [V] [TRT] Tactic: 0x0000000000000004 Time: 42.6525\n",
      "[10/18/2022-15:55:53] [V] [TRT] Tactic: 0x0000000000000005 Time: 25.811\n",
      "[10/18/2022-15:55:53] [V] [TRT] Tactic: 0x0000000000000038 Time: 3.34321\n",
      "[10/18/2022-15:55:53] [V] [TRT] Tactic: 0x0000000000000039 Time: 2.4693\n",
      "[10/18/2022-15:55:53] [V] [TRT] Tactic: 0x000000000000003a Time: 3.7172\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x000000000000003c Time: 41.9719\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x000000000000003d Time: 25.7856\n",
      "[10/18/2022-15:55:54] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 2.44894\n",
      "[10/18/2022-15:55:54] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CublasConvolution)\n",
      "[10/18/2022-15:55:54] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:54] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CaskConvolution)\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 2.29525\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 2.11504\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 2.56523\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 2.15456\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 3.25567\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 2.37888\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 2.24189\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 2.3695\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 2.64631\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 3.24988\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 3.20775\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 2.76386\n",
      "[10/18/2022-15:55:54] [V] [TRT] Fastest Tactic: 0x195431d38ba5af88 Time: 2.11504\n",
      "[10/18/2022-15:55:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:54] [V] [TRT] *************** Autotuning format combination: Float(100352,1,14336,2048) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:55:54] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CublasConvolution)\n",
      "[10/18/2022-15:55:54] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:54] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CaskConvolution)\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 2.44818\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 2.42743\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 2.28218\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 2.56731\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 3.50214\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 2.22732\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 2.25165\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 2.36158\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 2.52032\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 2.25469\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 2.70667\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 2.24903\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 3.5739\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 2.76704\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0xf92663d88255134b Time: 2.27818\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 2.6154\n",
      "[10/18/2022-15:55:54] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:55:54] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 2.72962\n",
      "[10/18/2022-15:55:54] [V] [TRT] Fastest Tactic: 0x810bd80d0531c0a0 Time: 2.22732\n",
      "[10/18/2022-15:55:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:54] [V] [TRT] *************** Autotuning format combination: Half(100352,49,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:55:54] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:54] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:54] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:54] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:54] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CudnnConvolution)\n",
      "[10/18/2022-15:55:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.30252\n",
      "[10/18/2022-15:55:55] [V] [TRT] Tactic: 0x0000000000000001 Time: 1.94046\n",
      "[10/18/2022-15:55:55] [V] [TRT] Tactic: 0x0000000000000002 Time: 3.87767\n",
      "[10/18/2022-15:55:55] [V] [TRT] Tactic: 0x0000000000000004 Time: 40.7649\n",
      "[10/18/2022-15:55:55] [V] [TRT] Tactic: 0x0000000000000005 Time: 24.667\n",
      "[10/18/2022-15:55:55] [V] [TRT] Tactic: 0x0000000000000038 Time: 3.96844\n",
      "[10/18/2022-15:55:55] [V] [TRT] Tactic: 0x000000000000003a Time: 4.02834\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x000000000000003c Time: 40.9226\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x000000000000003d Time: 24.8095\n",
      "[10/18/2022-15:55:56] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 1.94046\n",
      "[10/18/2022-15:55:56] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CublasConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:56] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CaskConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:55:56] [V] [TRT] *************** Autotuning format combination: Half(50176,49:2,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:55:56] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CaskConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:56] [V] [TRT] *************** Autotuning format combination: Half(50176,49:2,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:55:56] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:56] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CublasConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:56] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CaskConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 0.967365\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 0.974313\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 1.05939\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x446c8c788145836a Time: 1.27174\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 1.33968\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 1.24049\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 1.2313\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x97afba3735828021 Time: 1.35089\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 1.18544\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 1.37907\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 1.16677\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 1.25624\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 1.11716\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 1.16961\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 1.13957\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 1.10278\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 1.30032\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 1.08462\n",
      "[10/18/2022-15:55:56] [V] [TRT] Fastest Tactic: 0x16eafdbc5869b184 Time: 0.967365\n",
      "[10/18/2022-15:55:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:55:56] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,1792,256) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:55:56] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CublasConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:56] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CaskConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:56] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,1792,256) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:55:56] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:56] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CublasConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:56] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:55:56] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:55:56] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_105 + Relu_106 (CaskConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 0.719141\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 0.638962\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.324462\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 0.576178\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.374075\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.344713\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 0.411712\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x249110624ee04937 Time: 0.501769\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 0.63488\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 0.550034\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.33088\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 0.549006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 0.444617\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 0.701399\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 0.819849\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 0.533358\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 0.413737\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 0.803365\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 0.713486\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 0.674112\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 0.88549\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 0.681755\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 0.721189\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 0.616823\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 0.573225\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 0.666459\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 0.606469\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 0.678555\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 0.425239\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 0.434094\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 0.540283\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 0.677943\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 0.69749\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 0.95349\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.421303\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x83b35618df65874c Time: 0.865189\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 0.556137\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 0.540379\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 0.55925\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.432014\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 0.802656\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 0.646085\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 0.654222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 1.03193\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 0.641902\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 0.52571\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.37403\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.36101\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 0.689856\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.341138\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 0.63611\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 0.524288\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.396165\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 0.434537\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 0.689993\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.360183\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.447968\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 0.663525\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.337627\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 0.757481\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 1.07552\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 0.402267\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.375077\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 0.700814\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 0.597134\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 0.559136\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 0.555817\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 0.564453\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.368178\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 0.963328\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 0.435881\n",
      "[10/18/2022-15:55:56] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:55:56] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.331259\n",
      "[10/18/2022-15:55:56] [V] [TRT] Fastest Tactic: 0x105f56cf03ee5549 Time: 0.324462\n",
      "[10/18/2022-15:55:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:55:56] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:55:56] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:55:56] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (FusedConvActConvolution)\n",
      "[10/18/2022-15:55:56] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:57] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (CudnnConvolution)\n",
      "[10/18/2022-15:55:57] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.43062\n",
      "[10/18/2022-15:55:57] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.32682\n",
      "[10/18/2022-15:55:57] [V] [TRT] Tactic: 0x0000000000000002 Time: 6.90637\n",
      "[10/18/2022-15:55:57] [V] [TRT] Tactic: 0x0000000000000004 Time: 10.6276\n",
      "[10/18/2022-15:55:57] [V] [TRT] Tactic: 0x0000000000000005 Time: 34.0571\n",
      "[10/18/2022-15:55:57] [V] [TRT] Tactic: 0x0000000000000006 Time: 8.43319\n",
      "[10/18/2022-15:55:57] [V] [TRT] Tactic: 0x0000000000000038 Time: 6.31517\n",
      "[10/18/2022-15:55:57] [V] [TRT] Tactic: 0x0000000000000039 Time: 5.13825\n",
      "[10/18/2022-15:55:57] [V] [TRT] Tactic: 0x000000000000003a Time: 7.07028\n",
      "[10/18/2022-15:55:57] [V] [TRT] Tactic: 0x000000000000003c Time: 10.322\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x000000000000003d Time: 34.249\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x000000000000003e Time: 8.53308\n",
      "[10/18/2022-15:55:58] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 5.13825\n",
      "[10/18/2022-15:55:58] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (CaskConvolution)\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 4.86443\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0x268494f0a1c83de3\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x268494f0a1c83de3 Time: 7.90128\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x27728c886a448c5a\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x27728c886a448c5a Time: 5.1559\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 4.83972\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x597d29027694c20b\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x597d29027694c20b Time: 5.13559\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 4.7179\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x62b2ffd9a5c0cfb5\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x62b2ffd9a5c0cfb5 Time: 7.42928\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x8d5c64a52fab02c9\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x8d5c64a52fab02c9 Time: 9.14461\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 5.48193\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x93a1176336e5b9f6\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x93a1176336e5b9f6 Time: 6.00965\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x994f5b723e2d80da\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0x994f5b723e2d80da Time: 5.82972\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x128x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xac49795b871b0d29\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0xac49795b871b0d29 Time: 6.23467\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb6717e61503d5e9b\n",
      "[10/18/2022-15:55:58] [V] [TRT] Tactic: 0xb6717e61503d5e9b Time: 5.45721\n",
      "[10/18/2022-15:55:58] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 6.8134\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 6.04835\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd1338f4b38d341e2\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0xd1338f4b38d341e2 Time: 6.27481\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x256x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd2aa21bfe2167c0c\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0xd2aa21bfe2167c0c Time: 5.97423\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xe40b38338a3a7d7e\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0xe40b38338a3a7d7e Time: 6.93014\n",
      "[10/18/2022-15:55:59] [V] [TRT] Fastest Tactic: 0x5e7d1125e7896624 Time: 4.7179\n",
      "[10/18/2022-15:55:59] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:55:59] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:55:59] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (CaskConvolution)\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x0447933cc2be855a\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x0447933cc2be855a Time: 5.74667\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 4.36602\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x0e2033f7517a807f\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x0e2033f7517a807f Time: 4.6709\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x2595402367cdee5c\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x2595402367cdee5c Time: 7.86663\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x3eba442f4c9c4f50\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x3eba442f4c9c4f50 Time: 5.71333\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x43334a9c8840c773\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x43334a9c8840c773 Time: 5.52549\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 4.99065\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 4.54063\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 5.09564\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0x66e3239eee98201e\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x66e3239eee98201e Time: 5.63931\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 4.83094\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 4.88038\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 4.51581\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x963db12d24e61b80\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0x963db12d24e61b80 Time: 5.47502\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r3s3_aligna4_alignc4 Tactic: 0xb132670a7750e065\n",
      "[10/18/2022-15:55:59] [V] [TRT] Tactic: 0xb132670a7750e065 Time: 8.18637\n",
      "[10/18/2022-15:55:59] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca84742beb9f9767\n",
      "[10/18/2022-15:56:00] [V] [TRT] Tactic: 0xca84742beb9f9767 Time: 5.10371\n",
      "[10/18/2022-15:56:00] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xd2b62ec40baf8ee4\n",
      "[10/18/2022-15:56:00] [V] [TRT] Tactic: 0xd2b62ec40baf8ee4 Time: 4.86304\n",
      "[10/18/2022-15:56:00] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xde4165142218dab8\n",
      "[10/18/2022-15:56:00] [V] [TRT] Tactic: 0xde4165142218dab8 Time: 5.83559\n",
      "[10/18/2022-15:56:00] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:56:00] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 5.42843\n",
      "[10/18/2022-15:56:00] [V] [TRT] Fastest Tactic: 0x0bf55a7b77a6ff98 Time: 4.36602\n",
      "[10/18/2022-15:56:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:56:00] [V] [TRT] *************** Autotuning format combination: Half(25088,49,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:56:00] [W] [TRT] Weights [name=Conv_107 + Relu_108.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:00] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:00] [W] [TRT] Weights [name=Conv_107 + Relu_108.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:00] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:00] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:00] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (CudnnConvolution)\n",
      "[10/18/2022-15:56:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.05273\n",
      "[10/18/2022-15:56:00] [V] [TRT] Tactic: 0x0000000000000001 Time: 5.34146\n",
      "[10/18/2022-15:56:00] [V] [TRT] Tactic: 0x0000000000000002 Time: 8.72361\n",
      "[10/18/2022-15:56:00] [V] [TRT] Tactic: 0x0000000000000004 Time: 10.5603\n",
      "[10/18/2022-15:56:00] [V] [TRT] Tactic: 0x0000000000000005 Time: 33.9184\n",
      "[10/18/2022-15:56:00] [V] [TRT] Tactic: 0x0000000000000006 Time: 8.55251\n",
      "[10/18/2022-15:56:00] [V] [TRT] Tactic: 0x0000000000000038 Time: 7.82893\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x000000000000003a Time: 8.2902\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x000000000000003c Time: 10.2038\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x000000000000003d Time: 33.8463\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x000000000000003e Time: 8.56269\n",
      "[10/18/2022-15:56:01] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 5.34146\n",
      "[10/18/2022-15:56:01] [W] [TRT] Weights [name=Conv_107 + Relu_108.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:01] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:01] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:01] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:01] [W] [TRT] Weights [name=Conv_107 + Relu_108.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:01] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:01] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:01] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (CaskConvolution)\n",
      "[10/18/2022-15:56:01] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:56:01] [V] [TRT] *************** Autotuning format combination: Half(12544,49:2,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:56:01] [W] [TRT] Weights [name=Conv_107 + Relu_108.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:01] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:01] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:01] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:01] [W] [TRT] Weights [name=Conv_107 + Relu_108.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:01] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:01] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:01] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (FusedConvActConvolution)\n",
      "[10/18/2022-15:56:01] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:01] [W] [TRT] Weights [name=Conv_107 + Relu_108.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:01] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:01] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:01] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:01] [W] [TRT] Weights [name=Conv_107 + Relu_108.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:01] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:01] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:01] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (CaskConvolution)\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x0fe4a9cce7ed878b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x0fe4a9cce7ed878b Time: 2.24375\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 2.3728\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 2.73554\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 2.55152\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_large_nn_v1 Tactic: 0x4092cbc840fbea35\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x4092cbc840fbea35 Time: 2.53595\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x446c8c788145836a Time: 3.38207\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 3.12487\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 2.49543\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 2.39236\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0x98a00f59a4b141f0\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0x98a00f59a4b141f0 Time: 3.14873\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 2.85189\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 2.3382\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xcbe3f30275b04323\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0xcbe3f30275b04323 Time: 2.56188\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 2.57172\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_fp16x2_hcudnn_winograd_fp16x2_128x128_ldg1_ldg4_relu_tile148t_nt_v1 Tactic: 0xd46b3ee2b59f893c\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0xd46b3ee2b59f893c Time: 3.69366\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_large_nn_v1 Tactic: 0xd7d66d5d03a72c4e\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0xd7d66d5d03a72c4e Time: 3.59424\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 2.84379\n",
      "[10/18/2022-15:56:01] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:56:01] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 2.68675\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_large_nn_v1 Tactic: 0xfc994367fd14b2d9\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xfc994367fd14b2d9 Time: 2.54712\n",
      "[10/18/2022-15:56:02] [V] [TRT] Fastest Tactic: 0x0fe4a9cce7ed878b Time: 2.24375\n",
      "[10/18/2022-15:56:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0fe4a9cce7ed878b\n",
      "[10/18/2022-15:56:02] [V] [TRT] *************** Autotuning format combination: Half(3136,1:8,448,64) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:56:02] [W] [TRT] Weights [name=Conv_107 + Relu_108.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:02] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:02] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:02] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:02] [W] [TRT] Weights [name=Conv_107 + Relu_108.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:02] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:02] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:02] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (CaskConvolution)\n",
      "[10/18/2022-15:56:02] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:02] [V] [TRT] *************** Autotuning format combination: Half(3136,1:8,448,64) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:56:02] [W] [TRT] Weights [name=Conv_107 + Relu_108.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:02] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:02] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:02] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:02] [W] [TRT] Weights [name=Conv_107 + Relu_108.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:02] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:02] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:02] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:56:02] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:02] [W] [TRT] Weights [name=Conv_107 + Relu_108.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:02] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:02] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:02] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:02] [W] [TRT] Weights [name=Conv_107 + Relu_108.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:02] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:02] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:02] [V] [TRT] --------------- Timing Runner: Conv_107 + Relu_108 (CaskConvolution)\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x00a425145e84482b\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x00a425145e84482b Time: 1.21897\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x03512591e8ea2977\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x03512591e8ea2977 Time: 1.48217\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x0559d1d2893a8768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x0559d1d2893a8768 Time: 2.11204\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x095000b22a78f234\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x095000b22a78f234 Time: 1.14406\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x0b906efbde4dc01a\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x0b906efbde4dc01a Time: 1.36441\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x0c0088d5808566d2\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x0c0088d5808566d2 Time: 0.839095\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x0caa5410b61e6cc5\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x0caa5410b61e6cc5 Time: 1.18407\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x0e0f7f10867063ba\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x0e0f7f10867063ba Time: 0.887401\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x0e131ddbafdfe235\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x0e131ddbafdfe235 Time: 1.12888\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x0ecf8dc91198fd5e\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x0ecf8dc91198fd5e Time: 0.749253\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0x159236c6c22f62ce\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x159236c6c22f62ce Time: 1.13764\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x15ecbd82c22a023f\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x15ecbd82c22a023f Time: 0.711241\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x18ef97651ad5379a\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x18ef97651ad5379a Time: 1.59855\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x1981adfb6b6fd8b9\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x1981adfb6b6fd8b9 Time: 1.50009\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x1b099f7ac29a2a6a\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x1b099f7ac29a2a6a Time: 1.45077\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x1b9cb8d78519a728\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x1b9cb8d78519a728 Time: 1.80661\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0x1c23f4a19fbcb518\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x1c23f4a19fbcb518 Time: 0.880055\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.871863\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x1de724868edf11b0\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x1de724868edf11b0 Time: 1.08485\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.75659\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x30150d05024bc911\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x30150d05024bc911 Time: 0.811017\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x32789ed2e6c7b43b\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x32789ed2e6c7b43b Time: 0.730135\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x33fc6102b341eb5d\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x33fc6102b341eb5d Time: 1.46548\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 1.3865\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x348653930e0a64e2\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x348653930e0a64e2 Time: 1.5553\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x350e898a5a20ad00\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x350e898a5a20ad00 Time: 0.774437\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x36662b4d547eefc7\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x36662b4d547eefc7 Time: 1.20553\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x490a097d77573bff\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x490a097d77573bff Time: 0.699246\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x4c6a6da741444412\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x4c6a6da741444412 Time: 0.804718\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x4e34a65090c3b86f\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x4e34a65090c3b86f Time: 0.692517\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0x504f864880743a14\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x504f864880743a14 Time: 2.2616\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x5128cdf162fe56b6\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x5128cdf162fe56b6 Time: 1.21514\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 1.45467\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r3s3 Tactic: 0x5252dc6c9c5f3aff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x5252dc6c9c5f3aff Time: 1.51874\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4 Tactic: 0x54b287be85c1522c\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x54b287be85c1522c Time: 1.38525\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x55fb34a08663e5ae\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x55fb34a08663e5ae Time: 1.47127\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x56c66ffbce24b635\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x56c66ffbce24b635 Time: 1.35902\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x58eea09dffe038fd\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x58eea09dffe038fd Time: 1.1818\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x5bec1fbd955eb827\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x5bec1fbd955eb827 Time: 1.35507\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 1.40676\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x62bb371b230a886d\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x62bb371b230a886d Time: 1.6384\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 1.35245\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x69a5b2ac9c5bac16\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x69a5b2ac9c5bac16 Time: 1.16874\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6b44e6396887bed9\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x6b44e6396887bed9 Time: 1.13644\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x6cde8847e8cd796b\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x6cde8847e8cd796b Time: 1.20712\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0x6cee4d9c86b4cdd5\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x6cee4d9c86b4cdd5 Time: 1.2376\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 1.3429\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r3s3 Tactic: 0x721049a39aae27ff\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x721049a39aae27ff Time: 2.0501\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 1.36509\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8 Tactic: 0x75585ae3e9dedb93\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x75585ae3e9dedb93 Time: 1.62323\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x784dcede905d06c0\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x784dcede905d06c0 Time: 1.34398\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.949783\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0x86903737887c556d\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x86903737887c556d Time: 1.2704\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x8781623566dac7f0\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x8781623566dac7f0 Time: 0.924704\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0x8b86a8bb857fff79\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x8b86a8bb857fff79 Time: 1.79541\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x8d73ddfc444be692\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x8d73ddfc444be692 Time: 0.835899\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x9650edb797f919f3\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x9650edb797f919f3 Time: 0.856046\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0x969b1abbb567ac47\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x969b1abbb567ac47 Time: 1.39543\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0x9a0f43b4d1dc46d4\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0x9a0f43b4d1dc46d4 Time: 1.94862\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa13cdf70a9d99d45\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xa13cdf70a9d99d45 Time: 1.73319\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xa2dad76f719680b5\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xa2dad76f719680b5 Time: 1.50465\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4 Tactic: 0xa3e778b253a14ca9\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xa3e778b253a14ca9 Time: 2.27972\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8 Tactic: 0xa5f0bcb42cb01fc7\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xa5f0bcb42cb01fc7 Time: 0.933445\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r3s3 Tactic: 0xa84824f86c61d2d8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xa84824f86c61d2d8 Time: 0.908727\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.791634\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xab9c5449bde6902c\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xab9c5449bde6902c Time: 0.722629\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xac4736b5b00e1531\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xac4736b5b00e1531 Time: 1.21909\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.75755\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb0bf64026e546f4d\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xb0bf64026e546f4d Time: 0.783113\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xb26e93bd0702f504\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xb26e93bd0702f504 Time: 1.82663\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xb307bc772518d3d7\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xb307bc772518d3d7 Time: 1.12704\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb7dc3705357cc965\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xb7dc3705357cc965 Time: 0.868677\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0xbb3d6545e4864f26\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xbb3d6545e4864f26 Time: 0.829746\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xbfc71f913e286527\n",
      "[10/18/2022-15:56:02] [V] [TRT] Tactic: 0xbfc71f913e286527 Time: 1.56886\n",
      "[10/18/2022-15:56:02] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 1.45028\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xc684285f13ba11d0\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xc684285f13ba11d0 Time: 1.4933\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xc6e0905d983b4a62\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xc6e0905d983b4a62 Time: 1.91368\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4 Tactic: 0xc8ee1e4cdf0d8f84\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xc8ee1e4cdf0d8f84 Time: 1.18813\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r3s3 Tactic: 0xcb7b50f35a87094b\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xcb7b50f35a87094b Time: 1.64923\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd076fab92f5706c9\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xd076fab92f5706c9 Time: 1.12393\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd297ae2cdb8b1406\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xd297ae2cdb8b1406 Time: 1.14171\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 1.28497\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.682738\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4 Tactic: 0xd825f95894186a22\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xd825f95894186a22 Time: 2.26365\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xd9d1d89fceeca81a\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xd9d1d89fceeca81a Time: 1.56848\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r3s3 Tactic: 0xdb70c5e9779254fb\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xdb70c5e9779254fb Time: 1.84491\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.803634\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xe84b9aaa289245c0\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xe84b9aaa289245c0 Time: 0.853129\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4 Tactic: 0xe9fa7b19132889a8\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xe9fa7b19132889a8 Time: 1.53913\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4 Tactic: 0xf1d5fc0783e71536\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xf1d5fc0783e71536 Time: 1.39911\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0xf368aae1fb20baa1\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xf368aae1fb20baa1 Time: 0.791662\n",
      "[10/18/2022-15:56:03] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.66848\n",
      "[10/18/2022-15:56:03] [V] [TRT] Fastest Tactic: 0xfcd06da0f3c31fd1 Time: 0.66848\n",
      "[10/18/2022-15:56:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:56:03] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:56:03] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:56:03] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:56:03] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:03] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (FusedConvActConvolution)\n",
      "[10/18/2022-15:56:03] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:03] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.38928\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0x0000000000000001 Time: 3.73682\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.74774\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0x0000000000000004 Time: 48.5481\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0x0000000000000005 Time: 25.4361\n",
      "[10/18/2022-15:56:03] [V] [TRT] Tactic: 0x0000000000000038 Time: 4.65045\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x0000000000000039 Time: 3.73584\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x000000000000003a Time: 4.76955\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x000000000000003c Time: 47.8864\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x000000000000003d Time: 25.4124\n",
      "[10/18/2022-15:56:04] [V] [TRT] Fastest Tactic: 0x0000000000000039 Time: 3.73584\n",
      "[10/18/2022-15:56:04] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CublasConvolution)\n",
      "[10/18/2022-15:56:04] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:04] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CaskConvolution)\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 2.43102\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 2.38842\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 2.87301\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 2.68698\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 3.73982\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 2.92019\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 2.62007\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 2.62951\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 3.64293\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 3.52326\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:56:04] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 3.71421\n",
      "[10/18/2022-15:56:04] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 3.17548\n",
      "[10/18/2022-15:56:05] [V] [TRT] Fastest Tactic: 0x195431d38ba5af88 Time: 2.38842\n",
      "[10/18/2022-15:56:05] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:56:05] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(100352,1,14336,2048) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:56:05] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CublasConvolution)\n",
      "[10/18/2022-15:56:05] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:05] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CaskConvolution)\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 2.47282\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 2.38602\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 2.3281\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 3.10501\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 3.65047\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 2.42773\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 2.37275\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 2.48905\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 3.28148\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 2.35342\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 2.84672\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 2.44272\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 3.7171\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 3.18053\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0xf92663d88255134b Time: 2.58639\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 3.46775\n",
      "[10/18/2022-15:56:05] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 2.93675\n",
      "[10/18/2022-15:56:05] [V] [TRT] Fastest Tactic: 0x50ca8db54378cbac Time: 2.3281\n",
      "[10/18/2022-15:56:05] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:56:05] [V] [TRT] *************** Autotuning format combination: Half(25088,49,7,1), Half(100352,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:56:05] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:05] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:05] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:05] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:05] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CudnnConvolution)\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.54832\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.95497\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.87421\n",
      "[10/18/2022-15:56:05] [V] [TRT] Tactic: 0x0000000000000004 Time: 46.607\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x0000000000000005 Time: 24.2664\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x0000000000000038 Time: 4.63404\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x000000000000003a Time: 4.62204\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x000000000000003c Time: 46.6775\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x000000000000003d Time: 24.073\n",
      "[10/18/2022-15:56:06] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 2.95497\n",
      "[10/18/2022-15:56:06] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:06] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:06] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:06] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:06] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:06] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:06] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CublasConvolution)\n",
      "[10/18/2022-15:56:06] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:06] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:06] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:06] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:06] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:06] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:06] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:06] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CaskConvolution)\n",
      "[10/18/2022-15:56:06] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:06] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:56:06] [V] [TRT] *************** Autotuning format combination: Half(12544,49:2,7,1), Half(50176,49:2,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:56:06] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:06] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:06] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:06] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:06] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:06] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:06] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (FusedConvActConvolution)\n",
      "[10/18/2022-15:56:06] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:06] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:06] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:06] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:06] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:06] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:06] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:06] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CublasConvolution)\n",
      "[10/18/2022-15:56:06] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:06] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:06] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:06] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:06] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:06] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:06] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:06] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CaskConvolution)\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 1.00377\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 1.0109\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 1.06483\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x446c8c788145836a Time: 1.25572\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 1.31596\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 1.34286\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 1.39693\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x97afba3735828021 Time: 1.39695\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 1.27893\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 1.37209\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 1.30769\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:56:06] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 1.30732\n",
      "[10/18/2022-15:56:06] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 1.19186\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 1.28704\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 1.20667\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 1.18431\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 1.26793\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 1.33061\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0x16eafdbc5869b184 Time: 1.00377\n",
      "[10/18/2022-15:56:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(3136,1:8,448,64), Float(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CublasConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CaskConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(3136,1:8,448,64), Half(12544,1:8,1792,256) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CublasConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Conv_109 + Add_110 + Relu_111 (CaskConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 0.782875\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 0.72811\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.512878\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 0.743525\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.495831\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.466066\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 0.586569\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x249110624ee04937 Time: 0.533339\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 0.701518\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 0.630295\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.429074\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 0.629861\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 0.597723\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 0.808055\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 0.82912\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 0.559758\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 0.524987\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 0.863246\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 0.745303\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 0.818075\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 0.941947\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 0.728119\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 0.745161\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 0.783214\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 0.615648\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 0.823442\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 0.770798\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 0.814519\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 0.519003\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 0.521632\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 0.655954\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 0.747227\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 0.721033\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 1.08507\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.540864\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x83b35618df65874c Time: 0.969742\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 0.673207\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 0.717806\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 0.743397\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.543639\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 0.84981\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 0.743131\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 0.727918\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 1.10036\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 0.754766\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 0.559685\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.570185\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.554322\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 0.729961\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.467365\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 0.705669\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 0.55296\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.484224\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 0.533065\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 0.729088\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.466903\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.538533\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 0.822162\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.435054\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 0.829664\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 1.08954\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 0.485669\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.492128\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 0.784677\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 0.791698\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 0.735497\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 0.663781\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 0.626167\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.491808\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 1.08807\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 0.536069\n",
      "[10/18/2022-15:56:07] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.441669\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0x2a3615ad33745f0b Time: 0.429074\n",
      "[10/18/2022-15:56:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:07] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Float(100352,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Float(100352,1,14336,2048) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(100352,49,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(50176,49:2,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Conv_112 + Relu_113 (CaskConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(50176,49:2,7,1) -> Half(12544,49:2,7,1) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,1792,256) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Conv_112 + Relu_113 (CublasConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Conv_112 + Relu_113 (CaskConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,1792,256) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_112 + Relu_113.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512) -> Float(25088,1,3584,512) ***************\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(25088,49,7,1) -> Half(25088,49,7,1) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_114 + Relu_115.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(12544,49:2,7,1) -> Half(12544,49:2,7,1) ***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_114 + Relu_115.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(3136,1:8,448,64) -> Float(25088,49,7,1) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_114 + Relu_115.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Conv_114 + Relu_115 (CaskConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(3136,1:8,448,64) -> Half(3136,1:8,448,64) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_114 + Relu_115.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Float(25088,49,7,1), Float(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Float(25088,1,3584,512), Float(100352,1,14336,2048) -> Float(100352,1,14336,2048) ***************\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(25088,49,7,1), Half(100352,49,7,1) -> Half(100352,49,7,1) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_116 + Add_117 + Relu_118.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(12544,49:2,7,1), Half(50176,49:2,7,1) -> Half(50176,49:2,7,1) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_116 + Add_117 + Relu_118.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(3136,1:8,448,64), Float(100352,49,7,1) -> Float(100352,49,7,1) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_116 + Add_117 + Relu_118.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Conv_116 + Add_117 + Relu_118 (CublasConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_116 + Add_117 + Relu_118.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Conv_116 + Add_117 + Relu_118 (CaskConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(3136,1:8,448,64), Half(12544,1:8,1792,256) -> Half(12544,1:8,1792,256) ***************\n",
      "[10/18/2022-15:56:07] [W] [TRT] Weights [name=Conv_116 + Add_117 + Relu_118.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:07] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:07] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:07] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Float(100352,49,7,1) -> Float(2048,1,1,1) ***************\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (TiledPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x00000000007d0101 Time: 0.537806\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x00000000007e0101 Time: 0.496201\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x00000000007f0101 Time: 0.489111\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000800101 Time: 0.48549\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000810101 Time: 0.483118\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000820101 Time: 0.490857\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000830101 Time: 0.495767\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000840101 Time: 0.494354\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0x0000000000810101 Time: 0.483118\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (CudnnPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xffffffffffffffff Time: 0.255456\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0xffffffffffffffff Time: 0.255456\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (CaskPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] GlobalAveragePool_119 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Average_FastDiv Tactic: 0x933eceba7b866d59\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x933eceba7b866d59 Time: 0.260731\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0x933eceba7b866d59 Time: 0.260731\n",
      "[10/18/2022-15:56:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnPooling Tactic: 0xffffffffffffffff\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(100352,49,7,1) -> Half(2048,1,1,1) ***************\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (TiledPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] TiledPooling has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (CudnnPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xffffffffffffffff Time: 0.141525\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0xffffffffffffffff Time: 0.141525\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (CaskPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] GlobalAveragePool_119 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP16FP32NCHW_Average_FastDiv Tactic: 0x94ce450867316320\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x94ce450867316320 Time: 0.142562\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0x94ce450867316320 Time: 0.142562\n",
      "[10/18/2022-15:56:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnPooling Tactic: 0xffffffffffffffff\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(50176,49:2,7,1) -> Half(1024,1:2,1,1) ***************\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (TiledPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x00000000007d0101 Time: 0.282437\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x00000000007e0101 Time: 0.260361\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x00000000007f0101 Time: 0.256791\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000800101 Time: 0.254537\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000810101 Time: 0.255104\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000820101 Time: 0.257207\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000830101 Time: 0.260905\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000840101 Time: 0.275493\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0x0000000000800101 Time: 0.254537\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (CudaPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xfffffffffffffffd Time: 0.462885\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0xfffffffffffffffd Time: 0.462885\n",
      "[10/18/2022-15:56:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: TiledPooling Tactic: 0x0000000000800101\n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Half(12544,1:8,1792,256) -> Half(256,1:8,256,256) ***************\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (TiledPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] TiledPooling has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (CudaPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xfffffffffffffffe Time: 0.102384\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0xfffffffffffffffe Time: 0.102384\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (CudnnPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xffffffffffffffff Time: 0.153051\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0xffffffffffffffff Time: 0.153051\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: GlobalAveragePool_119 (CaskPooling)\n",
      "[10/18/2022-15:56:07] [V] [TRT] GlobalAveragePool_119 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP16FP32NHWC_Average_FastDiv_CAlign4 Tactic: 0x56d7b61f084f251e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x56d7b61f084f251e Time: 0.113557\n",
      "[10/18/2022-15:56:07] [V] [TRT] GlobalAveragePool_119 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP16FP32NHWC_Average_FastDiv_CAlign8 Tactic: 0xdaa6f29208dec74b\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0xdaa6f29208dec74b Time: 0.11285\n",
      "[10/18/2022-15:56:07] [V] [TRT] Fastest Tactic: 0xdaa6f29208dec74b Time: 0.11285\n",
      "[10/18/2022-15:56:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: 0xfffffffffffffffe\n",
      "[10/18/2022-15:56:07] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:56:07] [V] [TRT] *************** Autotuning format combination: Float(2048,1,1,1) -> Float(1000,1,1,1) ***************\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Gemm_121 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Gemm_121 (FusedConvActConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:07] [V] [TRT] --------------- Timing Runner: Gemm_121 (CudnnConvolution)\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.430377\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.109817\n",
      "[10/18/2022-15:56:07] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.255543\n",
      "[10/18/2022-15:56:08] [V] [TRT] Tactic: 0x0000000000000004 Time: 92.3179\n",
      "[10/18/2022-15:56:08] [V] [TRT] Tactic: 0x0000000000000005 Time: 8.76158\n",
      "[10/18/2022-15:56:08] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.51989\n",
      "[10/18/2022-15:56:08] [V] [TRT] Tactic: 0x0000000000000039 Time: 0.137945\n",
      "[10/18/2022-15:56:08] [V] [TRT] Tactic: 0x000000000000003a Time: 0.318112\n",
      "[10/18/2022-15:56:09] [V] [TRT] Tactic: 0x000000000000003c Time: 92.374\n",
      "[10/18/2022-15:56:09] [V] [TRT] Tactic: 0x000000000000003d Time: 8.48771\n",
      "[10/18/2022-15:56:09] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.109817\n",
      "[10/18/2022-15:56:09] [V] [TRT] --------------- Timing Runner: Gemm_121 (CublasConvolution)\n",
      "[10/18/2022-15:56:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.122117\n",
      "[10/18/2022-15:56:09] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.10861\n",
      "[10/18/2022-15:56:09] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.10861\n",
      "[10/18/2022-15:56:09] [V] [TRT] --------------- Timing Runner: Gemm_121 (CaskConvolution)\n",
      "[10/18/2022-15:56:09] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x128_relu_interior_nn_v1 Tactic: 0x18597bd4a7d0164d\n",
      "[10/18/2022-15:56:09] [V] [TRT] Tactic: 0x18597bd4a7d0164d Time: 0.530267\n",
      "[10/18/2022-15:56:09] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x128_relu_medium_nn_v1 Tactic: 0x195431d38ba5af88\n",
      "[10/18/2022-15:56:09] [V] [TRT] Tactic: 0x195431d38ba5af88 Time: 0.520608\n",
      "[10/18/2022-15:56:09] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x25eed4cfa195d49d\n",
      "[10/18/2022-15:56:09] [V] [TRT] Tactic: 0x25eed4cfa195d49d Time: 0.458126\n",
      "[10/18/2022-15:56:09] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x128_relu_small_nn_v1 Tactic: 0x365602d0613d4c36\n",
      "[10/18/2022-15:56:09] [V] [TRT] Tactic: 0x365602d0613d4c36 Time: 0.527931\n",
      "[10/18/2022-15:56:09] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5193693bc0732c65\n",
      "[10/18/2022-15:56:09] [V] [TRT] Tactic: 0x5193693bc0732c65 Time: 0.303374\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x64_relu_small_nn_v1 Tactic: 0x5e7d1125e7896624\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x5e7d1125e7896624 Time: 0.366624\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x64_relu_interior_nn_v1 Tactic: 0x7e29bdfccd92c42c\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x7e29bdfccd92c42c Time: 0.355168\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x64_relu_medium_nn_v1 Tactic: 0x90238daf8750ddb0\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x90238daf8750ddb0 Time: 0.377143\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa0dcf7c2b333d150\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0xa0dcf7c2b333d150 Time: 0.28085\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa3cd285aae791bdd\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0xa3cd285aae791bdd Time: 0.30277\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x32_relu_medium_nn_v1 Tactic: 0xc2a5fc6b5e7cef5e\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0xc2a5fc6b5e7cef5e Time: 0.471351\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x32_relu_small_nn_v1 Tactic: 0xc939b7b0a4d5a05f\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0xc939b7b0a4d5a05f Time: 0.493829\n",
      "[10/18/2022-15:56:10] [V] [TRT] Fastest Tactic: 0xa0dcf7c2b333d150 Time: 0.28085\n",
      "[10/18/2022-15:56:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CublasConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:56:10] [V] [TRT] *************** Autotuning format combination: Float(2048,1,2048,2048) -> Float(1000,1,1000,1000) ***************\n",
      "[10/18/2022-15:56:10] [V] [TRT] --------------- Timing Runner: Gemm_121 (CublasConvolution)\n",
      "[10/18/2022-15:56:10] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:10] [V] [TRT] --------------- Timing Runner: Gemm_121 (CaskConvolution)\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0x0bf55a7b77a6ff98\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x0bf55a7b77a6ff98 Time: 0.353993\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x48f8d75aa348d22f\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x48f8d75aa348d22f Time: 0.18315\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x50ca8db54378cbac\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x50ca8db54378cbac Time: 0.354011\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x516049bee7812ab0\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x516049bee7812ab0 Time: 0.102162\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x704db0897ce9340d\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x704db0897ce9340d Time: 0.153307\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x810bd80d0531c0a0\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x810bd80d0531c0a0 Time: 0.343547\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x849891f3d1d80c55\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x849891f3d1d80c55 Time: 0.342702\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x852b455de4263ff7\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x852b455de4263ff7 Time: 0.184462\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x90d45931b538d74f Time: 0.0997143\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x946eca69f99ddcb4\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x946eca69f99ddcb4 Time: 0.346016\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xa79cf41de521f476\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0xa79cf41de521f476 Time: 0.206455\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0xb90177ab6d659acd\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0xb90177ab6d659acd Time: 0.350391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xded29d328f8f7228\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0xded29d328f8f7228 Time: 0.155355\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage1_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xe957dcfcec24ec5d\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0xe957dcfcec24ec5d Time: 0.20171\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xf92663d88255134b\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0xf92663d88255134b Time: 0.177563\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfa1e150a2da08265\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0xfa1e150a2da08265 Time: 0.102432\n",
      "[10/18/2022-15:56:10] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage1_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfbba95cf52891795\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0xfbba95cf52891795 Time: 0.204046\n",
      "[10/18/2022-15:56:10] [V] [TRT] Fastest Tactic: 0x90d45931b538d74f Time: 0.0997143\n",
      "[10/18/2022-15:56:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x90d45931b538d74f\n",
      "[10/18/2022-15:56:10] [V] [TRT] *************** Autotuning format combination: Half(2048,1,1,1) -> Half(1000,1,1,1) ***************\n",
      "[10/18/2022-15:56:10] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:10] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:10] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:10] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:10] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:10] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:10] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:10] [V] [TRT] --------------- Timing Runner: Gemm_121 (CudnnConvolution)\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.399607\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.453221\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.354281\n",
      "[10/18/2022-15:56:10] [V] [TRT] Tactic: 0x0000000000000004 Time: 90.7123\n",
      "[10/18/2022-15:56:11] [V] [TRT] Tactic: 0x0000000000000005 Time: 8.50446\n",
      "[10/18/2022-15:56:11] [V] [TRT] Tactic: 0x0000000000000038 Time: 0.514496\n",
      "[10/18/2022-15:56:11] [V] [TRT] Tactic: 0x000000000000003a Time: 0.427662\n",
      "[10/18/2022-15:56:11] [V] [TRT] Tactic: 0x000000000000003c Time: 90.7893\n",
      "[10/18/2022-15:56:11] [V] [TRT] Tactic: 0x000000000000003d Time: 8.43751\n",
      "[10/18/2022-15:56:11] [V] [TRT] Fastest Tactic: 0x0000000000000002 Time: 0.354281\n",
      "[10/18/2022-15:56:11] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:11] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:11] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:11] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:11] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:11] [V] [TRT] --------------- Timing Runner: Gemm_121 (CublasConvolution)\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0448914\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0398251\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000002 Time: 0.044864\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000003 Time: 0.0398651\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000004 Time: 0.0614248\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.0610789\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.0454137\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000007 Time: 0.0416663\n",
      "[10/18/2022-15:56:12] [V] [TRT] Fastest Tactic: 0x0000000000000001 Time: 0.0398251\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: Gemm_121 (CaskConvolution)\n",
      "[10/18/2022-15:56:12] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CublasConvolution Tactic: 0x0000000000000001\n",
      "[10/18/2022-15:56:12] [V] [TRT] *************** Autotuning format combination: Half(1024,1:2,1,1) -> Half(1000,1,1,1) ***************\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: Gemm_121 (CaskConvolution)\n",
      "[10/18/2022-15:56:12] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:12] [V] [TRT] *************** Autotuning format combination: Half(1024,1:2,1,1) -> Half(500,1:2,1,1) ***************\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: Gemm_121 (FusedConvActConvolution)\n",
      "[10/18/2022-15:56:12] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: Gemm_121 (CublasConvolution)\n",
      "[10/18/2022-15:56:12] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: Gemm_121 (CaskConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x16eafdbc5869b184\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x16eafdbc5869b184 Time: 0.19355\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_medium_nn_v1 Tactic: 0x21904dd9d0cd407e\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x21904dd9d0cd407e Time: 0.193737\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x3bee4a098b4f8914\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x3bee4a098b4f8914 Time: 0.1864\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x446c8c788145836a\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x446c8c788145836a Time: 0.341202\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_medium_nn_v1 Tactic: 0x73163c1d09e17290\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x73163c1d09e17290 Time: 0.346336\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_small_nn_v1 Tactic: 0x7498280d2c59e4aa\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x7498280d2c59e4aa Time: 0.190245\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0x87e5c2a636a0d1f8\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x87e5c2a636a0d1f8 Time: 0.2712\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0x97afba3735828021\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x97afba3735828021 Time: 0.21851\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0x9ce6ebc390e62b01\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x9ce6ebc390e62b01 Time: 0.184613\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xacaaec9cc8134f6f\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xacaaec9cc8134f6f Time: 0.325339\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_small_nn_v1 Tactic: 0xb09f72c3be042002\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xb09f72c3be042002 Time: 0.272466\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_interior_nn_v1 Tactic: 0xc72182f0fce13bb0\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xc72182f0fce13bb0 Time: 0.22565\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x64_relu_interior_nn_v1 Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xcc68d30459859090 Time: 0.184311\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xccca8c966967f8f8\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xccca8c966967f8f8 Time: 0.267173\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdb5acaea7b0746d5\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xdb5acaea7b0746d5 Time: 0.27035\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x128_relu_interior_nn_v1 Tactic: 0xdcd3fec139dd130a\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xdcd3fec139dd130a Time: 0.269728\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_fp16x2_hcudnn_fp16x2_128x32_relu_small_nn_v1 Tactic: 0xe3dc8e986f0522d1\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xe3dc8e986f0522d1 Time: 0.33269\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_fp16x2_hcudnn_fp16x2_128x128_relu_medium_nn_v1 Tactic: 0xe4aed86f94a0620c\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xe4aed86f94a0620c Time: 0.270409\n",
      "[10/18/2022-15:56:12] [V] [TRT] Fastest Tactic: 0xcc68d30459859090 Time: 0.184311\n",
      "[10/18/2022-15:56:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xcc68d30459859090\n",
      "[10/18/2022-15:56:12] [V] [TRT] *************** Autotuning format combination: Half(256,1:8,256,256) -> Float(1000,1,1,1) ***************\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: Gemm_121 (CublasConvolution)\n",
      "[10/18/2022-15:56:12] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: Gemm_121 (CaskConvolution)\n",
      "[10/18/2022-15:56:12] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:12] [V] [TRT] *************** Autotuning format combination: Half(256,1:8,256,256) -> Half(125,1:8,125,125) ***************\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: Gemm_121 (CudaDepthwiseConvolution)\n",
      "[10/18/2022-15:56:12] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: Gemm_121 (CublasConvolution)\n",
      "[10/18/2022-15:56:12] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: Gemm_121 (CaskConvolution)\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x0129597ad9bbff14\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0129597ad9bbff14 Time: 0.054595\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x017a89ce2d82b850\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x017a89ce2d82b850 Time: 0.0347063\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x105f56cf03ee5549 Time: 0.0539017\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x1d38ef2fc1ec5804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x1d38ef2fc1ec5804 Time: 0.0918423\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x1dcf9babce3d9b3b Time: 0.0648381\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x21739cdb4c6113ed Time: 0.0945851\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_t1r1s1 Tactic: 0x22dbd03ae6f5a915\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x22dbd03ae6f5a915 Time: 0.0654979\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x249110624ee04937\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x249110624ee04937 Time: 0.059008\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x255200b1b31c45cd\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x255200b1b31c45cd Time: 0.177317\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x26d4c2773a9a6efc\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x26d4c2773a9a6efc Time: 0.0932046\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x2a3615ad33745f0b Time: 0.052669\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x2ae5fedb80fbd388\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x2ae5fedb80fbd388 Time: 0.0927589\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x2c6739dc8daca583 Time: 0.0641249\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x34192289eb1f5427\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x34192289eb1f5427 Time: 0.108729\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x3693535b668f43cb\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x3693535b668f43cb Time: 0.0376114\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x399448b5af8ca81a\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x399448b5af8ca81a Time: 0.0587109\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x3f3840edab5c9d44\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x3f3840edab5c9d44 Time: 0.0651048\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x41e8a431d0137286\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x41e8a431d0137286 Time: 0.0548008\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x4c17dc9d992e6a1d\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x4c17dc9d992e6a1d Time: 0.0657874\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x4ea23ec81add686f\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x4ea23ec81add686f Time: 0.10464\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x51e3312bfd062f36\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x51e3312bfd062f36 Time: 0.0663924\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x523aca1fca7ef548\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x523aca1fca7ef548 Time: 0.175849\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x53422c5d4478d3d7\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x53422c5d4478d3d7 Time: 0.0628983\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x5cb7625ea24db701\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x5cb7625ea24db701 Time: 0.0959817\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x62a22cfa1199e58e\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x62a22cfa1199e58e Time: 0.0324791\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x63566dea68ccc247\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x63566dea68ccc247 Time: 0.0930789\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x6d1428d5257a3dc9\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x6d1428d5257a3dc9 Time: 0.0943657\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x72f623a1c870d417\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x72f623a1c870d417 Time: 0.10699\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7585679fc3cc2536\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x7585679fc3cc2536 Time: 0.0950857\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x77a26840a2ace0b3\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x77a26840a2ace0b3 Time: 0.110885\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x77ef8bb029e1d4e0\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x77ef8bb029e1d4e0 Time: 0.0942811\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x7ca057c91d677737\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x7ca057c91d677737 Time: 0.0569082\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x128x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x7e665af4f37d210b\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x7e665af4f37d210b Time: 0.0631787\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x81a7be09ad63581a\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x81a7be09ad63581a Time: 0.0441497\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0x833510adbbf772c4\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x833510adbbf772c4 Time: 0.0698072\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x83b35618df65874c\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x83b35618df65874c Time: 0.0659642\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x83c3f470a0ec89f9\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x83c3f470a0ec89f9 Time: 0.0943497\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8480e919254b99f8\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x8480e919254b99f8 Time: 0.0964434\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_t1r1s1 Tactic: 0x8639a0d23c8a1708\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x8639a0d23c8a1708 Time: 0.0916571\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x86937c170a111d1f\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x86937c170a111d1f Time: 0.0950537\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x89c2d153627e52ba\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x89c2d153627e52ba Time: 0.0374994\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x8a37d1d6d41033e6\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x8a37d1d6d41033e6 Time: 0.176425\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x8b8a7a5cef8d932b\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x8b8a7a5cef8d932b Time: 0.174469\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0x911cdd8d308bed5c\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x911cdd8d308bed5c Time: 0.0557867\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_t1r1s1 Tactic: 0x93125939e1fba374\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x93125939e1fba374 Time: 0.176187\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x9774d044044b6a7d\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x9774d044044b6a7d Time: 0.0600213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xa8f10051cbdaaa96\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xa8f10051cbdaaa96 Time: 0.055171\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xaf407014f2c7f1cb\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xaf407014f2c7f1cb Time: 0.05376\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb26ad7a19a3195cc\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xb26ad7a19a3195cc Time: 0.0657189\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb3989f8802666c8a\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xb3989f8802666c8a Time: 0.0557973\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x128x32_stage1_warpsize4x2x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xb5342eac22cbe342\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xb5342eac22cbe342 Time: 0.175959\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x64x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xb5fdd9dd73a52c67\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xb5fdd9dd73a52c67 Time: 0.0596251\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xb8eb6a106c53cff6\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xb8eb6a106c53cff6 Time: 0.081696\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xba86f9c788dfb2dc\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xba86f9c788dfb2dc Time: 0.0673966\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xc110e19c9f5aa36e\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xc110e19c9f5aa36e Time: 0.174926\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xc399fdbffdc34032 Time: 0.0569768\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x256x32_stage1_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc6f99965cbd03fdf\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xc6f99965cbd03fdf Time: 0.111072\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd313af1c92b241c4\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xd313af1c92b241c4 Time: 0.0919429\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0xd47a5fce3824e4a4\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xd47a5fce3824e4a4 Time: 0.0532693\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xd8c128ae16cb4132 Time: 0.0347008\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x32x32_stage1_warpsize4x1x1_g1_tensor8x8x4_t1r1s1 Tactic: 0xdadc728a0ae041d9\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xdadc728a0ae041d9 Time: 0.0557608\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage1_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xdbe57b4edf7481d8\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xdbe57b4edf7481d8 Time: 0.081008\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xdc1c841ef1cd3e8e Time: 0.0951497\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x32x32_stage1_warpsize4x1x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xdc559b3944b0cdf8\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xdc559b3944b0cdf8 Time: 0.0572404\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: volta_h884cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xde62c240f3a7d930\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xde62c240f3a7d930 Time: 0.0940754\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe281d0b88acb38b8\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xe281d0b88acb38b8 Time: 0.0915154\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor8x8x4_simple_t1r1s1 Tactic: 0xe2866ff18c9049f9\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xe2866ff18c9049f9 Time: 0.092464\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xe67db95e0c20b618 Time: 0.0322386\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0xef1e5139c624a44f\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xef1e5139c624a44f Time: 0.0936229\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm70_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor8x8x4_t1r1s1 Tactic: 0xf883bd61103a5c32\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xf883bd61103a5c32 Time: 0.043512\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize256x64x32_stage1_warpsize4x1x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xfbff59172cce263c\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xfbff59172cce263c Time: 0.0645364\n",
      "[10/18/2022-15:56:12] [V] [TRT] Gemm_121 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0xfcd06da0f3c31fd1 Time: 0.0561036\n",
      "[10/18/2022-15:56:12] [V] [TRT] Fastest Tactic: 0xe67db95e0c20b618 Time: 0.0322386\n",
      "[10/18/2022-15:56:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:56:12] [V] [TRT] =============== Computing costs for \n",
      "[10/18/2022-15:56:12] [V] [TRT] *************** Autotuning format combination: Float(1000,1,1,1) -> Float(1000,1) ***************\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: reshape_after_Gemm_121 (Shuffle)\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0038394\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0145042\n",
      "[10/18/2022-15:56:12] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0038394\n",
      "[10/18/2022-15:56:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
      "[10/18/2022-15:56:12] [V] [TRT] *************** Autotuning format combination: Half(1000,1,1,1) -> Half(1000,1) ***************\n",
      "[10/18/2022-15:56:12] [V] [TRT] --------------- Timing Runner: reshape_after_Gemm_121 (Shuffle)\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00371833\n",
      "[10/18/2022-15:56:12] [V] [TRT] Tactic: 0x0000000000000001 Time: 0.0147086\n",
      "[10/18/2022-15:56:12] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00371833\n",
      "[10/18/2022-15:56:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000\n",
      "[10/18/2022-15:56:12] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to Conv_0 + Relu_1 (input) from Float(150528,50176,224,1) to Half(50176,1:4,224,1)\n",
      "[10/18/2022-15:56:12] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to reshape_after_Gemm_121 (Gemm_121_out_tensor) from Half(125,1:8,125,125) to Float(1000,1,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:12] [V] [TRT] Formats and tactics selection completed in 150.555 seconds.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] After reformat layers: 59 layers\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Pre-optimized block assignment.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 205520896\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 205520896\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 205520896\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 205520896\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 205520896\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 102760448\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 102760448\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 102760448\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 102760448\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 102760448\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 102760448\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 12845056\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 12845056\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 12845056\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 12845056\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 12845056\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 12845056\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 12845056\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 12845056\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 12845056\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 12845056\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 12845056\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 6422528\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 6422528\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 6422528\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 6422528\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 6422528\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 25690112\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 524288\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 256000\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 51380224\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 4\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Block size 15634661376\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Total Activation Memory: 18583382020\r\n",
      "[10/18/2022-15:56:12] [I] [TRT] Detected 1 inputs and 1 output network tensors.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_0 + Relu_1.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_0 + Relu_1 Set Tactic Name: trt_turing_cutlass_image_network_first_layer_hmma_fprop_f16f16f32_nhwc_nhwc_k64r7s7c4_stride2x2 Tactic: 0xe2222883a6602489\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_3 + Relu_4.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_3 + Relu_4.bias] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_3 + Relu_4 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_5 + Relu_6.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_5 + Relu_6.bias] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_5 + Relu_6 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_7.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_7.bias] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_7 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xc399fdbffdc34032\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_8 + Add_9 + Relu_10.bias] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_8 + Add_9 + Relu_10 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_11 + Relu_12.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_11 + Relu_12.bias] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_11 + Relu_12 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_13 + Relu_14.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_13 + Relu_14 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_15 + Add_16 + Relu_17.bias] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_15 + Add_16 + Relu_17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_18 + Relu_19.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_18 + Relu_19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x2_g1_tensor16x8x8_t1r1s1 Tactic: 0xd8c128ae16cb4132\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_20 + Relu_21.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_20 + Relu_21 Set Tactic Name: turing_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x1dcf9babce3d9b3b\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_22 + Add_23 + Relu_24.bias] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_22 + Add_23 + Relu_24 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x256x64_stage1_warpsize1x4x2_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2c6739dc8daca583\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_25 + Relu_26.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_25 + Relu_26 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_27 + Relu_28.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_27 + Relu_28 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_29.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_29.bias] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_29 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_30 + Add_31 + Relu_32.bias] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_30 + Add_31 + Relu_32 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_33 + Relu_34.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_33 + Relu_34.bias] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_33 + Relu_34 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_35 + Relu_36.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_35 + Relu_36 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_37 + Add_38 + Relu_39.bias] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_37 + Add_38 + Relu_39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\r\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_40 + Relu_41.weight] had the following issues when converted to FP16:\r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \r\n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\r\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_40 + Relu_41 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_42 + Relu_43.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_42 + Relu_43.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_42 + Relu_43 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_44 + Add_45 + Relu_46.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_44 + Add_45 + Relu_46 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_47 + Relu_48.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_47 + Relu_48 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_49 + Relu_50.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_49 + Relu_50 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_51 + Add_52 + Relu_53.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_51 + Add_52 + Relu_53 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_54 + Relu_55.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_54 + Relu_55 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_56 + Relu_57.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_56 + Relu_57 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xdc1c841ef1cd3e8e\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_58.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_58.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_58 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_59 + Add_60 + Relu_61.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_59 + Add_60 + Relu_61 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_62 + Relu_63.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_62 + Relu_63 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_64 + Relu_65.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_64 + Relu_65 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_66 + Add_67 + Relu_68.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_66 + Add_67 + Relu_68 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_69 + Relu_70.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_69 + Relu_70 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_71 + Relu_72.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_71 + Relu_72 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_73 + Add_74 + Relu_75.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_73 + Add_74 + Relu_75 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_76 + Relu_77.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_76 + Relu_77 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_78 + Relu_79.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_78 + Relu_79 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_80 + Add_81 + Relu_82.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_80 + Add_81 + Relu_82 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_83 + Relu_84.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_83 + Relu_84 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_85 + Relu_86.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_85 + Relu_86 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_87 + Add_88 + Relu_89.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_87 + Add_88 + Relu_89 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_90 + Relu_91.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_90 + Relu_91 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_92 + Relu_93.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_92 + Relu_93 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_94 + Add_95 + Relu_96.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_94 + Add_95 + Relu_96 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_97 + Relu_98.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_97 + Relu_98 Set Tactic Name: turing_h1688cudnn_256x128_ldg8_relu_exp_small_nhwc_tn_v1 Tactic: 0x21739cdb4c6113ed\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_99 + Relu_100.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_99 + Relu_100.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_99 + Relu_100 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_large_nhwc_tn_v1 Tactic: 0x4e34a65090c3b86f\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_101.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_101.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_101 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_102 + Add_103 + Relu_104.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_102 + Add_103 + Relu_104 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_105 + Relu_106.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_105 + Relu_106 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_107 + Relu_108.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [W] [TRT] Weights [name=Conv_107 + Relu_108.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:12] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:12] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:12] [V] [TRT] Conv_107 + Relu_108 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:56:13] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:13] [W] [TRT] Weights [name=Conv_109 + Add_110 + Relu_111.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:13] [V] [TRT] Conv_109 + Add_110 + Relu_111 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:13] [W] [TRT] Weights [name=Conv_112 + Relu_113.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:13] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:13] [W] [TRT] Weights [name=Conv_112 + Relu_113.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:13] [V] [TRT] Conv_112 + Relu_113 Set Tactic Name: turing_h1688cudnn_256x64_sliced1x2_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x105f56cf03ee5549\n",
      "[10/18/2022-15:56:13] [W] [TRT] Weights [name=Conv_114 + Relu_115.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:13] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:13] [V] [TRT] Conv_114 + Relu_115 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_medium_nhwc_tn_v1 Tactic: 0xfcd06da0f3c31fd1\n",
      "[10/18/2022-15:56:13] [W] [TRT] Weights [name=Conv_116 + Add_117 + Relu_118.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:13] [V] [TRT] Conv_116 + Add_117 + Relu_118 Set Tactic Name: turing_h1688cudnn_128x128_ldg8_relu_exp_interior_nhwc_tn_v1 Tactic: 0x2a3615ad33745f0b\n",
      "[10/18/2022-15:56:13] [W] [TRT] Weights [name=Gemm_121.weight] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:13] [W] [TRT]  - Values less than smallest positive FP16 Subnormal value detected. Converting to FP16 minimum subnormalized value. \n",
      "[10/18/2022-15:56:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n",
      "[10/18/2022-15:56:13] [W] [TRT] Weights [name=Gemm_121.bias] had the following issues when converted to FP16:\n",
      "[10/18/2022-15:56:13] [W] [TRT]  - Subnormal FP16 values detected. \n",
      "[10/18/2022-15:56:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to reduce the magnitude of the weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:13] [V] [TRT] Gemm_121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xe67db95e0c20b618\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Reformatting CopyNode for Input Tensor 0 to Conv_0 + Relu_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_0 + Relu_1 Host Persistent: 768 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: MaxPool_2 Host Persistent: 48 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_3 + Relu_4 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_5 + Relu_6 Host Persistent: 1664 Device Persistent: 18944 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_7 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_8 + Add_9 + Relu_10 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_11 + Relu_12 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_13 + Relu_14 Host Persistent: 1664 Device Persistent: 18944 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_15 + Add_16 + Relu_17 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_18 + Relu_19 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_20 + Relu_21 Host Persistent: 1664 Device Persistent: 18944 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_22 + Add_23 + Relu_24 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_25 + Relu_26 Host Persistent: 2176 Device Persistent: 18944 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_27 + Relu_28 Host Persistent: 2176 Device Persistent: 5120 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_29 Host Persistent: 3200 Device Persistent: 5120 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_30 + Add_31 + Relu_32 Host Persistent: 3200 Device Persistent: 5120 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_33 + Relu_34 Host Persistent: 3200 Device Persistent: 5120 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_35 + Relu_36 Host Persistent: 2176 Device Persistent: 5120 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_37 + Add_38 + Relu_39 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_40 + Relu_41 Host Persistent: 3200 Device Persistent: 5120 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_42 + Relu_43 Host Persistent: 2176 Device Persistent: 5120 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_44 + Add_45 + Relu_46 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_47 + Relu_48 Host Persistent: 3200 Device Persistent: 5120 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_49 + Relu_50 Host Persistent: 2176 Device Persistent: 5120 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_51 + Add_52 + Relu_53 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_54 + Relu_55 Host Persistent: 2176 Device Persistent: 5120 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_56 + Relu_57 Host Persistent: 2176 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_58 Host Persistent: 3200 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_59 + Add_60 + Relu_61 Host Persistent: 3200 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_62 + Relu_63 Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_64 + Relu_65 Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_66 + Add_67 + Relu_68 Host Persistent: 3200 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_69 + Relu_70 Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_71 + Relu_72 Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_73 + Add_74 + Relu_75 Host Persistent: 3200 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_76 + Relu_77 Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_78 + Relu_79 Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_80 + Add_81 + Relu_82 Host Persistent: 3200 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_83 + Relu_84 Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_85 + Relu_86 Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_87 + Add_88 + Relu_89 Host Persistent: 3200 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_90 + Relu_91 Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_92 + Relu_93 Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_94 + Add_95 + Relu_96 Host Persistent: 3200 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_97 + Relu_98 Host Persistent: 1664 Device Persistent: 1536 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_99 + Relu_100 Host Persistent: 4224 Device Persistent: 512 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_101 Host Persistent: 3200 Device Persistent: 512 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_102 + Add_103 + Relu_104 Host Persistent: 2176 Device Persistent: 512 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_105 + Relu_106 Host Persistent: 3200 Device Persistent: 512 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_107 + Relu_108 Host Persistent: 2176 Device Persistent: 512 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_109 + Add_110 + Relu_111 Host Persistent: 3200 Device Persistent: 512 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_112 + Relu_113 Host Persistent: 3200 Device Persistent: 512 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_114 + Relu_115 Host Persistent: 2176 Device Persistent: 512 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Conv_116 + Add_117 + Relu_118 Host Persistent: 3200 Device Persistent: 512 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: GlobalAveragePool_119 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Gemm_121 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: Reformatting CopyNode for Input Tensor 0 to reshape_after_Gemm_121 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Layer: reshape_after_Gemm_121 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [I] [TRT] Total Host Persistent Memory: 135120\r\n",
      "[10/18/2022-15:56:13] [I] [TRT] Total Device Persistent Memory: 160768\r\n",
      "[10/18/2022-15:56:13] [I] [TRT] Total Scratch Memory: 0\r\n",
      "[10/18/2022-15:56:13] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 52 MiB, GPU 5094 MiB\r\n",
      "[10/18/2022-15:56:13] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 1.85066ms to assign 3 blocks to 58 nodes requiring 513802240 bytes.\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Optimized block assignment.\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Block size 205520896\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Block size 205520896\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Block size 102760448\r\n",
      "[10/18/2022-15:56:13] [I] [TRT] Total Activation Memory: 513802240\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Disabling unused tactic source: CUBLAS, CUBLAS_LT\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Using cuDNN as a tactic source\r\n",
      "[10/18/2022-15:56:13] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 1837, GPU 861 (MiB)\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Engine generation completed in 151.705 seconds.\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Deleting timing cache: 517 entries, served 1506 hits since creation.\r\n",
      "[10/18/2022-15:56:13] [V] [TRT] Engine Layer Information:\r\n",
      "Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to Conv_0 + Relu_1, Tactic: 0x00000000000003ea, input[Float(-2,3,224,224)] -> Reformatted Input Tensor 0 to Conv_0 + Relu_1[Half(-2,3,224,224)]\r\n",
      "Layer(CaskConvolution): Conv_0 + Relu_1, Tactic: 0xe2222883a6602489, Reformatted Input Tensor 0 to Conv_0 + Relu_1[Half(-2,3,224,224)] -> onnx::MaxPool_323[Half(-2,64,112,112)]\r\n",
      "Layer(CudnnPooling): MaxPool_2, Tactic: 0xffffffffffffffff, onnx::MaxPool_323[Half(-2,64,112,112)] -> input.8[Half(-2,64,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_3 + Relu_4, Tactic: 0xd8c128ae16cb4132, input.8[Half(-2,64,56,56)] -> onnx::Conv_327[Half(-2,64,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_5 + Relu_6, Tactic: 0x1dcf9babce3d9b3b, onnx::Conv_327[Half(-2,64,56,56)] -> onnx::Conv_330[Half(-2,64,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_7, Tactic: 0xc399fdbffdc34032, onnx::Conv_330[Half(-2,64,56,56)] -> onnx::Add_505[Half(-2,256,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_8 + Add_9 + Relu_10, Tactic: 0x2c6739dc8daca583, input.8[Half(-2,64,56,56)], onnx::Add_505[Half(-2,256,56,56)] -> input.36[Half(-2,256,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_11 + Relu_12, Tactic: 0xd8c128ae16cb4132, input.36[Half(-2,256,56,56)] -> onnx::Conv_339[Half(-2,64,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_13 + Relu_14, Tactic: 0x1dcf9babce3d9b3b, onnx::Conv_339[Half(-2,64,56,56)] -> onnx::Conv_342[Half(-2,64,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_15 + Add_16 + Relu_17, Tactic: 0x2c6739dc8daca583, onnx::Conv_342[Half(-2,64,56,56)], input.36[Half(-2,256,56,56)] -> input.60[Half(-2,256,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_18 + Relu_19, Tactic: 0xd8c128ae16cb4132, input.60[Half(-2,256,56,56)] -> onnx::Conv_349[Half(-2,64,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_20 + Relu_21, Tactic: 0x1dcf9babce3d9b3b, onnx::Conv_349[Half(-2,64,56,56)] -> onnx::Conv_352[Half(-2,64,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_22 + Add_23 + Relu_24, Tactic: 0x2c6739dc8daca583, onnx::Conv_352[Half(-2,64,56,56)], input.60[Half(-2,256,56,56)] -> input.84[Half(-2,256,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_25 + Relu_26, Tactic: 0xfcd06da0f3c31fd1, input.84[Half(-2,256,56,56)] -> onnx::Conv_359[Half(-2,128,56,56)]\r\n",
      "Layer(CaskConvolution): Conv_27 + Relu_28, Tactic: 0xfcd06da0f3c31fd1, onnx::Conv_359[Half(-2,128,56,56)] -> onnx::Conv_362[Half(-2,128,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_29, Tactic: 0x2a3615ad33745f0b, onnx::Conv_362[Half(-2,128,28,28)] -> onnx::Add_535[Half(-2,512,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_30 + Add_31 + Relu_32, Tactic: 0x2a3615ad33745f0b, input.84[Half(-2,256,56,56)], onnx::Add_535[Half(-2,512,28,28)] -> input.112[Half(-2,512,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_33 + Relu_34, Tactic: 0x2a3615ad33745f0b, input.112[Half(-2,512,28,28)] -> onnx::Conv_371[Half(-2,128,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_35 + Relu_36, Tactic: 0xfcd06da0f3c31fd1, onnx::Conv_371[Half(-2,128,28,28)] -> onnx::Conv_374[Half(-2,128,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_37 + Add_38 + Relu_39, Tactic: 0xe67db95e0c20b618, onnx::Conv_374[Half(-2,128,28,28)], input.112[Half(-2,512,28,28)] -> input.136[Half(-2,512,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_40 + Relu_41, Tactic: 0x2a3615ad33745f0b, input.136[Half(-2,512,28,28)] -> onnx::Conv_381[Half(-2,128,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_42 + Relu_43, Tactic: 0xfcd06da0f3c31fd1, onnx::Conv_381[Half(-2,128,28,28)] -> onnx::Conv_384[Half(-2,128,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_44 + Add_45 + Relu_46, Tactic: 0xe67db95e0c20b618, onnx::Conv_384[Half(-2,128,28,28)], input.136[Half(-2,512,28,28)] -> input.160[Half(-2,512,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_47 + Relu_48, Tactic: 0x2a3615ad33745f0b, input.160[Half(-2,512,28,28)] -> onnx::Conv_391[Half(-2,128,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_49 + Relu_50, Tactic: 0xfcd06da0f3c31fd1, onnx::Conv_391[Half(-2,128,28,28)] -> onnx::Conv_394[Half(-2,128,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_51 + Add_52 + Relu_53, Tactic: 0xe67db95e0c20b618, onnx::Conv_394[Half(-2,128,28,28)], input.160[Half(-2,512,28,28)] -> input.184[Half(-2,512,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_54 + Relu_55, Tactic: 0xfcd06da0f3c31fd1, input.184[Half(-2,512,28,28)] -> onnx::Conv_401[Half(-2,256,28,28)]\r\n",
      "Layer(CaskConvolution): Conv_56 + Relu_57, Tactic: 0xdc1c841ef1cd3e8e, onnx::Conv_401[Half(-2,256,28,28)] -> onnx::Conv_404[Half(-2,256,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_58, Tactic: 0x2a3615ad33745f0b, onnx::Conv_404[Half(-2,256,14,14)] -> onnx::Add_574[Half(-2,1024,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_59 + Add_60 + Relu_61, Tactic: 0x2a3615ad33745f0b, input.184[Half(-2,512,28,28)], onnx::Add_574[Half(-2,1024,14,14)] -> input.212[Half(-2,1024,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_62 + Relu_63, Tactic: 0x21739cdb4c6113ed, input.212[Half(-2,1024,14,14)] -> onnx::Conv_413[Half(-2,256,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_64 + Relu_65, Tactic: 0x21739cdb4c6113ed, onnx::Conv_413[Half(-2,256,14,14)] -> onnx::Conv_416[Half(-2,256,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_66 + Add_67 + Relu_68, Tactic: 0x2a3615ad33745f0b, onnx::Conv_416[Half(-2,256,14,14)], input.212[Half(-2,1024,14,14)] -> input.236[Half(-2,1024,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_69 + Relu_70, Tactic: 0x21739cdb4c6113ed, input.236[Half(-2,1024,14,14)] -> onnx::Conv_423[Half(-2,256,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_71 + Relu_72, Tactic: 0x21739cdb4c6113ed, onnx::Conv_423[Half(-2,256,14,14)] -> onnx::Conv_426[Half(-2,256,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_73 + Add_74 + Relu_75, Tactic: 0x2a3615ad33745f0b, onnx::Conv_426[Half(-2,256,14,14)], input.236[Half(-2,1024,14,14)] -> input.260[Half(-2,1024,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_76 + Relu_77, Tactic: 0x21739cdb4c6113ed, input.260[Half(-2,1024,14,14)] -> onnx::Conv_433[Half(-2,256,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_78 + Relu_79, Tactic: 0x21739cdb4c6113ed, onnx::Conv_433[Half(-2,256,14,14)] -> onnx::Conv_436[Half(-2,256,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_80 + Add_81 + Relu_82, Tactic: 0x2a3615ad33745f0b, onnx::Conv_436[Half(-2,256,14,14)], input.260[Half(-2,1024,14,14)] -> input.284[Half(-2,1024,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_83 + Relu_84, Tactic: 0x21739cdb4c6113ed, input.284[Half(-2,1024,14,14)] -> onnx::Conv_443[Half(-2,256,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_85 + Relu_86, Tactic: 0x21739cdb4c6113ed, onnx::Conv_443[Half(-2,256,14,14)] -> onnx::Conv_446[Half(-2,256,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_87 + Add_88 + Relu_89, Tactic: 0x2a3615ad33745f0b, onnx::Conv_446[Half(-2,256,14,14)], input.284[Half(-2,1024,14,14)] -> input.308[Half(-2,1024,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_90 + Relu_91, Tactic: 0x21739cdb4c6113ed, input.308[Half(-2,1024,14,14)] -> onnx::Conv_453[Half(-2,256,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_92 + Relu_93, Tactic: 0x21739cdb4c6113ed, onnx::Conv_453[Half(-2,256,14,14)] -> onnx::Conv_456[Half(-2,256,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_94 + Add_95 + Relu_96, Tactic: 0x2a3615ad33745f0b, onnx::Conv_456[Half(-2,256,14,14)], input.308[Half(-2,1024,14,14)] -> input.332[Half(-2,1024,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_97 + Relu_98, Tactic: 0x21739cdb4c6113ed, input.332[Half(-2,1024,14,14)] -> onnx::Conv_463[Half(-2,512,14,14)]\r\n",
      "Layer(CaskConvolution): Conv_99 + Relu_100, Tactic: 0x4e34a65090c3b86f, onnx::Conv_463[Half(-2,512,14,14)] -> onnx::Conv_466[Half(-2,512,7,7)]\r\n",
      "Layer(CaskConvolution): Conv_101, Tactic: 0x2a3615ad33745f0b, onnx::Conv_466[Half(-2,512,7,7)] -> onnx::Add_631[Half(-2,2048,7,7)]\r\n",
      "Layer(CaskConvolution): Conv_102 + Add_103 + Relu_104, Tactic: 0xfcd06da0f3c31fd1, input.332[Half(-2,1024,14,14)], onnx::Add_631[Half(-2,2048,7,7)] -> input.360[Half(-2,2048,7,7)]\r\n",
      "Layer(CaskConvolution): Conv_105 + Relu_106, Tactic: 0x105f56cf03ee5549, input.360[Half(-2,2048,7,7)] -> onnx::Conv_475[Half(-2,512,7,7)]\r\n",
      "Layer(CaskConvolution): Conv_107 + Relu_108, Tactic: 0xfcd06da0f3c31fd1, onnx::Conv_475[Half(-2,512,7,7)] -> onnx::Conv_478[Half(-2,512,7,7)]\r\n",
      "Layer(CaskConvolution): Conv_109 + Add_110 + Relu_111, Tactic: 0x2a3615ad33745f0b, onnx::Conv_478[Half(-2,512,7,7)], input.360[Half(-2,2048,7,7)] -> input.384[Half(-2,2048,7,7)]\r\n",
      "Layer(CaskConvolution): Conv_112 + Relu_113, Tactic: 0x105f56cf03ee5549, input.384[Half(-2,2048,7,7)] -> onnx::Conv_485[Half(-2,512,7,7)]\r\n",
      "Layer(CaskConvolution): Conv_114 + Relu_115, Tacti[10/c: 0xfcd06da0f3c31fd1, onnx::Conv_485[Half(-2,512,7,7)] -> onnx::Conv_488[Half(-2,512,7,7)]\r\n",
      "Layer(CaskConvolution): Conv_116 + Add_117 + Relu_118, Tactic: 0x2a3615ad33745f0b, onnx::Conv_488[Half(-2,512,7,7)], input.384[Half(-2,2048,7,7)] -> input.408[Half(-2,2048,7,7)]\r\n",
      "Layer(CudaPooling): GlobalAveragePool_119, Tactic: 0xfffffffffffffffe, input.408[Half(-2,2048,7,7)] -> onnx::Flatten_493[Half(-2,2048,1,1)]\r\n",
      "Layer(CaskConvolution): Gemm_121, Tactic: 0xe67db95e0c20b618, onnx::Flatten_493[Half(-2,2048,1,1)] -> Gemm_121_out_region[Half(-2,1000,1,1)]\r\n",
      "Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to reshape_after_Gemm_121, Tactic: 0x0000000000000000, Gemm_121_out_region[Half(-2,1000,1,1)] -> Reformatted Input Tensor 0 to reshape_after_Gemm_121[Float(-2,1000,1,1)]\r\n",
      "Layer(NoOp): reshape_after_Gemm_121, Tactic: 0x0000000000000000, Reformatted Input Tensor 0 to reshape_after_Gemm_121[Float(-2,1000,1,1)] -> output[Float(-2,1000)]\r\n",
      "[10/18/2022-15:56:13] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +48, GPU +49, now: CPU 48, GPU 49 (MiB)\r\n",
      "18/2022-15:56:13] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/18/2022-15:56:13] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[10/18/2022-15:56:13] [I] Engine built in 155.171 sec.\n",
      "[10/18/2022-15:56:13] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1532, GPU 721 (MiB)\n",
      "[10/18/2022-15:56:13] [I] [TRT] Loaded engine size: 49 MiB\n",
      "[10/18/2022-15:56:13] [V] [TRT] Using cuDNN as a tactic source\n",
      "[10/18/2022-15:56:13] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 1533, GPU 781 (MiB)\n",
      "[10/18/2022-15:56:13] [V] [TRT] Deserialization required 21243 microseconds.\n",
      "[10/18/2022-15:56:13] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +48, now: CPU 0, GPU 48 (MiB)\n",
      "[10/18/2022-15:56:13] [I] Engine deserialized in 0.0237612 sec.\n",
      "[10/18/2022-15:56:13] [V] [TRT] Using cuDNN as a tactic source\n",
      "[10/18/2022-15:56:13] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1533, GPU 781 (MiB)\n",
      "[10/18/2022-15:56:13] [V] [TRT] Total per-runner device persistent memory is 160768\n",
      "[10/18/2022-15:56:13] [V] [TRT] Total per-runner host persistent memory is 135120\n",
      "[10/18/2022-15:56:13] [V] [TRT] Allocated activation device memory of size 513802240\n",
      "[10/18/2022-15:56:13] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +491, now: CPU 0, GPU 539 (MiB)\n",
      "[10/18/2022-15:56:13] [I] Using random values for input input\n",
      "[10/18/2022-15:56:13] [I] Created input binding for input with dimensions 128x3x224x224\n",
      "[10/18/2022-15:56:13] [I] Using random values for output output\n",
      "[10/18/2022-15:56:13] [I] Created output binding for output with dimensions 128x1000\n",
      "[10/18/2022-15:56:13] [I] Starting inference\n",
      "[10/18/2022-15:56:16] [I] Warmup completed 4 queries over 200 ms\n",
      "[10/18/2022-15:56:16] [I] Timing trace has 57 queries over 3.15905 s\n",
      "[10/18/2022-15:56:16] [I] \n",
      "[10/18/2022-15:56:16] [I] === Trace details ===\n",
      "[10/18/2022-15:56:16] [I] Trace averages of 10 runs:\n",
      "[10/18/2022-15:56:16] [I] Average on 10 runs - GPU latency: 54.3593 ms - Host latency: 66.8311 ms (enqueue 0.275088 ms)\n",
      "[10/18/2022-15:56:16] [I] Average on 10 runs - GPU latency: 55.0979 ms - Host latency: 67.5703 ms (enqueue 0.267462 ms)\n",
      "[10/18/2022-15:56:16] [I] Average on 10 runs - GPU latency: 54.3358 ms - Host latency: 66.8071 ms (enqueue 0.252881 ms)\n",
      "[10/18/2022-15:56:16] [I] Average on 10 runs - GPU latency: 54.347 ms - Host latency: 66.8195 ms (enqueue 0.252209 ms)\n",
      "[10/18/2022-15:56:16] [I] Average on 10 runs - GPU latency: 54.3635 ms - Host latency: 66.8364 ms (enqueue 0.254077 ms)\n",
      "[10/18/2022-15:56:16] [I] \n",
      "[10/18/2022-15:56:16] [I] === Performance summary ===\n",
      "[10/18/2022-15:56:16] [I] Throughput: 18.0434 qps\n",
      "[10/18/2022-15:56:16] [I] Latency: min = 66.167 ms, max = 73.0646 ms, mean = 66.9414 ms, median = 66.7378 ms, percentile(99%) = 73.0646 ms\n",
      "[10/18/2022-15:56:16] [I] Enqueue Time: min = 0.229248 ms, max = 0.314896 ms, mean = 0.259082 ms, median = 0.256958 ms, percentile(99%) = 0.314896 ms\n",
      "[10/18/2022-15:56:16] [I] H2D Latency: min = 12.3854 ms, max = 12.4011 ms, mean = 12.3889 ms, median = 12.3883 ms, percentile(99%) = 12.4011 ms\n",
      "[10/18/2022-15:56:16] [I] GPU Compute Time: min = 53.697 ms, max = 60.5947 ms, mean = 54.4694 ms, median = 54.2679 ms, percentile(99%) = 60.5947 ms\n",
      "[10/18/2022-15:56:16] [I] D2H Latency: min = 0.0810547 ms, max = 0.0847168 ms, mean = 0.0830742 ms, median = 0.0827942 ms, percentile(99%) = 0.0847168 ms\n",
      "[10/18/2022-15:56:16] [I] Total Host Walltime: 3.15905 s\n",
      "[10/18/2022-15:56:16] [I] Total GPU Compute Time: 3.10475 s\n",
      "[10/18/2022-15:56:16] [W] * GPU compute time is unstable, with coefficient of variance = 1.59734%.\n",
      "[10/18/2022-15:56:16] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
      "[10/18/2022-15:56:16] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[10/18/2022-15:56:16] [V] \n",
      "[10/18/2022-15:56:16] [V] === Explanations of the performance metrics ===\n",
      "[10/18/2022-15:56:16] [V] Total Host Walltime: the host walltime from when the first query (after warmups) is enqueued to when the last query is completed.\n",
      "[10/18/2022-15:56:16] [V] GPU Compute Time: the GPU latency to execute the kernels for a query.\n",
      "[10/18/2022-15:56:16] [V] Total GPU Compute Time: the summation of the GPU Compute Time of all the queries. If this is significantly shorter than Total Host Walltime, the GPU may be under-utilized because of host-side overheads or data transfers.\n",
      "[10/18/2022-15:56:16] [V] Throughput: the observed throughput computed by dividing the number of queries by the Total Host Walltime. If this is significantly lower than the reciprocal of GPU Compute Time, the GPU may be under-utilized because of host-side overheads or data transfers.\n",
      "[10/18/2022-15:56:16] [V] Enqueue Time: the host latency to enqueue a query. If this is longer than GPU Compute Time, the GPU may be under-utilized.\n",
      "[10/18/2022-15:56:16] [V] H2D Latency: the latency for host-to-device data transfers for input tensors of a single query.\n",
      "[10/18/2022-15:56:16] [V] D2H Latency: the latency for device-to-host data transfers for output tensors of a single query.\n",
      "[10/18/2022-15:56:16] [V] Latency: the summation of H2D Latency, GPU Compute Time, and D2H Latency. This is the latency to infer a single query.\n",
      "[10/18/2022-15:56:16] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8401] # trtexec --onnx=model.onnx --saveEngine=model.plan --explicitBatch --minShapes=input:1x3x224x224 --optShapes=input:128x3x224x224 --maxShapes=input:128x3x224x224 --fp16 --verbose\n"
     ]
    }
   ],
   "source": [
    "!docker run --gpus=all --rm -it \\\n",
    "            -v `pwd`/workspace:/workspace nvcr.io/nvidia/pytorch:22.07-py3 \\\n",
    "            /bin/bash generate_model_trt.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289b3a4",
   "metadata": {},
   "source": [
    "#### TensorRT Model Respository\n",
    "\n",
    "The model repository contains model to serve, for TensorRT model it will be the model.plan(created in above steps) and configuration file with input/output specifications and metadata.\n",
    "\n",
    "```\n",
    "resnet\n",
    "â”œâ”€â”€ 1\n",
    "â”‚   â””â”€â”€ model.plan\n",
    "â””â”€â”€ config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f5e332",
   "metadata": {},
   "source": [
    "#### TensorRT Model configuration\n",
    "\n",
    " For the TensorRT model, we specify tensorrt_plan as platform, input tensor specification of the image of dimension 224X224 which has 3 color channels. Output tensor with 1000 dimensions of type TYPE_FP32 corresponding the different object categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6288caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-trt/resnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cc8bbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing triton-serve-trt/resnet/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile triton-serve-trt/resnet/config.pbtxt\n",
    "name: \"resnet\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 128\n",
    "input {\n",
    "  name: \"input\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: 3\n",
    "  dims: 224\n",
    "  dims: 224\n",
    "}\n",
    "output {\n",
    "  name: \"output\"\n",
    "  data_type: TYPE_FP32\n",
    "  dims: 1000\n",
    "}\n",
    "model_warmup {\n",
    "    name: \"bs128 Warmup\"\n",
    "    batch_size: 128\n",
    "    inputs: {\n",
    "        key: \"input\"\n",
    "        value: {\n",
    "            data_type: TYPE_FP32\n",
    "            dims: 3\n",
    "            dims: 224\n",
    "            dims: 224\n",
    "            zero_data: false\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63b87b",
   "metadata": {},
   "source": [
    "#### 3. Export model artifacts to S3\n",
    "\n",
    "SageMaker expects the model artifacts in below format, it should also satisfy Triton container requirements such as model name, version, config.pbtxt files etc. `tar` the folder containing the model file as `model.tar.gz` and upload it to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb9ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-pt/resnet/1/\n",
    "!mv -f workspace/model.pt triton-serve-pt/resnet/1/\n",
    "!tar -C triton-serve-pt/ -czf resnet_pt_v0.tar.gz resnet\n",
    "model_uri_pt = sagemaker_session.upload_data(path=\"resnet_pt_v0.tar.gz\", key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be643ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-trt/resnet/1/\n",
    "!mv -f workspace/model.plan triton-serve-trt/resnet/1/\n",
    "!tar -C triton-serve-trt/ -czf resnet_trt_v0.tar.gz resnet\n",
    "model_uri_trt = sagemaker_session.upload_data(path=\"resnet_trt_v0.tar.gz\", key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b0454",
   "metadata": {},
   "source": [
    "Now that we have uploaded the model artifacts to S3, we can create a SageMaker multi-model endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae922c9",
   "metadata": {},
   "source": [
    "#### Deploy Models with MME\n",
    "\n",
    "We will now deploy ResNet-50 model with 2 different framework backends i.e. PyTorch and TensorRT to SageMaker MME. You can reproduce all the steps using step by step notebook on GitHub.\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Note </strong>\n",
    "you can deploy 100s of models. The models can use same framework. They can also use different frameworks as shown in this note.\n",
    "</div>\n",
    "\n",
    "We will use AWS SDK for Python (Boto) APIs [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model), [create_endpoint_config](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config) and [create_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint) to create a mulit-model endpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d724ef",
   "metadata": {},
   "source": [
    "#### Define the serving container "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467fa947",
   "metadata": {},
   "source": [
    " In the container definition, define the `ModelDataUrl` to specify the S3 directory that contains all the models that SageMaker multi-model endpoint will use to load  and serve predictions. Set `Mode` to `MultiModel` to indicates SageMaker would create the endpoint with MME container specifications. We set the container with an image that supports deploying multi-model endpoints with GPU, see MME container images for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44ee9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data_url,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"resnet\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4117289",
   "metadata": {},
   "source": [
    "#### Create a multi model object\n",
    "\n",
    "Using the SageMaker boto3 client, create the model using [create_model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model) API. We will pass the container definition to the create model API along with ModelName and ExecutionRoleArn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fa435d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:ap-south-1:917092859813:model/resnet-mme-gpu-mdl-2022-10-18-15-53-25\n"
     ]
    }
   ],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b526e21",
   "metadata": {},
   "source": [
    "#### Define configuration for the multi model endpoint\n",
    "\n",
    "Create a multi-model endpoint configurations using [create_endpoint_config](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config) boto3 API. Specify an accelerated GPU computing instance in InstanceType, in this post we will use g4dn.4xlarge instance. We recommend configuring your endpoints with at least two instances. This allows SageMaker to provide a highly available set of predictions across multiple Availability Zones for the models.\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Note </strong>\n",
    "Based on our findings, customers get price performance on ML optimized instances with single GPU core. Hence, this feature is only enabled for single GPU core instances. For full list of instances supported see this (http://LINK to Docs page where we capture list of isntances.)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2935290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:ap-south-1:917092859813:endpoint-config/resnet-mme-gpu-epc-2022-10-18-15-53-25\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1807bbde",
   "metadata": {},
   "source": [
    "#### Create Multi Model Endpoint\n",
    "\n",
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b08fa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:ap-south-1:917092859813:endpoint/resnet-mme-gpu-ep-2022-10-18-15-53-25\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38448b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:ap-south-1:917092859813:endpoint/resnet-mme-gpu-ep-2022-10-18-15-53-25\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81840474",
   "metadata": {},
   "source": [
    "#### Setup Autoscaling policies for GPU Multi Model Endpoint\n",
    "\n",
    "Amazon SageMaker multi-model endpoints supports automatic scaling (auto scaling) for your hosted models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, auto scaling brings more instances online. When the workload decreases, auto scaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.\n",
    "\n",
    "In the below scaling policy, use a custom metric GPUMemoryUtilization in TargetTrackingScalingPolicyConfiguration configuration and set a TargetValue of 75.0 for the target value of that metric. This autoscaling policy will provision additional instances upto MaxCapacity when GPU Utilization is more than 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bc42a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform auto-scaling of the endpoint based on GPU memory utilization\n",
    "\n",
    "# This is the format in which application autoscaling references the endpoint\n",
    "resource_id = \"endpoint/\" + endpoint_name + \"/variant/\" + \"AllTraffic\"\n",
    "response = auto_scaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=5,\n",
    ")\n",
    "\n",
    "\n",
    "# GPUMemoryUtilization metric\n",
    "response = auto_scaling_client.put_scaling_policy(\n",
    "    PolicyName=\"GPUMemoryUtil-ScalingPolicy\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n",
    "    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        # Scale out when GPU Memory utilization hits GPUMemoryUtilization target value.\n",
    "        \"TargetValue\": 60.0,\n",
    "        \"CustomizedMetricSpecification\": {\n",
    "            \"MetricName\": \"GPUMemoryUtilization\",\n",
    "            \"Namespace\": \"/aws/sagemaker/Endpoints\",\n",
    "            \"Dimensions\": [\n",
    "                {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                {\"Name\": \"VariantName\", \"Value\": \"AllTraffic\"},\n",
    "            ],\n",
    "            \"Statistic\": \"Average\",  # Possible - 'Statistic': 'Average'|'Minimum'|'Maximum'|'SampleCount'|'Sum'\n",
    "            \"Unit\": \"Percent\",\n",
    "        },\n",
    "        \"ScaleInCooldown\": 600,\n",
    "        \"ScaleOutCooldown\": 200,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ddac3",
   "metadata": {},
   "source": [
    "#### Prepare Input Payload for PyTorch and TensorRT model\n",
    "\n",
    "The following method transforms a sample image we will be using for inference into the payload that can be sent for inference to the Triton server.\n",
    "\n",
    "The `tritonclient` package provides utility methods to generate the payload without having to know the details of the specification. We'll use the following methods to convert our inference request into a binary format which provides lower latencies for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f474aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.download_file(\n",
    "    \"sagemaker-sample-files\", \"datasets/image/pets/shiba_inu_dog.jpg\", \"shiba_inu_dog.jpg\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_sample_image():\n",
    "    image_path = \"./shiba_inu_dog.jpg\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = img.resize((224, 224))\n",
    "    img = (np.array(img).astype(np.float32) / 255) - np.array(\n",
    "        [0.485, 0.456, 0.406], dtype=np.float32\n",
    "    ).reshape(1, 1, 3)\n",
    "    img = img / np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 1, 3)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return img.tolist()\n",
    "\n",
    "\n",
    "def _get_sample_image_binary(input_name, output_name):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    inputs.append(httpclient.InferInput(input_name, [1, 3, 224, 224], \"FP32\"))\n",
    "    input_data = np.array(get_sample_image(), dtype=np.float32)\n",
    "    input_data = np.expand_dims(input_data, axis=0)\n",
    "    inputs[0].set_data_from_numpy(input_data, binary_data=True)\n",
    "    outputs.append(httpclient.InferRequestedOutput(output_name, binary_data=True))\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length\n",
    "\n",
    "\n",
    "def get_sample_image_binary_pt():\n",
    "    return _get_sample_image_binary(\"INPUT__0\", \"OUTPUT__0\")\n",
    "\n",
    "\n",
    "def get_sample_image_binary_trt():\n",
    "    return _get_sample_image_binary(\"input\", \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f9a245",
   "metadata": {},
   "source": [
    "#### Invoke target model on Multi Model Endpoint\n",
    "\n",
    "Once the endpoint is successfully created, we can send inference request to multi-model endpoint using invoke_enpoint API. We specify the TargetModel in the invocation call and pass in the payload for each model type. Sample invocation for PyTorch model and TensorRT model is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f95274",
   "metadata": {},
   "source": [
    "#### TensorRT model prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f69d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorRT payload\n",
    "trt_payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"input\", \"shape\": [1, 3, 224, 224], \"datatype\": \"FP32\", \"data\": get_sample_image()}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c46a2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': '63dcc8cdd42bd73f53f862a664293a79', 'model_version': '1', 'outputs': [{'name': 'output', 'datatype': 'FP32', 'shape': [1, 1000], 'data': [-1.2470703125, 0.25146484375, -1.728515625, -2.984375, -3.009765625, -1.9609375, -2.501953125, -0.38134765625, 0.036956787109375, -1.6005859375, -3.86328125, -1.9638671875, -3.634765625, -5.375, -1.693359375, -2.912109375, -1.853515625, 0.3662109375, -0.6767578125, -1.7333984375, -4.43359375, -2.33203125, -3.107421875, -2.234375, -2.123046875, -3.501953125, -4.80859375, -2.873046875, -3.04296875, -3.12890625, -2.470703125, -0.42138671875, -2.35546875, -3.576171875, -0.78271484375, -4.30078125, -1.8388671875, -4.4921875, -2.84765625, -2.203125, -0.2047119140625, -3.90625, -0.91748046875, -2.98828125, -2.130859375, -3.94140625, -1.3408203125, -2.376953125, -5.25, -2.671875, -1.326171875, -1.0224609375, -1.2783203125, -1.4814453125, -1.759765625, -0.440673828125, -0.19482421875, -2.99609375, -2.80859375, -1.3681640625, -0.900390625, -1.109375, -3.470703125, -2.201171875, -2.5703125, -3.236328125, -1.71875, -1.572265625, -2.4453125, -4.0859375, -2.888671875, -1.9599609375, -2.8203125, -3.267578125, -1.5693359375, -4.06640625, -2.208984375, -1.2060546875, -1.25, -0.213134765625, -3.45703125, -5.69140625, -1.421875, -3.03515625, -0.943359375, -2.6875, -1.6845703125, -0.68896484375, 0.5888671875, -0.85107421875, -2.486328125, -4.859375, -1.4462890625, -1.736328125, -0.46142578125, -4.53515625, -2.68359375, -0.09820556640625, -1.154296875, -0.1309814453125, -0.384033203125, -2.912109375, -2.517578125, -4.1171875, 1.212890625, -1.40234375, -2.220703125, -3.515625, 0.1217041015625, -3.341796875, -1.443359375, -1.6845703125, 1.255859375, 0.278564453125, -1.220703125, -1.25, -3.01171875, -1.8857421875, -0.62060546875, -2.09375, -1.408203125, -1.3984375, -0.69384765625, -1.7021484375, -1.4287109375, -2.138671875, -0.8310546875, -0.59375, -4.41015625, -3.38671875, -0.400146484375, -4.16015625, -1.4716796875, -3.556640625, -0.27587890625, -2.630859375, -2.64453125, -2.560546875, -3.056640625, -3.4140625, -4.48828125, -3.490234375, -3.5859375, -4.1484375, -0.408447265625, -1.1943359375, -3.97265625, -3.91796875, -3.279296875, -4.546875, -3.75, 4.47265625, 2.8125, 0.459228515625, 2.92578125, 1.119140625, 2.185546875, 2.8828125, 2.658203125, 3.037109375, 0.75830078125, 1.0341796875, 3.7421875, 1.8154296875, 0.8232421875, -0.74560546875, 1.4208984375, 0.90966796875, 4.1328125, 1.7041015625, -0.79296875, 1.5126953125, 0.495361328125, 3.015625, 8.40625, 1.2412109375, 4.00390625, -1.5302734375, -0.50830078125, 2.076171875, 3.0234375, 0.849609375, 1.0517578125, 0.1561279296875, 2.564453125, 1.693359375, 4.80859375, 0.467529296875, 1.734375, 1.8662109375, -3.720703125, -0.17041015625, 0.61083984375, 3.39453125, -0.966796875, 2.240234375, 0.54248046875, 0.336181640625, -0.51708984375, -0.2802734375, 5.04296875, 1.0673828125, 0.6669921875, 1.060546875, 2.875, -0.245849609375, 0.261962890625, 5.765625, 2.73046875, 2.626953125, -3.716796875, 0.90966796875, -0.66357421875, 1.634765625, -2.322265625, 2.802734375, -1.0849609375, -1.31640625, 1.1025390625, 0.794921875, 0.287109375, -0.51171875, 5.67578125, 5.9921875, 4.6015625, 4.703125, 2.38671875, 5.32421875, 0.77490234375, 1.212890625, 6.1640625, 7.50390625, 4.69140625, 0.85693359375, 2.482421875, 5.60546875, 1.4013671875, 1.390625, 3.77734375, 3.513671875, 5.25390625, 2.490234375, 1.115234375, 1.8720703125, 7.1171875, 1.4970703125, 0.87353515625, 5.828125, 11.0390625, 11.109375, 10.859375, 2.126953125, -0.716796875, 10.4921875, 3.623046875, 4.10546875, 3.845703125, 4.82421875, 8.140625, 6.1640625, 10.125, 4.45703125, 1.7783203125, 8.390625, 6.30078125, 3.84375, 2.802734375, 4.17578125, -1.873046875, 3.84375, 3.91796875, 4.84765625, 2.931640625, 7.71875, 3.140625, -2.669921875, -2.48046875, 1.0234375, 0.099853515625, -1.4091796875, 0.10296630859375, 2.62890625, 3.810546875, 1.048828125, -0.171630859375, 2.27734375, -0.09027099609375, -1.408203125, -2.087890625, -4.1953125, -1.369140625, 1.5546875, 1.7041015625, 0.96728515625, -0.193603515625, -1.6591796875, -2.19140625, -3.041015625, -4.8671875, -2.70703125, -1.4267578125, -0.416015625, -1.412109375, -2.220703125, 0.431396484375, -1.228515625, -0.79638671875, -0.38671875, -2.185546875, 0.056671142578125, -0.397705078125, -0.849609375, -1.0625, -1.201171875, -0.74951171875, -3.525390625, -3.69140625, -0.12469482421875, -2.259765625, 1.0517578125, 2.4765625, -2.185546875, -2.404296875, 0.50390625, -2.376953125, -1.0341796875, -0.40185546875, 1.361328125, -0.055145263671875, -2.673828125, -0.9423828125, -0.5546875, 0.2509765625, -0.30224609375, -3.26953125, -2.28125, -4.3515625, -4.2421875, 0.409423828125, 6.25, 0.91943359375, -0.806640625, -2.212890625, -2.7890625, -4.50390625, -0.0830078125, -3.345703125, -1.7060546875, -0.044677734375, -1.87109375, -4.23046875, -4.15234375, -1.0400390625, 1.103515625, 0.55224609375, 1.7880859375, -0.89453125, -2.796875, -0.59130859375, 0.034820556640625, -4.4375, -0.0452880859375, -3.3984375, -2.185546875, -3.818359375, -2.794921875, -3.751953125, -5.61328125, 1.6220703125, -2.38671875, -3.865234375, -0.97607421875, -3.025390625, -2.642578125, -2.25390625, -3.34375, -2.263671875, -2.357421875, -3.60546875, -3.583984375, -3.798828125, -3.77734375, -1.822265625, -2.25390625, -0.1103515625, -3.384765625, -1.583984375, 1.2060546875, -1.267578125, -1.0107421875, -3.75, 0.173828125, -2.859375, -1.82421875, -1.853515625, -1.8994140625, -2.76171875, -3.474609375, 1.158203125, -0.9453125, -0.77392578125, 1.2666015625, -0.6279296875, -3.91796875, -3.94921875, -0.434326171875, -1.603515625, 1.40625, 0.1505126953125, 3.044921875, -1.6240234375, 1.693359375, 2.97265625, -1.3779296875, 4.08203125, -0.453369140625, -0.367919921875, 4.3671875, 4.51171875, 0.787109375, -0.4462890625, -0.254638671875, 1.470703125, -1.1123046875, -2.015625, -0.089599609375, 1.1376953125, 1.224609375, 2.947265625, 3.392578125, 0.0872802734375, 0.5673828125, 0.59912109375, 0.41552734375, 3.068359375, -0.24365234375, 0.1707763671875, -0.34521484375, -2.193359375, 2.193359375, -0.09637451171875, -0.97900390625, 2.234375, 0.85888671875, 0.810546875, 1.3671875, 0.409912109375, -0.83642578125, -0.173583984375, 0.5107421875, 0.2203369140625, -1.3212890625, 1.0029296875, 1.76953125, 0.92578125, -0.1756591796875, 0.477294921875, 1.5849609375, 0.34765625, 0.7587890625, -0.7255859375, -0.2371826171875, 2.61328125, 1.9912109375, -1.60546875, 1.568359375, -3.7578125, -3.1328125, 0.51416015625, 1.2998046875, 0.45361328125, -0.57958984375, 3.92578125, -2.52734375, 0.50732421875, -0.050872802734375, 0.73583984375, 0.6357421875, 2.419921875, 2.96484375, 0.80029296875, -0.2802734375, -0.2305908203125, 0.544921875, 1.0224609375, -0.74365234375, 0.266845703125, 1.78125, 0.5380859375, 2.865234375, 0.64111328125, -0.05609130859375, 0.01213836669921875, 0.80419921875, 0.229736328125, 0.10455322265625, 2.51171875, 1.40234375, -1.974609375, -2.556640625, -2.587890625, 2.61328125, 0.5869140625, -0.01177215576171875, 1.3271484375, 0.418212890625, -1.1142578125, -0.6416015625, 2.734375, -0.371826171875, -0.5732421875, 1.4462890625, -0.75732421875, -0.60302734375, -0.86474609375, 2.060546875, 0.52978515625, -0.0650634765625, 0.25390625, 0.7255859375, 1.185546875, 0.247802734375, 2.451171875, 1.7060546875, -1.1025390625, -1.6044921875, 0.89990234375, 0.40576171875, -0.0195159912109375, 1.2890625, 0.93798828125, 0.7548828125, -0.496826171875, 0.54541015625, -0.16015625, -3.515625, -0.50732421875, 5.40234375, -0.595703125, 3.646484375, -2.974609375, 0.496337890625, 0.8544921875, 2.77734375, 0.6240234375, 0.51611328125, -0.708984375, -2.69140625, 0.51611328125, -0.0859375, -2.962890625, -0.634765625, 1.7939453125, 1.23828125, -1.603515625, 0.3837890625, 0.07635498046875, 2.59375, 0.92236328125, 2.619140625, 3.8203125, -0.6328125, -1.1279296875, 1.1455078125, -1.0244140625, -0.837890625, 0.479248046875, -1.697265625, 2.068359375, -0.455810546875, -1.2939453125, -0.365966796875, 2.59375, -0.1796875, 4.2109375, 3.5625, -0.284423828125, 0.026702880859375, 0.931640625, 0.6015625, 1.7255859375, 0.70751953125, -0.7001953125, -0.497314453125, 0.6572265625, 0.13232421875, 0.4013671875, -1.4033203125, 2.5, 0.17578125, 0.4619140625, 1.681640625, -2.640625, -1.48046875, 0.2349853515625, -0.392333984375, 0.85400390625, 1.2001953125, -1.056640625, -0.70947265625, -0.6845703125, -0.492919921875, 2.13671875, 1.998046875, -0.305419921875, 0.75927734375, 0.38671875, 2.0859375, 1.7861328125, 2.298828125, 0.94873046875, 1.2861328125, -1.314453125, -1.5546875, 0.94970703125, 1.1435546875, 0.0849609375, -1.171875, -0.226806640625, 0.486572265625, 3.373046875, 3.23828125, 1.3486328125, 0.51220703125, 1.615234375, 0.40234375, 0.6220703125, -1.7998046875, -2.279296875, -1.0703125, 0.2274169921875, 0.471435546875, 0.47021484375, -1.755859375, -1.14453125, 0.5439453125, 2.24609375, 2.453125, 1.67578125, 1.271484375, -1.4833984375, 1.111328125, -1.6650390625, -0.77880859375, 0.92236328125, 1.7314453125, 1.4326171875, -1.51953125, -0.241943359375, -1.541015625, -1.923828125, -1.0771484375, 1.546875, 1.244140625, -1.0654296875, 1.890625, 1.6796875, -0.362060546875, 1.3427734375, -0.398193359375, -1.025390625, 1.0576171875, -2.14453125, -1.462890625, 0.81884765625, 1.30859375, -0.61962890625, 1.0517578125, -1.19921875, 0.308837890625, 1.2197265625, -0.1661376953125, 2.171875, 0.79736328125, -0.48095703125, -0.72802734375, 5.4375, -2.92578125, 0.1741943359375, 1.0908203125, 1.748046875, 3.873046875, 0.294677734375, 0.55078125, -0.708984375, -0.29833984375, -2.98046875, -1.4052734375, -2.15625, -0.71484375, 0.93994140625, -1.2119140625, 2.3046875, 2.63671875, -0.444091796875, -0.72900390625, 1.4052734375, 0.98876953125, 1.94140625, -0.59814453125, 2.31640625, 0.6328125, 0.2093505859375, 2.955078125, 1.3330078125, -1.2333984375, 3.25390625, 0.309814453125, -0.99755859375, 2.75, 2.953125, -0.16015625, -0.783203125, -0.92333984375, -0.019256591796875, -2.66015625, 3.037109375, 1.7431640625, 0.75048828125, 0.478515625, -0.54443359375, 1.283203125, 2.111328125, 2.189453125, -1.06640625, -0.0478515625, -2.642578125, -1.3779296875, 1.1337890625, -1.2314453125, 0.552734375, 1.7373046875, -0.626953125, 1.8642578125, 0.093994140625, 1.943359375, 1.8037109375, 0.04058837890625, -0.2010498046875, -4.203125, -1.328125, 2.19140625, 0.253662109375, -0.384033203125, -1.3662109375, -2.19921875, 0.07537841796875, 0.86083984375, 2.703125, 1.04296875, 3.125, 1.0419921875, 2.720703125, -0.339111328125, -0.54052734375, 0.90087890625, 3.400390625, -0.2288818359375, 0.08123779296875, -0.1295166015625, -0.30859375, 1.44921875, 0.3720703125, 0.42919921875, 1.2236328125, 3.1875, -0.9814453125, 2.142578125, 1.8583984375, 0.53564453125, 2.13671875, 0.06768798828125, 2.57421875, 2.078125, -0.0230255126953125, 1.1630859375, -0.703125, -0.498291015625, -0.96142578125, -2.62890625, -1.150390625, 0.90869140625, 0.51904296875, -0.93310546875, 0.61767578125, 2.5625, -0.74560546875, 2.794921875, 1.42578125, 1.7626953125, 1.640625, 2.013671875, 1.54296875, -0.66015625, 0.94580078125, -0.2203369140625, -0.040802001953125, 3.412109375, 0.806640625, 2.740234375, -1.185546875, -0.67626953125, -1.009765625, -0.049835205078125, 0.55419921875, 6.10546875, 0.414306640625, -0.38818359375, -0.56640625, 0.68408203125, 2.6015625, 1.1748046875, -2.5546875, 0.501953125, -0.74609375, -0.68603515625, -0.69970703125, 1.1884765625, -1.802734375, -0.4326171875, -0.7001953125, -2.029296875, -2.86328125, 1.03515625, 4.01171875, 0.210693359375, 3.330078125, -0.1307373046875, 0.75390625, -1.560546875, -0.69287109375, 1.4345703125, -1.421875, -2.826171875, -0.5771484375, -0.27783203125, 0.143310546875, 0.71630859375, 0.23193359375, 0.431884765625, 0.24365234375, 2.423828125, 2.384765625, 2.68359375, -1.6630859375, 0.564453125, 0.09112548828125, -1.326171875, -0.08837890625, 1.2822265625, 4.65625, 0.39794921875, 4.984375, 0.453857421875, 0.5078125, -0.294921875, -0.9306640625, 0.363037109375, 1.1943359375, -1.603515625, -1.39453125, 0.62451171875, 0.6513671875, -0.09393310546875, 0.84619140625, 2.919921875, 0.865234375, -1.1279296875, -0.0130767822265625, -0.051544189453125, 2.251953125, 2.173828125, 1.8916015625, 1.02734375, -3.462890625, 0.299560546875, 1.453125, -1.0400390625, 0.487060546875, 3.47265625, 1.08984375, 1.16796875, 1.837890625, 0.12237548828125, -0.43603515625, 2.91015625, -1.6845703125, -2.345703125, -0.63134765625, 0.5927734375, 1.4541015625, -1.6962890625, 2.2890625, 3.388671875, -0.11456298828125, -2.560546875, -1.4541015625, -1.791015625, 4.62890625, 0.46875, 1.5380859375, -1.3681640625, 3.41015625, 1.400390625, 3.4453125, 1.3115234375, -2.783203125, -0.2166748046875, -0.1966552734375, -1.376953125, 1.9892578125, 1.529296875, 1.9521484375, -0.779296875, -0.283447265625, 0.689453125, 1.421875, 0.1468505859375, 1.9189453125, -0.78173828125, -1.501953125, 1.5517578125, -0.736328125, -1.5126953125, 1.2890625, -1.6845703125, -3.1640625, -2.57421875, -1.373046875, 3.146484375, -1.587890625, 0.1588134765625, 0.59326171875, -0.6044921875, 1.412109375, -1.6591796875, -2.2265625, 0.880859375, -0.370849609375, 1.0478515625, -1.7421875, -1.724609375, -1.4267578125, 0.1583251953125, -1.658203125, -0.61328125, -1.3564453125, -0.5361328125, 1.375, -0.6328125, 0.21044921875, -1.20703125, -0.77392578125, -0.1302490234375, -0.2210693359375, -2.041015625, -2.341796875, 0.00318145751953125, 0.31396484375, -1.9033203125, -1.2177734375, -1.83203125, -2.267578125, -1.109375, -2.814453125, -1.4658203125, 0.23779296875, -1.0048828125, 1.4169921875, -0.7041015625, -0.58447265625, 1.4140625, -0.80810546875, -0.75732421875, -3.4609375, 3.4296875, -2.146484375, 1.5712890625, -0.478271484375, -1.1728515625, -2.21484375, 3.294921875, -2.03125, -0.6552734375, -0.31982421875, -0.52197265625, -3.314453125, 0.927734375, -0.298828125, -1.8017578125, -0.9560546875, -1.271484375, -0.396728515625, -2.974609375, -2.8828125, 0.222900390625, -1.994140625, -3.595703125, 0.369384765625, 2.6953125]}]}\n"
     ]
    }
   ],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(trt_payload),\n",
    "    TargetModel=\"resnet_trt_v0.tar.gz\",\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b7852",
   "metadata": {},
   "source": [
    "#### PyTorch Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f9ed482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch payload\n",
    "pt_payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"shape\": [1, 3, 224, 224],\n",
    "            \"datatype\": \"FP32\",\n",
    "            \"data\": get_sample_image(),\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d62d618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.242385983467102, 0.24701948463916779, -1.7194457054138184, -2.9837756156921387, -3.003139019012451, -1.9556491374969482, -2.4896509647369385, -0.3748360574245453, 0.04311799630522728, -1.6037157773971558, -3.8769567012786865, -1.9742563962936401, -3.648282766342163, -5.388746738433838, -1.6986432075500488, -2.915553569793701, -1.8570733070373535, 0.3685520887374878, -0.6721057891845703, -1.7402963638305664, -4.448188781738281, -2.3320815563201904, -3.112416982650757, -2.239777088165283, -2.1238701343536377, -3.492257833480835, -4.819538593292236, -2.875951051712036, -3.056042432785034, -3.1336770057678223, -2.461475133895874, -0.42221006751060486, -2.3585634231567383, -3.5724363327026367, -0.7773784399032593, -4.2943854331970215, -1.8330132961273193, -4.481720447540283, -2.8400700092315674, -2.1916580200195312, -0.20150895416736603, -3.9115798473358154, -0.9050751328468323, -2.9791600704193115, -2.123189687728882, -3.932497024536133, -1.3375135660171509, -2.37469744682312, -5.249118328094482, -2.6639719009399414, -1.3220094442367554, -1.024010181427002, -1.279827356338501, -1.4837520122528076, -1.7604997158050537, -0.44506263732910156, -0.19828946888446808, -2.9953091144561768, -2.812901020050049, -1.3711812496185303, -0.9056181907653809, -1.1093052625656128, -3.472838878631592, -2.2078096866607666, -2.5746278762817383, -3.2453720569610596, -1.7209901809692383, -1.5765990018844604, -2.4470207691192627, -4.086050033569336, -2.8952887058258057, -1.9573582410812378, -2.8241307735443115, -3.2803890705108643, -1.5759892463684082, -4.0739006996154785, -2.207242488861084, -1.206960678100586, -1.2588697671890259, -0.21239541471004486, -3.4536890983581543, -5.687176704406738, -1.407383918762207, -3.0367417335510254, -0.9377632141113281, -2.680985450744629, -1.675015926361084, -0.6902233958244324, 0.5914162993431091, -0.8433008193969727, -2.4826250076293945, -4.851303577423096, -1.4487847089767456, -1.7387944459915161, -0.46680453419685364, -4.541266441345215, -2.670036792755127, -0.08718514442443848, -1.1567070484161377, -0.1274050772190094, -0.38829630613327026, -2.8992044925689697, -2.512021541595459, -4.113963603973389, 1.2109885215759277, -1.4036356210708618, -2.207685947418213, -3.528992176055908, 0.12620505690574646, -3.3343400955200195, -1.449754238128662, -1.695319414138794, 1.2576106786727905, 0.28074559569358826, -1.2216922044754028, -1.2502566576004028, -3.0089874267578125, -1.8817261457443237, -0.6232070326805115, -2.0893330574035645, -1.41242516040802, -1.4048112630844116, -0.6954542398452759, -1.6978024244308472, -1.4307976961135864, -2.147010564804077, -0.8399057388305664, -0.5957845449447632, -4.41629695892334, -3.3890483379364014, -0.400394082069397, -4.153246879577637, -1.4695103168487549, -3.5651309490203857, -0.27520447969436646, -2.632575750350952, -2.6370224952697754, -2.5659048557281494, -3.039867877960205, -3.4027249813079834, -4.482636451721191, -3.4936177730560303, -3.5955100059509277, -4.155820846557617, -0.40469422936439514, -1.1806224584579468, -3.9712555408477783, -3.917482614517212, -3.259101390838623, -4.544681072235107, -3.7312724590301514, 4.481475830078125, 2.7998082637786865, 0.4556787312030792, 2.9352877140045166, 1.119849443435669, 2.1833293437957764, 2.8755369186401367, 2.6701912879943848, 3.0479557514190674, 0.7560579180717468, 1.0376800298690796, 3.7504727840423584, 1.8162273168563843, 0.8302631974220276, -0.7442873120307922, 1.4246245622634888, 0.9099907875061035, 4.139753341674805, 1.698012351989746, -0.7983959913253784, 1.5178117752075195, 0.4979194104671478, 3.034508228302002, 8.439632415771484, 1.2237529754638672, 4.001902103424072, -1.5365464687347412, -0.5020707249641418, 2.0821917057037354, 3.02547287940979, 0.8514304161071777, 1.0548402070999146, 0.15935155749320984, 2.5694632530212402, 1.6882548332214355, 4.791683197021484, 0.46597445011138916, 1.7344976663589478, 1.865371584892273, -3.7306764125823975, -0.17342066764831543, 0.6075802445411682, 3.3979244232177734, -0.9690950512886047, 2.244478702545166, 0.5438727140426636, 0.33992454409599304, -0.5145713090896606, -0.2764400243759155, 5.043140411376953, 1.0719255208969116, 0.675521731376648, 1.0635672807693481, 2.8970539569854736, -0.24503515660762787, 0.26779839396476746, 5.767853736877441, 2.720827579498291, 2.631786584854126, -3.7120673656463623, 0.9199159145355225, -0.6619486808776855, 1.638600468635559, -2.327491044998169, 2.802562952041626, -1.08970046043396, -1.3109580278396606, 1.104046106338501, 0.7990623116493225, 0.2818002700805664, -0.5115494132041931, 5.687518119812012, 5.994057655334473, 4.608816623687744, 4.708855152130127, 2.3742902278900146, 5.314054012298584, 0.7772896885871887, 1.214908242225647, 6.1676435470581055, 7.475282192230225, 4.660693645477295, 0.8593665957450867, 2.4900083541870117, 5.6235785484313965, 1.4078532457351685, 1.4069914817810059, 3.779740571975708, 3.519754409790039, 5.269972801208496, 2.490788221359253, 1.1211591958999634, 1.8801430463790894, 7.124375343322754, 1.5052053928375244, 0.8761273622512817, 5.83767032623291, 11.038844108581543, 11.087148666381836, 10.83184814453125, 2.1337265968322754, -0.7215690016746521, 10.503636360168457, 3.6399412155151367, 4.094265460968018, 3.8461005687713623, 4.839426517486572, 8.10223388671875, 6.168917179107666, 10.116711616516113, 4.459933280944824, 1.7802132368087769, 8.430289268493652, 6.315310955047607, 3.8544135093688965, 2.8066108226776123, 4.1978983879089355, -1.8742363452911377, 3.844466209411621, 3.912682056427002, 4.828442573547363, 2.9183833599090576, 7.7039713859558105, 3.1545586585998535, -2.6741678714752197, -2.4912784099578857, 1.0260030031204224, 0.10183272510766983, -1.404167652130127, 0.10056117922067642, 2.641960859298706, 3.8174126148223877, 1.0405009984970093, -0.16851823031902313, 2.279550313949585, -0.08556690812110901, -1.3939924240112305, -2.0821802616119385, -4.192112922668457, -1.3648496866226196, 1.5525039434432983, 1.7010834217071533, 0.9642347693443298, -0.20052699744701385, -1.6545944213867188, -2.1897008419036865, -3.049485206604004, -4.859721660614014, -2.698958158493042, -1.4257208108901978, -0.42741119861602783, -1.4028868675231934, -2.224156618118286, 0.4315846562385559, -1.2196317911148071, -0.7891849279403687, -0.38564467430114746, -2.195664167404175, 0.05167610943317413, -0.40053442120552063, -0.8555542230606079, -1.0657148361206055, -1.1980371475219727, -0.7499089241027832, -3.5165913105010986, -3.705483913421631, -0.1291821449995041, -2.267606496810913, 1.0474469661712646, 2.477273464202881, -2.195913553237915, -2.412867307662964, 0.503677487373352, -2.3777711391448975, -1.0409940481185913, -0.4000017046928406, 1.3676034212112427, -0.053797271102666855, -2.675827980041504, -0.9308925271034241, -0.5529848337173462, 0.2498243898153305, -0.3061739206314087, -3.279567003250122, -2.2931480407714844, -4.353571891784668, -4.246553897857666, 0.40308550000190735, 6.243842601776123, 0.9266063570976257, -0.8102243542671204, -2.2288503646850586, -2.7836503982543945, -4.507824897766113, -0.08835697919130325, -3.3499882221221924, -1.704323172569275, -0.04665275290608406, -1.8700965642929077, -4.243188381195068, -4.142364978790283, -1.035438060760498, 1.1121143102645874, 0.5489713549613953, 1.7854100465774536, -0.8899320363998413, -2.804229974746704, -0.5876503586769104, 0.03297177702188492, -4.439447402954102, -0.046001311391592026, -3.4029335975646973, -2.1875669956207275, -3.8296663761138916, -2.804439067840576, -3.741136074066162, -5.61972713470459, 1.6143378019332886, -2.4030938148498535, -3.8727033138275146, -0.9809318780899048, -3.026528835296631, -2.645667552947998, -2.2556560039520264, -3.3481876850128174, -2.260425567626953, -2.374419927597046, -3.6118099689483643, -3.5855495929718018, -3.8193299770355225, -3.7786097526550293, -1.8183320760726929, -2.2463197708129883, -0.10866280645132065, -3.3945505619049072, -1.578196406364441, 1.2050031423568726, -1.263005018234253, -1.007298231124878, -3.7409493923187256, 0.1771591603755951, -2.8512678146362305, -1.829738736152649, -1.8495402336120605, -1.8976249694824219, -2.7651853561401367, -3.474135398864746, 1.157999873161316, -0.9449211359024048, -0.7782096862792969, 1.2666178941726685, -0.6262881755828857, -3.9068944454193115, -3.9470112323760986, -0.4385111331939697, -1.6055965423583984, 1.4066005945205688, 0.1412150263786316, 3.039067029953003, -1.6238832473754883, 1.7142441272735596, 2.9801738262176514, -1.3860526084899902, 4.098142147064209, -0.44749021530151367, -0.37081146240234375, 4.346672534942627, 4.513944149017334, 0.7927073836326599, -0.44602683186531067, -0.25548142194747925, 1.4597046375274658, -1.1008957624435425, -2.0053765773773193, -0.0976053774356842, 1.1322002410888672, 1.2168469429016113, 2.942985773086548, 3.394153356552124, 0.08172228187322617, 0.5641719698905945, 0.5944890379905701, 0.4217010736465454, 3.0764071941375732, -0.2397545725107193, 0.18018180131912231, -0.3441052734851837, -2.1999106407165527, 2.186788558959961, -0.09990739822387695, -0.9786621928215027, 2.22766375541687, 0.8623676300048828, 0.8131488561630249, 1.3771005868911743, 0.41693732142448425, -0.8338800668716431, -0.17246414721012115, 0.509723961353302, 0.21408577263355255, -1.317720890045166, 1.0124942064285278, 1.7705212831497192, 0.929252028465271, -0.1795451045036316, 0.4750789701938629, 1.602271318435669, 0.35439571738243103, 0.7656732201576233, -0.7247801423072815, -0.2336030900478363, 2.603883981704712, 1.9868570566177368, -1.6057071685791016, 1.5703070163726807, -3.7741665840148926, -3.140000343322754, 0.5150390267372131, 1.3023791313171387, 0.45070523023605347, -0.589630126953125, 3.938748359680176, -2.530994176864624, 0.5184619426727295, -0.04503067582845688, 0.7382057905197144, 0.637591540813446, 2.423110246658325, 2.9671170711517334, 0.8056630492210388, -0.28853562474250793, -0.23231427371501923, 0.5387079119682312, 1.0304256677627563, -0.743928074836731, 0.26353561878204346, 1.7739304304122925, 0.5353230834007263, 2.8567631244659424, 0.6452627778053284, -0.05649464577436447, 0.01942385733127594, 0.8019274473190308, 0.22669845819473267, 0.10482487827539444, 2.52356219291687, 1.4046955108642578, -1.9748278856277466, -2.5508265495300293, -2.59018874168396, 2.6248178482055664, 0.5802379250526428, -0.012785714119672775, 1.3282228708267212, 0.41857829689979553, -1.1124659776687622, -0.6484687924385071, 2.744323968887329, -0.37855803966522217, -0.579504668712616, 1.4532630443572998, -0.7592792510986328, -0.6047624945640564, -0.8623843789100647, 2.063436985015869, 0.5325125455856323, -0.07227262109518051, 0.24996978044509888, 0.7232277393341064, 1.1935454607009888, 0.2458459883928299, 2.4594345092773438, 1.7067790031433105, -1.0971908569335938, -1.6077895164489746, 0.898906409740448, 0.4052867293357849, -0.023931682109832764, 1.2966893911361694, 0.9352522492408752, 0.7611365914344788, -0.49709129333496094, 0.5513364672660828, -0.155072420835495, -3.531649589538574, -0.5069416761398315, 5.40136194229126, -0.6017516851425171, 3.6606061458587646, -2.982283592224121, 0.4929423928260803, 0.8544524312019348, 2.7701539993286133, 0.6290284991264343, 0.5133351683616638, -0.7050284147262573, -2.6976914405822754, 0.5152353048324585, -0.07887144386768341, -2.9706404209136963, -0.636832594871521, 1.7919930219650269, 1.235923171043396, -1.6039891242980957, 0.3796198070049286, 0.07439081370830536, 2.5868165493011475, 0.9228172898292542, 2.625898838043213, 3.828376054763794, -0.6406844258308411, -1.1299878358840942, 1.1446707248687744, -1.0328019857406616, -0.8392481207847595, 0.4829893112182617, -1.6974679231643677, 2.0771257877349854, -0.4594328999519348, -1.28992760181427, -0.36673101782798767, 2.5901455879211426, -0.19045868515968323, 4.209321022033691, 3.580552101135254, -0.28117501735687256, 0.018902242183685303, 0.9365811347961426, 0.6035401821136475, 1.7300522327423096, 0.7132466435432434, -0.7031644582748413, -0.500429093837738, 0.6516425013542175, 0.1308852583169937, 0.396475613117218, -1.3985519409179688, 2.4942591190338135, 0.17934635281562805, 0.45591604709625244, 1.6974796056747437, -2.6538500785827637, -1.4799764156341553, 0.2336767315864563, -0.4010593891143799, 0.8545635342597961, 1.2081605195999146, -1.0631681680679321, -0.7097270488739014, -0.6915795803070068, -0.48807844519615173, 2.149048089981079, 1.9947779178619385, -0.3097258508205414, 0.7534958124160767, 0.3838607966899872, 2.0906665325164795, 1.7898954153060913, 2.3028714656829834, 0.9491927623748779, 1.2883849143981934, -1.3181761503219604, -1.5610746145248413, 0.950107216835022, 1.1505210399627686, 0.0841454267501831, -1.1702109575271606, -0.2275584638118744, 0.47900447249412537, 3.370723247528076, 3.217129707336426, 1.357729434967041, 0.5132471323013306, 1.62412428855896, 0.3967948853969574, 0.6182856559753418, -1.8020299673080444, -2.273179769515991, -1.065411925315857, 0.2382129430770874, 0.46932488679885864, 0.4665134847164154, -1.7554073333740234, -1.1448498964309692, 0.5380081534385681, 2.250924587249756, 2.457068681716919, 1.6912307739257812, 1.2825076580047607, -1.4788861274719238, 1.1049855947494507, -1.6648000478744507, -0.7823216915130615, 0.9179081320762634, 1.7279776334762573, 1.4362214803695679, -1.5274735689163208, -0.24446608126163483, -1.534751534461975, -1.93258535861969, -1.0709517002105713, 1.5472464561462402, 1.2437489032745361, -1.063279628753662, 1.9022948741912842, 1.6843701601028442, -0.3648708164691925, 1.352397084236145, -0.3981911838054657, -1.0235868692398071, 1.0606439113616943, -2.137662649154663, -1.463626503944397, 0.8209008574485779, 1.3147249221801758, -0.630517840385437, 1.0510815382003784, -1.2072651386260986, 0.30682337284088135, 1.225671648979187, -0.16383539140224457, 2.169780969619751, 0.7964351773262024, -0.4849572479724884, -0.722384512424469, 5.431890964508057, -2.9348883628845215, 0.1731135994195938, 1.0914664268493652, 1.7534987926483154, 3.865567684173584, 0.2960667014122009, 0.5469883680343628, -0.7199032306671143, -0.3004990220069885, -2.998429536819458, -1.4062843322753906, -2.1582298278808594, -0.710972249507904, 0.928543210029602, -1.216104507446289, 2.3051960468292236, 2.6452603340148926, -0.4512031376361847, -0.7393447756767273, 1.4055320024490356, 0.9890850782394409, 1.9254474639892578, -0.6046118140220642, 2.328507900238037, 0.6218729615211487, 0.205708846449852, 2.9660112857818604, 1.3359265327453613, -1.2419363260269165, 3.257913112640381, 0.3084458112716675, -0.9970975518226624, 2.7559309005737305, 2.947951555252075, -0.154603511095047, -0.7883457541465759, -0.9202600717544556, -0.020371844992041588, -2.6500084400177, 3.04435133934021, 1.7439491748809814, 0.7479650974273682, 0.4742521643638611, -0.5459203124046326, 1.291614055633545, 2.1087777614593506, 2.201493740081787, -1.0636907815933228, -0.04760601371526718, -2.6452808380126953, -1.3837445974349976, 1.1371276378631592, -1.2380865812301636, 0.5519888401031494, 1.7330124378204346, -0.6170193552970886, 1.8540014028549194, 0.09548789262771606, 1.946632742881775, 1.7942399978637695, 0.03925265371799469, -0.1974969357252121, -4.206842422485352, -1.326896071434021, 2.189582347869873, 0.2579881548881531, -0.39195114374160767, -1.3728307485580444, -2.2057886123657227, 0.07216914743185043, 0.8506224155426025, 2.7165050506591797, 1.0431472063064575, 3.134387731552124, 1.0341969728469849, 2.722299337387085, -0.33414778113365173, -0.5487476587295532, 0.899063229560852, 3.4038338661193848, -0.22376400232315063, 0.07326405495405197, -0.12549294531345367, -0.3097744882106781, 1.4494119882583618, 0.3737108111381531, 0.4293849766254425, 1.2288647890090942, 3.1989874839782715, -0.985267162322998, 2.1392946243286133, 1.8636281490325928, 0.531470537185669, 2.1410839557647705, 0.07178135216236115, 2.579829692840576, 2.0770041942596436, -0.01828313246369362, 1.1538100242614746, -0.7035794258117676, -0.5001261830329895, -0.965315043926239, -2.635425329208374, -1.1420174837112427, 0.9036309719085693, 0.5201398134231567, -0.9366670250892639, 0.6147935390472412, 2.563716173171997, -0.742855429649353, 2.7990550994873047, 1.4155992269515991, 1.7655420303344727, 1.6276639699935913, 2.014561176300049, 1.5509147644042969, -0.6600452065467834, 0.9519577026367188, -0.221612811088562, -0.03356000408530235, 3.4113316535949707, 0.8079084157943726, 2.7312440872192383, -1.1829596757888794, -0.6660905480384827, -1.0103631019592285, -0.05002973973751068, 0.5441375970840454, 6.094511985778809, 0.4210481345653534, -0.38986557722091675, -0.5602357387542725, 0.6837037205696106, 2.6086699962615967, 1.1682511568069458, -2.5701303482055664, 0.5038476586341858, -0.7558030486106873, -0.6899960041046143, -0.7047229409217834, 1.1704214811325073, -1.8128701448440552, -0.4380355477333069, -0.7041136026382446, -2.0361099243164062, -2.8651175498962402, 1.041190505027771, 4.023390769958496, 0.2144123613834381, 3.3224565982818604, -0.1287468522787094, 0.7509807348251343, -1.559312105178833, -0.6948845982551575, 1.43505859375, -1.4185093641281128, -2.82515549659729, -0.5716688632965088, -0.2701978087425232, 0.1466694325208664, 0.7213380932807922, 0.23499423265457153, 0.4298885464668274, 0.2408985197544098, 2.4241528511047363, 2.3816115856170654, 2.683875322341919, -1.6758170127868652, 0.5684135556221008, 0.08140240609645844, -1.328484058380127, -0.08933715522289276, 1.2805464267730713, 4.6678080558776855, 0.39497748017311096, 4.964327812194824, 0.4493218958377838, 0.4964278042316437, -0.30175963044166565, -0.9396913647651672, 0.35917454957962036, 1.1966496706008911, -1.5987671613693237, -1.401370882987976, 0.6252581477165222, 0.6375517845153809, -0.08944440633058548, 0.8369372487068176, 2.9173343181610107, 0.8579237461090088, -1.1364727020263672, -0.012663979083299637, -0.05252210795879364, 2.2555489540100098, 2.184932231903076, 1.890916347503662, 1.020843267440796, -3.465909004211426, 0.2995157837867737, 1.4601877927780151, -1.0427422523498535, 0.4867142140865326, 3.46932315826416, 1.0927120447158813, 1.171639323234558, 1.832642674446106, 0.12155257165431976, -0.44473034143447876, 2.9176065921783447, -1.6883790493011475, -2.3420844078063965, -0.632071316242218, 0.5942273736000061, 1.4546462297439575, -1.697863221168518, 2.283269166946411, 3.413587808609009, -0.1140359491109848, -2.5558066368103027, -1.450520396232605, -1.7825034856796265, 4.647157669067383, 0.46303850412368774, 1.5413988828659058, -1.3743263483047485, 3.4131076335906982, 1.4111440181732178, 3.457047939300537, 1.3101705312728882, -2.7795541286468506, -0.22327657043933868, -0.19650278985500336, -1.383360505104065, 2.0001659393310547, 1.5340595245361328, 1.9496262073516846, -0.7850513458251953, -0.27459537982940674, 0.6859872937202454, 1.4264650344848633, 0.15139226615428925, 1.927354335784912, -0.7839654684066772, -1.4971647262573242, 1.5665470361709595, -0.7320516705513, -1.5149905681610107, 1.2925809621810913, -1.6941879987716675, -3.1682538986206055, -2.579429864883423, -1.3699160814285278, 3.145193099975586, -1.595931053161621, 0.153254896402359, 0.5886276364326477, -0.5977714657783508, 1.4148268699645996, -1.6614097356796265, -2.2289063930511475, 0.8796274662017822, -0.3735045790672302, 1.0486299991607666, -1.7509034872055054, -1.7230207920074463, -1.4233064651489258, 0.15231260657310486, -1.6564300060272217, -0.6157920956611633, -1.3653186559677124, -0.5375298857688904, 1.373813271522522, -0.6402420401573181, 0.21477265655994415, -1.2033162117004395, -0.7748201489448547, -0.12748071551322937, -0.21898280084133148, -2.037301778793335, -2.3517818450927734, 0.0028174598701298237, 0.3099183440208435, -1.9125937223434448, -1.2079235315322876, -1.8245596885681152, -2.269404411315918, -1.107033610343933, -2.821638345718384, -1.4607176780700684, 0.23269805312156677, -1.0092543363571167, 1.40799880027771, -0.7087604999542236, -0.5778511166572571, 1.413687825202942, -0.8041061758995056, -0.760267972946167, -3.459501266479492, 3.448349952697754, -2.1474449634552, 1.570741891860962, -0.4728945791721344, -1.170865774154663, -2.2173688411712646, 3.286027431488037, -2.0372090339660645, -0.6541479825973511, -0.32200127840042114, -0.5214549899101257, -3.3337857723236084, 0.925971508026123, -0.29989251494407654, -1.807474136352539, -0.9611665606498718, -1.2654635906219482, -0.40001872181892395, -2.9874441623687744, -2.893066644668579, 0.22407561540603638, -1.9909253120422363, -3.592282295227051, 0.3668453097343445, 2.7051074504852295]\n"
     ]
    }
   ],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(pt_payload),\n",
    "    TargetModel=\"resnet_pt_v0.tar.gz\",\n",
    ")\n",
    "\n",
    "response = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "output = response[\"outputs\"][0][\"data\"]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974978c5",
   "metadata": {},
   "source": [
    "We can also use binary+json as the payload format to get better performance for the inference call. The specification of this format is provided [here](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md).\n",
    "\n",
    "**Note:** With the `binary+json` format, we have to specify the length of the request metadata in the header to allow Triton to correctly parse the binary payload. This is done using a custom Content-Type header `application/vnd.sagemaker-triton.binary+json;json-header-size={}`.\n",
    "\n",
    "Please not, this is different from using `Inference-Header-Content-Length` header on a stand-alone Triton server since custom headers are not allowed in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "273fd772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.24238598e+00  2.47019485e-01 -1.71944571e+00 -2.98377562e+00\n",
      "  -3.00313902e+00 -1.95564914e+00 -2.48965096e+00 -3.74836057e-01\n",
      "   4.31179963e-02 -1.60371578e+00 -3.87695670e+00 -1.97425640e+00\n",
      "  -3.64828277e+00 -5.38874674e+00 -1.69864321e+00 -2.91555357e+00\n",
      "  -1.85707331e+00  3.68552089e-01 -6.72105789e-01 -1.74029636e+00\n",
      "  -4.44818878e+00 -2.33208156e+00 -3.11241698e+00 -2.23977709e+00\n",
      "  -2.12387013e+00 -3.49225783e+00 -4.81953859e+00 -2.87595105e+00\n",
      "  -3.05604243e+00 -3.13367701e+00 -2.46147513e+00 -4.22210068e-01\n",
      "  -2.35856342e+00 -3.57243633e+00 -7.77378440e-01 -4.29438543e+00\n",
      "  -1.83301330e+00 -4.48172045e+00 -2.84007001e+00 -2.19165802e+00\n",
      "  -2.01508954e-01 -3.91157985e+00 -9.05075133e-01 -2.97916007e+00\n",
      "  -2.12318969e+00 -3.93249702e+00 -1.33751357e+00 -2.37469745e+00\n",
      "  -5.24911833e+00 -2.66397190e+00 -1.32200944e+00 -1.02401018e+00\n",
      "  -1.27982736e+00 -1.48375201e+00 -1.76049972e+00 -4.45062637e-01\n",
      "  -1.98289469e-01 -2.99530911e+00 -2.81290102e+00 -1.37118125e+00\n",
      "  -9.05618191e-01 -1.10930526e+00 -3.47283888e+00 -2.20780969e+00\n",
      "  -2.57462788e+00 -3.24537206e+00 -1.72099018e+00 -1.57659900e+00\n",
      "  -2.44702077e+00 -4.08605003e+00 -2.89528871e+00 -1.95735824e+00\n",
      "  -2.82413077e+00 -3.28038907e+00 -1.57598925e+00 -4.07390070e+00\n",
      "  -2.20724249e+00 -1.20696068e+00 -1.25886977e+00 -2.12395415e-01\n",
      "  -3.45368910e+00 -5.68717670e+00 -1.40738392e+00 -3.03674173e+00\n",
      "  -9.37763214e-01 -2.68098545e+00 -1.67501593e+00 -6.90223396e-01\n",
      "   5.91416299e-01 -8.43300819e-01 -2.48262501e+00 -4.85130358e+00\n",
      "  -1.44878471e+00 -1.73879445e+00 -4.66804534e-01 -4.54126644e+00\n",
      "  -2.67003679e+00 -8.71851444e-02 -1.15670705e+00 -1.27405077e-01\n",
      "  -3.88296306e-01 -2.89920449e+00 -2.51202154e+00 -4.11396360e+00\n",
      "   1.21098852e+00 -1.40363562e+00 -2.20768595e+00 -3.52899218e+00\n",
      "   1.26205057e-01 -3.33434010e+00 -1.44975424e+00 -1.69531941e+00\n",
      "   1.25761068e+00  2.80745596e-01 -1.22169220e+00 -1.25025666e+00\n",
      "  -3.00898743e+00 -1.88172615e+00 -6.23207033e-01 -2.08933306e+00\n",
      "  -1.41242516e+00 -1.40481126e+00 -6.95454240e-01 -1.69780242e+00\n",
      "  -1.43079770e+00 -2.14701056e+00 -8.39905739e-01 -5.95784545e-01\n",
      "  -4.41629696e+00 -3.38904834e+00 -4.00394082e-01 -4.15324688e+00\n",
      "  -1.46951032e+00 -3.56513095e+00 -2.75204480e-01 -2.63257575e+00\n",
      "  -2.63702250e+00 -2.56590486e+00 -3.03986788e+00 -3.40272498e+00\n",
      "  -4.48263645e+00 -3.49361777e+00 -3.59551001e+00 -4.15582085e+00\n",
      "  -4.04694229e-01 -1.18062246e+00 -3.97125554e+00 -3.91748261e+00\n",
      "  -3.25910139e+00 -4.54468107e+00 -3.73127246e+00  4.48147583e+00\n",
      "   2.79980826e+00  4.55678731e-01  2.93528771e+00  1.11984944e+00\n",
      "   2.18332934e+00  2.87553692e+00  2.67019129e+00  3.04795575e+00\n",
      "   7.56057918e-01  1.03768003e+00  3.75047278e+00  1.81622732e+00\n",
      "   8.30263197e-01 -7.44287312e-01  1.42462456e+00  9.09990788e-01\n",
      "   4.13975334e+00  1.69801235e+00 -7.98395991e-01  1.51781178e+00\n",
      "   4.97919410e-01  3.03450823e+00  8.43963242e+00  1.22375298e+00\n",
      "   4.00190210e+00 -1.53654647e+00 -5.02070725e-01  2.08219171e+00\n",
      "   3.02547288e+00  8.51430416e-01  1.05484021e+00  1.59351557e-01\n",
      "   2.56946325e+00  1.68825483e+00  4.79168320e+00  4.65974450e-01\n",
      "   1.73449767e+00  1.86537158e+00 -3.73067641e+00 -1.73420668e-01\n",
      "   6.07580245e-01  3.39792442e+00 -9.69095051e-01  2.24447870e+00\n",
      "   5.43872714e-01  3.39924544e-01 -5.14571309e-01 -2.76440024e-01\n",
      "   5.04314041e+00  1.07192552e+00  6.75521731e-01  1.06356728e+00\n",
      "   2.89705396e+00 -2.45035157e-01  2.67798394e-01  5.76785374e+00\n",
      "   2.72082758e+00  2.63178658e+00 -3.71206737e+00  9.19915915e-01\n",
      "  -6.61948681e-01  1.63860047e+00 -2.32749104e+00  2.80256295e+00\n",
      "  -1.08970046e+00 -1.31095803e+00  1.10404611e+00  7.99062312e-01\n",
      "   2.81800270e-01 -5.11549413e-01  5.68751812e+00  5.99405766e+00\n",
      "   4.60881662e+00  4.70885515e+00  2.37429023e+00  5.31405401e+00\n",
      "   7.77289689e-01  1.21490824e+00  6.16764355e+00  7.47528219e+00\n",
      "   4.66069365e+00  8.59366596e-01  2.49000835e+00  5.62357855e+00\n",
      "   1.40785325e+00  1.40699148e+00  3.77974057e+00  3.51975441e+00\n",
      "   5.26997280e+00  2.49078822e+00  1.12115920e+00  1.88014305e+00\n",
      "   7.12437534e+00  1.50520539e+00  8.76127362e-01  5.83767033e+00\n",
      "   1.10388441e+01  1.10871487e+01  1.08318481e+01  2.13372660e+00\n",
      "  -7.21569002e-01  1.05036364e+01  3.63994122e+00  4.09426546e+00\n",
      "   3.84610057e+00  4.83942652e+00  8.10223389e+00  6.16891718e+00\n",
      "   1.01167116e+01  4.45993328e+00  1.78021324e+00  8.43028927e+00\n",
      "   6.31531096e+00  3.85441351e+00  2.80661082e+00  4.19789839e+00\n",
      "  -1.87423635e+00  3.84446621e+00  3.91268206e+00  4.82844257e+00\n",
      "   2.91838336e+00  7.70397139e+00  3.15455866e+00 -2.67416787e+00\n",
      "  -2.49127841e+00  1.02600300e+00  1.01832725e-01 -1.40416765e+00\n",
      "   1.00561179e-01  2.64196086e+00  3.81741261e+00  1.04050100e+00\n",
      "  -1.68518230e-01  2.27955031e+00 -8.55669081e-02 -1.39399242e+00\n",
      "  -2.08218026e+00 -4.19211292e+00 -1.36484969e+00  1.55250394e+00\n",
      "   1.70108342e+00  9.64234769e-01 -2.00526997e-01 -1.65459442e+00\n",
      "  -2.18970084e+00 -3.04948521e+00 -4.85972166e+00 -2.69895816e+00\n",
      "  -1.42572081e+00 -4.27411199e-01 -1.40288687e+00 -2.22415662e+00\n",
      "   4.31584656e-01 -1.21963179e+00 -7.89184928e-01 -3.85644674e-01\n",
      "  -2.19566417e+00  5.16761094e-02 -4.00534421e-01 -8.55554223e-01\n",
      "  -1.06571484e+00 -1.19803715e+00 -7.49908924e-01 -3.51659131e+00\n",
      "  -3.70548391e+00 -1.29182145e-01 -2.26760650e+00  1.04744697e+00\n",
      "   2.47727346e+00 -2.19591355e+00 -2.41286731e+00  5.03677487e-01\n",
      "  -2.37777114e+00 -1.04099405e+00 -4.00001705e-01  1.36760342e+00\n",
      "  -5.37972711e-02 -2.67582798e+00 -9.30892527e-01 -5.52984834e-01\n",
      "   2.49824390e-01 -3.06173921e-01 -3.27956700e+00 -2.29314804e+00\n",
      "  -4.35357189e+00 -4.24655390e+00  4.03085500e-01  6.24384260e+00\n",
      "   9.26606357e-01 -8.10224354e-01 -2.22885036e+00 -2.78365040e+00\n",
      "  -4.50782490e+00 -8.83569792e-02 -3.34998822e+00 -1.70432317e+00\n",
      "  -4.66527529e-02 -1.87009656e+00 -4.24318838e+00 -4.14236498e+00\n",
      "  -1.03543806e+00  1.11211431e+00  5.48971355e-01  1.78541005e+00\n",
      "  -8.89932036e-01 -2.80422997e+00 -5.87650359e-01  3.29717770e-02\n",
      "  -4.43944740e+00 -4.60013114e-02 -3.40293360e+00 -2.18756700e+00\n",
      "  -3.82966638e+00 -2.80443907e+00 -3.74113607e+00 -5.61972713e+00\n",
      "   1.61433780e+00 -2.40309381e+00 -3.87270331e+00 -9.80931878e-01\n",
      "  -3.02652884e+00 -2.64566755e+00 -2.25565600e+00 -3.34818769e+00\n",
      "  -2.26042557e+00 -2.37441993e+00 -3.61180997e+00 -3.58554959e+00\n",
      "  -3.81932998e+00 -3.77860975e+00 -1.81833208e+00 -2.24631977e+00\n",
      "  -1.08662806e-01 -3.39455056e+00 -1.57819641e+00  1.20500314e+00\n",
      "  -1.26300502e+00 -1.00729823e+00 -3.74094939e+00  1.77159160e-01\n",
      "  -2.85126781e+00 -1.82973874e+00 -1.84954023e+00 -1.89762497e+00\n",
      "  -2.76518536e+00 -3.47413540e+00  1.15799987e+00 -9.44921136e-01\n",
      "  -7.78209686e-01  1.26661789e+00 -6.26288176e-01 -3.90689445e+00\n",
      "  -3.94701123e+00 -4.38511133e-01 -1.60559654e+00  1.40660059e+00\n",
      "   1.41215026e-01  3.03906703e+00 -1.62388325e+00  1.71424413e+00\n",
      "   2.98017383e+00 -1.38605261e+00  4.09814215e+00 -4.47490215e-01\n",
      "  -3.70811462e-01  4.34667253e+00  4.51394415e+00  7.92707384e-01\n",
      "  -4.46026832e-01 -2.55481422e-01  1.45970464e+00 -1.10089576e+00\n",
      "  -2.00537658e+00 -9.76053774e-02  1.13220024e+00  1.21684694e+00\n",
      "   2.94298577e+00  3.39415336e+00  8.17222819e-02  5.64171970e-01\n",
      "   5.94489038e-01  4.21701074e-01  3.07640719e+00 -2.39754573e-01\n",
      "   1.80181801e-01 -3.44105273e-01 -2.19991064e+00  2.18678856e+00\n",
      "  -9.99073982e-02 -9.78662193e-01  2.22766376e+00  8.62367630e-01\n",
      "   8.13148856e-01  1.37710059e+00  4.16937321e-01 -8.33880067e-01\n",
      "  -1.72464147e-01  5.09723961e-01  2.14085773e-01 -1.31772089e+00\n",
      "   1.01249421e+00  1.77052128e+00  9.29252028e-01 -1.79545105e-01\n",
      "   4.75078970e-01  1.60227132e+00  3.54395717e-01  7.65673220e-01\n",
      "  -7.24780142e-01 -2.33603090e-01  2.60388398e+00  1.98685706e+00\n",
      "  -1.60570717e+00  1.57030702e+00 -3.77416658e+00 -3.14000034e+00\n",
      "   5.15039027e-01  1.30237913e+00  4.50705230e-01 -5.89630127e-01\n",
      "   3.93874836e+00 -2.53099418e+00  5.18461943e-01 -4.50306758e-02\n",
      "   7.38205791e-01  6.37591541e-01  2.42311025e+00  2.96711707e+00\n",
      "   8.05663049e-01 -2.88535625e-01 -2.32314274e-01  5.38707912e-01\n",
      "   1.03042567e+00 -7.43928075e-01  2.63535619e-01  1.77393043e+00\n",
      "   5.35323083e-01  2.85676312e+00  6.45262778e-01 -5.64946458e-02\n",
      "   1.94238573e-02  8.01927447e-01  2.26698458e-01  1.04824878e-01\n",
      "   2.52356219e+00  1.40469551e+00 -1.97482789e+00 -2.55082655e+00\n",
      "  -2.59018874e+00  2.62481785e+00  5.80237925e-01 -1.27857141e-02\n",
      "   1.32822287e+00  4.18578297e-01 -1.11246598e+00 -6.48468792e-01\n",
      "   2.74432397e+00 -3.78558040e-01 -5.79504669e-01  1.45326304e+00\n",
      "  -7.59279251e-01 -6.04762495e-01 -8.62384379e-01  2.06343699e+00\n",
      "   5.32512546e-01 -7.22726211e-02  2.49969780e-01  7.23227739e-01\n",
      "   1.19354546e+00  2.45845988e-01  2.45943451e+00  1.70677900e+00\n",
      "  -1.09719086e+00 -1.60778952e+00  8.98906410e-01  4.05286729e-01\n",
      "  -2.39316821e-02  1.29668939e+00  9.35252249e-01  7.61136591e-01\n",
      "  -4.97091293e-01  5.51336467e-01 -1.55072421e-01 -3.53164959e+00\n",
      "  -5.06941676e-01  5.40136194e+00 -6.01751685e-01  3.66060615e+00\n",
      "  -2.98228359e+00  4.92942393e-01  8.54452431e-01  2.77015400e+00\n",
      "   6.29028499e-01  5.13335168e-01 -7.05028415e-01 -2.69769144e+00\n",
      "   5.15235305e-01 -7.88714439e-02 -2.97064042e+00 -6.36832595e-01\n",
      "   1.79199302e+00  1.23592317e+00 -1.60398912e+00  3.79619807e-01\n",
      "   7.43908137e-02  2.58681655e+00  9.22817290e-01  2.62589884e+00\n",
      "   3.82837605e+00 -6.40684426e-01 -1.12998784e+00  1.14467072e+00\n",
      "  -1.03280199e+00 -8.39248121e-01  4.82989311e-01 -1.69746792e+00\n",
      "   2.07712579e+00 -4.59432900e-01 -1.28992760e+00 -3.66731018e-01\n",
      "   2.59014559e+00 -1.90458685e-01  4.20932102e+00  3.58055210e+00\n",
      "  -2.81175017e-01  1.89022422e-02  9.36581135e-01  6.03540182e-01\n",
      "   1.73005223e+00  7.13246644e-01 -7.03164458e-01 -5.00429094e-01\n",
      "   6.51642501e-01  1.30885258e-01  3.96475613e-01 -1.39855194e+00\n",
      "   2.49425912e+00  1.79346353e-01  4.55916047e-01  1.69747961e+00\n",
      "  -2.65385008e+00 -1.47997642e+00  2.33676732e-01 -4.01059389e-01\n",
      "   8.54563534e-01  1.20816052e+00 -1.06316817e+00 -7.09727049e-01\n",
      "  -6.91579580e-01 -4.88078445e-01  2.14904809e+00  1.99477792e+00\n",
      "  -3.09725851e-01  7.53495812e-01  3.83860797e-01  2.09066653e+00\n",
      "   1.78989542e+00  2.30287147e+00  9.49192762e-01  1.28838491e+00\n",
      "  -1.31817615e+00 -1.56107461e+00  9.50107217e-01  1.15052104e+00\n",
      "   8.41454268e-02 -1.17021096e+00 -2.27558464e-01  4.79004472e-01\n",
      "   3.37072325e+00  3.21712971e+00  1.35772943e+00  5.13247132e-01\n",
      "   1.62412429e+00  3.96794885e-01  6.18285656e-01 -1.80202997e+00\n",
      "  -2.27317977e+00 -1.06541193e+00  2.38212943e-01  4.69324887e-01\n",
      "   4.66513485e-01 -1.75540733e+00 -1.14484990e+00  5.38008153e-01\n",
      "   2.25092459e+00  2.45706868e+00  1.69123077e+00  1.28250766e+00\n",
      "  -1.47888613e+00  1.10498559e+00 -1.66480005e+00 -7.82321692e-01\n",
      "   9.17908132e-01  1.72797763e+00  1.43622148e+00 -1.52747357e+00\n",
      "  -2.44466081e-01 -1.53475153e+00 -1.93258536e+00 -1.07095170e+00\n",
      "   1.54724646e+00  1.24374890e+00 -1.06327963e+00  1.90229487e+00\n",
      "   1.68437016e+00 -3.64870816e-01  1.35239708e+00 -3.98191184e-01\n",
      "  -1.02358687e+00  1.06064391e+00 -2.13766265e+00 -1.46362650e+00\n",
      "   8.20900857e-01  1.31472492e+00 -6.30517840e-01  1.05108154e+00\n",
      "  -1.20726514e+00  3.06823373e-01  1.22567165e+00 -1.63835391e-01\n",
      "   2.16978097e+00  7.96435177e-01 -4.84957248e-01 -7.22384512e-01\n",
      "   5.43189096e+00 -2.93488836e+00  1.73113599e-01  1.09146643e+00\n",
      "   1.75349879e+00  3.86556768e+00  2.96066701e-01  5.46988368e-01\n",
      "  -7.19903231e-01 -3.00499022e-01 -2.99842954e+00 -1.40628433e+00\n",
      "  -2.15822983e+00 -7.10972250e-01  9.28543210e-01 -1.21610451e+00\n",
      "   2.30519605e+00  2.64526033e+00 -4.51203138e-01 -7.39344776e-01\n",
      "   1.40553200e+00  9.89085078e-01  1.92544746e+00 -6.04611814e-01\n",
      "   2.32850790e+00  6.21872962e-01  2.05708846e-01  2.96601129e+00\n",
      "   1.33592653e+00 -1.24193633e+00  3.25791311e+00  3.08445811e-01\n",
      "  -9.97097552e-01  2.75593090e+00  2.94795156e+00 -1.54603511e-01\n",
      "  -7.88345754e-01 -9.20260072e-01 -2.03718450e-02 -2.65000844e+00\n",
      "   3.04435134e+00  1.74394917e+00  7.47965097e-01  4.74252164e-01\n",
      "  -5.45920312e-01  1.29161406e+00  2.10877776e+00  2.20149374e+00\n",
      "  -1.06369078e+00 -4.76060137e-02 -2.64528084e+00 -1.38374460e+00\n",
      "   1.13712764e+00 -1.23808658e+00  5.51988840e-01  1.73301244e+00\n",
      "  -6.17019355e-01  1.85400140e+00  9.54878926e-02  1.94663274e+00\n",
      "   1.79424000e+00  3.92526537e-02 -1.97496936e-01 -4.20684242e+00\n",
      "  -1.32689607e+00  2.18958235e+00  2.57988155e-01 -3.91951144e-01\n",
      "  -1.37283075e+00 -2.20578861e+00  7.21691474e-02  8.50622416e-01\n",
      "   2.71650505e+00  1.04314721e+00  3.13438773e+00  1.03419697e+00\n",
      "   2.72229934e+00 -3.34147781e-01 -5.48747659e-01  8.99063230e-01\n",
      "   3.40383387e+00 -2.23764002e-01  7.32640550e-02 -1.25492945e-01\n",
      "  -3.09774488e-01  1.44941199e+00  3.73710811e-01  4.29384977e-01\n",
      "   1.22886479e+00  3.19898748e+00 -9.85267162e-01  2.13929462e+00\n",
      "   1.86362815e+00  5.31470537e-01  2.14108396e+00  7.17813522e-02\n",
      "   2.57982969e+00  2.07700419e+00 -1.82831325e-02  1.15381002e+00\n",
      "  -7.03579426e-01 -5.00126183e-01 -9.65315044e-01 -2.63542533e+00\n",
      "  -1.14201748e+00  9.03630972e-01  5.20139813e-01 -9.36667025e-01\n",
      "   6.14793539e-01  2.56371617e+00 -7.42855430e-01  2.79905510e+00\n",
      "   1.41559923e+00  1.76554203e+00  1.62766397e+00  2.01456118e+00\n",
      "   1.55091476e+00 -6.60045207e-01  9.51957703e-01 -2.21612811e-01\n",
      "  -3.35600041e-02  3.41133165e+00  8.07908416e-01  2.73124409e+00\n",
      "  -1.18295968e+00 -6.66090548e-01 -1.01036310e+00 -5.00297397e-02\n",
      "   5.44137597e-01  6.09451199e+00  4.21048135e-01 -3.89865577e-01\n",
      "  -5.60235739e-01  6.83703721e-01  2.60867000e+00  1.16825116e+00\n",
      "  -2.57013035e+00  5.03847659e-01 -7.55803049e-01 -6.89996004e-01\n",
      "  -7.04722941e-01  1.17042148e+00 -1.81287014e+00 -4.38035548e-01\n",
      "  -7.04113603e-01 -2.03610992e+00 -2.86511755e+00  1.04119051e+00\n",
      "   4.02339077e+00  2.14412361e-01  3.32245660e+00 -1.28746852e-01\n",
      "   7.50980735e-01 -1.55931211e+00 -6.94884598e-01  1.43505859e+00\n",
      "  -1.41850936e+00 -2.82515550e+00 -5.71668863e-01 -2.70197809e-01\n",
      "   1.46669433e-01  7.21338093e-01  2.34994233e-01  4.29888546e-01\n",
      "   2.40898520e-01  2.42415285e+00  2.38161159e+00  2.68387532e+00\n",
      "  -1.67581701e+00  5.68413556e-01  8.14024061e-02 -1.32848406e+00\n",
      "  -8.93371552e-02  1.28054643e+00  4.66780806e+00  3.94977480e-01\n",
      "   4.96432781e+00  4.49321896e-01  4.96427804e-01 -3.01759630e-01\n",
      "  -9.39691365e-01  3.59174550e-01  1.19664967e+00 -1.59876716e+00\n",
      "  -1.40137088e+00  6.25258148e-01  6.37551785e-01 -8.94444063e-02\n",
      "   8.36937249e-01  2.91733432e+00  8.57923746e-01 -1.13647270e+00\n",
      "  -1.26639791e-02 -5.25221080e-02  2.25554895e+00  2.18493223e+00\n",
      "   1.89091635e+00  1.02084327e+00 -3.46590900e+00  2.99515784e-01\n",
      "   1.46018779e+00 -1.04274225e+00  4.86714214e-01  3.46932316e+00\n",
      "   1.09271204e+00  1.17163932e+00  1.83264267e+00  1.21552572e-01\n",
      "  -4.44730341e-01  2.91760659e+00 -1.68837905e+00 -2.34208441e+00\n",
      "  -6.32071316e-01  5.94227374e-01  1.45464623e+00 -1.69786322e+00\n",
      "   2.28326917e+00  3.41358781e+00 -1.14035949e-01 -2.55580664e+00\n",
      "  -1.45052040e+00 -1.78250349e+00  4.64715767e+00  4.63038504e-01\n",
      "   1.54139888e+00 -1.37432635e+00  3.41310763e+00  1.41114402e+00\n",
      "   3.45704794e+00  1.31017053e+00 -2.77955413e+00 -2.23276570e-01\n",
      "  -1.96502790e-01 -1.38336051e+00  2.00016594e+00  1.53405952e+00\n",
      "   1.94962621e+00 -7.85051346e-01 -2.74595380e-01  6.85987294e-01\n",
      "   1.42646503e+00  1.51392266e-01  1.92735434e+00 -7.83965468e-01\n",
      "  -1.49716473e+00  1.56654704e+00 -7.32051671e-01 -1.51499057e+00\n",
      "   1.29258096e+00 -1.69418800e+00 -3.16825390e+00 -2.57942986e+00\n",
      "  -1.36991608e+00  3.14519310e+00 -1.59593105e+00  1.53254896e-01\n",
      "   5.88627636e-01 -5.97771466e-01  1.41482687e+00 -1.66140974e+00\n",
      "  -2.22890639e+00  8.79627466e-01 -3.73504579e-01  1.04863000e+00\n",
      "  -1.75090349e+00 -1.72302079e+00 -1.42330647e+00  1.52312607e-01\n",
      "  -1.65643001e+00 -6.15792096e-01 -1.36531866e+00 -5.37529886e-01\n",
      "   1.37381327e+00 -6.40242040e-01  2.14772657e-01 -1.20331621e+00\n",
      "  -7.74820149e-01 -1.27480716e-01 -2.18982801e-01 -2.03730178e+00\n",
      "  -2.35178185e+00  2.81745987e-03  3.09918344e-01 -1.91259372e+00\n",
      "  -1.20792353e+00 -1.82455969e+00 -2.26940441e+00 -1.10703361e+00\n",
      "  -2.82163835e+00 -1.46071768e+00  2.32698053e-01 -1.00925434e+00\n",
      "   1.40799880e+00 -7.08760500e-01 -5.77851117e-01  1.41368783e+00\n",
      "  -8.04106176e-01 -7.60267973e-01 -3.45950127e+00  3.44834995e+00\n",
      "  -2.14744496e+00  1.57074189e+00 -4.72894579e-01 -1.17086577e+00\n",
      "  -2.21736884e+00  3.28602743e+00 -2.03720903e+00 -6.54147983e-01\n",
      "  -3.22001278e-01 -5.21454990e-01 -3.33378577e+00  9.25971508e-01\n",
      "  -2.99892515e-01 -1.80747414e+00 -9.61166561e-01 -1.26546359e+00\n",
      "  -4.00018722e-01 -2.98744416e+00 -2.89306664e+00  2.24075615e-01\n",
      "  -1.99092531e+00 -3.59228230e+00  3.66845310e-01  2.70510745e+00]]\n"
     ]
    }
   ],
   "source": [
    "request_body, header_length = get_sample_image_binary_pt()\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/vnd.sagemaker-triton.binary+json;json-header-size={}\".format(\n",
    "        header_length\n",
    "    ),\n",
    "    Body=request_body,\n",
    "    TargetModel=\"resnet_pt_v1.tar.gz\",\n",
    ")\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response[\"Body\"].read(), header_length=int(header_length_str)\n",
    ")\n",
    "output0_data = result.as_numpy(\"OUTPUT__0\")\n",
    "print(output0_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7788a02",
   "metadata": {},
   "source": [
    "#### Cloudwatch metrics for GPU Multi Model Endpoints\n",
    "\n",
    "Amazon SageMaker multi-model endpoints provides instance level metrics to monitor, for more details refer [Monitor Amazon SageMaker with Amazon CloudWatch](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html)\n",
    "\n",
    "\n",
    "*  Number of models loaded in the containers (LoadedModelCount),  \n",
    "* Precentage of GPU units that are used by the containers (GPUUtilization), \n",
    "* Precentage of GPU memory used by the containers (GPUMemoryUtilization), \n",
    "* Precentage of disk space used by the containers (DiskUtilization) etc. \n",
    "\n",
    "SageMaker MME also provides Model loading metrics such as-\n",
    "\n",
    "*  Time interval for model to be downloaded or loaded (ModelLoadingWaitTime),\n",
    "*  Time interval to unload model from container (ModelUnloadingTime),\n",
    "*  Time to download the model from S3 (ModelDownloadingTime), \n",
    "* Number of invocations to model that are already loaded onto the container(ModelCacheHit) etc to get model invocation level insights. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9ccc6",
   "metadata": {},
   "source": [
    "#### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ade6c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'fbefffd2-d5fe-4bf3-8c5d-b5ba6d9ba370',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'fbefffd2-d5fe-4bf3-8c5d-b5ba6d9ba370',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 18 Oct 2022 16:01:39 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_model(ModelName=sm_model_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
