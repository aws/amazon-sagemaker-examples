{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Built-in Algorithms with SageMaker (Part 4/4)\n",
    "Download | Structure | Preprocessing (Built-in) | **Train Model (Built-in)**\n",
    "```\n",
    "\n",
    "```\n",
    "**Notes**: \n",
    "* This notebook should be used with the conda_amazonei_mxnet_p36 kernel\n",
    "* This notebook is part of a series of notebooks beginning with `01_download_data`, `02_structuring_data` and `03a_builtin_preprocessing`.\n",
    "* You can also explore training with TensorFlow and PyTorch by running `04b_tensorflow_training` and `04c_pytorch_training`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will use the SageMaker SDK to create an Estimator for SageMaker's Built-in Image Classification algorithm and train it on a remote EC2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* #### [Dependecies](#ipg4a.1)\n",
    "* #### [Built-in Image Classification algorithm](#ipg4a.2)\n",
    "* #### [Understanding the training output](#ipg4a.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg4a.1'></a>\n",
    "## Dependencies\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and check SageMaker version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil\n",
    "import urllib\n",
    "import pickle\n",
    "import pathlib\n",
    "import tarfile\n",
    "import subprocess\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load S3 bucket name & category labels\n",
    "The `category_labels` file was generated from the first notebook in this series `01_download_data.ipynb`. You will need to run that notebook before running the code here. \n",
    "\n",
    "An S3 bucket for this guide was created in Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickled_data/builtin_bucket_name.pickle', 'rb') as f:\n",
    "    bucket_name = pickle.load(f)\n",
    "    print('Bucket Name: ', bucket_name)\n",
    "    \n",
    "with open('pickled_data/category_labels.pickle', 'rb') as f:\n",
    "    category_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg4a.2'></a>\n",
    "## Built-in Image Classification algorithm\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SageMaker training and validation channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput( \n",
    "    s3_data=f's3://{bucket_name}/data/train',\n",
    "    content_type='application/x-recordio',\n",
    "    s3_data_type='S3Prefix',\n",
    "    input_mode='Pipe')\n",
    "\n",
    "val_data = sagemaker.inputs.TrainingInput( \n",
    "    s3_data=f's3://{bucket_name}/data/val',\n",
    "    content_type='application/x-recordio',\n",
    "    s3_data_type='S3Prefix',\n",
    "    input_mode='Pipe')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': val_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the algorithm's hyperparameters\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html\n",
    "* **num_layers** - The built-in image classification algrorithm is based off the ResNet architecture. There are many different versions of this architecture differing by how many layers they use. We'll use the smallest one for this guide to speed up training. If the algorithm's accuracy is hitting a plateau and you need better accuracy, increasing the number of layers may help.\n",
    "* **use_pretrained_model** - This will initialize the weights from a pre-trained model for transfer learning. Otherwise weights are initialized randomly.\n",
    "* **augmentation_type** - Allows you to add augmentations to your trainingset to help your model generalize better. For small datasets, augmentation can greatly imporve training.\n",
    "* **image_shape** -  The channel, height, width of all the images\n",
    "* **num_classes** - Number of classes in your dataset\n",
    "* **num_training_samples** - Total number of images in your training set (used to help calculate progres)\n",
    "* **mini_batch_size** - The batch size you would like to use during training. \n",
    "* **epochs** - An epoch refers to one cycle through the training set and having more epochs to train means having more oppotunities to improve accracy. Suitable values range from 5 to 25 epochs depending on your time and budget constraints. Ideally, the right number of epochs is right before your validation accuracy plateaus.\n",
    "* **learning_rate**: After each batch of training we update the model's weights to give us the best possible results for that batch. The learning rate controls by how much we should update the weights. Best practices dictate a value between 0.2 and .001, typically never going higher than 1. The higher the learning rate, the faster your training will converge to the optimal weights, but going too fast can lead you to overshoot the target. In this example, we're using the weights from a pre-trained model so we'd want to start with a lower learning rate because the weights have already been optimized and we don't want move too far away from them.\n",
    "* **precision_dtype** -  Whether you want to use a 32-bit float data type for the model's weights or 16-bit. 16-bit can be used if you're running into memory management issues. However, weights can grow or shrink rapidly so having 32-bit weights make your training more robust to these issues and is typically the default in most frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(category_labels)\n",
    "num_training_samples = len(set(pathlib.Path('data_structured/train').rglob('*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'num_layers': 18,\n",
    "    'use_pretrained_model': 1,\n",
    "    'augmentation_type': 'crop_color_transform',\n",
    "    'image_shape': \"3,224,224\",\n",
    "    'num_classes': num_classes,\n",
    "    'num_training_samples': num_training_samples,\n",
    "    'mini_batch_size': 64,\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 0.001,\n",
    "    'precision_dtype': 'float32'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the type of algorithm and resources to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve('image-classification', sagemaker.Session().boto_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_config = {\n",
    "    'hyperparameters': hyperparameters,\n",
    "    'image_uri': training_image,\n",
    "    'role': sagemaker.get_execution_role(), \n",
    "    'instance_count': 1, \n",
    "    'instance_type': 'ml.p3.2xlarge',\n",
    "    'volume_size': 100,\n",
    "    'max_run': 360000,\n",
    "    'output_path': f's3://{bucket_name}/data/output'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = sagemaker.estimator.Estimator(**algo_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ipg4a.3'></a>\n",
    "## Understanding the training output\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[09/14/2020 05:37:38 INFO 139869866030912] Epoch[0] Batch [20]#011Speed: 111.811 samples/sec#011accuracy=0.452381\n",
    "[09/14/2020 05:37:54 INFO 139869866030912] Epoch[0] Batch [40]#011Speed: 131.393 samples/sec#011accuracy=0.570503\n",
    "[09/14/2020 05:38:10 INFO 139869866030912] Epoch[0] Batch [60]#011Speed: 139.540 samples/sec#011accuracy=0.617700\n",
    "[09/14/2020 05:38:27 INFO 139869866030912] Epoch[0] Batch [80]#011Speed: 144.003 samples/sec#011accuracy=0.644483\n",
    "[09/14/2020 05:38:43 INFO 139869866030912] Epoch[0] Batch [100]#011Speed: 146.600 samples/sec#011accuracy=0.664991\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training has begun:\n",
    "* Epoch[0]: One epoch corresponds to one training cycle through all the data. Stochastic optimizers like SGD and Adam improve accuracy by running multiple epochs. Random data augmentations is also applied with each new epoch allowing the training algorithm to learn on modified data.\n",
    "* Batch: The number of batches processed by the training algorithm. We specified one batch to be 64 images in the `mini_batch_size` hyperparameter. For algorithms like SGD, the model get a chance to update itself every batch.  \n",
    "* Speed: the number of images sent to the training algorithm per second. This information is important in determining how changes in your dataset affect the speed of training.\n",
    "* Accuracy: the training accuracy achieved at each interval (in this case, 20 batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "[09/14/2020 05:38:58 INFO 139869866030912] Epoch[0] Train-accuracy=0.677083\n",
    "[09/14/2020 05:38:58 INFO 139869866030912] Epoch[0] Time cost=102.745\n",
    "[09/14/2020 05:39:02 INFO 139869866030912] Epoch[0] Validation-accuracy=0.729492\n",
    "[09/14/2020 05:39:02 INFO 139869866030912] Storing the best model with validation accuracy: 0.729492\n",
    "[09/14/2020 05:39:02 INFO 139869866030912] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first epoch of training has ended (for this example we only train for one epoch). The final training accuracy is reported as well as the accuracy on the validation set. Comparing these two number is important in determining if your model is overfit or underfit as well as the bais/variance trade-off. The saved model uses the learned weights from the epoch with the best validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "2020-09-14 05:39:03 Uploading - Uploading generated training model\n",
    "2020-09-14 05:39:15 Completed - Training job completed\n",
    "Training seconds: 235\n",
    "Billable seconds: 235\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model parameters are saved as a `.tar.gz` in S3 to the directory specified in the `output_path` of `algo_config`. Total billable seconds is also reported to help compute the cost of training since you are only charged for the time the EC2 instance is training on the data. Other costs such as S3 storage also apply, but are not included here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollback to default version of SDK\n",
    "Only do this if you're done with this guide and want to use the same kernel for other notebooks with an incompatible version of the SageMaker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Original version: {original_sagemaker_version[0]}')\n",
    "# print(f'Current version:  {sagemaker.__version__}')\n",
    "# print('')\n",
    "# print(f'Rolling back to {original_sagemaker_version[0]}. Restart notebook kernel to use this version.')\n",
    "# print('')\n",
    "# s = f'sagemaker=={original_sagemaker_version[0]}'\n",
    "# !{sys.executable} -m pip install {s}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "This concludes the Image Data Guide for SageMaker's Built-in algorithms. If you'd like to deploy your model and get predictions on your test data, all the info you'll need to get going can be foud here: [Deploy Models for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
