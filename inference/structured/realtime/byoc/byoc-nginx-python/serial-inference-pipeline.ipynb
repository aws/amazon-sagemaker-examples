{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eeef4885-a6e6-41af-b6e0-9efd5894e86c",
   "metadata": {},
   "source": [
    "# Build and Deploy ML Application to SageMaker real-time endpoints\n",
    "\n",
    "- We build a fully custom ML application that encapsulates the following:\n",
    "  1. A [featurizer](./featurizer/) model (data pre-processing container) built using `SKLearn` column transformer\n",
    "     - The model transforms raw csv input data to features and returns the transformed data as output\n",
    "  1. A [predictor](./predictor/) XGBoost model trained on UCI Abalone dataset that accepts transformed features (generated by featurizer model) and returns predictions in JSON format.\n",
    "\n",
    "![Abalone Predictor Pipeline](./images/serial-inference-pipeline.png)\n",
    "\n",
    "## Building Custom inference containers\n",
    "\n",
    "1. **Step 1:** Build inference container with featurizer model - Refer to [`featurizer.ipynb`](./featurizer/featurizer.ipynb) Notebook\n",
    "1. **Step 2:** Build inference container with trained XGBoost model - Refer to [`predictor.ipynb`](./predictor/predictor.ipynb) Notebook\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd608469-ee3e-4086-9793-019e71057226",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "Ensure both [featurizer.ipynb](./featurizer/featurizer.ipynb) and [predictor.ipynb](./predictor/predictor.ipynb) are completed before running this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "443d02b3-4dc0-43c0-904a-93f3a3d87e52",
   "metadata": {},
   "source": [
    "### Upload Models to S3\n",
    "\n",
    "Upload models generated by **Step 1:** from [`featurizer.ipynb`](./featurizer/featurizer.ipynb) Notebook and **Step 2:** from [`predictor.ipynb`](./predictor/predictor.ipynb) Notebook to S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd3d6e-6e7d-4b0f-af1f-6ea002784ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role, session\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader, s3_path_join\n",
    "\n",
    "sm_session = session.Session()\n",
    "region = sm_session._region_name\n",
    "role = get_execution_role()\n",
    "bucket = sm_session.default_bucket()\n",
    "\n",
    "prefix = \"sagemaker/abalone/models/byoc\"\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "featurizer_model_data = s3_path_join(f\"s3://{bucket}/{prefix}\", \"featurizer\")\n",
    "predictor_model_data = s3_path_join(f\"s3://{bucket}/{prefix}\", \"predictor\")\n",
    "\n",
    "print(f\"Uploading featurizer model to {featurizer_model_data}\")\n",
    "S3Uploader.upload(\n",
    "    local_path=\"./featurizer/models/model.tar.gz\",\n",
    "    desired_s3_uri=featurizer_model_data,\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "\n",
    "print(f\"Uploading predictor model to {predictor_model_data}\")\n",
    "S3Uploader.upload(\n",
    "    local_path=\"./predictor/models/model.tar.gz\",\n",
    "    desired_s3_uri=predictor_model_data,\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "\n",
    "# Verify model files after upload\n",
    "print(f\"Listing files under s3://{bucket}/{prefix}\")\n",
    "print(S3Downloader.list(featurizer_model_data))\n",
    "print(S3Downloader.list(predictor_model_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2afca977-3f3d-4676-b339-ee19e1cd1025",
   "metadata": {},
   "source": [
    "### Create Models and Pipeline Model\n",
    "\n",
    "Now, we create two model objects to be combined later to a Pipeline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616caf67-66f6-4333-aef5-b9e7f0740bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "from sagemaker.model import Model\n",
    "\n",
    "suffix = f\"{str(uuid4())[:5]}-{datetime.now().strftime('%d%b%Y')}\"\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "\n",
    "# Featurizer Model (SKLearn Model)\n",
    "image_name = \"abalone/featurizer\"\n",
    "sklearn_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{image_name}:latest\"\n",
    "\n",
    "featurizer_model_name = f\"AbaloneXGB-featurizer-{suffix}\"\n",
    "print(f\"Creating Featurizer model: {featurizer_model_name}\")\n",
    "sklearn_model = Model(\n",
    "    image_uri=sklearn_image_uri,\n",
    "    name=featurizer_model_name,\n",
    "    model_data=f\"{featurizer_model_data}/model.tar.gz\",\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# Predictor Model (XGBoost Model)\n",
    "image_name = \"abalone/predictor\"\n",
    "xgboost_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{image_name}:latest\"\n",
    "\n",
    "predictor_model_name = f\"AbaloneXGB-Predictor-{suffix}\"\n",
    "print(f\"Creating Predictor model: {predictor_model_name}\")\n",
    "xgboost_model = Model(\n",
    "    image_uri=xgboost_image_uri,\n",
    "    name=predictor_model_name,\n",
    "    model_data=f\"{predictor_model_data}/model.tar.gz\",\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1a43ba0-b6c7-448e-9723-407363b7f54e",
   "metadata": {},
   "source": [
    "### Create Pipeline Model\n",
    "\n",
    "1. Create a Pipeline model with `sklearn_model` and `xgboost_model` to act a serial inference pipline.\n",
    "1. Deploy Pipeline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d113217c-e048-4f16-b645-504c82f310ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pipeline import PipelineModel\n",
    "\n",
    "pipeline_model_name = f\"Abalone-pipeline-{suffix}\"\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    name=pipeline_model_name,\n",
    "    role=role,\n",
    "    models=[sklearn_model, xgboost_model],\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "\n",
    "print(f\"Deploying pipeline model {pipeline_model_name}...\")\n",
    "predictor = pipeline_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63e90fac-cae7-4a9c-a4ed-90a7a9fe4f09",
   "metadata": {},
   "source": [
    "### Test inference on Endpoint with Pipeline Model\n",
    "\n",
    "- Instantiate a `Predictor` class from `sagemaker.predictor` module\n",
    "- Use `CSVSerialzier` to serialize payload\n",
    "- and `JSONDeSerializer` for deserializing output (JSON) from the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d6992-4f0d-461b-bd66-0c12d99b6ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Use the endpoint_name you specified when deploying the pipeline\n",
    "endpoint_name = pipeline_model_name\n",
    "\n",
    "# Let's use the test dataset in featurizer/data directory\n",
    "LOCALDIR = \"./data\"\n",
    "local_test_dataset = f\"{LOCALDIR}/abalone_test.csv\"\n",
    "\n",
    "limit = 15\n",
    "i = 0\n",
    "\n",
    "with open(local_test_dataset, \"r\") as _f:\n",
    "    for row in _f:\n",
    "        # Skip headers row\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "        elif i <= limit:\n",
    "            row = row.rstrip(\"\\n\")\n",
    "            splits = row.split(\",\")\n",
    "            # Remove the target column (last column)\n",
    "            label = splits.pop(-1)\n",
    "            input_cols = \",\".join(s for s in splits)\n",
    "            prediction = None\n",
    "            try:\n",
    "                predictor = Predictor(\n",
    "                    endpoint_name=endpoint_name,\n",
    "                    sagemaker_session=sm_session,\n",
    "                    serializer=CSVSerializer(),\n",
    "                    deserializer=JSONDeserializer(),\n",
    "                )\n",
    "                response = predictor.predict(input_cols)\n",
    "                # response = {'result' : [predicted_value]}\n",
    "                print(f\"True: {label} | Predicted: {response['result'][0]}\")\n",
    "                i += 1\n",
    "                sleep(0.15)\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction error: {e}\")\n",
    "                pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c71f4b74-6cb0-4020-8cdf-6c36b1d665b7",
   "metadata": {},
   "source": [
    "### (Optional) Verify Logs emitted by the endpoint in CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc7226-f527-4e94-984e-37836d5cc8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "logs_client = boto3.client(\"logs\")\n",
    "end_time = datetime.utcnow()\n",
    "start_time = end_time - timedelta(minutes=15)\n",
    "\n",
    "log_group_name = f\"/aws/sagemaker/Endpoints/{endpoint_name}\"\n",
    "log_streams = logs_client.describe_log_streams(logGroupName=log_group_name)\n",
    "log_stream_name = log_streams[\"logStreams\"][0][\"logStreamName\"]\n",
    "\n",
    "# Retrieve the logs\n",
    "logs = logs_client.get_log_events(\n",
    "    logGroupName=log_group_name,\n",
    "    logStreamName=log_stream_name,\n",
    "    startTime=int(start_time.timestamp() * 1000),\n",
    "    endTime=int(end_time.timestamp() * 1000),\n",
    ")\n",
    "\n",
    "# Print the logs\n",
    "for event in logs[\"events\"]:\n",
    "    print(f\"{datetime.fromtimestamp(event['timestamp'] // 1000)}: {event['message']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b3865d9-7743-4841-aa21-68fe011db86a",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba487c-5606-4d4d-879f-f1c6512a768a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete endpoint, model\n",
    "try:\n",
    "    print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting EP: {endpoint_name}\\n{e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting model: {endpoint_name}\")\n",
    "    sm_client.delete_model(ModelName=endpoint_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting model: {endpoint_name}\\n{e}\")\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_aiml",
   "language": "python",
   "name": "conda_aiml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
