{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise-Grade ML : Part 1 - Prepare and Use the ML Gateway Pattern for Inference\n",
    "\n",
    "## Data Preperation and Inference Using SageMaker Feature Store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contents'> </a>\n",
    "\n",
    "### Contents\n",
    "\n",
    "----\n",
    "\n",
    "- [Motivation](#motivation)\n",
    "- [Architecure](#arch)\n",
    "- [Import Libraries and SageMaker Session Variables](#imports)\n",
    "- [Data and Features](#data)\n",
    "- [Clean Up](#clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='motivation' > </a>\n",
    "\n",
    "## Motivation\n",
    "\n",
    "----\n",
    "\n",
    "Data Science projects often start in an experimental phase in which transformations on features are experimented with, algorithms are selected and tried for determining if they can fit the data distribution well enough for reliable predictions, tuning is done with various hyper-parameters and so on. \n",
    "\n",
    "As an organization matures in their Machine Learning (ML) Journey, they will find that they will then transition to an automated ML or MLOps phase where the pipelines for data preparation, training, deployment, monitoring will all need to be automated.\n",
    "\n",
    "In order to raise the maturity of projects to an Enterprise Scale that can fulfill business needs, sustain business-level continuity, scale, security and performance, the need for integrating data science experiments with machine learning deployment patterns and best-practices will grow in importance and will save you time and money.\n",
    "\n",
    "In this blog series on ML Patterns, we will start by focusing on Deployment Patterns and Best-Practices within the ML lifecycle : exploring the considerations and options that present themselves, post-training; on the serving/inference/prediction phases of the ML lifecycle.\n",
    "\n",
    "There are many ways in which we can expose an endpoint that was deployed as a hosted SageMaker endpoint: these variations are summarized in the ML Gateway Pattern with mandatory and optional components. Through this series of blogs we will outline options and their context, pros and cons for helping you decide what components to use for your specific workload and use-case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arch'> </a>\n",
    "\n",
    "## Architecture\n",
    "\n",
    "----\n",
    "\n",
    "Here we break down the example in this blog into four parts:\n",
    "\n",
    "1. Data prep\n",
    "    1. For preparation we will load the CSV into s3\n",
    "    2. Then create and populate a Feature Store that can be used for training our model\n",
    "    3. Later we will use Athena to load the data from the feature store into a dataframe\n",
    "2. Training and deployment\n",
    "3. Inference\n",
    "4. MLOps â€” deployment of a Cloud Formation Template \n",
    "\n",
    "\n",
    "\n",
    "![image](./images/ml-gateway-pattern.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imports'> </a>\n",
    "\n",
    "## Import Libraries and SageMaker Session Variables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "\n",
    "def import_or_install(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        pip.main(['install', package])  \n",
    "        \n",
    "import_or_install('sagemaker')\n",
    "import_or_install('boto3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.session import production_variant\n",
    "from sagemaker.model_monitor import DataCaptureConfig, CronExpressionGenerator, DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "import datetime as datetime\n",
    "import statistics\n",
    "import numpy as np\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Session variables\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"ml-gateway\"\n",
    "region = sess.boto_region_name\n",
    "\n",
    "print(f\"Region: {region}\\nBucket: {bucket}\\nPrefix: {prefix}\\n\")\n",
    "\n",
    "# Data source location\n",
    "claims_url = \"https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/end_to_end/fraud_detection/data/claims_preprocessed.csv\"\n",
    "customers_url = \"https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/end_to_end/fraud_detection/data/customers_preprocessed.csv\"\n",
    "\n",
    "# Feature Store parameters\n",
    "claims_feature_group_name = \"claims-feature-group\"\n",
    "customers_feature_group_name = \"customers-feature-group\"\n",
    "claims_feature_group_description = \"Claims feature group\"\n",
    "customers_feature_group_description = \"Customers feature group\"\n",
    "id_name = \"policy_id\"\n",
    "event_time_name = \"event_time\"\n",
    "claims_offline_feature_group_bucket = f\"s3://{bucket}/claims-feature-group\"\n",
    "customers_offline_feature_group_bucket = f\"s3://{bucket}/customers-feature-group\"\n",
    "\n",
    "# SageMaker training\n",
    "s3_input_train_uri = f\"s3://{bucket}/{prefix}/data/train/train.csv\"\n",
    "s3_input_test_uri = f\"s3://{bucket}/{prefix}/data/test/test.csv\"\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "train_base_job_name = \"xgboost-model\"\n",
    "\n",
    "# Model names\n",
    "model1_name = \"xgboost-model-1\"\n",
    "model2_name = \"xgboost-model-2\"\n",
    "\n",
    "# SageMaker endpoint\n",
    "endpoint_name = \"xgboost-claims-fraud\"\n",
    "deploy_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "# SageMaker Model Monitor\n",
    "monitor_schedule_name = f\"{prefix}-monitor-schedule\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'> </a>\n",
    "\n",
    "## Data and Features\n",
    "\n",
    "----\n",
    "\n",
    "The data we are using is the same synthetic data that was created in this blog post for the [End-to-end ML Lifecycle with Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/architect-and-build-the-full-machine-learning-lifecycle-with-amazon-sagemaker/). \n",
    "The use-case in the above link/blog is Autoclaim Fraud Detection. We will be using the same datasets to demonstrate the ML Gateway Pattern in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get claims and customer data from existing aws-samples location\n",
    "claims_df = pd.read_csv(claims_url)\n",
    "customers_df = pd.read_csv(customers_url)\n",
    "\n",
    "# If your DataFrame doesn't have a timestamp, you can just create one\n",
    "timestamp = pd.to_datetime(\"now\").timestamp()\n",
    "claims_df[event_time_name] = timestamp\n",
    "customers_df[event_time_name] = timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_dtypes, customers_dtypes = helpers.get_datatypes()\n",
    "claims_df = claims_df.astype(claims_dtypes)\n",
    "customers_df = customers_df.astype(customers_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add data to Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_feature_group, claims_feature_group_exists = helpers.create_feature_group(\n",
    "    claims_feature_group_name,\n",
    "    claims_feature_group_description,\n",
    "    claims_df,\n",
    "    id_name,\n",
    "    event_time_name,\n",
    "    claims_offline_feature_group_bucket,\n",
    "    sess,\n",
    "    role,\n",
    ")\n",
    "\n",
    "customers_feature_group, customers_feature_group_exists = helpers.create_feature_group(\n",
    "    customers_feature_group_name,\n",
    "    customers_feature_group_description,\n",
    "    customers_df,\n",
    "    id_name,\n",
    "    event_time_name,\n",
    "    customers_offline_feature_group_bucket,\n",
    "    sess,\n",
    "    role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data to Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data to Feature Store\n",
    "feature_store_client = boto3.client(\"sagemaker-featurestore-runtime\", region_name=region)\n",
    "if not claims_feature_group_exists:\n",
    "    helpers.ingest_df_to_feature_group(claims_df, claims_feature_group_name, feature_store_client)\n",
    "if not customers_feature_group_exists:\n",
    "    helpers.ingest_df_to_feature_group(\n",
    "        customers_df, customers_feature_group_name, feature_store_client\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and test data from Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for data to be synchronized with offline Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then query feature store to get training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_query = claims_feature_group.athena_query()\n",
    "customers_query = customers_feature_group.athena_query()\n",
    "\n",
    "claims_table = claims_query.table_name\n",
    "customers_table = customers_query.table_name\n",
    "database_name = customers_query.database\n",
    "\n",
    "feature_columns = list(set(claims_df.columns) ^ set(customers_df.columns))\n",
    "feature_columns_string = \", \".join(f'\"{c}\"' for c in feature_columns)\n",
    "feature_columns_string = f'\"{claims_table}\".{id_name} as {id_name}, ' + feature_columns_string\n",
    "\n",
    "query_string = f\"\"\"\n",
    "SELECT {feature_columns_string}\n",
    "FROM \"{claims_table}\" LEFT JOIN \"{customers_table}\" \n",
    "ON \"{claims_table}\".{id_name} = \"{customers_table}\".{id_name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_query.run(query_string=query_string, output_location=f\"s3://{bucket}/{prefix}/query_results\")\n",
    "claims_query.wait()\n",
    "dataset = claims_query.as_dataframe()\n",
    "\n",
    "# Create data directory to store local data\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "dataset.to_csv(\"data/claims_customer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save training and test sets locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = [\"fraud\"] + list(dataset.drop([\"fraud\", \"policy_id\"], axis=1).columns)\n",
    "train = dataset.sample(frac=0.80, random_state=0)[col_order]\n",
    "test = dataset.drop(train.index)[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"data/train.csv\", index=False)\n",
    "test.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload datasets to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "s3_client.upload_file(\n",
    "    Filename=\"data/train.csv\", Bucket=bucket, Key=f\"{prefix}/data/train/train.csv\"\n",
    ")\n",
    "s3_client.upload_file(Filename=\"data/test.csv\", Bucket=bucket, Key=f\"{prefix}/data/test/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and deploy an XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = TrainingInput(s3_input_train_uri, content_type=\"csv\")\n",
    "s3_input_test = TrainingInput(s3_input_test_uri, content_type=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"3\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\",\n",
    "}\n",
    "\n",
    "estimator_parameters = {\n",
    "    \"entry_point\": \"code/train_deploy.py\",\n",
    "    \"instance_type\": train_instance_type,\n",
    "    \"instance_count\": 1,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"role\": role,\n",
    "    \"base_job_name\": train_base_job_name,\n",
    "    \"framework_version\": \"1.0-1\",\n",
    "    \"py_version\": \"py3\",\n",
    "}\n",
    "\n",
    "estimator = XGBoost(**estimator_parameters)\n",
    "inputs = {\"train\": s3_input_train, \"test\": s3_input_test}\n",
    "\n",
    "# Train the model if it already hasn't been trained\n",
    "existing_training_jobs = sess.sagemaker_client.list_training_jobs(\n",
    "    NameContains=train_base_job_name, MaxResults=30\n",
    ")[\"TrainingJobSummaries\"]\n",
    "if not existing_training_jobs:\n",
    "    estimator.fit(inputs)\n",
    "# Else fetch the latest training job\n",
    "else:\n",
    "    latest_training_job_name = existing_training_jobs[0][\"TrainingJobName\"]\n",
    "    estimator = XGBoost.attach(latest_training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two SageMaker models to deploy behind a single endpoint using SageMaker Production Variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = estimator.create_model(entry_point=\"code/train_deploy.py\", role=role, name=model1_name)\n",
    "model1._create_sagemaker_model(instance_type=deploy_instance_type)\n",
    "\n",
    "model2 = estimator.create_model(entry_point=\"code/train_deploy.py\", role=role, name=model2_name)\n",
    "model2._create_sagemaker_model(instance_type=deploy_instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_1 = production_variant(\n",
    "    model_name=model1_name,\n",
    "    instance_type=deploy_instance_type,\n",
    "    initial_instance_count=1,\n",
    "    variant_name=\"Variant1\",\n",
    "    initial_weight=1,\n",
    ")\n",
    "\n",
    "\n",
    "variant_2 = production_variant(\n",
    "    model_name=model2_name,\n",
    "    instance_type=deploy_instance_type,\n",
    "    initial_instance_count=1,\n",
    "    variant_name=\"Variant2\",\n",
    "    initial_weight=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Model Monitor's Data Capture for Production Variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_capture_upload_path = f\"s3://{bucket}/{prefix}/model_monitor\"\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path\n",
    ")\n",
    "\n",
    "data_capture_config_dict = data_capture_config._to_request_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the Production Variant endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already deployed, deploy the model\n",
    "existing_endpoints = sess.sagemaker_client.list_endpoints(\n",
    "    NameContains=endpoint_name, MaxResults=30\n",
    ")[\"Endpoints\"]\n",
    "if not existing_endpoints:\n",
    "    sess.endpoint_from_production_variants(\n",
    "        name=endpoint_name,\n",
    "        production_variants=[variant_1, variant_2],\n",
    "        data_capture_config_dict=data_capture_config_dict,\n",
    "    )\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=sess,\n",
    "        serializer=CSVSerializer(),\n",
    "        deserializer=CSVDeserializer(),\n",
    "    )\n",
    "else:\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=sess,\n",
    "        serializer=CSVSerializer(),\n",
    "        deserializer=CSVDeserializer(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline data is the training data that we saved as CSV\n",
    "baseline_data_uri = s3_input_train_uri\n",
    "baseline_results_uri = f\"s3://{bucket}/{prefix}/model_monitor/baseline_output\"\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the monitoring job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_violations_uri = f\"s3://{bucket}/{prefix}/model_monitor/violations\"\n",
    "\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=monitor_schedule_name,\n",
    "    endpoint_input=endpoint_name,\n",
    "    output_s3_uri=baseline_violations_uri,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")\n",
    "\n",
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print(\"Schedule status: {}\".format(desc_schedule_result[\"MonitoringScheduleStatus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Feature Store in Real-Time Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how you can aggregate data from multiple Feature Groups and use those features as input to a SageMaker endpoint in a low-latency fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(policy_id, featurestore_runtime):\n",
    "    t0 = datetime.datetime.now()\n",
    "    customer_record_response = featurestore_runtime.get_record(\n",
    "        FeatureGroupName=\"customers-feature-group\", RecordIdentifierValueAsString=str(policy_id)\n",
    "    )\n",
    "\n",
    "    claims_record_response = featurestore_runtime.get_record(\n",
    "        FeatureGroupName=\"claims-feature-group\", RecordIdentifierValueAsString=str(policy_id)\n",
    "    )\n",
    "\n",
    "    t1 = datetime.datetime.now()\n",
    "\n",
    "    customer_record = customer_record_response[\"Record\"]\n",
    "    customer_df = pd.DataFrame(customer_record).set_index(\"FeatureName\")\n",
    "    claims_record = claims_record_response[\"Record\"]\n",
    "    claims_df = pd.DataFrame(claims_record).set_index(\"FeatureName\")\n",
    "\n",
    "    joined_df = pd.concat([claims_df, customer_df]).loc[col_order].drop(\"fraud\")\n",
    "    payload = \",\".join(joined_df[\"ValueAsString\"])\n",
    "    prediction = float(\n",
    "        predictor.predict(\n",
    "            payload, initial_args={\"ContentType\": \"text/csv\"}, target_variant=\"Variant1\"\n",
    "        )[0][0]\n",
    "    )\n",
    "\n",
    "    diff = t1 - t0\n",
    "    minutes, seconds = divmod(diff.total_seconds(), 60)\n",
    "    timer.append(seconds)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Instantiate Feature Store Runtime client\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "featurestore_runtime = boto_session.client(\n",
    "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
    ")\n",
    "\n",
    "MAX_POLICY_IDS = 100\n",
    "timer = []\n",
    "for policy_id in range(1, MAX_POLICY_IDS + 1):\n",
    "    prediction = get_prediction(policy_id, featurestore_runtime)\n",
    "    print(f\"Probablitity the claim from policy {int(policy_id)} is fraudulent:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get latencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_array = np.array(timer)\n",
    "print(\n",
    "    f\"p95: {np.percentile(timer_array,95)}, p99: {np.percentile(timer_array,99)}, mean: {np.mean(timer_array)} for {MAX_POLICY_IDS} distinct Feature Store gets across two Feature Groups\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ML Gateway with Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, write out a Lambda function script. Make sure to replace the `ENDPOINT_NAME` variable with the name of your deployed SageMaker endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lambda function will check if the policy ID from a user request already exists in Feature Store. If so, it will fetch the features associated with the policy ID from both Feature Groups and feed them as inputs into the SageMaker endpoint.\n",
    "\n",
    "If there are no features in Feature Store for the given policy ID, then take the raw data from the request, transform it, store it in Feature Store, and return a prediction back to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_function.py\n",
    "\n",
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import re\n",
    "\n",
    "ENDPOINT_NAME = \"xgboost-claims-fraud\"  # REPLACE WITH SAGEMAKER ENDPOINT NAME\n",
    "ENDPOINT_NAME = ENDPOINT_NAME.strip()\n",
    "runtime = boto3.client(\"runtime.sagemaker\")\n",
    "\n",
    "# Instantiate Feature Store Runtime client\n",
    "# get current region\n",
    "region = boto3.Session().region_name\n",
    "print(f\"region : {region}\\n\")\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "featurestore_runtime = boto_session.client(\n",
    "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
    ")\n",
    "\n",
    "\n",
    "def get_payload(policy_id):\n",
    "    \"\"\"Get records associated with the policy id from both\n",
    "    Feature Groups\n",
    "\n",
    "    Args:\n",
    "        policy_id: int or str\n",
    "\n",
    "    Returns:\n",
    "        str\n",
    "    \"\"\"\n",
    "\n",
    "    col_order = [\n",
    "        \"fraud\",\n",
    "        \"driver_relationship_child\",\n",
    "        \"num_insurers_past_5_years\",\n",
    "        \"incident_severity\",\n",
    "        \"driver_relationship_self\",\n",
    "        \"authorities_contacted_none\",\n",
    "        \"months_as_customer\",\n",
    "        \"driver_relationship_na\",\n",
    "        \"policy_liability\",\n",
    "        \"collision_type_side\",\n",
    "        \"collision_type_front\",\n",
    "        \"incident_month\",\n",
    "        \"num_claims_past_year\",\n",
    "        \"customer_gender_male\",\n",
    "        \"num_vehicles_involved\",\n",
    "        \"customer_education\",\n",
    "        \"authorities_contacted_ambulance\",\n",
    "        \"police_report_available\",\n",
    "        \"incident_dow\",\n",
    "        \"vehicle_claim\",\n",
    "        \"collision_type_rear\",\n",
    "        \"customer_gender_female\",\n",
    "        \"incident_day\",\n",
    "        \"policy_state_or\",\n",
    "        \"customer_age\",\n",
    "        \"policy_state_wa\",\n",
    "        \"injury_claim\",\n",
    "        \"policy_state_id\",\n",
    "        \"driver_relationship_spouse\",\n",
    "        \"policy_deductable\",\n",
    "        \"num_injuries\",\n",
    "        \"collision_type_na\",\n",
    "        \"driver_relationship_other\",\n",
    "        \"incident_hour\",\n",
    "        \"incident_type_theft\",\n",
    "        \"incident_type_breakin\",\n",
    "        \"num_witnesses\",\n",
    "        \"policy_state_ca\",\n",
    "        \"policy_state_nv\",\n",
    "        \"incident_type_collision\",\n",
    "        \"auto_year\",\n",
    "        \"authorities_contacted_police\",\n",
    "        \"policy_state_az\",\n",
    "        \"policy_annual_premium\",\n",
    "        \"total_claim_amount\",\n",
    "        \"authorities_contacted_fire\",\n",
    "    ]\n",
    "    t0 = datetime.datetime.now()\n",
    "    customer_record_response = featurestore_runtime.get_record(\n",
    "        FeatureGroupName=\"customers-feature-group\", RecordIdentifierValueAsString=str(policy_id)\n",
    "    )\n",
    "    claims_record_response = featurestore_runtime.get_record(\n",
    "        FeatureGroupName=\"claims-feature-group\", RecordIdentifierValueAsString=str(policy_id)\n",
    "    )\n",
    "    t1 = datetime.datetime.now()\n",
    "    customer_record = customer_record_response[\"Record\"]\n",
    "    customer_df = pd.DataFrame(customer_record).set_index(\"FeatureName\")\n",
    "    claims_record = claims_record_response[\"Record\"]\n",
    "    claims_df = pd.DataFrame(claims_record).set_index(\"FeatureName\")\n",
    "    joined_df = pd.concat([claims_df, customer_df]).loc[col_order].drop(\"fraud\")\n",
    "    payload = \",\".join(joined_df[\"ValueAsString\"])\n",
    "    return payload\n",
    "\n",
    "\n",
    "def response(message, status_code):\n",
    "    return {\n",
    "        \"statusCode\": str(status_code),\n",
    "        \"body\": json.dumps(message),\n",
    "        \"headers\": {\"Content-Type\": \"application/json\", \"Access-Control-Allow-Origin\": \"*\"},\n",
    "    }\n",
    "\n",
    "\n",
    "def one_hot_encoder(df: pd.DataFrame, input_column: str, categories: list) -> None:\n",
    "    \"\"\"A one hot encoder similiar to the one in Data Wrangler.\n",
    "\n",
    "    Args:\n",
    "        df: A Pandas DataFrame.\n",
    "        input_column: The name of the column which contains the categorical values.\n",
    "        categories: The list of categorical values which was available during training.\n",
    "\n",
    "    Returns:\n",
    "        None: The DataFrame is updated in place with the encoded features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # NaN types are converted to literal `na` in Data Wrangler during one-hot encoding\n",
    "    if \"na\" in categories:\n",
    "        df[input_column].fillna(\"na\", inplace=True)\n",
    "    for c in categories:\n",
    "        df[f\"{input_column}_{c}\"] = 0\n",
    "    for idx, val in df[input_column].iteritems():\n",
    "        df.at[idx, f\"{input_column}_{val}\"] = 1\n",
    "    df.drop(input_column, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def transform_claims_data(claims_data: dict) -> pd.DataFrame:\n",
    "    \"\"\"Transforms the inbound claims data to the feature store format.\n",
    "\n",
    "    Args:\n",
    "        claims_data: A dictionary containing the claims data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A Pandas DataFrame containing the processed claims data.\n",
    "    \"\"\"\n",
    "\n",
    "    claims_df = pd.DataFrame.from_dict(claims_data)\n",
    "\n",
    "    # (3) convert cat columns to lowercase\n",
    "    claims_df = claims_df.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "    # (4-6) format string\n",
    "    invalid_char = re.compile(\"[-@#$%^&*()_+=/\\`~{}|<>?]\")\n",
    "    claims_df[\"driver_relationship\"].replace(invalid_char, \" \", regex=True, inplace=True)\n",
    "    claims_df[\"collision_type\"].replace(invalid_char, \" \", regex=True, inplace=True)\n",
    "    claims_df[\"incident_type\"].replace(invalid_char, \" \", regex=True, inplace=True)\n",
    "\n",
    "    # (7-10) one hot encode\n",
    "    one_hot_encoder(claims_df, \"driver_relationship\", [\"spouse\", \"self\", \"child\", \"na\", \"other\"])\n",
    "    one_hot_encoder(claims_df, \"incident_type\", [\"collision\", \"breakin\", \"theft\"])\n",
    "    one_hot_encoder(claims_df, \"collision_type\", [\"front\", \"rear\", \"side\", \"na\"])\n",
    "    one_hot_encoder(claims_df, \"authorities_contacted\", [\"none\", \"police\", \"ambulance\", \"fire\"])\n",
    "\n",
    "    # (11-12) ordinal encode\n",
    "    claims_df[\"incident_severity\"] = claims_df[\"incident_severity\"].replace(\n",
    "        {\"minor\": 0, \"major\": 1, \"totaled\": 2, \"na\": 3}\n",
    "    )\n",
    "    claims_df[\"police_report_available\"] = claims_df[\"police_report_available\"].replace(\n",
    "        {\"no\": 0, \"yes\": 1, \"na\": 2}\n",
    "    )\n",
    "\n",
    "    # (13) create event_time\n",
    "    claims_df[\"event_time\"] = pd.to_datetime(\"now\").timestamp()\n",
    "\n",
    "    # NOTE: remaining steps in Flow file involve casting encoded columns from Float to Long, which is not\n",
    "    # necessary here.\n",
    "\n",
    "    return claims_df\n",
    "\n",
    "\n",
    "def transform_customers_data(customers_data: dict) -> pd.DataFrame:\n",
    "    \"\"\"Transforms the inbound customers data to the feature store format.\n",
    "\n",
    "    Args:\n",
    "        customers_data: A dictionary containing the customers data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A Pandas DataFrame containing the processed customers data.\n",
    "    \"\"\"\n",
    "    customers_df = pd.DataFrame.from_dict(customers_data)\n",
    "\n",
    "    # (3) convert cat columns to lowercase\n",
    "    customers_df = customers_df.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "    # (4) drop customer_zip\n",
    "    customers_df.drop(\"customer_zip\", axis=1, inplace=True)\n",
    "\n",
    "    # (5-6) one hot encode\n",
    "    one_hot_encoder(customers_df, \"customer_gender\", [\"unkown\", \"male\", \"female\", \"other\"])\n",
    "    one_hot_encoder(customers_df, \"policy_state\", [\"wa\", \"ca\", \"az\", \"or\", \"nv\", \"id\"])\n",
    "\n",
    "    # (7-8) ordinal encode\n",
    "    customers_df[\"customer_education\"] = customers_df[\"customer_education\"].replace(\n",
    "        {\n",
    "            \"below high school\": 0,\n",
    "            \"high school\": 1,\n",
    "            \"associate\": 2,\n",
    "            \"bachelor\": 3,\n",
    "            \"advanced degree\": 4,\n",
    "        }\n",
    "    )\n",
    "    customers_df[\"policy_liability\"] = customers_df[\"policy_liability\"].replace(\n",
    "        {\"15/30\": 0, \"25/50\": 1, \"30/60\": 2, \"100/200\": 3}\n",
    "    )\n",
    "\n",
    "    # NOTE: steps 9-18 in Flow file involve casting encoded columns from Float to Long, which is not\n",
    "    # necessary here.\n",
    "\n",
    "    # (19) create event_time\n",
    "    customers_df[\"event_time\"] = pd.to_datetime(\"now\").timestamp()\n",
    "\n",
    "    # (20-21) drop unused columns\n",
    "    customers_df.drop(\"customer_gender_unkown\", axis=1, inplace=True)\n",
    "    customers_df.drop(\"customer_gender_other\", axis=1, inplace=True)\n",
    "\n",
    "    return customers_df\n",
    "\n",
    "\n",
    "def ingest_df_to_feature_group(df, feature_group_name):\n",
    "    \"\"\"Ingests data from a DataFrame into a Feature Groups\n",
    "\n",
    "    Args:\n",
    "        df: pd.DataFrame\n",
    "        feature_group_name: str\n",
    "\n",
    "    Returns:\n",
    "        None: Data is already ingested into Feature Group\n",
    "    \"\"\"\n",
    "    success, fail = 0, 0\n",
    "    for row_num, row_series in df.astype(str).iterrows():\n",
    "        record = []\n",
    "        for key, value in row_series.to_dict().items():\n",
    "            record.append({\"FeatureName\": key, \"ValueAsString\": str(value)})\n",
    "        print(record)\n",
    "        response = featurestore_runtime.put_record(\n",
    "            FeatureGroupName=feature_group_name, Record=record\n",
    "        )\n",
    "        if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "    print(f\"Success = {success}\")\n",
    "    print(f\"Fail = {fail}\")\n",
    "\n",
    "\n",
    "def get_prediction(policy, target_variant):\n",
    "    \"\"\"Get records from Feature Groups and invoke SageMaker endpoint\n",
    "\n",
    "    Args:\n",
    "        policy: int or str\n",
    "\n",
    "    Returns:\n",
    "        dict to be used as a json response\n",
    "    \"\"\"\n",
    "    feature_record = get_payload(policy)\n",
    "    sm_response = runtime.invoke_endpoint(\n",
    "        EndpointName=ENDPOINT_NAME,\n",
    "        ContentType=\"text/csv\",\n",
    "        Accept=\"application/json\",\n",
    "        Body=feature_record,\n",
    "        TargetVariant=target_variant,\n",
    "    )\n",
    "    result = json.loads(sm_response[\"Body\"].read().decode())\n",
    "    pred = result[0]\n",
    "    return response({\"prediction\": pred}, 200)\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(\"Received event: \" + json.dumps(event, indent=2))\n",
    "\n",
    "    # If request came from API Gateway\n",
    "    try:\n",
    "        data = json.loads(event[\"body\"])\n",
    "\n",
    "    # Otherwise it's just a test case\n",
    "    except:\n",
    "        data = json.loads(json.dumps(event))\n",
    "\n",
    "    policy = data[\"claim\"][\"policy_id\"][\"0\"]\n",
    "    target_variant = data[\"variant\"]\n",
    "\n",
    "    try:\n",
    "        return get_prediction(policy, target_variant)\n",
    "    except:\n",
    "        # Get raw data from request\n",
    "        claim = data[\"claim\"]\n",
    "        customer = data[\"customer\"]\n",
    "        # Transform data\n",
    "        processed_claims_df = transform_claims_data(claim)\n",
    "        processed_customers_df = transform_customers_data(customer)\n",
    "        # Ingest newly processed records into Feature Groups\n",
    "        ingest_df_to_feature_group(processed_claims_df, \"claims-feature-group\")\n",
    "        ingest_df_to_feature_group(processed_customers_df, \"customers-feature-group\")\n",
    "        # Return prediction\n",
    "        return get_prediction(policy, target_variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Lambda code to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(\"function\", \"zip\", \".\", \"lambda_function.py\")\n",
    "s3_bucket_uri = f\"s3://{bucket}\"\n",
    "\n",
    "!aws s3 cp function.zip {s3_bucket_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `helpers` library to deploy what we call an ML Gateway pattern. This will spin up an API Gateway endpoint that's attached to a Lambda function with code you've seen above. This is the gateway that ties together the SageMaker Feature Store and a model deployed as a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy this ML Gateway pattern, you need to add the following permissions to your SageMaker execution role:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Effect\": \"Allow\",\n",
    "    \"Action\": [\n",
    "        \"apigateway:*\"\n",
    "    ],\n",
    "    \"Resource\": [\n",
    "        \"*\"\n",
    "    ]\n",
    "},\n",
    "{\n",
    "    \"Effect\": \"Allow\",\n",
    "    \"Action\": [\n",
    "        \"lambda:GetLayerVersion\"\n",
    "    ],\n",
    "    \"Resource\": [\n",
    "        \"*\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Alternatively, you can add the managed `AWSLambdaFullAccess` and `AmazonAPIGatewayAdministrator` policies to your SageMaker execution role but keep in mind these particular managed policies are overly permissive and should be reviewed for least privileges before production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_gateway_url = helpers.deploy_ml_gateway_pattern(endpoint_name, region, bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above API Gateway URL, we can call our endpoint with Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_gateway_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint\n",
    "url = api_gateway_url\n",
    "\n",
    "# User request data\n",
    "input_data = {\n",
    "    \"variant\": \"Variant1\",\n",
    "    \"claim\": {\n",
    "        \"policy_id\": {\"0\": \"999999999\"},\n",
    "        \"driver_relationship\": {\"0\": \"Spouse\"},\n",
    "        \"incident_type\": {\"0\": \"Collision\"},\n",
    "        \"collision_type\": {\"0\": \"Front\"},\n",
    "        \"incident_severity\": {\"0\": \"Minor\"},\n",
    "        \"authorities_contacted\": {\"0\": \"None\"},\n",
    "        \"num_vehicles_involved\": {\"0\": 2},\n",
    "        \"num_injuries\": {\"0\": 0},\n",
    "        \"num_witnesses\": {\"0\": 0},\n",
    "        \"police_report_available\": {\"0\": \"No\"},\n",
    "        \"injury_claim\": {\"0\": 71600},\n",
    "        \"vehicle_claim\": {\"0\": 8913.6687631788},\n",
    "        \"total_claim_amount\": {\"0\": 80513.6687631788},\n",
    "        \"incident_month\": {\"0\": 3},\n",
    "        \"incident_day\": {\"0\": 17},\n",
    "        \"incident_dow\": {\"0\": 6},\n",
    "        \"incident_hour\": {\"0\": 8},\n",
    "        \"fraud\": {\"0\": 0},\n",
    "    },\n",
    "    \"customer\": {\n",
    "        \"policy_id\": {\"0\": \"999999999\"},\n",
    "        \"customer_age\": {\"0\": 54},\n",
    "        \"months_as_customer\": {\"0\": 94},\n",
    "        \"num_claims_past_year\": {\"0\": 0},\n",
    "        \"num_insurers_past_5_years\": {\"0\": 1},\n",
    "        \"policy_state\": {\"0\": \"WA\"},\n",
    "        \"policy_deductable\": {\"0\": 750},\n",
    "        \"policy_annual_premium\": {\"0\": 3000},\n",
    "        \"policy_liability\": {\"0\": \"25/50\"},\n",
    "        \"customer_zip\": {\"0\": 99207},\n",
    "        \"customer_gender\": {\"0\": \"Unkown\"},\n",
    "        \"customer_education\": {\"0\": \"Associate\"},\n",
    "        \"auto_year\": {\"0\": 2006},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Hit endpoint\n",
    "r = requests.post(url, json=input_data)\n",
    "\n",
    "# Print response\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up():\n",
    "    # Delete the online Feature Groups\n",
    "    claims_feature_group.delete()\n",
    "    customers_feature_group.delete()\n",
    "\n",
    "    # Delete the offline Feature Groups\n",
    "    !aws s3 rm {claims_offline_feature_group_bucket} --recursive\n",
    "    !aws s3 rm {customers_offline_feature_group_bucket} --recursive\n",
    "    !aws s3 rm {prefix} --recursive\n",
    "\n",
    "    # Delete training and test data\n",
    "    s3_prefix_uri = f\"s3://{bucket}/{prefix}\"\n",
    "    !aws s3 rm {s3_prefix_uri} --recursive\n",
    "\n",
    "    # Delete model monitor\n",
    "    !aws sagemaker delete-monitoring-schedule --monitoring-schedule-name {monitor_schedule_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell to clean up the Feature Groups, the offline Featrue Group S3 buckets, and the Model Monitor schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_up()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
