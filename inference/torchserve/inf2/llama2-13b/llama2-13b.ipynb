{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0524d4b4",
   "metadata": {},
   "source": [
    "#  Using TorchServe on SageMaker Inf2.24xlarge with LLAMAv2-13B\n",
    "\n",
    "\n",
    "## Step 1: Let's bump up SageMaker and import stuff\n",
    "\n",
    "The wheel installed here is a private preview wheel, you need to add into allowlist to run this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6901ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26737573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest aws cli v2 if it is not installed\n",
    "!curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "!unzip awscliv2.zip\n",
    "!ln -s /usr/local/aws-cli/v2/2.13.6/bin/aws /home/ec2-user/anaconda3/envs/python3/bin/aws \n",
    "!aws --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d68be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install sagemaker pip --upgrade  --quiet\n",
    "!pip install numpy\n",
    "!pip install pillow\n",
    "!pip install -U sagemaker\n",
    "!pip install -U boto \n",
    "!pip install -U botocore\n",
    "!pip install -U boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e69324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note the following may error depending on which awscli is installed in your jupyter kernel, but that is ok \n",
    "#%pip install botocore-*-py3-none-any.whl boto3-*-py3-none-any.whl --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9f3132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws configure add-model --service-model file://runtime.sagemaker-2017-05-13.normal.json --service-name sagemaker-runtime-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4874a3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account=084495728311, region=us-west-2, role=arn:aws:iam::084495728311:role/service-role/AmazonSageMaker-ExecutionRole-20230505T104760\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "boto3_session=boto3.session.Session(region_name=\"us-west-2\")\n",
    "smr = boto3.client('sagemaker-runtime-demo')\n",
    "sm = boto3.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess= sagemaker.session.Session(boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr)  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "print(f'account={account_id}, region={region}, role={role}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c2c9a",
   "metadata": {},
   "source": [
    "## Step 2: Build a BYOD TorchServe Docker container and push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a00114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install our own dependencies\n",
    "!cat workspace/docker/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c82d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture build_output\n",
    "\n",
    "baseimage = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.12.0-ubuntu20.04\"\n",
    "reponame = \"neuronx212\"\n",
    "versiontag = \"ts\"\n",
    "\n",
    "# Build our own docker image\n",
    "!cd workspace/docker && ./build_and_push.sh {reponame} {versiontag} {baseimage} {region} {account}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d62e7",
   "metadata": {},
   "source": [
    "## [WIP]Step 3: AOT Pre-Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45acfc15",
   "metadata": {},
   "source": [
    "### [WIP] Precompile the model at local EC2\n",
    "Follow [Instruction](https://github.com/pytorch/serve/pull/2458/files#diff-bc416c811f749ead11ab8f100d5a4198fa453adc995b0760272563971638307d) at local EC2 to precompile the model and save it dir neuron_cache\n",
    "\n",
    "#### main steps\n",
    "* download llama-2-13b from HF \n",
    "* Save the model split checkpoints compatible with `transformers-neuronx`\n",
    "* Create EC2 model artifacts at local\n",
    "* [Start torchserve at local EC2](https://docs.google.com/document/d/1mfHTvc65bD9rbx0TBdYhxM5DjzQIZkWAt8v7y1pBCKc/edit?usp=sharing) to load llama-2-13b to generate dir neuron_cache\n",
    "\n",
    "#### Note: Turn on neuron_cache in function initialize of inf2_handler.py\n",
    "```\n",
    "os.environ[\"NEURONX_CACHE\"] = \"on\"\n",
    "os.environ[\"NEURONX_DUMP_TO\"] = f\"{model_dir}/neuron_cache\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48ab5d",
   "metadata": {},
   "source": [
    "### Upload model artifacts to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115df40",
   "metadata": {},
   "source": [
    "The model artifacts is available in s3://torchserve/mar_files/llama-2-13b/ which supports llama2-13b on neuronx batchSize = 1. You can copy it to your SM S3 model repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c259c",
   "metadata": {},
   "source": [
    "## Step 4: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771053b",
   "metadata": {},
   "source": [
    "### Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8bcfdfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.model.Model object at 0x7f61cc7dae30>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "instance_type = \"ml.inf2.24xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"ts-inf2-llama2-13b\")\n",
    "\n",
    "model = Model(\n",
    "    name=\"torchserve-inf2-llama2-13b\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n",
    "    model_data={\n",
    "        \"S3DataSource\": {\n",
    "                # In this example, I copied s3://torchserve/mar_files/llama-2-13b/ to my SM S3.\n",
    "                # s3://sagemaker-us-west-2-084495728311/torchserve/llama-2-13b/\n",
    "                \"S3Uri\": f\"s3://sagemaker-us-west-2-084495728311/torchserve/llama-2-13b/\", \n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"CompressionType\": \"None\",\n",
    "        }\n",
    "    },\n",
    "    #image_uri=f\"084495728311.dkr.ecr.us-west-2.amazonaws.com/neuronx212:latest\",\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53eaa416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your model is not compiled. Please compile your model before using Inferentia.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    volume_size=512, # increase the size to store large model\n",
    "    model_data_download_timeout=3600, # increase the timeout to download large model\n",
    "    container_startup_health_check_timeout=3600, # ncrease the timeout to load large model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39513f65",
   "metadata": {},
   "source": [
    "## Run the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "360839a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "class Parser:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "        data = self.buff.getvalue()\n",
    "        \n",
    "    def scan_lines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b'\\n':\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "                \n",
    "    def reset(self):\n",
    "        self.read_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f760056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the weather is really nice and I am planning on going to the beach. I am going to take my camera and take some pictures of the beach. I am going to take pictures of the sand, the water, and the people. I am also going to take pictures of the sunset. I am really excited to go to the beach and take pictures. The beach is a great place to take pictures. The sand, the water, and the people are all great subjects for pictures. The sunset is also a great subject for pictures "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "body = \"Today the weather is really nice and I am planning on\".encode('utf-8')\n",
    "resp = smr.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=body, ContentType=\"application/json\")\n",
    "event_stream = resp['Body']\n",
    "parser = Parser()\n",
    "for event in event_stream:\n",
    "    parser.write(event['PayloadPart']['Bytes'])\n",
    "    for line in parser.scan_lines():\n",
    "        print(line.decode(\"utf-8\"), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2e7b4",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bcaf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
