{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision for Medical Imaging - Pipeline Mode\n",
    "This notebook showcases techniques and services offer by SageMaker to build a model which predicts if an image of cells contains cancer. This notebook describes how to automate the ML workflow using SageMaker Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset for this demo comes from the [Camelyon16 Challenge](https://camelyon16.grand-challenge.org/) made available under the CC0 licencse. The raw data provided by the challenge has been processed into 96x96 pixel tiles by [Bas Veeling](https://github.com/basveeling/pcam) and also made available under the CC0 license. For detailed information on each dataset please see the papers below:\n",
    "* Ehteshami Bejnordi et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer. JAMA: The Journal of the American Medical Association, 318(22), 2199â€“2210. [doi:jama.2017.14585](https://doi.org/10.1001/jama.2017.14585)\n",
    "* B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling. \"Rotation Equivariant CNNs for Digital Pathology\". [arXiv:1806.03962](http://arxiv.org/abs/1806.03962)\n",
    "\n",
    "The tiled dataset from Bas Veeling is over 6GB of data. In order to easily run this demo, the dataset has been pruned to the first 14,000 images of the tiled dataset and comes included in the repo with this notebook for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Sagemaker SDK and Boto3\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>NOTE</b> You may get an error from pip's dependency resolver; you can ignore this error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade sagemaker boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "\n",
    "\n",
    "def import_or_install(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        ! pip install $package\n",
    "\n",
    "\n",
    "required_packages = [\"sagemaker\", \"boto3\", \"h5py\", \"tqdm\", \"matplotlib\", \"opencv-python\"]\n",
    "\n",
    "for package in required_packages:\n",
    "    import_or_install(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import zipfile\n",
    "import h5py\n",
    "import mxnet as mx\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Boto3 Clients and Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-west-2\"  # Change region as needed\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if directory exists\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# download zip file from public s3 bucket\n",
    "!wget -P data https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/pcam/medical_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"data/medical_images.zip\") as zf:\n",
    "    zf.extractall()\n",
    "with open(\"data/camelyon16_tiles.h5\", \"rb\") as hf:\n",
    "    f = h5py.File(hf, \"r\")\n",
    "\n",
    "    X = f[\"x\"][()]\n",
    "    y = f[\"y\"][()]\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to session s3 bucket\n",
    "s3_client.upload_file(\"data/medical_images.zip\", bucket, f\"data/medical_images.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete local copy\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"data/medical_images.zip\"):\n",
    "    os.remove(\"data/medical_images.zip\")\n",
    "else:\n",
    "    print(\"The file does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Sample Images from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_images(X, y, n, cols):\n",
    "    sample_images = X[:n]\n",
    "    sample_labels = y[:n]\n",
    "\n",
    "    rows = int(np.ceil(n / cols))\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(11.5, 7))\n",
    "\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        image = sample_images[i]\n",
    "        label = sample_labels[i]\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "preview_images(X, y, 15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_numpy = X[:]\n",
    "y_numpy = y[:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_numpy, y_numpy, test_size=1000, random_state=0\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=2000, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Splits to RecordIO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_recordio(X: np.ndarray, y: np.ndarray, prefix: str):\n",
    "    record = mx.recordio.MXIndexedRecordIO(idx_path=f\"{prefix}.idx\", uri=f\"{prefix}.rec\", flag=\"w\")\n",
    "    for idx, arr in enumerate(tqdm(X)):\n",
    "        header = mx.recordio.IRHeader(0, y[idx], idx, 0)\n",
    "        s = mx.recordio.pack_img(\n",
    "            header,\n",
    "            arr,\n",
    "            quality=95,\n",
    "            img_fmt=\".jpg\",\n",
    "        )\n",
    "        record.write_idx(idx, s)\n",
    "    record.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_recordio(X_train, y_train, prefix=\"data/train\")\n",
    "write_to_recordio(X_val, y_val, prefix=\"data/val\")\n",
    "write_to_recordio(X_test, y_test, prefix=\"data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data Splits to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"cv-metastasis\"\n",
    "\n",
    "try:\n",
    "    s3_client.create_bucket(\n",
    "        Bucket=bucket, ACL=\"private\", CreateBucketConfiguration={\"LocationConstraint\": region}\n",
    "    )\n",
    "    print(f\"Created S3 bucket: {bucket}\")\n",
    "\n",
    "except Exception as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"BucketAlreadyOwnedByYou\":\n",
    "        print(f\"Using existing bucket: {bucket}\")\n",
    "    else:\n",
    "        raise (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\"data/train.rec\", bucket, f\"{prefix}/data/train/train.rec\")\n",
    "s3_client.upload_file(\"data/val.rec\", bucket, f\"{prefix}/data/val/val.rec\")\n",
    "s3_client.upload_file(\"data/test.rec\", bucket, f\"{prefix}/data/test/test.rec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve(\"image-classification\", region)\n",
    "num_training_samples = X_train.shape[0]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "hyperparameters = {\n",
    "    \"num_layers\": 18,\n",
    "    \"use_pretrained_model\": 1,\n",
    "    \"augmentation_type\": \"crop_color_transform\",\n",
    "    \"image_shape\": \"3,96,96\",\n",
    "    \"num_classes\": num_classes,\n",
    "    \"num_training_samples\": num_training_samples,\n",
    "    \"mini_batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"precision_dtype\": \"float32\",\n",
    "}\n",
    "\n",
    "estimator_config = {\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"image_uri\": training_image,\n",
    "    \"role\": sagemaker.get_execution_role(),\n",
    "    \"instance_count\": 1,\n",
    "    \"instance_type\": \"ml.p3.2xlarge\",\n",
    "    \"volume_size\": 100,\n",
    "    \"max_run\": 360000,\n",
    "    \"output_path\": f\"s3://{bucket}/{prefix}/training_jobs\",\n",
    "}\n",
    "\n",
    "image_classifier = sagemaker.estimator.Estimator(**estimator_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create RecordIO Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_uri = f\"s3://{bucket}/{prefix}/data\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=\"data/camelyon16_tiles.h5\", desired_s3_uri=base_uri\n",
    ")\n",
    "\n",
    "input_data = ParameterString(name=\"InputData\", default_value=input_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename=\"split_data.py\", Bucket=bucket, Key=f\"{prefix}/code/split_data.py\")\n",
    "split_data_script_uri = f\"s3://{bucket}/{prefix}/code/split_data.py\"\n",
    "split_data_instance_type = \"ml.t3.large\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.0-1\",\n",
    "    instance_type=split_data_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"image-classication-split-data\",\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "\n",
    "split_data_step = ProcessingStep(\n",
    "    name=\"SplitData\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            source=input_data, destination=\"/opt/ml/processing/input\"\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"train_data\", source=\"/opt/ml/processing/output/data/train\"\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"val_data\", source=\"/opt/ml/processing/output/data/val\"\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"test_data\", source=\"/opt/ml/processing/output/data/test\"\n",
    "        ),\n",
    "    ],\n",
    "    code=split_data_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_inputs = {\n",
    "    \"train\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=split_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "            \"train_data\"\n",
    "        ].S3Output.S3Uri,\n",
    "        content_type=\"application/x-recordio\",\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        input_mode=\"Pipe\",\n",
    "    ),\n",
    "    \"validation\": sagemaker.inputs.TrainingInput(\n",
    "        s3_data=split_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "            \"val_data\"\n",
    "        ].S3Output.S3Uri,\n",
    "        content_type=\"application/x-recordio\",\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        input_mode=\"Pipe\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "train_step = TrainingStep(name=\"TrainModel\", estimator=image_classifier, inputs=train_step_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_name = \"cv-metastasis-{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=image_classifier,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"image/jpeg\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=mpg_name,\n",
    "    approval_status=model_approval_status,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.model.Model(\n",
    "    name=f\"{mpg_name}-pipline\",\n",
    "    image_uri=training_image,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(instance_type=\"ml.m4.xlarge\")\n",
    "\n",
    "create_model_step = CreateModelStep(name=\"ModelPreDeployment\", model=model, inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"deploy_model.py\", Bucket=bucket, Key=f\"{prefix}/code/deploy_model.py\"\n",
    ")\n",
    "deploy_model_script_uri = f\"s3://{bucket}/{prefix}/code/deploy_model.py\"\n",
    "deploy_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "deploy_model_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.0-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{prefix}-deploy-model\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "deploy_step = ProcessingStep(\n",
    "    name=\"DeployModel\",\n",
    "    processor=deploy_model_processor,\n",
    "    job_arguments=[\n",
    "        \"--model-name\",\n",
    "        create_model_step.properties.ModelName,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-instance-type\",\n",
    "        deploy_instance_type,\n",
    "        \"--endpoint-name\",\n",
    "        \"cv-model-pipeline\",\n",
    "    ],\n",
    "    code=deploy_model_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"{}-pipeline-{}\".format(prefix, datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[input_data, model_approval_status],\n",
    "    steps=[split_data_step, train_step, register_step, create_model_step, deploy_step],\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"ModelApprovalStatus\": \"Approved\"}\n",
    "\n",
    "start_response = pipeline.start(parameters=parameters)\n",
    "start_response.wait(max_attempts=100)\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineage\n",
    "\n",
    "Review the lineage of the artifacts generated by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker_session)\n",
    "for execution_step in reversed(start_response.list_steps()):\n",
    "    pprint(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_model_package_group(sm_client, package_group_name):\n",
    "    try:\n",
    "        model_versions = sm_client.list_model_packages(ModelPackageGroupName=package_group_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return\n",
    "\n",
    "    for model_version in model_versions[\"ModelPackageSummaryList\"]:\n",
    "        try:\n",
    "            sm_client.delete_model_package(ModelPackageName=model_version[\"ModelPackageArn\"])\n",
    "        except Exception as e:\n",
    "            print(\"{} \\n\".format(e))\n",
    "        time.sleep(0.5)  # Ensure requests aren't throttled\n",
    "\n",
    "    try:\n",
    "        sm_client.delete_model_package_group(ModelPackageGroupName=package_group_name)\n",
    "        print(\"{} model package group deleted\".format(package_group_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "    return\n",
    "\n",
    "\n",
    "def delete_sagemaker_pipeline(sm_client, pipeline_name):\n",
    "    try:\n",
    "        sm_client.delete_pipeline(\n",
    "            PipelineName=pipeline_name,\n",
    "        )\n",
    "        print(\"{} pipeline deleted\".format(pipeline_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = sagemaker.Session().sagemaker_client\n",
    "delete_model_package_group(client, mpg_name)\n",
    "delete_sagemaker_pipeline(client, pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
