Finetuning and deploying Mixtral 8x7B MoE with Hugging Face, using QLoRA
==================================================

.. toctree::
    :maxdepth: 1

    mixtral-8x7b
