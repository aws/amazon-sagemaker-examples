{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Falcon model using SageMaker Distributed Data Parallel Library (SMDDP) and PyTorch Fully Sharded Data Parallelism (FSDP)\n",
    "\n",
    "In this tutorial, we will show how to train or fine-tune [Falcon-7B-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) on the [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset.  We will use 2 p4d.24xlarge instances, which come with 8x NVIDIA A100 40GB GPUs along with the PyTorch Fully Sharded Data Parallelism (FSDP) technique to efficiently train this large model on limited GPU memory.  \n",
    "\n",
    "To acclerate training speed, we will also use the **SageMaker Distributed Data Parallel Library (SMDDP)** which speeds up GPU communication across P4d instances during sharded data parallel training.  \n",
    "\n",
    "Within `scripts/train.py` is the entry point for the training script where we initialize the SMDDP library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll install some dependencies necessary for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers\" \"datasets[s3]\" \"sagemaker\" \"boto3\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r scripts/requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment, you need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker,boto3\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sagemaker_session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sagemaker_session.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "As the base dataset, we will use the [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset, but before training the model, we need to preprocess the data. We will create chunks of `2048` tokens ([model max length](https://huggingface.co/EleutherAI/gpt-neox-20b)) to avoid unnecessary padding and computing. \n",
    "\n",
    "The first step is to load our dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"tiiuae/falcon-7b\"\n",
    "dataset_name = \"tatsu-lab/alpaca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load dataset from huggingface.co\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# downsample dataset to 10k\n",
    "dataset = dataset.shuffle(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset into Train and Valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"validation\" not in dataset.keys():\n",
    "    dataset[\"validation\"] = load_dataset(\n",
    "        dataset_name,\n",
    "        split=\"train[:5%]\"\n",
    "    )\n",
    "\n",
    "    dataset[\"train\"] = load_dataset(\n",
    "        dataset_name,\n",
    "        split=\"train[5%:]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset contains 4 fields instruction, input , output and text. The text field is obtained by combining the remaining 3 fields and we will use the text field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of the data preparation is to tokenize and chunk our dataset. We convert our inputs (text) to token IDs by tokenizing, which the model can understand. Additionally, we concatenate our dataset samples into chunks of `2048` to avoid unnecessary padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "def group_texts(examples,block_size = 2048):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "column_names = dataset[\"train\"].column_names\n",
    "\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"],return_token_type_ids=False), batched=True, remove_columns=list(column_names)\n",
    ").map(\n",
    "    partial(group_texts, block_size=2048),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by saving the tokenized data locally ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data locally\n",
    "\n",
    "training_input_path = f'processed/data/'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"Saved data to: {training_input_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Falcon model using FSDP and SMDDP on Amazon SageMaker\n",
    "\n",
    "We will begin by uploading the tokenized data to S3 which will be uploaded to the training cluster during training.\n",
    "\n",
    "After we process the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sagemaker_session.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_input_path = f's3://{sagemaker_session.default_bucket()}/processed/data/'\n",
    "print(f\"training dataset to: {training_input_path}\")# save train_dataset to s3\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"uploaded data to: {training_input_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the beginning, we will use Amazon SageMaker and PyTorch FSDP to train our model. Amazon SageMaker makes it easy to create a multi-node cluster to train our model in a distributed manner. The `sagemaker` python SDK supports running training jobs using `torchrun`, to distribute the script across multiple nodes and GPUs. \n",
    "\n",
    "To use `torchrun` to execute our scripts, we only have to define the `distribution` parameter in our Estimator and set it to `\"torch_distributed\": {\"enabled\": True}`.\n",
    "\n",
    "In our example, we will use `full shard auto_wrap` and `FalconDecoderLayer` as transformer layer policy. If you run this example and change the model, make sure to also adjust the transformer layer policy in `scripts/train.py`.\n",
    "\n",
    "Within the entry point of the training script, we also import SMDDP and use it as the backend for the process group. This enables faster communication between P4d instances than the open source Nvidia Collective Communications Library (NCCL) and ultimately speeds up training.  \n",
    "\n",
    "To create a sagemaker training job, we create a `PyTorch` Estimator and provide all our information. SageMaker takes care of starting and managing all the required EC2 instances for us, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n",
    "Note that SageMaker by default uses the latest [AWS Deep Learning Container (DLC)](https://aws.amazon.com/machine-learning/containers/), so you can comment out the `ecr_image` variable if you don't want to use your own custom image built from a DLC. Also note that if using FSx when launching the SageMaker notebook instance, you will need to use the same `subnet` and `security_group_config`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "ecr_image=\"<ECR_IMAGE_URI\"\n",
    "job_name = f'huggingface-fsdp-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "subnet_config = [\"<SUBNET_ID>\"]\n",
    "security_group_config = [\"<SECURITY_GROUP_ID>\"]\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_id': model_id, # model id from huggingface.co/models\n",
    "    'dataset_path': '/opt/ml/input/data/train', # path where sagemaker will save training dataset\n",
    "    'valid_path':\"/opt/ml/input/data/valid\",\n",
    "    'gradient_checkpointing': True, # enable gradient checkpointing\n",
    "    'bf16': True, # enable mixed precision training\n",
    "    'optimizer': \"adamw_torch\", # optimizer\n",
    "    'per_device_train_batch_size': 1, # batch size per device during training\n",
    "    'epochs': 1, # number of epochs to train\n",
    "    'fsdp': '\"full_shard auto_wrap\"', # fully sharded data parallelism\n",
    "    'cache_dir': \"/opt/ml/sagemaker/warmpoolcache\", #change this to /tmp if not using warmpools\n",
    "    'max_steps':30\n",
    "}\n",
    "\n",
    "# estimator \n",
    "estimator = PyTorch(\n",
    "  entry_point=\"train.py\",\n",
    "  max_run=1800,\n",
    "  job_name=job_name,\n",
    "  role=role,\n",
    "  framework_version=\"2.0.1\",\n",
    "  py_version=\"py310\",\n",
    "  image_uri=ecr_image,\n",
    "  source_dir=\"./scripts\",\n",
    "  instance_count=2,\n",
    "  instance_type=\"ml.p4d.24xlarge\",\n",
    "  sagemaker_session=sagemaker_session,\n",
    "  subnets=subnet_config,\n",
    "  security_group_ids=security_group_config,\n",
    "  disable_output_compression=True,\n",
    "  distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "  keep_alive_period_in_seconds=600,\n",
    "  hyperparameters=hyperparameters,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'train': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate the warm pool cluster if no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.update_training_job(estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\":0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
