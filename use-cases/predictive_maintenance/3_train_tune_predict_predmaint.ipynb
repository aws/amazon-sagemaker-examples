{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fleet Predictive Maintenance: Part 3. Training, Hyperparameter Tuning, and Prediction\n",
    "\n",
    "*Using SageMaker Studio to Predict Fault Classification*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This notebook is part of a sequence of notebooks whose purpose is to demonstrate a Predictive Maintenance (PrM) solution for automobile fleet maintenance via Amazon SageMaker Studio so that business users have a quick path towards a PrM POC. In this notebook, we will be focusing on training, tuning, and deploying a model. It is the third notebook in a series of notebooks. You can choose to run this notebook by itself or in sequence with the other notebooks listed below. Please see the [README.md](README.md) for more information about this use case implement of this sequence of notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Data Prep: Processing Job from SageMaker Data Wrangler Output](./1_dataprep_dw_job_predmaint.ipynb)\n",
    "1. [Data Prep: Featurization](./2_dataprep_predmaint.ipynb)\n",
    "1. [Train, Tune and Predict using Batch Transform](./3_train_tune_predict_predmaint.ipynb) (current notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes: \n",
    "\n",
    "* Due to cost consideration, the goal of this example is to show you how to use some of SageMaker Studio's features, not necessarily to achieve the best result. \n",
    "* We use the built-in classification algorithm in this example, and a Python 3 (Data Science) Kernel is required.\n",
    "* The nature of predictive maintenace solutions, requires a domain knowledge expert of the system or machinery. With this in mind, we will make assumptions here for certain elements of this solution with the acknowldgement that these assumptions should be informed by a domain expert and a main business stakeholder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "Let's start by:\n",
    "\n",
    "* Installing and importing any dependencies\n",
    "* Instantiating SageMaker session\n",
    "* Specifying the S3 bucket and prefix that you want to use for your training and model data. This should be within the same region as SageMaker training\n",
    "* Defining the IAM role used to give training access to your data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing dependencies\n",
    "!pip install -qU 'sagemaker-experiments==0.1.24' 'sagemaker>=2.16.1' 'boto3' 'awswrangler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import collections\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SageMaker dependencies\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "import awswrangler as wr\n",
    "\n",
    "# This instantiates a SageMaker session that we will be operating in.\n",
    "smclient = boto3.Session().client(\"sagemaker\")\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# This object represents the IAM role that we are assigned.\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "# prefix is the path within the bucket where SageMaker stores the output from training jobs.\n",
    "prefix_prm = \"predmaint\"  # place to upload training files within the bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we must first upload our data in S3. To see how the existing train, test, and validation datasets were generated, take a look at [Data Prep: Processing Job from SageMaker Data Wrangler Output](./1_dataprep_dw_job_predmaint.ipynb) (which is the first part of this notebook series) followed by [Data Prep: Featurization](./2_dataprep_predmaint.ipynb) (which is the second part of this notebook series). See the [Background](#Background) section at the beginning of the notebook for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for converting data to csv(necessary for Linear Learner) and upload to S3\n",
    "def upload_file_to_bucket(bucket, prefix, file_path):\n",
    "    file_dir, file_name = os.path.split(file_path)\n",
    "    df = pd.read_csv(file_path)\n",
    "    boto3.resource(\"s3\").meta.client.upload_file(\n",
    "        Filename=file_path, Bucket=bucket, Key=(prefix + \"/\" + file_name)\n",
    "    )\n",
    "    print(f\"uploaded {prefix} data location: s3://{bucket}/{prefix}/{file_name}\")\n",
    "    path_to_data = f\"s3://{bucket}/{prefix}/{file_name}\"\n",
    "    return path_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert and upload to S3\n",
    "path_to_train_data_prm = upload_file_to_bucket(bucket, \"train\", \"train.csv\")\n",
    "path_to_test_data_prm = upload_file_to_bucket(bucket, \"test\", \"test.csv\")\n",
    "path_to_test_x_data_prm = upload_file_to_bucket(bucket, \"test\", \"test_x.csv\")\n",
    "path_to_valid_data_prm = upload_file_to_bucket(bucket, \"validation\", \"validation.csv\")\n",
    "\n",
    "# let's also setup an output S3 location for the model artifact that will be output as the result of training with the algorithm.\n",
    "output_location = f\"s3://{bucket}/output\"\n",
    "print(\"training artifacts will be uploaded to: {}\".format(output_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_channel = TrainingInput(path_to_train_data_prm, content_type=\"text/csv\")\n",
    "test_channel = TrainingInput(path_to_test_data_prm, content_type=\"text/csv\")\n",
    "test_x_channel = TrainingInput(path_to_test_x_data_prm, content_type=\"text/csv\")\n",
    "valid_channel = TrainingInput(path_to_valid_data_prm, content_type=\"text/csv\")\n",
    "\n",
    "data_channels = {\"train\": train_channel, \"validation\": valid_channel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in S3 and is ready for use in the estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "### SageMaker Estimator and Experiments\n",
    "\n",
    "Once you have selected some models that you would like to try out, SageMaker Experiments can be a great tool to track and compare all of the models before selecting the best model to deploy. We will set up an experiment using SageMaker experiments to track all the model training iterations for the Linear Learner Estimator we will try. You can read more about [SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) to learn about experiment features, tracking and comparing outputs.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "# import dependencies\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"create_date\" not in locals():\n",
    "    create_date = strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    %store create_date\n",
    "\n",
    "    # location within S3 for outputs\n",
    "    exp_prefix = f\"sagemaker-experiments/linear-learner-{create_date}\"\n",
    "    %store exp_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used the storemagic previously, you can pick up from here using the `create_date` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the experiment\n",
    "experiment_name = f\"ll-failure-classification-{create_date}\"\n",
    "\n",
    "try:\n",
    "    my_experiment = Experiment.load(experiment_name=experiment_name)\n",
    "    print(f\"Experiment loaded {experiment_name}: SUCCESS\")\n",
    "except Exception as e:\n",
    "    if \"ResourceNotFound\" in str(e):\n",
    "        my_experiment = Experiment.create(\n",
    "            experiment_name=experiment_name,\n",
    "            description=\"Classification PrM Experiment\",\n",
    "            tags=[{\"Key\": \"my-experiments\", \"Value\": \"exp\"}],\n",
    "            sagemaker_boto_client=smclient,\n",
    "        )\n",
    "        print(f\"Experiment creation {experiment_name}: SUCCESS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The tags parameter is optional. You can search for the tag using Studio, the SageMaker console, and the SDK. Tags can also be applied to trials and trial components. For information on how to search tags using Studio, see [Search by Tag](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments-search-studio.html#experiments-search-studio-tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracker.create(display_name=\"training\", sagemaker_boto_client=smclient) as tracker:\n",
    "    tracker.log_input(name=\"prm-dataset\", media_type=\"s3/uri\", value=path_to_train_data_prm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin to specify our linear model from the Amazon SageMaker Linear Learner Estimator. For this binary classification problem, we have the option of selecting between logistic regression or hinge loss (Support Vector Machines). Here are additional resources to learn more about the [Input/Output Interface for the Linear Learner Algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output) and the [Linear Learner Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html). One piece to note is that Amazon SageMaker's Linear Learner actually fits many models in parallel, each with slightly different hyperparameters, and then returns the one with the best fit.  This functionality is automatically enabled.  There are a number of additional parameters available for the Linear Learner Estimator, so we will start be using the default features as well as:\n",
    "\n",
    "- `loss` which controls how we penalize mistakes in our model estimates.  For this case, we will start with logistic and move to using hinge loss if necessary for model improvement.\n",
    "- `predictor_type` is set to 'binary_classifier' since we are trying to predict whether a failure occurs or it doesn't.\n",
    "- `mini_batch_size` is set to 99.  This value can be tuned for relatively minor improvements in fit and speed, but selecting a reasonable value relative to the dataset is appropriate in most cases.\n",
    "- `wd` or `l1` which control regularization.  Regularization can prevent model overfitting by preventing our estimates from becoming too finely tuned to the training data, which can actually hurt generalizability.  In this case, we'll leave these parameters as their default \"auto\" though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by first building a logistic regression Linear Learner Estimator, setting the hyperparameters and configuring the SageMaker Experiment with the trial created above. We will then evaluate the results of the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set output path\n",
    "lr_output_path = f\"s3://{bucket}/{exp_prefix}/output/lr_default\"\n",
    "\n",
    "# with SageMaker v2.0, Image URI function get_image_uri has been replaced with sagemaker.image_uris.retrieve()\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    framework=\"linear-learner\", region=region, version=\"1\", image_scope=\"training\"\n",
    ")\n",
    "\n",
    "# create the trail component and the first experiment for linear learner\n",
    "training_trail_component = tracker.trial_component\n",
    "trial_name_1 = trial_name = f\"linear-learner-lr-training-job-{create_date}\"\n",
    "\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    my_trial = Trial.load(trial_name=trial_name_1)\n",
    "    print(f\"Loaded existing trial: {trial_name_1}\")\n",
    "except Exception as e:\n",
    "    if \"ResourceNotFound\" in str(e):\n",
    "        my_trial = Trial.create(experiment_name=experiment_name, trial_name=trial_name_1)\n",
    "        print(f\"Create trial {my_trial.trial_name}: SUCCESSFUL \\n\")\n",
    "\n",
    "        print(f\"Creating logistic regression estimator. \\n\")\n",
    "        lr = sagemaker.estimator.Estimator(\n",
    "            container,\n",
    "            role,\n",
    "            instance_count=1,\n",
    "            instance_type=\"ml.c4.xlarge\",\n",
    "            output_path=lr_output_path,\n",
    "            sagemaker_session=sess,\n",
    "            enable_sagemaker_metrics=True,\n",
    "        )\n",
    "\n",
    "        lr.set_hyperparameters(\n",
    "            predictor_type=\"binary_classifier\",\n",
    "            loss=\"logistic\",  # default for auto is logistic regression\n",
    "            epochs=20,  # high number of epochs as early stopping feature will stop training\n",
    "            mini_batch_size=99,\n",
    "        )\n",
    "\n",
    "        lr.fit(\n",
    "            inputs=data_channels,\n",
    "            experiment_config={\n",
    "                \"ExperimentName\": my_experiment.experiment_name,\n",
    "                \"TrialName\": my_trial.trial_name,\n",
    "                \"TrialComponentDisplayName\": \"ll-lr-training-job\",\n",
    "            },\n",
    "            logs=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train a Linear Learner model with hinge loss, i.e. Support Vector Machines, and use the default hyperparmeters listed below. We will add this trial to the experiment for later comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_output_path = f\"s3://{bucket}/{exp_prefix}/output/svm_default\"\n",
    "trial_name_2 = f\"linear-learner-svm-{create_date}\"\n",
    "\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    my_trial = Trial.load(trial_name=trial_name_2)\n",
    "    print(f\"Loaded existing trial: {trial_name_2}\")\n",
    "except Exception as e:\n",
    "    if \"ResourceNotFound\" in str(e):\n",
    "        my_trial = Trial.create(experiment_name=experiment_name, trial_name=trial_name_2)\n",
    "        print(f\"Create trial {my_trial.trial_name}: SUCCESSFUL\")\n",
    "\n",
    "        svm = sagemaker.estimator.Estimator(\n",
    "            container,\n",
    "            role,\n",
    "            instance_count=1,\n",
    "            instance_type=\"ml.c4.xlarge\",\n",
    "            output_path=svm_output_path,\n",
    "            sagemaker_session=sess,\n",
    "            enable_sagemaker_metrics=True,\n",
    "        )\n",
    "        svm.set_hyperparameters(\n",
    "            predictor_type=\"binary_classifier\", loss=\"hinge_loss\", epochs=20, mini_batch_size=99\n",
    "        )\n",
    "\n",
    "        svm.fit(\n",
    "            inputs=data_channels,\n",
    "            experiment_config={\n",
    "                \"ExperimentName\": my_experiment.experiment_name,\n",
    "                \"TrialName\": my_trial.trial_name,\n",
    "                \"TrialComponentDisplayName\": \"ll-svm-training-job\",\n",
    "            },\n",
    "            logs=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add in a selection for the hyperparameter `binary_classifier_model_selection_criteria` which allows us to define which metric we would like our model to optimize for (auto threshold tuning). We can choose between percision, recall, F1 and accuracy among other metric choices. \n",
    "\n",
    "It's important to note here that for PrM, we must carefully select our evaluation metric based on domain knowlege. For example, recall increases the chance of catching all failures even false ones. Whereas, precision decreases the chance of catching false failures along with real failures. It is not hard to see that these metrics do not always incorporate the business needs, especially the dollar costs and benefits associated with machinery failures. \n",
    "\n",
    "With this in mind, we will select F1 as the evaluation metric as it is generally a good evaluation metric for imbalanced classification proglems. If you would like to learn more about selecting a custom cost sensitive evaluation metric for your business use case, you can review the article listen in the *Additional Resources* section below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_output_path = f\"s3://{bucket}/{exp_prefix}/output/svm_threshold\"\n",
    "trial_name_3 = f\"linear-learner-svm-thresh-{create_date}\"\n",
    "\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    my_trial = Trial.load(trial_name=trial_name_3)\n",
    "    print(f\"Loaded existing trial: {trial_name_3}\")\n",
    "except Exception as e:\n",
    "    if \"ResourceNotFound\" in str(e):\n",
    "        my_trial = Trial.create(experiment_name=experiment_name, trial_name=trial_name_3)\n",
    "        print(f\"Create trial {my_trial.trial_name}: SUCCESSFUL\")\n",
    "\n",
    "        svm_thresh = sagemaker.estimator.Estimator(\n",
    "            container,\n",
    "            role,\n",
    "            instance_count=1,\n",
    "            instance_type=\"ml.c4.xlarge\",\n",
    "            output_path=svm_output_path,\n",
    "            sagemaker_session=sess,\n",
    "        )\n",
    "\n",
    "        svm_thresh.set_hyperparameters(\n",
    "            predictor_type=\"binary_classifier\",\n",
    "            loss=\"hinge_loss\",\n",
    "            binary_classifier_model_selection_criteria=\"f_beta\",\n",
    "            epochs=20,\n",
    "            mini_batch_size=99,\n",
    "        )\n",
    "\n",
    "        # linear.fit({'train': path_to_train_data_prm})\n",
    "        svm_thresh.fit(\n",
    "            inputs=data_channels,\n",
    "            experiment_config={\n",
    "                \"ExperimentName\": my_experiment.experiment_name,\n",
    "                \"TrialName\": my_trial.trial_name,\n",
    "                \"TrialComponentDisplayName\": \"ll-svm-thresh-training-job\",\n",
    "            },\n",
    "            logs=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try dealing with class imbalances to try to improve precision and recall\n",
    "\n",
    "We will set the hyperparameter `positive_example_weight_mult` to *balanced* in order to use weighting by class to address the class imbalance issue. Since we have only 19% failures compared to non-failures, we can leverage this built-in hyperparameter to try to improve model performance. Here is more documentation about [Linear Learner Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training a binary classifier with hinge loss and balanced class weights\n",
    "\n",
    "# set output path\n",
    "svm_output_path = f\"s3://{bucket}/{exp_prefix}/output/svm_balanced\"\n",
    "trial_name_4 = f\"linear-learner-svm-balanced-{create_date}\"\n",
    "\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    my_trial = Trial.load(trial_name=trial_name_4)\n",
    "    print(f\"Loaded existing trial: {trial_name_4}\")\n",
    "except Exception as e:\n",
    "    if \"ResourceNotFound\" in str(e):\n",
    "        my_trial = Trial.create(experiment_name=experiment_name, trial_name=trial_name_4)\n",
    "        print(f\"Create trial {my_trial.trial_name}: SUCCESSFUL\")\n",
    "\n",
    "        # specify algorithm containers and instantiate an Estimator with hyperparams\n",
    "        svm_balanced = sagemaker.estimator.Estimator(\n",
    "            container,\n",
    "            role,\n",
    "            instance_count=1,\n",
    "            instance_type=\"ml.c4.xlarge\",\n",
    "            output_path=svm_output_path,\n",
    "            sagemaker_session=sess,\n",
    "            enable_sagemaker_metrics=True,\n",
    "        )\n",
    "\n",
    "        svm_balanced.set_hyperparameters(\n",
    "            predictor_type=\"binary_classifier\",\n",
    "            loss=\"hinge_loss\",\n",
    "            positive_example_weight_mult=\"balanced\",  # this is for dealing with class imbalances\n",
    "            epochs=20,\n",
    "            mini_batch_size=99,\n",
    "        )\n",
    "        # fit model to data\n",
    "        svm_balanced.fit(\n",
    "            inputs=data_channels,\n",
    "            experiment_config={\n",
    "                \"ExperimentName\": my_experiment.experiment_name,\n",
    "                \"TrialName\": my_trial.trial_name,\n",
    "                \"TrialComponentDisplayName\": \"ll-svm-bal-training-job\",\n",
    "            },\n",
    "            logs=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we can look at all the trials together to evaluate the performance\n",
    "trial_component_analytics = ExperimentAnalytics(experiment_name=my_experiment.experiment_name)\n",
    "analytic_table = trial_component_analytics.dataframe()\n",
    "analytic_table = analytic_table[\n",
    "    [\n",
    "        \"TrialComponentName\",\n",
    "        \"DisplayName\",\n",
    "        \"positive_example_weight_mult\",\n",
    "        \"validation:recall - Avg\",\n",
    "        \"validation:binary_classification_accuracy - Avg\",\n",
    "        \"validation:roc_auc_score - Avg\",\n",
    "        \"train:objective_loss - Avg\",\n",
    "        \"validation:objective_loss:final - Avg\",\n",
    "        \"validation:objective_loss - Avg\",\n",
    "        \"validation:binary_f_beta - Avg\",\n",
    "        \"validation:precision - Avg\",\n",
    "        \"Trials\",\n",
    "        \"Experiments\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "analytic_table.sort_values(\n",
    "    [\"validation:binary_classification_accuracy - Avg\", \"validation:binary_f_beta - Avg\"],\n",
    "    ascending=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* Balancing class weights improved precision, but decreased recall significantly \n",
    "* Accuracy for all the models is relatively consistent at around 84%\n",
    "* Across all the metrics, SVM with auto hyperparameters performed the best cumulatively\n",
    "\n",
    "We will move forward with with the auto SMV model, labeled as `svm-default`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # option to deploy a predictor here and make predictions against this endpoint\n",
    "\n",
    "# # initialize the deserializer and serializer\n",
    "# serializer = sagemaker.serializers.CSVSerializer()\n",
    "# deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "# svm_predictor = svm.deploy(initial_instance_count=1,\n",
    "#                            instance_type='local'\n",
    "#                            instance_type='ml.m4.xlarge',\n",
    "#                            serializer=serializer,\n",
    "#                            deserializer=deserializer,\n",
    "#                            endpoint_name='svm',\n",
    "#                            model_name='svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter Tuning \n",
    "\n",
    "### Next, we set up the hyperparmeter tuning job using SageMaker Automatic Tuning\n",
    "\n",
    "*Note, with the settings below, the hyperparameter tuning job can take about 30 minutes to complete.*\n",
    "\n",
    "Hyperparameters can dramtically affect the performance of trained models. Thus, we need to pick the right values to achieve the best model result. Since model results are also affected by the data set as well, it is important to select the best hyperparmateters by searching for them using an algorithmic approach that can be automated and perform efficiently.\n",
    "\n",
    "Using Automatic Tuning, we will specify a range, or a list of possible values in the case of categorical hyperparameters, for each of the hyperparameter that we plan to tune. SageMaker hyperparameter tuning will automatically launch multiple training jobs with different hyperparameter settings, evaluate results of those training jobs based on a predefined \"objective metric\", and select the hyperparameter settings for future attempts based on previous results. For each hyperparameter tuning job, we will give it a budget (max number of training jobs) and it will complete once that many training jobs have been executed.\n",
    "\n",
    "In this example, we are using SageMaker Python SDK to set up and manage the hyperparameter tuning job. We first configure the training jobs the hyperparameter tuning job will launch by initiating an estimator, which includes the following configuration:\n",
    "\n",
    "* hyperparameters that SageMaker Automatic Model Tuning will tune: `learning_rate` \n",
    "* the maximum number of training jobs it will run to optimize the objective metric: 5\n",
    "* the number of parallel training jobs that will run in the tuning job: 2\n",
    "* the objective metric that Automatic Model Tuning will use: validation:accuracy\n",
    "\n",
    "We will also demonstrates how to associate trial components created by a hyperparameter tuning job with an experiment management trial.\n",
    "\n",
    "Read the following link more information on how to [Tune a Linear Learner Model](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html) and about [How Hyperparameter Tuning Works](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom job name\n",
    "prm_tuning_job_name = f\"ll-svm-tuning-job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set output path\n",
    "svm_output_path = f\"s3://{bucket}/{exp_prefix}/output/tuning/svm_default\"\n",
    "\n",
    "# create the tuning job if it doesn't exist\n",
    "try:\n",
    "    svm_tune = sagemaker.estimator.Estimator(\n",
    "        container,\n",
    "        role,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.c4.xlarge\",\n",
    "        output_path=svm_output_path,\n",
    "        sagemaker_session=sess,\n",
    "    )\n",
    "\n",
    "    svm_tune.set_hyperparameters(\n",
    "        predictor_type=\"binary_classifier\", loss=\"hinge_loss\", epochs=20, mini_batch_size=99\n",
    "    )\n",
    "\n",
    "    hyperparameter_ranges = {\n",
    "        \"learning_rate\": ContinuousParameter(0.01, 0.5, scaling_type=\"Logarithmic\")\n",
    "    }\n",
    "\n",
    "    # configure HyperparameterTuner\n",
    "    my_tuner = HyperparameterTuner(\n",
    "        estimator=svm_tune,  # previously-configured Estimator object\n",
    "        objective_metric_name=\"validation:binary_classification_accuracy\",\n",
    "        hyperparameter_ranges=hyperparameter_ranges,\n",
    "        max_jobs=5,\n",
    "        max_parallel_jobs=2,\n",
    "        strategy=\"Random\",\n",
    "        base_tuning_job_name=prm_tuning_job_name,\n",
    "    )\n",
    "\n",
    "    # start hyperparameter tuning job\n",
    "    my_tuner.fit(inputs=data_channels, include_cls_metadata=False)\n",
    "    print(f\"Create tuning job {prm_tuning_job_name}: SUCCESSFUL\")\n",
    "except ClientError as e:\n",
    "    if \"ResourceInUse\" in str(e):\n",
    "        my_tuner = HyperparameterTuner.attach(prm_tuning_job_name)\n",
    "        print(f\"Attach tuning job {prm_tuning_job_name}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check status\n",
    "boto3.client(\"sagemaker\").describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=my_tuner.latest_tuning_job.job_name\n",
    ")[\"HyperParameterTuningJobStatus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_analytics = sagemaker.HyperparameterTuningJobAnalytics(\n",
    "    my_tuner.latest_tuning_job.job_name\n",
    ").dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most recently created tuning jobs\n",
    "list_tuning_jobs_response = smclient.list_hyper_parameter_tuning_jobs(\n",
    "    SortBy=\"CreationTime\", SortOrder=\"Descending\"\n",
    ")\n",
    "\n",
    "# inspect output\n",
    "print(f'Found {len(list_tuning_jobs_response[\"HyperParameterTuningJobSummaries\"])} tuning jobs.')\n",
    "tuning_jobs = list_tuning_jobs_response[\"HyperParameterTuningJobSummaries\"]\n",
    "most_recently_created_tuning_job = tuning_jobs[0]\n",
    "tuning_job_name = most_recently_created_tuning_job[\"HyperParameterTuningJobName\"]\n",
    "experiment_name = my_experiment.experiment_name\n",
    "tune_trial_name = tuning_job_name + \"-trial\"\n",
    "%store tune_trial_name\n",
    "\n",
    "print(f\"Associate all training jobs created by {tuning_job_name} with trial {tune_trial_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    tune_trial = Trial.load(trial_name=tune_trial_name)\n",
    "    print(f\"Loaded existing trial: {tune_trial_name}\")\n",
    "except Exception as e:\n",
    "    if \"ResourceNotFound\" in str(e):\n",
    "        tune_trial = Trial.create(experiment_name=experiment_name, trial_name=tune_trial_name)\n",
    "        print(f\"Create trial {tune_trial.trial_name}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import HyperparameterTuningJobAnalytics, Session\n",
    "from smexperiments.search_expression import Filter, Operator, SearchExpression\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "\n",
    "# get the training jobs associated with the tuning job\n",
    "tuning_analytics = HyperparameterTuningJobAnalytics(tuning_job_name, sess)\n",
    "\n",
    "training_job_summaries = tuning_analytics.training_job_summaries()\n",
    "training_job_arns = list(map(lambda x: x[\"TrainingJobArn\"], training_job_summaries))\n",
    "print(\n",
    "    f\"Found {len(training_job_arns)} training jobs for hyperparameter tuning job {tuning_job_name}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# get the trial components derived from the training jobs\n",
    "creation_time = most_recently_created_tuning_job[\"CreationTime\"]\n",
    "creation_time = creation_time.astimezone(timezone.utc)\n",
    "creation_time = creation_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "created_after_filter = Filter(\n",
    "    name=\"CreationTime\",\n",
    "    operator=Operator.GREATER_THAN_OR_EQUAL,\n",
    "    value=str(creation_time),\n",
    ")\n",
    "\n",
    "# the training job names contain the tuning job name (and the training job name is in the source arn)\n",
    "source_arn_filter = Filter(\n",
    "    name=\"Source.SourceArn\", operator=Operator.CONTAINS, value=tuning_job_name\n",
    ")\n",
    "source_type_filter = Filter(\n",
    "    name=\"Source.SourceType\", operator=Operator.EQUALS, value=\"SageMakerTrainingJob\"\n",
    ")\n",
    "\n",
    "search_expression = SearchExpression(\n",
    "    filters=[created_after_filter, source_arn_filter, source_type_filter]\n",
    ")\n",
    "\n",
    "# search iterates over every page of results by default\n",
    "trial_component_search_results = list(\n",
    "    TrialComponent.search(search_expression=search_expression, sagemaker_boto_client=smclient)\n",
    ")\n",
    "print(f\"Found {len(trial_component_search_results)} trial components.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate the trial components with the trial\n",
    "for tc in trial_component_search_results:\n",
    "    print(\n",
    "        f\"Associating trial component {tc.trial_component_name} with trial {tune_trial.trial_name}.\"\n",
    "    )\n",
    "    tune_trial.add_trial_component(tc.trial_component_name)\n",
    "    # sleep to avoid throttling\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the output of all of the hyperparameter tuning trial runs\n",
    "tuning_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Predict\n",
    "\n",
    "### Deploy Model with Batch Transform and Get Inferences for the Test Dataset\n",
    "\n",
    "Let's predict on our test dataset to understand how accurate our model is. We will create batch inferences and use a helper function to evaludate the results. \n",
    "\n",
    "Use batch transform when you:\n",
    "- Want to get inferences for an entire dataset and index them to serve inferences in real time\n",
    "- Don't need a persistent endpoint that applications (for example, web or mobile apps) can call to get inferences\n",
    "- Don't need the subsecond latency that SageMaker hosted endpoints provide\n",
    "\n",
    "Here is additional information about how to [Use Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a `sagemaker.transformer.Transformer` object from the model that was trained in the sections above. Then it calls that object's transform method to create a transform job. When you create the `sagemaker.transformer.Transformer` object, you specify the number and type of ML instances to use to perform the batch transform job, and the location in Amazon S3 where you want to store the inferences.For more information, see [SageMaker Transformer](https://sagemaker.readthedocs.io/en/stable/transformer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best model, deploy and get batch predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get best model\n",
    "best_model = my_tuner.best_estimator()\n",
    "\n",
    "# the location of the test dataset\n",
    "batch_input = path_to_test_x_data_prm\n",
    "\n",
    "# the location to store the results of the batch transform job\n",
    "batch_output = f\"s3://{bucket}/transform/batch-inference\"\n",
    "\n",
    "# create transformer for best model\n",
    "transformer = best_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=batch_output,\n",
    "    accept=\"application/json\",\n",
    ")\n",
    "\n",
    "# get batch inferences\n",
    "transformer.transform(data=batch_input, content_type=\"text/csv\", split_type=\"Line\")\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that our Batch Transform job is complete, we can download the predictions and evaluate the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $transformer.output_path ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the endpoint returned JSON which contains `predictions`, including the `score` and `predicted_label`.  In this case, `score` will be a continuous value between [0, 1] representing the probability there will be a failure in the window. `predicted_label` will take a value of either `0` or `1` where `1` denotes that we predict a failure within the next designated time window, while `0` denotes that we are predicting that there is not a failure within the next time window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(batch_file_path, test_labels, model_name, metrics=True):\n",
    "    \"\"\"\n",
    "    Download batch predictions and iterate through results. Evaluate model results by comparing to actuals from test data.\n",
    "    Return binary classification metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # open batch prediction file and parse json to exctract predicted label\n",
    "    with open(batch_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            result = json.loads(line)\n",
    "            predictions = pd.Series(\n",
    "                [\n",
    "                    result[\"predictions\"][i][\"predicted_label\"]\n",
    "                    for i in range(len(result[\"predictions\"]))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    # calculate true positives, false positives, true negatives, false negatives\n",
    "    tp = np.logical_and(test_labels, predictions).sum()\n",
    "    fp = np.logical_and(1 - test_labels, predictions).sum()\n",
    "    tn = np.logical_and(1 - test_labels, 1 - predictions).sum()\n",
    "    fn = np.logical_and(test_labels, 1 - predictions).sum()\n",
    "\n",
    "    # calculate binary classification metrics\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    if metrics:\n",
    "        print(pd.crosstab(test_labels, predictions, rownames=[\"actuals\"], colnames=[\"predictions\"]))\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"F1: {f1}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "\n",
    "    return {\n",
    "        \"TP\": tp,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn,\n",
    "        \"TN\": tn,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1\": f1,\n",
    "        \"Model\": model_name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call evaluation function and inspect results\n",
    "test = pd.read_csv(\"test.csv\", header=None)\n",
    "test_y = test[0]\n",
    "evaluate_model(\"test_x.csv.out\", test_y, \"PrM-Classification-SVM\", metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up  (Optional)\n",
    "\n",
    "Once we're done don't forget to clean up the endpoint and experiments to prevent unnecessary billing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint(predictor):\n",
    "    try:\n",
    "        predictor.delete_model()\n",
    "        predictor.delete_endpoint()\n",
    "        print(\"Deleted {}\".format(predictor.endpoint))\n",
    "    except:\n",
    "        print(\"Already deleted: {}\".format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can run this cell if you deployed your estimator instead of doing Batch Transform\n",
    "# delete_endpoint(svm_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) In order to delete the experiments created, you can `delete_all` or use the `cleanup_boto3` helper function to delete individual experiments by passing the experiment name to the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to delete the experiment and all its related trials and trial components\n",
    "# my_experiment.delete_all(action='--force')\n",
    "sm = boto3.Session().client(\"sagemaker\")\n",
    "\n",
    "\n",
    "def cleanup_boto3(experiment_name):\n",
    "    trials = sm.list_trials(ExperimentName=experiment_name)[\"TrialSummaries\"]\n",
    "    print(\"TrialNames:\")\n",
    "    for trial in trials:\n",
    "        trial_name = trial[\"TrialName\"]\n",
    "        print(f\"\\n{trial_name}\")\n",
    "\n",
    "        components_in_trial = sm.list_trial_components(TrialName=trial_name)\n",
    "        print(\"\\tTrialComponentNames:\")\n",
    "        for component in components_in_trial[\"TrialComponentSummaries\"]:\n",
    "            component_name = component[\"TrialComponentName\"]\n",
    "            print(f\"\\t{component_name}\")\n",
    "            sm.disassociate_trial_component(TrialComponentName=component_name, TrialName=trial_name)\n",
    "            try:\n",
    "                # comment out to keep trial components\n",
    "                sm.delete_trial_component(TrialComponentName=component_name)\n",
    "            except:\n",
    "                # component is associated with another trial\n",
    "                continue\n",
    "            # to prevent throttling\n",
    "            time.sleep(0.5)\n",
    "        sm.delete_trial(TrialName=trial_name)\n",
    "    sm.delete_experiment(ExperimentName=experiment_name)\n",
    "    print(f\"\\nExperiment {experiment_name} deleted\")\n",
    "\n",
    "\n",
    "# Call cleanup_boto3\n",
    "\n",
    "# Use experiment name not display name\n",
    "experiment_name = my_experiment.experiment_name\n",
    "# cleanup_boto3(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "---\n",
    "## Extensions\n",
    "\n",
    "* Our linear model does a relatively good job of predicting if a failure will occur in the next window of time and has an overall accuracy of close to 84%, but we can re-run the model with different values of the hyperparameters, loss functions etc and see if we get improved prediction\n",
    "    * Re-running the model with further tweaks to these hyperparameters may provide more accurate out-of-sample predictions\n",
    "* We also did not do much feature engineering\n",
    "    * We can create additional features by considering cross-product/intreaction of multiple features, squaring or raising higher powers of the features to induce non-linear effects, etc. \n",
    "    * If we expand the features using non-linear terms and interactions, we can then tweak the regulaization parameter to optimize the expanded model and hence generate improved forecasts\n",
    "* As a further extension, we can experiment with many tree-based models such as XGBoost, Random Forest, or gradient boosting to address the class imbalance as these models are less sensitive to class imbalances \n",
    "* Once the model performance has met the business's desired output, it can be deployed at the edge using [AWS IoT Greengrass](https://aws.amazon.com/greengrass/)\n",
    "    * Here is an additional example of [Predictive Maintenance Deployed at the Edge](https://github.com/aws-samples/amazon-sagemaker-predictive-maintenance-deployed-at-edge) using SageMaker and Greengrass for guidance \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additonal Resources\n",
    "\n",
    "- [A Survey of Predictive Maintenance: Systems, Purposes and Approaches](https://arxiv.org/pdf/1912.07383.pdf)\n",
    "-[Cost-Sensitive Learning for Predictive Maintenance](https://arxiv.org/pdf/1809.10979.pdf)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
