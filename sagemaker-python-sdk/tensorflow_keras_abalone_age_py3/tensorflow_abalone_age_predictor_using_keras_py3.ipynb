{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Estimators with Keras and TensorFlow backend\n",
    "\n",
    "This tutorial covers how to create your own training script using the building\n",
    "blocks provided in `keras`, which will predict the ages of\n",
    "[abalones](https://en.wikipedia.org/wiki/Abalone) based on their physical\n",
    "measurements. You'll learn how to do the following:\n",
    "\n",
    "*   Construct a custom model function\n",
    "*   Configure a neural network using `keras`\n",
    "*   Define a training op for your model\n",
    "*   Define your model metric\n",
    "*   Generate and return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Abalone Age Predictor\n",
    "\n",
    "It's possible to estimate the age of an\n",
    "[abalone](https://en.wikipedia.org/wiki/Abalone) (sea snail) by the number of\n",
    "rings on its shell. However, because this task requires cutting, staining, and\n",
    "viewing the shell under a microscope, it's desirable to find other measurements\n",
    "that can predict age.\n",
    "\n",
    "The [Abalone Data Set](https://archive.ics.uci.edu/ml/datasets/Abalone) contains\n",
    "the following\n",
    "[feature data](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names)\n",
    "for abalone:\n",
    "\n",
    "| Feature        | Description                                               |\n",
    "| -------------- | --------------------------------------------------------- |\n",
    "| Length         | Length of abalone (in longest direction; in mm)           |\n",
    "| Diameter       | Diameter of abalone (measurement perpendicular to length; in mm)|\n",
    "| Height         | Height of abalone (with its meat inside shell; in mm)     |\n",
    "| Whole Weight   | Weight of entire abalone (in grams)                       |\n",
    "| Shucked Weight | Weight of abalone meat only (in grams)                    |\n",
    "| Viscera Weight | Gut weight of abalone (in grams), after bleeding          |\n",
    "| Shell Weight   | Weight of dried abalone shell (in grams)                  |\n",
    "\n",
    "The label to predict is number of rings, as a proxy for abalone age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environmentÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/abalone_train.csv', names=['Length','Diameter', 'Height', 'WholeWeight', 'ShuckedWeight', 'VisceraWeight','ShellWeight', 'age'])\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data to a S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_prefix = sagemaker_session.upload_data(path='data', key_prefix='abalone_dataset')\n",
    "print(s3_input_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $s3_input_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sagemaker_session.upload_data** will upload the abalone dataset from your machine to a bucket named **sagemaker-{your aws account number}**, if you don't have this bucket yet, sagemaker_session will create it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete source code\n",
    "Here is the full code for the network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ./source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*   **`setup.py & requirements.txt`** If you use setup.py and specify the dependencies in a requirements.txt, Sagemaker will pip install them for you when it launches the training job\n",
    "\n",
    "\n",
    "*  **`model_exporter_keras_to_pb.py`** This exports keras model into TensorFlow protobuf format.\n",
    "\n",
    "\n",
    "*  **`main_train.py`** This is the entry point file to start training.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat 'source/main_train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*   **`Environment variable: SM_MODEL_DIR `**  This is where the model needs to be saved to in tensorflow protobof format. This is required for the tensorflow serving container.\n",
    "`\n",
    "\n",
    "*   **`Model Saving`** The model must be saved in TensorFlow protobuf format for the default serving container to work. The default setting uses SageMaker TensorFlow serving container, which is capable of serving more than one model. Hence the container expects the saved_model.pb to be within a directory structure model_name/model_version.\n",
    "\n",
    "\n",
    "* **`Model Metric`** Model metric is printed in the console, so a regex can be used to extract the metrics. E.g the regex **`## validation_metric_mse ##: (\\d*[.]?\\d*)`** matches the following print\n",
    "    ```python\n",
    "    print(\"## validation_metric_{} ##: {}\".format(\"mse\", scores[1+i]))\n",
    "    ```\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting script for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Git config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_id = \"9358cfe8ed267a1d49cfcc9f6447917c5759c933\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_config = {'repo': 'https://github.com/elangovana/amazon-sagemaker-examples.git',\n",
    "              'branch': 'master',\n",
    "              'commit': commit_id\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source directory\n",
    " \n",
    "Path relative to the root source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'sagemaker-python-sdk/tensorflow_keras_abalone_age_py3/source'\n",
    "entry_point_file = 'main_train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric definitions\n",
    "Plots these on sagemaker console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_def = [{\"Name\": \"mean_squared_error\",\n",
    " \"Regex\": \"## validation_metric_mse ##: (\\d*[.]?\\d*)\"}\n",
    ",{\"Name\": \"mean_absolute_error\",\n",
    " \"Regex\": \"## validation_metric_mae ##: (\\d*[.]?\\d*)\"}\n",
    ",{\"Name\": \"mean_absolute_percentage_error\",\n",
    " \"Regex\": \"## validation_metric_mape ##: (\\d*[.]?\\d*)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training mode: local vs remote instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type =   'local' # \"ml.c4.xlarge\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use spot instances\n",
    "\n",
    "Only valid when **not in** local mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set if you need spot instance\n",
    "use_spot = True\n",
    "train_max_run_secs =   24 * 60 * 60\n",
    "max_wait_time_secs = train_max_run_secs +  60 * 60\n",
    "\n",
    "\n",
    "# During local mode, no spot..\n",
    "if train_instance_type == 'local':\n",
    "    use_spot = False\n",
    "    max_wait_time_secs = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit training job\n",
    "\n",
    "We can use the SDK to run our local training script on SageMaker infrastructure.\n",
    "\n",
    "1. Pass the path to the abalone.py file, which contains the functions for defining your estimator, to the sagemaker.TensorFlow init method.\n",
    "2. Pass the S3 location that we uploaded our data to previously to the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from time import gmtime, strftime\n",
    "\n",
    "s3_model_path = \"s3://{}/models\".format(sagemaker_session.default_bucket())\n",
    "\n",
    "abalone_estimator = TensorFlow(entry_point='main_train.py',\n",
    "                               source_dir=source_dir,\n",
    "                               role=role,\n",
    "                               py_version=\"py3\",\n",
    "                               git_config = git_config,\n",
    "                               framework_version = \"1.11.0\",\n",
    "                               hyperparameters={'traindata' : 'abalone_train.csv',\n",
    "                                                'validationdata' : 'abalone_test.csv',\n",
    "                                                'epochs': 10, \n",
    "                                                'batch-size': 32},\n",
    "                               model_dir = s3_model_path,\n",
    "                               metric_definitions = metric_def,\n",
    "                               train_instance_count=1,\n",
    "                               train_use_spot_instances = use_spot,\n",
    "                               train_max_run =  train_max_run_secs,\n",
    "                               train_max_wait = max_wait_time_secs     ,                         \n",
    "                               train_instance_type=train_instance_type)\n",
    "\n",
    "abalone_estimator.fit( {'train': s3_input_prefix, \n",
    "                        'validation':s3_input_prefix}, \n",
    "                      job_name=\"ablone-age-py3-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`estimator.fit` will deploy a script in a container for training and returns the SageMaker model name using the following arguments:\n",
    "\n",
    "*   **`entry_point=\"main_train.py\"`** The path to the script that will be deployed to the container.\n",
    "*   **`training_steps=100`** The number of training steps of the training job.\n",
    "*   **`evaluation_steps=100`** The number of evaluation steps of the training job.\n",
    "*   **`role`**. AWS role that gives your account access to SageMaker training and hosting\n",
    "*   **`hyperparameters={'epochs' :10, ''batch-size:32}`**. Training hyperparameters. \n",
    "\n",
    "Running the code block above will do the following actions:\n",
    "* deploy your script in a container with tensorflow installed\n",
    "* Pip install the dependencies in the requirements.txt for you.\n",
    "* copy the data from the bucket to the container\n",
    "* save the estimator model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submiting a trained model for hosting\n",
    "\n",
    "The deploy() method creates an endpoint which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_predictor = abalone_estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "prediction_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "    filename=os.path.join('data/abalone_predict.csv'), target_dtype=np.int, features_dtype=np.float32)\n",
    "\n",
    "data = prediction_set.data\n",
    "prediction_set.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(abalone_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
