{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GluonCV YoloV3 Darknet training and optimizing using Neo\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Preparation](#Data-Preparation)\n",
    "  1. [Download data](#Download-Data)\n",
    "  2. [Convert data into RecordIO](#Convert-data-into-RecordIO)\n",
    "  3. [Upload to S3](#Upload-to-S3)\n",
    "4. [Training](#Training)\n",
    "5. [Compile the trained model using SageMaker Neo](#Compile-the-trained-model-using-SageMaker-Neo)\n",
    "6. [Deploy the compiled model](#Deploy-the-compiled-model)\n",
    "  1. [Inference](#Inference)\n",
    "7. [Delete the Endpoint](#Delete-the-Endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is an end-to-end example of GluonCV YoloV3 model training inside of Amazon SageMaker notebook and then compile the trained model using Neo runtime. In this demo, we will demonstrate how to train and to host a darknet53 model on the [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/) using the YoloV3 algorithm. We will also demonstrate how to optimize this trained model using Neo.\n",
    "\n",
    "***This notebook is for demonstration purpose only. Please fine tune the training parameters based on your own dataset.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To train the YoloV3 Darknet53 model on Amazon SageMaker, we need to setup and authenticate the use of AWS services. \n",
    "\n",
    "To start, we need to upgrade the [SageMaker SDK for Python](https://sagemaker.readthedocs.io/en/stable/v2.html) to the latest version if it is not and verify the same before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/anaconda3/envs/mxnet_p36/bin/pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "if sagemaker.__version__.split('.')[0] == '1':\n",
    "    raise Exception(\"Please upgrade sagemaker SDK by running the above cell while ensuring kernel name is the same as the one being used. Restart the kernel after upgrade.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need an AWS account role with SageMaker access. This role is used to give SageMaker access to your data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the S3 bucket that is used for training, and storing the tranied model artifacts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket() \n",
    "prefix = 'DEMO-ObjectDetectionYolo'\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) was a popular computer vision challenge and they released annual challenge datasets for object detection from 2005 to 2012. In this notebook, we will use the data sets from 2007 and 2012, named as VOC07 and VOC12 respectively. Cumulatively, we have more than 20,000 images containing about 50,000 annotated objects. These annotated objects are grouped into 20 categories.\n",
    "\n",
    "***Notes:***\n",
    "1. While using the Pascal VOC dataset, please be aware of the database usage rights. The VOC data includes images obtained from flickr's website. Use of these images must respect the corresponding terms of use: https://www.flickr.com/help/terms\n",
    "2. Default EBS Volume size for SageMaker Notebook instances is 5GB. While performing this step if you run out of storage then consider increasing the volume size. One way to do so is by using AWS CLI as documented [here](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/sagemaker/update-notebook-instance.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Download the Pascal VOC datasets from 2007 and 2012 from Oxford University's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Download and extract the datasets\n",
    "![ ! -f /tmp/pascal-voc.tgz ] && { wget -P /tmp https://s3.amazonaws.com/fast-ai-imagelocal/pascal-voc.tgz; }\n",
    "![ ! -d VOCdevkit ] && { tar -xf /tmp/pascal-voc.tgz; mv pascal-voc VOCdevkit; }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data into RecordIO\n",
    "[RecordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html) is a highly efficient binary data format from [MXNet](https://mxnet.incubator.apache.org/). Using this format, dataset is simple to prepare and transfer to the instance that will run the training job. Please refer to [object_detection_recordio_format](https://github.com/awslabs/amazon-sagemaker-examples/blob/80333fd4632cf6d924d0b91c33bf80da3bdcf926/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb) for more information about how to prepare RecordIO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tools/prepare_dataset.py --dataset pascal --year 2007,2012 --set trainval --target VOCdevkit/train.lst\n",
    "!python tools/prepare_dataset.py --dataset pascal --year 2007 --set test --target VOCdevkit/val.lst --no-shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "Upload the data to the S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the RecordIO files to train and validation channels\n",
    "train_channel = prefix + '/train'\n",
    "val_channel = prefix + '/val'\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_val_data = 's3://{}/{}'.format(bucket, val_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.upload_data(path='VOCdevkit/train.rec', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='VOCdevkit/train.idx', bucket=bucket, key_prefix=train_channel)\n",
    "\n",
    "sess.upload_data(path='VOCdevkit/val.rec', bucket=bucket, key_prefix=val_channel)\n",
    "sess.upload_data(path='VOCdevkit/val.idx', bucket=bucket, key_prefix=val_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "# Location to save your custom code in tar.gz format.\n",
    "custom_code_upload_location = 's3://{}/{}/customcode/mxnet'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sagemaker.MXNet`` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "data_shape = 320 # use 320, 416, 608...\n",
    "yolo_estimator = MXNet(entry_point='train_yolo.py',\n",
    "                       role=role,\n",
    "                       output_path=s3_output_location,\n",
    "                       code_location=custom_code_upload_location,\n",
    "                       instance_count=1,\n",
    "                       instance_type='ml.p3.16xlarge',\n",
    "                       framework_version='1.7.0',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters={'num-epochs': 10,\n",
    "                                       'data-shape': data_shape,\n",
    "                                       'gpus': '0,1,2,3,4,5,6,7',\n",
    "                                       'network': 'darknet53'\n",
    "                                      }\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yolo_estimator.fit({'train': s3_train_data, 'val': s3_val_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the trained model using SageMaker Neo\n",
    "\n",
    "After training the model we can use SageMaker Neo's ``compile_model()`` API to compile the trained model. When calling ``compile_model()`` user is expected to provide all the correct input shapes required by the model for successful compilation. We also specify the target instance family, the name of our IAM execution role, S3 bucket to which the compiled model would be stored and we set ``MMS_DEFAULT_RESPONSE_TIMEOUT`` environment variable to 500. \n",
    "\n",
    "For this example, we will choose `ml_p3` as the target instance family while compiling the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "compiled_model = yolo_estimator.compile_model(target_instance_family='ml_p3', \n",
    "                                             input_shape={'data':[1, 3, data_shape, data_shape]},\n",
    "                                             role=role,\n",
    "                                             output_path=s3_output_location,\n",
    "                                             framework='mxnet', \n",
    "                                             framework_version='1.7',\n",
    "                                             env={'MMS_DEFAULT_RESPONSE_TIMEOUT': '500'}\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the compiled model\n",
    "\n",
    "We have to deploy the compiled model on one of the instance family for which the trained model was compiled for. Since we have compiled for `ml_p3` we can deploy to any `ml.p3` instance type. For this example we will choose `ml.p3.2xlarge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "neo_object_detector = compiled_model.deploy(initial_instance_count = 1, instance_type = 'ml.p3.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now that the trained model is deployed at an endpoint that is up-and-running, we can use this endpoint for inference. To do this, we use an image from [PEXELS](https://www.pexels.com/) which the algorithm has so-far not seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "test_file = 'test.jpg'\n",
    "test_image = PIL.Image.open(test_file)\n",
    "test_image = np.asarray(test_image.resize((data_shape, data_shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = neo_object_detector.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the results. Here is a helper function to do the plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def visualize_detection(img_file, dets, classes=[], thresh=0.6):\n",
    "        \"\"\"\n",
    "        visualize detections in one image\n",
    "        Parameters:\n",
    "        ----------\n",
    "        img_file : numpy.array\n",
    "            image, in bgr format\n",
    "        dets : numpy.array\n",
    "            yolo detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n",
    "            each row is one object\n",
    "        classes : tuple or list of str\n",
    "            class names\n",
    "        thresh : float\n",
    "            score threshold\n",
    "        \"\"\"\n",
    "        import random\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.image as mpimg\n",
    "        from matplotlib.patches import Rectangle\n",
    "\n",
    "        img=mpimg.imread(img_file)\n",
    "        plt.imshow(img)\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        colors = dict()\n",
    "        klasses = dets[0][0]\n",
    "        scores = dets[1][0]\n",
    "        bbox = dets[2][0]\n",
    "        for i in range(len(classes)):\n",
    "            klass = klasses[i][0]\n",
    "            score = scores[i][0]\n",
    "            x0, y0, x1, y1 = bbox[i]\n",
    "            if score < thresh:\n",
    "                continue\n",
    "            cls_id = int(klass)\n",
    "            if cls_id not in colors:\n",
    "                colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "            xmin = int(x0 * width / data_shape)\n",
    "            ymin = int(y0 * height / data_shape)\n",
    "            xmax = int(x1 * width / data_shape)\n",
    "            ymax = int(y1 * height / data_shape)\n",
    "            rect = Rectangle((xmin, ymin), xmax - xmin,\n",
    "                                 ymax - ymin, fill=False,\n",
    "                                 edgecolor=colors[cls_id],\n",
    "                                 linewidth=3.5)\n",
    "            plt.gca().add_patch(rect)\n",
    "            class_name = str(cls_id)\n",
    "            if classes and len(classes) > cls_id:\n",
    "                class_name = classes[cls_id]\n",
    "            plt.gca().text(xmin, ymin-2,\n",
    "                            '{:s} {:.3f}'.format(class_name, score),\n",
    "                            bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n",
    "                                    fontsize=12, color='white')\n",
    "        plt.tight_layout(rect=[0, 0, 2, 2])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing object categories\n",
    "object_categories = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', \n",
    "                     'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', \n",
    "                     'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a threshold 0.1 will only plot detection results that have a confidence score greater than 0.1.\n",
    "threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the detections.\n",
    "visualize_detection(test_file, response, object_categories, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Endpoint name: \" + neo_object_detector.endpoint_name)\n",
    "neo_object_detector.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
