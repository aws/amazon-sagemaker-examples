{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bloom with HuggingFace Trainer + the SageMaker Model Parallelism Library with Sharded Data Parallelism"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through how to use the SMP Trainer as a drop-in replacement for Hugging Face Transformer's Trainer. This lets us enable sharded data parallelism with the SageMaker model parallelism (SMP) library to train a Bloom model. You'll learn how to train the model with sharded data parallelism on a text dataset.\n",
    "\n",
    "The Bloom model was proposed by BigScience in the paper [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/pdf/2211.05100.pdf). The original Bloom is a large transformer-based language model with 176 billion parameters. In this notebook, we will be experimenting with the 560 million parameter version. This notebook uses the [Hugging Face Transformers Bloom](https://bigscience.huggingface.co/blog/bloom) implementation with SageMaker model parallel integration.\n",
    "\n",
    "Sharded data parallelism is a memory-saving distributed training technique that splits the state of a model (model parameters, gradients, and optimizer states) across GPUs in a data parallel group. There are two main benefits: one, you can fit larger models, which would otherwise run out of memory with standard data parallelism, on fewer GPUs; or two, you can increase the batch size using the freed-up GPU memory.\n",
    "- To learn more about sharded data parallelism, see [Sharded Data Parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html).\n",
    "\n",
    "This notebook requires the following prerequisites:\n",
    "- `run_clm.py`: This is an entry point script, which is the example training script for the SageMaker Hugging Face estimator. This script is responsible for end-to-end training of the Bloom model.\n",
    "- `requirements.txt`: This file lists additional Python library dependencies that SageMaker will automatically install. This needs to be in the same directory as your entry point script.\n",
    "- `smp_trainer.py`: This file inherits from Hugging Face Transformer's Trainer and adds support for Sharded Data Parallelism through SageMaker Model Parallel.\n",
    "\n",
    "**Note**: To run this example training job, you must be in `us-west-2`. The container image used is located in this region. If your AWS Region is different from `us-west-2`, you must make sure you change the region code throughout this notebook.\n",
    "\n",
    "### Additional Resources\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to learn more about SMP and using SageMaker with PyTorch.\n",
    "\n",
    "- To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "- To learn more about using the SageMaker Python SDK with PyTorch, see [Using PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "- To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt your entrypoint script to use SMP Trainer\n",
    "To update your entrypoint script to use SMP Trainer instead of Hugging Face Trainer, all you need to do is update the Trainer import from Hugging Face to from SMP Trainer instead.  \n",
    "\n",
    "Replace the following line:  \n",
    "  `from transformers import Trainer`  \n",
    "with the SMP Trainer version:  \n",
    "  `from smp_trainer import SMPTrainer as Trainer`  \n",
    "\n",
    "You can now follow the rest of this notebook for details on how to enable sharded data parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Upgrade Libraries\n",
    "\n",
    "The SageMaker model parallelism library's tensor parallelism feature requires the SageMaker Python SDK and the SageMaker Experiments library. Run the following cell to install or upgrade the libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To finish applying the changes, you must restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run once, restart kernel, then comment out this cell\n",
    "# # update sagemaker to the latest 2.x version\n",
    "# ! pip3 install -qU pip\n",
    "! pip3 install -qU \"sagemaker>=2,<3\"\n",
    "\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and check if the SageMaker Python SDK version is successfully set to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Throughout this example, you'll use a training script of the Bloom model and a text dataset.\n",
    "\n",
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment: your AWS account ID, the AWS Region you are using to run the notebook, and the ARN of your Amazon SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)\n",
    "\n",
    "# You also need to specify an Amazon S3 bucket to store the output data such as training artifacts.\n",
    "# The following cell sets up the default S3 bucket paired with the current SageMaker session. You can also modify this as needed.\n",
    "s3_output_bucket = f\"s3://{default_bucket}/output\"\n",
    "print(f\"Your output data will be stored in: {s3_output_bucket}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Hyperparameters, Metric Definitions, and MPI Options\n",
    "The following `hyperparameters` dictionary is to pass arguments to the training script (`run_clm.py`) and set the model parallel configuration when creating the training job.\n",
    "\n",
    "Note that the `run_clm.py` file is currently modified to work with SageMaker. If you want to run your own script, you'll need to add the relevant lines as seen in `run_clm.py`. You can find them quickly by searching for `SageMaker Support`.\n",
    "\n",
    "You can also add custom mpi flags. By default, we have `--mca btl_vader_single_copy_mechanism none` to remove unnecessary logs.\n",
    "\n",
    "Next, we add a base metric definition to upload the training metrics for SageMaker Experiments. You can also add custom metric definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_steps = 60  # Set the interval for saving checkpoints\n",
    "max_steps = 100  # Set the total number of steps you want to run\n",
    "\n",
    "hyperparameters = {\n",
    "    \"model_name_or_path\": \"bigscience/bloom-560m\",\n",
    "    \"output_dir\": \"/opt/ml/checkpoints\",\n",
    "    \"overwrite_output_dir\": \"\",\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"save_steps\": save_steps,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"max_eval_samples\": 50,\n",
    "    \"preprocessing_num_workers\": 1,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"eval_accumulation_steps\": 2,\n",
    "    \"logging_steps\": 1,\n",
    "    \"dataloader_drop_last\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify a HuggingFace Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you specify the dataset from Hugging Face that you want to train on. Here we use the `glue` dataset. Note that larger datasets will take longer to download and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use any dataset available from Hugging Face\n",
    "# Modify these parameters as needed\n",
    "dataset_name = \"glue\"\n",
    "\n",
    "if dataset_name == \"glue\":\n",
    "    hyperparameters[\"dataset_name\"] = \"glue\"\n",
    "    hyperparameters[\"dataset_config_name\"] = \"sst2\"\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown HuggingFace dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model configuration below or define your own.\n",
    "\n",
    "You can also specify different training parameters here such as sharded data parallel degree, batch size, and fp16 which will affect if your model can fit on your instance configuration.\n",
    "\n",
    "For more information on these parameters and how to use them, please visit [SageMaker Distributed Training](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html).\n",
    "\n",
    "Note: you may need to adjust these parameters such as `sdp_degree` and `per_device_train_batch_size` if you choose to train another size model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \"bloom-560m\"\n",
    "\n",
    "if model_config == \"bloom-560m\":\n",
    "    # 560M parameters\n",
    "    hyperparameters[\"per_device_train_batch_size\"] = 4\n",
    "    hyperparameters[\"per_device_eval_batch_size\"] = 2\n",
    "    sdp_degree = 2\n",
    "    microbatches = 1\n",
    "    fp16 = False\n",
    "    hyperparameters[\"fp16\"] = fp16\n",
    "    prescaled_batch = True\n",
    "    shard_optimizer_state = False\n",
    "    pp_degree = 1\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown model config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Essential Parameters for a SageMaker Training Job\n",
    "\n",
    "Next, you will use the [SageMaker Estimator API](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to define a SageMaker training job, passing values through the following parameters, such as the training job name, the number of EC2 instances, the instance type, and the size of the volume attached to the instances.\n",
    "\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "* `base_job_name`\n",
    "\n",
    "### Update the Type and Number of EC2 Instance to Use\n",
    "\n",
    "The instance type and the number of instances you specify to the `instance_type` and `instance_count` parameters, respectively, will determine the total number of GPUs (world size).\n",
    "\n",
    "$$ \\text{(world size) = (the number of GPUs on a single instance)}\\times\\text{(the number of instance)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the instance_type here\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "\n",
    "# Set to the number of instances you want to use\n",
    "# bloom-560m needs >= 1 p4d instances\n",
    "instance_count = 1\n",
    "\n",
    "# set to the number of GPUs on that instance\n",
    "# p3d's and p4d's have 8 GPUs each\n",
    "processes_per_host = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look up the number of GPUs of different instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that, for example, a given instance type `p4d.24xlarge` has a corresponding instance type `ml.p4d.24xlarge` in SageMaker.\n",
    "For SageMaker supported `ml` instances and cost information, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach an EBS Volume to the Training Instance\n",
    "The volume size you specify in `volume_size` must be larger than your input data size. In this example, the volume size is set to 500 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a Base Job Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SM_HP_MP_PARAMETERS = {\n",
    "    \"microbatches\": microbatches,\n",
    "    \"optimize\": \"speed\",\n",
    "    \"pipeline\": \"interleaved\",\n",
    "    \"placement_strategy\": \"cluster\",\n",
    "    \"partitions\": pp_degree,\n",
    "    \"prescaled_batch\": prescaled_batch,\n",
    "    \"shard_optimizer_state\": shard_optimizer_state,\n",
    "    \"fp16\": fp16,\n",
    "    \"sharded_data_parallel_degree\": sdp_degree,  # Add this to activate sharded data parallelism\n",
    "}\n",
    "\n",
    "machine_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "\n",
    "base_job_name = f'smp-trainer-{model_config}-{machine_str}-sdp{sdp_degree}-bs{hyperparameters[\"per_device_train_batch_size\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "if instance_type in [\"ml.p3dn.24xlarge\", \"ml.p4d.24xlarge\"]:\n",
    "    mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 \"\n",
    "if SM_HP_MP_PARAMETERS[\"partitions\"] > 1:\n",
    "    mpioptions += \"-x SMP_ENABLE_CROSS_NODE_D2D=1 \"\n",
    "# Uncomment out the following line if you want to save the full model checkpoint\n",
    "# instead of the partial model checkpoint.\n",
    "# Setting the flag to anything will enable the save full model logic.\n",
    "# mpioptions += \"-x HF_TRAINER_SMP_SDP_SAVE_FULL_MODEL=1 \"\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
    "]  # Add your custom metric definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume Training from a Previous Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can choose to resume training from a previous checkpoint saved with HuggingFace Trainer.\n",
    "Simply set `resume_from_checkpoint` to `True` and specify the bucket in which the checkpoint is stored. For convenience, we use the same bucket to load checkpoints and save output artifacts. You can also customize and set your own. You can also specify whether to load from a partial checkpoint or full checkpoint. Trainer saves both \n",
    "\n",
    "Note: The checkpoint path (`checkpoint_s3_uri`) is not unique per job.\n",
    "You need to modify as needed for different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_checkpoint = False\n",
    "# Set `resume_from_full_checkpoint` to true if you want to load full ckpt instead of partial.\n",
    "# Note: You need to uncomment the above\n",
    "#   HF_TRAINER_SMP_SDP_SAVE_FULL_MODEL environment option.\n",
    "resume_from_full_checkpoint = False\n",
    "\n",
    "\n",
    "# We label our job with the model configuration and the number of nodes\n",
    "job_name = f\"{model_config}_nodes-{instance_count}\"\n",
    "# Here, we use the same bucket for both checkpoints and outputs\n",
    "checkpoint_bucket = s3_output_bucket\n",
    "# If you want to resume training, set checkpoint_s3_uri to the same checkpoint_s3_uri path as a previous job.\n",
    "checkpoint_s3_uri = f\"{checkpoint_bucket}/{job_name}/checkpoints\"\n",
    "\n",
    "# The previous checkpoint to load must have the same model config.\n",
    "if resume_from_checkpoint:\n",
    "    # The checkpoint step you want to resume training from.\n",
    "    # Here, we set it to the first checkpoint saved, but you can set it to any.\n",
    "    checkpoint_step = save_steps\n",
    "    checkpoint_dir = f\"/opt/ml/checkpoints/checkpoint-{checkpoint_step}\"\n",
    "    hyperparameters[\"resume_from_checkpoint\"] = checkpoint_dir\n",
    "    if resume_from_full_checkpoint:\n",
    "        hyperparameters[\"load_full\"] = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker PyTorch Estimator\n",
    "\n",
    "The following cell constructs a `PyTorch` estimator using the parameters defined above. To see how the SageMaker tensor parallelism modules and functions are applied to the script, see the `run_clm.py` file and the private preview documentation. We will be using `PyTorch 1.13.1` along with `Transformers 4.21.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"run_clm.py\",\n",
    "    source_dir=os.getcwd(),  # copies your current working directory to S3 for SageMaker\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=volume_size,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": processes_per_host,\n",
    "            \"custom_mpi_options\": mpioptions,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"ddp\": True,\n",
    "                    # partitions is a required param in the current SM SDK so it needs to be passed,\n",
    "                    # these two map to the same config\n",
    "                    \"partitions\": SM_HP_MP_PARAMETERS[\"partitions\"],\n",
    "                    \"microbatches\": SM_HP_MP_PARAMETERS[\"microbatches\"],\n",
    "                    \"shard_optimizer_state\": SM_HP_MP_PARAMETERS[\"shard_optimizer_state\"],\n",
    "                    \"prescaled_batch\": SM_HP_MP_PARAMETERS[\"prescaled_batch\"],\n",
    "                    \"fp16\": True,\n",
    "                    \"optimize\": SM_HP_MP_PARAMETERS[\"optimize\"],\n",
    "                    \"auto_partition\": True,\n",
    "                    \"default_partition\": 0,\n",
    "                    \"sharded_data_parallel_degree\": SM_HP_MP_PARAMETERS[\n",
    "                        \"sharded_data_parallel_degree\"\n",
    "                    ],  # Add this to activate sharded data parallelism\n",
    "                    \"sdp_reduce_bucket_size\": int(5e8),  # Optional\n",
    "                    \"sdp_param_persistence_threshold\": int(1e6),  # Optional\n",
    "                    \"sdp_max_live_parameters\": int(1e9),  # Optional\n",
    "                    \"sdp_gradient_clipping\": 1.0,  # Optional\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    py_version=\"py39\",\n",
    "    output_path=s3_output_bucket,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    framework_version=\"1.13.1\",\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    base_job_name=base_job_name,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the estimator to launch the SageMaker training job of the Bloom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "smp_estimator.fit(\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Training Logs\n",
    "\n",
    "You can access the training logs using [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html). Make sure to look at the logs of **algo-1**, which is the main node whose output stream has the entire training job logs.\n",
    "\n",
    "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see **Processing Job, Training Job, Batch Transform Job, and Endpoint Instance Metrics** in [Monitor Amazon SageMaker with Amazon CloudWatch](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html).\n",
    "\n",
    "If you are a new user of Amazon CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html).\n",
    "\n",
    "For additional information about monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/training|distributed_training|pytorch|model_parallel|bloom_smp_trainer|submit_smp_trainer.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
