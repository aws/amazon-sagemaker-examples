{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd15c8e",
   "metadata": {},
   "source": [
    "# SageMaker Training Compiler - Finding Max Batch Size for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1cb84",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "3. [Finding max batch size](#Finding-max-batch-size)\n",
    "    1. [Model and instance type specifications](#Model-and-instance-type-specifications)  \n",
    "    2. [Finding max batch size for SageMaker Training Compiler with Hugging Face and PyTorch](#Finding-max-batch-size-for-SageMaker-Training-Compiler-with-Hugging-Face-and-PyTorch)\n",
    "    3. [Wait for find max batch job to complete](#Wait-for-find-max-batch-job-to-complete)\n",
    "4. [Results](#Results)  \n",
    "    1. [Load logs for find max batch job](#Load-logs-for-find-max-batch-job)  \n",
    "5. [Clean up](#Clean-up) \n",
    "6. [Conclusion](#Conclusion) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c125ff",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The SageMaker Training Compiler allows AWS customers to train deep learning models faster on scalable GPU instances managed by SageMaker. The memory optimizations made by SageMaker Training Compiler typically allow for your training job to fit more data into GPU memory. By increasing the batch size as much as possible in your training job, you can speed up your training jobs even further.\n",
    "\n",
    "For example, for a PyTorch fine-tuning job (Sequence_Len=512, Automatic Mixed Precision (AMP)) with a GPT-2 model from Hugging Face, the maximum batch size that can fit on an ml.p3.2xlarge instance increased from 6 to 20 with the Training Compiler enabled. A list of model examples and maximum batch sizes is available in the SageMaker Training Compiler documentation under \"Tested Models\": https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html\n",
    "\n",
    "The goal of this Notebook is to give you an example of how you can find the max batch size for a particular model and instance type. We show you how to find the max batch size for a gpt2 model below running on an `ml.p3.8xlarge` instance. You can customize this Notebook to fit your use case, and use the resulting max batch size as the value of your batch size parameter in your full training job.\n",
    "\n",
    "In this demo, you'll use Hugging Face's `transformers` and `datasets` libraries with Amazon SageMaker Training Compiler to train the `gpt-2` model on the `Stanford Sentiment Treebank v2 (SST2)` dataset. Please note that by using this notebook you will be downloading SST2 from https://huggingface.co/datasets/sst2 and can check dataset information and terms there. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "The Notebook uses the HuggingFace training scripts (`run_mlm.py` and `run_clm.py`) and a hands-on script (`find_max_batch_size.py`) to iteratively search for the maximum batch for a given GPU instance. \n",
    "\n",
    "This Notebook runs the `run_clm.py` by default, as will be shown in the following sections. If you want to test with your own training script, you need to update the following:\n",
    "- The `find_max_batch_size.py` script - In line 23 to 28 of the script, specify the right directory path and the file name of your training script.\n",
    "- `hyperparameters` - In the following Tune a Native PyTorch Training Job section, modify the hyperparameters that your training script requires accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e33ba",
   "metadata": {},
   "source": [
    "## Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4801c",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "This example notebook requires the **SageMaker Python SDK v2.70.0** and **transformers v4.11.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465feea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall sagemaker==2.70.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c266b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import sagemaker\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"sagemaker: {sagemaker.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7598fb",
   "metadata": {},
   "source": [
    "### Development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# SageMaker session bucket -> used for uploading data, models and logs\n",
    "# SageMaker will automatically create this bucket if it does not exist\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa609c",
   "metadata": {},
   "source": [
    "## Finding max batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326fa565",
   "metadata": {},
   "source": [
    "### Model and instance type specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d91c1",
   "metadata": {},
   "source": [
    "This notebook uses HF training script to demonstrate how to find the max batch size that can fit in memory, if you're using a customized training script, please update `find_max_batch_size.py` script and `hyperparameters` accordingly. Below, we specify the model we would like to find the max batch size for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_MODELING_LOSS = \"clm\"\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "TOKENIZER_NAME = \"gpt2\"\n",
    "MODEL_CONFIG = \"model_name_or_path\"\n",
    "\n",
    "INSTANCE_TYPE = \"ml.p3.8xlarge\"\n",
    "\n",
    "# hyperparameters are passed to the training entrypoint as arguments\n",
    "hyperparameters = {\n",
    "    \"training_script\": f\"run_{LANGUAGE_MODELING_LOSS}.py\",\n",
    "    MODEL_CONFIG: MODEL_NAME,\n",
    "    \"tokenizer_name\": TOKENIZER_NAME,\n",
    "    \"fp16\": True,\n",
    "    \"sequence_len\": 512,\n",
    "    \"per_device_train_batch_size_min\": 1,\n",
    "    \"per_device_train_batch_size_max\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd53820",
   "metadata": {},
   "source": [
    "### Finding max batch size for SageMaker Training Compiler with Hugging Face and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prints the training script for reference\n",
    "# The script iteratively tests different batch sizes\n",
    "!pygmentize ./scripts/find_max_batch_size.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temp-e6aa609c",
   "metadata": {},
   "source": [
    "### Configure a SageMaker HuggingFace estimator with the SageMaker Training Compiler configuration and the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98587d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n",
    "\n",
    "# configure the training job\n",
    "optimized_estimator = HuggingFace(\n",
    "    entry_point=\"find_max_batch_size.py\",  # Wrapper around training script that finds the maximum batch size\n",
    "    compiler_config=TrainingCompilerConfig(),  # We are enabling SageMaker Training Compiler here !\n",
    "    source_dir=\"./scripts\",\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    volume_size=100,\n",
    "    py_version=\"py38\",\n",
    "    transformers_version=\"4.11.0\",\n",
    "    pytorch_version=\"1.9.0\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_profiler=True,  # Disabling SageMaker Profiler to avoid overheads during benchmarking\n",
    "    debugger_hook_config=False,  # Disabling SageMaker Debugger to avoid overheads during benchmarking\n",
    ")\n",
    "\n",
    "# start the training job\n",
    "optimized_estimator.fit(wait=False)\n",
    "optimized_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbecffb3",
   "metadata": {},
   "source": [
    "### Wait for the training job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = optimized_estimator.sagemaker_session.sagemaker_client.get_waiter(\n",
    "    \"training_job_completed_or_stopped\"\n",
    ")\n",
    "waiter.wait(TrainingJobName=optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa2ff6",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca81ba",
   "metadata": {},
   "source": [
    "### Load logs for training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture optimized\n",
    "\n",
    "# access the logs of the optimized training job\n",
    "optimized_estimator.sagemaker_session.logs_for_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the max batch size below\n",
    "\n",
    "for line in optimized.stdout.split(\"\\n\"):\n",
    "    if \"result\" in line and \"max_batch_size\" in line or \"Total max batch\" in line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ea509",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Stop all training jobs launched if the jobs are still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "def stop_training_job(name):\n",
    "    status = sm.describe_training_job(TrainingJobName=name)[\"TrainingJobStatus\"]\n",
    "    if status == \"InProgress\":\n",
    "        sm.stop_training_job(TrainingJobName=name)\n",
    "\n",
    "\n",
    "stop_training_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e33f76",
   "metadata": {},
   "source": [
    "Also, to find instructions on cleaning up resources, see [Clean Up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html) in the *Amazon SageMaker Developer Guide*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c5820",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "SageMaker Training Compiler improves the efficiency of your training job by typically decreasing the memory footprint of the job. In this notebook, you found the largest `batch_size` that can fit in memory with Training Compiler's optimizations. Increasing the `batch_size` can decrease the time needed to train a model, reducing cost and enabling faster iteration.\n",
    "\n",
    "Remember that learning rate should be adjusted when `batch_size` is changed to minimize difference in convergence behavior during training. For more information, see https://arxiv.org/abs/1706.02677"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
