{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GPT2 with HuggingFace Trainer + the SageMaker Model Parallelism Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through how to use Hugging Face Transformer's Trainer with the SageMaker model parallelism (SMP) library to train a GPT-2 model. You'll learn how to train the model with tensor parallelism on a synthetic text dataset.\n",
    "\n",
    "The GPT-2 model was proposed by OpenAI in paper [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). The original GPT-2 is a large transformer-based language model with 1.5 billion parameters. In this notebook, you can experiment with the model parameters to achieve different model sizes. This notebook uses the [Hugging Face Transformers GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html) implementation with SageMaker model parallel integration.\n",
    "\n",
    "This notebook requires the following prerequisites:\n",
    "- `run_clm.py`: This is an entry point script, which is the example training script for the SageMaker Hugging Face estimator. This script is responsible for end-to-end training of the GPT-2 model.\n",
    "- `requirements.txt`: This file lists additional Python library dependencies that SageMaker will automatically install. This needs to be in the same directory as your entry point script. \n",
    "\n",
    "**Note**: To run this example training job, you must be in `us-west-2`. The container image used is located in this region. If your AWS Region is different from `us-west-2`, you must make sure you change the region code throughout this notebook.\n",
    "\n",
    "### Additional Resources\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to learn more about SMP and using SageMaker with PyTorch.\n",
    "\n",
    "- To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "- To learn more about using the SageMaker Python SDK with PyTorch, see [Using PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "- To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Upgrade Libraries\n",
    "\n",
    "The SageMaker model parallelism library's tensor parallelism feature requires the SageMaker Python SDK and the SageMaker Experiments library. Run the following cell to install or upgrade the libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To finish applying the changes, you must restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run once, restart kernel, then comment out this cell\n",
    "# # update sagemaker to the latest 2.x version\n",
    "# ! pip3 install -qU pip\n",
    "# ! pip3 install -qU \"sagemaker>=2,<3\"\n",
    "# ! pip3 install -qU sagemaker-experiments\n",
    "\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and check if the SageMaker Python SDK version is successfully set to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Throughout this example, you'll use a training script of the GPT-2 model and a text dataset.\n",
    "\n",
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment: your AWS account ID, the AWS Region you are using to run the notebook, and the ARN of your Amazon SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import boto3\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need to specify an Amazon S3 bucket to store the output data such as training artifacts. The following cell sets up the default S3 bucket paired with the current SageMaker session. You can also modify this as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = f\"s3://{default_bucket}/output/\"\n",
    "print(f\"Your output data will be stored in: {s3_output_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Hyperparameters, Metric Definitions, and MPI Options\n",
    "The following `hyperparameters` dictionary is to pass arguments to the training script (`run_clm.py`) and set the model parallel configuration when creating the training job.\n",
    "\n",
    "Note that the `run_clm.py` file is currently modified to work with SageMaker. If you want to run your own script, you'll need to add the relevant lines as seen in `run_clm.py`. You can find them quickly by searching for `SageMaker Support`.\n",
    "\n",
    "You can also add custom mpi flags. By default, we have `--mca btl_vader_single_copy_mechanism none` to remove unnecessary logs.\n",
    "\n",
    "Next, we add a base metric definitions to upload the training metrics for SageMaker Experiments. You can also add custom metric definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_steps = 60  # Set the interval for saving checkpoints\n",
    "max_steps = 100  # Set the total number of steps you want to run\n",
    "\n",
    "hyperparameters = {\n",
    "    \"output_dir\": \"/opt/ml/checkpoints\",\n",
    "    \"overwrite_output_dir\": \"\",\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"do_train\": \"\",\n",
    "    \"save_steps\": save_steps,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"eval_steps\": 20,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"model_type\": \"gpt2\",\n",
    "    \"tokenizer_name\": \"gpt2\",\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"dataloader_drop_last\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify a HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you specify the dataset from Hugging Face that you want to train on. Here we use the `wikitext` dataset. Note that larger datasets will take longer to download and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use any dataset available from Hugging Face\n",
    "# Modify these parameters as needed\n",
    "dataset_name = \"wikitext\"\n",
    "\n",
    "if dataset_name == \"wikitext\":\n",
    "    # 0.37 MB download, 1.1 GB generated\n",
    "    hyperparameters[\"dataset_name\"] = \"wikitext\"  # primary dataset name\n",
    "    hyperparameters[\"dataset_config_name\"] = \"wikitext-2-raw-v1\"  # set config for your data subset\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown HuggingFace dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model configuration below. Choose one of `gpt2-small`, `gpt2-xl`, `gpt2-5b`, or define your own. If you want to start from the smallest model, specify `gpt2-small`. The other larger models require `p4d` instances with more GPU memory.\n",
    "\n",
    "You can also specify different training parameters here such as batch size, tensor parallelism, data parallelism, and fp16 which will affect if your model can fit on your instance configuration.\n",
    "\n",
    "For more information on these parameters and how to use them, please visit [SageMaker Distributed Training](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html).\n",
    "\n",
    "Note: you may need to adjust these parameters such as `tp_degree` and `pp_degree` if you choose to train another size model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \"gpt2-small\"  # ['gpt2-small', 'gpt2-xl', 'gpt2-5b']\n",
    "\n",
    "if model_config == \"gpt2-small\":\n",
    "    # 100M parameters\n",
    "    hyperparameters[\"per_device_train_batch_size\"] = 2\n",
    "    hyperparameters[\"per_device_eval_batch_size\"] = 2\n",
    "    tp_degree = 4\n",
    "    pp_degree = 1\n",
    "    microbatches = 1\n",
    "    fp16 = True\n",
    "    hyperparameters[\"fp16\"] = fp16\n",
    "    prescaled_batch = False\n",
    "    shard_optimizer_state = False\n",
    "elif model_config == \"gpt2-xl\":\n",
    "    # 1.5B parameters\n",
    "    # Requires p4d\n",
    "    hyperparameters[\n",
    "        \"config_overrides\"\n",
    "    ] = \"n_embd=1536,n_layer=48,n_head=24\"  # note: last param must not have trailing ','\n",
    "    hyperparameters[\"per_device_train_batch_size\"] = 2\n",
    "    hyperparameters[\"per_device_eval_batch_size\"] = 4\n",
    "    tp_degree = 8\n",
    "    pp_degree = 1\n",
    "    microbatches = 1\n",
    "    fp16 = True\n",
    "    hyperparameters[\"fp16\"] = fp16\n",
    "    prescaled_batch = True\n",
    "    shard_optimizer_state = True\n",
    "elif model_config == \"gpt2-5b\":\n",
    "    # 4.5B parameters\n",
    "    # Requires p4d\n",
    "    hyperparameters[\"config_overrides\"] = \"n_embd=3080,n_layer=40,n_head=40\"\n",
    "    hyperparameters[\"per_device_train_batch_size\"] = 2\n",
    "    hyperparameters[\"per_device_eval_batch_size\"] = 4\n",
    "    tp_degree = 8\n",
    "    pp_degree = 2\n",
    "    microbatches = 2\n",
    "    fp16 = True\n",
    "    hyperparameters[\"fp16\"] = fp16\n",
    "    prescaled_batch = True\n",
    "    shard_optimizer_state = True\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown model config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up SageMaker Studio Experiment\n",
    "Create or load [SageMaker Experiment](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) for the example training job. This will create an experiment trial object in SageMaker Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Specify your experiment name\n",
    "experiment_name = \"gpt2-hf-trainer\"\n",
    "# Specify your trial name\n",
    "trial_name = f\"{experiment_name}-trial\"\n",
    "\n",
    "all_experiment_names = [exp.experiment_name for exp in Experiment.list()]\n",
    "# Load the experiment if it exists, otherwise create\n",
    "if experiment_name not in all_experiment_names:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client\n",
    "    )\n",
    "else:\n",
    "    experiment = Experiment.load(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client\n",
    "    )\n",
    "\n",
    "# Create the trial\n",
    "trial = Trial.create(\n",
    "    trial_name=\"gpt2-hf-trainer-{}-{}\".format(trial_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())),\n",
    "    experiment_name=experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm_boto_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Essential Parameters for a SageMaker Training Job\n",
    "\n",
    "Next, you will use the [SageMaker Estimator API](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to define a SageMaker training job, passing values through the following parameters, such as the training job name, the number of EC2 instances, the instance type, and the size of the volume attached to the instances.\n",
    "\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "* `base_job_name`\n",
    "\n",
    "### Update the Type and Number of EC2 Instance to Use\n",
    "\n",
    "The instance type and the number of instances you specify to the `instance_type` and `instance_count` parameters, respectively, will determine the total number of GPUs (world size).\n",
    "\n",
    "$$ \\text{(world size) = (the number of GPUs on a single instance)}\\times\\text{(the number of instance)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the instance_type here\n",
    "# Note: to run models bigger than gpt2-small, please use p4d.24xlarge instances\n",
    "instance_type = \"ml.p3.16xlarge\"  # ['ml.p3.16xlarge', 'ml.p4d.24xlarge]\n",
    "\n",
    "\n",
    "# Set to the number of instances you want to use\n",
    "# gpt2-small needs >= 2 p3d instances\n",
    "# gpt2-xl needs >= 1 p4d instance\n",
    "# gpt2-5b needs >= 2 p4d instances\n",
    "instance_count = 2\n",
    "\n",
    "# set to the number of GPUs on that instance\n",
    "# p3d's and p4d's have 8 GPUs each\n",
    "processes_per_host = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look up the number of GPUs of different instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that, for example, a given instance type `p4d.24xlarge` has a corresponding instance type `ml.p4d.24xlarge` in SageMaker.\n",
    "For SageMaker supported `ml` instances and cost information, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach an EBS Volume to the Training Instance\n",
    "The volume size you specify in `volume_size` must be larger than your input data size. In this example, the volume size is set to 500GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a Base Job Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SM_HP_MP_PARAMETERS = {\n",
    "    \"microbatches\": microbatches,\n",
    "    \"optimize\": \"speed\",\n",
    "    \"pipeline\": \"interleaved\",\n",
    "    \"placement_strategy\": \"cluster\",\n",
    "    \"tensor_parallel_degree\": tp_degree,\n",
    "    \"partitions\": pp_degree,\n",
    "    \"prescaled_batch\": prescaled_batch,\n",
    "    \"shard_optimizer_state\": shard_optimizer_state,\n",
    "    \"fp16\": fp16,\n",
    "}\n",
    "\n",
    "machine_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "\n",
    "base_job_name = f'smp-hf-trainer-{model_config}-{machine_str}-tp{tp_degree}-pp{pp_degree}-bs{hyperparameters[\"per_device_train_batch_size\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "if instance_type in [\"ml.p3dn.24xlarge\", \"ml.p4d.24xlarge\"]:\n",
    "    mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 \"\n",
    "if SM_HP_MP_PARAMETERS[\"partitions\"] > 1:\n",
    "    mpioptions += \"-x SMP_ENABLE_CROSS_NODE_D2D=1 \"\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
    "]  # Add your custom metric definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume Training from a Previous Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can choose to resume training from a previous checkpoint saved with HuggingFace Trainer.\n",
    "Simply set `resume_from_checkpoint` to `True` and specify the bucket in which the checkpoint is stored. For convenience, we use the same bucket to load checkpoints and save output artifacts. You can also customize and set your own.\n",
    "\n",
    "Note: The checkpoint path (`checkpoint_s3_uri`) is not unique per job.\n",
    "You need to modify as needed for different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_checkpoint = False\n",
    "\n",
    "# We label our job with the model configuration and the number of nodes\n",
    "job_name = f\"{model_config}_nodes-{instance_count}\"\n",
    "# Here, we use the same bucket for both checkpoints and outputs\n",
    "checkpoint_bucket = s3_output_bucket\n",
    "# If you want to resume training, set checkpoint_s3_uri to the same checkpoint_s3_uri path as a previous job.\n",
    "checkpoint_s3_uri = f\"{checkpoint_bucket}/{job_name}/checkpoints\"\n",
    "\n",
    "# The previous checkpoint to load must have the same model config.\n",
    "if resume_from_checkpoint:\n",
    "    # the checkpoint step you want to resume training from\n",
    "    # here, we set it to the first checkpoint saved, but you can set it to any\n",
    "    checkpoint_step = save_steps\n",
    "    checkpoint_dir = f\"/opt/ml/checkpoints/checkpoint-{checkpoint_step}\"\n",
    "    hyperparameters[\"resume_from_checkpoint\"] = checkpoint_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker HuggingFace Estimator\n",
    "\n",
    "The following cell constructs a `HuggingFace` estimator using the parameters defined above. To see how the SageMaker tensor parallelism modules and functions are applied to the script, see the `run_clm.py` file and the private preview documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "\n",
    "smp_estimator = HuggingFace(\n",
    "    entry_point=\"run_clm.py\",\n",
    "    source_dir=os.getcwd(),  # copies your current working directory to S3 for SageMaker\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=volume_size,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": processes_per_host,\n",
    "            \"custom_mpi_options\": mpioptions,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"ddp\": True,\n",
    "                    \"tensor_parallel_degree\": SM_HP_MP_PARAMETERS[\"tensor_parallel_degree\"],\n",
    "                    # partitions is a required param in the current SM SDK so it needs to be passed,\n",
    "                    # these two map to the same config\n",
    "                    \"partitions\": SM_HP_MP_PARAMETERS[\"partitions\"],\n",
    "                    \"microbatches\": SM_HP_MP_PARAMETERS[\"microbatches\"],\n",
    "                    \"shard_optimizer_state\": SM_HP_MP_PARAMETERS[\"shard_optimizer_state\"],\n",
    "                    \"prescaled_batch\": SM_HP_MP_PARAMETERS[\"prescaled_batch\"],\n",
    "                    \"fp16\": SM_HP_MP_PARAMETERS[\"fp16\"],\n",
    "                    \"optimize\": SM_HP_MP_PARAMETERS[\"optimize\"],\n",
    "                    \"auto_partition\": True,\n",
    "                    \"default_partition\": 0,\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    py_version=\"py38\",\n",
    "    output_path=s3_output_bucket,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    image_uri=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.12.0-gpu-py38-cu113-ubuntu20.04-sagemaker\",\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    base_job_name=base_job_name,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the estimator to launch the SageMaker training job of GPT2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "smp_estimator.fit(\n",
    "    experiment_config={\n",
    "        \"ExperimentName\": experiment.experiment_name,\n",
    "        \"TrialName\": trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Training\",\n",
    "    },\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Training Logs\n",
    "\n",
    "You can access the training logs using [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html). Make sure to look at the logs of **algo-1**, which is the main node whose output stream has the entire training job logs.\n",
    "\n",
    "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see **Processing Job, Training Job, Batch Transform Job, and Endpoint Instance Metrics** in [Monitor Amazon SageMaker with Amazon CloudWatch](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html).\n",
    "\n",
    "If you are a new user of Amazon CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html).\n",
    "\n",
    "For additional information about monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
