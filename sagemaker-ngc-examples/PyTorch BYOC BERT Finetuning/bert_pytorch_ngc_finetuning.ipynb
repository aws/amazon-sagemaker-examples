{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning PyTorch BERT with NGC\n",
    "The BERT family of models are a powerful set of natural language understanding models based on the transformer architecture from the paper Attention Is All You Need, which you can find here:  https://arxiv.org/abs/1706.03762\n",
    "\n",
    "These models work by running unsupervised pre-training on massive sets of text data. This process requires an enormous amount of time and compute. Luckily for us, BERT models are built for transfer learning. BERT models are able to be finetuned to perform many different NLU tasks like question answering, sentiment analysis, document summarization, and more.\n",
    "\n",
    "For this tutorial, we are going to download a BERT base model and finetune this model on the Stanford Question Answering Dataset and walk through the steps necessary to deploy it to a Sagemaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://api.ngc.nvidia.com/v2/models/nvidia/bert_base_pyt_amp_ckpt_pretraining_lamb/versions/1/files/bert_base.pt -O bert_base.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "import os, tarfile, json\n",
    "import time, datetime\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import estimator, PyTorchModel, PyTorchPredictor, PyTorch\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "from file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from modeling import BertForQuestionAnswering, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from tokenization import (BasicTokenizer, BertTokenizer, whitespace_tokenize)\n",
    "from types import SimpleNamespace\n",
    "from helper_funcs import *\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket() # can replace with your own S3 bucket 'privisaa-bucket-virginia'\n",
    "prefix = 'bert_pytorch_ngc'\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "with open('s3_bucket.txt','w') as f:\n",
    "    f.write(f's3://{bucket}')\n",
    "with open('hyperparameters.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "params['save_to_s3'] = bucket\n",
    "with open('hyperparameters.json', 'w') as f:\n",
    "    json.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our training docker container\n",
    "\n",
    "Now we are going to create a custom docker container based on the NGC Bert container and push it to AWS Elastic Container Registry (ECR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=bert-ngc-torch-train\n",
    "\n",
    "chmod +x train\n",
    "chmod +x serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "# some kind of security auth issue with pushing this to ecr, not authorized to perform ecr:InitiateLayerUpload\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model\n",
    "\n",
    "Now we are going to instantiate our model, here we are going to specify our hyperparameters for training as well as the number of GPUs we are going to use. The ml.p3.16xlarge instances contain 8 V100 volta GPUs, making them ideal for heavy duty deep learning training. \n",
    "\n",
    "Once we have set our hyperparameters, we will instantiate a Sagemaker Estimator that we will use to run our training job. We specify the Docker image we just pushed to ECR as well as an entrypoint giving instructions for what operations our container should perform when it starts up. Our Docker container has two commands, train and serve. When we instantiate a training job, behind the scenes Sagemaker is running our Docker container and telling it to run the train command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account=!aws sts get-caller-identity --query Account --output text\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=!aws configure get region\n",
    "\n",
    "algoname = 'bert-ngc-torch-train'\n",
    "\n",
    "fullname=\"{}.dkr.ecr.{}.amazonaws.com/{}\".format(account[0], region[0], algoname)\n",
    "\n",
    "fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our hyperparameters\n",
    "hyperparameters = {'bert_model': 'bert-base-uncased',  'num_train_epochs': 1, \n",
    "                   'vocab_file': '/workspace/bert/data/bert_vocab.txt',\n",
    "                   'config_file':'/workspace/bert/bert_config.json', \n",
    "                  'output_dir': 'opt/ml/model',\n",
    "                  'train_file': '/workspace/bert/data/squad/v1.1/train-v1.1.json',\n",
    "                  'num_gpus':8, 'train_batch_size':8, 'max_seq_length':512, 'doc_stride':128, 'seed':1,\n",
    "                  'learning_rate':3e-5,\n",
    "                  'save_to_s3':bucket}\n",
    "\n",
    "# instantiate model\n",
    "torch_model = PyTorch( role=role,\n",
    "                      train_instance_count=2,\n",
    "                      train_instance_type='ml.p3.16xlarge',\n",
    "                      entry_point='transform_script.py',\n",
    "                      image_name=fullname,\n",
    "                      framework_version='1.4.0',\n",
    "                      hyperparameters=hyperparameters\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "If you use an instance with 4 GPUs and a batch size of 3 this process will take ~15 minutes to complete for this particular finetuning task with 2 epochs. Each additional epoch will add another 7 or so minutes. It's recommended to at minimum use a training instance with 4 GPUs, although you will likely get better performance with one of the ml.p3.16xlarge or ml.p3dn.24xlarge instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy our trained model\n",
    "\n",
    "Now that we've finetuned our base BERT model, what now? Let's deploy our trained model to an endpoint and ask it some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'bert-endpoint-byoc'\n",
    "\n",
    "#     bert_end = torch_model.deploy(instance_type='ml.g4dn.4xlarge', initial_instance_count=1, \n",
    "#                           endpoint_name=endpoint_name)\n",
    "\n",
    "model_data = f's3://{bucket}/model.tar.gz'\n",
    "\n",
    "torch_model = PyTorchModel(model_data=model_data,\n",
    "                       role=role,\n",
    "                      entry_point='transform_script.py',\n",
    "                      framework_version='1.4.0')\n",
    "bert_end = torch_model.deploy(instance_type='ml.g4dn.4xlarge', initial_instance_count=1, \n",
    "                          endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our endpoint has been deployed, let's send it some requests! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who loves Steve? : Danielle\n",
      "CPU times: user 61.3 ms, sys: 0 ns, total: 61.3 ms\n",
      "Wall time: 121 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "context='Danielle is a girl who really loves her cat, Steve. Steve is a large cat with a very furry belly. He gets very excited by the prospect of eating chicken covered in gravy.'\n",
    "question='who loves Steve?'  # 'What kind of food does Steve like?'\n",
    "\n",
    "pass_in_data = {'context':context, 'question':question}\n",
    "json_data = json.dumps(pass_in_data)\n",
    "\n",
    "\n",
    "if model_data:\n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                           ContentType='application/json',\n",
    "                                           Body=json_data)\n",
    "    response = eval(response['Body'].read().decode('utf-8'))\n",
    "    doc_tokens = context.split()\n",
    "    tokenizer = BertTokenizer('vocab', do_lower_case=True, max_len=512)\n",
    "    query_tokens = tokenizer.tokenize(question)\n",
    "    feature = preprocess_tokenized_text(doc_tokens, \n",
    "                                        query_tokens, \n",
    "                                        tokenizer, \n",
    "                                        max_seq_length=384, \n",
    "                                        max_query_length=64)\n",
    "    tensors_for_inference, tokens_for_postprocessing = feature\n",
    "    response = get_predictions(doc_tokens, tokens_for_postprocessing, \n",
    "                             response[0], response[1], n_best_size=1, \n",
    "                             max_answer_length=64, do_lower_case=True, \n",
    "                             can_give_negative_answer=True, \n",
    "                             null_score_diff_threshold=-11.0)\n",
    "\n",
    "#response = bert_end.predict(json.dumps(pass_in_data), initial_args={'ContentType':'application/json'}) \n",
    "\n",
    "# print result\n",
    "print(f'{question} : {response[0][\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm bert_base.pt\n",
    "!rm s3_bucket.txt\n",
    "bert_end.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
