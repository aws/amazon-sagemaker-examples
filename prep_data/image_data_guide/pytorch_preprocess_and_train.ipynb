{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, Structure, and Preprocess Image Data for PyTorch Models\n",
    "\n",
    "**Notes**: \n",
    "* This notebook should be used with the conda_pytorch_latest_p36 kernel\n",
    "* You can also explore image preprocessing with TensorFlow and SageMaker Built-in Algorithms by running [Download, Structure, and Preprocess Image Data for TensorFlow Models](tensorflow_preprocess_and_train.ipynb) and [Download, Structure, and Preprocess Image Data for SageMaker Built-In Algorithms](builtin_preprocess_and_train.ipynb), respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of this notebook is to demonstrate how you can preprocess image data to train PyTorch Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Part 1: Download the Dataset](#Part-1:-Download-the-Dataset)\n",
    "1. [Part 2: Structure the Dataset](#Part-2:-Structure-the-Dataset)\n",
    "1. [Part 3: Preprocess Images for PyTorch Models](#Part-3:-Preprocess-Images-for-PyTorch-Models)\n",
    "1. [Part 4: Train the PyTorch Model](#Part-4:-Train-the-PyTorch-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Download the Dataset\n",
    "----\n",
    "----\n",
    "In this section, you will use a dataset manifest to download animal images from the COCO dataset for all ten animal classes. You will then download frog images from the CIFAR dataset and add them to your COCO animal images. In order to simulate coming to SageMaker with your own dataset, we will keep the data in an unstructured form until the next notebook where you will learn the best practices for structuring an image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import shutil\n",
    "import urllib\n",
    "import pathlib\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread, imwrite\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from sagemaker.pytorch import PyTorch  # PyTorch Estimator for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The COCO and CIFAR Datasets\n",
    "___\n",
    "For this series of notebooks we will be sampling images from the [COCO dataset](https://cocodataset.org) and [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) (before beginning the notebooks in this series, it's a good idea to browse each dataset website to familiaraize youreself with the data). Both are datasets of images, but come formatted very differently. The COCO dataset contains images from Flickr that represent a real-world dataset which isn't formatted or resized specifically for deep learning. This makes it a good dataset for this guide because we want it to be as comprehensive as possible. The CIFAR-10 images, on the other hand, are preprocessed specifically for deep learning as they come cropped, resized and vectorized (i.e. not in a readable image format). This notebooks will show you how to work with both types of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the annotations\n",
    "____\n",
    "The dataset annotation file contains info on each image in the dataset such as the class, superclass, file name and url to download the file. Just the annotations for the COCO dataset are about 242MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "urllib.request.urlretrieve(anno_url, \"coco-annotations.zip\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.unpack_archive(\"coco-annotations.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the annotations into Python\n",
    "The training and validation annotations come in separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"annotations/instances_train2017.json\", \"r\") as f:\n",
    "    train_metadata = json.load(f)\n",
    "\n",
    "with open(\"annotations/instances_val2017.json\", \"r\") as f:\n",
    "    val_metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract only the animal annotations\n",
    "___\n",
    "To limit the scope of the dataset for this guide we're only using the images of animals in the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = {\n",
    "    c[\"id\"]: c[\"name\"] for c in train_metadata[\"categories\"] if c[\"supercategory\"] == \"animal\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract metadata and image filepaths\n",
    "For the train and validation sets, the data we need for the image labels and the filepaths are under different headings in the annotations. We have to extract each out and combine them into a single annotation in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annos = {}\n",
    "for a in train_metadata[\"annotations\"]:\n",
    "    if a[\"category_id\"] in category_labels:\n",
    "        train_annos[a[\"image_id\"]] = {\"category_id\": a[\"category_id\"]}\n",
    "\n",
    "train_images = {}\n",
    "for i in train_metadata[\"images\"]:\n",
    "    train_images[i[\"id\"]] = {\"coco_url\": i[\"coco_url\"], \"file_name\": i[\"file_name\"]}\n",
    "\n",
    "val_annos = {}\n",
    "for a in val_metadata[\"annotations\"]:\n",
    "    if a[\"category_id\"] in category_labels:\n",
    "        val_annos[a[\"image_id\"]] = {\"category_id\": a[\"category_id\"]}\n",
    "\n",
    "val_images = {}\n",
    "for i in val_metadata[\"images\"]:\n",
    "    val_images[i[\"id\"]] = {\"coco_url\": i[\"coco_url\"], \"file_name\": i[\"file_name\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine label and filepath info\n",
    "Later in this series of guides we'll make our own train, validation and test splits. For this reason we'll combine the training and validation datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, anno in train_annos.items():\n",
    "    anno.update(train_images[id])\n",
    "\n",
    "for id, anno in val_annos.items():\n",
    "    anno.update(val_images[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_annos = {}\n",
    "for k, v in train_annos.items():\n",
    "    all_annos.update({k: v})\n",
    "for k, v in val_annos.items():\n",
    "    all_annos.update({k: v})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the dataset\n",
    "___\n",
    "In order to make working with the data easier, we'll select 250 images from each class at random. To make sure you get the same set of cell images for each run of this we'll also set Numpy's random seed to 0. This is a small fraction of the dataset, but it demonstrates how using transfer learning can give you good results without needing very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_annos = {}\n",
    "\n",
    "for category_id in category_labels:\n",
    "    subset = [k for k, v in all_annos.items() if v[\"category_id\"] == category_id]\n",
    "    sample = np.random.choice(subset, size=250, replace=False)\n",
    "    for k in sample:\n",
    "        sample_annos[k] = all_annos[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a download function\n",
    "In order to parallelize downloading the images we must wrap the download and save process with a function for multi-threading with joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, path):\n",
    "    data = imread(url)\n",
    "    imwrite(path / url.split(\"/\")[-1], data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the sample of the dataset (2,500 images, ~5min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = pathlib.Path(\"data_sample_2500\")\n",
    "sample_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_backend(\"threading\", n_jobs=5):\n",
    "    Parallel(verbose=3)(\n",
    "        delayed(download_image)(a[\"coco_url\"], sample_dir) for a in sample_annos.values()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with CIFAR-10 frog data\n",
    "___\n",
    "The COCO dataset doesn't include any images of frogs, but let's say our model must also be able to label images of frogs. To fix this we can download another dataset of images which includes frogs, sample 250 frog images and add them to our existing image data. These images are much smaller (32x32) so they will appear pixelated and blurry when we increase the size of them to (244x244). We'll use the CIFAR-10 dataset to achieve this. As you'll see the CIFAR-10 dataset comes formatted in a very different manner from COCO dataset. We must process the CIFAR-10 data into individual image files so that it's congruent to our COCO images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and extract the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cs.toronto.edu/%7Ekriz/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tarfile.open(\"cifar-10-python.tar.gz\")\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open first batch of CIFAR-10 dataset\n",
    "The CIFAR-10 dataset comes in five training batches and one test batch. Each training batch has 10,000 randomly ordered images. Since we only need 250 frog images for our dataset, just pulling from the first batch will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./cifar-10-batches-py/data_batch_1\", \"rb\") as f:\n",
    "    batch_1 = pickle.load(f, encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = batch_1[b\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull 250 sample frog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frog_indices = np.array(batch_1[b\"labels\"]) == 6\n",
    "sample_frog_indices = np.random.choice(frog_indices.nonzero()[0], size=250, replace=False)\n",
    "sample_data = image_data[sample_frog_indices, :]\n",
    "frog_images = sample_data.reshape(len(sample_data), 3, 32, 32).transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View frog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4, figsize=(10, 7))\n",
    "indices = np.random.randint(low=0, high=249, size=12)\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(frog_images[indices[i]])\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write sample frog images to `data_sample_2500` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frog_filenames = np.array(batch_1[b\"filenames\"])[sample_frog_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, filename in enumerate(frog_filenames):\n",
    "    filename = filename.decode()\n",
    "    data = frog_images[idx]\n",
    "    if filename.endswith(\".png\"):\n",
    "        filename = filename.replace(\".png\", \".jpg\")\n",
    "    imwrite(sample_dir / filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir.rename(\"data_sample_2750\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add frog annotations to `sample_annos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels[26] = \"frog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_anno_idx = np.array(list(sample_annos.keys())).max() + 1\n",
    "\n",
    "frog_anno_ids = range(next_anno_idx, next_anno_idx + len(frog_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, frog_id in enumerate(frog_anno_ids):\n",
    "    sample_annos[frog_id] = {\n",
    "        \"category_id\": 26,\n",
    "        \"file_name\": frog_filenames[idx].decode().replace(\".png\", \".jpg\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Structure the Dataset\n",
    "----\n",
    "----\n",
    "\n",
    "In this section, you will properly structure your image files for ingestion by the model. Then, we will use Python to create the new folder structure and copy the files into the correct set and label folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper folder structure\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although most tools can accommodate data in any file structure with enough tinkering, it makes most sense to use the sensible defaults that frameworks like MXNet, TensorFlow and PyTorch all share to make data ingestion as smooth as possible. By default, most tools will look for image data in the file structure depicted below:\n",
    "```\n",
    "+-- train\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|\n",
    "+-- val\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|\n",
    "+-- test\n",
    "|   +-- class_A\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|   +-- class_B\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "|       +-- filename.jpg\n",
    "```\n",
    "You will notice that the COCO dataset does not come structured like above so we must use the annotation data to help restructure the folders of the COCO dataset so they match the pattern above. Once the new directory structures are created you can use your desired framework's data loading tool to gracefully load and define transformation for your image data. Many datasets may already be in this structure in which case you can skip this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train, validation and test splits\n",
    "___\n",
    "We should divide our data into train, validation and test splits. A typical split ratio is 80/10/10. Our image classification algorithm will train on the first 80% (training) and evaluate its performance at each epoch with the next 10% (validation) and we'll give our model's final accuracy results using the last 10% (test). It's important that before we split the data we make sure to shuffle it randomly so that class distribution among splits is roughly proportional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "image_ids = sorted(list(sample_annos.keys()))\n",
    "np.random.shuffle(image_ids)\n",
    "first_80 = int(len(image_ids) * 0.8)\n",
    "next_10 = int(len(image_ids) * 0.9)\n",
    "train_ids, val_ids, test_ids = np.split(image_ids, [first_80, next_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new folder structure and copy image files\n",
    "___\n",
    "This new folder structure can then be read by data loaders for SageMaker's built-in algorithms, TensorFlow or PyTorch for easy loading of the image data into your framework of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstruct_dir = Path(\"data_sample_2750\")\n",
    "struct_dir = Path(\"data_structured\")\n",
    "struct_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for name, split in zip([\"train\", \"val\", \"test\"], [train_ids, val_ids, test_ids]):\n",
    "    split_dir = struct_dir / name\n",
    "    split_dir.mkdir(exist_ok=True)\n",
    "    for image_id in tqdm(split):\n",
    "        category_dir = split_dir / f'{category_labels[sample_annos[image_id][\"category_id\"]]}'\n",
    "        category_dir.mkdir(exist_ok=True)\n",
    "        source_path = (unstruct_dir / sample_annos[image_id][\"file_name\"]).as_posix()\n",
    "        target_path = (category_dir / sample_annos[image_id][\"file_name\"]).as_posix()\n",
    "        shutil.copy(source_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Preprocess Images for PyTorch Models\n",
    "----\n",
    "----\n",
    "\n",
    "In this section, you will create resizing and data augmentation transforms for training with PyTorch. You will also upload your dataset to S3 for training with SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "___\n",
    "For this guide we'll use the SageMaker Python SDK version 2.9.2. By default, SageMaker Notebooks come with version 1.72.0. Other guides provided by Amazon may be set up to work with other versions of the Python SDK so you may wish to roll-back to 1.72.0. We will also be using PyTorch 1.6.0 which can also be rolled back at the end of this guide to 1.4.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the SageMaker Python SDK and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "original_sagemaker_version = !conda list | grep -E \"sagemaker\\s\" | awk '{print $2}'\n",
    "original_pytorch_version = !conda list | grep -E \"torch\\s\" | awk '{print $2}'\n",
    "!{sys.executable} -m pip install -q \"sagemaker==2.9.2\" \"torch==1.6.0\" \"torchvision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import boto3\n",
    "import torch\n",
    "import shutil\n",
    "import pickle\n",
    "import pathlib\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker updated  {original_sagemaker_version[0]} -> {sagemaker.__version__}\")\n",
    "print(f\"pytorch   updated  {original_pytorch_version[0]} -> {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Define the Resize and Augmentation Transformations\n",
    "___\n",
    "\n",
    "#### Resize\n",
    "Before going to the GPU for training, all image data must have the same dimensions for length, width and channel. Typically, algorithms use a square format so the length and width are the same and many pre-made datasets areadly have the images nicely cropped into squares. However, most real-world datasets will begin with images in many different dimensions and ratios. In order to prep our dataset for training we will need to resize and crop the images if they aren't already square. \n",
    "\n",
    "This transformation is deceptivley simple. If we want to keep the images from looking squished or stretched, we need to crop it to a square *and* we want to make sure the important object in the image doesn't get cropped out. Unfortunately, there is no easy way to make sure each crop is optimal so we typically choose a center crop which works well most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = tv.transforms.Compose(\n",
    "    [tv.transforms.Resize(224), tv.transforms.CenterCrop(224), tv.transforms.ToTensor()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tv.datasets.ImageFolder(root=\"data_structured/train\", transform=tv.transforms.ToTensor())\n",
    "\n",
    "sample_resized = tv.datasets.ImageFolder(root=\"data_structured/train\", transform=resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = iter(sample)\n",
    "sample_resized = iter(sample_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-rull the cell below to sample another image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "image = next(iter(sample))[0]\n",
    "image_resized = next(iter(sample_resized))[0]\n",
    "\n",
    "ax[0].imshow(image.permute(1, 2, 0))\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_title(f\"Before - {tuple(image.shape)}\")\n",
    "ax[1].imshow(image_resized.permute(1, 2, 0))\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set_title(f\"After - {tuple(image_resized.shape)}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation\n",
    "An easy way to improve trainging is to randomly augment the images to help our training algorithm generalize better. Threre are many augmentations to choose from, but keep in mind that the more we add to our augment function, the more processing will be required before we can send the image to the GPU for training. Also, it's important to note that we don't need to augment the validation data because we want to generate a prediction on the image as it normally would be presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = tv.transforms.Compose(\n",
    "    [\n",
    "        tv.transforms.RandomResizedCrop(224),\n",
    "        tv.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        tv.transforms.RandomVerticalFlip(p=0.5),\n",
    "        tv.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        tv.transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tv.datasets.ImageFolder(root=\"data_structured/train\", transform=tv.transforms.ToTensor())\n",
    "\n",
    "sample_augmented = tv.datasets.ImageFolder(root=\"data_structured/train\", transform=augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = iter(sample)\n",
    "sample_augmented = iter(sample_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-rull the cell below to sample another image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "image = next(iter(sample))[0]\n",
    "image_augmented = next(iter(sample_augmented))[0]\n",
    "\n",
    "ax[0].imshow(image.permute(1, 2, 0))\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_title(f\"Before - {tuple(image.shape)}\")\n",
    "ax[1].imshow(image_augmented.permute(1, 2, 0))\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set_title(f\"After - {tuple(image_augmented.shape)}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on applying the transformations\n",
    "\n",
    "The training data set will get the resize and augment functions applied to it, but the validation dataset only gets resized because it's not directly used for training. We will apply the transforms by passing them to the corresponding dataset with the `transform` kwarg. However, it doesn't actually transform the image yet. Rather, the transformation will be fully applied by the CPU right before it gets sent to the GPU for training. This is nice beause we can experiment quickly without having to wait for all the images to be transformed.\n",
    "\n",
    "You may be wondering why we're applying the transformations randomly. This is done because our training algorithm will cycle through the data in epochs. Each epoch it will get a chance to view the image again so instead of sending the same image through each time, we'll apply a random augmentation. Ideally, we'd let the algorithm see all versions of the image each epoch, but this would scale the size of the training dataset by the number of augmentations. Scaling the data storage and training time by that factor isn't worth the relatively minor changes introduced into the dataset.\n",
    "\n",
    "More documentation on all the transforms supported directly by Torchvision is available [here](https://pytorch.org/docs/stable/torchvision/transforms.html#transforms-on-pil-image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": tv.transforms.Compose(\n",
    "        [\n",
    "            tv.transforms.RandomResizedCrop(224),\n",
    "            tv.transforms.RandomHorizontalFlip(p=0.5),\n",
    "            tv.transforms.RandomVerticalFlip(p=0.5),\n",
    "            tv.transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            tv.transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": tv.transforms.Compose(\n",
    "        [tv.transforms.Resize(224), tv.transforms.CenterCrop(224), tv.transforms.ToTensor()]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the PyTorch datasets and dataloaders\n",
    "____\n",
    "\n",
    "#### Datasets\n",
    "Datasets in PyTorch keep track of all the data in your dataset--where to find them (their path), what class they belong to and what transformations they get. In this case, we'll use PyTorch's handy `ImageFolder` to easily generate the dataset from the directory structure created in the previous guide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(\"./data_structured\")\n",
    "splits = [\"train\", \"val\"]\n",
    "\n",
    "datasets = {}\n",
    "for s in splits:\n",
    "    datasets[s] = tv.datasets.ImageFolder(root=data_dir / s, transform=data_transforms[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloaders\n",
    "Dataloaders structure how the images get sent to the CPU and GPU for training. Thye include important hyper-parameters such as:\n",
    "* **batch_size**: this tells the data loader how many images to send to the training algorithm at once for back propogagation. It will therefore also control the number to gradient updates which occur in one epoch for optimizers like SGD.\n",
    "* **shuffle**: this will randomize the orders of your training data\n",
    "* **num_workers**: this defines how many parallel processes you want to load and transform images before being sent to the GPU for training. Adding more workers will therefore speed up training. However, too many workers will slow training down due to the overhead of trying manage all the workers. Also, each worker will consume a considerable amount of RAM (depending on batch_size) and you cannot have more workers than cpu cores available on the EC2 instance used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "\n",
    "dataloaders = {}\n",
    "for s in splits:\n",
    "    dataloaders[s] = torch.utils.data.DataLoader(\n",
    "        datasets[s], batch_size=batch_size, shuffle=shuffle, num_workers=num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the transforms\n",
    "___\n",
    "Just to make sure everything is working we can apply some transformations on a few images and view them to make sure thye outout looks good. Simply re-run the cell to see a fresh batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = batch_size\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(10, 7))\n",
    "\n",
    "for row in range(rows):\n",
    "    batch = next(iter(dataloaders[\"train\"]))\n",
    "    images, labels = batch\n",
    "    for col, image in enumerate(images):\n",
    "        ax = axs[row, col]\n",
    "        ax.imshow(image.permute(2, 1, 0))\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your datasets and dataloaders defined, you're now ready to define the training architecture for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data to S3\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resize images and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(\"./data_structured\")\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "datasets = {}\n",
    "for s in splits:\n",
    "    datasets[s] = tv.datasets.ImageFolder(\n",
    "        root=data_dir / s,\n",
    "        transform=tv.transforms.Compose([tv.transforms.Resize(224), tv.transforms.ToTensor()]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_path = pathlib.Path(\"./data_resized\")\n",
    "resized_path.mkdir(exist_ok=True)\n",
    "for s in splits:\n",
    "    split_path = resized_path / s\n",
    "    split_path.mkdir(exist_ok=True)\n",
    "    for idx, (img_tensor, label) in enumerate(tqdm(datasets[s])):\n",
    "        label_path = split_path / f\"{label:02}\"\n",
    "        label_path.mkdir(exist_ok=True)\n",
    "        filename = datasets[s].imgs[idx][0].split(\"/\")[-1]\n",
    "        tv.utils.save_image(img_tensor, label_path / filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload augmented images to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "prefix = \"DEMO-sm-preprocess-train-image-data-pytorch-algo\"\n",
    "s3 = boto3.resource(\"s3\")\n",
    "region = sagemaker.Session().boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload data to S3 (~3min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uploader = sagemaker.s3.S3Uploader()\n",
    "\n",
    "for s in splits:\n",
    "    data_s3_uri = s3_uploader.upload(\n",
    "        local_path=(resized_path / s).as_posix(),\n",
    "        desired_s3_uri=f\"s3://{bucket_name}/{prefix}/data/{s}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the PyTorch Model\n",
    "----\n",
    "----\n",
    "In this section, you will use the SageMaker SDK to create a PyTorch Estimator and train it on a remote EC2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='idg4c.2'></a>\n",
    "### Algorithm hyperparameters\n",
    "___\n",
    "Hyperparamters represent the tuning knobs for our algorithm which we set before training begins. Typically they are pre-set to defaults so if we don't specify them we can still run the training algorithm, but they usually need tweaking to get optimal results. What these values should be depend entirely on the dataset. Unfortunately, there's no formula to tell us what the best settings are, we just have to try them ourselves and see what we get, but there are best practices and tips to help guide us in choosing them.\n",
    "\n",
    "* **Optimizer** - The optimizer refers to the optimization algorithm being used to choose the best weights. For deep learning on image data, SGD or ADAM is typically used.\n",
    "\n",
    "* **Learning Rate** - After each batch of training we update the model's weights to give us the best possible results for that batch. The learning rate controls by how much we should update the weights. Best practices dictate a value between 0.2 and .001, typically never going higher than 1. The higher the learning rate, the faster your training will converge to the optimal weights, but going too fast can lead you to overshoot the target. In this example, we're using the weights from a pre-trained model so we'd want to start with a lower learning rate because the weights have already been optimized and we don't want move too far away from them.\n",
    "\n",
    "*  **Epochs** - An epoch refers to one cycle through the training set and having more epochs to train means having more oppotunities to improve accracy. Suitable values range from 5 to 25 epochs depending on your time and budget constraints. Ideally, the right number of epochs is right before your validation accuracy plateaus.\n",
    "\n",
    "* **Batch Size** - Training on batches reduces the amount of data you need to hold in RAM and can speed up the training algorithm. For these reasons the training data is nearly always batched. The optimal batch size will depended on the dataset, how large the images are and how much RAM the training computer has. For a dataset like ours reasonable vaules would be bewteen 8 and 64 images per batch.\n",
    "\n",
    "* **Criterion** - This is the type of loss function that will be used by the optimizer to update the model's weights during training. For training on a dataset with with more than two classes, the most common loss function is Cross-Entropy Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the training script\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training function\n",
    "Unlike other frameworks, PyTorch doesn't use model objects with a `.fit()` method to train them. Instead the user must define their own training function. This adds more code to our training script, but offers more transparency for customizing and debugging the model training. This is one major reasaon why researchers enjoy using PyTorch. In this example we use the training fuction defined in the PyTorch tutorial for transfer learning here: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_pytorch/pytorch_train.py\" | sed -n 12,78p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution safety\n",
    "For safety we wrap the training code in this standard if statement though it is not strictly required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_pytorch/pytorch_train.py\" | sed -n 81p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse argument variables\n",
    "These argument variables are passed via the hyperparameter argument for the estimator configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_pytorch/pytorch_train.py\" | sed -n 83,90p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data transformations and load data\n",
    "These are the transformations from the pre-processing guide. Since the data was resized before it was saved to S3, we don't need to do any resizing except for random cropping of the training dataset and center cropping the valications dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"training_pytorch/pytorch_train.py\" | sed -n 92,127p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect device and create and modify the base model\n",
    "The base model for this guide is a RestNet18 model using pre-trained weights. We need to modify the base model by replacing the fully connected layer with a dense layer to classify our animal images. The model is then loaded for the device (GPU or CPU) that our EC@ instance is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"pytorch_train/pytorch_train-revised.py\" | sed -n 128,140p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss criterion, optimization algorithm and train the model\n",
    "The weights for the epoch with the best accuracy are saved so we can load the model after training and make predictions on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize \"pytorch_train/pytorch_train-revised.py\" | sed -n 142,150p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator configuration\n",
    "___\n",
    "\n",
    "These define the the resources to use for training and how they are configured. Here are some important one to single out:\n",
    "\n",
    "* **entry_point (str)** – Path (absolute or relative) to the Python source file which should be executed as the entry point to training. If source_dir is specified, then entry_point must point to a file located at the root of source_dir.\n",
    "\n",
    "* **framework_version (str)** – PyTorch version you want to use for executing your model training code. Defaults to None. Required unless image_uri is provided. List of supported versions: https://github.com/aws/sagemaker-python-sdk#pytorch-sagemaker-estimators.\n",
    "\n",
    "* **py_version (str)** – Python version you want to use for executing your model training code. One of ‘py2’ or ‘py3’. Defaults to None. Required unless image_uri is provided.\n",
    "\n",
    "* **source_dir (str)** – Path (absolute, relative or an S3 URI) to a directory with any other training source code dependencies aside from the entry point file (default: None). If source_dir is an S3 URI, it must point to a tar.gz file. Structure within this directory are preserved when training on Amazon SageMaker.\n",
    "\n",
    "* **dependencies (list[str])** – A list of paths to directories (absolute or relative) with any additional libraries that will be exported to the container (default: []). The library folders will be copied to SageMaker in the same folder where the entrypoint is copied. If ‘git_config’ is provided, ‘dependencies’ should be a list of relative locations to directories with any additional libraries needed in the Git repo.\n",
    "\n",
    "* **git_config (dict[str, str])** – Git configurations used for cloning files, including repo, branch, commit, 2FA_enabled, username, password and token. The repo field is required. All other fields are optional. repo specifies the Git repository where your training script is stored. If you don’t provide branch, the default value ‘master’ is used. If you don’t provide commit, the latest commit in the specified branch is used.\n",
    "\n",
    "* **role (str)** – An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs that create Amazon SageMaker endpoints use this role to access training data and model artifacts. After the endpoint is created, the inference code might use the IAM role, if it needs to access an AWS resource.\n",
    "\n",
    "* **instance_count (int)** – Number of Amazon EC2 instances to use for training.\n",
    "\n",
    "* **instance_type (str)** – Type of EC2 instance to use for training, for example, ‘ml.c4.xlarge’.\n",
    "\n",
    "* **volume_size (int)** – Size in GB of the EBS volume to use for storing input data during training (default: 30). Must be large enough to store training data if File Mode is used (which is the default).\n",
    "\n",
    "* **model_uri (str)** – URI where a pre-trained model is stored, either locally or in S3 (default: None). If specified, the estimator will create a channel pointing to the model so the training job can download it. This model can be a ‘model.tar.gz’ from a previous training job, or other artifacts coming from a different source. In local mode, this should point to the path in which the model is located and not the file itself, as local Docker containers will try to mount the URI as a volume.\n",
    "\n",
    "* **output_path (str)** - S3 location for saving the training result (model artifacts and output files). If not specified, results are stored to a default bucket. If the bucket with the specific name does not exist, the estimator creates the bucket during the fit() method execution. file:// urls are used for local mode. For example: ‘file://model/’ will save to the model folder in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on an EC2 instance\n",
    "___\n",
    "Now that we've worked out any bugs in our trainging script we can send the training job to an EC2 instance with a GPU with a larger batch size, number of workers and number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the hyperparameters for EC2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"epochs\": 10, \"batch-size\": 64, \"learning-rate\": 0.001, \"workers\": 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the estimator configuration for EC2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_config = {\n",
    "    \"entry_point\": \"pytorch_train.py\",\n",
    "    \"source_dir\": \"training_pytorch\",\n",
    "    \"framework_version\": \"1.6.0\",\n",
    "    \"py_version\": \"py3\",\n",
    "    \"instance_type\": \"ml.p3.2xlarge\",\n",
    "    \"instance_count\": 1,\n",
    "    \"role\": sagemaker.get_execution_role(),\n",
    "    \"output_path\": f\"s3://{bucket_name}/{prefix}\",\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the estimator configured for EC2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(**estimator_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the data channels using the proper S3 URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    \"train\": f\"s3://{bucket_name}/{prefix}/data/train\",\n",
    "    \"val\": f\"s3://{bucket_name}/{prefix}/data/val\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator.fit(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Trained Model and Predict\n",
    "___\n",
    "After training the model and saving its parameters (weights) to S3, we can retrive the parameters and load them back into PyTorch to generate predicions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the trained weights from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.s3.S3Downloader().download(pytorch_estimator.model_data, \"training_pytorch\")\n",
    "tf = tarfile.open(\"training_pytorch/model.tar.gz\")\n",
    "tf.extractall(\"training_pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the weights back into a PyTorch model\n",
    "Since the model was trained on a GPU we need to use the `map_location=torch.device('cpu')` kwarg to load the model on a CPU backed notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tv.models.resnet18()\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 11)\n",
    "model.load_state_dict(torch.load(\"training_pytorch/model.pt\", map_location=torch.device(\"cpu\")))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link the model predictions (0 to 10) back to original class names (bear to zebra)\n",
    "To map the index number back to the category label, we need to use the category labels created in the first guide of this series (Downloading Data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = {idx: name for idx, name in enumerate(sorted(category_labels.values()))}\n",
    "category_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load validation images for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = sample = tv.datasets.ImageFolder(\n",
    "    root=\"data_resized/test\",\n",
    "    transform=tv.transforms.Compose([tv.transforms.CenterCrop(244), tv.transforms.ToTensor()]),\n",
    ")\n",
    "\n",
    "test_ds = torch.utils.data.DataLoader(test_ds, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show validation images with model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 4\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(10, 7))\n",
    "\n",
    "for row in range(rows):\n",
    "    batch = next(iter(test_ds))\n",
    "    images, labels = batch\n",
    "    _, preds = torch.max(model(images), 1)\n",
    "    preds = preds.numpy()\n",
    "    for col, image in enumerate(images):\n",
    "        ax = axs[row, col]\n",
    "        ax.imshow(image.permute(1, 2, 0))\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"predicted: {category_labels[preds[col]]}\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
