{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wrapped-smith",
   "metadata": {},
   "source": [
    "# Compile and Deploy a MXNet model on Inf1 instances\n",
    "\n",
    "\n",
    "Amazon SageMaker now supports Inf1 instances for high performance and cost-effective inferences. Inf1 instances are ideal for large scale machine learning inference applications like image recognition, speech recognition, natural language processing, personalization, and fraud detection. In this example, we train a classification model on the MNIST dataset using MXNet, compile it using Amazon SageMaker Neo, and deploy the model on Inf1 instances on a SageMaker endpoint and use the Neo Deep Learning Runtime to make inferences in real-time and with low latency. \n",
    "\n",
    "### Inf 1 instances \n",
    "Inf1 instances are built from the ground up to support machine learning inference applications and feature up to 16 AWS Inferentia chips, high-performance machine learning inference chips designed and built by AWS. The Inferentia chips are coupled with the latest custom 2nd generation Intel® Xeon® Scalable processors and up to 100 Gbps networking to enable high throughput inference. With 1 to 16 AWS Inferentia chips per instance, Inf1 instances can scale in performance to up to 2000 Tera Operations per Second (TOPS) and deliver extremely low latency for real-time inference applications. The large on-chip memory on AWS Inferentia chips used in Inf1 instances allows caching of machine learning models directly on the chip. This eliminates the need to access outside memory resources during inference, enabling low latency without impacting bandwidth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-reset",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "We need to first upgrade the SageMaker SDK for Python to v2.33.0 or greater & restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/anaconda3/envs/mxnet_p36/bin/pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = Session().default_bucket()\n",
    "\n",
    "# Location to save your custom code in tar.gz format.\n",
    "custom_code_upload_location = \"s3://{}/customcode/mxnet\".format(bucket)\n",
    "\n",
    "# Location where results of model training are saved.\n",
    "model_artifacts_location = \"s3://{}/artifacts\".format(bucket)\n",
    "\n",
    "# IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "# We can use the SageMaker Python SDK to get the role from our notebook environment.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-petite",
   "metadata": {},
   "source": [
    "# Construct a script for training and hosting\n",
    "The mnist.py script provides all the code we need for training and hosting a SageMaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-vampire",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.MXNet estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "mnist_estimator = MXNet(\n",
    "    entry_point=\"mnist.py\",\n",
    "    role=role,\n",
    "    output_path=model_artifacts_location,\n",
    "    code_location=custom_code_upload_location,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    framework_version=\"1.8\",\n",
    "    py_version=\"py37\",\n",
    "    hyperparameters={\"learning-rate\": 0.1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-guitar",
   "metadata": {},
   "source": [
    "The **```fit```** method will create a training job in a **ml.m4.xlarge** instances. The logs below will show the instances doing training, evaluation, and incrementing the number of **training steps**. \n",
    "\n",
    "In the end of the training, the training job will generate a saved model for compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "train_data_location = \"s3://sagemaker-sample-data-{}/mxnet/mnist/train\".format(region)\n",
    "test_data_location = \"s3://sagemaker-sample-data-{}/mxnet/mnist/test\".format(region)\n",
    "\n",
    "mnist_estimator.fit({\"train\": train_data_location, \"test\": test_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-empire",
   "metadata": {},
   "source": [
    "# Deploy the trained model on Inf1 instance for real-time inferences\n",
    "\n",
    "Once the training is complete, we compile the model using Amazon SageMaker Neo to optize performance for our desired deployment target. Amazon SageMaker Neo enables you to train machine learning models once and run them anywhere in the cloud and at the edge. To compile our trained model for deploying on Inf1 instances, we are using the  ``MXNetEstimator.compile_model`` method and select ``'ml_inf1'`` as our deployment target. The compiled model will then be deployed on an endpoint using Inf1 instances in Amazon SageMaker. \n",
    "\n",
    "## Compile the model \n",
    "\n",
    "The ``input_shape`` is the definition for the model's input tensor and ``output_path`` is where the compiled model will be stored in S3. **Important. If the following command result in a permission error, scroll up and locate the value of execution role returned by `get_execution_role()`. The role must have access to the S3 bucket specified in ``output_path``.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/\".join(mnist_estimator.output_path.split(\"/\")[:-1])\n",
    "mnist_estimator.framework_version = \"1.5.1\"\n",
    "\n",
    "optimized_estimator = mnist_estimator.compile_model(\n",
    "    target_instance_family=\"ml_inf1\",\n",
    "    input_shape={\"data\": [1, 1, 28, 28]},\n",
    "    role=role,\n",
    "    framework=\"mxnet\",\n",
    "    framework_version=\"1.5.1\",\n",
    "    output_path=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-ethnic",
   "metadata": {},
   "source": [
    "## Deploy the compiled model on a SageMaker endpoint\n",
    "\n",
    "Now that we have the compiled model, we will deploy it on an Amazon SageMaker endpoint. Inf1 instances in Amazon SageMaker are available in four sizes: ml.inf1.xlarge, ml.inf1.2xlarge, ml.inf1.6xlarge, and ml.inf1.24xlarge. In this example, we are using ``'ml.inf1.xlarge'`` for deploying our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import NumpySerializer\n",
    "\n",
    "npy_serializer = NumpySerializer()\n",
    "optimized_predictor = optimized_estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.inf1.xlarge\", serializer=npy_serializer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-latin",
   "metadata": {},
   "source": [
    "## Invoking the endpoint\n",
    "\n",
    "Once the endpoint is ready, you can send requests to it and receive inference results in real-time with low latency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_ndarray = np.load(\"input.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = optimized_predictor.predict(data=numpy_ndarray)\n",
    "print(\"Raw prediction result:\")\n",
    "print(response)\n",
    "\n",
    "labeled_predictions = list(zip(range(10), response))\n",
    "print(\"Labeled predictions: \")\n",
    "print(labeled_predictions)\n",
    "\n",
    "labeled_predictions.sort(key=lambda label_and_prob: 1.0 - label_and_prob[1])\n",
    "print(\"Most likely answer: {}\".format(labeled_predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-charles",
   "metadata": {},
   "source": [
    "## Deleting endpoint (optional)\n",
    "\n",
    "Delete the endpoint if you no longer need it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Endpoint name: \" + optimized_predictor.endpoint_name)\n",
    "optimized_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
