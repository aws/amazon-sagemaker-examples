{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Pre-trained Faster-Whisper-Large-v3 Model on SageMaker with Multi-Model Endpoint and Triton Serve as custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this notebook, where we will explore the process of deploying a pre-trained Faster-Whisper-Large-v3 model from Hugging Face on Amazon SageMaker. This deployment will be facilitated through the use of a Multi-Model Endpoint, allowing us to serve multiple models on a single SageMaker instance efficiently.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this demonstration, we'll cover the following key aspects:\n",
    "\n",
    "1. **Model Selection:** We will leverage the Hugging Face model hub to choose the Faster-Whisper-Large-v3 model, a powerful pre-trained model for various natural language processing tasks.\n",
    "\n",
    "2. **SageMaker Deployment:** Learn how to deploy the selected model on Amazon SageMaker, a fully managed service that enables the training and deployment of machine learning models at scale.\n",
    "\n",
    "3. **Multi-Model Endpoint:** Explore the advantages of using a Multi-Model Endpoint on SageMaker. This approach allows us to host and serve multiple models on a single instance, optimizing resource utilization and cost efficiency.\n",
    "\n",
    "4. **Triton Serve Integration:** Understand how Triton Serve, an open-source model serving platform, can be employed to dynamically load and unload models from the GPU. This capability is crucial for managing resources effectively and reducing operational costs.\n",
    "\n",
    "## Cost Optimization\n",
    "\n",
    "By utilizing a Multi-Model Endpoint on SageMaker, we aim to showcase how you can achieve significant cost savings. Serving multiple models on a single instance helps maximize the utilization of resources, leading to more efficient and economical deployment.\n",
    "\n",
    "## Note\n",
    "Ensure that you have [git-lfs](https://git-lfs.com/) installed \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs\n",
    "Installs the dependencies required to package the model and run inferences using Triton server.</br>Update SageMaker, boto3, AWS CLI etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU pip awscli boto3 sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up SageMaker Execution Role\n",
    "\n",
    "1. **Create Role:**\n",
    "   - Go to IAM Console.\n",
    "   - Choose \"Roles\" &rarr; \"Create role.\"\n",
    "   - Select \"AWS service\" &rarr; \"SageMaker\"\n",
    "   - Attach necessary policies.\n",
    "   - Name the role: `AmazonSageMaker-ExecutionRole-mme-FasterWishper`.\n",
    "\n",
    "2. **Assign Permissions:**\n",
    "   - Go to SageMaker Console.\n",
    "   - Select your instance or job.\n",
    "   - In settings, choose the created role.\n",
    "   - Save changes.\n",
    "\n",
    "Now, your SageMaker instance/job is set up with the required execution role.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# sagemaker variables\n",
    "region_name = \"eu-west-1\"  # change to your region\n",
    "sm_client = boto3.client(service_name=\"sagemaker\", region_name=\"eu-west-1\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "role = f\"arn:aws:iam::{boto3.client('sts').get_caller_identity().get('Account')}:role/AmazonSageMaker-ExecutionRole-mme-FasterWishper\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "print(\"S3 Bucket: {}\".format(bucket))\n",
    "prefix = \"faster-whisper-large-v3-mme-gpu\"\n",
    "\n",
    "# account mapping for SageMaker MME Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "region = boto3.Session(region_name=region_name).region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:23.05-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate model artifact for faster-whisper-large-v3 from hugging face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will clone the model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/Systran/faster-whisper-large-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to arrange the model as the Triton server expects it\n",
    "\n",
    "Model repository structure for Faster Whisper large v3 Model.\n",
    "\n",
    "```\n",
    "faster-whisper-large-v3\n",
    "├── 1\n",
    "│   └── model.py\n",
    "└── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./faster-whisper-large-v3/1\n",
    "!mv ./faster-whisper-large-v3/*.* ./faster-whisper-large-v3/1/\n",
    "!rm -f ./faster-whisper-large-v3/.gitattributes ./faster-whisper-large-v3/.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile faster-whisper-large-v3/config.pbtxt\n",
    "name: \"faster-whisper-large-v3\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 1\n",
    "\n",
    "input [\n",
    "    {\n",
    "        name: \"audio_array\"\n",
    "        data_type: TYPE_STRING\n",
    "        dims: [1]\n",
    "    },\n",
    "    {\n",
    "        name: \"audio_lang\"\n",
    "        data_type: TYPE_STRING\n",
    "        dims: [1]\n",
    "    }\n",
    "]\n",
    "\n",
    "output [\n",
    "    {\n",
    "        name: \"transcript\"\n",
    "        data_type: TYPE_STRING\n",
    "        dims: [ 1 ]\n",
    "    }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    kind: KIND_GPU\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package models and upload to S3\n",
    "Next, we will package our model as `*.tar.gz` files for uploading to S3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C ./ -czf faster-whisper-large-v3.tar.gz faster-whisper-large-v3\n",
    "model_uri_faster_whisper_large = sagemaker_session.upload_data(\n",
    "    path=\"faster-whisper-large-v3.tar.gz\", key_prefix=prefix\n",
    ")\n",
    "print(\"Model URI: {}\".format(model_uri_faster_whisper_large))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Push docker image\n",
    "Now we will build our docker file that will be based on [SageMaker deep learning images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the docker file\n",
    "In the docker image we will add all our requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, \"w\") as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate Dockerfile\n",
    "FROM {mme_triton_image_uri}\n",
    "\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    ffmpeg \\\n",
    "    libcublas11\n",
    "\n",
    "RUN pip3 install faster-whisper ffmpeg ffmpeg-python \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the docker image and push to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_version = \"0.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t {sagemaker_session.account_id()}.dkr.ecr.{region}.amazonaws.com/triton/faster-whisper:{docker_version} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ecr create-repository --repository-name triton/faster-whisper --region {region}\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {sagemaker_session.account_id()}.dkr.ecr.{region}.amazonaws.com\n",
    "!docker push {sagemaker_session.account_id()}.dkr.ecr.{region}.amazonaws.com/triton/faster-whisper:{docker_version}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint\n",
    "\n",
    "Now that we have uploaded the model artifacts to S3, we can create a SageMaker multi-model endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the serving container\n",
    "In the container definition, define the `ModelDataUrl` to specify the S3 directory that contains all the models that SageMaker multi-model endpoint will use to load and serve predictions. Set `Mode` to `MultiModel` to indicate SageMaker would create the endpoint with MME container specifications. We set the container with an image that supports deploying multi-model endpoints with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_url = f\"s3://{bucket}/{prefix}/\"\n",
    "\n",
    "container = {\n",
    "    \"Image\": f\"{sagemaker_session.account_id()}.dkr.ecr.{region}.amazonaws.com/triton/faster-whisper:{docker_version}\",\n",
    "    \"ModelDataUrl\": model_data_url,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "}\n",
    "print(\"Container: {}\".format(container))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a multi-model object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the image, data location are set we create the model using `create_model` by specifying the `ModelName` and the Container definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "sagemaker_execution_role = role\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=sagemaker_execution_role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define configuration for the multi-model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model above, we create an [endpoint configuration](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html) where we can specify the type and number of instances we want in the endpoint. Here we are deploying to `g4dn.2xlarge` NVIDIA GPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{prefix}-epc-{ts}-2xl\"\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.2xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Multi-Model Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **In-service** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"{prefix}-ep-{ts}-2xl\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the endpoint running we can use some sample raw data to do an inference using JSON as the payload format. For the inference request format, Triton uses the KFServing community standard [inference protocols](https://github.com/triton-inference-server/server/blob/main/docs/protocol/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "\n",
    "\n",
    "def get_payload(wav_file_path, language):\n",
    "    with open(wav_file_path, \"rb\") as f:\n",
    "        wav = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "\n",
    "    payload = {}\n",
    "    payload[\"inputs\"] = []\n",
    "    payload[\"inputs\"].append(\n",
    "        {\n",
    "            \"name\": \"audio_array\",\n",
    "            \"shape\": [1, 1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [wav],\n",
    "        }\n",
    "    )\n",
    "    payload[\"inputs\"].append(\n",
    "        {\n",
    "            \"name\": \"audio_lang\",\n",
    "            \"shape\": [1, 1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [language],\n",
    "        }\n",
    "    )\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = get_payload(\"./audio/sample.wav\", \"fr\")\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"faster-whisper-large-v3.tar.gz\",\n",
    ")\n",
    "\n",
    "response_body = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "print(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_model(ModelName=sm_model_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|nlp|realtime|triton|multi-model|mme-triton-custom-faster-whisper|mme-triton-custom-faster-whisper.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
