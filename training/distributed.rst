Distributed Training
====================

Examples on how to use distributed training on SageMaker.


Apache MXNet
====================================

.. toctree::
   :maxdepth: 1

   ../sagemaker-python-sdk/mxnet_mnist/mxnet_mnist
   ../sagemaker-python-sdk/mxnet_horovod_fasterrcnn/horovod_deployment_notebook
   ../sagemaker-python-sdk/mxnet_horovod_maskrcnn/horovod_deployment_notebook
   ../sagemaker-python-sdk/mxnet_horovod_mnist/mxnet_mnist_horovod


In addition to the notebook, this topic is covered in this workshop topic: `Parallelized data distribution (sharding) <https://sagemaker-workshop.com/builtin/parallelized.html>`_


PyTorch
====================================

.. toctree::
   :maxdepth: 1

   ../sagemaker-python-sdk/pytorch_horovod_mnist/pytorch_mnist_horovod


TensorFlow
====================================

.. toctree::
   :maxdepth: 1

   ../sagemaker-python-sdk/keras_script_mode_pipe_mode_horovod/tensorflow_keras_CIFAR10
   ../advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-s3
