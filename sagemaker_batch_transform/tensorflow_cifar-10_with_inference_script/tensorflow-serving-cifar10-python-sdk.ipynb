{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Highly Performant TensorFlow Batch Inference and Training  \n",
    "\n",
    "For use cases involving large datasets, particularly those where the data is images, it often is necessary to perform distributed training on a cluster of multiple machines. Similarly, when it is time to set up an inference workflow, it also may be necessary to perform highly performant batch inference using a cluster.  In this notebook, we'll examine how to do these tasks with TensorFlow in Amazon SageMaker, with emphasis on batch inference. \n",
    "\n",
    "For training a model, we'll use a basic Convolutional Neural Network (CNN) based on [the Keras examples](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py), but using the tf.Keras API rather than the separate reference implementation of Keras.  We'll train the CNN to classify images using the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), a well-known computer vision dataset. It consists of 60,000 32x32 images belonging to 10 different classes, with 6,000 images per class. Here is a graphic of the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "![cifar10](https://maet3608.github.io/nuts-ml/_images/cifar10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "We'll begin with some necessary imports, and get an Amazon SageMaker session to help perform certain tasks, as well as an IAM role with the necessary permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-tf-batch-inference-script'\n",
    "print('Bucket:\\n{}'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run a script that fetches the dataset and converts it to the TFRecord format, which is convenient and often more performant for training models in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_cifar10_tfrecords.py --data-dir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Amazon SageMaker hosted training on a cluster separate from this notebook instance, training data must be stored in Amazon S3, so we'll upload the data to S3 now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-batch-cifar10-tf')\n",
    "display(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training with Horovod\n",
    "\n",
    "Although batch inference is the focus of this notebook, to begin we need a trained model. Sometimes it makes sense to perform training on a single machine. For large datasets, however, it may be necessary to perform distributed training on a cluster of multiple machines. In fact, it may be not only faster but cheaper to do distributed training on several machines rather than one machine. Fortunately, Amazon SageMaker makes it easy to run distributed training without having to manage cluster setup and tear down.  Distributed training can be done using methods such as parameter servers or Ring-AllReduce with Horovod, an open source distributed training framework for TensorFlow, Keras, PyTorch, and MXNet.  \n",
    "\n",
    "For this notebook we'll use Amazon SageMaker Script Mode to set up a Horovod-based training job.  In Script Mode, your training job uses Amazon SageMaker's prebuilt TensorFlow containers with TensorFlow training scripts similar to those you would use outside SageMaker.  A major advantage of Script Mode is that only a few lines of code are necessary to use Horovod for distributed training with the tf.keras API.  For details, see the `train.py` script included with this notebook; the changes primarily relate to:\n",
    "\n",
    "- importing Horovod.\n",
    "- initializing Horovod.\n",
    "- configuring GPU options and setting a Keras/tf.session with those options.\n",
    "\n",
    "Horovod is only available with TensorFlow version 1.12 or newer in Script Mode. \n",
    "\n",
    "Once we have a training script, the next step is to set up an Amazon SageMaker TensorFlow Estimator object with the details of the training job.  It is very similar to an Estimator for training on a single machine, except we specify a `distributions` parameter describing Horovod attributes such as the number of process per host, which is set here to the number of GPUs per machine.  Beyond these few simple parameters and the few lines of code in the training script, there is nothing else you need to do to use distributed training with Horovod; Amazon SageMaker handles the heavy lifting for you and manages the underlying cluster setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "hvd_instance_type = 'ml.p3.8xlarge'\n",
    "hvd_processes_per_host = 4\n",
    "hvd_instance_count = 2\n",
    "\n",
    "distributions = {'mpi': {\n",
    "                    'enabled': True,\n",
    "                    'processes_per_host': hvd_processes_per_host,\n",
    "                    'custom_mpi_options': '-verbose --NCCL_DEBUG=INFO -x OMPI_MCA_btl_vader_single_copy_mechanism=none'\n",
    "                        }\n",
    "                }\n",
    "hyperparameters = {'epochs': 60, 'batch-size' : 256}\n",
    "\n",
    "estimator_hvd = TensorFlow(base_job_name='dist-cifar10-tf',\n",
    "                       source_dir='code',\n",
    "                       entry_point='train.py', \n",
    "                       role=role,\n",
    "                       framework_version='1.13',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       train_instance_count=hvd_instance_count, \n",
    "                       train_instance_type=hvd_instance_type,\n",
    "                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n",
    "                       distributions=distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the `fit` method of the Estimator object to start training. After training completes, the tf.keras model will be saved in the SavedModel .pb format so it can be served by a TensorFlow Serving container.  Note that the model is only saved by the the master, rank = 0 process (disregard any warnings about the model not being saved by all the processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_inputs = {'train' : inputs+'/train', 'validation' : inputs+'/validation', 'eval' : inputs+'/eval'}\n",
    "estimator_hvd.fit(remote_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Batch Transform with TFS pre/post-processing scripts\n",
    "\n",
    "Amazon SageMaker lets you deploy a model to an endpoint for real-time inferences, or create a Transform Job for offline inference. If a use case does not require individual predictions in near real-time, an Amazon SageMaker Batch Transform job is likely a better alternative. Although hosted endpoints also can be used for pseudo-batch prediction, the process is more involved than using the alternative Batch Transform, which is designed for large-scale, asynchronous batch inference.\n",
    "\n",
    "A typical problem in working with batch inference is how to convert data into tensors that can be input to the model.  For example, image data in .png or .jpg format cannot be directly input to a model, but rather must be converted first.  Additionally, sometimes other preprocessing of the data must be performed, such as resizing.  The Amazon SageMaker TFS container provides functionality for doing this efficiently.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre/post-postprocessing inference script\n",
    "\n",
    "The TFS container in Amazon SageMaker by default uses the REST API to serve prediction requests. This requires the input data to be converted to JSON format.  One way to do this is to create a Docker container to do the conversion, then create an overall Amazon SageMaker model that links the conversion container to the TensorFlow Serving container with the model. This is known as an Amazon SageMaker Inference Pipeline, as demonstrated in another [sample notebook](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker_batch_transform/working_with_tfrecords).  \n",
    "\n",
    "However, as a more convenient alternative for many use cases, the Amazon SageMaker TFS container provides a data pre/post-processing script feature that allows you to simply supply a data transformation script.  Using such a script, there is no need to build containers or directly work with Docker.  The simplest form of a script must only (1) implement an `input_handler` and `output_handler` interface, as shown in the code below, (2) be named `inference.py`, and (3) be placed in a `/code` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the input preprocessing side, the code takes an image read from Amazon S3 and converts it to the required TFS REST API input format.  On the output postprocessing side, the script simply passes through the predictions in the standard TFS format without modifying them. Alternatively, we could have just returned a class label for the class with the highest score, or performed other postprocessing that would be helpful to the application consuming the predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements.txt\n",
    "\n",
    "Besides an `inference.py` script implementing the handler interface, it also may be necessary to supply a `requirements.txt` file to ensure any necessary dependencies are installed in the container along with the script.  For this script, in addition to the Python standard libraries we need the Pillow and Numpy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Batch Transform job\n",
    "\n",
    "Next, we'll run a Batch Transform job using our data processing script and GPU-based Amazon SageMaker Model. More specifically, we'll perform inference on a cluster of two instances. The objects in the S3 path will be distributed between the two instances. In this example, our input can't be split by newline characters and batched together, so the cluster will receive one HTTP request per serialized image.\n",
    "\n",
    "The code below creates a SageMaker Model entity that will be used for Batch inference, and runs a Transform Job using that Model. The Model contains a reference to the TFS container, the TensorFlow SavedModel we trained above, the pre- and post-processing script, and the `requirements.txt` file.\n",
    "\n",
    "For improved performance, we'll set the `max_concurrent_transforms` and `max_payload` parameters of the Transformer object, which control the maximum number of parallel requests that can be sent to each instance in a transform job and the maximum size of each request body.\n",
    "\n",
    "When performing inference on entire, unsplit S3 objects, it is recommended that you set `max_payload` to be slightly larger than the largest S3 object in your dataset, and that you experiment with the `max_concurrent_transforms` parameter in powers of two to find a value that maximizes throughput for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "tensorflow_serving_model = Model(model_data=estimator_hvd.model_data,\n",
    "                                 role=role,\n",
    "                                 framework_version='1.13',\n",
    "                                 sagemaker_session=sagemaker_session)\n",
    "\n",
    "input_data_path='s3://sagemaker-sample-data-{}/tensorflow/cifar10/images/png'.format(sagemaker_session.boto_region_name)\n",
    "output_data_path='s3://{}/{}/{}'.format(bucket, prefix, 'batch-predictions')\n",
    "batch_instance_count=2\n",
    "batch_instance_type = 'ml.p3.2xlarge'\n",
    "concurrency=32\n",
    "max_payload_in_mb=1\n",
    "\n",
    "transformer = tensorflow_serving_model.transformer(\n",
    "    instance_count=batch_instance_count,\n",
    "    instance_type=batch_instance_type,\n",
    "    max_concurrent_transforms=concurrency,\n",
    "    max_payload=max_payload_in_mb,\n",
    "    output_path=output_data_path\n",
    ")\n",
    "\n",
    "transformer.transform(data=input_data_path, content_type='application/x-image')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Batch Transform output\n",
    "\n",
    "Finally, we can inspect the output files of our Batch Transform job to see the predictions.  First we'll download the prediction files locally, then extract the predictions from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --quiet --recursive $transformer.output_path ./batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "predicted = []\n",
    "actual = []\n",
    "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "for entry in os.scandir('batch_predictions'):\n",
    "    try:\n",
    "        if entry.is_file() and entry.name.endswith(\"out\"):\n",
    "            with open(entry, 'r') as f:\n",
    "                jstr = json.load(f)\n",
    "                results = [float('%.3f'%(item)) for sublist in jstr['predictions'] for item in sublist]\n",
    "                class_index = np.argmax(np.array(results))\n",
    "                predicted_label = labels[class_index]\n",
    "                predicted.append(predicted_label)\n",
    "                actual_label = re.search('([a-zA-Z]+).png.out', entry.name).group(1)\n",
    "                actual.append(actual_label)\n",
    "                is_correct = (predicted_label in entry.name) or False\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the accuracy of the predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Out of {} total images, accurate predictions were returned for {}'.format(total, correct))\n",
    "accuracy = correct / total\n",
    "print('Accuracy is {:.1%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy from the batch transform job on 10000 test images never seen during training is fairly close to the accuracy achieved during training on the validation set.  This is an indication that the model is not overfitting and should generalize fairly well to other unseen data. \n",
    "\n",
    "Next we'll plot a confusion matrix, which is a tool for visualizing the performance of a multiclass model. It has entries for all possible combinations of correct and incorrect predictions, and shows how often each one was made by our model. Ours will be row-normalized: each row sums to one, so that entries along the diagonal correspond to recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "confusion_matrix = pd.crosstab(pd.Series(actual), pd.Series(predicted), rownames=['Actuals'], colnames=['Predictions'], normalize='index')\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model had 100% accuracy, and therefore 100% recall in every class, then all of the predictions would fall along the diagonal of the confusion matrix.  Here our model definitely is not 100% accurate, but manages to achieve good recall for most of the classes, though it performs worse for some classes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment with Amazon Elastic Inference\n",
    "\n",
    "Amazon SageMaker also lets you deploy a TensorFlow Serving model to a hosted Endpoint for real-time inference. The processes for setting up hosted endpoints and Batch Transform jobs have significant differences, as we will see.  Additionally, we will discuss why and how to use Amazon Elastic Inference with the hosted endpoint.\n",
    "\n",
    "### Deploying the Model\n",
    "\n",
    "When considering the overall cost of a machine learning workload, inference often is the largest part, up to 90% of the total.  If a GPU instance type is used for real time inference, it typically is not fully utilized because, unlike training, real time inference does not involve continuously inputting large batches of data to the model.  Elastic Inference provides GPU acceleration suited for inference, allowing you to add inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance.\n",
    "\n",
    "Instead of a Transformer object, we'll instantiate a Predictor object now. The `deploy` method of the Estimator object instantiates a Predictor object representing an endpoint which serves prediction requests in near real time.  To utilize Elastic Inference with the SageMaker TFS container, simply provide an `accelerator_type` parameter, which determines the type of accelerator that is attached to your endpoint. Refer to the **Inference Acceleration** section of the [instance types chart](https://aws.amazon.com/sagemaker/pricing/instance-types) for a listing of the supported types of accelerators. \n",
    "\n",
    "Here we'll use a general purpose CPU compute instance type along with an Elastic Inference accelerator:  together they are much cheaper than the smallest P3 GPU instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator_hvd.deploy(initial_instance_count=1,\n",
    "                                  instance_type='ml.m5.xlarge',\n",
    "                                  accelerator_type='ml.eia1.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Real time inference\n",
    "  \n",
    "Now that we have a Predictor object wrapping a real time Amazon SageMaker hosted endpoint, we'll define the label names and look at a sample of 10 images, one from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "images = []\n",
    "for entry in os.scandir('sample-img'):\n",
    "    if entry.is_file() and entry.name.endswith(\"png\"):\n",
    "        images.append('sample-img/' + entry.name)\n",
    "\n",
    "for image in images:\n",
    "    display(Image(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll modify some properties of the Predictor object created by the `deploy` method call above.  The TFS container in Amazon SageMaker by default uses the TFS REST API, which requires requests in a specific JSON format.  However, for many real time use cases involving image data it is more convenient to have the client application send the image data directly to an endpoint for predictions, without converting and preprocessing it on the client side. \n",
    "\n",
    "Fortunately, our endpoint includes the same pre/post-processing script used in the Batch Transform section of this notebook because the same model artifact is used in both cases. This model artifact includes the same `inference.py` script.  With this preprocessing script in place, we just specify the Predictor's content type as `application/x-image` and override the default serializer. Then we can simply provide the raw .png image bytes to the Predictor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.content_type = 'application/x-image'\n",
    "predictor.serializer = None\n",
    "\n",
    "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "def get_prediction(file_path):\n",
    "    \n",
    "    with open(file_path, \"rb\") as image:\n",
    "        f = image.read()\n",
    "    b = bytearray(f)\n",
    "    return labels[np.argmax(predictor.predict(b)['predictions'], axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [get_prediction(image) for image in images]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "\n",
    "Although we did not demonstrate them in this notebook, Amazon SageMaker provides additional ways to make distributed training more efficient for very large datasets:\n",
    "- **VPC training**:  performing Horovod training inside a VPC improves the network latency between nodes, leading to higher performance and stability of Horovod training jobs.\n",
    "\n",
    "- **Pipe Mode**:  using [Pipe Mode](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container-inputdataconfig) reduces startup and training times.  Pipe Mode streams training data from S3 as a Linux FIFO directly to the algorithm, without saving to disk.  For a small dataset such as CIFAR-10, Pipe Mode does not provide any advantage, but for very large datasets where training is I/O bound rather than CPU/GPU bound, Pipe Mode can substantially reduce startup and training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "To avoid incurring charges due to a stray endpoint, delete the Amazon SageMaker endpoint if you no longer need it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
