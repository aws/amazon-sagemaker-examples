{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc310c2",
   "metadata": {},
   "source": [
    "# Model Regression Free Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "606ff58e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362e2dd",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6b757",
   "metadata": {},
   "source": [
    "Reducing inconsistencies in the behavior of different versions of an AI system can be as important in practice as reducing its overall error. In image classification, sample wise inconsistencies appear as “negative flips”: A new model incorrectly predicts the output for a test sample that was correctly classified by the old (reference) model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78623ed9",
   "metadata": {},
   "source": [
    "In [1] and [2], authors show that, even for models trained on the same data with different initial conditions, data augmentations, and hyperparameters, the error rates could yield similar, but with errors occurring on different samples. Some samples are correctly classified by the old model but incorrectly by the new one. We call such samples as Negative Flips. Their fraction of the total number is called Negative Flip Rate (NFR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46cddc",
   "metadata": {},
   "source": [
    "To reduce the NFR between two models, in [1], authors propose a simple approach for reducing NFR, Focal Distillation (FD), which enforces congruence with the reference model by giving more weights to samples that were correctly classified. In [2], authors propose to use Logit Difference Inhibition (LDI) loss, that penalizes changes in the logits between the new and old model, without forcing them to coincide as in ordinary distillation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb511e83",
   "metadata": {},
   "source": [
    "In this notebook, we show how to train 2 models with SageMaker notebook instance and measure the regression metrics like negative flip rates between their outputs. Then we show how to apply FD/LDI loss for new model training to reduce their NFR against the old model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74881f5e",
   "metadata": {},
   "source": [
    "## Set up SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ae899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torchvision, torch\n",
    "import sagemaker, boto3, json, logging\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.local import LocalSession\n",
    "from time import gmtime, strftime\n",
    "\n",
    "logging.disable(logging.CRITICAL)\n",
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d622d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use remote mode\n",
    "sagemaker_region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "instance_type = \"ml.p3.2xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1fb2dc",
   "metadata": {},
   "source": [
    "## Set up SageMaker training env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb313a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85719461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# git configuration to download regression-free training script\n",
    "git_config = {\n",
    "    \"repo\": \"https://github.com/amazon-science/regression-constraint-model-upgrade.git\",\n",
    "    \"branch\": \"main\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67415c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_job_name = f\"jumpstart-example-regression-free-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "\n",
    "cifar10_estimator = PyTorch(\n",
    "    base_job_name=base_job_name,\n",
    "    git_config=git_config,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"ConstrainedUpgrade/\",\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters={\n",
    "        \"use_cifar\": True,\n",
    "        \"epochs\": 30,\n",
    "        \"arch\": \"resnet18\",\n",
    "        \"batch-size\": 128,\n",
    "        \"lr\": 0.1,\n",
    "        \"lr_step\": 20,\n",
    "        \"bucket_name\": bucket_name,\n",
    "        \"seed\": 42,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97defba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar10_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76783d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train another resnet-18 with a different random seed\n",
    "\n",
    "base_job_name = f\"jumpstart-example-regression-free-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "\n",
    "cifar10_estimator = PyTorch(\n",
    "    base_job_name=base_job_name,\n",
    "    git_config=git_config,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"ConstrainedUpgrade/\",\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters={\n",
    "        \"use_cifar\": True,\n",
    "        \"epochs\": 30,\n",
    "        \"arch\": \"resnet18\",\n",
    "        \"batch-size\": 128,\n",
    "        \"lr\": 0.1,\n",
    "        \"lr_step\": 20,\n",
    "        \"bucket_name\": bucket_name,\n",
    "        \"seed\": 114514,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c24499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar10_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da68b98",
   "metadata": {},
   "source": [
    "## Pull the model prediction from S3 for regression testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d05eee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model outputs from S3 to local machine\n",
    "\n",
    "with open(\"model_resnet18_seed_42.result\", \"wb\") as f:\n",
    "    s3.download_fileobj(bucket_name, \"model_resnet18_seed_42.result\", f)\n",
    "\n",
    "with open(\"model_resnet18_seed_114514.result\", \"wb\") as f:\n",
    "    s3.download_fileobj(bucket_name, \"model_resnet18_seed_114514.result\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373424b-9c3a-461e-8c6f-b40d9b02fb7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define regression analyzer\n",
    "\n",
    "\n",
    "class ModelAnalyzer:\n",
    "    def __init__(self, model_info):\n",
    "        if type(model_info) == str:\n",
    "            model_info = torch.load(model_info)\n",
    "        self.pred = model_info[\"pred\"].cpu()\n",
    "        self.gt = model_info[\"gt\"].cpu()\n",
    "\n",
    "    def NFR(self, old_model):  # Negative Flip Rate\n",
    "        return float(((old_model.pred == self.gt) & (self.pred != self.gt)).sum()) / len(self.gt)\n",
    "\n",
    "    def PFR(self, old_model):  # Positive Flip Rate\n",
    "        return float(((old_model.pred != self.gt) & (self.pred == self.gt)).sum()) / len(self.gt)\n",
    "\n",
    "    def Acc(self):  # Top-1 Accuracy\n",
    "        return (self.pred == self.gt).sum() * 1.0 / len(self.gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143739fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# regression rate testing\n",
    "s42_result = ModelAnalyzer(\n",
    "    torch.load(\"model_resnet18_seed_42.result\", map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "s114514_result = ModelAnalyzer(\n",
    "    torch.load(\"model_resnet18_seed_114514.result\", map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "\n",
    "print(\"NFR between 2 models is {}\".format(s42_result.NFR(s114514_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fe033",
   "metadata": {},
   "source": [
    "# FD/LDI regression-free training with SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0631f935",
   "metadata": {},
   "source": [
    "After training few models with different random seeds, we can use them as a guidance to implement regression-free training with different losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636b23c8",
   "metadata": {},
   "source": [
    "## FD training with trained resnet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42abbbd5",
   "metadata": {},
   "source": [
    "Focal Distillation (FD) [1] enforces congruence with the reference model by giving more weights to samples that were correctly classified, which is discribed as follows,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06701f5",
   "metadata": {},
   "source": [
    "$L_{\\text{focal}} = - \\sum_{i=1}^{N} (\\alpha + \\beta * \\textbf{1} * ((\\hat{y}_{\\text{old}}(x_i) = y_i)\\cal{D}(\\phi_{\\text{new}(x_i)} , \\phi_{old}(x_i) ) ) $,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eee93d",
   "metadata": {},
   "source": [
    "where $\\hat{y}_{\\text{old}}(x_i)$ is the predicted label of sample $x_i$ by old model $\\phi_{old}(x_i) $, $\\cal{D}$ is  a distance metric (we use KL divergence here). The filter function $\\textbf{1}$ applies a basic weight $\\alpha$ for all samples in the training set and an additional weight to the samples correctly predicted by the old model. When $\\alpha$ = 1 and $\\beta$ = 0, focal distillation reduces to ordinary distillation. When $\\alpha$ = 0 and $\\beta$ > 0, we are only applying the distillation objective to the training samples predicted correctly by the old model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9379040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_job_name = f\"jumpstart-example-regression-free-FD-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "\n",
    "cifar10_FD_estimator = PyTorch(\n",
    "    base_job_name=base_job_name,\n",
    "    git_config=git_config,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"ConstrainedUpgrade/\",\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters={\n",
    "        \"use_cifar\": True,\n",
    "        \"gpu\": 0,\n",
    "        \"epochs\": 30,\n",
    "        \"arch\": \"resnet18\",\n",
    "        \"batch-size\": 128,\n",
    "        \"lr\": 0.01,\n",
    "        \"lr_step\": 20,\n",
    "        \"bucket_name\": bucket_name,\n",
    "        \"seed\": 1,\n",
    "        \"kd_model_num_classes\": 10,\n",
    "        \"kd_model_arch\": \"resnet18\",\n",
    "        \"kd_model_path\": \"best_model_resnet18_seed_114514.pth.tar\",\n",
    "        \"load_from_s3\": True,\n",
    "        \"kd_loss_weight\": 1,\n",
    "        \"kd_alpha\": 0.9,\n",
    "        \"kd_loss_mode\": \"kl\",\n",
    "        \"kd_temperature\": 100,\n",
    "        \"kd_filter\": \"old_correct\",\n",
    "        \"filter-base\": 1,\n",
    "        \"filter-scale\": 5,\n",
    "        \"desc\": \"FD\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322fafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar10_FD_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd86c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"model_resnet18_seed_1_loss_kl_FD.result\", \"wb\") as f:\n",
    "    s3.download_fileobj(bucket_name, \"model_resnet18_seed_1_loss_kl_FD.result\", f)\n",
    "\n",
    "s1_fd_result = ModelAnalyzer(\n",
    "    torch.load(\"model_resnet18_seed_1_loss_kl_FD.result\", map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "\n",
    "print(\"NFR between 2 models is {}\".format(s1_fd_result.NFR(s114514_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52856e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Acc of original model is {}\".format(s114514_result.Acc()))\n",
    "print(\"Acc of FD model is {}\".format(s1_fd_result.Acc()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53e526",
   "metadata": {},
   "source": [
    "## LDI training with trained resnet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81fd0c",
   "metadata": {},
   "source": [
    "Another loss term can be used for regression-free training is Logit Difference Inhibition (LDI) loss [2], which penalizes changes in the logits between the new and old model, without forcing them to coincide as in ordinary distillation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aff7b5",
   "metadata": {},
   "source": [
    "LDI loss is defined as, $L_{\\text{LDI}} = - \\sum_{i=1}^{N} \\text{max}(||\\phi_{\\text{new}(x_i)} - \\phi_{old}(x_i)||^p - \\xi, \\ 0) $,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21389b95",
   "metadata": {},
   "source": [
    "where $\\xi$ is truncating threshold such that difference below $\\xi$ is tolerated. $p$ is normally set to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120e80fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_job_name = f\"jumpstart-example-regression-free-LDI-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "\n",
    "cifar10_LDI_estimator = PyTorch(\n",
    "    base_job_name=base_job_name,\n",
    "    git_config=git_config,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"ConstrainedUpgrade/\",\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters={\n",
    "        \"use_cifar\": True,\n",
    "        \"gpu\": 0,\n",
    "        \"epochs\": 30,\n",
    "        \"arch\": \"resnet18\",\n",
    "        \"batch-size\": 128,\n",
    "        \"lr\": 0.1,\n",
    "        \"lr_step\": 20,\n",
    "        \"bucket_name\": bucket_name,\n",
    "        \"seed\": 1,\n",
    "        \"kd_model_num_classes\": 10,\n",
    "        \"kd_model_arch\": \"resnet18\",\n",
    "        \"kd_model_path\": \"best_model_resnet18_seed_114514.pth.tar\",\n",
    "        \"load_from_s3\": True,\n",
    "        \"kd_loss_weight\": 1,\n",
    "        \"kd_alpha\": 0.5,\n",
    "        \"kd_loss_mode\": \"li\",\n",
    "        \"kd_filter\": \"all_pass\",\n",
    "        \"li_p\": 2,\n",
    "        \"li_margin\": 0.5,\n",
    "        \"desc\": \"LDI\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081ea80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar10_LDI_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1577601d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"model_resnet18_seed_1_loss_li_LDI.result\", \"wb\") as f:\n",
    "    s3.download_fileobj(bucket_name, \"model_resnet18_seed_1_loss_li_LDI.result\", f)\n",
    "\n",
    "s1_ldi_result = ModelAnalyzer(\n",
    "    torch.load(\"model_resnet18_seed_1_loss_li_LDI.result\", map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "\n",
    "print(\"NFR between 2 models is {}\".format(s1_ldi_result.NFR(s114514_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd5c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Acc of original model is {}\".format(s42_result.Acc()))\n",
    "print(\"Acc of LDI model is {}\".format(s1_ldi_result.Acc()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa5556",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c7e8a",
   "metadata": {},
   "source": [
    "[1] Yan, Sijie, et al. \"Positive-congruent training: Towards regression-free model updates.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n",
    "\n",
    "[2] Zhao, Yue, et al. \"ELODI: Ensemble Logit Difference Inhibition for Positive-Congruent Training.\" arXiv preprint arXiv:2205.06265 (2022)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3faf386a",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/introduction_to_amazon_algorithms|jumpstart_regression_free_training|Amazon_JumpStart_Regression_Free_Training.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
