{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing building HVAC with Amazon SageMaker RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "from sagemaker.rl import RLEstimator\n",
    "\n",
    "from source.common.docker_utils import build_and_push_docker_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.session.Session()\n",
    "\n",
    "# SageMaker SDK creates a default bucket. Change this bucket to your own bucket, if needed.\n",
    "s3_bucket = sm_session.default_bucket()\n",
    "\n",
    "s3_output_path = f's3://{s3_bucket}'\n",
    "print(f'S3 bucket path: {s3_output_path}')\n",
    "print(f'Role: {role}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the framework you want to use\n",
    "\n",
    "Set `framework` to `'tf'` or `'torch'` for TensorFlow or PyTorch, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix = 'energyplus-hvac-ray'\n",
    "    \n",
    "framework = 'tf'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure where training happens\n",
    "\n",
    "You can train your RL training jobs using the SageMaker notebook instance or local notebook instance. In both of these scenarios, you can run the following in either local or SageMaker modes. The local mode uses the SageMaker Python SDK to run your code in a local container before deploying to SageMaker. This can speed up iterative testing and debugging while using the same familiar Python SDK interface. You just need to set `local_mode = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in local_mode on this machine, or as a SageMaker TrainingJob?\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = 'local'\n",
    "else:\n",
    "    # If on SageMaker, pick the instance type.\n",
    "    instance_type = 'ml.g4dn.16xlarge' # g4dn.16x large has 1 GPU and 64 cores\n",
    "\n",
    "if 'ml.p' in instance_type or 'ml.g' in instance_type:\n",
    "    cpu_or_gpu = \"gpu\"\n",
    "else:\n",
    "    cpu_or_gpu = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your homogeneous scaling job here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit the training code\n",
    "\n",
    "The training code is written in the file `train-sagemaker-hvac.py` which is uploaded in the /source directory.\n",
    "\n",
    "*Note that ray will automatically set `\"ray_num_cpus\"` and `\"ray_num_gpus\"` in `_get_ray_config`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize source/train-sagemaker-hvac.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RL model using the Python SDK Script mode\n",
    "\n",
    "When using SageMaker for distributed training, you can select a GPU or CPU instance. The RLEstimator is used for training RL jobs.\n",
    "\n",
    "1. Specify the source directory where the environment, presets and training code is uploaded.\n",
    "2. Specify the entry point as the training code\n",
    "3. Specify the image (CPU or GPU) to be used for the training environment.\n",
    "4. Define the training parameters such as the instance count, job name, S3 path for output and job name.\n",
    "5. Define the metrics definitions that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build custom docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a custom docker image depending on if cpu or gpu is used\n",
    "\n",
    "suffix = 'py37' if framework == 'tf' else 'py36'\n",
    "\n",
    "repository_short_name = f'sagemaker-hvac-ray-{cpu_or_gpu}'\n",
    "docker_build_args = {\n",
    "    'CPU_OR_GPU': cpu_or_gpu, \n",
    "    'AWS_REGION': boto3.Session().region_name,\n",
    "    'FRAMEWORK': framework,\n",
    "    'SUFFIX' : suffix\n",
    "}\n",
    "\n",
    "image_name = build_and_push_docker_image(repository_short_name, build_args=docker_build_args)\n",
    "print(\"Using ECR image %s\" % image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions =  [\n",
    "    {'Name': 'training_iteration', 'Regex': 'training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episodes_total', 'Regex': 'episodes_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'num_steps_trained', 'Regex': 'num_steps_trained: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'timesteps_total', 'Regex': 'timesteps_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'training_iteration', 'Regex': 'training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "\n",
    "    {'Name': 'episode_reward_max', 'Regex': 'episode_reward_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_reward_mean', 'Regex': 'episode_reward_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_reward_min', 'Regex': 'episode_reward_min: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray homogeneous scaling - Specify `train_instance_count` > 1\n",
    "\n",
    "Homogeneous scaling allows us to use multiple instances of the same type.\n",
    "\n",
    "Spot instances are unused EC2 instances that could be used at 90% discount compared to On-Demand prices (more information about spot instances can be found [here](https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&cards.sort-order=asc) and [here](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html))\n",
    "\n",
    "To use spot instances, set `train_use_spot_instances = True`. To use On-Demand instances, `train_use_spot_instances = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    # no. of days to simulate. Remember to adjust the dates in RunPeriod of \n",
    "    # 'source/eplus/envs/buildings/MediumOffice/RefBldgMediumOfficeNew2004_Chicago.idf' to match simulation days.\n",
    "    'n_days': 365,\n",
    "    'n_iter': 50, # no. of training iterations\n",
    "    'algorithm': 'PPO', # only APEX_DDPG and PPO are tested\n",
    "    'multi_zone_control': True, # if each zone temperature set point has to be independently controlled\n",
    "    'energy_temp_penalty_ratio': 10,\n",
    "    'framework' : framework\n",
    "}\n",
    "\n",
    "# Set additional training parameters\n",
    "training_params = {\n",
    "    'base_job_name': job_name_prefix,    \n",
    "    'train_instance_count': 1,\n",
    "    'tags': [{'Key': k, 'Value': str(v)} for k,v in hyperparameters.items()]\n",
    "}\n",
    "\n",
    "# Defining the RLEstimator\n",
    "estimator = RLEstimator(entry_point=f'train-sagemaker-hvac.py',\n",
    "                        source_dir='source',\n",
    "                        dependencies=[\"source/common/\"],\n",
    "                        image_uri=image_name,\n",
    "                        role=role,\n",
    "                        train_instance_type=instance_type,  \n",
    "#                         train_instance_type='local',                          \n",
    "                        output_path=s3_output_path,\n",
    "                        metric_definitions=metric_definitions,\n",
    "                        hyperparameters=hyperparameters,\n",
    "                        **training_params\n",
    "                    )\n",
    "\n",
    "estimator.fit(wait=True)\n",
    "\n",
    "print(' ')\n",
    "print(estimator.latest_training_job.job_name)\n",
    "print('type=', instance_type, 'count=', training_params['train_instance_count'])\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
