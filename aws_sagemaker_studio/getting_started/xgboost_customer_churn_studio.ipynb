{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Studio Walkthrough\n",
    "_**Using Gradient Boosted Trees to Predict Mobile Customer Departure**_\n",
    "\n",
    "---\n",
    "\n",
    "This notebook walks you through some of the main features of Amazon SageMaker Studio. \n",
    "\n",
    "* [Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html)\n",
    "  * Manage multiple trials\n",
    "  * Experiment with hyperparameters and charting\n",
    "* [Amazon SageMaker Debugger](https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html)\n",
    "  * Debug your model \n",
    "* [Model hosting](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)\n",
    "  * Set up a persistent endpoint to get predictions from your model\n",
    "* [SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html)\n",
    "  * Monitor the quality of your model\n",
    "  * Set alerts for when model quality deviates\n",
    "  \n",
    "You must run this walkthrough in Amazon SageMaker Studio. For Studio onboarding and setup instructions, see the [README](README.md).\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background) - Predict customer churn with XGBoost\n",
    "1. [Data](#Data) - Prep the dataset and upload it to Amazon S3\n",
    "1. [Train](#Train) - Train with the Amazon SageMaker XGBoost algorithm\n",
    "  - [Amazon SageMaker Experiments](#Amazon-SageMaker-Experiments)\n",
    "  - [Amazon SageMaker Debugger](#Amazon-SageMaker-Debugger)\n",
    "1. [Host](#Host)\n",
    "1. [SageMaker Model Monitor](#SageMaker-Model-Monitor)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "_This notebook has been adapted from an [AWS blog post](https://aws.amazon.com/blogs/ai/predicting-customer-churn-with-amazon-machine-learning/). \n",
    "\n",
    "Losing customers is costly for any business.  Identifying unhappy customers early on gives you a chance to offer them incentives to stay.  This notebook describes using machine learning (ML) for automated identification of unhappy customers, also known as customer churn prediction. It uses Amazon SageMaker features for managing experiments, training and debugging the model, and monitoring it after it has been deployed. \n",
    "\n",
    "Import the Python libraries we'll need for this walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sagemaker -U\n",
    "!{sys.executable} -m pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig\n",
    "from sagemaker.model_monitor import DataCaptureConfig, DatasetFormat, DefaultModelMonitor\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "Mobile operators have historical records that tell them which customers ended up churning and which continued using the service. We can use this historical information to train an ML model that can predict customer churn. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model to have the model predict whether this customer will churn. \n",
    "\n",
    "The dataset we use is publicly available and was mentioned in [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets. The `data` folder that came with this notebook contains the dataset, which we've already preprocessed for this walkthrough. The dataset has been split into training and validation sets. To see how the dataset was preprocessed, see this notebook: [XGBoost customer churn notebook that starts with the original dataset](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.ipynb). \n",
    "\n",
    "We'll train on a .csv file without the header. But for now, the following cell uses `pandas` to load some of the data from a version of the training data that has a header. \n",
    "\n",
    "Explore the data to see the dataset's features and the data that will be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path we can find the data files that go with this notebook\n",
    "%cd /root/amazon-sagemaker-examples/aws_sagemaker_studio/getting_started\n",
    "local_data_path = './data/training-dataset-with-header.csv'\n",
    "data = pd.read_csv(local_data_path)\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 10)         # Keep the output on one page\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload the files to S3 for training but first we will create an S3 bucket for the data if one does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = sess.client('sts', region_name=sess.region_name).get_caller_identity()[\"Account\"]\n",
    "bucket = 'sagemaker-studio-{}-{}'.format(sess.region_name, account_id)\n",
    "prefix = 'xgboost-churn'\n",
    "\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(Bucket=bucket, \n",
    "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
    "except Exception as e:\n",
    "    print(\"Looks like you already have a bucket of this name. That's good. Uploading the data files...\")\n",
    "\n",
    "# Return the URLs of the uploaded file, so they can be reviewed or used elsewhere\n",
    "s3url = S3Uploader.upload('data/train.csv', 's3://{}/{}/{}'.format(bucket, prefix,'train'))\n",
    "print(s3url)\n",
    "s3url = S3Uploader.upload('data/validation.csv', 's3://{}/{}/{}'.format(bucket, prefix,'validation'))\n",
    "print(s3url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "We'll use the XGBoost library to train a class of models known as gradient boosted decision trees on the data that we just uploaded. \n",
    "\n",
    "Because we're using XGBoost, we first need to specify the locations of the XGBoost algorithm containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "docker_image_name = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version='0.90-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Experiments\n",
    "\n",
    "Amazon SageMaker Experiments allows us to keep track of model training; organize related models together; and log model configuration, parameters, and metrics to reproduce and iterate on previous models and compare models. We'll create a single experiment to keep track of the different approaches we'll try to train the model.\n",
    "\n",
    "Each approach or block of training code that we run will be an experiment trial. Later, we'll be able to compare different trials in Amazon SageMaker Studio.\n",
    "\n",
    "Let's create the experiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.session.Session()\n",
    "\n",
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "customer_churn_experiment = Experiment.create(experiment_name=\"customer-churn-prediction-xgboost-{}\".format(create_date), \n",
    "                                              description=\"Using xgboost to predict customer churn\", \n",
    "                                              sagemaker_boto_client=boto3.client('sagemaker'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "Now we can specify our XGBoost hyperparameters.  Among them are these key hyperparameters:\n",
    "- `max_depth` Controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  Typically, you need to explore some trade-offs in model performance between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` Controls sampling of the training data.  This hyperparameter can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` Controls the number of boosting rounds.  This value specifies the models that are subsequently trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` Controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` Controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "- `min_child_weight` Also controls how aggresively trees are grown. Large values lead to a more conservative model.\n",
    "\n",
    "For more information about these hyperparameters, see [XGBoost's hyperparameters GitHub page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\"max_depth\":5,\n",
    "               \"subsample\":0.8,\n",
    "               \"num_round\":600,\n",
    "               \"eta\":0.2,\n",
    "               \"gamma\":4,\n",
    "               \"min_child_weight\":6,\n",
    "               \"silent\":0,\n",
    "               \"objective\":'binary:logistic'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 1 - XGBoost in algorithm mode\n",
    "\n",
    "For our first trial, we'll use the built-in XGBoost algorithm to train a model without supplying any additional code. This way, we can use XGBoost to train and deploy a model as we would with other Amazon SageMaker built-in algorithms.\n",
    "\n",
    "We'll create a new `Trial` object and associate the trial with the experiment that we created earlier. To train the model, we'll create an estimator and specify a few parameters, like the type of training instances we'd like to use and how many, and where the artifacts of the trained model should be stored. \n",
    "\n",
    "We'll also associate the training job with the experiment trial that we just created when we call the `fit` method of the `estimator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = Trial.create(trial_name=\"algorithm-mode-trial-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())), \n",
    "                     experiment_name=customer_churn_experiment.experiment_name,\n",
    "                     sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(image_name=docker_image_name,\n",
    "                                    role=role,\n",
    "                                    hyperparameters=hyperparams,\n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    base_job_name=\"demo-xgboost-customer-churn\",\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.fit({'train': s3_input_train,\n",
    "         'validation': s3_input_validation}, \n",
    "        experiment_config={\n",
    "            \"ExperimentName\": customer_churn_experiment.experiment_name, \n",
    "            \"TrialName\": trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        }\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the results\n",
    "\n",
    "After the training job completes successfully, you can view metrics, logs, and graphs related to the trial on the **Experiments** tab in Amazon SageMaker Studio. \n",
    "\n",
    "To see the experiments, choose the **Experiments** button.\n",
    "\n",
    "![experiments_button](./images/studio_experiments_icon.png)\n",
    "\n",
    "To see an experiment's components, in the **Experiments** list, double-click it. To see the componenets for multiple experiments, hold down the Crtl key and choose the experiments, then open the context menu (by right-clicking on an experiment). To see the compenents together, choose \"Open in trial component list\". This enables charting across experiments.\n",
    "\n",
    "![studio_experiments_list](./images/view_multiple_trial_components.gif)\n",
    "\n",
    "The components are sorted so that the best model is at the top. \n",
    "\n",
    "![studio_trial_list](./images/studio_trial_list.png)\n",
    "\n",
    "#### Download the model\n",
    "\n",
    "You can find and download the model in Amazon SageMaker Studio. To find the model, choose the **Experiments** button in the left tray, and keep drilling down through the experiment, the most recent trial listed, and its most recent component until you see the **Describe Trial Components** page. Choose the **Artifacts** tab. It contains links to the training and validation data in the **Input Artifacts** section, and the generated model artifact in the **Output Artifacts** section.\n",
    "\n",
    "![find the model artifact](./images/find_the_model_artifact.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying other hyperparameter values\n",
    "\n",
    "To improve a model, you typically try other hyperparameter values to see if they affect the final validation error. Let's vary the `min_child_weight` parameter and start other training jobs with those different values to see how they affect the validation error. For each value, we'll create a separate trial so that we can compare the results in Amazon SageMaker Studio later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_child_weights = [1, 2, 4, 8, 10]\n",
    "\n",
    "for weight in min_child_weights:\n",
    "    hyperparams[\"min_child_weight\"] = weight\n",
    "    trial = Trial.create(trial_name=\"algorithm-mode-trial-{}-weight-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()), weight), \n",
    "                         experiment_name=customer_churn_experiment.experiment_name,\n",
    "                         sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "    t_xgb = sagemaker.estimator.Estimator(image_name=docker_image_name,\n",
    "                                          role=role,\n",
    "                                          hyperparameters=hyperparams,\n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.m4.xlarge',\n",
    "                                          output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                          base_job_name=\"demo-xgboost-customer-churn\",\n",
    "                                          sagemaker_session=sess)\n",
    "\n",
    "    t_xgb.fit({'train': s3_input_train,\n",
    "               'validation': s3_input_validation},\n",
    "                wait=False,\n",
    "                experiment_config={\n",
    "                    \"ExperimentName\": customer_churn_experiment.experiment_name, \n",
    "                    \"TrialName\": trial.trial_name,\n",
    "                    \"TrialComponentDisplayName\": \"Training\",\n",
    "                }\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create charts\n",
    "\n",
    "To create a chart, multi-select the components. Because this is a sample training run and the data is sparse, there's not much to chart in a time series. However, we can create a scatter plot for the parameter sweep. The following image is an example.\n",
    "\n",
    "![scatter plot example](./images/scatter_plot_example.png)\n",
    "\n",
    "##### How to create a scatter plot\n",
    "\n",
    "Multi-select components, then choose **Add chart**. In the **Chart Properties** panel, choose **Summary Statistics** as the **Data type**. For **Chart type**, choose **Scatter plot**. Choose the hyperparameter `min_child_weight` as the X-axis (because this is the hyperparameter that you are iterating on in this notebook). For Y-axis metrics, choose either `validation:error_last` or `validation:error_avg`. Color them by choosing `trialComponentName`.\n",
    "\n",
    "![create a scatter plot](./images/create_a_scatter_plot.gif)\n",
    "\n",
    "You can also adjust the chart at any time by changing the components that are selected. And you can zoom in and out. Each item on the graph displays contextual information.\n",
    "\n",
    "![adjust a scatter plot](./images/adjust_a_scatter_plot.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Debugger\n",
    "\n",
    "Amazon SageMaker Debugger lets you debug a model during training. As you train, Debugger periodicially saves tensors, which fully specify the state of the model at that point in time. Debugger saves the tensors to an Amazon S3 bucket. You can then use Amazon SageMaker Studio for analysis and visualization to diagnose training issues.\n",
    "\n",
    "#### Specify SageMaker Debugger Rules\n",
    "\n",
    "To enable automated detection of common issues during training, Amazon SageMaker Debugger also allows you to attach a list of rules to evaluate the training job against.\n",
    "\n",
    "Some rules that apply to XGBoost include `AllZero`, `ClassImbalance`, `Confusion`, `LossNotDecreasing`, `Overfit`, `Overtraining`, `SimilarAcrossRuns`, `TensorVariance`, `UnchangedTensor`, and `TreeDepth`. \n",
    "\n",
    "We'll use the `LossNotDecreasing` rule--which is triggered if the loss doesn't decrease monotonically at any point during training--the `Overtraining` rule, and the `Overfit` rule. Let's create the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_rules = [Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "               Rule.sagemaker(rule_configs.overtraining()),\n",
    "               Rule.sagemaker(rule_configs.overfit())\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 2 - XGBoost in framework mode\n",
    "\n",
    "For the next trial, we'll train a similar model, but use XGBoost in framework mode. If you've worked with the open source XGBoost, using XGBoost this way will be familiar to you. Using XGBoost as a framework provides more flexibility than using it as a built-in algorithm because it enables more advanced scenarios that allow incorporating preprocessing and post-processing scripts into your training script. Specifically, we'll be able to specify a list of rules that we want Amazon SageMaker Debugger to evaluate our training  against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit estimator\n",
    "\n",
    "To use XGBoost as a framework, you need to specify an entry-point script that can incorporate additional processing into your training jobs.\n",
    "\n",
    "We've made a couple of simple changes to enable the Amazon SageMaker Debugger `smdebug` library. We created a `SessionHook`, which we pass as a callback function when creating a `Booster`. We passed a `SaveConfig` object that tells the hook to save the evaluation metrics, feature importances, and SHAP values at regular intervals. (Debugger is highly configurable. You can choose exactly what to save.) We describe the changes in more detail after we train this example. For even more detail, see the [Developer Guide for XGBoost](https://github.com/awslabs/sagemaker-debugger/tree/master/docs/xgboost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize xgboost_customer_churn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our framwork estimator and call `fit` to start the training job. As before, we'll create a separate trial for this run so that we can use Amazon SageMaker Studio to compare it with other trials later. Because we are running in framework mode, we also need to pass additional parameters, like the entry point script and the framework version, to the estimator. \n",
    "\n",
    "As training progresses, you'll be able to see Amazon SageMaker Debugger logs that evaluate the rule against the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point_script = \"xgboost_customer_churn.py\"\n",
    "\n",
    "trial = Trial.create(trial_name=\"framework-mode-trial-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())), \n",
    "                     experiment_name=customer_churn_experiment.experiment_name,\n",
    "                     sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "framework_xgb = sagemaker.xgboost.XGBoost(image_name=docker_image_name,\n",
    "                                          entry_point=entry_point_script,\n",
    "                                          role=role,\n",
    "                                          framework_version=\"0.90-2\",\n",
    "                                          py_version=\"py3\",\n",
    "                                          hyperparameters=hyperparams,\n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.m4.xlarge',\n",
    "                                          output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                          base_job_name=\"demo-xgboost-customer-churn\",\n",
    "                                          sagemaker_session=sess,\n",
    "                                          rules=debug_rules\n",
    "                                          )\n",
    "\n",
    "framework_xgb.fit({'train': s3_input_train,\n",
    "                   'validation': s3_input_validation}, \n",
    "                  experiment_config={\n",
    "                      \"ExperimentName\": customer_churn_experiment.experiment_name, \n",
    "                      \"TrialName\": trial.trial_name,\n",
    "                      \"TrialComponentDisplayName\": \"Training\",\n",
    "                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training has been running for a while you can view debug info in the Debugger panel. To get to this panel you must click through the experiment, trial, and then component.\n",
    "![view_debugger panel](./images/view_studio_debugger_output.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Host the model\n",
    "\n",
    "Now that we've trained the model, let's deploy it to a hosted endpoint. To monitor the model after it's hosted and serving requests, we'll also add configurations to capture data that is being sent to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_prefix = '{}/datacapture'.format(prefix)\n",
    "\n",
    "endpoint_name = \"demo-xgboost-customer-churn-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName = {}\".format(endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1, \n",
    "                           instance_type='ml.m4.xlarge',\n",
    "                           endpoint_name=endpoint_name,\n",
    "                           data_capture_config=DataCaptureConfig(enable_capture=True,\n",
    "                                                                 sampling_percentage=100,\n",
    "                                                                 destination_s3_uri='s3://{}/{}'.format(bucket, data_capture_prefix)\n",
    "                                                                )\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the deployed model\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model by making an http POST request.  But first, we need to set up serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll loop over our test dataset and collect predictions by invoking the XGBoost endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait for a minute...\".format(endpoint_name))\n",
    "\n",
    "with open('data/test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = xgb_predictor.predict(data=payload)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that data is captured in Amazon S3\n",
    "\n",
    "When we made some real-time predictions by sending data to our endpoint, we should have also captured that data for monitoring purposes. \n",
    "\n",
    "Let's list the data capture files stored in Amazon S3. Expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_endpoint_capture_prefix = '{}/{}'.format(data_capture_prefix, endpoint_name)\n",
    "print(\"Found Data Capture Files:\")\n",
    "capture_files = S3Downloader.list(\"s3://{}/{}\".format(bucket, current_endpoint_capture_prefix))\n",
    "print(capture_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data captured is stored in a SageMaker specific json-line formatted file. Next, Let's take a quick peek at the contents of a single line in a pretty formatted json so that we can observe the format a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_file = S3Downloader.read_file(capture_files[-1])\n",
    "\n",
    "print(\"=====Single Data Capture====\")\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2)[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each inference request is captured in one line in the jsonl file. The line contains both the input and output merged together. In our example, we provided the ContentType as `text/csv` which is reflected in the `observedContentType` value. Also, we expose the enconding that we used to encode the input and output payloads in the capture format with the `encoding` value.\n",
    "\n",
    "To recap, we have observed how you can enable capturing the input and/or output payloads to an Endpoint with a new parameter. We have also observed how the captured format looks like in S3. Let's continue to explore how SageMaker helps with monitoring the data collected in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon SageMaker Model Monitor\n",
    "\n",
    "Amazon SageMaker Model Monitor lets you monitor and evaluate the data observed by endpoints. It works like this:\n",
    "1. We need to create a baseline that we can use to compare real-time traffic against. \n",
    "1. When a baseline is ready, we can set up a schedule to continously evaluate and compare against the baseline.\n",
    "1. We can send synthetic traffic to trigger alarms.\n",
    "\n",
    "**Important**: It takes an hour or more to complete this section because the shortest monitoring polling time is one hour. The following graphic shows how the monitoring results look after running for a few hours and some of the errors triggered by synthetic traffic.\n",
    "\n",
    "![model monitor example](./images/view_model_monitor_output.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselining and continous monitoring\n",
    "\n",
    "#### 1. Constraint suggestion with the baseline (training) dataset\n",
    "\n",
    "The training dataset that you use to train a model is usually a good baseline dataset. Note that the training dataset data schema and the inference dataset schema must match exactly (for example, they should have the same number and type of features).\n",
    "\n",
    "Using our training dataset, let's ask Amazon SageMaker Model Monitor to suggest a set of baseline `constraints` and generate descriptive `statistics` so we can explore the data. For this example, let's upload the training dataset, which we used to train model. We'll use the dataset file with column headers so we have descriptive feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prefix = prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(bucket,baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(bucket, baseline_results_prefix)\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))\n",
    "baseline_data_path = S3Uploader.upload(\"data/training-dataset-with-header.csv\", baseline_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a baselining job with the training dataset\n",
    "\n",
    "Now that we have the training data ready in S3, let's start a job to `suggest` constraints. To generate the constraints, the convenient helper starts a `ProcessingJob` using a ProcessingJob container provided by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor = DefaultModelMonitor(role=role,\n",
    "                                         instance_count=1,\n",
    "                                         instance_type='ml.m5.xlarge',\n",
    "                                         volume_size_in_gb=20,\n",
    "                                         max_runtime_in_seconds=3600,\n",
    "                                        )\n",
    "\n",
    "baseline_job = my_default_monitor.suggest_baseline(baseline_dataset=baseline_data_path,\n",
    "                                                   dataset_format=DatasetFormat.csv(header=True),\n",
    "                                                   output_s3_uri=baseline_results_uri,\n",
    "                                                   wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job succeeds, we can explore the `baseline_results_uri` location in s3 to see what files where stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found Files:\")\n",
    "S3Downloader.list(\"s3://{}/{}\".format(bucket, baseline_results_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a`constraints.json` file that has information about suggested constraints. We also have a `statistics.json` which contains statistical information about the data in the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Analyzing subsequent captures for data quality issues\n",
    "\n",
    "Now that we've generated a baseline dataset and processed it to get baseline statistics and constraints, let's monitor and analyze the data being sent to the endpoint with monitoring schedules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a schedule\n",
    "First, let's create a monitoring schedule for the endpoint. The schedule specifies the cadence at which we want to run a new processing job so that we can compare recent data captures to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, copy over some test scripts to the S3 bucket so that they can be used for pre and post processing\n",
    "code_prefix = '{}/code'.format(prefix)\n",
    "pre_processor_script = S3Uploader.upload('preprocessor.py', 's3://{}/{}'.format(bucket,code_prefix))\n",
    "s3_code_postprocessor_uri = S3Uploader.upload('postprocessor.py', 's3://{}/{}'.format(bucket,code_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create a model monitoring schedule for the Endpoint created before and also the baseline resources (constraints and statistics) which were generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(bucket,reports_prefix)\n",
    "\n",
    "mon_schedule_name = 'demo-xgboost-customer-churn-model-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(monitor_schedule_name=mon_schedule_name,\n",
    "                                              endpoint_input=xgb_predictor.endpoint,\n",
    "                                              #record_preprocessor_script=pre_processor_script,\n",
    "                                              post_analytics_processor_script=s3_code_postprocessor_uri,\n",
    "                                              output_s3_uri=s3_report_path,\n",
    "                                              statistics=my_default_monitor.baseline_statistics(),\n",
    "                                              constraints=my_default_monitor.suggested_constraints(),\n",
    "                                              schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "                                              enable_cloudwatch_metrics=True,\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Start generating some artificial traffic\n",
    "The following block starts a thread to send some traffic to the endpoint. This allows us to continue to send traffic to the endpoint so that we'll have data continually captured for analysis. If there is no traffic, the monitoring jobs will start to fail later.\n",
    "\n",
    "To terminate this thread, you need to stop the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, 'r') as f:\n",
    "        for row in f:\n",
    "            payload = row.rstrip('\\n')\n",
    "            response = runtime_client.invoke_endpoint(EndpointName=ep_name,\n",
    "                                          ContentType='text/csv', \n",
    "                                          Body=payload)\n",
    "            time.sleep(1)\n",
    "            \n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        invoke_endpoint(endpoint_name, 'data/test-dataset-input-cols.csv', runtime_client)\n",
    "        \n",
    "thread = Thread(target = invoke_endpoint_forever)\n",
    "thread.start()\n",
    "\n",
    "# Note that you need to stop the kernel to stop the invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List executions\n",
    "Once the schedule is set up, jobs start at the specified intervals. The following code lists the last five executions. If you run this code soon after creating the hourly schedule, you might not see any executions listed. To see executions, you might have to wait until you cross the hour boundary (in UTC). The code includes the logic for waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "if len(mon_executions) == 0:\n",
    "    print(\"We created a hourly schedule above and it will kick off executions ON the hour.\\nWe will have to wait till we hit the hour...\")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the latest execution and list the generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = mon_executions[-1]\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()['ExitMessage']))\n",
    "report_uri = latest_execution.output.destination\n",
    "\n",
    "print(\"Found Report Files:\")\n",
    "S3Downloader.list(report_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List violations\n",
    "\n",
    "If there are any violations compared to the baseline, they will be generated here. Let's list the violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plug in the processing job arn for a single execution of the monitoring into this notebook to see more detailed visualizations of the violations and distribution statistics of the data captue that was processed in that execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "If you no longer need this notebook, clean up your environment by running the following cell. It removes the hosted endpoint that you created for this walkthrough and prevents you from incurring charges for running an instance that you no longer need. It also cleans up all artifacts related to the experiments. \n",
    "\n",
    "You might also want to delete artifacts stored in the S3 bucket used in this notebook. To do so, open the Amazon S3 console, find the `sagemaker-studio-<region-name>-<account-name>` bucket, and delete the files associated with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_monitoring_schedule(mon_schedule_name)\n",
    "sess.delete_endpoint(xgb_predictor.endpoint)\n",
    "def cleanup(experiment):\n",
    "    '''Clean up everything in the given experiment object'''\n",
    "    for trial_summary in experiment.list_trials():\n",
    "        trial = Trial.load(trial_name=trial_summary.trial_name)\n",
    "        \n",
    "        for trial_comp_summary in trial.list_trial_components():\n",
    "            trial_step=TrialComponent.load(trial_component_name=trial_comp_summary.trial_component_name)\n",
    "            print('Starting to delete TrialComponent..' + trial_step.trial_component_name)\n",
    "            sm.disassociate_trial_component(TrialComponentName=trial_step.trial_component_name, TrialName=trial.trial_name)\n",
    "            trial_step.delete()\n",
    "            time.sleep(1)\n",
    "         \n",
    "        trial.delete()\n",
    "    \n",
    "    experiment.delete()\n",
    "\n",
    "cleanup(customer_churn_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:environment/datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
