{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to training a model using data from AWS Data Exchange and an algorithm from AWS Marketplace\n",
    "\n",
    "We have a tendency to get attracted to certain fragrant elements or a combination of elements. These combination of elements play an important role in our psychophysiological activity as explained in this [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5198031/). Although each of us is unique and has unique fragrance preferences, there are certain combinations of elements that are widely popular. Understanding these combinations is quite important when you are creating products that would appeal to masses. Having a decision support system that can tell you whether products in a product line you wish to launch contain those elements or not can be immensly beneficial.\n",
    "\n",
    "Today, we will conduct an experiment to identify combination of elements that are widely popular. \n",
    "\n",
    "As part of our experiment, we would use product popularity dataset containing information of Bath and Body works products, a popular bath products company, and train a machine learning model.\n",
    "\n",
    "Our model would be based on two simple features:\n",
    "1. Name of the product\n",
    "2. Category of the product.\n",
    "\n",
    "For training a machine learning model, we would use a third-party decision forest classification algorithm.\n",
    "\n",
    "Note: You may use any algorithm supported by Amazon SageMaker for training a model. However, for this experiment, we would use a third-party algorithm from AWS Marketplace.\n",
    "\n",
    "\n",
    "### Contents:\n",
    "* [Pre-requisites](#Pre-requisites)\n",
    "* [Step 1: Export data from AWS Data Exchange to an Amazon S3 bucket](#Step-1:-Export-data-from-AWS-Data-Exchange-to-an-Amazon-S3-bucket)\n",
    "* [Step 2: Data analysis & Feature Engineering](#Step-2:-Data-analysis-&-Feature-Engineering)\n",
    "    * [Step 2.1: Remove unnecessary features](#Step-2.1:-Remove-unnecessary-features)\n",
    "    * [Step 2.2: Create the outcome variable](#Step-2.2:-Create-the-outcome-variable)\n",
    "    * [Step 2.3: Feature engineer categorical columns](#Step-2.3:-Feature-engineer-categorical-columns)\n",
    "* [Step 3: Train a machine learning model](#Step-3:-Train-a-machine-learning-model)\n",
    "    * [Step 3.1 Set up environment](#Step-3.1-Set-up-environment)\n",
    "    * [Step 3.2 Prepare input dataset](#Step-3.2-Prepare-input-dataset)\n",
    "    * [Step 3.3 Train a model](#Step-3.3-Train-a-model)\n",
    "    * [Step 3.4: Tune your model! (Optional)](#Step-3.4:-Tune-your-model!-(Optional))\n",
    "* [Step 4: Deploy model and verify results](#Step-4:-Deploy-model-and-verify-results)\n",
    "* [Step 5: Cleanup](#Step-5:-Cleanup)\n",
    "\n",
    "\n",
    "#### Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites:\n",
    "\n",
    "You need to provide following IAM permissions to the sagemaker execution role to run this notebook successfully.\n",
    "1. \"dataexchange:CreateJob\"\n",
    "2. \"dataexchange:StartJob\"\n",
    "3. \"dataexchange:GetJob\"\n",
    "4. \"dataexchange:ListRevisionAssets\"\n",
    "\n",
    "\n",
    "This sample notebook requires subscription to following entities Marketplace:\n",
    "1. A Dataset : [VK Retail Data Sets Trial product](https://console.aws.amazon.com/dataexchange/home?region=us-east-1#/products/prodview-gq5plolrup4va)\n",
    "2. An Algorithm : [Intel®DAAL DecisionForest Classification](https://aws.amazon.com/marketplace/pp/prodview-begzvcpjty3g2?qid=1569615711264&sr=0-2&ref_=srh_res_product_title)\n",
    "\n",
    "If your AWS account has not been subscribed to these listings, here is the process you can follow:\n",
    "\n",
    "\n",
    "#### Subscribe to data from AWS Data Exchange:\n",
    "\n",
    "1. Open the [VK Retail Data Sets Trial product](https://console.aws.amazon.com/dataexchange/home?region=us-east-1#/products/prodview-gq5plolrup4va) from AWS Data Exchange console\n",
    "2. Read the overview and other information such as pricing, usage, support. \n",
    "3. Choose __Continue to Subscribe__\n",
    "4. If your organization agrees to subscription terms, pricing information, and  Data subscription agreement, then review/update the renewal settings and choose __Subscribe__\n",
    "5. Once subscription has been successfully created (This step may take 5-10 minutes), you will find the dataset listed in the [__Subscriptions__](https://console.aws.amazon.com/dataexchange/home?region=us-east-1#/subscriptions) section of the console\n",
    "6. From [subscription page](https://console.aws.amazon.com/dataexchange/home?region=us-east-1#/subscriptions), open **Retail Data Sets (TRIAL)**,  and for this use-case, choose the __retail_trials-bathbodyworks__ dataset. This is the dataset we would use to train a machine learning model.\n",
    "\n",
    "\n",
    "#### Subscribe to algorithm from AWS Marketplace:\n",
    "1. Open the [Intel®DAAL DecisionForest Classification listing](https://aws.amazon.com/marketplace/pp/prodview-begzvcpjty3g2?qid=1569615711264&sr=0-2&ref_=srh_res_product_title) from AWS Marketplace\n",
    "2. Read the **Highlights** section and then **product overview** section of the listing.\n",
    "3. View **usage information** and then **additional resources**.\n",
    "4. Note the supported instance types.\n",
    "5. Next, click on **Continue to subscribe**.\n",
    "6. Review **End user license agreement**, **support terms**, as well as **pricing information**.\n",
    "7. **\"Accept Offer\"** button needs to be clicked if your organization agrees with EULA, pricing information as well as support terms.\n",
    "\n",
    "**Notes**: \n",
    "1. If **Continue to configuration** button is active, it means your account already has a subscription to this listing.\n",
    "2. Once you click on **Continue to configuration** button and then choose region, you will see that a **Product Arn** will appear. This is the model package ARN that you need to specify while creating a deployable model. However, for this notebook, the algorithm ARN has been specified in **src/model_package_arns.py** file and you do not need to specify the same explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! you are now ready to import the data from AWS Data Exchange to your S3 bucket and train a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries.\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter,ContinuousParameter,CategoricalParameter\n",
    "\n",
    "import boto3\n",
    "import sagemaker as sage\n",
    "from sagemaker import AlgorithmEstimator\n",
    "from sagemaker.amazon.amazon_estimator import RecordSet\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from src.algorithm_arns import AlgorithmArnProvider\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "#Download necessary libraries.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define common variables.\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#NLP specific variables\n",
    "stop_words = stopwords.words('english') \n",
    "ps = PorterStemmer() \n",
    "\n",
    "#visualization variables\n",
    "palette=sns.color_palette(\"RdBu\", n_colors=7)\n",
    "\n",
    "#Amazon SageMaker interaction variables\n",
    "region_name = boto3.Session().region_name\n",
    "bucket_name=sage.Session().default_bucket()\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Export data from AWS Data Exchange to an Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we copy the data from AWS Data Exchange into our S3 bucket. Once subscription to the dataset has been created, you can open the [subscription](https://console.aws.amazon.com/junto/home?region=us-east-1#/subscriptions/prodview-gq5plolrup4va). Choose [retail_trials-bathbodyworks](https://console.aws.amazon.com/junto/home?region=us-east-1#/subscriptions/prodview-gq5plolrup4va/data-sets/b2ef8479168ba3d93979a779431fecd0) and review the value of **dataset_id**. Choose **Revisions** tab and review revision_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare Dataset specific variables\n",
    "dataset_id='b2ef8479168ba3d93979a779431fecd0'\n",
    "revision_id='1bb695f6aa40aa7ba0d234e849dafcf3'\n",
    "\n",
    "assets=[]\n",
    "dataexchange=boto3.client(service_name='dataexchange',region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An asset in AWS Data Exchange is a piece of data that can be stored as an Amazon S3 object. A revision of a dataset contains one or more assets.\n",
    "In this step, we will list the assets part of the specified revision, put them in an array, and then run a job which would export the assets to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_assets_response = dataexchange.list_revision_assets(\n",
    "    DataSetId=dataset_id,\n",
    "    RevisionId=revision_id\n",
    ")\n",
    "#Add asset-ids to an array\n",
    "for asset in list_assets_response['Assets']:\n",
    "    assets.append({'AssetId':asset['Id'],'Bucket': bucket_name})\n",
    "\n",
    "#Next, create a job that exports the data from AWS Data Exchange to Amazon S3\n",
    "create_job_response = dataexchange.create_job(\n",
    "    Details={\n",
    "        'ExportAssetsToS3': {\n",
    "            'AssetDestinations': assets,\n",
    "            'DataSetId': dataset_id,\n",
    "            'RevisionId': revision_id\n",
    "        }\n",
    "    },\n",
    "    Type='EXPORT_ASSETS_TO_S3'\n",
    ")\n",
    "job_id=create_job_response['Id']\n",
    "\n",
    "#Trigger the job.\n",
    "dataexchange.start_job(JobId=job_id)\n",
    "#Wait while import job runs \n",
    "\n",
    "max_time = time.time() + 60*60 # 1 hour\n",
    "while time.time() < max_time:\n",
    "    response = dataexchange.get_job(JobId=job_id);\n",
    "    status = response['State']\n",
    "    print('get_job_status'+\": {}\".format(status))\n",
    "    if status == \"COMPLETED\" or status == \"ERROR\":\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that data is available in S3, let us download it to our notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data analysis & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive s3://$bucket_name/'retail_trials/bathbodyworks' ./data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are two types of files in the dataset. One is a product file and another is a variants file. Let us take a look at each of these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/raw/products-2018-01-15.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are multiple columns in the file which are useful. Number of reviews as well as average rating is useful infromation. Also, the difference between first_discovered and last_discovered can be used to see how long the product lasted in the market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the product name itself also contains the sub-category information as well. Sub-category would make a relevant feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/raw/products-2018-01-08.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at another file from the dataset shows the trend available, i.e. whether number of reviews/rating went up for the product or not. We are not interested in the trend but how long did the product stay in the market and how popular it became. Which is why we are interested only in the latest numbers available for the product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us analyze a sample variants file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/raw/variants-2018-01-15.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is additional information available in this file that we could use, such as promotional text, price, and size of the product. These features can potentially improve the model.\n",
    "\n",
    "However, our experiment setup is  and we will limit our features to name and category. We will not use variant files for our experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/raw/variants-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us load these files into a pandas dataframe and keep interesting attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function combines CSV files into a single dataframe and adds an additional feature \n",
    "#with name extract-date based on the date available in the file's name.\n",
    "def read_csv_files_from_folder(location):\n",
    "    \n",
    "    df_list = []\n",
    "    files = glob.glob(location + \"/*.csv\")\n",
    "    \n",
    "    #Read each file into a dataframe and then add the dataframe to the list\n",
    "    for filename in files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        df['extract-date']=pd.to_datetime(filename.replace('data/raw/products-', '').replace('.csv', ''))\n",
    "        df_list.append(df)\n",
    "    \n",
    "    #Concatenate the list of dataframes.\n",
    "    frame = pd.concat(df_list, axis=0, ignore_index=True, sort=False)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=read_csv_files_from_folder('data/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Remove unnecessary features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us create a new column that indicates how long did the product last in the market\n",
    "df['to']=pd.to_datetime(df['last_discovered'])\n",
    "df['from']=pd.to_datetime(df['first_discovered'])\n",
    "df['lasted_for']=(df['to'] - df['from'] ).dt.days\n",
    "\n",
    "#Let us drop all unnecessary columns\n",
    "df.drop(['id','brand_name','site_url','first_discovered','last_discovered','to','from','external_id','extract-date'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Products\")\n",
    "print(df.shape)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, let us keep only the latest stats available for a product. \n",
    "df.drop_duplicates(subset='name', keep='last',inplace=True)\n",
    "\n",
    "#Drop rows containing data with products that lasted for 0 days as the data might be erroneous.\n",
    "df.drop(df[(df.lasted_for == 0) & (df.review_count==0)].index, inplace=True)\n",
    "\n",
    "#Print percent missing values.\n",
    "print(\"Missing data before removing the missing category data\")\n",
    "print((df.isna().sum()/df.shape[0])*100)\n",
    "\n",
    "#Missing category data is ~0.074%. Let us drop the missing category data.\n",
    "df.drop(df[df['category'].isna()].index, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True)\n",
    "\n",
    "#Print percent missing values.\n",
    "print()\n",
    "print(\"Missing data\")\n",
    "(df.isna().sum()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average_rating column has missing information, and we have multiple columns which indicate popularity. We will address missing data in average_rating column while creating the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data looks much better! We will do the feature engineering for categorical columns after we define the outcome variable.\n",
    "\n",
    "### Step 2.2: Create the outcome variable\n",
    "\n",
    "Our goal is to determine popularity based on review_count, lasted_for, average_rating, which are indicators of popularity. Let us analyze these and create an outcome variable .\n",
    "\n",
    "#### Analyze review count for products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_count_threshold_percentile=df['review_count'].quantile(0.70)\n",
    "review_count_50_percentile=df['review_count'].quantile(0.50)\n",
    "\n",
    "print(df['review_count'].quantile([0.01,.1, 0.25, .5,0.70,0.75,0.9,0.99]))\n",
    "sns.set(rc={'figure.figsize':(14,1.27)})\n",
    "sns.set(style=\"whitegrid\")\n",
    "bplot = sns.boxplot(x=df['review_count'],orient ='h', palette=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze how long did the products last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasted_for_threshold_percentile=df['lasted_for'].quantile(0.70)\n",
    "lasted_for_50_percentile=df['lasted_for'].quantile(0.50)\n",
    "print(df['lasted_for'].quantile([0.01,.1, 0.25, .5,0.70,0.75,0.9,0.99]))\n",
    "\n",
    "bplot = sns.boxplot(x=df['lasted_for'],orient ='h',palette=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze product review ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_threshold_percentile=df['average_rating'].quantile(0.70)\n",
    "print(df['average_rating'].quantile([0.01,.1, 0.25, .5,0.70,0.75,0.9,0.99]))\n",
    "\n",
    "bplot = sns.boxplot(x=df['average_rating'],orient ='h',palette=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict whether based on the name, the product will become popular or not. Let us define popularity_status using following rules:\n",
    "1. Product is __popular__ if:\n",
    "\n",
    "    A. The product's number of reviews are higher than __70%__ of the products OR   \n",
    "    \n",
    "    B. It lasted longer than __70%__ of the products OR\n",
    "    \n",
    "    C. It's rating is higher than __70%__ of the products and lasted longer than __50% products__ with number of reviews higher than __50% products__. \n",
    "    \n",
    "\n",
    "2. Else:\n",
    "    Product is __not popular__.\n",
    "    \n",
    "With this criteria, the problem becomes a simple binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method accepts a row and returns the outcome value based on rules defined in the \n",
    "#previous cell.\n",
    "def is_popular(row):\n",
    "    if  row['lasted_for'] > lasted_for_threshold_percentile:\n",
    "        return 'popular'\n",
    "    \n",
    "    if  row['review_count'] > review_count_threshold_percentile:\n",
    "        return 'popular'\n",
    "\n",
    "#note that the sequence of evaluation starts from the left most predicate. \n",
    "#If the review_count and lasted for predicate are false, and average_rating is NaN,\n",
    "#then we automatically mark the product as not being popular.\n",
    "  \n",
    "    if (row['review_count'] > review_count_50_percentile \\\n",
    "        and row['lasted_for'] > lasted_for_50_percentile \\\n",
    "        and not math.isnan(row['average_rating']) \\\n",
    "        and (row['average_rating']>=rating_threshold_percentile)):\n",
    "        return 'popular'\n",
    "    \n",
    "    return 'not_popular'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label']=df.apply(lambda row:is_popular(row),axis=1)\n",
    "\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we have a clear outcome variable, we dont need the following three variables anymore.\n",
    "#Let us drop review_count, lasted_for, and average_rating features from the dataframe.\n",
    "\n",
    "df.drop(['review_count','lasted_for','average_rating'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us perform feature engineering on categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Feature engineer categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a product-length feature \n",
    "Let us create an additional feature that contains the length of the original name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length']=df['name'].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the data in newly created column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(14,1.27)})\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.boxplot(y='label',x='length',data=df, orient ='y',order=['popular','not_popular'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply by looking at the data we can see that 75% of most popular products have names less than 6 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the 'name' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a quick look at the dataframe, we can clearly see that last few words are repeating indicating presence of a subcatgory. Let us clean the name feature and then extract a subcategory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2words = {1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five', \\\n",
    "             6: 'six', 7: 'seven', 8: 'eight', 9: 'nine'}\n",
    "#The following method accepts a text and performs following tasks:\n",
    "#1. Convert text into lowercase.\n",
    "#2. Replaces dashes.\n",
    "#3. Removes all special characters.\n",
    "#4. Removes any words that contain less than 3 characters or is a stopword.\n",
    "#5. Creates stem of each word using porterstemmer algorithm.\n",
    "#6. Convert numbers into their word representations\n",
    "def clean_text(text):\n",
    "    \n",
    "    text=text.lower().strip()\n",
    "    \n",
    "    text = re.sub('-',' ', text)\n",
    "\n",
    "    text = re.sub('[^A-Za-z0-9 ]+','', text)\n",
    "    \n",
    "    tokenized_name=nltk.word_tokenize(text)\n",
    "    \n",
    "    clean_words=[]\n",
    "    \n",
    "    for word in tokenized_name:\n",
    "        stem=ps.stem(word) \n",
    "        if ((stem not in stop_words) and (len(stem)>=3)):\n",
    "            clean_words.append(stem)  \n",
    "        elif(stem.isnumeric()):\n",
    "            if  (int(stem) in num2words):\n",
    "                clean_words.append(num2words[int(stem)])  \n",
    "            \n",
    "    return clean_words\n",
    "\n",
    "#Let us perform a test!\n",
    "#print(clean_text('Vanilla Spiced Pear Wallflowers Fragrance Refill 3-wick candle'))\n",
    "#print(clean_text('Mahogany Apple 3-Wick Candle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us create a new column that contains clean representations\n",
    "df['name']=df['name'].apply(lambda x:clean_text(x))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create subcategory feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned the 'name' column, we can create a sub-category column based on number of occurances of a suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the length of the 'name' feature, emit one or more suffixes to identify the \n",
    "#popular sub-categories.\n",
    "\n",
    "#This function accepts dataframe and extracts sub-categories from the name.\n",
    "def get_sub_categories(df):\n",
    "    \n",
    "    potential_sub_categories=[]\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        name=row['name']\n",
    "        sub_category=''\n",
    "    \n",
    "        if(row['length']>=4):\n",
    "            potential_sub_categories.append(' '.join(name[-3:]).strip())\n",
    "            potential_sub_categories.append(' '.join(name[-2:]).strip())\n",
    "            potential_sub_categories.append(' '.join(name[-1:]).strip())\n",
    "        \n",
    "        elif(row['length']>2):\n",
    "            potential_sub_categories.append(' '.join(name[-2:]).strip())\n",
    "            potential_sub_categories.append(' '.join(name[-1:]).strip())\n",
    "        \n",
    "        elif(row['length']==2):\n",
    "            potential_sub_categories.append(' '.join(name[-1:]).strip())\n",
    "    \n",
    "    #For this experiment, we would consider only those words that have occured fifteen\n",
    "    #or more times, as valid subcategories.\n",
    "    \n",
    "    sub_category_counts=pd.Index(potential_sub_categories).value_counts()>15\n",
    "    sub_categories=sub_category_counts[sub_category_counts].index\n",
    "    \n",
    "    #Print size and a few sample categories from the list.\n",
    "    print(len(sub_categories))\n",
    "    print(sub_categories)\n",
    "    \n",
    "    return sub_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "sub_categories=get_sub_categories(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a quick look, these look like real sub-categories. Let us remove these from the name and populate it in a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, let us create additional features such as length and a sub-category. The preference is \n",
    "#given to the longest sub-category if more than one sub-category has been found in the name.\n",
    "\n",
    "def set_sub_category_and_name(row):\n",
    "\n",
    "    name=row['name']\n",
    "    length=row['length']\n",
    "    \n",
    "    if(length>=4):\n",
    "        sub_category = ' '.join(name[-3:]).strip()\n",
    "        if sub_category in sub_categories:\n",
    "            row['sub_category']=sub_category.replace(\" \",\"_\")\n",
    "            row['name']= name[:-3]\n",
    "            return row\n",
    "    \n",
    "    if (length>=3):\n",
    "        sub_category = ' '.join(name[-2:]).strip()\n",
    "\n",
    "        if sub_category in sub_categories:\n",
    "            row['sub_category']=sub_category.replace(\" \",\"_\")\n",
    "            row['name']= name[:-2]\n",
    "            return row\n",
    "    \n",
    "    if (length>=2):\n",
    "        sub_category = ' '.join(name[-1:]).strip()\n",
    "        if sub_category in sub_categories:\n",
    "            row['sub_category']=sub_category.replace(\" \",\"_\")\n",
    "            row['name']= name[:-1]\n",
    "            return row\n",
    "    return row\n",
    "#row={'name':['mahogani','appl','three','wick','candl'],'length':5}\n",
    "#print(set_sub_category_and_name(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.apply(lambda row: set_sub_category_and_name(row),axis=1) \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df['label'],orient ='h',hue=df['category'],order=['popular','not_popular'])\n",
    "sns.set(rc={'figure.figsize':(14,8.27)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick analysis based on the data:\n",
    "1. We can clearly see that specific categories such as __Body care__ do predictably better than __Home fragrance__. \n",
    "2. We can also see that __Gifts__ category is less popular which might be due to seasonal aspect associated with it. \n",
    "3. We can also see that __Hidden categories__ do somewhat well.\n",
    "3. We can also see that __Hand Soaps__ do not seem to perform that well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that products with shorter names tend to do better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing data\")\n",
    "print((df.isna().sum()/df.shape[0])*100)\n",
    "\n",
    "df=df.fillna(\"None\")\n",
    "print((df.isna().sum()/df.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us perform one-hot encoding for category and sub_category features\n",
    "df = pd.concat([df,pd.get_dummies(df['category'], prefix='category')],axis=1)\n",
    "df = pd.concat([df,pd.get_dummies(df['sub_category'], prefix='sub_category')],axis=1)\n",
    "\n",
    "# drop original columns\n",
    "df.drop(['sub_category','category'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would be generating embeddings for product names using Machine learning. Before we do so, let us split the data into two sets: Train & Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us split the dataset into train and test\n",
    "np.random.seed(0)\n",
    "\n",
    "#Shuffle the dataset.\n",
    "df=df.sample(frac=1)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create embeddings for the name column using training dataset. Embeddings is a language modelling technique of mapping words or phrases to vectors of real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create embeddings for name using train dataset.\n",
    "labels={\"popular\":1,\"not_popular\":0}\n",
    "documents =[]\n",
    "for index,row in train.iterrows():\n",
    "    name=row['name']\n",
    "    documents.append(TaggedDocument(words=name, tags=[labels.get(row['label'])]))\n",
    "\n",
    "model = Doc2Vec(vector_size=12, min_count=4, window=2,dbow_words=1)\n",
    "model.build_vocab(documents)\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.infer_vector(['blossom','shea','butter']))\n",
    "print(model.wv.most_similar(['orang']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function sets embeddings for the row in the dataframe\n",
    "def set_name_embeddings(row):\n",
    "    #Extract embedding\n",
    "    embedding= model.infer_vector(name)\n",
    "    i=0\n",
    "    #Set the column with value based on the embedding\n",
    "    for entry in embedding:\n",
    "        row['name_'+str(i)]=entry\n",
    "        i=i+1\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.apply(lambda row: set_name_embeddings(row),axis=1) \n",
    "test=test.apply(lambda row: set_name_embeddings(row),axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have embeddings, we dont need the original \"name\" feature anymore, lets drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['name',],axis=1, inplace=True)\n",
    "test.drop(['name',],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "#Add class as the label-encoded column to the dataframe and drop the original label column.\n",
    "train['class'] = le.fit_transform(train['label'])\n",
    "test['class'] = le.transform(test['label'])\n",
    "\n",
    "train.drop(['label'],axis=1, inplace=True)\n",
    "test.drop(['label'],axis=1, inplace=True)\n",
    "\n",
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset is ready and all columns are in numeric format, we are ready to train a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries and initialize variables\n",
    "sagemaker_session = sage.Session()\n",
    "bucket=sagemaker_session.default_bucket()\n",
    "role = get_execution_role()\n",
    "output_location = 's3://{}/{}'.format(bucket, 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 Prepare input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us split the dataset into train and test and upload the training data to an S3 bucket.\n",
    "file='data/training.csv'\n",
    "\n",
    "np.savetxt(file,train,delimiter=',')\n",
    "\n",
    "data=sagemaker_session.upload_data(file, bucket=bucket, key_prefix='data_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3 Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "hyperparameters={\"nClasses\": 2, \\\n",
    "                 \"nTrees\":571,\\\n",
    "                 \"maxTreeDepth\":13,\\\n",
    "                 \"varImportance\":\"none\",\\\n",
    "                 \"resultsToCompute\":\"computeOutOfBagError\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us load the algorithm's ARN into a variable.\n",
    "algo_arn = AlgorithmArnProvider.get_decision_forest_algorithm_arn(region_name)\n",
    "algo_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an estimator object for running a training job\n",
    "estimator = sage.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=algo_arn,\n",
    "    base_job_name=\"daal-decision-forest\",\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.2xlarge',\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters=hyperparameters\n",
    ")\n",
    "#Run the training job.\n",
    "estimator.fit({\"training\": data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an experiment,, you do not need to run a hyperparameter tuning job. However, if you would like to see how to tune a model trained using a third-party algorithm with Amazon SageMaker's hyperparameter tuning functionality, you can run the optional tuning step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: Tune your model! (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Algorithm's [product detail page](https://aws.amazon.com/marketplace/pp/prodview-begzvcpjty3g2) specifies that **minObservationsInLeafNode**, **maxTreeDepth**, **nTrees**, and **minObservationsInLeafNode** are tunable parameters supported. Let us specify ranges for these parameters and perfom hyperparameter tuning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'maxTreeDepth':IntegerParameter(10, 500),'nTrees':IntegerParameter(100, 1000),'featuresPerNode':IntegerParameter(10, 110)}\n",
    "\n",
    "tuner = HyperparameterTuner(estimator=estimator, base_tuning_job_name='decision-forest',\n",
    "                                objective_metric_name='OutOfBagError',\n",
    "                            objective_type='Minimize',\n",
    "                                hyperparameter_ranges=hyperparameter_ranges,\n",
    "                                max_jobs=50, max_parallel_jobs=7)\n",
    "\n",
    "#Uncomment following two lines to run Hyperparameter optimization job. \n",
    "#tuner.fit({'training':  data})\n",
    "#tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deploy model and verify results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us deploy the model for performing real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate metrics such as accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features from the test dataset\n",
    "features_test = np.array(test.drop([\"class\"], axis=1)).astype(\"float32\")\n",
    "\n",
    "#perform prediction on the features \n",
    "prediction = predictor.predict(features_test).decode('utf-8')\n",
    "\n",
    "#Extract predictions and put them into an Array\n",
    "predicted=[]\n",
    "\n",
    "predictions = np.fromstring(prediction, dtype=np.float64, sep=' ').reshape(1,features_test.shape[0])[0]\n",
    "\n",
    "for pred in predictions:\n",
    "    predicted.append(int(pred))\n",
    "\n",
    "#Extract labels from the test dataset\n",
    "actual = np.array(test[\"class\"]).astype(\"float32\")\n",
    "\n",
    "#Print metric\n",
    "print(\"Accuracy on test data: \", str(accuracy_score(actual, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform_inference method accepts name and category and converts them into the \n",
    "#payload format supported by model. \n",
    "#To do so, it performs following steps:\n",
    "# 1. Extracts length feature\n",
    "# 2. cleans the name \n",
    "# 3. extracts sub-category and cleaned name\n",
    "# 4. sets one-hot encoded category & subcategory column to 1.\n",
    "# 5. Sets embeddings for the name column.\n",
    "# 6. Performs prediction \n",
    "# 7. Returns label -  popular/not_popular\n",
    "def perform_inference(name,category):\n",
    "    row={}\n",
    "    \n",
    "    row[\"length\"]=len(name.split())\n",
    "    \n",
    "    row[\"name\"]=clean_text(name)\n",
    "\n",
    "    row['category_'+category]=1\n",
    "    \n",
    "    row = set_sub_category_and_name(row)\n",
    "    if 'sub_category' in row:\n",
    "        row['sub_category_'+row['sub_category']]=1\n",
    "        del row[\"sub_category\"]\n",
    "    else:\n",
    "        row[\"sub_category_None\"]=1\n",
    "    \n",
    "    row=set_name_embeddings(row)\n",
    "    del row[\"name\"]\n",
    "    \n",
    "    df_infer = pd.DataFrame(data=None, columns=df.columns)\n",
    "    df_infer=df_infer.append(row,ignore_index=True)\n",
    "    df_infer=df_infer.fillna(0)\n",
    "    features = df_infer.values.astype(\"float64\")\n",
    "        \n",
    "    prediction = predictor.predict(features).decode('utf-8')\n",
    "    \n",
    "    prediction_label = int(np.fromstring(prediction, dtype=np.float64, sep=' ')[0])\n",
    "\n",
    "    print(le.inverse_transform([prediction_label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_inference(\"Ginger orange 3-Wick Candle\",\"Home Fragrance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just used data available from AWS Data Exchange to train a machine learning model that can predict popularity of a product based on its name within the category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "predictor.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if the AWS Marketplace subscription was created just for the experiment and you would like to unsubscribe to the product, here are the steps that can be followed.\n",
    "Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to un-subscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=lbr_tab_ml)\n",
    "2. Locate the listing that you would need to cancel subscription for, and then __Cancel Subscription__ can be clicked to cancel the subscription.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
