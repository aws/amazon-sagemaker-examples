{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker - Tensorflow 2.x\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a machine learning platform to build, train, and host state-of-the-art ML and AI models. [Amazon SageMaker Debugger](https://github.com/awslabs/sagemaker-debugger) offers the capability to debug machine learning models during training and identifies problems with the models in real-time.\n",
    "\n",
    "Experimental support for TF 2.x was initially introduced in v0.7.1 of the Debugger's smdebug library. A full description of support is available at [Amazon SageMaker Debugger with TensorFlow](https://github.com/awslabs/sagemaker-debugger/tree/master/docs/tensorflow.md)\n",
    "\n",
    "With the smdebug v0.9.0 release, its support has been extended to cover TF 2.x [model_to_estimator](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator) and Estimator APIs\n",
    "\n",
    "In this notebook, you will learn how to run your training job with the TF 2.x [model_to_estimator](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator) API and the Debugger built-in rules to watch training anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training TensorFlow Keras models with Amazon SageMaker Debugger\n",
    "\n",
    "### Amazon SageMaker TensorFlow as a framework\n",
    "\n",
    "We will train a TensorFlow Keras model in this notebook with Amazon Sagemaker Debugger, and monitor the training jobs with the Debugger built-in rules. The training job will be run on a pre-built [AWS Deep Learning Container](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html) with Tensorflow 2.1.0 and smdebug 0.9.0 installed.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Execute the following code cell for a one-time smdebug setup to get your notebook kernel ready for a full experience of using the Debugger features. This smdebug library provides tools to perform interactive analysis throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install smdebug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the AWS boto3 Python SDK, the SageMaker Python SDK, the SageMaker TensorFlow class, and other python utility libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the SageMaker Debugger classes for configuring hooks and rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, DebuggerHookConfig, TensorBoardOutputConfig, CollectionConfig, rule_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the entry point for the training script.\n",
    "\n",
    "Since our demo training script `tf_keras_to_estimator.py` uses the `tensorflow-datasets` package that is not available in the pre-built container, we need to install it as a requirement for the training job. To add the installation step, we simply add the package in a `requirements.txt` file. You can add a list of required libraries to run your own training script in the same way.\n",
    "\n",
    "The `launcher.sh` script is provided along with this notebook, which will install the `tensorflow-datasets` package and initiate the training script. For more information about installing third-party libraries, see [Use third-party libraries with SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#use-third-party-libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the entrypoint script\n",
    "entrypoint_script='launcher.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the SageMaker TensorFlow Estimator\n",
    "\n",
    "Now it's time to setup the SageMaker TensorFlow estimator. We will add the following Debugger-specific parameters to the estimator to enable for debugging the demo training script.\n",
    "\n",
    "**debugger_hook_config**: This new parameter accepts a local path where you wish your tensors to be written to and also accepts the S3 bucket to store tensors. Debugger will take care of uploading these tensors transparently during execution.\n",
    "**rules**: This rules parameter will accept a list of rules you want to evaluate the tensors output and training behaviors while the training job is running. There are two types of Debugger rules:\n",
    "**Built-in Rules**: These are rules specially curated by the Amazon SageMaker Debugger which you can opt to evaluate your training job.\n",
    "**Custom Rules**: You can write your own rules as a Python source file and use them to evaluate your training job. To provide Amazon SageMaker Debugger to evaluate this rule, you have to provide the S3 location of the rule source and the evaluator image. For more information about how to create and use Debugger custom rules, see [Create Debugger Custom Rules for Training Job Analysis](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-custom-rules.html).\n",
    " \n",
    "### Using Amazon SageMaker Rules\n",
    " \n",
    "In this notebook, we will demonstrate how to use SageMaker built-in rules to evaluate the training script. You can find the list of SageMaker rules and the configurations best suited for using them [here](https://github.com/awslabs/sagemaker-debugger-rulesconfig).\n",
    "\n",
    "The rules we are using in this notebook are **VanishingGradient** and **LossNotDecreasing**. As the names suggest, the rules will attempt to evaluate if there are vanishing gradients in the tensors captured by the debugging hook during training and also if the loss is not decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()), \n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script expects an argument \"out_dir\" which specifies where SageMaker Debugger should save the tensors. This will be passed to the script throught the Estimator API's hyperparameters argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"out_dir\": \"/opt/ml/output/tensors\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create the estimator and call `fit()` on our estimator to start the training job and rule evaluation job in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator = TensorFlow(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    base_job_name='smdebug-tf2-model-to-estimator',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    entry_point=entrypoint_script,\n",
    "    source_dir=\"src\",\n",
    "    framework_version='2.1.0',\n",
    "    train_max_run=3600,\n",
    "    script_mode=True,\n",
    "    py_version='py3',\n",
    "    hyperparameters=hyperparameters,\n",
    "    ## New parameter\n",
    "    rules = rules\n",
    ")\n",
    "\n",
    "# After calling fit, Amazon SageMaker starts one training job and one rule job for you.\n",
    "# The rule evaluation status is visible in the training logs\n",
    "# at regular intervals\n",
    "\n",
    "estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result \n",
    "\n",
    "As a result of calling the `fit(wait=False)`, two jobs were kicked off in the background. Amazon SageMaker Debugger kicked off a rule evaluation job for our custom gradient logic in parallel with the training job. You can review the status of the above rule job as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "status = estimator.latest_training_job.rule_job_summary()\n",
    "while status[0]['RuleEvaluationStatus'] == 'InProgress':\n",
    "    status = estimator.latest_training_job.rule_job_summary()\n",
    "    print(status)\n",
    "    time.sleep(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the rule job starts, this will return RuleEvaluationJabArn values. We can see the logs for the rule job in CloudWatch. To do that, we will use the following utlity functions to get the CloudWatch link to the rule job logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VanishingGradient': 'https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=smdebug-tf2-model-to-estim-VanishingGradient-a81f7777;streamFilter=typeLogStreamPrefix',\n",
       " 'LossNotDecreasing': 'https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=smdebug-tf2-model-to-estim-LossNotDecreasing-85e1058d;streamFilter=typeLogStreamPrefix'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_rule_job_name(training_job_name, rule_configuration_name, rule_job_arn):\n",
    "        \"\"\"Helper function to get the rule job name with correct casing\"\"\"\n",
    "        return \"{}-{}-{}\".format(\n",
    "            training_job_name[:26], rule_configuration_name[:26], rule_job_arn[-8:]\n",
    "        )\n",
    "    \n",
    "def _get_cw_url_for_rule_job(rule_job_name, region):\n",
    "    return \"https://{}.console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\".format(region, region, rule_job_name)\n",
    "\n",
    "\n",
    "def get_rule_jobs_cw_urls(estimator):\n",
    "    training_job = estimator.latest_training_job\n",
    "    training_job_name = training_job.describe()[\"TrainingJobName\"]\n",
    "    rule_eval_statuses = training_job.describe()[\"DebugRuleEvaluationStatuses\"]\n",
    "    \n",
    "    result={}\n",
    "    for status in rule_eval_statuses:\n",
    "        if status.get(\"RuleEvaluationJobArn\", None) is not None:\n",
    "            rule_job_name = _get_rule_job_name(training_job_name, status[\"RuleConfigurationName\"], status[\"RuleEvaluationJobArn\"])\n",
    "            result[status[\"RuleConfigurationName\"]] = _get_cw_url_for_rule_job(rule_job_name, boto3.Session().region_name)\n",
    "    return result\n",
    "\n",
    "get_rule_jobs_cw_urls(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis - Interactive Exploration\n",
    "\n",
    "Now that we have trained a job and looked at automated Debugger analysis through its rules. Now let's have a look at another aspect of Amazon SageMaker Debugger. It allows us to perform interactive exploration of the tensors saved in real time or after the job has finished. Here we focus on after-the-fact analysis of the above job. We import the `smdebug` library to use its tools to create trials that picks up the most recent training job. The following code cells show you how to fetch the path to the Debugger artifacts of the latest training job and access the saved tensors by collection names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-06-25 20:19:48.381 ip-172-16-189-249:1818 INFO s3_trial.py:42] Loading trial debug-output at path s3://sagemaker-us-east-1-920076894685/smdebug-tf2-model-to-estimator-2020-06-25-20-02-23-993/debug-output\n"
     ]
    }
   ],
   "source": [
    "from smdebug.trials import create_trial\n",
    "trial = create_trial(estimator.latest_job_debugger_artifacts_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `smdebug` Trial class provides tools to parse the saved tensor based on names which are auto-assigned by TensorFlow. In other frameworks, the tensor names may vary and we have to use appropriate tensor names or regex based on the names of tensors such as weight, bias, gradient, input or output.\n",
    "\n",
    "For simple examples of fetching the tensors in the following code cells, we print the total number of tensors saved for losses, weights, and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-06-25 20:19:50.662 ip-172-16-189-249:1818 INFO trial.py:198] Training has ended, will refresh one final time in 1 sec.\n",
      "[2020-06-25 20:19:51.685 ip-172-16-189-249:1818 INFO trial.py:210] Loaded all steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trial.tensor_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.tensor_names(collection=\"losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trial.tensor_names(collection=\"weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trial.tensor_names(collection=\"gradients\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
