{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGluon-Tabular in AWS Marketplace\n",
    "\n",
    "[AutoGluon](https://github.com/awslabs/autogluon) automates machine learning tasks enabling you to easily achieve strong predictive performance in your applications. With just a few lines of code, you can train and deploy high-accuracy deep learning models on tabular, image, and text data.\n",
    "This notebook shows how to use AutoGluon-Tabular in AWS Marketplace.\n",
    "\n",
    "### Contents:\n",
    "* [Step 1: Subscribe to AutoML algorithm from AWS Marketplace](#Step-1:-Subscribe-to-AutoML-algorithm-from-AWS-Marketplace)\n",
    "* [Step 2: Set up environment](#Step-2-:-Set-up-environment)\n",
    "* [Step 3: Prepare and upload data](#Step-3:-Prepare-and-upload-data)\n",
    "* [Step 4: Train a model](#Step-4:-Train-a-model)\n",
    "* [Step 5: Deploy the model and perform a real-time inference](#Step-5:-Deploy-the-model-and-perform-a-real-time-inference)\n",
    "* [Step 6: Use Batch Transform](#Step-6:-Use-Batch-Transform)\n",
    "* [Step 7: Clean-up](#Step-7:-Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Subscribe to AutoML algorithm from AWS Marketplace\n",
    "\n",
    "1. Open [AutoGluon-Tabular listing from AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-n4zf5pmjt7ism)\n",
    "2. Read the **Highlights** section and then **product overview** section of the listing.\n",
    "3. View **usage information** and then **additional resources**.\n",
    "4. Note the supported instance types and specify the same in the following cell.\n",
    "5. Next, click on **Continue to subscribe**.\n",
    "6. Review **End user license agreement**, **support terms**, as well as **pricing information**.\n",
    "7. Next, \"Accept Offer\" button needs to be clicked only if your organization agrees with EULA, pricing information as well as support terms. Once **Accept offer** button has been clicked, specify compatible training and inference types you wish to use. \n",
    "\n",
    "**Notes**: \n",
    "1. If **Continue to configuration** button is active, it means your account already has a subscription to this listing.\n",
    "2. Once you click on **Continue to configuration** button and then choose region, you will see that a product ARN will appear. This is the algorithm ARN that you need to specify in your training job. However, for this notebook, the algorithm ARN has been specified in **src/algorithm_arns.py** file and you do not need to specify the same explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role, local, Model, utils, fw_utils, s3\n",
    "from sagemaker.algorithm import AlgorithmEstimator\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import StringDeserializer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Print settings\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 10)\n",
    "\n",
    "# Account/s3 setup\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"sagemaker/autogluon-tabular\"\n",
    "region = session.boto_region_name\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compatible_training_instance_type = \"ml.m5.4xlarge\"\n",
    "compatible_inference_instance_type = \"ml.m5.4xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify algorithm ARN for AutoGluon-Tabular from AWS Marketplace.  However, for this notebook, the algorithm ARN\n",
    "# has been specified in src/algorithm_arns.py file and you do not need to specify the same explicitly.\n",
    "\n",
    "from src.algorithm_arns import AlgorithmArnProvider\n",
    "\n",
    "algorithm_arn = AlgorithmArnProvider.get_algorithm_arn(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(\"apt-get update -y\", shell=True)\n",
    "subprocess.run(\"apt install unzip\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get the data\n",
    "\n",
    "In this example we'll use the direct-marketing dataset to build a binary classification model that predicts whether customers will accept or decline a marketing offer.  \n",
    "First we'll download the data and split it into train and test sets. AutoGluon does not require a separate validation set (it uses bagged k-fold cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the data\n",
    "subprocess.run(\n",
    "    f\"aws s3 cp --region {region} s3://sagemaker-sample-data-{region}/autopilot/direct_marketing/bank-additional.zip .\",\n",
    "    shell=True,\n",
    ")\n",
    "subprocess.run(\"unzip -qq -o bank-additional.zip\", shell=True)\n",
    "subprocess.run(\"rm bank-additional.zip\", shell=True)\n",
    "\n",
    "local_data_path = \"./bank-additional/bank-additional-full.csv\"\n",
    "data = pd.read_csv(local_data_path)\n",
    "\n",
    "# Split train/test data\n",
    "train = data.sample(frac=0.7, random_state=42)\n",
    "test = data.drop(train.index)\n",
    "\n",
    "# Split test X/y\n",
    "label = \"y\"\n",
    "y_test = test[label]\n",
    "X_test = test.drop(columns=[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)\n",
    "train.shape\n",
    "\n",
    "test.head(3)\n",
    "test.shape\n",
    "\n",
    "X_test.head(3)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"train.csv\"\n",
    "train.to_csv(train_file, index=False)\n",
    "train_s3_path = session.upload_data(train_file, key_prefix=\"{}/data\".format(prefix))\n",
    "\n",
    "test_file = \"test.csv\"\n",
    "test.to_csv(test_file, index=False)\n",
    "test_s3_path = session.upload_data(test_file, key_prefix=\"{}/data\".format(prefix))\n",
    "\n",
    "X_test_file = \"X_test.csv\"\n",
    "X_test.to_csv(X_test_file, index=False)\n",
    "X_test_s3_path = session.upload_data(X_test_file, key_prefix=\"{}/data\".format(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a model\n",
    "Next, let us train a model.\n",
    "\n",
    "**Note:** Depending on how many underlying models are trained, `train_volume_size` may need to be increased so that they all fit on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define required label and optional additional parameters\n",
    "init_args = {\"label\": \"y\"}\n",
    "\n",
    "# Define additional parameters\n",
    "fit_args = {\n",
    "    # Adding 'best_quality' to presets list will result in better performance (but longer runtime)\n",
    "    \"presets\": [\"optimize_for_deployment\"],\n",
    "}\n",
    "\n",
    "# Pass fit_args to SageMaker estimator hyperparameters\n",
    "hyperparameters = {\"init_args\": init_args, \"fit_args\": fit_args, \"feature_importance\": True}\n",
    "\n",
    "tags = [{\"Key\": \"AlgorithmName\", \"Value\": \"AutoGluon-Tabular\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = AlgorithmEstimator(\n",
    "    algorithm_arn=algorithm_arn,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=compatible_training_instance_type,\n",
    "    sagemaker_session=session,\n",
    "    base_job_name=\"autogluon\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    train_volume_size=100,\n",
    ")\n",
    "\n",
    "inputs = {\"training\": train_s3_path}\n",
    "\n",
    "algo.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Deploy the model and perform a real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deploy a remote endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictor = algo.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=compatible_inference_instance_type,\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=StringDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict on unlabeled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predictor.predict(X_test.to_csv(index=False)).splitlines()\n",
    "\n",
    "# Check output\n",
    "y_results = np.array([i.split(\",\")[0] for i in results])\n",
    "print(Counter(y_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict on data that includes label column\n",
    "Prediction performance metrics will be printed to endpoint logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predictor.predict(test.to_csv(index=False)).splitlines()\n",
    "\n",
    "# Check output\n",
    "y_results = np.array([i.split(\",\")[0] for i in results])\n",
    "print(Counter(y_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check that classification performance metrics match evaluation printed to endpoint logs as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_results = np.array([i.split(\",\")[0] for i in results])\n",
    "\n",
    "print(\"accuracy: {}\".format(accuracy_score(y_true=y_test, y_pred=y_results)))\n",
    "print(classification_report(y_true=y_test, y_pred=y_results, digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Use Batch Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By including the label column in the test data, you can also evaluate prediction performance (In this case, passing `test_s3_path` instead of `X_test_s3_path`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"s3://{bucket}/{prefix}/output/\"\n",
    "\n",
    "transformer = algo.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=compatible_inference_instance_type,\n",
    "    strategy=\"MultiRecord\",\n",
    "    max_payload=6,\n",
    "    max_concurrent_transforms=1,\n",
    "    output_path=output_path,\n",
    ")\n",
    "\n",
    "transformer.transform(test_s3_path, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have finished performing predictions, you can delete the endpoint to avoid getting charged for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if the AWS Marketplace subscription was created just for the experiment and you would like to unsubscribe to the product, here are the steps that can be followed.\n",
    "Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model-package or using the algorithm. Note - You can find this by looking at container associated with the model. \n",
    "\n",
    "Steps to un-subscribe to product from AWS Marketplace:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=lbr_tab_ml)\n",
    "2. Locate the listing that you would need to cancel subscription for, and then __Cancel Subscription__ can be clicked to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
