{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering using Embeddings\n",
    "\n",
    "Many use cases require text (text2text) generation models like **BloomZ** and **Flan T5** to respond to user questions with insightful answers. For example, a customer support chatbot may need to provide answers to common questions. The **BloomZ** and **Flan T5** models have picked up a lot of general knowledge in training, but we often need to ingest and use a large library of more specific information.\n",
    "\n",
    "In this notebook we will demonstrate a method for enabling **BloomZ** and **Flan T5** to answer questions using a library of text as a reference, by using document embeddings and retrieval. We'll be using a dataset of Wikipedia articles about the 2020 Summer Olympic Games. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deploy Flan T5 XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# model_version=\"*\" fetches the latest version of the model\n",
    "model_id, model_version = \"huggingface-text2text-flan-t5-xl\", \"*\"\n",
    "\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name_flan_t5 = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri. This includes all dependencies and scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "model_inference = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name_flan_t5,\n",
    ")\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model_predictor_inference = model_inference.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name_flan_t5,\n",
    "    volume_size=30,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Responses from the flan T5 XL without providing the Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which instances can I use with Managed Spot Training?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type='application/json'):\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json)\n",
    "    return response\n",
    "\n",
    "def parse_response_model_flan_t5(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_texts\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spot Training allows you to create courses in .NET Core 3.0 and up, and for Azure SQL Server.']\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"text_inputs\": question, \n",
    "    \"max_length\":50, \n",
    "    \"num_return_sequences\":1, \n",
    "    \"top_k\":50, \n",
    "    \"top_p\":0.95, \n",
    "    \"do_sample\":True\n",
    "}\n",
    "\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'), endpoint_name=endpoint_name_flan_t5)\n",
    "\n",
    "generated_texts = parse_response_model_flan_t5(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieving the most relevant context from the database for the question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Deploying the model endpoint for getting the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------!"
     ]
    }
   ],
   "source": [
    "model_id, model_version = \"huggingface-textembedding-gpt-j-6b\", \"*\"\n",
    "\n",
    "endpoint_name_embed = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "\n",
    "embed_model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=\"s3://sagemaker-jumpstart-cache-contributor-staging/jumpstart-1p/textembedding/infer-huggingface-textembedding-huggingface-textembedding-gpt-j-6b-20230320-2050-repack.tar.gz\",\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name_embed,\n",
    ")\n",
    "\n",
    "model_predictor_embed = embed_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name_embed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response_text_embed(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    generated_text.append(model_predictions['embedding'])\n",
    "    return generated_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Preprocess the document library\n",
    "We plan to use document embeddings to fetch the most relevant parts of our document library and insert them into the prompt that we provide to **Flan T5 Xl**.\n",
    "\n",
    "Sections should be large enough to contain enough information to answer a question; but small enough to fit one or several into the **Flan T5 Xl** prompt. We find that approximately a paragraph of text is usually a good length, but you should experiment for your particular use case. In this example, we prepared the database based on the answers here - https://aws.amazon.com/sagemaker/faqs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/OpenSSL/crypto.py:12: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography import x509\n",
      "download: s3://hemamsin-jump-test-pdx/data/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv to ./Amazon_SageMaker_FAQs.csv\n"
     ]
    }
   ],
   "source": [
    "# Downloading the Database\n",
    "!aws s3 cp s3://hemamsin-jump-test-pdx/data/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv Amazon_SageMaker_FAQs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_answers = pd.read_csv('Amazon_SageMaker_FAQs.csv',names =[\"Question\", \"Answer\"])\n",
    "df_answers.set_index([\"Question\"])\n",
    "\n",
    "res_embed = []\n",
    "for idx, row in df_answers.iterrows():\n",
    "    query_response = query_endpoint_with_json_payload(row[\"Answer\"], endpoint_name_embed, content_type=\"application/x-text\")\n",
    "    generated_embed = parse_response_text_embed(query_response)[0][0]\n",
    "    res_embed.append(generated_embed)\n",
    "res_embed_df = pd.DataFrame(res_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape =  (154, 4096)\n",
      "train_labels shape =  (154,)\n",
      "uploaded training data location: s3://sagemaker-us-west-2-802376408542/Database/train/Amazon-SageMaker-FAQs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "\n",
    "train_features = np.array(res_embed)\n",
    "\n",
    "# Providing each answer embedding label of the answer\n",
    "train_labels = np.array([i for i in range(len(train_features))])\n",
    "#train_labels = np.array(df_answers[\"Answer\"])\n",
    "\n",
    "print(\"train_features shape = \", train_features.shape)\n",
    "print(\"train_labels shape = \", train_labels.shape)\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, train_features, train_labels)\n",
    "buf.seek(0)\n",
    "\n",
    "\n",
    "bucket = sess.default_bucket()  # modify to your bucket name\n",
    "prefix = \"Database\"\n",
    "key = \"Amazon-SageMaker-FAQs\"\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).Object(os.path.join(prefix, \"train\", key)).upload_fileobj(buf)\n",
    "s3_train_data = f\"s3://{bucket}/{prefix}/train/{key}\"\n",
    "print(f\"uploaded training data location: {s3_train_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker:Creating training-job with name: knn-2023-03-24-14-18-29-762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-24 14:18:30 Starting - Starting the training job...\n",
      "2023-03-24 14:18:44 Starting - Preparing the instances for training...\n",
      "2023-03-24 14:19:30 Downloading - Downloading input data...\n",
      "2023-03-24 14:19:49 Training - Downloading the training image...............\n",
      "2023-03-24 14:22:30 Training - Training image download completed. Training in progress...Docker entrypoint called with argument(s): train\n",
      "Running default environment configuration script\n",
      "[03/24/2023 14:22:54 INFO 139628795332416] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'_kvstore': 'dist_async', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_tuning_objective_metric': '', '_faiss_index_nprobe': '5', 'epochs': '1', 'feature_dim': 'auto', 'faiss_index_ivf_nlists': 'auto', 'index_metric': 'L2', 'index_type': 'faiss.Flat', 'mini_batch_size': '5000', '_enable_profiler': 'false'}\n",
      "[03/24/2023 14:22:54 INFO 139628795332416] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '4096', 'k': '5', 'predictor_type': 'classifier', 'sample_size': '154'}\n",
      "[03/24/2023 14:22:54 INFO 139628795332416] Final configuration: {'_kvstore': 'dist_async', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_tuning_objective_metric': '', '_faiss_index_nprobe': '5', 'epochs': '1', 'feature_dim': '4096', 'faiss_index_ivf_nlists': 'auto', 'index_metric': 'L2', 'index_type': 'faiss.Flat', 'mini_batch_size': '5000', '_enable_profiler': 'false', 'k': '5', 'predictor_type': 'classifier', 'sample_size': '154'}\n",
      "[03/24/2023 14:22:58 WARNING 139628795332416] Loggers have already been setup.\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] Final configuration: {'_kvstore': 'dist_async', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_tuning_objective_metric': '', '_faiss_index_nprobe': '5', 'epochs': '1', 'feature_dim': '4096', 'faiss_index_ivf_nlists': 'auto', 'index_metric': 'L2', 'index_type': 'faiss.Flat', 'mini_batch_size': '5000', '_enable_profiler': 'false', 'k': '5', 'predictor_type': 'classifier', 'sample_size': '154'}\n",
      "[03/24/2023 14:22:58 WARNING 139628795332416] Loggers have already been setup.\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] Launching parameter server for role scheduler\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-83-192.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'knn-2023-03-24-14-18-29-762', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:802376408542:training-job/knn-2023-03-24-14-18-29-762', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-072fddb84406f666289f0e61ef2d961f545f3d3d7e41b6236b5075d5fd194bf9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '4', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE'}\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-83-192.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'knn-2023-03-24-14-18-29-762', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:802376408542:training-job/knn-2023-03-24-14-18-29-762', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-072fddb84406f666289f0e61ef2d961f545f3d3d7e41b6236b5075d5fd194bf9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '4', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'DMLC_ROLE': 'scheduler', 'DMLC_PS_ROOT_URI': '10.0.83.192', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '1', 'DMLC_NUM_WORKER': '1'}\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] Launching parameter server for role server\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-83-192.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'knn-2023-03-24-14-18-29-762', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:802376408542:training-job/knn-2023-03-24-14-18-29-762', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-072fddb84406f666289f0e61ef2d961f545f3d3d7e41b6236b5075d5fd194bf9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '4', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE'}\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-83-192.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'knn-2023-03-24-14-18-29-762', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:802376408542:training-job/knn-2023-03-24-14-18-29-762', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-072fddb84406f666289f0e61ef2d961f545f3d3d7e41b6236b5075d5fd194bf9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '4', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'DMLC_ROLE': 'server', 'DMLC_PS_ROOT_URI': '10.0.83.192', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '1', 'DMLC_NUM_WORKER': '1'}\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] Environment: {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-83-192.us-west-2.compute.internal', 'TRAINING_JOB_NAME': 'knn-2023-03-24-14-18-29-762', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:802376408542:training-job/knn-2023-03-24-14-18-29-762', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-072fddb84406f666289f0e61ef2d961f545f3d3d7e41b6236b5075d5fd194bf9-customer', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'all', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'CUDA_VERSION': '11.1', 'HOME': '/root', 'SHLVL': '1', 'CUDA_COMPAT_NDRIVER_SUPPORTED_VERSION': '455.32.00', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '4', 'DMLC_INTERFACE': 'eth0', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'DMLC_ROLE': 'worker', 'DMLC_PS_ROOT_URI': '10.0.83.192', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '1', 'DMLC_NUM_WORKER': '1'}\n",
      "Process 40 is a shell:scheduler.\n",
      "Process 49 is a shell:server.\n",
      "Process 7 is a worker.\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] Using default worker.\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] Loading faiss with AVX2 support.\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] Successfully loaded faiss with AVX2 support.\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] Checkpoint loading and saving are disabled.\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] nvidia-smi: took 0.035 seconds to run.\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] nvidia-smi identified 0 GPUs.\n",
      "[03/24/2023 14:22:58 ERROR 139628795332416] nvidia-smi: failed to run (127): b'/bin/sh: nvidia-smi: command not found'/\n",
      "[03/24/2023 14:22:58 WARNING 139628795332416] Could not determine free memory in MB for GPU device with ID (0).\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] Using per-machine sample size = 154 (Available virtual memory = 31343439872 bytes, GPU free memory = 0 bytes, number of machines = 1). If an out-of-memory error occurs, choose a larger instance type, use dimension reduction, decrease sample_size, increase the number of instances, and/or decrease mini_batch_size.\n",
      "#metrics {\"StartTime\": 1679667778.7386112, \"EndTime\": 1679667778.738656, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "[2023-03-24 14:22:58.739] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 166, \"num_examples\": 1, \"num_bytes\": 5054280}\n",
      "[2023-03-24 14:22:58.937] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 197, \"num_examples\": 1, \"num_bytes\": 5054280}\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] #progress_metric: host=algo-1, completed 100.0 % of epochs\n",
      "#metrics {\"StartTime\": 1679667778.7392986, \"EndTime\": 1679667778.9379208, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 154.0, \"count\": 1, \"min\": 154, \"max\": 154}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 154.0, \"count\": 1, \"min\": 154, \"max\": 154}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 154.0, \"count\": 1, \"min\": 154, \"max\": 154}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] #throughput_metric: host=algo-1, train throughput=774.7869273268576 records/second\n",
      "[03/24/2023 14:22:58 INFO 139628795332416] Create Store: dist_async\n",
      "[14:22:58] ../src/base.cc:47: Please install cuda driver for GPU use.  No cuda driver detected.\n",
      "[14:22:59] ../src/base.cc:47: Please install cuda driver for GPU use.  No cuda driver detected.\n",
      "[14:22:59] ../src/base.cc:47: Please install cuda driver for GPU use.  No cuda driver detected.\n",
      "[03/24/2023 14:22:59 INFO 139628795332416] Using in-memory reservoir sample from master machine...\n",
      "[03/24/2023 14:22:59 INFO 139628795332416] ...Got reservoir sample from algo-1: data=(154, 4096), labels=(154,), NaNs=0\n",
      "[03/24/2023 14:22:59 INFO 139628795332416] Training index...\n",
      "[03/24/2023 14:22:59 INFO 139628795332416] ...Finished training index in 0 second(s)\n",
      "[03/24/2023 14:22:59 INFO 139628795332416] Adding data to index...\n",
      "[03/24/2023 14:22:59 INFO 139628795332416] ...Finished adding data to index in 0 second(s)\n",
      "#metrics {\"StartTime\": 1679667778.5722766, \"EndTime\": 1679667779.4741938, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 70.6174373626709, \"count\": 1, \"min\": 70.6174373626709, \"max\": 70.6174373626709}, \"epochs\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"update.time\": {\"sum\": 198.3184814453125, \"count\": 1, \"min\": 198.3184814453125, \"max\": 198.3184814453125}, \"finalize.time\": {\"sum\": 6.640434265136719, \"count\": 1, \"min\": 6.640434265136719, \"max\": 6.640434265136719}, \"model.serialize.time\": {\"sum\": 7.643222808837891, \"count\": 1, \"min\": 7.643222808837891, \"max\": 7.643222808837891}}}\n",
      "[03/24/2023 14:22:59 INFO 139628795332416] Test data is not provided.\n",
      "#metrics {\"StartTime\": 1679667779.4742708, \"EndTime\": 1679667779.480167, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 15.316009521484375, \"count\": 1, \"min\": 15.316009521484375, \"max\": 15.316009521484375}, \"totaltime\": {\"sum\": 1276.3171195983887, \"count\": 1, \"min\": 1276.3171195983887, \"max\": 1276.3171195983887}}}\n",
      "\n",
      "2023-03-24 14:23:17 Uploading - Uploading generated training model\n",
      "2023-03-24 14:23:17 Completed - Training job completed\n",
      "Training seconds: 227\n",
      "Billable seconds: 227\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "\n",
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data,\n",
    "    and return a deployed predictor\n",
    "\n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(\n",
    "        get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "        aws_role,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.2xlarge\",\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sess,\n",
    "    )\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "\n",
    "    # train a model. fit_input contains the locations of the train data\n",
    "    fit_input = {\"train\": s3_train_data}\n",
    "    knn.fit(fit_input)\n",
    "    return knn\n",
    "\n",
    "\n",
    "hyperparams = {\"feature_dim\": train_features.shape[1], \"k\": 5,\"sample_size\": train_features.shape[0], \"predictor_type\": \"classifier\"}\n",
    "output_path = f\"s3://{bucket}/{prefix}/default_example/output\"\n",
    "knn_estimator = trained_estimator_from_hyperparams(\n",
    "    s3_train_data, hyperparams, output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: knn-2023-03-24-14-23-35-012\n",
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-example-knn-2023-03-24-14-23-35-012\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-example-knn-2023-03-24-14-23-35-012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "def predictor_from_estimator(knn_estimator, instance_type, endpoint_name=None):\n",
    "    knn_predictor = knn_estimator.deploy(\n",
    "        initial_instance_count=1, instance_type=instance_type, endpoint_name=endpoint_name\n",
    "    )\n",
    "    knn_predictor.serializer = CSVSerializer()\n",
    "    knn_predictor.deserializer = JSONDeserializer()\n",
    "    return knn_predictor\n",
    "\n",
    "\n",
    "instance_type = \"ml.m4.xlarge\"\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-knn\")\n",
    "\n",
    "knn_predictor = predictor_from_estimator(\n",
    "    knn_estimator, instance_type, endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Doing the inference with context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Getting the Context For the Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SECTION_LEN = 500\n",
    "SEPARATOR = \"\\n* \"\n",
    "\n",
    "def construct_context(contexts) -> str:\n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "     \n",
    "    for document_section in contexts:\n",
    "        # Add contexts until we run out of space.        \n",
    "        \n",
    "        chosen_sections_len += len(document_section) + 2 \n",
    "        if chosen_sections_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(SEPARATOR + document_section.replace(\"\\n\", \" \"))\n",
    "    # Useful diagnostic information\n",
    "    print(f\"Selected {len(chosen_sections)} document sections:\")    \n",
    "        \n",
    "    return \"\".join(chosen_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = query_endpoint_with_json_payload(question, endpoint_name_embed, content_type=\"application/x-text\")\n",
    "question_embedding = parse_response_text_embed(query_response)[0][0]\n",
    "\n",
    "#Getting the most relevant context using KNN\n",
    "context_predictions = knn_predictor.predict(np.array(question_embedding), initial_args={\"ContentType\": \"text/csv\",\"Accept\": \"application/json; verbose=true\"})['predictions'][0][\"labels\"]\n",
    "\n",
    "#context_predictions = knn_predictor.predict(np.array(question_embedding), initial_args={\"ContentType\": \"text/csv\",\"Accept\": \"application/json; verbose=true\"})['predictions'][0][\"labels\"]\n",
    "context_embed_retrieve = construct_context(context_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'), endpoint_name=endpoint_name_flan_t5)\n",
    "\n",
    "generated_texts = parse_response_model_flan_t5(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_flan_t5 = \"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\"\n",
    "\n",
    "input_flan_t5 = prompts_flan_t5.replace(\"{context}\", context_embed_retrieve)\n",
    "input_flan_t5 = prompts_flan_t5.replace(\"{question}\", question)\n",
    "\n",
    "payload = {\n",
    "    \"text_inputs\": input_flan_t5, \n",
    "    \"max_length\":500, \n",
    "    \"num_return_sequences\":1, \n",
    "    \"top_k\":50, \n",
    "    \"top_p\":0.95, \n",
    "    \"do_sample\":True\n",
    "}\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'), endpoint_name=endpoint_name_flan_t5)\n",
    "\n",
    "generated_texts = parse_response_model_flan_t5(query_response)\n",
    "print(generated_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
