{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow's tf.data.service with Amazon SageMaker Training Heterogeneous Clusters\n",
    "\n",
    "---\n",
    "### Introduction\n",
    "\n",
    "Heterogeneous clusters enable launching training jobs that use multiple instance types in a single job. This capability can improve your training cost and speed by running different parts of the model training on the most suitable instance type. This use case typically happens in computer vision (CV) deep learning (DL) training, where training is bottleneck on CPU resources needed for data augmentation, leaving the expensive GPU underutilized. Heterogeneous clusters enable you to add more CPU resources to fully utilize GPUs to increase training speed and cost-efficiency. For more details, you can find the documentation of this feature [here](https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html).\n",
    "\n",
    "This notebook demonstrates how to use Heterogeneous Clusters with TensorFlow's [tf.data.service](https://www.TensorFlow.org/api_docs/python/tf/data/experimental/service). It includes training a CPU intensive DL CV workload. Comparing cost and performance between homogeneous and heterogeneous training configurations.  \n",
    "\n",
    "💡To get started quickly with heterogenous clusters, we suggest you'll reuse the provided code as a quick way to migrate your workload from a local tf.data pipeline to a distributed tf.data.service pipeline. You'll need to change [code/train_dnn.py](./code/train_dnn.py), while keeping [code/train_data.py](./code/train_data.py) and [code/launcher.py](code/launcher.py) intact. This is explained below in the [Workload Details] section.\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td width=43%><b>Homogeneous Training Job</b><br/>\n",
    "    In a Homogeneous training job the ml.p4d.24xlarge instance GPUs are under-utilized due to a CPU bottleneck.</td>\n",
    "    <td width=57%><b>Heterogeneous Training Job</b><br/>\n",
    "    In a Heterogeneous training job, we add two ml.c5.18xlarge instances with extra CPU cores, to reduce the CPU bottleneck and drive up GPU usage, to improve training speed cost-efficiency.\n",
    "    </td>\n",
    "   </tr> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <img src=images/basic-homogeneous-job.png alt=\"homogeneous training job\" /></td>\n",
    "    <td><img src=images/basic-heterogeneous-job.png alt=\"Heterogeneous training job\" /></td>\n",
    "   </tr> \n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workload Details\n",
    "Training data is an arteficially generated dataset consisting of 32x32x3 images with random pixel values, and a corresponding random label representing 10 different classes. As this dataset is randomly generated, you should not expect the model to converge in a meaningful way. This shouldn't matter as our intent is only to measure data pipeline and neaural network optimization throughput expressd in epoch/step time.  \n",
    "The model we used is [Resnet50](https://www.TensorFlow.org/api_docs/python/tf/keras/applications/ResNet50). The job runs on an 8 GPUs instance, ml.p4d.24xlarge, and uses Horovod for data parallelization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up heterogeneous clusters training\n",
    "To switch to heterogenous clusters, we'll define two `instance_groups`:\n",
    "- **data_group** - A group of CPU instances that will run data pre-processing code.\n",
    "- **dnn_group** -  A group of GPU instances that will run Deep Neural Network training code.\n",
    "\n",
    "In this example, the inter-node communication between CPU and GPU instance groups is implemented using [TensorFlow data service feature](https://www.TensorFlow.org/api_docs/python/tf/data/experimental/service). This feature allows to offload a configurable amount of preprocessing work to worker machines. Note that SageMaker's Heterogeneous cluster does not provide out-of-the-box support for inter-instance_group communication and it is up to the user to implement (we provide reference implementation here).\n",
    "\n",
    "This notebook refers following files and folders:\n",
    "- [code/train_dnn.py](./code/train_dnn.py) - this is standard TF training script, it has a single reference to tf.data.service when setting up the tf.data pipeline. This script will be executed on GPU instances belonging to the dnn_group.\n",
    "- [code/train_data.py](./code/train_data.py) - this script starts tf.data.service services like a tf.data.service Dispatcher and tf.data.service Worker processes. You shouldn't edit this script when adjusting to your workload.\n",
    "- [code/launcher.py](./code/launcher.py) - Entry point training script. This is the script that SageMaker Training will start on all instances (all instances groups must share the same entrypoint script in heterogeneous clusters). `launcher.py` is responsible for detecting the instance group the instance belong to, and start `train_dnn.py` and `train_data.py` accordingly. It is also responsible for shutting down tf.data.services the training script completes (`train_dnn.py`) so all instances exit allowing the SageMaker training job to complete. \n",
    "In every instance `luncher.py` will use `train_data.py` to start a tf.data.service worker server (As all instance types have CPUs that could be used for preprocessing). `luncher.py` will start a single tf.data.service dispatcher server (on the first instance of the `data_group`).  \n",
    "`luncher.py` will start the `train_dnn.py` script in all GPU instances (`dnn_group` instances).\n",
    "\n",
    "#### Learn more about tf.data.service processes\n",
    "`tf.data.service Dispatcher` - The dispatcher server acts as the control plain for tf.data.service; Being responsible for registering worker servers and assinging preprocessing tasks to them. Each training job has a single Dispatcher running in the first instance of the `data_group` and listens on port 6000.\n",
    "`tf.data.service Workers` - Worker servers carry out the data processing. Each instance could have one or more workers (listen on port 6001/6002/...).\n",
    "\n",
    "#### Defining what part of your pipeline runs in which instance\n",
    " Applying `tf.data.experimental.service.distribute` to your dataset, you can program the dataset to run all preprocessing operations up to the point of application, on the workers. \n",
    " As all instances will run a tf.data.service Worker, all instances will need access to a dataset you'll make available through a SageMaker training data channel. You do have the option of limiting which instance group will see which training data channel.\n",
    "\n",
    "Thie below figutre shows sequence of events of setting up and running in a tf.data.service based heterogeneous cluster training job.\n",
    "\n",
    "<img src=images/tf.data.service-diagram.png width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "As an alternative to this notebook, you can follow (readme.md)[./readme.md] which allows you to set up and launch the training job from an IDE or command line.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "At a high level, the notebook covers:\n",
    "-  Set up Amazon SageMaker Studio Notebook \n",
    "-  Run homogeneous cluster training job \n",
    "   -  Setting up the Training environment\n",
    "   -  Submitting the Training job\n",
    "   -  Monitor and visualize the CloudWatch metrics\n",
    "-  Run heterogeneous cluster training job \n",
    "   -  Setting up the Training environment\n",
    "   -  Submitting the Training job\n",
    "   -  Monitor and visualize the CloudWatch metrics\n",
    "-  Compare time-to-train and cost-to-train\n",
    "-  Conclusion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Set up SageMaker Studio notebook\n",
    "#### Before you start\n",
    "Ensure you have selected Python 3 (_TensorFlow 2.6 Python 3.8 CPU Optimized_) image for your SageMaker Studio Notebook instance, and running on _ml.t3.medium_ instance type.\n",
    "\n",
    "#### Step 1 - Upgrade SageMaker SDK and dependent packages \n",
    "Heterogeneous Clusters for Amazon SageMaker model training was [announced](https://aws.amazon.com/about-aws/whats-new/2022/07/announcing-heterogeneous-clusters-amazon-sagemaker-model-training) on 07/08/2022. This feature release requires you to have updated SageMaker SDK, PyTorch, and Boto3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (1.24.72)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.24.80-py3-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.5/132.5 kB 925.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: botocore in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (1.27.72)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.27.80-py3-none-any.whl (9.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 16.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: awscli in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (1.25.73)\n",
      "Collecting awscli\n",
      "  Downloading awscli-1.25.81-py3-none-any.whl (3.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 38.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sagemaker in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (2.109.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/jmespath-1.0.0-py3.9.egg (from boto3) (1.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/urllib3-1.26.9-py3.9.egg (from botocore) (1.26.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/python_dateutil-2.8.2-py3.9.egg (from botocore) (2.8.2)\n",
      "Requirement already satisfied: PyYAML<5.5,>=3.10 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from awscli) (5.4.1)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from awscli) (0.16)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from awscli) (0.4.4)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/importlib_metadata-4.11.3-py3.9.egg (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: pathos in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pathos-0.2.8-py3.9.egg (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/protobuf3_to_dict-0.1.5-py3.9.egg (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: pandas in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pandas-1.4.2-py3.9-macosx-10.9-x86_64.egg (from sagemaker) (1.4.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from sagemaker) (1.22.4)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/attrs-20.3.0-py3.9.egg (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/smdebug_rulesconfig-1.0.1-py3.9.egg (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: google-pasta in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/google_pasta-0.2.0-py3.9.egg (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from sagemaker) (3.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/packaging-21.3-py3.9.egg (from sagemaker) (21.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/zipp-3.7.0-py3.9.egg (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pyparsing-3.0.7-py3.9.egg (from packaging>=20.0->sagemaker) (3.0.7)\n",
      "Requirement already satisfied: six in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pytz-2022.1-py3.9.egg (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: dill>=0.3.4 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/dill-0.3.4-py3.9.egg (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/multiprocess-0.70.12.2-py3.9.egg (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pox-0.3.0-py3.9.egg (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/ppft-1.6.6.4-py3.9.egg (from pathos->sagemaker) (1.6.6.4)\n",
      "Installing collected packages: botocore, boto3, awscli\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.27.72\n",
      "    Uninstalling botocore-1.27.72:\n",
      "      Successfully uninstalled botocore-1.27.72\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.24.72\n",
      "    Uninstalling boto3-1.24.72:\n",
      "      Successfully uninstalled boto3-1.24.72\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.25.73\n",
      "    Uninstalling awscli-1.25.73:\n",
      "      Successfully uninstalled awscli-1.25.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-training 4.2.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed awscli-1.25.81 boto3-1.24.80 botocore-1.27.80\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m pip install --upgrade boto3 botocore awscli sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Restart the notebook kernel \n",
    "From the Jupyter Lab menu bar **Kernel > Restart Kernel...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Valdiate SageMaker Python SDK and TensorFlow versions\n",
    "Ensure the output of the cell below reflects:\n",
    "\n",
    "- SageMaker Python SDK version 2.98.0 or above, \n",
    "- boto3 1.24 or above \n",
    "- botocore 1.27 or above \n",
    "- TensorFlow 2.6 or above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sagemaker\n",
      "Version: 2.109.0\n",
      "---\n",
      "Name: boto3\n",
      "Version: 1.24.80\n",
      "---\n",
      "Name: botocore\n",
      "Version: 1.27.80\n",
      "---\n",
      "Name: tensorflow\n",
      "Version: 2.8.0\n",
      "---\n",
      "Name: protobuf\n",
      "Version: 3.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip show sagemaker boto3 botocore tensorflow protobuf |egrep 'Name|Version|---'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Run a homogeneous training job\n",
    "#### Step 1: Set up the training environment\n",
    "In this step, we define and submit a homogeneous training job. It uses a single instance type (p4d.24xlarge) with 8 GPUs, and analysis shows that is CPU bound causing its GPUs being underutilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "import os\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": 10,\n",
    "    \"steps_per_epoch\": 500,\n",
    "    \"batch_size\": 1024,\n",
    "    \"tf_data_mode\": \"local\",  # We won't be using tf.data.service ('service') for this homogeneous job\n",
    "    \"num_of_data_workers\": 0,  # We won't be using tf.data.service ('service') for this homogeneous job\n",
    "}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"launcher.py\",\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"2.9.1\",\n",
    "    py_version=\"py39\",\n",
    "    role=role,\n",
    "    volume_size=10,\n",
    "    max_run=1800,  # 30 minutes\n",
    "    disable_profiler=True,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    instance_count=1,\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": 8,  # 8 GPUs per host\n",
    "            \"custom_mpi_options\": \"--NCCL_DEBUG WARN\",\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Submit the training job\n",
    "\n",
    "Note: For the logs, click on **View logs** from the **Training Jobs** node in **Amazon SageMaker Console**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-24 11:28:23 Starting - Starting the training job......\n",
      "2022-09-24 11:29:08 Starting - Preparing the instances for training........................\n",
      "2022-09-24 11:33:34 Downloading - Downloading input data\n",
      "2022-09-24 11:33:34 Training - Downloading the training image........"
     ]
    }
   ],
   "source": [
    "from start_job_utils import fit_with_retries\n",
    "fit_with_retries(5, estimator, \n",
    "    job_name=\"homogeneous-\" + datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Analyzing the homogeneous training job throughput and resource usage\n",
    "We'll examine: CPU and GPU usage. Epoch time and step time\n",
    "\n",
    "**CPU and GPU usage analysis** \n",
    "\n",
    "In the screenshot below we observe that close to all the 96 vCPU of the instance is utilized. While GPU utilization is only ~45%. Clearly if we had more vCPUs we could increase GPU usage significantly to increase job throughput\n",
    "\n",
    "Note: To view your own job Click on **View instance metrics** from the **Training jobs** in **Amazon SageMaker Console**. Then to rescale the CloudWatch Metrics to 100% on CPU utilization for algo-1 and algo-2, use CloudWatch \"Add Math\" feature and average it out by no. of vCPUs/GPUs on those instance types. We captured metrics definitions used to produce this graph [here](./cloudwatch-metric-definitions/homogenous-workload%20copy.json).  \n",
    "<img src=\"images/metrics homogeneous cpu and gpu usage.png\" width=75%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epoch time and step time analysis**\n",
    "\n",
    "For 2nd and 3rd epochs the below should print out: 105s/epoch - 209ms/step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture homogeneous_logs\n",
    "estimator.sagemaker_session.logs_for_job(estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing step time for epochs and steps for homogeneous-20220923T231801Z\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 117s - loss: 2.4153 - lr: 0.0033 - 117s/epoch - 234ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 92s - loss: 2.3755 - lr: 0.0057 - 92s/epoch - 184ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 92s - loss: 2.3472 - lr: 0.0080 - 92s/epoch - 184ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 92s - loss: 2.3175 - lr: 0.0080 - 92s/epoch - 184ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 92s - loss: 2.3066 - lr: 0.0080 - 92s/epoch - 183ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 90s - loss: 2.3043 - lr: 0.0080 - 90s/epoch - 181ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 94s - loss: 2.3028 - lr: 0.0080 - 94s/epoch - 189ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 92s - loss: 2.3024 - lr: 0.0080 - 92s/epoch - 184ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 93s - loss: 2.3021 - lr: 0.0080 - 93s/epoch - 185ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 89s - loss: 2.3018 - lr: 0.0080 - 89s/epoch - 177ms/step\n"
     ]
    }
   ],
   "source": [
    "print(f\"Printing step time for epochs and steps for {estimator.latest_training_job.name}\")\n",
    "for line in homogeneous_logs.stdout.split(\"\\n\"):\n",
    "    if \"mpirank:0\" in line and \"/epoch\" in line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Run a heterogeneous cluster training job\n",
    "\n",
    "#### Step 1: Set up training environment\n",
    "We'll now run a training job in heterogeneous cluster mode.  \n",
    "Note the changes from the homogeneous cluster job:  \n",
    "- We define two new instance groups that are provided to the `estimator` as the `instance_groups` parameter that replaces the homogeneous parameters `instance_type` and `instance_count`.\n",
    "- In the `distribution` parameter for Horovod we added a new parameter `instance_groups` that is used to limit the MPI cluster to run in the  `dnn_group`. The MPI cluster should include only the GPU nodes that run Horovod (which needs MPI). The `data_group` instances should not be part of the MPI cluster, as they set up their on `tf.data.service` cluster.\n",
    "\n",
    "More on the two instance groups config we use:\n",
    "- `data_group` - two ml.c5.18xlarge instances, each with 72 vCPUs to handle data preprocessing. Reading data from S3, preprocessing it, and forwarding it to the `dnn_group`.\n",
    "- `dnn_group` - a single p4d.24xlarge instance, with 8 GPUs and 96 vCPUs to handle deep neural network optimization (forward backward passes). To fully utilize 96 vCPUs in the `dnn_group`, we'll be starting data workers on all the instances in both groups, therefore we have 240 vCPUs (96+72+72) in total available for preprocessing (minus vCPUs used for the neural network optimization process).\n",
    "\n",
    "There are three Python scripts to know about:\n",
    "The 1st is `train_dnn.py` - This is your training script for the neural network, you should edit it to match your own use case. Note that this script isn't aware of the Heterogeneous cluster set up, except when it initializes the tf.data dataset calling this line: `ds = ds.apply(tf.data.experimental.service.distribute(...)`.  \n",
    "The 2nd and 3rd scripts, which you're not suppose to edit when adapting to your own use case, do the heavy lifting required for using tf.data.service over the Heterogeneous cluster feature.  \n",
    "`train_data.py` include functions to start/stop tf.service.data process like a dispatcher and WorkerServer. \n",
    "`launcher.py` has several responsibilities: \n",
    "- A single entrypoint script for all instances in all instance groups (SageMaker will start the same script on all instances).\n",
    "- Identifies which instance group the node belong to, and start the relevant script accordingly (`train_dnn.py` or  `train_data.py` or sometimes both).\n",
    "- Takes measures to ensure that tf.data.sevice processes shutdown when training completes, as the training job completes only when all instances exit.\n",
    "- Allows to start more than one process (for example, on the dnn_group instances we'll run both the `train_dnn.py` and a tf.data.service worker to utilize the instance CPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import os\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": 10,\n",
    "    \"steps_per_epoch\": 500,\n",
    "    \"batch_size\": 1024,\n",
    "    \"tf_data_mode\": \"service\",  # Using tf.data.service for this Heterogeneous cluster job\n",
    "    \"num_of_data_workers\": 1,  # One tf.data.service worker per node\n",
    "}\n",
    "\n",
    "# Group for CPU instances to run tf.data.service dispatcher/workers processes.\n",
    "data_group = InstanceGroup(\"data_group\", \"ml.c5.18xlarge\", 2)\n",
    "# Group for deep neural network (dnn) with accleartors (e.g., GPU, FPGA, etc.)\n",
    "dnn_group = InstanceGroup(\"dnn_group\", \"ml.p4d.24xlarge\", 1)\n",
    "\n",
    "estimator2 = TensorFlow(\n",
    "    entry_point=\"launcher.py\",\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"2.9.1\",\n",
    "    py_version=\"py39\",\n",
    "    role=role,\n",
    "    volume_size=10,\n",
    "    max_run=1800,  # 30 minutes\n",
    "    disable_profiler=True,\n",
    "    # instance_type='ml.p4d.24xlarge',\n",
    "    # instance_count=1,\n",
    "    instance_groups=[data_group, dnn_group],\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": 8,  # p4d.24xlarge has 8 GPUs per host\n",
    "            \"custom_mpi_options\": \"--NCCL_DEBUG WARN\",\n",
    "        },\n",
    "        \"instance_groups\": [dnn_group],  # Apply distribution strategy to the dnn_group only\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Submit the training job\n",
    "\n",
    "Note1: For the logs, click on **View logs** from the **Training Jobs** node in **Amazon SageMaker Console**. \n",
    "Note2: Ignore the 0 billable seconds shown below. See actual billable seconds in the AWS web console > SageMaker > Training Jobs > this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-23 23:52:33 Starting - Starting the training job......\n",
      "2022-09-23 23:53:17 Starting - Preparing the instances for training........................\n",
      "2022-09-23 23:57:33 Downloading - Downloading input data...\n",
      "2022-09-23 23:57:48 Training - Downloading the training image...........................\n",
      "2022-09-24 00:02:25 Training - Training image download completed. Training in progress....................................................\n",
      "2022-09-24 00:11:23 Uploading - Uploading generated training model...\n",
      "2022-09-24 00:11:59 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "from start_job_utils import fit_with_retries\n",
    "fit_with_retries(5, estimator2, \n",
    "    job_name=\"heterogeneous-\" + datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Analyze the heterogeneous cluster training job's throughput and resource usage\n",
    "We'll examine: CPU and GPU usage. Epoch time and step time.\n",
    "\n",
    "**CPU and GPU usage analysis** \n",
    "\n",
    " In the screenshot below we observe that GPU usage has increase to 74% (compared to ~45% in the homogeneous training run) which is what we were aiming for. The CPU usage on all 3 instances are close to 80% CPU uage.  \n",
    " \n",
    "Note: To view your own job Click on **View instance metrics** from the **Training jobs** node in **Amazon SageMaker Console**. Then to rescale the CloudWatch Metrics to 100% on CPU utilization for algo-1 and algo-2, use CloudWatch \"Add Math\" feature and average it out by no. of vCPUs/GPUs on those instance types.  We captured metrics definitions used to produce this graph [here](./cloudwatch-metric-definitions/heterogenenous-workload.json).  \n",
    "<img src=\"images/metrics Heterogeneous cpu and gpu usage.png\" width=75%/>\n",
    "\n",
    "**Epoch time and step time analysis** \n",
    "\n",
    "For 2nd epoch onwards you should see this print out in the logs of the dnn_group instance (p4d.24xlarge): 43s/epoch - 86ms/step.\n",
    "Note that the instances are named: Algo1, Algo2, Algo3 randomly on each execution, so you'll need to open all instances logs to find the dnn_group instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Comparing time-to-train and cost-to-train\n",
    "The table below summarizes both jobs. We can see that:\n",
    "- The Heterogeneous job is <b>2.2x faster to train</b> (86ms/step) than the homogeneous job (192ms/step).\n",
    "- The Heterogeneous job is <b>45% cheaper to train</b> than the homogeneous job. This is despite the heterogeneous costs more per hour ($45/hour vs $37/hour), due to the two extra c5.18xlarge instances included in the heterogeneous job `($45 = $37.7 + 2 * $3.67` \n",
    "The cost-to-train formula we used: change in hourly price `($45/$37.7) ` times `reduction-in-time-to-train (86ms/192ms)`  =  45% = `($45/$37.7) * (86ms/192ms)`. \n",
    "\n",
    "<img src=images/homogeneous-vs-heterogeneous-results-table.png alt=\"results table\" />\n",
    "\n",
    "## F. Conclusion\n",
    "In this notebook, we demonstrated how to leverage Heterogeneous cluster feature of SageMaker Training, with TensorFlow to achieve better price performance and increase training speed. To get started you can copy this example project and change  `train_dnn.py` to match your worklaod. To run the job, you could use this notebook, or the `start_job.py`."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "77c0de85c2cb739aa5100af7b92fb9d2075368f0e653f4148499a56c989df5f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
