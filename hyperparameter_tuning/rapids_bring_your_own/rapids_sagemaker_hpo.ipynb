{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/header.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameter Optimization (HPO) imporves model quality by searching the space of possible 'architecture parameters,' parameters not usually trained during the learning process.  This search can significantly boost model quality relative to default parameters and non-expert tuning; however, the search over architectures can take a very long time on a non-accelerated platform. In this notebook, we containerize a RAPIDS workflow and run Bring-Yor-Own-Container SageMaker HPO to show how we can overcome the computational complexity of model search. \n",
    "\n",
    "We accelerate HPO in two key ways: \n",
    "* by *scaling within a node* (e.g., multi-GPU where each GPU brings a magnitude higher core count relative to CPUs), and \n",
    "* by *scaling across nodes* and running parallel trials on cloud instances.\n",
    "\n",
    "By combining these two powers HPO experiments that feel unapproachable and may take multiple days on CPU instances can complete in just hours. For example, we find a <span style=\"color:#8735fb; font-size:14pt\"> **12X** </span> speedup in wall clock time and a <span style=\"color:#8735fb; font-size:14pt\"> **4.5x** </span> reduction in cost when comparing between GPU and CPU [EC2 Spot instances](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html) on 100 XGBoost HPO trials using 10 parallel workers on 10 years of the Airline Dataset (~63M flights). For additional details refer to the <a href='#experiments'>end of the notebook</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these powerful tools at our disposal, every data scientist should feel empowered to uplevel their model before serving it to the world!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from helper_functions import *\n",
    "working_directory = get_notebook_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/hpo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:22pt\"> **Key Choices** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and choose the configuration options for our HPO run.\n",
    "\n",
    "Note that we've set the configuration defaults to values that are on the low end, however you are welcome to scale them up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Dataset and S3 Bucket ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We target a large real-world structured dataset of flight logs for US airlines ( published monthly since 1987 by the Bureau of Transportation [dataset link](https://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&DB_URL=), additional details in <a href='#dataset'>Section 1.1</a>). \n",
    "\n",
    "We offer several sizes of this dataset in <a href='https://parquet.apache.org/'>parquet</a> (compressed column storage) format: `1_year` (2019, 7.2M flights), `3_year` (2016-2019, 18M flights) or `10_year` (2009-2019, 63M flights). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = '1_year'\n",
    "\n",
    "assert( dataset_directory in [ '1_year', '3_year', '10_year'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demo purposes we host these pre-built datasets in a public S3 bucket, you are of course welcome to copy the data into the corresponding region of your compute, or host your own dataset in a bucket of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-rapids-hpo-us-east-1'\n",
    "\n",
    "s3_data_input = f\"s3://{bucket}/{dataset_directory}\"\n",
    "s3_model_output = f\"s3://{bucket}/trained-models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Algorithm ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a ML/algorithm perspective, we offer [XGBoost](https://xgboost.readthedocs.io/en/latest/#) and [RandomForest](https://docs.rapids.ai/api/cuml/stable/cuml_blogs.html#tree-and-forest-models) decision tree models which do quite well on this structured dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_choice = 'XGBoost'\n",
    "\n",
    "assert ( algorithm_choice in [ 'XGBoost', 'RandomForest' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also optionally increase robustness via reshuffles of the train-test split (i.e., [cross-validation folds](https://scikit-learn.org/stable/modules/cross_validation.html)). Typical values here are between 3 and 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = 3\n",
    "\n",
    "assert ( cv_folds >= 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Code ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable the option of running different code variations that unlock increasing amounts of parallelism in the compute workflow.\n",
    "\n",
    "* `singleCPU`* = [pandas](https://pandas.pydata.org/) + [sklearn](https://scikit-learn.org/stable/)\n",
    "* `multiCPU`   = [dask](https://dask.org/) + [pandas](https://pandas.pydata.org/) + [sklearn](https://scikit-learn.org/stable/)\n",
    "\n",
    "* `singleGPU` = [cudf](https://github.com/rapidsai/cudf) + [cuml](https://github.com/rapidsai/cuml)\n",
    "* `multiGPU`  = [dask](https://dask.org/) + [cudf](https://github.com/rapidsai/cudf) + [cuml](https://github.com/rapidsai/cuml) \n",
    "\n",
    "For some context, cuDF and cuML are part of the RAPIDS library ecosystem. cuDF is a GPU accelerated dataframe library aimed to mirror pandas, while cuML is a GPU accelerated machine-learning library aimed to mirror sklearn.\n",
    "\n",
    "Note that the single-CPU option still uses multiple cores in the model training portion of the workflow; however, to unlock parallelism in each stage of the workflow we use [Dask](https://dask.org/). We've integrated all to of these options in the `rapids_cloud_ml.py` file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_choice = 'singleGPU' \n",
    "\n",
    "assert ( code_choice in [ 'singleCPU', 'singleGPU', 'multiCPU', 'multiGPU'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Search Ranges and Strategy ] </span>\n",
    "<a id='strategy-and-param-ranges'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important choices when running HPO is to choose the bounds of the hyper-parameter search process. Below we've set the ranges of the hyper-parameters to allow for interesting variation, you are of course welcome to revise these ranges based on domain knowledge especially if you plan to plug in your own dataset. \n",
    "\n",
    "> Note that we support additional algorithm specific parameters (refer to the `parse_hyper_parameter_inputs` function in `rapids_cloud_ml.py`), but for demo purposes have limited our choice to the three parameters that overlap between the XGBoost and RandomForest algorithms. For more details see the documentation for [XGBoost parameters](https://xgboost.readthedocs.io/en/latest/parameter.html) and [RandomForest parameters](https://docs.rapids.ai/api/cuml/stable/api.html#random-forest).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'max_depth'    : sagemaker.parameter.IntegerParameter        ( 5, 15 ),\n",
    "    'n_estimators' : sagemaker.parameter.IntegerParameter        ( 100, 500 ),\n",
    "    'max_features' : sagemaker.parameter.ContinuousParameter     ( 0.1, 1.0 ),\n",
    "    # please see note above for adding additional model specific parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'XGBoost' in algorithm_choice: \n",
    "    # number of trees parameter name difference b/w XGBoost and RandomForest\n",
    "    hyperparameter_ranges['num_boost_round'] = hyperparameter_ranges.pop('n_estimators')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose between a Random and Bayesian search strategy for picking parameter combinations. \n",
    "\n",
    "**Random Search**: Choose a random combination of values from within the ranges for each training job it launches. The choice of hyperparameters doesn't depend on previous results so you can run the maximum number of concurrent workers without affecting the performance of the search. \n",
    "\n",
    "**Bayesian Search**: Make a guess about which hyperparameter combinations are likely to get the best results. After testing the first set of hyperparameter values, hyperparameter tuning uses regression to choose the next set of hyperparameter values to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_strategy = 'Random'\n",
    "\n",
    "assert ( search_strategy in [ 'Random', 'Bayesian' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Experiment Scale ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to decide how may total experiments to run, and how many should run in parallel. Below we have a very conservative number of maximum jobs to run so that you don't accidently spawn large computations when starting out, however for meaningful HPO searches this number should be much higher (e.g., in our experiments we often run 100 max_jobs). Note that you may need to request a [quota limit increase](https://docs.aws.amazon.com/general/latest/gr/sagemaker.html) for additional  `max_parallel_jobs` parallel workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_jobs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set the max duration for an individual job to 24 hours so we don't have run-away compute jobs taking too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration_of_experiment_seconds = 60 * 60 * 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> [ Compute Platform ] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dataset size and compute choice we will try to recommend an instance choice*, you are of course welcome to select alternate configurations. \n",
    "> e.g., For the 10_year dataset option, we suggest ml.p3.8xlarge GPU instances and ml.m5.24xlarge CPU instances ( we'll need upwards of 200GB CPU RAM during model training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommended instance type : ml.p3.2xlarge \n",
      "instance details          : 1x V100, 16GB GPU memory, 61GB CPU memory\n"
     ]
    }
   ],
   "source": [
    "instance_type = recommend_instance_type ( code_choice, dataset_directory  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to choosing our instance type, we can also enable significant savings by leveraging [AWS EC2 Spot Instances](https://aws.amazon.com/ec2/spot/). We highly recommend that you enable this choice as it typically leads to 60-70% cost savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_spot_instances_flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:22pt\"> **Validate** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 data input    =\ts3://sagemaker-rapids-hpo-us-east-1/1_year\n",
      "s3 model output  =\ts3://sagemaker-rapids-hpo-us-east-1/trained-models\n",
      "compute          =\tsingleGPU\n",
      "algorithm        =\tXGBoost, 3 cv-fold\n",
      "instance         =\tml.p3.2xlarge\n",
      "spot instances   =\tTrue\n",
      "hpo strategy     =\tRandom\n",
      "max_experiments  =\t8\n",
      "max_parallel     =\t2\n",
      "max runtime      =\t86400 sec\n"
     ]
    }
   ],
   "source": [
    "summarize_choices( s3_data_input, s3_model_output, code_choice, algorithm_choice, \n",
    "                   cv_folds, instance_type, use_spot_instances_flag, search_strategy, \n",
    "                   max_jobs, max_parallel_jobs, max_duration_of_experiment_seconds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: block; text-align: center; color:#8735fb; font-size:30pt\"> **1. ML Workflow** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/ml_workflow.png' width='800'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 1.1 - Dataset </span>\n",
    "<a id ='dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we'll utilize the Airline dataset (Carrier On-Time Performance 1987-2020, available from the [Bureau of Transportation Statistics](https://transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline%20On-Time%20Performance%20Data&DB_Short_Name=On-Time#)). \n",
    "\n",
    "The public dataset contains logs/features about flights in the United States (17 airlines) including:\n",
    "\n",
    "* Locations and distance  ( `Origin`, `Dest`, `Distance` )\n",
    "* Airline / carrier ( `Reporting_Airline` )\n",
    "* Scheduled departure and arrival times ( `CRSDepTime` and `CRSArrTime` )\n",
    "* Actual departure and arrival times ( `DpTime` and `ArrTime` )\n",
    "* Difference between scheduled & actual times ( `ArrDelay` and `DepDelay` )\n",
    "* Binary encoded version of late, aka our target variable ( `ArrDelay15` )\n",
    "\n",
    "Using these features we'll build a classifier model to predict whether a flight is going to be more than 15 minutes late on arrival as it prepares to depart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 1.2 - Python ML Workflow </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a RAPIDS enabled SageMaker HPO we first need to build an Estimator. An Estimator is a container image that captures all the software needed to run an HPO experiment. The container is augmented with entrypoint code that will be trggered at runtime by each worker. The entrypoint code enables us to write custom models and hook them up to data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with SageMaker HPO, the entrypoint logic should parse hyper-parameters (supplied by AWS SageMaker), load and split data, build and train a model, score/evaluate the trained model, and emit an output representing the final score for the given hyper-parameter setting. We've already built multiple variations of this code.\n",
    "\n",
    "If you would like to make changes by adding your custom model logic feel free to modify the **train.py** and **rapids_cloud_ml.py** files in the code directory. \n",
    "You are also welcome to uncomment the cells below to load the read/review the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../code/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../code/rapids_cloud_ml.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: block; text-align: center; color:#8735fb; font-size:30pt\"> **2. Build Estimator** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/estimator.png' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've already mentioned, the SageMaker Estimator represents the containerized software stack that AWS SageMaker will replicate to each worker node.\n",
    "\n",
    "The first step to building our Estimator, is to augment a RAPIDS container with our ML Workflow code from above, and push this image to Amazon Elastic Cloud Registry so it is available to SageMaker.\n",
    "\n",
    "For additional options and details see the [Estimator documentation](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 2.1 - Containerize and Push to ECR </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets turn to building our container so that it can integrate with the AWS SageMaker HPO API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our container can either be built on top of the latest RAPIDS [ nightly ] image as a starting layer or the RAPIDS stable image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rapids_stable = 'rapidsai/rapidsai:0.14-cuda10.1-runtime-ubuntu18.04-py3.7'\n",
    "rapids_nightly = 'rapidsai/rapidsai-nightly:0.15-cuda10.1-runtime-ubuntu18.04-py3.7'\n",
    "\n",
    "rapids_base_container = rapids_stable\n",
    "assert ( rapids_base_container in [ rapids_stable, rapids_nightly ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get things rolling lets make sure we can query our AWS SageMaker execution role and session as well as our account ID and AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "account=!(aws sts get-caller-identity --query Account --output text)\n",
    "region=!(aws configure get region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['561241433344'], ['us-east-1'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account, region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also decide on the full name of our container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base = 'cloud-ml-sagemaker'\n",
    "image_tag  = rapids_base_container.split(':')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_fullname = f\"{account[0]}.dkr.ecr.{region[0]}.amazonaws.com/{image_base}:{image_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'561241433344.dkr.ecr.us-east-1.amazonaws.com/cloud-ml-sagemaker:0.14-cuda10.1-runtime-ubuntu18.04-py3.7'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecr_fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> 2.1.1 - Write Dockerfile </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write out the Dockerfile in this cell, write it to disk, and in the next cell execute the docker build command. \n",
    "\n",
    "First, lets switch our working directory to the location of the Estimator entypoint and library code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/rapids_bring_your_own/code\n"
     ]
    }
   ],
   "source": [
    "%cd code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write our selected RAPDIS image layer as the first FROM statement in the the Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dockerfile', 'w') as dockerfile_handle: \n",
    "    dockerfile_handle.writelines( 'FROM ' + rapids_base_container + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets write the remaining pieces of the Dockerfile, namely adding the sagemaker-training-toolkit and copying our python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a Dockerfile\n",
    "\n",
    "# install https://github.com/aws/sagemaker-training-toolkit\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends build-essential \\ \n",
    "    && source activate rapids && pip3 install sagemaker-training\n",
    "\n",
    "# path where sagemaker looks for our code\n",
    "ENV CLOUD_PATH=\"/opt/ml/code\"\n",
    "\n",
    "# copy our latest [local] code into the container \n",
    "COPY rapids_cloud_ml.py $CLOUD_PATH/rapids_cloud_ml.py\n",
    "COPY train.py $CLOUD_PATH/train.py\n",
    "\n",
    "# sagemaker entrypoint will be train.py\n",
    "ENV SAGEMAKER_PROGRAM train.py \n",
    "\n",
    "WORKDIR $CLOUD_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's ensure that our Dockerfile correctly captured our base image selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM rapidsai/rapidsai:0.14-cuda10.1-runtime-ubuntu18.04-py3.7\n",
      "\n",
      "# install https://github.com/aws/sagemaker-training-toolkit\n",
      "RUN apt-get update && apt-get install -y --no-install-recommends build-essential \\ \n",
      "    && source activate rapids && pip3 install sagemaker-training\n",
      "\n",
      "# path where sagemaker looks for our code\n",
      "ENV CLOUD_PATH=\"/opt/ml/code\"\n",
      "\n",
      "# copy our latest [local] code into the container \n",
      "COPY rapids_cloud_ml.py $CLOUD_PATH/rapids_cloud_ml.py\n",
      "COPY train.py $CLOUD_PATH/train.py\n",
      "\n",
      "# sagemaker entrypoint will be train.py\n",
      "ENV SAGEMAKER_PROGRAM train.py \n",
      "\n",
      "WORKDIR $CLOUD_PATH\n"
     ]
    }
   ],
   "source": [
    "validate_dockerfile( rapids_base_container )\n",
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> 2.1.2 Build and Tag </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The build step will be dominated by the download of the RAPIDS image (base layer). If it's already been downloaded the build will take less than 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14-cuda10.1-runtime-ubuntu18.04-py3.7: Pulling from rapidsai/rapidsai\n",
      "Digest: sha256:200cbe0922b26cbddba2cfcf50a97ec91817c40767516fde45ebaaf801b7477d\n",
      "Status: Image is up to date for rapidsai/rapidsai:0.14-cuda10.1-runtime-ubuntu18.04-py3.7\n",
      "docker.io/rapidsai/rapidsai:0.14-cuda10.1-runtime-ubuntu18.04-py3.7\n"
     ]
    }
   ],
   "source": [
    "!docker pull $rapids_base_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon     64kB\n",
      "Step 1/7 : FROM rapidsai/rapidsai:0.14-cuda10.1-runtime-ubuntu18.04-py3.7\n",
      " ---> 65edd55ef398\n",
      "Step 2/7 : RUN apt-get update && apt-get install -y --no-install-recommends build-essential     && source activate rapids && pip3 install sagemaker-training\n",
      " ---> Using cache\n",
      " ---> 66c7e23b7262\n",
      "Step 3/7 : ENV CLOUD_PATH=\"/opt/ml/code\"\n",
      " ---> Using cache\n",
      " ---> f4171e572ab0\n",
      "Step 4/7 : COPY rapids_cloud_ml.py $CLOUD_PATH/rapids_cloud_ml.py\n",
      " ---> 836af9aebea8\n",
      "Step 5/7 : COPY train.py $CLOUD_PATH/train.py\n",
      " ---> fc5b3a3455f9\n",
      "Step 6/7 : ENV SAGEMAKER_PROGRAM train.py\n",
      " ---> Running in f81e2ac42334\n",
      "Removing intermediate container f81e2ac42334\n",
      " ---> 7493ced5e8ff\n",
      "Step 7/7 : WORKDIR $CLOUD_PATH\n",
      " ---> Running in efb9ae3cf9b2\n",
      "Removing intermediate container efb9ae3cf9b2\n",
      " ---> 2629cf8af0f9\n",
      "Successfully built 2629cf8af0f9\n",
      "Successfully tagged 561241433344.dkr.ecr.us-east-1.amazonaws.com/cloud-ml-sagemaker:0.14-cuda10.1-runtime-ubuntu18.04-py3.7\n",
      "CPU times: user 22.7 ms, sys: 14.5 ms, total: 37.2 ms\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!docker build . -t $ecr_fullname -f Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:18pt\"> 2.1.3 - Publish to Elastic Cloud Registry (ECR) </span>\n",
    "\n",
    "Now that we've built and tagged our container its time to push it to Amazon's container registry (ECR). Once in ECR, AWS SageMaker will be able to leverage our image to build Estimators and run experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker Login to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_login_str = !(aws ecr get-login --region {region[0]} --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!{docker_login_str[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ECR repository [ if it doesn't already exist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_query = !(aws ecr describe-repositories --repository-names $image_base)\n",
    "if repository_query[0] == '':\n",
    "    !(aws ecr create-repository --repository-name $image_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now actually push the container to ECR\n",
    "> Note the first push to ECR may take some time (hopefully less than 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'561241433344.dkr.ecr.us-east-1.amazonaws.com/cloud-ml-sagemaker:0.14-cuda10.1-runtime-ubuntu18.04-py3.7'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecr_fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [561241433344.dkr.ecr.us-east-1.amazonaws.com/cloud-ml-sagemaker]\n",
      "\n",
      "\u001b[1B9cc8bae6: Preparing \n",
      "\u001b[1B407e4868: Preparing \n",
      "\u001b[1Bce92dc39: Preparing \n",
      "\u001b[1B7905ac2b: Preparing \n",
      "\u001b[1Bf23a2c1c: Preparing \n",
      "\u001b[1B3564a770: Preparing \n",
      "\u001b[1Bfea6d95e: Preparing \n",
      "\u001b[1B74f76be4: Preparing \n",
      "\u001b[1Bd332a58a: Preparing \n",
      "\u001b[1Bf11cbf29: Preparing \n",
      "\u001b[1Ba4b22186: Preparing \n",
      "\u001b[1Bafb09dc3: Preparing \n",
      "\u001b[1Bb5a53aac: Preparing \n",
      "\u001b[1Bc8e5063e: Preparing \n",
      "\u001b[14B07e4868: Pushed lready exists 7kB4A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[3A\u001b[2K\u001b[14A\u001b[2K0.14-cuda10.1-runtime-ubuntu18.04-py3.7: digest: sha256:425bae3a186dc3befe9cd577b095ea7bc382bfb81f9cf8b96e356f4bc92e8c89 size: 3476\n"
     ]
    }
   ],
   "source": [
    "!docker push $ecr_fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 2.2 - Create Estimator </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built our container [ +custom logic] and pushed it to ECR, we can finally compile all of efforts into an Estimator instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_params = {\n",
    "    'image_name' : ecr_fullname,\n",
    "    \n",
    "    'train_use_spot_instances': use_spot_instances_flag,\n",
    "    'train_instance_type' : instance_type, \n",
    "    'train_instance_count' : 1,     \n",
    "    \n",
    "    'train_max_run'  : max_duration_of_experiment_seconds, # 24 hours \n",
    "    'train_max_wait' : max_duration_of_experiment_seconds + 1,  \n",
    "    \n",
    "    'input_mode'  : 'File',\n",
    "    'output_path' : s3_model_output,\n",
    "    \n",
    "    'sagemaker_session' : session,     \n",
    "    'role' : execution_role,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator( **estimator_params  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 2.3 - Test Estimator </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to test by asking SageMaker to run the BYOContainer logic inside our Estimator. This is a useful step if you've made changes to your custom logic and are interested in making sure everything works before launching a large HPO search. \n",
    "\n",
    "> Note: This verification step will use the default hyper-parameter values declared in our custom train code, as SageMaker HPO will not be orchestrating a search for this single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 data input    =\ts3://sagemaker-rapids-hpo-us-east-1/1_year\n",
      "s3 model output  =\ts3://sagemaker-rapids-hpo-us-east-1/trained-models\n",
      "compute          =\tsingleGPU\n",
      "algorithm        =\tXGBoost, 3 cv-fold\n",
      "instance         =\tml.p3.2xlarge\n",
      "spot instances   =\tTrue\n",
      "hpo strategy     =\tRandom\n",
      "max_experiments  =\t8\n",
      "max_parallel     =\t2\n",
      "max runtime      =\t86400 sec\n"
     ]
    }
   ],
   "source": [
    "summarize_choices( s3_data_input, s3_model_output, code_choice, algorithm_choice, \n",
    "                   cv_folds, instance_type, use_spot_instances_flag, search_strategy, \n",
    "                   max_jobs, max_parallel_jobs, max_duration_of_experiment_seconds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "confirm test run? [ y / n ] :  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated job name : 1y-sGPU-XGB-3cv-p3-2xlarge-dfc51\n",
      "\n",
      "2020-08-03 20:32:23 Starting - Starting the training job...\n",
      "2020-08-03 20:32:25 Starting - Launching requested ML instances......\n",
      "2020-08-03 20:33:41 Starting - Preparing the instances for training......\n",
      "2020-08-03 20:34:54 Downloading - Downloading input data\n",
      "2020-08-03 20:34:54 Training - Downloading the training image..................\n",
      "2020-08-03 20:37:52 Training - Training image download completed. Training in progress..\u001b[34m2020-08-03 20:37:52,771 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"1y-sgpu-xgb-3cv-p3-2xlarge-dfc51\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"1y-sgpu-xgb-3cv-p3-2xlarge-dfc51\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/envs/rapids/bin:/opt/conda/envs/rapids/lib/python37.zip:/opt/conda/envs/rapids/lib/python3.7:/opt/conda/envs/rapids/lib/python3.7/lib-dynload:/opt/conda/envs/rapids/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/envs/rapids/bin/python3.7 train.py\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mparsing compute & algorithm choices from job-name...\n",
      "\n",
      "  Compute: single-GPU\n",
      "  Algorithm: XGBoost\n",
      "  CV_folds: 3\n",
      "  RAPIDS version: 14\n",
      "\u001b[0m\n",
      "\u001b[34mparsing model hyper-parameters from command line arguments...\n",
      "\u001b[0m\n",
      "\u001b[34m{    'gamma': 0.0,\n",
      "     'lambda': 1,\n",
      "     'learning_rate': 0.3,\n",
      "     'max_depth': 5,\n",
      "     'num_boost_round': 10,\n",
      "     'objective': 'binary:logistic',\n",
      "     'random_state': 0,\n",
      "     'seed': 0,\n",
      "     'tree_method': 'gpu_hist',\n",
      "     'verbosity': 0}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training/*.parquet\u001b[0m\n",
      "\u001b[34m'/opt/ml/input/data/training/*.parquet'\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mdetected 13 files as input \n",
      "\u001b[0m\n",
      "\u001b[34m--- starting workflow --- \n",
      " \u001b[0m\n",
      "\u001b[34m|-> ETL : 4.8534\n",
      "\u001b[0m\n",
      "\u001b[34m|-> training XGBoost classifier on single-GPU : 0.3603\n",
      "\n",
      "    subfold score: 0.9264193773269653\n",
      "\u001b[0m\n",
      "\u001b[34m|-> predict [ XGBoost ] : 0.0135\n",
      "\u001b[0m\n",
      "\u001b[34m|-> ETL : 0.0619\n",
      "\u001b[0m\n",
      "\u001b[34m|-> training XGBoost classifier on single-GPU : 0.1066\n",
      "\n",
      "    subfold score: 0.9262214303016663\n",
      "\u001b[0m\n",
      "\u001b[34m|-> predict [ XGBoost ] : 0.0086\n",
      "\u001b[0m\n",
      "\u001b[34m|-> ETL : 0.0612\n",
      "\u001b[0m\n",
      "\u001b[34m|-> training XGBoost classifier on single-GPU : 0.1082\n",
      "\n",
      "    subfold score: 0.9271332025527954\n",
      "\u001b[0m\n",
      "\u001b[34m|-> predict [ XGBoost ] : 0.0084\n",
      "\u001b[0m\n",
      "\u001b[34m|-> saving model into /opt/ml/model/saved_model : 0.0029\n",
      "\u001b[0m\n",
      "\u001b[34m#011 averaging over fold scores : [0.9264193773269653, 0.9262214303016663, 0.9271332025527954]\u001b[0m\n",
      "\u001b[34m#011 final-score: 0.9265913367271423; \n",
      "\u001b[0m\n",
      "\u001b[34mtotal elapsed time = 6 seconds\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-03 20:38:04,142 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-08-03 20:38:14 Uploading - Uploading generated training model\n",
      "2020-08-03 20:38:14 Completed - Training job completed\n",
      "Training seconds: 211\n",
      "Billable seconds: 63\n",
      "Managed Spot Training savings: 70.1%\n"
     ]
    }
   ],
   "source": [
    "assert ( input('confirm test run? [ y / n ] : ').lower() == 'y' )\n",
    "\n",
    "job_name = new_job_name_from_config( dataset_directory, code_choice, \n",
    "                                     algorithm_choice, cv_folds,\n",
    "                                     instance_type  )\n",
    "\n",
    "estimator.fit(inputs = s3_data_input, job_name=job_name.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: block; text-align: center; color:#8735fb; font-size:30pt\"> **3. Run HPO** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a working SageMaker Estimator in hand, the hardest part is behind us. In the key choices section we <a href='#strategy-and-param-ranges'>already defined our search strategy and hyper-parameter ranges</a>, so all that remains is to choose a metric to evaluate performance on. For more documentation check out the AWS SageMaker [HyperParameter Tuner documentation](https://sagemaker.readthedocs.io/en/stable/tuner.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/run_hpo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 3.1 - Define Metric </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only focus on a single metric, which we call 'final-score', that captures the accuracy of our model on the test data unseen during training. You are of course welcome to add aditional metrics, see [AWS SageMaker documentation on Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html). When defining a metric we provide a regular expression (i.e., string parsing rule) to extract the key metric from the output of each Estimator/worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{'Name': 'final-score', 'Regex': 'final-score: (.*);'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'final-score'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 3.2 - Define Tuner </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we put all of the elements we've been building up together into a HyperparameterTuner declaration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = sagemaker.tuner.HyperparameterTuner( estimator = estimator,\n",
    "                                           metric_definitions = metric_definitions, \n",
    "                                           objective_metric_name = objective_metric_name,\n",
    "                                           objective_type = 'Maximize',\n",
    "                                           hyperparameter_ranges = hyperparameter_ranges,\n",
    "                                           strategy = search_strategy,  \n",
    "                                           max_jobs = max_jobs,\n",
    "                                           max_parallel_jobs = max_parallel_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 3.3 - Run HPO </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 data input    =\ts3://sagemaker-rapids-hpo-us-east-1/1_year\n",
      "s3 model output  =\ts3://sagemaker-rapids-hpo-us-east-1/trained-models\n",
      "compute          =\tsingleGPU\n",
      "algorithm        =\tXGBoost, 3 cv-fold\n",
      "instance         =\tml.p3.2xlarge\n",
      "spot instances   =\tTrue\n",
      "hpo strategy     =\tRandom\n",
      "max_experiments  =\t8\n",
      "max_parallel     =\t2\n",
      "max runtime      =\t86400 sec\n"
     ]
    }
   ],
   "source": [
    "summarize_choices( s3_data_input, s3_model_output, code_choice, algorithm_choice, \n",
    "                   cv_folds, instance_type, use_spot_instances_flag, search_strategy, \n",
    "                   max_jobs, max_parallel_jobs, max_duration_of_experiment_seconds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be sure we take a moment to confirm before launching all of our HPO experiments. Depending on your configuration options running this cell can kick off a massive amount of computation!\n",
    "> Once this process begins, we recommend that you use the SageMaker UI to keep track of the <a href='../img/gpu_hpo_100x10.png'>health of the HPO process and the individual workers</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "confirm HPO launch? [ y / n ] :  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated job name : 1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d\n",
      "\n",
      ".............................................................................................................................................................................................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "assert ( input('confirm HPO launch? [ y / n ] : ').lower() == 'y' )\n",
    "\n",
    "tuning_job_name = new_job_name_from_config( dataset_directory, code_choice, \n",
    "                                            algorithm_choice, cv_folds, \n",
    "                                            instance_type )\n",
    "hpo.fit( inputs = s3_data_input, \n",
    "         job_name = tuning_job_name, \n",
    "         wait = True, logs = 'All')\n",
    "\n",
    "hpo.wait() # block until the .fit call above is completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 3.4 - Results and Summary </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your job is complete there are multiple ways to analyze the results.\n",
    "Below we display the performance of the best job, as well printing each HPO trial/job as a row of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.9360807538032532\n",
      "best params: {'max_depth': '6', 'max_features': '0.704100184794978', 'num_boost_round': '293'}\n",
      "best job-name: 1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-007-46dea283\n"
     ]
    }
   ],
   "source": [
    "hpo_results = summarize_hpo_results ( tuning_job_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>num_boost_round</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.364925</td>\n",
       "      <td>305.0</td>\n",
       "      <td>1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-008-a4d0d5b6</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.930250</td>\n",
       "      <td>2020-08-03 21:09:43+00:00</td>\n",
       "      <td>2020-08-03 21:14:01+00:00</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>293.0</td>\n",
       "      <td>1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-007-46dea283</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.936081</td>\n",
       "      <td>2020-08-03 21:08:35+00:00</td>\n",
       "      <td>2020-08-03 21:12:18+00:00</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.386082</td>\n",
       "      <td>385.0</td>\n",
       "      <td>1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-006-d8fc5553</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.931477</td>\n",
       "      <td>2020-08-03 21:02:28+00:00</td>\n",
       "      <td>2020-08-03 21:06:59+00:00</td>\n",
       "      <td>271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.877136</td>\n",
       "      <td>252.0</td>\n",
       "      <td>1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-005-e7e3c8f4</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.934227</td>\n",
       "      <td>2020-08-03 21:01:13+00:00</td>\n",
       "      <td>2020-08-03 21:05:34+00:00</td>\n",
       "      <td>261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.310162</td>\n",
       "      <td>484.0</td>\n",
       "      <td>1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-004-8a3d1501</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.930274</td>\n",
       "      <td>2020-08-03 20:54:41+00:00</td>\n",
       "      <td>2020-08-03 20:59:35+00:00</td>\n",
       "      <td>294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.303105</td>\n",
       "      <td>471.0</td>\n",
       "      <td>1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-003-12cb8f08</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.931133</td>\n",
       "      <td>2020-08-03 20:53:31+00:00</td>\n",
       "      <td>2020-08-03 20:57:58+00:00</td>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.965175</td>\n",
       "      <td>253.0</td>\n",
       "      <td>1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-002-2dea4d7c</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.932481</td>\n",
       "      <td>2020-08-03 20:46:45+00:00</td>\n",
       "      <td>2020-08-03 20:50:51+00:00</td>\n",
       "      <td>246.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.370175</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-001-50986a4f</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.930297</td>\n",
       "      <td>2020-08-03 20:46:44+00:00</td>\n",
       "      <td>2020-08-03 20:51:52+00:00</td>\n",
       "      <td>308.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  max_features  num_boost_round  \\\n",
       "0       13.0      0.364925            305.0   \n",
       "1        6.0      0.704100            293.0   \n",
       "2       11.0      0.386082            385.0   \n",
       "3        9.0      0.877136            252.0   \n",
       "4       13.0      0.310162            484.0   \n",
       "5       11.0      0.303105            471.0   \n",
       "6       11.0      0.965175            253.0   \n",
       "7       15.0      0.370175            180.0   \n",
       "\n",
       "                                 TrainingJobName TrainingJobStatus  \\\n",
       "0  1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-008-a4d0d5b6         Completed   \n",
       "1  1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-007-46dea283         Completed   \n",
       "2  1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-006-d8fc5553         Completed   \n",
       "3  1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-005-e7e3c8f4         Completed   \n",
       "4  1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-004-8a3d1501         Completed   \n",
       "5  1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-003-12cb8f08         Completed   \n",
       "6  1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-002-2dea4d7c         Completed   \n",
       "7  1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-001-50986a4f         Completed   \n",
       "\n",
       "   FinalObjectiveValue         TrainingStartTime           TrainingEndTime  \\\n",
       "0             0.930250 2020-08-03 21:09:43+00:00 2020-08-03 21:14:01+00:00   \n",
       "1             0.936081 2020-08-03 21:08:35+00:00 2020-08-03 21:12:18+00:00   \n",
       "2             0.931477 2020-08-03 21:02:28+00:00 2020-08-03 21:06:59+00:00   \n",
       "3             0.934227 2020-08-03 21:01:13+00:00 2020-08-03 21:05:34+00:00   \n",
       "4             0.930274 2020-08-03 20:54:41+00:00 2020-08-03 20:59:35+00:00   \n",
       "5             0.931133 2020-08-03 20:53:31+00:00 2020-08-03 20:57:58+00:00   \n",
       "6             0.932481 2020-08-03 20:46:45+00:00 2020-08-03 20:50:51+00:00   \n",
       "7             0.930297 2020-08-03 20:46:44+00:00 2020-08-03 20:51:52+00:00   \n",
       "\n",
       "   TrainingElapsedTimeSeconds  \n",
       "0                       258.0  \n",
       "1                       223.0  \n",
       "2                       271.0  \n",
       "3                       261.0  \n",
       "4                       294.0  \n",
       "5                       267.0  \n",
       "6                       246.0  \n",
       "7                       308.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For a more in depth look at the HPO process we invite you to check out the <a href='https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning/analyze_results'>HPO_Analyze_TuningJob_Results.ipynb</a> notebook which shows how we can explore interesting things like the <a href='img/results_analysis.png'>impact of each individual hyper-parameter on the performance metric</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 3.5 - Getting the best Model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's download the best trained model from our HPO runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/rapids_bring_your_own'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded best model\n",
      "> filename: best_model.tar.gz\n",
      "> local directory : /home/ec2-user/SageMaker/rapids_bring_your_own\n",
      "\n",
      "full S3 path : s3://sagemaker-rapids-hpo-us-east-1/trained-models/1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-007-46dea283/output/trained-models/1y-sGPU-XGB-3cv-p3-2xlarge-b4b5d-007-46dea283/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "local_filename = download_best_model( bucket, s3_model_output, hpo_results, working_directory )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> 3.6 - Model Serving </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your best model in hand, you can now move on to serving this model. \n",
    "\n",
    "The [SageMaker Inference Toolkit](https://github.com/aws/sagemaker-inference-toolkit) implements a model serving stack and can be easily added to any Docker container, making it deployable to SageMaker. This library's serving stack is built on Multi Model Server, and it can serve your own models or those you trained on SageMaker using machine learning frameworks with native SageMaker support. \n",
    "\n",
    "We'll leave you with pointers to documentation in case you want to go further, however a full implementation is out of scope for our HPO notebook. \n",
    "\n",
    "> Note that the best model we just downloaded is stored in a [Treelite](https://treelite.readthedocs.io/en/latest/) format and can run optimized inference using the GPU or the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> Summary </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now succesfully built a RAPIDS ML workflow, containerized it (as a SageMaker Estimator), and launched a set of HPO experiments to find the best hyper-paramters for our model.\n",
    "\n",
    "If you are curious to go further, we invite you to plug in your own dataset and tweak the configuration settings to find your champion model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HPO Experiment Details**\n",
    "<a id='experiments'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction we find a <span style=\"color:#8735fb; font-size:14pt\"> **12X** </span> speedup in wall clock time and a <span style=\"color:#8735fb; font-size:14pt\"> **4.5x** </span> reduction in cost when comparing between GPU and CPU instances on 100 HPO trials using 10 parallel workers on 10 years of the Airline Dataset (~63M flights). In these experiments we used the XGBoost algorithm with the multi-GPU vs multi-CPU Dask cluster and 10 cross validaiton folds. Below we offer a table with additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/results.png' width='70%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the CPU runs, 12 jobs were stopped since they exceeded the 24 hour limit we set. <a href='img/cpu_hpo_100x10.png'>CPU Job Summary Image</a>\n",
    "\n",
    "In the case of the GPU runs, no jobs were stopped. <a href='img/gpu_hpo_100x10.png'>GPU Job Summary Image</a>\n",
    "\n",
    "Note that in both cases 1 job failed because a spot instance was terminated. But 1 failed job out of 100 is a minimal tradeoff for the significant cost savings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> Rapids References </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [cloud-ml-examples](http://github.com/rapidsai/cloud-ml-examples)\n",
    "\n",
    "> [RAPIDS HPO](https://rapids.ai/hpo)\n",
    "\n",
    "> [cuML Documentation](https://docs.rapids.ai/api/cuml/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8735fb; font-size:20pt\"> SageMaker References </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [SageMaker Training Toolkit](https://github.com/aws/sagemaker-training-toolkit)\n",
    "\n",
    "> [Estimator Parameters](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)\n",
    "\n",
    "> Spot Instances [docs](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html), and [blog]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
