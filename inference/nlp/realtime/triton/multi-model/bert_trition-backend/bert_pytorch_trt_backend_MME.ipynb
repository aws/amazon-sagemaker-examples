{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10765e2",
   "metadata": {},
   "source": [
    "# Run Multiple NLP Bert Models on GPU with Amazon SageMaker Multi-Model Endpoints (MME)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10765e2",
   "metadata": {},
   "source": [
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) multi-model endpoints(MME) provide a scalable and cost-effective way to deploy large number of deep learning models. Previously, customers had limited options to deploy 100s of deep learning models that need accelerated compute with GPUs. Now customers can deploy 1000s of deep learning models behind one SageMaker endpoint. MME can run multiple models on a GPU core, share GPU instances behind an endpoint across multiple models and dynamically load/unload models based on the incoming traffic. With this, customers can significantly save cost and achieve best price performance.\n",
    "\n",
    "<div class=\"alert alert-info\"> <strong> Note </strong>\n",
    "This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `g5`.\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3507418",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Multi-Model endpoints with GPU Support\n",
    "\n",
    "Amazon SageMaker multi-model endpoints with GPU work using [NVIDIA Triton Inference Server](https://github.com/triton-inference-server/server/). NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance. Triton supports all major training and inference frameworks, such as TensorFlow, NVIDIA TensorRT, PyTorch, MXNet, Python, ONNX, XGBoost, scikit-learn, RandomForest, OpenVINO, custom C++, and more. It offers dynamic batching, concurrent execution, post-training quantization, optimal model configuration to achieve high performance inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25ac5b76",
   "metadata": {},
   "source": [
    "## How it works?\n",
    "\n",
    "1. SageMaker routes traffic to the right instance behind the endpoint where the target model is loaded. SageMaker takes care of model management behind the endpoint, loads model to the container's memory and unloads the model based on the endpoint's traffic pattern.\n",
    "2. Dynamically loads models from Amazon Simple Storage Service(S3) to the instance\u2019s storage volume. If the invoked models are not available on instance storage volume, the model is downloaded onto instance storage volume. If the instance storage volume reaches capacity, SageMaker deletes any unused models from the storage volume.\n",
    "3. SageMaker loads the model to NVIDIA Triton container\u2019s memory on GPU accelerated instance and serve the inference request. If the model is already loaded in the container memory, the subsequent requests are served faster as SageMaker does not need to download and load it again.\n",
    "4. SageMaker takes care of traffic shaping to the MME endpoint, SageMaker continues to route traffics to the instance where the model is loaded. If the instance resources reach capacity due to high utilization, SageMaker unloads the least used models from the container to free up resource to load more frequently used models.\n",
    "5. SageMaker MME can horizontally scale using auto-scaling policy, provision additional GPU compute instances based on metrics such as GPU utilization, memory utilization etc. to serve spiky traffic to MME endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605eb9f",
   "metadata": {},
   "source": [
    "In this notebook, we will show you how to use the new features Amazon SageMaker MME with GPU with a Natural Language Processing (NLP) use case. For demonstration purpose, we will use a PyTorch NLP-Bert pre-trained model. We will:\n",
    "\n",
    "*  Show how to use NVIDIA Triton inference container on SageMaker MME. \n",
    "*  Walk you through steps to convert Bert models to optimized TensorRT engine format and deploy it with SageMaker MME. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ecd426d",
   "metadata": {},
   "source": [
    "## Introduction to NVIDIA Triton Server\n",
    "\n",
    "[NVIDIA Triton Inference Server](https://github.com/triton-inference-server/server/) was developed specifically to enable scalable, cost-effective, and easy deployment of models in production. NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance.\n",
    "\n",
    "Some key features of Triton are:\n",
    "* **Support for Multiple frameworks**: Triton can be used to deploy models from all major frameworks. Triton supports TensorFlow GraphDef, TensorFlow SavedModel, ONNX, PyTorch TorchScript, TensorRT, RAPIDS FIL for tree based models, and OpenVINO model formats. \n",
    "* **Model pipelines**: Triton model ensemble represents a pipeline of one or more models or pre/post-processing logic and the connection of input and output tensors between them. A single inference request to an ensemble will trigger the execution of the entire pipeline.\n",
    "* **Concurrent model execution**: Multiple models (or multiple instances of the same model) can run simultaneously on the same GPU or on multiple GPUs for different model management needs.\n",
    "* **Dynamic batching**: For models that support batching, Triton has multiple built-in scheduling and batching algorithms that combine individual inference requests together to improve inference throughput. These scheduling and batching decisions are transparent to the client requesting inference.\n",
    "* **Diverse CPUs and GPUs**: The models can be executed on CPUs or GPUs for maximum flexibility and to support heterogeneous computing requirements.\n",
    "\n",
    "**Note**: This initial release of NVIDIA Triton on SageMaker will only support a single model. Future releases will have multi-model support. A minimal `config.pbtxt` configuration file is **required** in the model artifacts. This release doesn't support inferring the model config automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4833d22",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848b90c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker\n",
    "!pip install transformers==4.26.1\n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3841173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69aa959",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50939fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:23.02-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a471bcf",
   "metadata": {},
   "source": [
    "## Add utility methods for preparing request payload\n",
    "\n",
    "The following method transforms the sample text we will be using for inference into the payload that can be sent for inference to the Triton server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75bb28",
   "metadata": {},
   "source": [
    "The `tritonclient` package provides utility methods to generate the payload without having to know the details of the specification. We'll use the following methods to convert our inference request into a binary format which provides lower latencies for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2316be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_tokenizer():\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    enc = get_tokenizer()\n",
    "    encoded_text = enc(text, padding=\"max_length\", max_length=128)\n",
    "    return encoded_text[\"input_ids\"], encoded_text[\"attention_mask\"]\n",
    "\n",
    "\n",
    "def _get_sample_tokenized_text_binary(text, input_names, output_names):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    inputs.append(httpclient.InferInput(input_names[0], [1, 128], \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(input_names[1], [1, 128], \"INT32\"))\n",
    "    indexed_tokens, attention_mask = tokenize_text(text)\n",
    "\n",
    "    indexed_tokens = np.array(indexed_tokens, dtype=np.int32)\n",
    "    indexed_tokens = np.expand_dims(indexed_tokens, axis=0)\n",
    "    inputs[0].set_data_from_numpy(indexed_tokens, binary_data=True)\n",
    "\n",
    "    attention_mask = np.array(attention_mask, dtype=np.int32)\n",
    "    attention_mask = np.expand_dims(attention_mask, axis=0)\n",
    "    inputs[1].set_data_from_numpy(attention_mask, binary_data=True)\n",
    "\n",
    "    outputs.append(httpclient.InferRequestedOutput(output_names[0], binary_data=True))\n",
    "    outputs.append(httpclient.InferRequestedOutput(output_names[1], binary_data=True))\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length\n",
    "\n",
    "\n",
    "def get_sample_tokenized_text_binary_pt(text):\n",
    "    return _get_sample_tokenized_text_binary(\n",
    "        text, [\"INPUT__0\", \"INPUT__1\"], [\"OUTPUT__0\", \"1634__1\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_sample_tokenized_text_binary_trt(text):\n",
    "    return _get_sample_tokenized_text_binary(\n",
    "        text, [\"token_ids\", \"attn_mask\"], [\"output\", \"pooled_output\"]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c77de240",
   "metadata": {},
   "source": [
    "### Prepare TensorRT Model\n",
    "1. We export the pre-trained Bert model into an ONNX file, which runs the model once to trace its execution and then export the traced model to the specified file. It is one of the better options in terms model conversion and deployment when converting using ONNX.\n",
    "2. We use `trtexec` to automatically convert ONNX model to TensorRT plan. As ONNX is framework-agnostic it works with models in TF, PyTorch and more. You will export the weights of your model from the framework and load them into your TensorRT network.\n",
    "\n",
    "In this step, we load pre-trained Bert model and convert to onnx representation using torch onnx exporter. Once onnx model is created, we use TensorRT trtexec command to create the model plan to be hosted with Triton. This is run as part of the `generate_model.sh` script from the below cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1ab43",
   "metadata": {},
   "source": [
    "The below cell would take around 30 minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a30625e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it \\\n",
    "            -v `pwd`/workspace:/workspace nvcr.io/nvidia/pytorch:23.02-py3 \\\n",
    "            /bin/bash generate_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b25a0a",
   "metadata": {},
   "source": [
    "## TensorRT NLP-Bert Model Repository\n",
    "\n",
    "Another way to improve performance is to convert the PyTorch NLP-Bert model to a TensorRT plan and use it natively to run inferences on Triton. By using the [onnx_exporter.py](./workspace/onnx_exporter.py) script and `trtexec` we create a TensorRT plan from the pre-trained PyTorch NLP-Bert model. This is already done as part of the `generate_models.sh` script that we ran earlier in this notebook. We'll package the model and provide the `config.pbtxt` file according the Triton model specification and upload to s3 for creating a SageMaker model and endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd553db",
   "metadata": {},
   "source": [
    "### TensorRT Model configuration\n",
    "The `config.pbtxt` files contains the flow of how an inference request is first passed to the tokenizer and then the tokenized output is passed to the model in the order to get the final output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repo_0/bert_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb6f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_repo_0/bert_0/config.pbtxt\n",
    "name: \"bert\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 128\n",
    "input [\n",
    "  {\n",
    "    name: \"token_ids\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"attn_mask\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [128]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [128, 768]\n",
    "  },\n",
    "  {\n",
    "    name: \"pooled_output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: 16\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77901b1",
   "metadata": {},
   "source": [
    "### TensorRT: Packaging model files and uploading to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b4ee3",
   "metadata": {},
   "source": [
    "SageMaker expects a .tar.gz file containing each Triton model repository to be hosted on the multi-model endpoint. To simulate several similar models being hosted, you might think all it takes is to tar the model repository we have already built, and then copy it with different file names. However, Triton requires unique model names. Therefore, we will first copy the model repo `N` times, changing the model directory names and their corresponding config.pbtxt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f471a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repo_0/bert_0/1/\n",
    "!cp workspace/model_bs16.plan model_repo_0/bert_0/1/model.plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e94dd1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> <strong> Note </strong>\n",
    "In this example, we are using many copies of the same model to simulate a situation where different personalized (fine-tuned) instances of the same architecture need be served, and because it is easy to demonstrate; however, you can place a different model and inference script in each tar.gz file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "N = 5\n",
    "prefix = 'bert-mme'\n",
    "model_repo_base = 'model_repo'\n",
    "\n",
    "# Get model names from model_repo_0\n",
    "model_names = [name for name in os.listdir(f'{model_repo_base}_0') if os.path.isdir(f'{model_repo_base}_0/{name}')]\n",
    "\n",
    "for i in range(N):\n",
    "    # Make copy of previous model repo, increment # id\n",
    "    shutil.copytree(f'{model_repo_base}_0', f'{model_repo_base}_{i+1}')\n",
    "    time.sleep(5)\n",
    "    for name in model_names:\n",
    "        model_dirs_path = f'{model_repo_base}_{i+1}/{name}'\n",
    "\n",
    "        # Open each model's config file to increment model # id there \n",
    "        fin = open(f'{model_dirs_path}/config.pbtxt', \"rt\")\n",
    "        data = fin.read()\n",
    "        data = data.replace(name, name[:-1] + str(i+1))\n",
    "        fin.close()\n",
    "        fin = open(f'{model_dirs_path}/config.pbtxt', \"wt\")\n",
    "        fin.write(data)\n",
    "        fin.close()\n",
    "    \n",
    "        # Change model directory name to match new config\n",
    "        os.rename(model_dirs_path,model_dirs_path[:-1]+str(i+1))\n",
    "        time.sleep(2)\n",
    "        \n",
    "    if i == 0:\n",
    "        tar_file_name = f'bert-{i}.tar.gz'\n",
    "        model_repo_target = f'{model_repo_base}_{i}/'\n",
    "        !tar -C $model_repo_target -czf $tar_file_name .\n",
    "        sagemaker_session.upload_data(path=tar_file_name, key_prefix=prefix)\n",
    "\n",
    "    tar_file_name = f'bert-{i+1}.tar.gz'\n",
    "    model_repo_target = f'{model_repo_base}_{i+1}/'\n",
    "    !tar -C $model_repo_target -czf $tar_file_name .\n",
    "    sagemaker_session.upload_data(path=tar_file_name, key_prefix=prefix)\n",
    "    !sudo rm -r \"$tar_file_name\" \"$model_repo_target\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0c6cf2",
   "metadata": {},
   "source": [
    "### TensorRT: Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1140ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"triton-nlp-bert-trt-mme-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "model_data_uri = f\"s3://{bucket}/{prefix}/\"\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data_uri,\n",
    "    #     \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"bert\"},\n",
    "    \"Mode\": \"MultiModel\",\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6943b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"triton-nlp-bert-trt-mme-\" + time.strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\", time.gmtime()\n",
    ")\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37deacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"triton-nlp-bert-trt-mme-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674b6fb",
   "metadata": {},
   "source": [
    "### TensorRT: Run inference\n",
    "\n",
    "We can send inference request to multi-model endpoint using `invoke_endpoint` API. We specify the `TargetModel` in the invocation call and pass in the payload for each model type.\n",
    "\n",
    "Once we have the endpoint running we can run the inference both using a json payload and binary+json payload as described in the standard PyTorch deployment section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92789136",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_triton = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\"\n",
    "input_ids, attention_mask = tokenize_text(text_triton)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"token_ids\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"attn_mask\", \"shape\": [1, 128], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "for i in range(N):\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/octet-stream\",\n",
    "        Body=json.dumps(payload),\n",
    "        TargetModel=f\"bert-{i}.tar.gz\",\n",
    "    )\n",
    "\n",
    "    print(json.loads(response[\"Body\"].read().decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5bec51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_sm = \"Amazon SageMaker helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML.\"\n",
    "request_body, header_length = get_sample_tokenized_text_binary_trt(text_sm)\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/vnd.sagemaker-triton.binary+json;json-header-size={}\".format(\n",
    "        header_length\n",
    "    ),\n",
    "    Body=request_body,\n",
    "    TargetModel=\"bert-0.tar.gz\",\n",
    ")\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response[\"Body\"].read(), header_length=int(header_length_str)\n",
    ")\n",
    "# print(response)\n",
    "# print(result)\n",
    "output0_data = result.as_numpy(\"output\")\n",
    "output1_data = result.as_numpy(\"pooled_output\")\n",
    "print(output0_data)\n",
    "print(output1_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec04953a",
   "metadata": {},
   "source": [
    "#### Cloudwatch metrics for GPU Multi Model Endpoints\n",
    "\n",
    "Amazon SageMaker multi-model endpoints provides instance level metrics to monitor, for more details refer [Monitor Amazon SageMaker with Amazon CloudWatch](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html)\n",
    "\n",
    "\n",
    "*  Number of models loaded in the containers (LoadedModelCount),  \n",
    "* Precentage of GPU units that are used by the containers (GPUUtilization), \n",
    "* Precentage of GPU memory used by the containers (GPUMemoryUtilization), \n",
    "* Precentage of disk space used by the containers (DiskUtilization) etc. \n",
    "\n",
    "SageMaker MME also provides Model loading metrics such as-\n",
    "\n",
    "*  Time interval for model to be downloaded or loaded (ModelLoadingWaitTime),\n",
    "*  Time interval to unload model from container (ModelUnloadingTime),\n",
    "*  Time to download the model from S3 (ModelDownloadingTime), \n",
    "* Number of invocations to model that are already loaded onto the container(ModelCacheHit) etc. to get model invocation level insights. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5656529",
   "metadata": {},
   "source": [
    "### TensorRT: Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14863e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|nlp|realtime|triton|multi-model|bert_trition-backend|bert_pytorch_trt_backend_MME.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g5.xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}