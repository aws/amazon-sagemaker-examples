{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker XGBoost model applied to NYC TLC trip data\n",
    "\n",
    "In this example, we will train a model using  [Amazon SageMaker XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to predict NYC TLC trip duration in minutes based on feature vector that includes ride pickup source zone, destination zone, month, day of the week and hour of the day for the ride. \n",
    "\n",
    "The data used to train the XGBoost model was prepared using PySpark cluster as described in this [companion notebook](emr-pyspark-nyc-tlc.ipynb) and saved in S3 bucket. So, if you have not executed the companion notebook, please do that before executing this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Amazon SageMaker Execution Role\n",
    "\n",
    "This notebook requires a Python 3 kernel with SageMaker support, for example `conda_python3` kernel.\n",
    "\n",
    "The first step in using Amazon SageMaker is to create an execution role that encapsulates permissions used by Amazon SageMaker to access other AWS services, so we do that as the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "# Create SageMaker role \n",
    "role = get_execution_role()\n",
    "print(f'Amazon SageMaker execution role: {role}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get XGBoost Amazon ECR URI\n",
    "Every Amazon SageMaker model training requires an [Amazon ECR](https://aws.amazon.com/ecr/) image that provides an [Amazon SageMaker training container](https://docs.aws.amazon.com/sagemaker/latest/dg/amazon-sagemaker-containers.html). So, next, we specify the URI for the Amazon ECR docker image for Amazon SageMaker XGBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the url to the container image for using linear-learner\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "xgboost_image = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')\n",
    "print(f'Amazon SageMaker XGBoost Algorithm ECR Image URI: {xgboost_image}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training data into training, validation and test\n",
    "\n",
    "At the end of the data preparation step executed in [companion notebook](emr-pyspark-nyc-tlc.ipynb), we get multiple CSV files saved in your S3 bucket. Each CSV file contains data with columns for trip duration in minutes, origin zone, destination zone, and pickup month, ordinal day of the week, and hour of day. \n",
    "\n",
    "Specify `source_bucket` S3 bucket where you saved the prepared data in the [companion notebook](emr-pyspark-nyc-tlc.ipynb). Specify `dest_bucket` S3 bucket where you want to load the split training, validaiton and test datasets. The `source_bucket` and `dest_bucket` can be the same S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source bucket with CSV files produced by Amazon EMR spark cluster data preparation\n",
    "source_bucket=\n",
    "source_prefix='emr/output/uber_nyc/v1'\n",
    "\n",
    "# destination bucket to upload SageMaker training input data files\n",
    "# can be the same as source_bucket\n",
    "dest_bucket=source_bucket\n",
    "dest_prefix = 'sagemaker/input/uber_nyc/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split input data into <b>training, validation and test</b> datasets for SageMaker XGBoost model training. We split the data by downloading the input CSV data files from `source_bucket` and separating the downloaded files into three folders. We reserve 80% of the downloaded files for training, 15% for validation and 5% for test, and upload the separated data files using `training`, `validation` and `test` prefixes into the `dest_bucket` S3 bucket to stage the data for SageMaker XGBoost model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import csv\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response=s3.list_objects_v2(Bucket=source_bucket, Prefix=source_prefix)\n",
    "contents=response['Contents']\n",
    "count=len(contents)\n",
    "\n",
    "sbucket = boto3.resource('s3').Bucket(source_bucket)\n",
    "ntrain=int(count*0.80)\n",
    "nval = int(count*0.15)\n",
    "ntest = count - ntrain - nval\n",
    "\n",
    "def stage_data(start, end, name):  \n",
    "    for i in range(start, end, 1):\n",
    "        item=contents[i]\n",
    "        key =item['Key']    \n",
    "        if not key.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        with tempfile.NamedTemporaryFile(mode='w+b', suffix='.csv', prefix='data-', delete=True) as csv_file:\n",
    "            print(f'Download {key} file')\n",
    "            sbucket.download_fileobj(key, csv_file)\n",
    "            \n",
    "            with open(csv_file.name, 'rb') as data_reader:\n",
    "                dest_key = f'{dest_prefix}/{name}/part-{i}.csv'\n",
    "                print(f'upload {dest_key} file')\n",
    "                s3.upload_fileobj(data_reader, dest_bucket, dest_key)\n",
    "                data_reader.close()\n",
    "        \n",
    "            csv_file.close()\n",
    "            \n",
    "stage_data(0, ntrain, 'train')\n",
    "stage_data(ntrain, ntrain+nval, 'validation')\n",
    "stage_data(ntrain+nval, count, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Amazon SageMaker data input channels\n",
    "\n",
    "Next, we  define <b>train and validation</b> input channels for Amazon SageMaker XGBoost model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import s3_input\n",
    "\n",
    "s3_train = s3_input(s3_data=f's3://{dest_bucket}/{dest_prefix}/train', content_type='csv')\n",
    "s3_validation = s3_input(s3_data=f's3://{dest_bucket}/{dest_prefix}/validation', content_type='csv')\n",
    "\n",
    "output_path=f's3://{dest_bucket}/sagemaker/output/uber_nyc/xgboost'\n",
    "data_channels = {'train': s3_train, 'validation': s3_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Amazon SageMaker XGBoost Estimator\n",
    "\n",
    "Next, we define [SageMaker Estimator](https://sagemaker.readthedocs.io/en/stable/estimators.html) for training [Amazon SageMaker XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). In this example, we use `ml.c5.9xlarge` instance type and we use 1 training instance. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "\n",
    "xgb = Estimator(image_name=xgboost_image,\n",
    "                            role=role, \n",
    "                            train_instance_count=1, \n",
    "                            train_instance_type='ml.c5.9xlarge',\n",
    "                            output_path=output_path,\n",
    "                            sagemaker_session=sagemaker_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify XGBoost Hyper-parameters\n",
    "Next, we specify required [XGBoost hyper-parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html) that we know at this point. However, there are some  XGBoost hyper-parameters, `num_rounds` and `tree_depth`, which we will need to discover through hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(objective='reg:linear',\n",
    "                       grow_policy='depthwise',\n",
    "                       tree_method = 'approx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Amazon SageMaker XGBoost Hyper-parameter Tuner ###\n",
    "\n",
    "Before we train the XGBoost model, we need to discover optimal values for the hyper-parameters required by the model, `num_round` and `max_depth`, using hyper-parameter tuner.\n",
    "\n",
    "The hyper-parameter tuner expects ranges for the hyper-parameters that need to be tuned, so we define those ranges next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.tuner import CategoricalParameter\n",
    "\n",
    "objective_metric_name = \"validation:rmse\"\n",
    "\n",
    "num_round = IntegerParameter(50,200)\n",
    "max_depth = IntegerParameter(8,32)\n",
    "hyperparameter_ranges={}\n",
    "hyperparameter_ranges['num_round'] = num_round\n",
    "hyperparameter_ranges['max_depth'] = max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a [hyper-parameter tuner](https://sagemaker.readthedocs.io/en/stable/tuner.html) that will use Bayesian search to minimize validation <b>Root Mean Squere Error (RMSE)</b> objective. We limit the maximum total hyper-parameter tuning `max_jobs` to 10 and `max_parallel_jobs` to 1. \n",
    "\n",
    "Each Hyper-parameter tuning job spwans a set of `max_parallel_jobs` training jobs with different values for `num_round` and `max_depth` hyper-parameters. The objective value obtained from the training jobs is used with Bayesian search strategy to explore new values for `num_round` and `max_depth` hyper-parameters. The total number of training jobs attempted is limited by `max_jobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tuner=HyperparameterTuner(xgb, \n",
    "                                         objective_metric_name, \n",
    "                                         hyperparameter_ranges, \n",
    "                                         strategy='Bayesian', \n",
    "                                         objective_type='Minimize', \n",
    "                                         max_jobs=15, \n",
    "                                         max_parallel_jobs=1, \n",
    "                                         base_tuning_job_name='xgboost-tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Hyper-parameter Tuning ###\n",
    "Next, we start the hyper-parameter tuning jobs, which are launched <b>asynchornously</b>. You can use Amazon SageMaker console to monitor hyper-parameter tuning jobs. The duration of this asynchronous step depends on the `max_jobs` and `max_parallel_jobs` specified above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tuner.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Best Hyper-parameters\n",
    "\n",
    "Next,we will use the best hyper-parameters values found in Amazon SageMaker console to set `num_round` and `max_depth` hyper-parameters for the training estimator. Below is an example. You may see different best values for `num_round` and `max_depth` hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(objective='reg:linear',\n",
    "                        grow_policy='depthwise',\n",
    "                        tree_method = 'approx',\n",
    "                        num_round=166, \n",
    "                        max_depth=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training Job ###\n",
    "After specifying best values for `num_round` and `max_depth` hyper-parameters, we are ready to start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model to Endpoint ###\n",
    "\n",
    "Once training is complete we are ready to deploy the trained model to an Amazon SageMaker model endpoint. We use  `ml.m5.xlarge` instance type and 1 instance to serve the model from a REST service endpoint. This step can take several minutes, so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1, \n",
    "               instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions for Test Data ###\n",
    "Below we import the classes to make prediction with test data. These classes are used to serialize and de-serialize the data to SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we download the test data and submit the test data to Amazon SageMaker deployed endpoint to make predctions on test data and compare the predictions to expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket = boto3.resource('s3').Bucket(dest_bucket)\n",
    "with tempfile.NamedTemporaryFile(mode='w+b', suffix='.csv', prefix='data-', delete=True) as test_csv:\n",
    "    key=f'{dest_prefix}/test/part-21.csv'\n",
    "    print(f'download: {key}')\n",
    "    bucket.download_fileobj(key, test_csv)\n",
    "    print(\"read test csv file\")\n",
    "    array = np.genfromtxt(test_csv.name, delimiter=',', skip_header=False)\n",
    "    np.random.shuffle(array)\n",
    "    for i in range(100):\n",
    "        print(f'test input: {array[i, 1:]}')\n",
    "        result = xgb_predictor.predict(array[i, 1:])\n",
    "        print(f'predicted: {result}')\n",
    "        print(f'expected: {array[i,0]}')\n",
    "    test_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
