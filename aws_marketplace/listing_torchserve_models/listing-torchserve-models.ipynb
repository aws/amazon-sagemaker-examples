{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TorchServe to list PyTorch models at scale in AWS Marketplace\n",
    "\n",
    "TorchServe provides a convenient framework for AWS Marketplace sellers to list their products without writing their own endpoint controllers and handlers. Before the release of TorchServe, if you wanted to list a PyTorch model, you needed to develop custom handlers and build your own docker image, figure out how to make correct API calls in and out of the container network, and solve other ad-hoc problems in developing the model server. TorchServe can simplify this process, and the whole listing process can happen in less than 10 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This solution has the following prerequisites:\n",
    "\n",
    "- An active AWS account\n",
    "- IAM roles and policies to access AWS services\n",
    "  - You need a role with `AmazonSageMakerFullAccess`, `AmazonS3FullAccess` and `AmazonEC2ContainerRegistryFullAccess`\n",
    "  - For more information, see [Adding and removing IAM identity permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html) in the AWS Identity and Access Management User Guide\n",
    "- AWS services:\n",
    "  - [Amazon SageMaker](https://aws.amazon.com/sagemaker/)\n",
    "  - [Amazon S3](https://aws.amazon.com/s3/)\n",
    "  - [Amazon ECR](https://aws.amazon.com/ecr/)\n",
    "  - [AWS Marketplace](https://aws.amazon.com/marketplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution overview\n",
    "The following steps show you how to install TorchServe, create a docker image of TorchServe, create a model archive format (`.mar`) file from a PyTorch data format (`.pth`) file, create a SageMaker model package with the TorchServe docker image and model archive file, and finally validate it and list your product in AWS Marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: (Optional) Update AWS CLI, AWS SDK and Amazon SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip -q install sagemaker awscli boto3 --upgrade "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Git clone TorchServe and install the model archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/serve.git\n",
    "!pip install serve/model-archiver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build a TorchServe docker image and push it to Amazon ECR\n",
    "#### 1. Create a boto3 session and get the account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, time, json\n",
    "sess    = boto3.Session()\n",
    "sm      = sess.client('sagemaker')\n",
    "region  = sess.region_name\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create an Amazon ECR registry through AWS CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_name = 'torchserve-base'\n",
    "!aws ecr create-repository --repository-name {registry_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Build the docker image and push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_label = 'v1'\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{registry_name}:{image_label}'\n",
    "\n",
    "!docker build -t {registry_name}:{image_label} .\n",
    "!$(aws ecr get-login --no-include-email --region {region})\n",
    "!docker tag {registry_name}:{image_label} {image}\n",
    "!docker push {image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember to scan your docker image in Amazon ECR  after you push the image.**\n",
    "\n",
    "- Sign in your AWS console and go to Amazon ECR.\n",
    "- Click the repository you created, select the image and then click Scan to scan your image.\n",
    "- You will see the Scan status as Complete after you scan the docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./img/docker_image_scan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! You have created a TorchServe docker image.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a TorchServe model archive with a PyTorch model and upload it to Amazon S3\n",
    "#### 1. Create a TorchServe archive with a PyTorch model (your own model or a downloaded version)\n",
    "\n",
    "In this notebook let’s download a [DenseNet-161](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py) model for demonstration.\n",
    "You can use your own trained version here instead of downloaded one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://download.pytorch.org/models/densenet161-8d451a50.pth\n",
    "    \n",
    "model_file_name = 'densenet161'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert the `.pth` model file to `.mar` using `torch-model-archiver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver --model-name {model_file_name} \\\n",
    "--version 1.0 --model-file serve/examples/image_classifier/densenet_161/model.py \\\n",
    "--serialized-file densenet161-8d451a50.pth \\\n",
    "--extra-files serve/examples/image_classifier/index_to_name.json \\\n",
    "--handler image_classifier\n",
    "\n",
    "!ls *.mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Upload the generated .mar archive file to Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker expects that models are in a `tar.gz` file. You need to convert the `.mar` file to `tar.gz` and then upload the model to your default SageMaker S3 bucket in the models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'torchserve'\n",
    "\n",
    "!tar cvfz {model_file_name}.tar.gz densenet161.mar\n",
    "!aws s3 cp {model_file_name}.tar.gz s3://{bucket_name}/{prefix}/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Deploy an endpoint and make a prediction using Amazon SageMaker SDK\n",
    "#### 1. Create a SageMaker model with the TorchServe docker image and model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "model_data = f's3://{bucket_name}/{prefix}/models/{model_file_name}.tar.gz'\n",
    "sm_model_name = 'torchserve-densenet161'\n",
    "\n",
    "torchserve_model = Model(model_data = model_data, \n",
    "                         image_uri = image,\n",
    "                         role  = role,\n",
    "                         predictor_cls=Predictor,\n",
    "                         name  = sm_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Deploy an endpoint with the SageMaker model that you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'torchserve-endpoint-' + sm_model_name + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "predictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',\n",
    "                                    initial_instance_count=1,\n",
    "                                    endpoint_name = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test the TorchServe hosted endpoint\n",
    "Use a public image from Amazon S3 to test the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://s3.amazonaws.com/model-server/inputs/kitten.jpg    \n",
    "file_name = 'kitten.jpg'\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = payload\n",
    "    \n",
    "response = predictor.predict(data=payload)\n",
    "print(*json.loads(response), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Delete the endpoint\n",
    "To avoid unnecessary billing, delete the endpoint that you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: (Optional) Test the batch transform on SageMaker before listing the PyTorch model in AWS Marketplace\n",
    "\n",
    "#### 1. Create the batch transform input folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = 'torchserve-densenet161'\n",
    "batch_inference_input_prefix = \"batch-inference-input-data\"\n",
    "TRANSFORM_WORKDIR = \"transform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use two public images from Amazon S3 to test the transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# mkdir transform\n",
    "cd transform\n",
    "wget https://s3.amazonaws.com/model-server/inputs/kitten.jpg\n",
    "wget https://s3.amazonaws.com/model-server/inputs/flower.jpg  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Upload the batch transform input folder to an Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_input = sagemaker_session.upload_data(TRANSFORM_WORKDIR, key_prefix=batch_inference_input_prefix)\n",
    "print(\"Transform input uploaded to \" + transform_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create the batch transform job in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = sagemaker.transformer.Transformer(model_name=sm_model_name, instance_count=1, instance_type='ml.m4.xlarge',\n",
    "                            strategy=None, assemble_with=None, output_path=None, sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(transform_input, content_type='image/jpeg')\n",
    "transformer.wait()\n",
    "\n",
    "print(\"Batch Transform output saved to \" + transformer.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! The batch transform succeeded.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create the model package\n",
    "Now you can start creating your own model package for listing. Before you create it, you need to specify several fields for [inference specification](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_InferenceSpecification.html) and [model package validation specification](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ModelPackageValidationSpecification.html). Creating the model package also does the validation job.\n",
    "\n",
    "#### 1. Create the model package inference specification\n",
    "Specify several fields in the pre-defined inference specification template that I provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference_specification import InferenceSpecification\n",
    "import json\n",
    "\n",
    "modelpackage_inference_specification = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=image,\n",
    "    supports_gpu=True,\n",
    "    supported_content_types=[\"image/jpeg\", \"image/png\"],\n",
    "    supported_mime_types=[\"application/json\"])\n",
    "\n",
    "# Specify the model data resulting from the previously completed training job\n",
    "modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]= model_data\n",
    "print(json.dumps(modelpackage_inference_specification, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create the model package validation specification\n",
    "Specify several fields in the pre-defined model package validation specification template that I provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modelpackage_validation_specification import ModelPackageValidationSpecification\n",
    "import time\n",
    "\n",
    "modelpackage_validation_specification = ModelPackageValidationSpecification().get_validation_specification_dict(\n",
    "    validation_role = role,\n",
    "    batch_transform_input = transform_input,\n",
    "    input_content_type = \"image/jpeg\",\n",
    "    output_content_type = \"application/json\",\n",
    "    instance_type = \"ml.c4.xlarge\",\n",
    "    output_s3_location = 's3://{}/{}'.format(sagemaker_session.default_bucket(), \"/batch-inference-output-data\"))\n",
    "\n",
    "print(json.dumps(modelpackage_validation_specification, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create the model package with inference specification and validation specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_name = sm_model_name + \"-\" + str(round(time.time()))\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageName\" : model_package_name,\n",
    "    \"ModelPackageDescription\" : \"Model of pre-trained DenseNet161\",\n",
    "    \"CertifyForMarketplace\" : True\n",
    "}\n",
    "create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "create_model_package_input_dict.update(modelpackage_validation_specification)\n",
    "print(json.dumps(create_model_package_input_dict, indent=4, sort_keys=True))\n",
    "\n",
    "sm.create_model_package(**create_model_package_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model package is an asynchronous process. To check its status, run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = sm.describe_model_package(ModelPackageName=model_package_name)\n",
    "    status = response[\"ModelPackageStatus\"]\n",
    "    print (status)\n",
    "    if (status == \"Completed\" or status == \"Failed\"):\n",
    "        print (response[\"ModelPackageStatusDetails\"])\n",
    "        break\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! You have created a model package for listing in AWS Marketplace.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: (Optional) Create another model package\n",
    "\n",
    "This step demonstrates that the same TorchServer docker image works for different PyTorch model packages. Download a [VGG-11](https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py) model and create a corresponding model package. You can also use your own model.\n",
    "\n",
    "#### 1. Create the .mar file and upload to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://download.pytorch.org/models/vgg11-bbd30ac9.pth\n",
    "    \n",
    "model_file_name_vgg11 = 'vgg11'\n",
    "\n",
    "!torch-model-archiver --model-name {model_file_name_vgg11} \\\n",
    "--version 1.0 --model-file serve/examples/image_classifier/vgg_11/model.py \\\n",
    "--serialized-file vgg11-bbd30ac9.pth \\\n",
    "--extra-files serve/examples/image_classifier/index_to_name.json \\\n",
    "--handler image_classifier\n",
    "\n",
    "!ls *.mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'torchserve'\n",
    "\n",
    "!tar cvfz {model_file_name_vgg11}.tar.gz vgg11.mar\n",
    "!aws s3 cp {model_file_name_vgg11}.tar.gz s3://{bucket_name}/{prefix}/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = f's3://{bucket_name}/{prefix}/models/{model_file_name_vgg11}.tar.gz'\n",
    "sm_model_name_vgg11 = 'torchserve-vgg11'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a new model and its corresponding inference specification and validation specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpackage_inference_specification = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=image,\n",
    "    supports_gpu=True,\n",
    "    supported_content_types=[\"image/jpeg\", \"image/png\"],\n",
    "    supported_mime_types=[\"application/json\"])\n",
    "\n",
    "# Specify the model data resulting from the previously completed training job\n",
    "modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]= model_data\n",
    "print(json.dumps(modelpackage_inference_specification, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "modelpackage_validation_specification = ModelPackageValidationSpecification().get_validation_specification_dict(\n",
    "    validation_role = role,\n",
    "    batch_transform_input = transform_input,\n",
    "    input_content_type = \"image/jpeg\",\n",
    "    output_content_type = \"application/json\",\n",
    "    instance_type = \"ml.c4.xlarge\",\n",
    "    output_s3_location = 's3://{}/{}'.format(sagemaker_session.default_bucket(), \"/batch-inference-output-data\"))\n",
    "\n",
    "print(json.dumps(modelpackage_validation_specification, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create the model package with inference specification and validation specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_name = sm_model_name_vgg11 + \"-\" + str(round(time.time()))\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageName\" : model_package_name,\n",
    "    \"ModelPackageDescription\" : \"Model of pre-trained VGG11\",\n",
    "    \"CertifyForMarketplace\" : True\n",
    "}\n",
    "create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "create_model_package_input_dict.update(modelpackage_validation_specification)\n",
    "print(json.dumps(create_model_package_input_dict, indent=4, sort_keys=True))\n",
    "\n",
    "sm.create_model_package(**create_model_package_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the creation status, run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = sm.describe_model_package(ModelPackageName=model_package_name)\n",
    "    status = response[\"ModelPackageStatus\"]\n",
    "    print (status)\n",
    "    if (status == \"Completed\" or status == \"Failed\"):\n",
    "        print (response[\"ModelPackageStatusDetails\"])\n",
    "        break\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! You have created another model package for listing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: List your model package in AWS Marketplace management portal\n",
    "\n",
    "After you created your model packages, they appear on the SageMaker console. Go to SageMaker console, click model packages on the left panel and see the model packages you just created. Select a model package and choose `Publish new ML Marketplace listing`. You’re redirected to [AWS Marketplace Management Portal](https://aws.amazon.com/marketplace/management/ml-products). To start publishing your first PyTorch model, follow the instructions in the Management Portal. For more information, see [Machine learning products](https://docs.aws.amazon.com/marketplace/latest/userguide/machine-learning-products.html) in the AWS Marketplace Seller Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./img/listing_on_marketplace.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, I showed you how to use TorchServe to create two model package listings (with the same TorchServe docker image) for AWS Marketplace. TorchServe provides a convenient and flexible way to host an endpoint for PyTorch models. TorchServe supports a variety of default [torch handlers](https://github.com/pytorch/serve/tree/master/ts/torch_handler). You can also write your own handler to better support your unique model and then update your docker image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
