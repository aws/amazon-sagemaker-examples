{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring Your Own R Algorithm\n",
    "_**Create a Docker container for training R algorithms and hosting R models**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Preparation](#Preparation)\n",
    "1. [Code](#Code)\n",
    "  1. [Fit](#Fit)\n",
    "  1. [Serve](#Serve)\n",
    "  1. [Dockerfile](#Dockerfile)\n",
    "  1. [Publish](#Publish)\n",
    "1. [Building The Container](#Building-The-Container)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "1. [Predict](#Predict)\n",
    "1. [Review And Extensions](#Review-And-Extensions)\n",
    "\n",
    "---\n",
    "## Background\n",
    "\n",
    "R is a popular open source statistical programming language, with a lengthy history in Data Science and Machine Learning.  The breadth of algorithms available as an R package is impressive, which fuels a growing community of users.  The R kernel can be installed into Amazon SageMaker Notebooks, and Docker containers which use R can be used to take advantage of Amazon SageMaker's flexible training and hosting functionality.  This notebook illustrates a simple use case for creating an R container and then using it to train and host a model.  We'll use R in both this notebook and the container - using the [reticulate](https://rstudio.github.io/reticulate/) package to connect with the AWS Python SDKs.\n",
    "\n",
    "---\n",
    "## Preparation\n",
    "\n",
    "_This notebook was created and tested on an ml.t3.medium SageMaker Notebook Instance using the default R kernel._\n",
    "\n",
    "> ⚠️ **Notes for SageMaker Studio Users:**\n",
    ">\n",
    "> - If you don't see an R kernel option available to run this notebook, you can use the step-by-step guide in the [documentation on custom Studio Images](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-byoi.html) with the [sample R image code](https://github.com/aws-samples/sagemaker-studio-custom-image-samples/tree/main/examples/r-image) to create and install one.\n",
    "> - The [docker-build-push.sh](docker-build-push.sh) script used in this notebook uses the `docker` CLI, which is not available from SageMaker Studio notebooks at the time of writing. Instead of using this script, you can set up the [SageMaker Studio Docker Build CLI](https://github.com/aws-samples/sagemaker-studio-image-build-cli) and then use a command like: `sm-docker build . --repository sagemaker-rmars:latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R Libraries:\n",
    "library(IRdisplay)\n",
    "library(repr)\n",
    "library(reticulate)\n",
    "\n",
    "# Local utilities:\n",
    "source(\"util.R\")\n",
    "\n",
    "# Python Libraries via Reticulate:\n",
    "boto3 <- import(\"boto3\")\n",
    "sagemaker <- import(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AWS SDKs:\n",
    "botosess <- boto3$Session()\n",
    "smclient <- boto3$client(\"sagemaker\")\n",
    "smsess <- sagemaker$Session()\n",
    "\n",
    "# Set up role and bucket:\n",
    "role <- sagemaker$get_execution_role()\n",
    "bucket <- smsess$default_bucket()\n",
    "prefix <- \"sagemaker/DEMO-r-byo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permissions\n",
    "\n",
    "Running this notebook requires permissions in addition to the normal `SageMakerFullAccess` permissions. This is because we'll be creating a new repository in Amazon ECR. The easiest way to add these permissions is simply to add the managed policy `AmazonEC2ContainerRegistryFullAccess` to the role that you used to start your notebook instance. There's no need to restart your notebook instance when you do this, the new permissions will be available immediately.\n",
    "\n",
    "---\n",
    "## Code\n",
    "\n",
    "For this example, we'll need 3 supporting code files.\n",
    "\n",
    "### Fit\n",
    "\n",
    "[`mars.R`](mars.R) creates functions to fit and serve our model.  The algorithm we've chosen to use is [Multivariate Adaptive Regression Splines](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines).  This is a suitable example as it's a unique and powerful algorithm, but isn't as broadly used as Amazon SageMaker algorithms, and it isn't available in Python's scikit-learn library.  R's repository of packages is filled with algorithms that share these same criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The top of the code is devoted to setup.  Bringing in the libraries we'll need and setting up the file paths as detailed in Amazon SageMaker documentation on bringing your own container._\n",
    "\n",
    "```R\n",
    "# Bring in library that contains multivariate adaptive regression splines (MARS)\n",
    "library(mda)\n",
    "\n",
    "# Bring in library that allows parsing of JSON training parameters\n",
    "library(jsonlite)\n",
    "\n",
    "# Bring in library for prediction server\n",
    "library(plumber)\n",
    "\n",
    "\n",
    "# Setup parameters\n",
    "# Container directories\n",
    "prefix <- '/opt/ml'\n",
    "input_path <- paste(prefix, 'input/data', sep='/')\n",
    "output_path <- paste(prefix, 'output', sep='/')\n",
    "model_path <- paste(prefix, 'model', sep='/')\n",
    "param_path <- paste(prefix, 'input/config/hyperparameters.json', sep='/')\n",
    "\n",
    "# Channel holding training data\n",
    "channel_name = 'train'\n",
    "training_path <- paste(input_path, channel_name, sep='/')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Next, we define a train function that actually fits the model to the data.  For the most part this is idiomatic R, with a bit of maneuvering up front to take in parameters from a JSON file, and at the end to output a success indicator._\n",
    "\n",
    "```R\n",
    "# Setup training function\n",
    "train <- function() {\n",
    "    # Read in hyperparameters\n",
    "    training_params <- read_json(param_path)\n",
    "\n",
    "    target <- training_params$target\n",
    "\n",
    "    if (!is.null(training_params$degree)) {\n",
    "        degree <- as.numeric(training_params$degree)\n",
    "    } else {\n",
    "        degree <- 2\n",
    "    }\n",
    "\n",
    "    # Bring in data\n",
    "    training_files = list.files(path=training_path, full.names=TRUE)\n",
    "    training_data = do.call(rbind, lapply(training_files, read.csv))\n",
    "    \n",
    "    # Convert to model matrix\n",
    "    training_X <- model.matrix(~., training_data[, colnames(training_data) != target])\n",
    "\n",
    "    # Save factor levels for scoring\n",
    "    factor_levels <- lapply(\n",
    "        training_data[, sapply(training_data, is.factor), drop=FALSE],\n",
    "        function(x) { levels(x) }\n",
    "    )\n",
    "    \n",
    "    # Run multivariate adaptive regression splines algorithm\n",
    "    model <- mars(x=training_X, y=training_data[, target], degree=degree)\n",
    "    \n",
    "    # Generate outputs\n",
    "    mars_model <- model[!(names(model) %in% c('x', 'residuals', 'fitted.values'))]\n",
    "    attributes(mars_model)$class <- 'mars'\n",
    "    save(mars_model, factor_levels, file=paste(model_path, 'mars_model.RData', sep='/'))\n",
    "    print(summary(mars_model))\n",
    "\n",
    "    write.csv(model$fitted.values, paste(output_path, 'data/fitted_values.csv', sep='/'), row.names=FALSE)\n",
    "    write('success', file=paste(output_path, 'success', sep='/'))\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Then, we setup the serving function (which is really just a short wrapper around our plumber.R file that we'll discuss [next](#Serve)._\n",
    "\n",
    "```R\n",
    "# Setup scoring function\n",
    "serve <- function() {\n",
    "    app <- plumb(paste(prefix, 'plumber.R', sep='/'))\n",
    "    app$run(host='0.0.0.0', port=8080)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Finally, a bit of logic to determine if, based on the options passed when Amazon SageMaker Training or Hosting call this script, we are using the container to train an algorithm or host a model._\n",
    "\n",
    "```R\n",
    "# Run at start-up\n",
    "args <- commandArgs()\n",
    "if (any(grepl('train', args))) {\n",
    "    train()\n",
    "}\n",
    "if (any(grepl('serve', args))) {\n",
    "    serve()\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve\n",
    "[`plumber.R`](plumber.R) uses the [plumber](https://www.rplumber.io/) package to create a lightweight HTTP server for processing requests in hosting.  Note the specific syntax, and see the plumber help docs for additional detail on more specialized use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the Amazon SageMaker documentation, our service needs to accept post requests to ping and invocations.  plumber specifies this with custom comments, followed by functions that take specific arguments.\n",
    "\n",
    "Here invocations does most of the work, ingesting our trained model, handling the HTTP request body, and producing a CSV output of predictions.\n",
    "\n",
    "```R\n",
    "# plumber.R\n",
    "\n",
    "#' Ping to show server is there\n",
    "#' @get /ping\n",
    "function() {\n",
    "    return('')\n",
    "}\n",
    "\n",
    "\n",
    "#' Parse input and return prediction from model\n",
    "#' @param req The http request sent\n",
    "#' @post /invocations\n",
    "function(req) {\n",
    "    # Setup locations\n",
    "    prefix <- '/opt/ml'\n",
    "    model_path <- paste(prefix, 'model', sep='/')\n",
    "\n",
    "    # Bring in model file and factor levels\n",
    "    load(paste(model_path, 'mars_model.RData', sep='/'))\n",
    "\n",
    "    # Read in data\n",
    "    conn <- textConnection(gsub('\\\\\\\\n', '\\n', req$postBody))\n",
    "    data <- read.csv(conn)\n",
    "    close(conn)\n",
    "\n",
    "    # Convert input to model matrix\n",
    "    scoring_X <- model.matrix(~., data, xlev=factor_levels)\n",
    "\n",
    "    # Return prediction\n",
    "    return(paste(predict(mars_model, scoring_X, row.names=FALSE), collapse=','))\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile\n",
    "\n",
    "The [`Dockerfile`](Dockerfile) defines the container to run these scripts in, and keeps it minimal as smaller containers lead to faster spin-up times in training and endpoint creation.  It simply starts with Ubuntu; installs R, mda, and plumber libraries; then adds the `mars.R` and `plumber.R` scripts; and configures `mars.R` as the entry point script to run when the container is launched.\n",
    "\n",
    "```Dockerfile\n",
    "FROM ubuntu:20.04\n",
    "\n",
    "# Don't prompt for tzdata on new versions of Ubuntu:\n",
    "ARG DEBIAN_FRONTEND=noninteractive \n",
    "\n",
    "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
    "    wget \\\n",
    "    libcurl4-openssl-dev\\\n",
    "    libsodium-dev \\\n",
    "    r-base \\\n",
    "    r-base-dev \\\n",
    "    ca-certificates\n",
    "\n",
    "RUN R -e \"install.packages(c('mda', 'plumber'), repos='https://cloud.r-project.org')\"\n",
    "\n",
    "COPY mars.R /opt/ml/mars.R\n",
    "COPY plumber.R /opt/ml/plumber.R\n",
    "\n",
    "ENTRYPOINT [\"/usr/bin/Rscript\", \"/opt/ml/mars.R\", \"--no-save\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish\n",
    "\n",
    "For SageMaker to use this container image, it needs to be actually built and published to [Amazon ECR](https://aws.amazon.com/ecr/).\n",
    "\n",
    "In [`docker-build-push.sh`](docker-build-push.sh), we provide a shell script to simplify running this process:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-rmars\n",
    "\n",
    "set -e # stop if anything fails\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "echo \"AWS Account ID $account\"\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "echo \"AWS Region $region\"\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "echo \"Target image URI $fullname\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "echo \"Checking for existing repository...\"\n",
    "set +e\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\"\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    set -e\n",
    "    echo \"Creating repository\"\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\"\n",
    "else\n",
    "    set -e\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building The Container\n",
    "\n",
    "First, we'll build the container image in Amazon ECR.\n",
    "\n",
    "> ⏰ **Note: This command will take several minutes to run the first time, and will not show outputs until it's complete.**\n",
    ">\n",
    "> **If you'd like to see live updates as the command runs, consider running it from a terminal window intsead!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the output of terminal calls into the notebook is a little trickier than with Python,\n",
    "# so we made a util.R function based on system2():\n",
    "nbsystem2(\"./docker-build-push.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "For this illustrative example, we'll simply use `iris`.  This a classic, but small, dataset used to test supervised learning algorithms.  Typically the goal is to predict one of three flower species based on various measurements of the flowers' attributes.  Further detail can be found [here](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
    "\n",
    "Then let's copy the data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file <- \"iris.csv\"\n",
    "smsess$upload_data(\n",
    "    train_file,\n",
    "    bucket=bucket,\n",
    "    key_prefix=paste(prefix, \"train\", sep=\"/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Although we could, we'll avoid doing any preliminary transformations on the data, instead choosing to do those transformations inside the container.  This is not typically the best practice for model efficiency, but provides some benefits in terms of flexibility._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Now, let's setup the information needed to train a Multivariate Adaptive Regression Splines (MARS) model on iris data.  In this case, we'll predict `Sepal.Length` rather than the more typical classification of `Species` to show how factors might be included in a model and limit the case to regression.\n",
    "\n",
    "First, we'll get our region and account information so that we can point to the ECR container we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region <- botosess$region_name\n",
    "account <- boto3$client(\"sts\")$get_caller_identity()$Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Specify the role to use\n",
    "- Give the training job a name\n",
    "- Point the algorithm to the container we created\n",
    "- Specify training instance resources (in this case our algorithm is only single-threaded so stick to 1 instance)\n",
    "- Point to the S3 location of our input data and the `train` channel expected by our algorithm\n",
    "- Point to the S3 location for output\n",
    "- Provide hyperparamters (keeping it simple)\n",
    "- Maximum run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_job <- paste(\"DEMO-r-byo\", strftime(Sys.time(), \"%Y-%m-%d-%H-%M-%S\"), sep=\"-\")\n",
    "\n",
    "smclient$create_training_job(\n",
    "    RoleArn=role,\n",
    "    TrainingJobName=r_job,\n",
    "    AlgorithmSpecification=list(\n",
    "        TrainingImage=paste(account, \".dkr.ecr.\", region, \".amazonaws.com/sagemaker-rmars:latest\", sep=\"\"),\n",
    "        TrainingInputMode=\"File\"\n",
    "    ),\n",
    "    ResourceConfig=list(\n",
    "        InstanceCount=1L,\n",
    "        InstanceType=\"ml.m4.xlarge\",\n",
    "        VolumeSizeInGB=10L\n",
    "    ),\n",
    "    InputDataConfig=list(\n",
    "        list(\n",
    "            ChannelName=\"train\",\n",
    "            DataSource=list(\n",
    "                S3DataSource=list(\n",
    "                    S3DataType=\"S3Prefix\",\n",
    "                    S3Uri=paste(\"s3:/\", bucket, prefix, \"train\", sep=\"/\"),\n",
    "                    S3DataDistributionType=\"FullyReplicated\"\n",
    "                )\n",
    "            ),\n",
    "            CompressionType=\"None\",\n",
    "            RecordWrapperType=\"None\"\n",
    "        )\n",
    "    ),\n",
    "    OutputDataConfig=list(\n",
    "        S3OutputPath=paste(\"s3:/\", bucket, prefix, \"output\", sep=\"/\")\n",
    "    ),\n",
    "    HyperParameters=list(\n",
    "        target=\"Sepal.Length\",\n",
    "        degree=\"2\"\n",
    "    ),\n",
    "    StoppingCondition=list(\n",
    "        MaxRuntimeInSeconds=60L * 60L\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's kick off our training job on Amazon SageMaker Training, using the parameters we just created.  Because training is managed (AWS takes care of spinning up and spinning down the hardware), we don't have to wait for our job to finish to continue, but for this case, let's setup a waiter so we can monitor the status of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart <- Sys.time()\n",
    "\n",
    "status <- smclient$describe_training_job(TrainingJobName=r_job)$TrainingJobStatus\n",
    "display(paste(\"Initial status:\", status))\n",
    "\n",
    "smclient$get_waiter(\"training_job_completed_or_stopped\")$wait(TrainingJobName=r_job)\n",
    "\n",
    "jobdesc <- smclient$describe_training_job(TrainingJobName=r_job)\n",
    "status <- jobdesc$TrainingJobStatus\n",
    "display(paste(\"Training job ended with status:\", status))\n",
    "if (status == \"Failed\") {\n",
    "    message <- jobdesc$FailureReason\n",
    "    display(paste(\"Training failed with the following error:\", message))\n",
    "    stop(\"Training job failed\")\n",
    "}\n",
    "\n",
    "tend <- Sys.time()\n",
    "display(paste(\"Wall clock time elapsed:\", repr(tend - tstart)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Host\n",
    "\n",
    "Hosting the model we just trained takes three steps in Amazon SageMaker.  First, we **define the model** we want to host, pointing the service to the model artifact our training job just wrote to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_response <- smclient$create_model(\n",
    "    ModelName=r_job,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer=list(\n",
    "        Image=paste(account, \".dkr.ecr.\", region, \".amazonaws.com/sagemaker-rmars:latest\", sep=\"\"),\n",
    "        ModelDataUrl=jobdesc$ModelArtifacts$S3ModelArtifacts\n",
    "    )\n",
    ")\n",
    "\n",
    "display(create_model_response$ModelArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an **endpoint configuration** which defines the compute infrastructure to be used for inference. For our initial, low-volume testing in this notebook a single `ml.m4.xlarge` should be easily enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_endpoint_config <- paste(\"DEMO-r-byo-config\", strftime(Sys.time(), \"%Y-%m-%d-%H-%M-%S\"), sep=\"-\")\n",
    "display(r_endpoint_config)\n",
    "create_endpoint_config_response = smclient$create_endpoint_config(\n",
    "    EndpointConfigName=r_endpoint_config,\n",
    "    ProductionVariants=list(\n",
    "        list(\n",
    "            InstanceType=\"ml.m4.xlarge\",\n",
    "            InitialInstanceCount=1L,\n",
    "            ModelName=r_job,\n",
    "            VariantName=\"AllTraffic\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "display(paste(\"Endpoint Config Arn:\", create_endpoint_config_response$EndpointConfigArn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll create the **endpoint** itself - starting up the model(s) for inference as described in the endpoint configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart <- Sys.time()\n",
    "\n",
    "r_endpoint <- paste(\"DEMO-r-endpoint-\", strftime(Sys.time(), \"%Y-%m-%d-%H-%M-%S\"), sep=\"-\")\n",
    "print(r_endpoint)\n",
    "create_endpoint_response = smclient$create_endpoint(\n",
    "    EndpointName=r_endpoint,\n",
    "    EndpointConfigName=r_endpoint_config\n",
    ")\n",
    "display(create_endpoint_response$EndpointArn)\n",
    "\n",
    "resp = smclient$describe_endpoint(EndpointName=r_endpoint)\n",
    "status = resp$EndpointStatus\n",
    "display(paste(\"Initial status:\", status))\n",
    "\n",
    "tryCatch({\n",
    "    smclient$get_waiter(\"endpoint_in_service\")$wait(EndpointName=r_endpoint)\n",
    "}, finally={\n",
    "    resp <- smclient$describe_endpoint(EndpointName=r_endpoint)\n",
    "    display(paste(\"Arn:\", resp$EndpointArn))\n",
    "    status <- resp$EndpointStatus\n",
    "    display(paste(\"Endpoint deployment ended with status:\", status))\n",
    "\n",
    "    if (status != \"InService\") {\n",
    "        stop(\"Endpoint creation did not succeed\")\n",
    "    }\n",
    "})\n",
    "\n",
    "tend <- Sys.time()\n",
    "display(paste(\"Wall clock time elapsed:\", repr(tend - tstart)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⏰ **Note:** This deployment can take a few minutes to complete\n",
    "\n",
    "---\n",
    "## Predict\n",
    "\n",
    "Once our endpoint is successfully deployed, we can use it to generate real-time predictions.\n",
    "\n",
    "_Note: The payload we're passing in the request is a CSV string with a header record, followed by multiple new lines.  It also contains text columns, which the serving code converts to the set of indicator variables needed for our model predictions.  Again, this is not a best practice for highly optimized code, however, it showcases the flexibility of bringing your own algorithm._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris <- read.csv(\"iris.csv\")\n",
    "\n",
    "runtime <- botosess$client(\"runtime.sagemaker\")\n",
    "\n",
    "to_csv_str <- function(data) {\n",
    "    # (Need to wrap this in a function or the `payload` var will be global and not reusable)\n",
    "    payloadconn <- textConnection(\"payload\", \"w\", local=TRUE)\n",
    "    write.table(\n",
    "        data,\n",
    "        payloadconn,\n",
    "        sep=\",\",\n",
    "        row.names=FALSE\n",
    "    )\n",
    "    return(paste(payload, collapse=\"\\n\"))\n",
    "}\n",
    "\n",
    "response <- runtime$invoke_endpoint(\n",
    "    EndpointName=r_endpoint,\n",
    "    ContentType=\"text/csv\",\n",
    "    Body=to_csv_str(iris[, !(names(iris) == \"Sepal.Length\")])\n",
    ")\n",
    "\n",
    "result <- response$Body$read()$decode()\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the result is a CSV of predictions for our target variable.  Let's compare them to the actuals to see how our model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed <- jsonlite::fromJSON(result)\n",
    "\n",
    "plot(\n",
    "    iris[,\"Sepal.Length\"],\n",
    "    as.numeric(unlist(strsplit(parsed, \",\"))),\n",
    "    xlab=\"Actual Sepal Length\",\n",
    "    ylab=\"Predicted Sepal Length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Review And Extensions\n",
    "\n",
    "In this notebook we demonstrated a straightforward example to train and host an R algorithm in Amazon SageMaker - allowing us to separately specify the compute infrastructure required for the training job and online inference, and so optimize costs.\n",
    "\n",
    "Because the \"SageMaker Model\" is defined by the combination of the container image we built and the `model.tar.gz` archive of artifacts the training job saves to Amazon S3, the process is very flexible. We could, for example:\n",
    "\n",
    "- Train the algorithm entirely within a notebook (or somewhere else) and use SageMaker only for deploying the inference/serving endpoint, or\n",
    "- Train the algorithm on SageMaker, but then download the result from Amazon S3 to run inference in a notebook or some other environment, or\n",
    "- Easily scale out our SageMaker inference capacity by increasing the number (or size) of instances specified in the endpoint configuration, and\n",
    "- Build algorithms using any libraries/packages we have access to and save model parameters in whatever formats we like.\n",
    "\n",
    "Although R is not the easiest language to build distributed applications on top of, it is possible and by adapting the script you could also utilize multiple training job instances: For example to train multiple sub-models or conduct a custom parameter search in parallel.\n",
    "\n",
    "This core workflow example has only scratched the surface of the functionality offered by the SageMaker platform. For example:\n",
    "\n",
    "- By modifying the training script to accept (hyper)-parameters and print accuracy metrics to the console, you could take advantage of SageMaker's efficient Bayesian [Hyperparameter tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) - instead of manually tuning models.\n",
    "- If your use case requires generating predictions in batch, rather than deploying endpoints to query in real-time, you could instead use [SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) without needing to change any model/container code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (Optional) Clean-up\n",
    "\n",
    "Unlike training jobs which start and stop compute infrastructure for defined jobs, SageMaker endpoints are deployed with the resources you configure and remain active until switched off.\n",
    "\n",
    "When you're finished with this notebook, please run the cell below to delete the hosted endpoint you created and prevent ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient$delete_endpoint(EndpointName=r_endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
