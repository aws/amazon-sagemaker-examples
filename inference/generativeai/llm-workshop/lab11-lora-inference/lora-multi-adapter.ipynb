{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946d5a3d",
   "metadata": {},
   "source": [
    "# Serve Multiple Fine-Tuned LoRA Adapters on a single endpoint using SageMaker LMI DLC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70205187408f54",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35756ca1",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how you can deploy multiple fine-tuned LoRA adapters with a single base model copy on SageMaker using the DJL Serving Large Model Inference DLC. LoRA (Low Rank Adapters) is a powerful technique for fine-tuning large language models. This technique significantly reduces the number of trainable parameters compared to traditional fine-tuning while achieving comparable or superior performance. You can learn more about the LoRA technique in this [paper](https://arxiv.org/abs/2106.09685).\n",
    "\n",
    "A major benefit of LoRA is that the fine-tuned adapters can easily be added to and removed from the base model, which makes switching adapters pretty cheap and viable at runtime. In this notebook we will show how you can deploy a SageMaker endpoint with a single base model and multiple LoRA adapters, and change adapters for different requests.\n",
    "\n",
    "Since LoRA adapters are much smaller than the size of a base model (can realistically be 100x-1000x smaller), we can deploy an endpoint with a single base model and multiple LoRA adapters using much less hardware than deploying an equivalent number of fully fine-tuned models.\n",
    "\n",
    "The example we will work through in this notebook is guided by the multi adapter example in HuggingFace's PEFT library: https://github.com/huggingface/peft/blob/main/examples/multi_adapter_examples/PEFT_Multi_LoRA_Inference.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e3b6d",
   "metadata": {},
   "source": [
    "## Install Packages and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub sagemaker boto3 awscli --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sagemaker.utils import name_from_base\n",
    "from huggingface_hub import snapshot_download, notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02022f7e",
   "metadata": {},
   "source": [
    "In this example we'll be using a model that requires a huggingface account and a token for read permissions. You can read more on the setup here https://huggingface.co/docs/hub/security-tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90369a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37b203",
   "metadata": {},
   "source": [
    "## Download Model Artifacts and Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1515c5",
   "metadata": {},
   "source": [
    "We will be deploying an endpoint with 2 LoRA adapters. These are the models we will be using:\n",
    "- Base Model: https://huggingface.co/decapoda-research/llama-7b-hf\n",
    "- LoRA Fine Tuned Adapter 1: https://huggingface.co/tloen/alpaca-lora-7b\n",
    "- LoRA Fine Tuned Adapter 2: https://huggingface.co/22h/cabrita-lora-v0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf lora-multi-adapter\n",
    "!mkdir -p lora-multi-adapter/base_model\n",
    "!mkdir -p lora-multi-adapter/adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c41301",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(\"decapoda-research/llama-7b-hf\", local_dir=\"lora-multi-adapter/base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978224f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(\"tloen/alpaca-lora-7b\", local_dir=\"lora-multi-adapter/adapters/eng_alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba747f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(\n",
    "    \"22h/cabrita-lora-v0-1\", local_dir=\"lora-multi-adapter/adapters/portuguese_alpaca\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0230f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session()\n",
    "s3_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync lora-multi-adapter s3://{s3_bucket}/lora-mutli-adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89777de",
   "metadata": {},
   "source": [
    "## Creating Inference Handler and DJL Serving Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd1577",
   "metadata": {},
   "source": [
    "The following files cover the model server configuration (`serving.properties`) and custom inference handler (`model.py`). This configuration can be used as an example to write your own inference handler for different models. \n",
    "\n",
    "The core structure to cover here is the model directory. We include both the base model and LoRA adapters in the model directory like this:\n",
    "\n",
    "```\n",
    "|- base_model/\n",
    "|- adapters/\n",
    "|--- <adapter_1>/\n",
    "|--- <adapter_2>/\n",
    "|--- ...\n",
    "|--- <adapter_n>/\n",
    "|- serving.properties\n",
    "|- model.py\n",
    "\n",
    "```\n",
    "\n",
    "The `base_model` directory contains all the base model artifacts from the corresponding repository on the huggingface hub. These model artifacts are generated from the `model.save_pretrained()` method of huggingface's transformers library.\n",
    "\n",
    "Each of the adapters in the `adapters` directory contains the LoRA adapter artifacts. Typically there are two files: `adapter_model.bin` and `adapter_config.json` which are the adapter weights and adapter configuration respectively. These are typically obtained from the Peft library via the `PeftModel.save_pretrained()` method. In this example, the inference handler will register the adapters with names equivalent to their directory name, and we will use that name in the request to target a specific adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf lora-multi-adapter-code\n",
    "!mkdir -p lora-multi-adapter-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e988852",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lora-multi-adapter-code/serving.properties\n",
    "# Python engine is currently the only engine supported for multi adapter use-case\n",
    "engine=Python\n",
    "option.model_id=s3://{{s3_bucket}}/lora-mutli-adapter\n",
    "option.base_model_path=base_model\n",
    "option.adapters_path=adapters\n",
    "option.dtype=fp16\n",
    "option.entryPoint=model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2\n",
    "\n",
    "jinja_env = jinja2.Environment()\n",
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(\"lora-multi-adapter-code/serving.properties\").open().read())\n",
    "Path(\"lora-multi-adapter-code/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3_bucket=s3_bucket)\n",
    ")\n",
    "!pygmentize lora-multi-adapter-code/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lora-multi-adapter-code/model.py\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "from djl_python.inputs import Input\n",
    "from djl_python.outputs import Output\n",
    "import logging\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "        Write a response that appropriately completes the request. ### Instruction: {instruction} ### Input: {input} \n",
    "        ### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the \n",
    "        request.### Instruction: {instruction} ### Response:\"\"\"\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=256,\n",
    "    **kwargs,\n",
    "):\n",
    "    prompt = generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(torch.cuda.current_device())\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        no_repeat_ngram_size=3,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output.split(\"### Response:\")[1].strip()\n",
    "\n",
    "\n",
    "def load_model(base_model_path, adapters_path=None):\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model_path, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model_path)\n",
    "    logging.info(f\"Loaded Base Model {base_model_path}\")\n",
    "\n",
    "    print(os.listdir(adapters_path))\n",
    "    if os.path.exists(adapters_path):\n",
    "        first_adapter = True\n",
    "        for adapter_dir in os.listdir(adapters_path):\n",
    "            logging.info(f\"Registering Adapter {os.path.join(adapters_path, adapter_dir)}\")\n",
    "            if not os.path.isdir(os.path.join(adapters_path, adapter_dir)):\n",
    "                continue\n",
    "            if first_adapter:\n",
    "                model = PeftModel.from_pretrained(\n",
    "                    model, os.path.join(adapters_path, adapter_dir), adapter_name=adapter_dir\n",
    "                )\n",
    "                first_adapter = False\n",
    "            else:\n",
    "                model.load_adapter(\n",
    "                    os.path.join(adapters_path, adapter_dir), adapter_name=adapter_dir\n",
    "                )\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        properties = inputs.get_properties()\n",
    "        model_dir = properties.get(\"model_id\")\n",
    "        base_model_path = os.path.join(model_dir, properties.get(\"base_model_path\"))\n",
    "        adapters_path = os.path.join(model_dir, properties.get(\"adapters_path\"))\n",
    "        logging.info(base_model_path)\n",
    "        logging.info(adapters_path)\n",
    "        model, tokenizer = load_model(base_model_path, adapters_path=adapters_path)\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # initialization request\n",
    "        return None\n",
    "\n",
    "    json_inputs = inputs.get_as_json()\n",
    "    sentence = json_inputs.get(\"inputs\")\n",
    "    adapter_name = json_inputs.get(\"adapter_name\", None)\n",
    "    generation_kwargs = json_inputs.get(\"parameters\", {})\n",
    "    if adapter_name is not None:\n",
    "        model.set_adapter(adapter_name)\n",
    "        outputs = evaluate(sentence, **generation_kwargs)\n",
    "    else:\n",
    "        with model.disable_adapter():\n",
    "            outputs = evaluate(sentence, **generation_kwargs)\n",
    "\n",
    "    return Output().add_as_json(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c45680",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f model.tar.gz\n",
    "!rm -rf lora-multi-adapter-code/.ipynb_checkpoints\n",
    "!tar czvf model.tar.gz -C lora-multi-adapter-code ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6dca13",
   "metadata": {},
   "source": [
    "## Create SageMaker Model and Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = (\n",
    "    \"hf-large-model-djl/lora-multi-adapter\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab57364",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_artifact_accelerate = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e67228",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "\n",
    "model_name_acc = name_from_base(f\"lora-multi-adapter\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name_acc,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"ModelDataUrl\": s3_code_artifact_accelerate},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8699f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name_acc}-config\"\n",
    "endpoint_name = f\"{model_name_acc}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name_acc,\n",
    "            \"InstanceType\": \"ml.g5.8xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "            # \"VolumeSizeInGB\": 512\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ad681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680b38a",
   "metadata": {},
   "source": [
    "## Make Inference Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ac360",
   "metadata": {},
   "source": [
    "We show how you can make requests targetting each of the adapters configured in the endpoint, as well as requests targetting just the base model with no adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f50dc",
   "metadata": {},
   "source": [
    "Inference Request targetting the tloen/alpaca-lora-7b adapter (named eng_alpaca based on the directory name of the adapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84948200",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"inputs\": \"Tell me about Alpacas\", \"adapter_name\": \"eng_alpaca\"}),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7883ea4",
   "metadata": {},
   "source": [
    "Inference Request targetting the 22h/cabrita-lora-v0-1 adapter (named portuguese_alpaca based on the direcotry name of the adapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": \"Invente uma desculpa criativa pra dizer que não preciso ir à festa.\",\n",
    "            \"adapter_name\": \"portuguese_alpaca\",\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d434c23",
   "metadata": {},
   "source": [
    "Inference Request targetting the base model without any adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096aeefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"inputs\": \"Tell me about Alpacas\"}),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef62f3e",
   "metadata": {},
   "source": [
    "## Clean up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe24ef7bcee67d5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/inference|generativeai|llm-workshop|lab11-lora-inference|lora-multi-adapter.ipynb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
