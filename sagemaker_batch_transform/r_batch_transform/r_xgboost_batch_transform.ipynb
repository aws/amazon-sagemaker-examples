{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Batch Transform Using R with Amazon SageMaker</h1>\n",
    "\n",
    "**Note:** You will need to use R kernel in SageMaker for this notebook.\n",
    "\n",
    "This sample Notebook describes how to do batch transform to make predictions for abalone age as measured by the number of rings in the shell. The notebook will use the public [abalone dataset](https://archive.ics.uci.edu/ml/datasets/abalone) hosted by [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n",
    "\n",
    "You can find more details about SageMaker's Batch Trsnform here: \n",
    "- [Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) using a Transformer\n",
    "\n",
    "We will use `reticulate` library to interact with SageMaker:\n",
    "- [`Reticulate` library](https://rstudio.github.io/reticulate/): provides an R interface to make API calls [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/latest/index.html) to make API calls to Amazon SageMaker. The `reticulate` package translates between R and Python objects, and Amazon SageMaker provides a serverless data science environment to train and deploy ML models at scale.\n",
    "\n",
    "Table of Contents:\n",
    "- [Reticulating the Amazon SageMaker Python SDK](#Reticulating-the-Amazon-SageMaker-Python-SDK)\n",
    "- [Creating and Accessing the Data Storage](#Creating-and-accessing-the-data-storage)\n",
    "- [Downloading and Processing the Dataset](#Downloading-and-processing-the-dataset)\n",
    "- [Preparing the Dataset for Model Training](#Preparing-the-dataset-for-model-training)\n",
    "- [Creating a SageMaker Estimator](#Creating-a-SageMaker-Estimator)\n",
    "- [Batch Transform using SageMaker Transformer](#Batch-Transform-using-SageMaker-Transformer)\n",
    "- [Download the Batch Transform Output](#Download-the-Batch-Transform-Output)\n",
    "\n",
    "\n",
    "**Note:** The first portion of this notebook focused on data ingestion and preparing the data for model training is inspired by the data preparation part outlined in [\"Using R with Amazon SageMaker\"](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/r_kernel/using_r_with_amazon_sagemaker.ipynb) notebook on AWS SageMaker Examples Github repository with some modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reticulating the Amazon SageMaker Python SDK</h3>\n",
    "\n",
    "First, load the `reticulate` library and import the `sagemaker` Python module. Once the module is loaded, use the `$` notation in R instead of the `.` notation in Python to use available classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn warnings off globally\n",
    "options(warn=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install reticulate library and import sagemaker\n",
    "library(reticulate)\n",
    "sagemaker <- import('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating and Accessing the Data Storage</h3>\n",
    "\n",
    "The `Session` class provides operations for working with the following [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) resources with Amazon SageMaker:\n",
    "\n",
    "* [S3](https://boto3.readthedocs.io/en/latest/reference/services/s3.html)\n",
    "* [SageMaker](https://boto3.readthedocs.io/en/latest/reference/services/sagemaker.html)\n",
    "\n",
    "Let's create an [Amazon Simple Storage Service](https://aws.amazon.com/s3/) bucket for your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session <- sagemaker$Session()\n",
    "bucket <- session$default_bucket()\n",
    "prefix <- 'r-batch-transform'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** - The `default_bucket` function creates a unique Amazon S3 bucket with the following name: \n",
    "\n",
    "`sagemaker-<aws-region-name>-<aws account number>`\n",
    "\n",
    "Specify the IAM role's [ARN](https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html) to allow Amazon SageMaker to access the Amazon S3 bucket. You can use the same IAM role used to create this Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_arn <- sagemaker$get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Downloading and Processing the Dataset</h3>\n",
    "\n",
    "The model uses the [abalone dataset](https://archive.ics.uci.edu/ml/datasets/abalone) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). First, download the data and start the [exploratory data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis). Use tidyverse packages to read the data, plot the data, and transform the data into ML format for Amazon SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(readr)\n",
    "data_file <- 'http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'\n",
    "abalone <- read_csv(file = data_file, col_names = FALSE)\n",
    "names(abalone) <- c('sex', 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight', 'viscera_weight', 'shell_weight', 'rings')\n",
    "head(abalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that `sex` is a factor data type but is currently a character data type (F is Female, M is male, and I is infant). Change `sex` to a factor and view the statistical summary of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone$sex <- as.factor(abalone$sex)\n",
    "summary(abalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary above shows that the minimum value for `height` is 0.\n",
    "\n",
    "Visually explore which abalones have height equal to 0 by plotting the relationship between `rings` and `height` for each value of `sex`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "options(repr.plot.width = 5, repr.plot.height = 4) \n",
    "ggplot(abalone, aes(x = height, y = rings, color = sex)) + geom_point() + geom_jitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows multiple outliers: two infant abalones with a height of 0 and a few female and male abalones with greater heights than the rest. Let's filter out the two infant abalones with a height of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "abalone <- abalone %>%\n",
    "  filter(height != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preparing the Dataset for Model Training</h3>\n",
    "\n",
    "The model needs three datasets: one each for training, testing, and validation. First, convert `sex` into a [dummy variable](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) and move the target, `rings`, to the first column. Amazon SageMaker algorithm require the target to be in the first column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone <- abalone %>%\n",
    "  mutate(female = as.integer(ifelse(sex == 'F', 1, 0)),\n",
    "         male = as.integer(ifelse(sex == 'M', 1, 0)),\n",
    "         infant = as.integer(ifelse(sex == 'I', 1, 0))) %>%\n",
    "  select(-sex)\n",
    "abalone <- abalone %>%\n",
    "  select(rings:infant, length:shell_weight)\n",
    "head(abalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, sample 70% of the data for training the ML algorithm. Split the remaining 30% into two halves, one for testing and one for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_train <- abalone %>%\n",
    "  sample_frac(size = 0.7)\n",
    "abalone <- anti_join(abalone, abalone_train)\n",
    "abalone_test <- abalone %>%\n",
    "  sample_frac(size = 0.5)\n",
    "abalone_valid <- anti_join(abalone, abalone_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in the notebook, we are going to use Batch Transform and Endpoint to make inference in two different ways and we will compare the results. The maximum number of rows that we can send to an endpoint for inference in one batch is 500 rows. We are going to reduce the number of rows for the test dataset to 500 and use this for batch and online inference for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predict_rows <- 500\n",
    "abalone_test <- abalone_test[1:num_predict_rows, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the training and validation data to Amazon S3 so that you can train the model. First, write the training and validation datasets to the local filesystem in .csv format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(abalone_train, 'abalone_train.csv', col_names = FALSE)\n",
    "write_csv(abalone_valid, 'abalone_valid.csv', col_names = FALSE)\n",
    "\n",
    "# Remove target from test\n",
    "write_csv(abalone_test[-1], 'abalone_test.csv', col_names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, upload the two datasets to the Amazon S3 bucket into the `data` key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train <- session$upload_data(path = 'abalone_train.csv', \n",
    "                                bucket = bucket, \n",
    "                                key_prefix = paste(prefix,'data', sep = '/'))\n",
    "s3_valid <- session$upload_data(path = 'abalone_valid.csv', \n",
    "                                bucket = bucket, \n",
    "                                key_prefix = paste(prefix,'data', sep = '/'))\n",
    "\n",
    "s3_test <- session$upload_data(path = 'abalone_test.csv', \n",
    "                                bucket = bucket, \n",
    "                                key_prefix = paste(prefix,'data', sep = '/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the Amazon S3 input types for the Amazon SageMaker algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_input <- sagemaker$s3_input(s3_data = s3_train,\n",
    "                                     content_type = 'csv')\n",
    "s3_valid_input <- sagemaker$s3_input(s3_data = s3_valid,\n",
    "                                     content_type = 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>Creating a SageMaker Estimator</h3>\n",
    "\n",
    "Amazon SageMaker algorithm are available via a [Docker](https://www.docker.com/) container. To train an [XGBoost](https://en.wikipedia.org/wiki/Xgboost) model, specify the training containers in [Amazon Elastic Container Registry](https://aws.amazon.com/ecr/) (Amazon ECR) for the AWS Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry <- sagemaker$amazon$amazon_estimator$registry(session$boto_region_name, algorithm='xgboost')\n",
    "container <- paste(registry, '/xgboost:latest', sep='')\n",
    "cat('XGBoost Container Image URL: ', container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an Amazon SageMaker [Estimator](http://sagemaker.readthedocs.io/en/latest/estimators.html), which can train any supplied algorithm that has been containerized with Docker. When creating the Estimator, use the following arguments:\n",
    "* **image_name** - The container image to use for training\n",
    "* **role** - The Amazon SageMaker service role\n",
    "* **train_instance_count** - The number of Amazon EC2 instances to use for training\n",
    "* **train_instance_type** - The type of Amazon EC2 instance to use for training\n",
    "* **train_volume_size** - The size in GB of the [Amazon Elastic Block Store](https://aws.amazon.com/ebs/) (Amazon EBS) volume to use for storing input data during training\n",
    "* **train_max_run** - The timeout in seconds for training\n",
    "* **input_mode** - The input mode that the algorithm supports\n",
    "* **output_path** - The Amazon S3 location for saving the training results (model artifacts and output files)\n",
    "* **output_kms_key** - The [AWS Key Management Service](https://aws.amazon.com/kms/) (AWS KMS) key for encrypting the training output\n",
    "* **base_job_name** - The prefix for the name of the training job\n",
    "* **sagemaker_session** - The Session object that manages interactions with Amazon SageMaker API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model artifacts and batch output\n",
    "s3_output <- paste('s3:/', bucket, prefix,'output', sep = '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator\n",
    "estimator <- sagemaker$estimator$Estimator(image_name = container,\n",
    "                                           role = role_arn,\n",
    "                                           train_instance_count = 1L,\n",
    "                                           train_instance_type = 'ml.m5.4xlarge',\n",
    "                                           train_volume_size = 30L,\n",
    "                                           train_max_run = 3600L,\n",
    "                                           input_mode = 'File',\n",
    "                                           output_path = s3_output,\n",
    "                                           output_kms_key = NULL,\n",
    "                                           base_job_name = NULL,\n",
    "                                           sagemaker_session = NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** - The equivalent to `None` in Python is `NULL` in R.\n",
    "\n",
    "Next, we Specify the [XGBoost hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html) for the estimator. \n",
    "\n",
    "Once the Estimator and its hyperparamters are specified, you can train (or fit) the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "estimator$set_hyperparameters(eval_metric='rmse',\n",
    "                              objective='reg:linear',\n",
    "                              num_round=100L,\n",
    "                              rate_drop=0.3,\n",
    "                              tweedie_variance_power=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training job name\n",
    "job_name <- paste('sagemaker-r-xgboost', format(Sys.time(), '%H-%M-%S'), sep = '-')\n",
    "\n",
    "# Define the data channels for train and validation datasets\n",
    "input_data <- list('train' = s3_train_input,\n",
    "                   'validation' = s3_valid_input)\n",
    "\n",
    "# train the estimator\n",
    "estimator$fit(inputs = input_data, job_name = job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h3> Batch Transform using SageMaker Transformer </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on SageMaker Batch Transform, you can visit this example notebook on [Amazon SageMaker Batch Transform](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_batch_transform/introduction_to_batch_transform/batch_transform_pca_dbscan_movie_clusters.ipynb).\n",
    "\n",
    "In many situations, using a deployed model for making inference is not the best option, especially when the goal is not to make online real-time inference but to generate predictions from a trained model on a large dataset. In these situations, using Batch Transform may be more efficient and appropriate.\n",
    "\n",
    "This section of the notebook explain how to set up the Batch Transform Job, and generate predictions.\n",
    "\n",
    "To do this, we need to define the batch input data path on S3, and also where to save the generated predictions on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 path for Test data \n",
    "s3_test_url <- paste('s3:/', bucket, prefix, 'data','abalone_test.csv', sep = '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a `Transformer`. [Transformers](https://sagemaker.readthedocs.io/en/stable/transformer.html#transformer) take multiple paramters, including the following. For more details and the complete list visit the [documentation page](https://sagemaker.readthedocs.io/en/stable/transformer.html#transformer).\n",
    "\n",
    "- **model_name** (str) – Name of the SageMaker model being used for the transform job.\n",
    "- **instance_count** (int) – Number of EC2 instances to use.\n",
    "- **instance_type** (str) – Type of EC2 instance to use, for example, ‘ml.c4.xlarge’.\n",
    "\n",
    "- **output_path** (str) – S3 location for saving the transform result. If not specified, results are stored to a default bucket.\n",
    "\n",
    "- **base_transform_job_name** (str) – Prefix for the transform job when the transform() method launches. If not specified, a default prefix will be generated based on the training image name that was used to train the model associated with the transform job.\n",
    "\n",
    "- **sagemaker_session** (sagemaker.session.Session) – Session object which manages interactions with Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one using the default AWS configuration chain.\n",
    "\n",
    "Once we create a `Transformer` we can transform the batch input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformer\n",
    "transformer <- estimator$transformer(instance_count=1L, \n",
    "                                     instance_type='ml.m4.xlarge',\n",
    "                                     output_path = s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the batch transform\n",
    "transformer$transform(s3_test_url,\n",
    "                     wait = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3> Download the Batch Transform Output </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file from S3 using S3Downloader to local SageMaker instance 'batch_output' folder\n",
    "sagemaker$s3$S3Downloader$download(paste(s3_output,\"abalone_test.csv.out\",sep = '/'),\n",
    "                          \"batch_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the batch csv from sagemaker local files\n",
    "library(readr)\n",
    "predictions <- read_csv(file = 'batch_output/abalone_test.csv.out', col_names = 'predicted_rings')\n",
    "head(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column-bind the predicted rings to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate predictions and test for comparison\n",
    "abalone_predictions <- cbind(predicted_rings = predictions, \n",
    "                      abalone_test)\n",
    "# Convert predictions to Integer\n",
    "abalone_predictions$predicted_rings = as.integer(abalone_predictions$predicted_rings);\n",
    "head(abalone_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate RMSE\n",
    "rmse <- function(m, o){\n",
    "  sqrt(mean((m - o)^2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calucalte RMSE\n",
    "abalone_rmse <- rmse(abalone_predictions$rings, abalone_predictions$predicted_rings)\n",
    "cat('RMSE for Batch Transform: ', round(abalone_rmse, digits = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
