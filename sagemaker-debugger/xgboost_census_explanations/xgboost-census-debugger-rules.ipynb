{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability with Amazon SageMaker Debugger\n",
    "_**Explain a XGBoost model that predicts an individual's income**_\n",
    "\n",
    "This notebook demonstrates how to use Amazon SageMaker Debugger to capture the feature importance and SHAP values for a XGBoost model.\n",
    "\n",
    "This notebook was created and tested on an ml.t2.medium notebook instance.\n",
    "\n",
    "**Table of Contents** \n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Section 1 - Setup](#setup)\n",
    "3. [Section 2 - Prepare Data](#prep-data)\n",
    "4. [Section 3 - Train XGBoost model with Amazon SageMaker with debugger enabled](#train)\n",
    "5. [Section 4 - Analyze debugger output](#analyze-debugger-ouput)\n",
    "6. [Clean up](#cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id='intro'></a>\n",
    "\n",
    "Amazon SageMaker Debugger is a new capability of Amazon SageMaker that allows debugging machine learning training. The capability helps you monitor the training jobs in near real time using rules and alert you once it has detected inconsistency in training. \n",
    "\n",
    "Using Amazon SageMaker Debugger is a two step process: Saving tensors and Analysis.\n",
    "Let's look at each one of them closely.\n",
    "\n",
    "### Saving tensors\n",
    "\n",
    "In deep learning algorithms, tensors define the state of the training job at any particular instant in its lifecycle.  Amazon SageMaker Debugger exposes a library which allows you to capture these tensors and save them for analysis.\n",
    "\n",
    "Although XGBoost is not a deep learning algorithm, Amazon SageMaker Debugger is highly customizable and can help provide interpretability by saving insightful metrics, such as performance metrics or feature importances, at different frequencies.\n",
    "Refer to [documentation](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/xgboost.md) for details on how to save the metrics you want.\n",
    "\n",
    "Metrics saved can also include feature importance and SHAP values for all features in the dataset. TODO : Add links to the collections / metrics collected.  The feature importance and SHAP values collected are what we will use to provide local and global explainability.\n",
    "\n",
    "\n",
    "### Analysis\n",
    "\n",
    "After the tensors are saved, perform automatic analysis by running debugging ***Rules***.\n",
    "On a very broad level, a rule is Python code used to detect certain conditions during training.\n",
    "Some of the conditions that a data scientist training an algorithm may care about are monitoring for gradients getting too large or too small, detecting overfitting, and so on.\n",
    "Amazon SageMaker Debugger comes pre-packaged with certain rules that can be invoked on Amazon SageMaker. Users can also write their own rules using the Amazon SageMaker Debugger APIs. \n",
    "For more information about automatic analysis using a rule, see the [rules documentation](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/analysis.md).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Setup <a id='setup'></a>\n",
    "\n",
    "In this section, we will import the necessary libraries, setup variables and examine dataset used. that was used to train the XGBoost model to predict an individual's income.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* The AWS region used to host your model.\n",
    "* The IAM role associated with this SageMaker notebook instance.\n",
    "* The S3 bucket used to store the data used to train the model, save debugger information during training and the trained model artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install the 'smdebug' open source library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install smdebug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `shap` library (link?) to perform global and local SHAP analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 AWS region and  IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 S3 bucket and prefix to hold training data, debugger information and model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"DEMO-smdebug-xgboost-adult-income-prediction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Debugger is available in Amazon SageMaker XGBoost container version 0.90-2 or later. If you want to use XGBoost with Amazon SageMaker Debugger, you have to specify `repo_version='0.90-2'` in the `get_image_uri` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# Below changes the Region to be one where this notebook is running\n",
    "#region = boto3.Session().region_name\n",
    "\n",
    "container = get_image_uri(region, \"xgboost\", repo_version=\"0.90-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Data preparation <a id='prep-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "X, y = shap.datasets.adult()\n",
    "X_display, y_display = shap.datasets.adult(display=True)\n",
    "feature_names = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train/test split\n",
    "from sklearn.model_selection import train_test_split   # For splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "X_train_display = X_display.loc[X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_display.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([pd.Series(y_train, index=X_train.index, name='Income>50K', dtype=int), X_train], axis=1)\n",
    "test = pd.concat([pd.Series(y_test, index=X_test.index, name='Income>50K', dtype=int), X_test], axis=1)\n",
    "\n",
    "# Use 'csv' format to store the data\n",
    "# The first column is expected to be the output column\n",
    "train.to_csv('train.csv', index=False, header=False)\n",
    "test.to_csv('validation.csv', index=False, header=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'data/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'data/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Train XGBoost model in Amazon SageMaker with  debugger enabled. <a id='train'></a>\n",
    "\n",
    "Now train an XGBoost model with Amazon SageMaker Debugger enabled and monitor the training jobs. This is done using the Amazon SageMaker Estimator API. While the training job is running, use Amazon SageMaker Debugger API to access saved tensors in real time and visualize them. You can rely on Amazon SageMaker Debugger to take care of downloading a fresh set of tensors every time you query for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Debugger is available in Amazon SageMaker XGBoost container version 0.90-2 or later. If you want to use XGBoost with Amazon SageMaker Debugger, you have to specify `repo_version='0.90-2'` in the `get_image_uri` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# Below changes the Region to be one where this notebook is running\n",
    "#region = boto3.Session().region_name\n",
    "\n",
    "container = get_image_uri(region, \"xgboost\", repo_version=\"0.90-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sagemaker import get_execution_role\n",
    "#role = get_execution_role()\n",
    "\n",
    "base_job_name = \"demo-smdebug-xgboost-adult-income-prediction-classification\"\n",
    "bucket_path = 's3://{}'.format(bucket)\n",
    "\n",
    "hyperparameters={\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"silent\": \"0\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"51\",\n",
    "}\n",
    "\n",
    "save_interval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling Debugger in Estimator object\n",
    "\n",
    "\n",
    "#### DebuggerHookConfig\n",
    "\n",
    "Enabling Amazon SageMaker Debugger in training job can be accomplished by adding its configuration into Estimator object constructor:\n",
    "\n",
    "```python\n",
    "from sagemaker.debugger import DebuggerHookConfig, CollectionConfig\n",
    "\n",
    "estimator = Estimator(\n",
    "    ...,\n",
    "    debugger_hook_config = DebuggerHookConfig(\n",
    "        s3_output_path=\"s3://{bucket_name}/{location_in_bucket}\",  # Required\n",
    "        collection_configs=[\n",
    "            CollectionConfig(\n",
    "                name=\"metrics\",\n",
    "                parameters={\n",
    "                    \"save_interval\": \"10\"\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "```\n",
    "Here, the `DebuggerHookConfig` object instructs `Estimator` what data we are interested in.\n",
    "Two parameters are provided in the example:\n",
    "\n",
    "- `s3_output_path`: it points to S3 bucket/path where we intend to store our debugging tensors.\n",
    "  Amount of data saved depends on multiple factors, major ones are: training job / data set / model / frequency of saving tensors.\n",
    "  This bucket should be in your AWS account, and you should have full access control over it.\n",
    "  **Important Note**: this s3 bucket should be originally created in the same region where your training job will be running, otherwise you might run into problems with cross region access.\n",
    "\n",
    "- `collection_configs`: it enumerates named collections of tensors we want to save.\n",
    "  Collections are a convinient way to organize relevant tensors under same umbrella to make it easy to navigate them during analysis.\n",
    "  In this particular example, you are instructing Amazon SageMaker Debugger that you are interested in a single collection named `metrics`.\n",
    "  We also instructed Amazon SageMaker Debugger to save metrics every 10 iteration.\n",
    "  See [Collection](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#collection) documentation for all parameters that are supported by Collections and DebuggerConfig documentation for more details about all parameters DebuggerConfig supports.\n",
    "  \n",
    "#### Rules\n",
    "\n",
    "Enabling Rules in training job can be accomplished by adding the `rules` configuration into Estimator object constructor.\n",
    "\n",
    "- `rules`: This new parameter will accept a list of rules you wish to evaluate against the tensors output by this training job.\n",
    "  For rules, Amazon SageMaker Debugger supports two types:\n",
    "  - SageMaker Rules: These are rules specially curated by the data science and engineering teams in Amazon SageMaker which you can opt to evaluate against your training job.\n",
    "  - Custom Rules: You can optionally choose to write your own rule as a Python source file and have it evaluated against your training job.\n",
    "    To provide Amazon SageMaker Debugger to evaluate this rule, you would have to provide the S3 location of the rule source and the evaluator image.\n",
    "\n",
    "In this example, you will use a Amazon SageMaker's LossNotDecreasing rule, which helps you identify if you are running into a situation where the training loss is not going down.\n",
    "\n",
    "```python\n",
    "from sagemaker.debugger import rule_configs, Rule\n",
    "\n",
    "estimator = Estimator(\n",
    "    ...,\n",
    "    rules=[\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.loss_not_decreasing(),\n",
    "            rule_parameters={\n",
    "                \"collection_names\": \"metrics\",\n",
    "                \"num_steps\": \"10\",\n",
    "            },\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "```\n",
    "\n",
    "- `rule_parameters`: In this parameter, you provide the runtime values of the parameter in your constructor.\n",
    "  You can still choose to pass in other values which may be necessary for your rule to be evaluated.\n",
    "  In this example, you will use Amazon SageMaker's LossNotDecreasing rule to monitor the `metircs` collection.\n",
    "  The rule will alert you if the tensors in `metrics` has not decreased for more than 10 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig, CollectionConfig\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "xgboost_estimator = Estimator(\n",
    "    role=role,\n",
    "    base_job_name=base_job_name,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.4xlarge',\n",
    "    image_name=container,\n",
    "    hyperparameters=hyperparameters,\n",
    "    train_max_run=1800,\n",
    "\n",
    "    debugger_hook_config=DebuggerHookConfig(\n",
    "        s3_output_path=bucket_path,  # Required\n",
    "        collection_configs=[\n",
    "            CollectionConfig(\n",
    "                name=\"metrics\",\n",
    "                parameters={\n",
    "                    \"save_interval\": str(save_interval)\n",
    "                }\n",
    "            ),\n",
    "            CollectionConfig(\n",
    "                name=\"feature_importance\",\n",
    "                parameters={\n",
    "                    \"save_interval\": str(save_interval)\n",
    "                }\n",
    "            ),\n",
    "            CollectionConfig(\n",
    "                name=\"full_shap\",\n",
    "                parameters={\n",
    "                    \"save_interval\": str(save_interval)\n",
    "                }\n",
    "            ),\n",
    "            CollectionConfig(\n",
    "                name=\"average_shap\",\n",
    "                parameters={\n",
    "                    \"save_interval\": str(save_interval)\n",
    "                }\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "\n",
    "    rules=[\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.loss_not_decreasing(),\n",
    "            rule_parameters={\n",
    "                \"collection_names\": \"metrics\",\n",
    "                \"num_steps\": str(save_interval * 2),\n",
    "            },\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the next step, start a training job by using the Estimator object you created above. This job is started in an asynchronous, non-blocking way. This means that control is passed back to the notebook and further commands can be run while the training job is progressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import s3_input\n",
    "\n",
    "train_input = s3_input(\"s3://{}/{}/{}\".format(bucket, prefix, \"data/train.csv\"), content_type=\"csv\")\n",
    "validation_input = s3_input( \"s3://{}/{}/{}\".format(bucket, prefix, \"data/validation.csv\"), content_type=\"csv\")\n",
    "xgboost_estimator.fit(\n",
    "    {\"train\": train_input, \"validation\": validation_input},\n",
    "    # This is a fire and forget event. By setting wait=False, you submit the job to run in the background.\n",
    "    # Amazon SageMaker starts one training job and release control to next cells in the notebook.\n",
    "    # Follow this notebook to see status of the training job.\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "As a result of the above command, Amazon SageMaker starts **one training job and one rule job** for you. The first one is the job that produces the tensors to be analyzed. The second one analyzes the tensors to check if `train-error` and `validation-error` are not decreasing at any point during training.\n",
    "\n",
    "Check the status of the training job below.\n",
    "After your training job is started, Amazon SageMaker starts a rule-execution job to run the LossNotDecreasing rule.\n",
    "\n",
    "**Note that the next cell blocks until the rule execution job ends. You can stop it at any point to proceed to the rest of the notebook. Once it says Rule Evaluation Status is Started, and shows the `RuleEvaluationJobArn`, you can look at the status of the rule being monitored.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for _ in range(36):\n",
    "    job_name = xgboost_estimator.latest_training_job.name\n",
    "    client = xgboost_estimator.sagemaker_session.sagemaker_client\n",
    "    description = client.describe_training_job(TrainingJobName=job_name)\n",
    "    training_job_status = description[\"TrainingJobStatus\"]\n",
    "    rule_job_summary = xgboost_estimator.latest_training_job.rule_job_summary()\n",
    "    rule_evaluation_status = rule_job_summary[0][\"RuleEvaluationStatus\"]\n",
    "    print(\"Training job status: {}, Rule Evaluation Status: {}\".format(training_job_status, rule_evaluation_status))\n",
    "\n",
    "    if rule_evaluation_status in [\"Stopped\", \"IssuesFound\", \"NoIssuesFound\"]:\n",
    "        break\n",
    "\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the status of the Rule Evaluation Job\n",
    "\n",
    "To get the rule evaluation job that Amazon SageMaker started for you, run the command below. The results show you the `RuleConfigurationName`, `RuleEvaluationJobArn`, `RuleEvaluationStatus`, `StatusDetails`, and `RuleEvaluationJobArn`.\n",
    "If the tensors meets a rule evaluation condition, the rule execution job throws a client error with `RuleEvaluationConditionMet`.\n",
    "\n",
    "The logs of the rule evaluation job are available in the Cloudwatch Logstream `/aws/sagemaker/ProcessingJobs` with `RuleEvaluationJobArn`.\n",
    "\n",
    "You can see that once the rule execution job starts, it identifies the loss not decreasing situation in the training job, it raises the `RuleEvaluationConditionMet` exception, and it ends the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_estimator.latest_training_job.rule_job_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - Analyze debugger output <a id='analyze-debugger-ouput'></a>\n",
    "\n",
    "Now that you've trained the system, analyze the data.  Here, you focus on after-the-fact analysis.\n",
    "\n",
    "You import a basic analysis library, which defines the concept of trial, which represents a single training run.\n",
    "\n",
    "### Retrieving and Analyzing tensors\n",
    "\n",
    "Before getting to analysis, here are some notes on concepts being used in Amazon SageMaker Debugger that help with analysis.\n",
    "- ***Trial*** - Object that is a centerpiece of the SageMaker Debugger API when it comes to getting access to tensors. It is a top level abstract that represents a single run of a training job. All tensors emitted by a training job are associated with its *trial*.\n",
    "- ***Step*** - Object that represents next level of abstraction. In SageMaker Debugger, *step* is a representation of a single batch of a training job. Each trial has multiple steps. Each tensor is associated with multiple steps and has a particular value at each of the steps.\n",
    "- ***Tensor*** - object that represent actual *tensor* saved during training job. *Note* - it could be a scalar as well (for example, metrics are saved as scalars).\n",
    "\n",
    "For more details on aforementioned concepts as well as on SageMaker Debugger API in general (including examples) see [SageMaker Debugger Analysis API](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/analysis.md) documentation.\n",
    "\n",
    "In the following code cell, use a ***Trial*** to access tensors. You can do that by inspecting currently running training job and extract necessary parameters from its debug configuration to instruct SageMaker Debugger where the data you are looking for is located. Keep in mind the following:\n",
    "- Tensors are being stored in your own S3 bucket to which you can navigate and manually inspect its content if desired.\n",
    "- You might notice a slight delay before trial object is created. This is normal as SageMaker Debugger monitors the corresponding bucket with tensors and waits until tensors appear in it. The delay is introduced by less than instantaneous upload of tensors from a training container to your S3 bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Debugger output is showing up in the s3 bucket only after the training job is complete.  Should we be seeing the output during training?  For our purpose of Explainability, post training analysis might be ok and sufficient??**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "s3_output_path = xgboost_estimator.latest_job_debugger_artifacts_path()\n",
    "trial = create_trial(s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can list all the tensors that you know something about. Each one of these names is the name of a tensor. The name is a combination of the feature name, which in these cases, is auto-assigned by XGBoost, and whether it's an evaluation metric, feature importance, or SHAP value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each tensor, for each step we can get the values at that step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensor(\"train-error\").values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensor(\"average_shap/f1\").values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Performance metrics\n",
    "\n",
    "You can also create a simple function that visualizes the training and validation errors as the training progresses.\n",
    "Each gradient should get smaller over time, as the system converges to a good solution.\n",
    "Remember that this is an interactive analysis. You are showing these tensors to give an idea of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "MAX_PLOTS = 35\n",
    "from itertools import islice\n",
    "\n",
    "def get_data(trial, tname):\n",
    "    \"\"\"\n",
    "    For the given tensor name, walks though all the iterations\n",
    "    for which you have data and fetches the values.\n",
    "    Returns the set of steps and the values.\n",
    "    \"\"\"\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps()\n",
    "    vals = [tensor.value(s) for s in steps]\n",
    "    return steps, vals\n",
    "\n",
    "def plot_collection(trial, collection_name, regex='.*', figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Takes a `trial` and a collection name, and \n",
    "    plots all tensors that match the given regex.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    tensors = sorted(trial.collection(collection_name).tensor_names)\n",
    "    matched_tensors = [t for t in tensors if re.match(regex, t)]\n",
    "    for tensor_name in islice(matched_tensors, MAX_PLOTS):\n",
    "        steps, data = get_data(trial, tensor_name)\n",
    "        ax.plot(steps, data, label=tensor_name)\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collection(trial, \"metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances\n",
    "\n",
    "You can also visualize the feature priorities as determined by\n",
    "[xgboost.get_score()](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.get_score).\n",
    "If you instructed Estimator to log the `feature_importance` collection, all importance types supported by `xgboost.get_score()` will be available in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(trial, importance_type=\"weight\"):\n",
    "    SUPPORTED_IMPORTANCE_TYPES = [\"weight\", \"gain\", \"cover\", \"total_gain\", \"total_cover\"]\n",
    "    if importance_type not in SUPPORTED_IMPORTANCE_TYPES:\n",
    "        raise ValueError(f\"{importance_type} is not one of the supported importance types.\")\n",
    "    plot_collection(\n",
    "        trial,\n",
    "        \"feature_importance\",\n",
    "        regex=f\"feature_importance/{importance_type}/.*\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(trial, importance_type=\"cover\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP\n",
    "\n",
    "[SHAP](https://github.com/slundberg/shap) (SHapley Additive exPlanations) is\n",
    "another approach to explain the output of machine learning models.\n",
    "SHAP values represent a feature's contribution to a change in the model output.\n",
    "You instructed Estimator to log the average SHAP values in this example so the SHAP values (as calculated by [xgboost.predict(pred_contribs=True)](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.predict)) will be available the `average_shap` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collection(trial,\"average_shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global explanatory methods allow understanding the model and its feature contributions in aggregate over multiple datapoints. Here we show an aggregate bar plot that plots the mean absolute SHAP value for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = trial.tensor(\"full_shap/f0\").value(trial.last_complete_step)\n",
    "shap_expected_value = shap_values[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_no_bias, plot_type='bar', feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary plot below can provide more context over the bar chart. It tells which features are most important, and also their range of effects over the dataset. The color allows us to match how changes in the value of a feature effect the change in prediction (such as 'increase in age leads to higher log odds for prediction, eventually leading to `True` predictions more often). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_no_bias, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add note about local explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Force plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A force plot explanation shows how features are contributing to push the model output from the base value (the average model output over the dataset) to the model output. Features pushing the prediction higher are shown in **red**, those pushing the prediction lower are in **blue**.\n",
    "\n",
    "Plot below indicates that for this particular data point the prediction (-0.09) is higher than the average (-1.397) primarily because this person is in a relationship (`Relationship = Wife`), and to smaller degree because of the higher age. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(shap_expected_value, shap_no_bias[100,:], X_train_display.iloc[100,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked force plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N_ROWS = shap_no_bias.shape[0]\n",
    "N_SAMPLES = min(500, N_ROWS)\n",
    "sampled_indices = np.random.randint(N_ROWS, size=N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(shap_expected_value, \n",
    "                shap_no_bias[sampled_indices,:], \n",
    "                X_train_display.iloc[sampled_indices,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "Outliers are extreme values that deviate from other observations on data. It's useful to understand the influence of various features for outlier predictions to determine if it's a novelty, an experimental error, or a shortcoming in the model.\n",
    "\n",
    "Here we show force plot for prediction outliers that are on either side of the baseline value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top outliers\n",
    "from scipy import stats\n",
    "N_OUTLIERS = 3  # number of outliers on each side of the tail\n",
    "\n",
    "shap_sum = np.sum(shap_values, axis=1)\n",
    "z_scores = stats.zscore(shap_sum)\n",
    "outlier_indices = (np.argpartition(z_scores, -N_OUTLIERS)[-N_OUTLIERS:]).tolist()\n",
    "outlier_indices += (np.argpartition(z_scores, N_OUTLIERS)[:N_OUTLIERS]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fig_index, outlier_index in enumerate(outlier_indices, start=1): \n",
    "    shap.force_plot(shap_expected_value, \n",
    "                    shap_no_bias[outlier_index, :], \n",
    "                    X_train_display.iloc[outlier_index, :], \n",
    "                    matplotlib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sireesha - Adding additional analysis below to see if we can show SHAP values better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q : Can I use the mean of the average_shap values from all steps?  Or is there another metric that already captures this?**\n",
    "\n",
    "**Q : What values does the SHAP value at each iteration provide?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the average shap value for each feature.\n",
    "from pandas import Series\n",
    "\n",
    "features = []\n",
    "features_avg_shap = []\n",
    "\n",
    "for tensor in  tensors: \n",
    "    #print(tensor)\n",
    "    feature = tensor.split(\"/\")[1]\n",
    "    \n",
    "    feature_shap_values = trial.tensor(tensor).values()\n",
    "    #print(feature_shap_values)\n",
    "    avg_feature_shap = Series([feature_shap_values[k] for k in feature_shap_values]).mean()[0]\n",
    "    \n",
    "    if avg_feature_shap != 0.0 :\n",
    "        features.append(feature)\n",
    "        features_avg_shap.append(avg_feature_shap)\n",
    "        \n",
    "        print(\"feature \", feature, \" avg_feature_shap : \" , avg_feature_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##Using the below as a smaple to show the bar graph.\n",
    "#objects = ('Python', 'C++', 'Java', 'Perl', 'Scala', 'Lisp')\n",
    "#type(objects)\n",
    "\n",
    "#y_pos = np.arange(len(objects))\n",
    "#performance = [10,8,6,4,2,1]\n",
    "\n",
    "#plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "#plt.yticks(y_pos, objects)\n",
    "#plt.xlabel('Usage')\n",
    "#plt.title('Programming language usage')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Only showing 25 features for simplicity and clear graph.  Should we sort by shap values to show only top 20/25?**\n",
    "\n",
    "**Q : How do we get the actualy feature names??**\n",
    "\n",
    "**Q : Is this the right graph to demonstrate global explainability.  If so we need to add content/analysis to make it clear**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = features[0:20]\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "performance = features_avg_shap[0:20]\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Avg SHAP Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
