{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Data Preparation, Process, and Store Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='all-up-overview'></a>\n",
    "\n",
    "## [Overview](./0-AutoClaimFraudDetection.ipynb)\n",
    "* [Notebook 0: Overview, Architecture and Data Exploration](./0-AutoClaimFraudDetection.ipynb)\n",
    "* **[Notebook 1: Data Preparation, Process, and Store Features](./1-data-prep-e2e.ipynb)**\n",
    "  * **[Architecture](#arch)**\n",
    "  * **[Getting started](#aud-getting-started)**\n",
    "  * **[DataSets](#aud-datasets)**\n",
    "  * **[SageMaker Feature Store](#aud-feature-store)**\n",
    "  * **[Create train and test datasets](#aud-dataset)**\n",
    "* [Notebook 2: Train, Check Bias, Tune, Record Lineage, and Register a Model](./2-lineage-train-assess-bias-tune-registry-e2e.ipynb)\n",
    "* [Notebook 3: Mitigate Bias, Train New Model, Store in Registry](./3-mitigate-bias-train-model2-registry-e2e.ipynb)\n",
    "* [Notebook 4: Deploy Model, Run Predictions](./4-deploy-run-inference-e2e.ipynb)\n",
    "* [Notebook 5: Create and Run an End-to-End Pipeline to Deploy the Model](./5-pipeline-e2e.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to perform the Data Prep phase of the ML life cycle. The main Data Wrangling, data ingestion, and multiple transformations will be done through the SageMaker Studio Data Wrangler GUI.\n",
    "\n",
    "In this notebook, we will take the `.flow` files that define the transformations to the raw data. and apply them using a SageMaker Processing job that will apply those transformations to the raw data deposited in the S3 bucket as `.csv` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arch'> </a>\n",
    "## Architecture for Data Prep, Process and Store Features\n",
    "[overview](#all-up-overview)\n",
    "___\n",
    "![Data Prep and Store](./images/e2e-1-pipeline-v3b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required and/or update third-party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -Uq pip\n",
    "!python -m pip install -q awswrangler==2.2.0 imbalanced-learn==0.7.0 sagemaker==2.23.1 boto3==1.16.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading stored variables\n",
    "If you ran this notebook before, you may want to re-use the resources you aready created with AWS. Run the cell below to load any prevously created variables. You should see a print-out of the existing variables. If you don't see anything printed then it's probably the first time you are running the notebook! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Important</font>: You must have run the previous sequential notebooks to retrieve variables using the StoreMagic command.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import string\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-getting-started'></a>\n",
    "## Getting started: Creating Resources\n",
    "\n",
    "[overview](#all-up-overview)\n",
    "___\n",
    "In order to successfully run this notebook you will need to create some AWS resources. \n",
    "First, an S3 bucket will be created to store all the data for this tutorial. \n",
    "Once created, you will then need to create an AWS Glue role using the IAM console then attach a policy to the S3 bucket to allow FeatureStore access to this notebook. If you've already run this notebook and are picking up where you left off, then running the cells below should pick up the resources you already created without creating any additional resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add FeatureStore policy to Studio's execution role\n",
    "\n",
    "![title](images/iam-policies.png)\n",
    "\n",
    "\n",
    "1. In a separate brower tab go to the IAM section of the AWS Console\n",
    "2. Navigate to the Roles section and select the execution role you're using for your SageMaker Studio user\n",
    "    * If you're not sure what role you're using, run the cell below to print it out\n",
    "3. Attach the <font color='green'> AmazonSageMakerFeatureStoreAccess </font> policy to this role. Once attached, the changes take  effect immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SageMaker Role:', sagemaker.get_execution_role().split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set region, boto3 and SageMaker SDK variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can change this to a region of your choice\n",
    "import sagemaker\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"Using AWS Region: {}\".format(region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.setup_default_session(region_name=region)\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: if you are not running this notebook from SageMaker Studio or SageMaker Classic Notebooks you will need to instanatiate \n",
    "the sagemaker_execution_role_name with an AWS role that has SageMakerFullAccess and SageMakerFeatureStoreFullAccess\n",
    "\"\"\"\n",
    "sagemaker_execution_role_name = 'AmazonSageMaker-ExecutionRole-20210107T234882'\n",
    "try:\n",
    "    sagemaker_role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    sagemaker_role = iam.get_role(RoleName=sagemaker_execution_role_name)['Role']['Arn']\n",
    "    print(f\"\\n instantiating sagemaker_role with supplied role name : {sagemaker_role}\")\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a directory in the SageMaker default bucket for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'bucket' not in locals():\n",
    "    bucket = sagemaker_session.default_bucket()\n",
    "    prefix = 'fraud-detect-demo'\n",
    "    %store bucket\n",
    "    %store prefix\n",
    "    print(f'Creating bucket: {bucket}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use your own S3 bucket that's already existing, uncomment and utilize the following example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "try:\n",
    "    s3_client.create_bucket(Bucket=bucket, ACL='private', CreateBucketConfiguration={'LocationConstraint': region})\n",
    "    print('Create S3 bucket: SUCCESS')\n",
    "    \n",
    "except Exception as e:\n",
    "    if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "        print(f'Using existing bucket: {bucket}/{prefix}')\n",
    "    else:\n",
    "        raise(e)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======> Tons of output_paths\n",
    "traing_job_output_path = f's3://{bucket}/{prefix}/training_jobs'\n",
    "bias_report_1_output_path = f's3://{bucket}/{prefix}/clarify-bias-1'\n",
    "bias_report_2_output_path = f's3://{bucket}/{prefix}/clarify-bias-2'\n",
    "explainability_output_path = f's3://{bucket}/{prefix}/clarify-explainability'\n",
    "\n",
    "train_data_uri = f's3://{bucket}/{prefix}/data/train/train.csv'\n",
    "test_data_uri = f's3://{bucket}/{prefix}/data/test/test.csv'\n",
    "\n",
    "#=======> variables used for parameterizing the notebook run\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "claify_instance_count = 1\n",
    "clairfy_instance_type = 'ml.c5.xlarge'\n",
    "\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.c5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3\n",
    "Before you can preprocess the raw data with Data Wrangler, it must exist in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='data/claims.csv', Bucket=bucket, Key=f'{prefix}/data/raw/claims.csv')\n",
    "s3_client.upload_file(Filename='data/customers.csv', Bucket=bucket, Key=f'{prefix}/data/raw/customers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update attributes within the  `.flow` file \n",
    "DataWrangler will generate a .flow file. It contains a reference to an S3 bucket used during the Wrangling. This may be different from the one you have as a default in this notebook eg if the Wrangling was done by someone else, you will probably not have access to their bucket and you now need to point to your own S3 bucket so you can actually load the .flow file into Wrangler or access the data.\n",
    "\n",
    "After running the cell below you can open the `claims.flow` and `customers.flow` files and export the data to S3 or you can continue the guide using the provided `data/claims_preprocessed.csv` and `data/customers_preprocessed.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_flow_template_file = \"claims_flow_template\"\n",
    "\n",
    "with open(claims_flow_template_file, 'r') as f:\n",
    "    variables   = {'bucket': bucket, 'prefix': prefix}\n",
    "    template    = string.Template(f.read())\n",
    "    claims_flow = template.substitute(variables)\n",
    "    claims_flow = json.loads(claims_flow)\n",
    "\n",
    "with open('claims.flow', 'w') as f:\n",
    "    json.dump(claims_flow, f)\n",
    "    \n",
    "customers_flow_template_file = \"customers_flow_template\"\n",
    "\n",
    "with open(customers_flow_template_file, 'r') as f:\n",
    "    variables      = {'bucket': bucket, 'prefix': prefix}\n",
    "    template       = string.Template(f.read())\n",
    "    customers_flow = template.substitute(variables)\n",
    "    customers_flow = json.loads(customers_flow)\n",
    "    \n",
    "with open('customers.flow', 'w') as f:\n",
    "    json.dump(customers_flow, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed data from Data Wrangler job\n",
    "If you ran the Data Wrangler jobs from  `claims.flow` and `customers.flow`, you can load your preprocessed data here. If you did not run the Data Wrangler job, you can still get started by loading the pre-made data sets from the `/data` directory of this example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-datasets'></a>\n",
    "## DataSets and Feature Types\n",
    "[overview](#all-up-overview)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_dtypes = {'policy_id': int,\n",
    " 'incident_severity': int,\n",
    " 'num_vehicles_involved': int,\n",
    " 'num_injuries': int,\n",
    " 'num_witnesses': int,\n",
    " 'police_report_available': int,\n",
    " 'injury_claim': float,\n",
    " 'vehicle_claim': float,\n",
    " 'total_claim_amount': float,\n",
    " 'incident_month': int,\n",
    " 'incident_day': int,\n",
    " 'incident_dow': int,\n",
    " 'incident_hour': int,\n",
    " 'fraud': int,\n",
    " 'driver_relationship_self': int,\n",
    " 'driver_relationship_na': int,\n",
    " 'driver_relationship_spouse': int,\n",
    " 'driver_relationship_child': int,\n",
    " 'driver_relationship_other': int,\n",
    " 'incident_type_collision': int,\n",
    " 'incident_type_breakin': int,\n",
    " 'incident_type_theft': int,\n",
    " 'collision_type_front': int,\n",
    " 'collision_type_rear': int,\n",
    " 'collision_type_side': int,\n",
    " 'collision_type_na': int,\n",
    " 'authorities_contacted_police': int,\n",
    " 'authorities_contacted_none': int,\n",
    " 'authorities_contacted_fire': int,\n",
    " 'authorities_contacted_ambulance': int,\n",
    " 'event_time': float}\n",
    "\n",
    "customers_dtypes = {'policy_id': int,\n",
    " 'customer_age': int,\n",
    " 'customer_education': int,\n",
    " 'months_as_customer': int,\n",
    " 'policy_deductable': int,\n",
    " 'policy_annual_premium': int,\n",
    " 'policy_liability': int,\n",
    " 'auto_year': int,\n",
    " 'num_claims_past_year': int,\n",
    " 'num_insurers_past_5_years': int,\n",
    " 'customer_gender_male': int,\n",
    " 'customer_gender_female': int,\n",
    " 'policy_state_ca': int,\n",
    " 'policy_state_wa': int,\n",
    " 'policy_state_az': int,\n",
    " 'policy_state_or': int,\n",
    " 'policy_state_nv': int,\n",
    " 'policy_state_id': int,\n",
    " 'event_time': float}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======> This is your DataFlow output path if you decide to redo the work in DataFlow on your own\n",
    "flow_output_path = 'YOUR_PATH_HERE'\n",
    "\n",
    "try:\n",
    "    # this will try to load the exported dataframes from the claims and customers .flow files\n",
    "    claims_s3_path = f'{flow_output_path}/claims_output'\n",
    "    customers_s3_path = f'{flow_output_path}/customers_output'\n",
    "    \n",
    "    claims_preprocessed = wr.s3.read_csv(\n",
    "        path=claims_s3_path, \n",
    "        dataset=True, \n",
    "        index_col=0, \n",
    "        dtype=claims_dtypes)\n",
    "    \n",
    "    customers_preprocessed = wr.s3.read_csv(\n",
    "        path=customers_s3_path, \n",
    "        dataset=True, \n",
    "        index_col=0, \n",
    "        dtype=customers_dtypes)\n",
    "\n",
    "except:\n",
    "    # if the Data Wrangler job was not run, the claims and customers dataframes will be loaded from local copies\n",
    "    timestamp = pd.to_datetime('now').timestamp()\n",
    "    print('Unable to load Data Wrangler output. Loading pre-made dataframes...')\n",
    "    \n",
    "    claims_preprocessed = pd.read_csv(\n",
    "        filepath_or_buffer='data/claims_preprocessed.csv', \n",
    "        dtype=claims_dtypes)\n",
    "    \n",
    "    # a timestamp column is required by the feature store, so one is added with a current timestamp\n",
    "    claims_preprocessed['event_time'] = timestamp\n",
    "    \n",
    "    customers_preprocessed = pd.read_csv(\n",
    "        filepath_or_buffer='data/customers_preprocessed.csv', \n",
    "        dtype=customers_dtypes)\n",
    "    \n",
    "    customers_preprocessed['event_time'] = timestamp\n",
    "    \n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a set of Pandas DataFrames that contain the customer and claim data, with the correct data types. When Dat Wrangler encodes a feature as one-hot-encoded feature, it will default to float data types for those resulting features (one feature --> many columns for the one hot encoding). \n",
    "\n",
    "<font color ='red'> Note: </font> the reason for explicitly converting the data types for categorical features generated by Data Wrangler, is to ensure they are of type integer so that Clarify will treat them as categorical variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-feature-store'></a>\n",
    "## SageMaker Feature Store\n",
    "\n",
    "[overview](#all-up-overview)\n",
    "___\n",
    "Amazon SageMaker Feature Store is a purpose-built repository where you can store and access features so it’s much easier to name, organize, and reuse them across teams. SageMaker Feature Store provides a unified store for features during training and real-time inference without the need to write additional code or create manual processes to keep features consistent. SageMaker Feature Store keeps track of the metadata of stored features (e.g. feature name or version number) so that you can query the features for the right attributes in batches or in real time using Amazon Athena, an interactive query service. SageMaker Feature Store also keeps features updated, because as new data is generated during inference, the single repository is updated so new features are always available for models to use during training and inference.\n",
    "\n",
    "A feature store consists of an offline componet stored in S3 and an online component stored in a low-latency database. The online database is optional, but very useful if you need supplemental features to be available at inference. In this section, we will create a feature groups for our Claims and Customers datasets. After inserting the claims and customer data into their respective feature groups, you need to query the offline store with Athena to build the training dataset.\n",
    "\n",
    "You can reference the [SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html) for more information about the SageMaker Feature Store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto_session.client(\n",
    "    service_name='sagemaker-featurestore-runtime', \n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the feature groups\n",
    "The datatype for each feature is set by passing a dataframe and inferring the proper datatype. Feature data types can also be set via a config variable, but it will have to match the correspongin Python data type in the Pandas dataframe when it's ingested to the Feature Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_fg_name = f'{prefix}-claims'\n",
    "customers_fg_name = f'{prefix}-customers'\n",
    "%store claims_fg_name \n",
    "%store customers_fg_name\n",
    "\n",
    "claims_feature_group = FeatureGroup(\n",
    "    name=claims_fg_name, \n",
    "    sagemaker_session=feature_store_session)\n",
    "\n",
    "customers_feature_group = FeatureGroup(\n",
    "    name=customers_fg_name, \n",
    "    sagemaker_session=feature_store_session)\n",
    "\n",
    "claims_feature_group.load_feature_definitions(data_frame=claims_preprocessed);\n",
    "customers_feature_group.load_feature_definitions(data_frame=customers_preprocessed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the feature groups\n",
    "You must tell the Feature Group which columns in the dataframe correspond to the required record indentifier and event time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{customers_fg_name} -- {claims_fg_name} are the feature group names in use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_feature_name = 'policy_id'\n",
    "event_time_feature_name = 'event_time'\n",
    "\n",
    "try:\n",
    "    print(f\"\\n Using s3://{bucket}/{prefix}\")\n",
    "    claims_feature_group.create(\n",
    "        s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "        record_identifier_name=record_identifier_feature_name,\n",
    "        event_time_feature_name=event_time_feature_name,\n",
    "        role_arn=sagemaker_role,\n",
    "        enable_online_store=True\n",
    "    )\n",
    "    print(f'Create \"claims\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get('Error').get('Code')\n",
    "    if code == 'ResourceInUse':\n",
    "        print(f'Using existing feature group: {claims_fg_name}')\n",
    "    else:\n",
    "        raise(e)\n",
    "\n",
    "try:\n",
    "    customers_feature_group.create(\n",
    "        s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "        record_identifier_name=record_identifier_feature_name,\n",
    "        event_time_feature_name=event_time_feature_name,\n",
    "        role_arn=sagemaker_role,\n",
    "        enable_online_store=True\n",
    "    )\n",
    "    print(f'Create \"customers\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get('Error').get('Code')\n",
    "    if code == 'ResourceInUse':\n",
    "        print(f'Using existing feature group: {customers_fg_name}')\n",
    "    else:\n",
    "        raise(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait until feature group creation has fully completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "    \n",
    "wait_for_feature_group_creation_complete(feature_group=claims_feature_group)\n",
    "wait_for_feature_group_creation_complete(feature_group=customers_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest records into the Feature Groups\n",
    "After the Feature Groups have been created, we can put data into each store by using the PutRecord API. This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to s3 in chunks. The files will be written to the offline store within a few minutes of ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'claims_table' in locals():\n",
    "    print(\"You may have already ingested the data into your Feature Groups. If you'd like to do this again, you can run the ingest methods outside of the 'if/else' statement.\")\n",
    "\n",
    "else:\n",
    "    claims_feature_group.ingest(\n",
    "    data_frame=claims_preprocessed, max_workers=3, wait=True\n",
    "    );\n",
    "\n",
    "    customers_feature_group.ingest(\n",
    "        data_frame=customers_preprocessed, max_workers=3, wait=True\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for offline store data to become available\n",
    "This usually takes 5-8 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_feature_group_s3_prefix = f'{prefix}/{account_id}/sagemaker/{region}/offline-store/{claims_fg_name}/data'\n",
    "customers_feature_group_s3_prefix = f'{prefix}/{account_id}/sagemaker/{region}/offline-store/{customers_fg_name}/data'\n",
    "\n",
    "offline_store_contents = None\n",
    "while (offline_store_contents is None):\n",
    "    objects_in_bucket = s3_client.list_objects(Bucket=bucket, Prefix=customers_feature_group_s3_prefix)\n",
    "    if ('Contents' in objects_in_bucket and len(objects_in_bucket['Contents']) > 1):\n",
    "        offline_store_contents = objects_in_bucket['Contents']\n",
    "    else:\n",
    "        print('Waiting for data in offline store...')\n",
    "        time.sleep(60)\n",
    "    \n",
    "print('\\nData available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-dataset'></a>\n",
    "## Create train and test datasets\n",
    "\n",
    "[overview](#all-up-overview)\n",
    "___\n",
    "Once the data is available in the offline store, it will automatically be cataloged and loaded into an Athena table (this is done by default, but can be turned off). In order to build our training and test datasets, you will submit a SQL query to join the the Claims and Customers tables created in Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_query = claims_feature_group.athena_query()\n",
    "customers_query = customers_feature_group.athena_query()\n",
    "\n",
    "claims_table = claims_query.table_name\n",
    "customers_table = customers_query.table_name\n",
    "database_name = customers_query.database\n",
    "%store claims_table\n",
    "%store customers_table\n",
    "%store database_name\n",
    "\n",
    "feature_columns = list( set(claims_preprocessed.columns) ^ set(customers_preprocessed.columns) )\n",
    "feature_columns_string = \", \".join(f'\\\"{c}\\\"' for c in feature_columns)\n",
    "feature_columns_string = f'\"{claims_table}\".policy_id as policy_id, ' + feature_columns_string\n",
    "\n",
    "query_string = f\"\"\"\n",
    "SELECT DISTINCT {feature_columns_string}\n",
    "FROM \"{claims_table}\" LEFT JOIN \"{customers_table}\" \n",
    "ON \"{claims_table}\".policy_id = \"{customers_table}\".policy_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_query.run(query_string=query_string, output_location=f's3://{bucket}/{prefix}/query_results')\n",
    "claims_query.wait()\n",
    "dataset = claims_query.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"./data/claims_customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = ['fraud'] + list(dataset.drop(['fraud', 'policy_id'], axis=1).columns)\n",
    "%store col_order\n",
    "\n",
    "train = dataset.sample(frac=.80, random_state=0)[col_order]\n",
    "test = dataset.drop(train.index)[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write train, test data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/train.csv', index=False)\n",
    "test.to_csv('data/test.csv', index=False)\n",
    "dataset.to_csv('data/dataset.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='data/train.csv', Bucket=bucket, Key=f'{prefix}/data/train/train.csv')\n",
    "s3_client.upload_file(Filename='data/test.csv', Bucket=bucket, Key=f'{prefix}/data/test/test.csv')\n",
    "%store train_data_uri\n",
    "%store test_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>driver_relationship_self</th>\n",
       "      <th>num_witnesses</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>incident_day</th>\n",
       "      <th>policy_state_nv</th>\n",
       "      <th>policy_state_az</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>policy_state_or</th>\n",
       "      <th>...</th>\n",
       "      <th>authorities_contacted_police</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>incident_hour</th>\n",
       "      <th>policy_state_ca</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>authorities_contacted_ambulance</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>customer_gender_male</th>\n",
       "      <th>driver_relationship_other</th>\n",
       "      <th>num_claims_past_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>21500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>750</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2450</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>1</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>750</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2450</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4572</th>\n",
       "      <td>0</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>0</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fraud  vehicle_claim  driver_relationship_self  num_witnesses  \\\n",
       "398       0        21500.0                         1              5   \n",
       "3833      1        16000.0                         1              1   \n",
       "4836      0         4000.0                         1              2   \n",
       "4572      0        19500.0                         1              1   \n",
       "636       0         9500.0                         1              0   \n",
       "\n",
       "      policy_deductable  incident_day  policy_state_nv  policy_state_az  \\\n",
       "398                 750            24                0                1   \n",
       "3833                750             5                0                0   \n",
       "4836                750            19                0                0   \n",
       "4572                750             4                0                1   \n",
       "636                 750            22                0                0   \n",
       "\n",
       "      auto_year  policy_state_or  ...  authorities_contacted_police  \\\n",
       "398        2012                0  ...                             1   \n",
       "3833       2017                0  ...                             1   \n",
       "4836       2009                0  ...                             0   \n",
       "4572       2018                0  ...                             1   \n",
       "636        2012                0  ...                             0   \n",
       "\n",
       "      total_claim_amount  incident_hour  policy_state_ca  injury_claim  \\\n",
       "398              23000.0             20                0        1500.0   \n",
       "3833             16000.0              8                0           0.0   \n",
       "4836              4000.0              5                1           0.0   \n",
       "4572             19500.0             13                0           0.0   \n",
       "636               9500.0             20                1           0.0   \n",
       "\n",
       "      authorities_contacted_ambulance  policy_annual_premium  \\\n",
       "398                                 0                   2450   \n",
       "3833                                0                   2600   \n",
       "4836                                0                   2450   \n",
       "4572                                0                   3000   \n",
       "636                                 0                   2750   \n",
       "\n",
       "      customer_gender_male  driver_relationship_other  num_claims_past_year  \n",
       "398                      1                          0                     0  \n",
       "3833                     0                          0                     0  \n",
       "4836                     1                          0                     0  \n",
       "4572                     1                          0                     0  \n",
       "636                      1                          0                     0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>driver_relationship_self</th>\n",
       "      <th>num_witnesses</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>incident_day</th>\n",
       "      <th>policy_state_nv</th>\n",
       "      <th>policy_state_az</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>policy_state_or</th>\n",
       "      <th>...</th>\n",
       "      <th>authorities_contacted_police</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>incident_hour</th>\n",
       "      <th>policy_state_ca</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>authorities_contacted_ambulance</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>customer_gender_male</th>\n",
       "      <th>driver_relationship_other</th>\n",
       "      <th>num_claims_past_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>41000.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>17500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fraud  vehicle_claim  driver_relationship_self  num_witnesses  \\\n",
       "0       0         8500.0                         0              0   \n",
       "7       0        16000.0                         1              1   \n",
       "21      0         7000.0                         1              0   \n",
       "24      0        17500.0                         0              0   \n",
       "25      0        17000.0                         0              0   \n",
       "\n",
       "    policy_deductable  incident_day  policy_state_nv  policy_state_az  \\\n",
       "0                 750            27                0                0   \n",
       "7                 750             2                0                0   \n",
       "21                750            19                0                0   \n",
       "24                750             1                0                0   \n",
       "25                750            17                1                0   \n",
       "\n",
       "    auto_year  policy_state_or  ...  authorities_contacted_police  \\\n",
       "0        2014                0  ...                             0   \n",
       "7        2014                1  ...                             1   \n",
       "21       2014                0  ...                             0   \n",
       "24       2020                0  ...                             0   \n",
       "25       2018                0  ...                             1   \n",
       "\n",
       "    total_claim_amount  incident_hour  policy_state_ca  injury_claim  \\\n",
       "0               8500.0             15                1           0.0   \n",
       "7              41000.0              8                0       25000.0   \n",
       "21              7000.0              9                1           0.0   \n",
       "24             17500.0              4                1           0.0   \n",
       "25             17000.0             18                0           0.0   \n",
       "\n",
       "    authorities_contacted_ambulance  policy_annual_premium  \\\n",
       "0                                 0                   3000   \n",
       "7                                 0                   3000   \n",
       "21                                0                   3000   \n",
       "24                                0                   3000   \n",
       "25                                0                   3000   \n",
       "\n",
       "    customer_gender_male  driver_relationship_other  num_claims_past_year  \n",
       "0                      1                          0                     0  \n",
       "7                      1                          0                     0  \n",
       "21                     1                          0                     0  \n",
       "24                     1                          0                     0  \n",
       "25                     1                          0                     0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Next Notebook: [Train, Check Bias, Tune, Record Lineage, Register Model](./2-lineage-train-assess-bias-tune-registry-e2e.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
