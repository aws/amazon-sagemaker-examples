{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52854c40-9e83-474f-b33b-8cbbeea9bb0d",
   "metadata": {},
   "source": [
    "# Benchmarking AWS Graviton Instance Performance on SageMaker with Tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66124d2a-c22f-4d76-8b03-54e766d6d880",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Amazon SageMaker real-time endpoints allow you to host ML applications at scale. In this notebook, we provide a design pattern for benchmarking AWS Graviton instance performance so you can choose the right deployment configuration for your application. \n",
    "\n",
    "## Environment Setup\n",
    "This notebook assumes you are running on AWS SageMaker today and have access to an S3 bucket from your SageMaker environment. If you are not and would like to get started, take a look at the getting started documentation [here.](https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html)\n",
    "In the next steps, you import standard methods and libraries as well as set variables that will be used in this notebook. The get_execution_role function retrieves the AWS Identity and Access Management (IAM) role you created at the time of creating your notebook instance.\n",
    "\n",
    "In SageMaker Studio, when prompted to select a kernel select `TensorFlow 2.11 Python 3.9 CPU Optimized`.\n",
    "\n",
    "**Note: This notebook needs to be executed in the us-west-2 region**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ceafe-516f-4efe-b188-a182571ab0aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip awscli botocore boto3 sagemaker  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89db44f2-62fc-4205-96a2-4d1adbe4e4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.model import Model\n",
    "import boto3\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54c9977-44c4-4721-bc56-038950dac914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sagemaker_session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88cb3cc-42ce-40f0-a93d-a9571f28a45b",
   "metadata": {},
   "source": [
    "## Machine learning model details\n",
    "\n",
    "Inference Recommender uses metadata about your ML model to recommend the best instance types and endpoint configurations for deployment. You can provide as much or as little information as you'd like but the more information you provide, the better your recommendations will be.\n",
    "\n",
    "ML Frameworks: `TENSORFLOW, PYTORCH, XGBOOST, SAGEMAKER-SCIKIT-LEARN`\n",
    "\n",
    "ML Domains: `COMPUTER_VISION, NATURAL_LANGUAGE_PROCESSING, MACHINE_LEARNING`\n",
    "\n",
    "Example ML Tasks: `CLASSIFICATION, REGRESSION, IMAGE_CLASSIFICATION, OBJECT_DETECTION, SEGMENTATION, FILL_MASK, TEXT_CLASSIFICATION, TEXT_GENERATION, OTHER`\n",
    "\n",
    "Note: Select the task that is the closest match to your model. Chose `OTHER` if none apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ad46b-431e-4665-8671-cfc5464185dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# ML framework details\n",
    "framework = \"tensorflow\"\n",
    "# Note that only the framework major and minor version is supported for Neo compilation\n",
    "framework_version = \"2.11.0\"\n",
    "\n",
    "# model name as standardized by model zoos or a similar open source model\n",
    "model_name = \"resnet50\"\n",
    "\n",
    "# ML model details\n",
    "ml_domain = \"COMPUTER_VISION\"\n",
    "ml_task = \"IMAGE_CLASSIFICATION\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4247f20-eec1-4db1-9712-0896c446ec29",
   "metadata": {},
   "source": [
    "## Create a model archive\n",
    "\n",
    "SageMaker models need to be packaged in `.tar.gz` files. When your SageMaker Endpoint is provisioned, the files in the archive will be extracted and put in `/opt/ml/model/` on the Endpoint. \n",
    "\n",
    "In this step, there are two optional tasks to:\n",
    "\n",
    "   (1) Download a pretrained model from Keras applications\n",
    "   \n",
    "   (2) Download a sample inference script (inference.py) from S3\n",
    "   \n",
    "These tasks are provided as a sample reference but can and should be modified when using your own trained models with Inference Recommender. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698af69-5935-421f-90e7-ccc67e4b6846",
   "metadata": {},
   "source": [
    "### Optional: Download model from Keras applications\n",
    "\n",
    "Let's download the model from Keras applications. By setting the variable download_the_model=False, you can skip the download and provide your own model archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1bf6c6-68bd-4390-a453-1426ed696e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_the_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "785b2c63-7be7-4706-b487-b319e63ffaa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0eee4-9599-4e01-b375-b5646699f544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if download_the_model:\n",
    "    tf.keras.backend.set_learning_phase(0)\n",
    "    input_tensor = tf.keras.Input(name=\"input_1\", shape=(224, 224, 3))\n",
    "    model = tf.keras.applications.resnet50.ResNet50(input_tensor=input_tensor)\n",
    "\n",
    "    # Creating the directory strcture\n",
    "    model_version = \"1\"\n",
    "    export_dir = \"./model/\" + model_version\n",
    "    if not os.path.exists(export_dir):\n",
    "        os.makedirs(export_dir)\n",
    "        print(\"Directory \", export_dir, \" Created \")\n",
    "    else:\n",
    "        print(\"Directory \", export_dir, \" already exists\")\n",
    "\n",
    "    # Save to SavedModel\n",
    "    model.save(export_dir, save_format=\"tf\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5831267e-c9ce-4395-bcc9-f40f612e971f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0802119b-d9a8-46da-ae52-ab32938025f4",
   "metadata": {},
   "source": [
    "### Create Inference Script For Sagemaker Inference Recommender Job\n",
    "We need a script that SageMaker will call to execute the inference. This code gets packaged along with the model and is a pre-requisite for running an inference recommender job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa65c7-b6d5-4551-9b09-0e0c073e9b20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "\n",
    "def input_handler(data, context):\n",
    "    \"\"\"Pre-process request input before it is sent to TensorFlow Serving REST API\n",
    "    https://github.com/aws/amazon-sagemaker-examples/blob/0e57a288f54910a50dcbe3dfe2acb8d62e3b3409/sagemaker-python-sdk/tensorflow_serving_container/sample_utils.py#L61\n",
    "\n",
    "    Args:\n",
    "        data (obj): the request data stream\n",
    "        context (Context): an object containing request and configuration details\n",
    "\n",
    "    Returns:\n",
    "        (dict): a JSON-serializable dict that contains request body and headers\n",
    "    \"\"\"\n",
    "\n",
    "    if context.request_content_type == \"application/x-image\":\n",
    "        buf = np.fromstring(data.read(), np.uint8)\n",
    "        image = Image.open(io.BytesIO(buf)).resize(IMAGE_SIZE)\n",
    "        image = np.array(image)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        return json.dumps({\"instances\": image.tolist()})\n",
    "    else:\n",
    "        _return_error(\n",
    "            415, 'Unsupported content type \"{}\"'.format(context.request_content_type or \"Unknown\")\n",
    "        )\n",
    "\n",
    "\n",
    "def output_handler(response, context):\n",
    "    \"\"\"Post-process TensorFlow Serving output before it is returned to the client.\n",
    "\n",
    "    Args:\n",
    "        response (obj): the TensorFlow serving response\n",
    "        context (Context): an object containing request and configuration details\n",
    "\n",
    "    Returns:\n",
    "        (bytes, string): data to return to client, response content type\n",
    "    \"\"\"\n",
    "    if response.status_code != 200:\n",
    "        _return_error(response.status_code, response.content.decode(\"utf-8\"))\n",
    "    response_content_type = context.accept_header\n",
    "    prediction = response.content\n",
    "    return prediction, response_content_type\n",
    "\n",
    "\n",
    "def _return_error(code, message):\n",
    "    raise ValueError(\"Error: {}, {}\".format(str(code), message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da9663-a375-4233-ba4c-74dd59f142aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "\n",
    "numpy\n",
    "pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa4ffac-0715-49d5-9afb-11966f961412",
   "metadata": {},
   "source": [
    "### Create a tarball\n",
    "\n",
    "To bring your own TensorFlow model, SageMaker expects a single archive file in .tar.gz format, containing a model file (\\*.pb) in TF SavedModel format and the script (\\*.py) for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9807376b-d650-43bc-97a9-272d5d5c9629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_archive_name = \"tfmodel.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaececda-a060-4411-9664-640b9c73a52c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -cvpzf {model_archive_name} ./model ./code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665812f3-23fb-479e-bd2b-e0cc65808609",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "\n",
    "We now have a model archive ready. We need to upload it to S3 before we can use with Inference Recommender. Furthermore, we will use the SageMaker Python SDK to handle the upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f2415-6bee-48d3-a643-2ab833a77e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model package tarball (model artifact + inference code)\n",
    "model_url = sagemaker_session.upload_data(path=model_archive_name, key_prefix=\"tfmodel\")\n",
    "print(\"model uploaded to: {}\".format(model_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e162a8-c790-47a4-8136-45f6e295850e",
   "metadata": {},
   "source": [
    "## Create a sample payload archive\n",
    "\n",
    "We need to create an archive that contains individual files that Inference Recommender can send to your Endpoint. Inference Recommender will randomly sample files from this archive so make sure it contains a similar distribution of payloads you'd expect in production. Note that your inference code must be able to read in the file formats from the sample payload.\n",
    "\n",
    "*Here we are only adding four images for the example. For your own use case(s), it's recommended to add a variety of samples that is representative of your payloads.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3014128b-5d10-4de7-bf62-f08e9ede809a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload_archive_name = \"tf_payload.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6aeaf-b95a-4d78-af50-6ffa012428f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## optional: download sample images\n",
    "SAMPLES_BUCKET = \"sagemaker-sample-files\"\n",
    "PREFIX = \"datasets/image/pets/\"\n",
    "payload_location = \"./sample-payload/\"\n",
    "\n",
    "if not os.path.exists(payload_location):\n",
    "    os.makedirs(payload_location)\n",
    "    print(\"Directory \", payload_location, \" Created \")\n",
    "else:\n",
    "    print(\"Directory \", payload_location, \" already exists\")\n",
    "\n",
    "sagemaker_session.download_data(payload_location, SAMPLES_BUCKET, PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5d57c-a709-48b9-a0b2-91ac45c29e8c",
   "metadata": {},
   "source": [
    "### Tar the payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e7a96-5a15-46b7-892a-c83fe6658d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd ./sample-payload/ && tar czvf ../{payload_archive_name} *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3131ac-0c0b-4f46-a8a4-31b628c0c97e",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "\n",
    "Next, we'll upload the packaged payload examples (payload.tar.gz) that was created above to S3.  The S3 location will be used as input to our Inference Recommender job later in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fb5ab9c-140a-45d9-ade0-a1c16c4bd46f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_payload_url = sagemaker_session.upload_data(\n",
    "    path=payload_archive_name, key_prefix=\"tf_payload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa1d2a-e6d4-47c6-b587-a0424df43d2b",
   "metadata": {},
   "source": [
    "### Setup Job Details\n",
    "We're almost ready to create our Inference Recommender job. We just need to specify a few more details to let Inference Recommender know what framework and DLC we're running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb540f18-b70b-43e4-8aa9-8350677d77ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"resnet50\"\n",
    "ml_domain: \"COMPUTER_VISION\"\n",
    "ml_task: \"IMAGE_CLASSIFICATION\"\n",
    "ml_framework: \"TENSORFLOW\"\n",
    "supported_content_types = [\"application/x-image\"]\n",
    "supported_response_types = [\"application/json\"]\n",
    "framework_version = \"2.11.0\"\n",
    "container_url = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference-graviton:2.9.1-cpu-py38-ubuntu20.04-sagemaker\"\n",
    "benchmark_instance_types = [\"ml.c7g.4xlarge\"]\n",
    "\n",
    "# For the best throughput, we recommend setting \n",
    "# SAGEMAKER_MODEL_SERVER_WORKERS to the number of vCPUs the instance being evaluated has\n",
    "# and to not to over subscribe the threads, we recommend setting \n",
    "# OMP_NUM_THREADS to 1 so that each model server workers gets 1 thread.\n",
    "model_container_environment_variables = {\n",
    "    'OMP_NUM_THREADS': '4',\n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '16',\n",
    "    'SAGEMAKER_NGINX_PROXY_READ_TIMEOUT_SECONDS': '600',\n",
    "    'DNNL_DEFAULT_FPMATH_MODE': 'BF16',\n",
    "    'DNNL_VERBOSE': '1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0a94014-7f03-4b13-a188-9ade04458252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_and_benchmark_model(model_name, container_url, model_url, execution_role, sample_payload_url, model_container_environment_variables, supported_content_types, supported_response_types, benchmark_instance_types, framework, framework_version, ml_domain, ml_task, sagemaker_session):\n",
    "    model_package_name = model_name + str(round(time.time()))\n",
    "    job_name = model_name + \"-ir-job-\" + str(round(time.time()))\n",
    "\n",
    "    benchmark_model = Model(\n",
    "        image_uri=container_url,\n",
    "        model_data=model_url,\n",
    "        role=execution_role,\n",
    "        env=model_container_environment_variables,\n",
    "        name=model_package_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    benchmark_model.right_size(sample_payload_url, supported_response_types,\n",
    "                               benchmark_instance_types, job_name, framework)\n",
    "    return job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4abd7e-4b3e-4cfd-8bdc-7da757664217",
   "metadata": {},
   "source": [
    "### Launch Inference Recommender Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df4093-5f72-4c25-96c2-d8eb93c40fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_name = create_and_benchmark_model(model_name, container_url, model_url, role, sample_payload_url, model_container_environment_variables, supported_content_types,\n",
    "                           supported_response_types, benchmark_instance_types, framework, framework_version, ml_domain, ml_task, sagemaker_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc440c2-8bb3-44c2-8648-bb5c949171f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Inference Recommender Results\n",
    "The next bit of code will allow you to pull the relevant cost metrics from the results of the SageMaker Inference Recommender job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9fa67e9-63a2-4273-bda4-207bddb8f6dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_ir_job_results(job_name, instance_type):\n",
    "    response=sm_client.describe_inference_recommendations_job(JobName=job_name)\n",
    "    inference_recommendations =response['InferenceRecommendations'][0]['Metrics']\n",
    "    initial_instance_count = response['InferenceRecommendations'][0]['EndpointConfiguration']['InitialInstanceCount']\n",
    "    cost_per_hour = inference_recommendations['CostPerHour']\n",
    "    cost_per_inference = inference_recommendations['CostPerInference']\n",
    "    cost_per_million_inferences = cost_per_inference * 1000000\n",
    "    \n",
    "    data_frame_data = {\n",
    "        'InstanceType' : [instance_type],\n",
    "        'CostPerInference' : [cost_per_inference],\n",
    "        'CostPerHour' : [cost_per_hour],\n",
    "        'CostPerMillionInferences' : [cost_per_million_inferences]\n",
    "    }\n",
    "    \n",
    "    pd.set_option(\"max_colwidth\", 400)\n",
    "    \n",
    "    data_frame = pd.DataFrame(data_frame_data)\n",
    "    data_frame = data_frame.reindex(columns=['InstanceType', 'CostPerInference', 'CostPerHour', 'CostPerMillionInferences'])\n",
    "\n",
    "    \n",
    "    print(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2cf9ff-cc29-4ae3-8879-c4b1d0850f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ir_job_results(job_name, benchmark_instance_types[0])"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8 (main, Nov 10 2021, 06:03:50) \n[Clang 12.0.0 (clang-1200.0.32.29)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
