{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GluonCV SSD Mobilenet training and optimizing using SageMaker Neo\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Preparation](#Data-Preparation)\n",
    "  1. [Download data](#Download-data)\n",
    "  2. [Convert data into RecordIO](#Convert-data-into-RecordIO)\n",
    "  3. [Upload data to S3](#Upload-data-to-S3)\n",
    "4. [Train the model](#Train-the-model)\n",
    "5. [Compile the trained model using SageMaker Neo](#Compile-the-trained-model-using-SageMaker-Neo)\n",
    "6. [Deploy the compiled model and request Inferences](#Deploy-the-compiled-model-and-request-Inferences)\n",
    "7. [Delete the Endpoint](#Delete-the-Endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is an end-to-end example of GluonCV SSD model training inside sagemaker notebook and then compile the trained model using SageMaker Neo. In this demo, we will demonstrate how to train a mobilenet model on the [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/) using the Single Shot multibox Detector ([SSD](https://arxiv.org/abs/1512.02325)) algorithm. We will also demonstrate how to optimize this trained model using SageMaker Neo and host it.\n",
    "\n",
    "***This notebook is for demonstration purpose only. Please fine tune the training parameters based on your own dataset.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before getting started, make sure to select `Python 3 (Data Science)` kernel. Ensure that `Apache MXNet` and `OpenCV` packages are installed in the kernel which is required to pre-process the dataset.\n",
    "\n",
    "Next, we need to define a few variables and obtain certain permissions that will be needed later in the example. These are:\n",
    "* A SageMaker session\n",
    "* IAM role to  give learning, storage & hosting access to your data\n",
    "* An S3 bucket, a folder & sub folders that will be used to store data and artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /root/amazon-sagemaker-examples/aws_sagemaker_studio/sagemaker_neo_compilation_jobs/gluoncv_ssd_mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mxnet\n",
    "!apt-get update\n",
    "!apt-get install -y python3-opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need an AWS account role with SageMaker access. This role is used to give SageMaker access to your data in S3. We also create a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need an S3 bucket that would be used for storing the model artifacts generated after training and compilation, training data and custom code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket and folders for saving code and model artifacts.\n",
    "# Feel free to specify different bucket/folders here if you wish.\n",
    "bucket = sess.default_bucket() \n",
    "folder = 'StudioDemo-ObjectDetection-SSD-MobileNet'\n",
    "custom_code_sub_folder = folder + '/custom-code'\n",
    "training_data_sub_folder = folder + '/training-data'\n",
    "training_output_sub_folder = folder + '/training-output'\n",
    "compilation_output_sub_folder = folder + '/compilation-output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily visualize the detection outputs we also define the following function. The function visualizes the high-confidence predictions with bounding box by filtering out low-confidence detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def visualize_detection(img_file, dets, classes=[], thresh=0.6):\n",
    "        \"\"\"\n",
    "        visualize detections in one image\n",
    "        Parameters:\n",
    "        ----------\n",
    "        img_file : numpy.array\n",
    "            image, in bgr format\n",
    "        dets : numpy.array\n",
    "            ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n",
    "            each row is one object\n",
    "        classes : tuple or list of str\n",
    "            class names\n",
    "        thresh : float\n",
    "            score threshold\n",
    "        \"\"\"\n",
    "        import random\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.image as mpimg\n",
    "        from matplotlib.patches import Rectangle\n",
    "\n",
    "        img=mpimg.imread(img_file)\n",
    "        plt.imshow(img)\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        colors = dict()\n",
    "        klasses = dets[0][0]\n",
    "        scores = dets[1][0]\n",
    "        bbox = dets[2][0]\n",
    "        for i in range(len(classes)):\n",
    "            klass = klasses[i][0]\n",
    "            score = scores[i][0]\n",
    "            x0, y0, x1, y1 = bbox[i]\n",
    "            if score < thresh:\n",
    "                continue\n",
    "            cls_id = int(klass)\n",
    "            if cls_id not in colors:\n",
    "                colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "            xmin = int(x0 * width / 512)\n",
    "            ymin = int(y0 * height / 512)\n",
    "            xmax = int(x1 * width / 512)\n",
    "            ymax = int(y1 * height / 512)\n",
    "            rect = Rectangle((xmin, ymin), xmax - xmin,\n",
    "                                 ymax - ymin, fill=False,\n",
    "                                 edgecolor=colors[cls_id],\n",
    "                                 linewidth=3.5)\n",
    "            plt.gca().add_patch(rect)\n",
    "            class_name = str(cls_id)\n",
    "            if classes and len(classes) > cls_id:\n",
    "                class_name = classes[cls_id]\n",
    "            plt.gca().text(xmin, ymin-2,\n",
    "                            '{:s} {:.3f}'.format(class_name, score),\n",
    "                            bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n",
    "                                    fontsize=12, color='white')\n",
    "        plt.tight_layout(rect=[0, 0, 2, 2])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing object categories\n",
    "object_categories = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', \n",
    "                     'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', \n",
    "                     'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "# Setting a threshold 0.20 will only plot detection results that have a confidence score greater than 0.20\n",
    "threshold = 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we load the test image into the memory. The test image used in this notebook is from [PEXELS](https://www.pexels.com/) which remains unseen until the time of preditcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "test_file = 'test.jpg'\n",
    "test_image = PIL.Image.open(test_file)\n",
    "test_image = np.asarray(test_image.resize((512, 512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) was a popular computer vision challenge and they released annual challenge datasets for object detection from 2005 to 2012. In this notebook, we will use the data sets from 2007 and 2012, named as VOC07 and VOC12 respectively. Cumulatively, we have more than 20,000 images containing about 50,000 annotated objects. These annotated objects are grouped into 20 categories.\n",
    "\n",
    "***Notes:***\n",
    "1. While using the Pascal VOC dataset, please be aware of the database usage rights. The VOC data includes images obtained from flickr's website. Use of these images must respect the corresponding terms of use: https://www.flickr.com/help/terms\n",
    "2. If you are running this notebook inside of a SageMaker Notebook instance then while performing this step you might run out of storage as the default EBS volume size for SageMaker Notebook instances is 5GB. One way to increase the EBS Volume size of your Notebook instance is by using AWS CLI as documented [here](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/sagemaker/update-notebook-instance.html).\n",
    "3. Amazon SageMaker Studio uses Amazon Elastic File System (EFS). To manage EFS volume size you can follow our doc [here](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-manage-storage.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Download and extract the Pascal VOC datasets for 2007 and 2012 from Oxford University's website.\n",
    "\n",
    "_Note: This step may take around 30mins to complete._\n",
    "\n",
    "***Following is an alternative link to download the dataset if there is some connection problem: https://course.fast.ai/datasets#image-localization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Download the dataset\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Extract the data\n",
    "!tar -xf VOCtrainval_11-May-2012.tar\n",
    "!tar -xf VOCtrainval_06-Nov-2007.tar\n",
    "!tar -xf VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data into RecordIO\n",
    "[RecordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html) is a highly efficient binary data format from [MXNet](https://mxnet.incubator.apache.org/). Using this format, dataset is simple to prepare and transfer to the instance that will run the training job. Please refer to [object_detection_recordio_format](https://github.com/awslabs/amazon-sagemaker-examples/blob/80333fd4632cf6d924d0b91c33bf80da3bdcf926/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb) for more information about how to prepare RecordIO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tools/prepare_dataset.py --dataset pascal --year 2007,2012 --set trainval --target VOCdevkit/train.lst\n",
    "!python tools/prepare_dataset.py --dataset pascal --year 2007 --set test --target VOCdevkit/val.lst --no-shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "Upload the converted data to the S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the RecordIO files to train and validation channels\n",
    "sess.upload_data(path='VOCdevkit/train.rec', bucket=bucket, key_prefix=training_data_sub_folder)\n",
    "sess.upload_data(path='VOCdevkit/train.idx', bucket=bucket, key_prefix=training_data_sub_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to setup training and compilation output locations in S3, where the respective model artifacts will be dumped. We also setup the s3 location for training data and custom code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Location where the training data is stored in the previous step\n",
    "s3_training_data_location = 's3://{}/{}'.format(bucket, training_data_sub_folder)\n",
    "\n",
    "# S3 Location to save the model artifact after training\n",
    "s3_training_output_location = 's3://{}/{}'.format(bucket, training_output_sub_folder)\n",
    "\n",
    "# S3 Location to save the model artifact after compilation\n",
    "s3_compilation_output_location = 's3://{}/{}'.format(bucket, compilation_output_sub_folder)\n",
    "\n",
    "# S3 Location to save your custom code in tar.gz format\n",
    "s3_custom_code_upload_location = 's3://{}/{}'.format(bucket, custom_code_sub_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, we will create a [SageMaker MXNet estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/mxnet/sagemaker.mxnet.html#mxnet-estimator) object which allows us to run single machine or distributed training in SageMaker, using CPU or GPU-based instances. After creating the estimator, training is started by calling ``fit()`` on this estimator. When we create the estimator, we pass:\n",
    "- ``entry_point``: filename of the python script which defines training and hosting methods. Here we use `ssd_entry_point.py`\n",
    "- ``role``: name of our IAM execution role.\n",
    "- ``output_path``: S3 path where the training artifacts will be stored. We defined this in the previous step.\n",
    "- ``code_location``: S3 path where the custom code including the entry_point script will be stored. We defined this in the previous step.\n",
    "- ``instance_count`` & ``instance_type``: allows us to specify the number & type of SageMaker instances that will be used for the training job. For this example, we will choose one ``ml.p3.2xlarge`` instance.\n",
    "- ``framework_version`` & ``py_version``\n",
    "- ``distribution``: dict with information on how to run distributed training. Here we will use distributed training with parameter_server.\n",
    "- ``hyperparameters``: dict of values that will be passed to the entry_point script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "ssd_estimator = MXNet(entry_point='ssd_entry_point.py',\n",
    "                      role=role,\n",
    "                      output_path=s3_training_output_location,\n",
    "                      code_location=s3_custom_code_upload_location,\n",
    "                      instance_count=1,\n",
    "                      instance_type='ml.p3.2xlarge',\n",
    "                      framework_version='1.8.0',\n",
    "                      py_version='py37',\n",
    "                      distribution={'parameter_server': {'enabled': True}},\n",
    "                      hyperparameters={'epochs': 1,\n",
    "                                       'data-shape': 512,\n",
    "                                      }\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssd_estimator.fit({'train': s3_training_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the trained model using SageMaker Neo\n",
    "\n",
    "After training the model we can use SageMaker Neo's ``compile_model()`` API to compile the trained model. When calling ``compile_model()`` user is expected to provide all the correct input shapes required by the model for successful compilation. We also specify the target instance family, the name of our IAM execution role, S3 bucket to which the compiled model would be stored and we set ``MMS_DEFAULT_RESPONSE_TIMEOUT`` environment variable to 500. \n",
    "\n",
    "For this example, we will choose `ml_p3` as the target instance family while compiling the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "compiled_model = ssd_estimator.compile_model(target_instance_family='ml_p3', \n",
    "                                             input_shape={'data':[1, 3, 512, 512]},\n",
    "                                             role=role,\n",
    "                                             output_path=s3_compilation_output_location,\n",
    "                                             framework='mxnet', \n",
    "                                             framework_version='1.8',\n",
    "                                             env={'MMS_DEFAULT_RESPONSE_TIMEOUT': '500'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the compiled model and request Inferences\n",
    "\n",
    "We have to deploy the compiled model on one of the instance family for which the trained model was compiled for. Since we have compiled for `ml_p3` we can deploy to any `ml.p3` instance type. For this example we will choose `ml.p3.2xlarge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "neo_object_detector = compiled_model.deploy(initial_instance_count = 1, instance_type = 'ml.p3.2xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = neo_object_detector.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the detections.\n",
    "visualize_detection(test_file, response, object_categories, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as an optional clean-up job, you can delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Endpoint name: \" + neo_object_detector.endpoint_name)\n",
    "neo_object_detector.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
