{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engine for E-Commerce Sales\n",
    "This notebook gives an overview of techniques and services offer by SageMaker to build and deploy a personalized recommendation engine.\n",
    "\n",
    "## Dataset\n",
    "The dataset for this demo comes from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail). It contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. The following attributes are included in our dataset:\n",
    "+ InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "+ StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "+ Description: Product (item) name. Nominal.\n",
    "+ Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "+ InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "+ UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n",
    "+ CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "+ Country: Country name. Nominal, the name of the country where each customer resides. \n",
    "\n",
    "Citation: Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197â€“208, 2012 (Published online before print: 27 August 2012. doi: 10.1057/dbm.2012.17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sagemaker==2.21.0 boto3==1.16.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "input_dims                      -> 9284\n",
      "test_data_location              -> 's3://sagemaker-us-east-2-645431112437/personaliza\n",
      "train_data_location             -> 's3://sagemaker-us-east-2-645431112437/personaliza\n"
     ]
    }
   ],
   "source": [
    "# %store -z\n",
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.lineage import context, artifact, association, action\n",
    "import boto3\n",
    "\n",
    "from model_package_src.inference_specification import InferenceSpecification\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from scipy.sparse import csr_matrix, hstack, load_npz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sagemaker.__version__ >= '2.21.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-2\"\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = 'personalization'\n",
    "\n",
    "output_prefix  = f's3://{bucket}/{prefix}/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data For Modeling\n",
    "+ Split the data into training and testing sets\n",
    "+ Write the data to protobuf recordIO format for Pipe mode. [Read more](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html) about protobuf recordIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array\n",
    "X_train = load_npz('./data/X_train.npz')\n",
    "X_test = load_npz('./data/X_test.npz')\n",
    "y_train_npzfile = np.load('./data/y_train.npz')\n",
    "y_test_npzfile = np.load('./data/y_test.npz')\n",
    "y_train = y_train_npzfile.f.arr_0\n",
    "y_test = y_test_npzfile.f.arr_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((219519, 9284), (54880, 9284), (219519,), (54880,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape,y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'input_dims' (int)\n"
     ]
    }
   ],
   "source": [
    "input_dims = X_train.shape[1]\n",
    "%store input_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the factorization machine model\n",
    "Once we have the data preprocessed and available in the correct format for training, the next step is to actually train the model using the data. \n",
    "\n",
    "We'll use the Amazon SageMaker Python SDK to kick off training and monitor status until it is completed. In this example that takes only a few minutes. Despite the model only need 1-2 minutes to train, there is some extra time required upfront to provision hardware and load the algorithm container.\n",
    "\n",
    "First, let's specify our containers. To find the rigth container, we'll create a small lookup. More details on algorithm containers can be found in [AWS documentation.](https://docs-aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"factorization-machines\", region=boto_session.region_name)\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(container,\n",
    "                                   sagemaker_role, \n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.c5.xlarge',\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=sagemaker_session)\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=input_dims,\n",
    "                       predictor_type='regressor',\n",
    "                       mini_batch_size=1000,\n",
    "                       num_factors=64,\n",
    "                       epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 21:36:56 Starting - Starting the training job...\n",
      "2021-03-01 21:37:19 Starting - Launching requested ML instancesProfilerReport-1614634616: InProgress\n",
      "......\n",
      "2021-03-01 21:38:21 Starting - Preparing the instances for training......\n",
      "2021-03-01 21:39:21 Downloading - Downloading input data\n",
      "2021-03-01 21:39:21 Training - Downloading the training image.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:87: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:120: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '9284', 'predictor_type': 'regressor', 'num_factors': '64', 'epochs': '20', 'mini_batch_size': '1000'}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] Final configuration: {'epochs': '20', 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '9284', 'predictor_type': 'regressor', 'num_factors': '64'}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 WARNING 140624701892416] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] Using default worker.\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:30.105] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:30.111] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 99840}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] nvidia-smi took: 0.025166034698486328 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634770.1016335, \"EndTime\": 1614634770.1573327, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 47.62077331542969, \"count\": 1, \"min\": 47.62077331542969, \"max\": 47.62077331542969}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634770.1574454, \"EndTime\": 1614634770.157473, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[21:39:30] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.204326.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[21:39:30] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.204326.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=72.22661905419636\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=5216.6845\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:30 INFO 140624701892416] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=17.63289453125\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:32.852] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 2660, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:32 INFO 140624701892416] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=91.21242742344528\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:32 INFO 140624701892416] #quality_metric: host=algo-1, epoch=0, train mse <loss>=8319.706916477273\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:32 INFO 140624701892416] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=16.162756942471592\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634770.1574018, \"EndTime\": 1614634772.8525524, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"update.time\": {\"sum\": 2694.862127304077, \"count\": 1, \"min\": 2694.862127304077, \"max\": 2694.862127304077}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:32 INFO 140624701892416] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634770.1576657, \"EndTime\": 1614634772.852746, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 220519.0, \"count\": 1, \"min\": 220519, \"max\": 220519}, \"Total Batches Seen\": {\"sum\": 221.0, \"count\": 1, \"min\": 221, \"max\": 221}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:32 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=81449.29823998353 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:32 INFO 140624701892416] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=70.0600956322499\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:32 INFO 140624701892416] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=4908.417\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:32 INFO 140624701892416] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=17.41325390625\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:35.230] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 2375, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:35 INFO 140624701892416] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=90.60802512809579\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:35 INFO 140624701892416] #quality_metric: host=algo-1, epoch=1, train mse <loss>=8209.814217613637\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:35 INFO 140624701892416] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=18.42056477716619\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634772.8526192, \"EndTime\": 1614634775.231555, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2377.918481826782, \"count\": 1, \"min\": 2377.918481826782, \"max\": 2377.918481826782}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:35 INFO 140624701892416] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634772.853602, \"EndTime\": 1614634775.2318163, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 440038.0, \"count\": 1, \"min\": 440038, \"max\": 440038}, \"Total Batches Seen\": {\"sum\": 441.0, \"count\": 1, \"min\": 441, \"max\": 441}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:35 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=92300.07399969846 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:35 INFO 140624701892416] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=69.8893983376592\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:35 INFO 140624701892416] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=4884.528\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:35 INFO 140624701892416] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=18.55230078125\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:37.540] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 2306, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:37 INFO 140624701892416] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=90.47375148602242\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:37 INFO 140624701892416] #quality_metric: host=algo-1, epoch=2, train mse <loss>=8185.499707954546\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:37 INFO 140624701892416] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=18.580566246448864\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634775.2316153, \"EndTime\": 1614634777.5404866, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2307.6961040496826, \"count\": 1, \"min\": 2307.6961040496826, \"max\": 2307.6961040496826}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:37 INFO 140624701892416] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634775.2327683, \"EndTime\": 1614634777.5406227, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 659557.0, \"count\": 1, \"min\": 659557, \"max\": 659557}, \"Total Batches Seen\": {\"sum\": 661.0, \"count\": 1, \"min\": 661, \"max\": 661}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:37 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=95115.03466582757 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:37 INFO 140624701892416] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=69.72750533326142\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:37 INFO 140624701892416] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=4861.925\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:37 INFO 140624701892416] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=18.33094140625\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:39.913] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 2370, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:39 INFO 140624701892416] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=90.33190701046739\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:39 INFO 140624701892416] #quality_metric: host=algo-1, epoch=3, train mse <loss>=8159.853424147727\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:39 INFO 140624701892416] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=18.27153690962358\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634777.5405374, \"EndTime\": 1614634779.9137866, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2372.194766998291, \"count\": 1, \"min\": 2372.194766998291, \"max\": 2372.194766998291}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:39 INFO 140624701892416] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634777.5415664, \"EndTime\": 1614634779.9139848, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 879076.0, \"count\": 1, \"min\": 879076, \"max\": 879076}, \"Total Batches Seen\": {\"sum\": 881.0, \"count\": 1, \"min\": 881, \"max\": 881}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:39 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=92525.46552438667 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:39 INFO 140624701892416] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=69.57381332081778\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:39 INFO 140624701892416] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=4840.5155\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:39 INFO 140624701892416] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=18.07842578125\u001b[0m\n",
      "\n",
      "2021-03-01 21:39:41 Training - Training image download completed. Training in progress.\u001b[34m[2021-03-01 21:39:42.283] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 2367, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:42 INFO 140624701892416] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=90.2005097842629\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:42 INFO 140624701892416] #quality_metric: host=algo-1, epoch=4, train mse <loss>=8136.131965340909\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:42 INFO 140624701892416] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=18.01931514115767\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634779.9138517, \"EndTime\": 1614634782.2841408, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2369.23885345459, \"count\": 1, \"min\": 2369.23885345459, \"max\": 2369.23885345459}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:42 INFO 140624701892416] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634779.9148765, \"EndTime\": 1614634782.284339, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1098595.0, \"count\": 1, \"min\": 1098595, \"max\": 1098595}, \"Total Batches Seen\": {\"sum\": 1101.0, \"count\": 1, \"min\": 1101, \"max\": 1101}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:42 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=92640.88619450403 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:42 INFO 140624701892416] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=69.4391388195447\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:42 INFO 140624701892416] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=4821.794\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:42 INFO 140624701892416] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=17.880751953125\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:44.658] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 2371, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:44 INFO 140624701892416] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=90.08360983045266\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:44 INFO 140624701892416] #quality_metric: host=algo-1, epoch=5, train mse <loss>=8115.056760085227\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:44 INFO 140624701892416] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=17.845939994673294\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634782.2842014, \"EndTime\": 1614634784.6587517, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2373.4755516052246, \"count\": 1, \"min\": 2373.4755516052246, \"max\": 2373.4755516052246}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:44 INFO 140624701892416] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634782.2852511, \"EndTime\": 1614634784.6590147, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1318114.0, \"count\": 1, \"min\": 1318114, \"max\": 1318114}, \"Total Batches Seen\": {\"sum\": 1321.0, \"count\": 1, \"min\": 1321, \"max\": 1321}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:44 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=92473.52805377616 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:44 INFO 140624701892416] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=69.32097085298214\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:44 INFO 140624701892416] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=4805.397\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:44 INFO 140624701892416] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=17.763322265625\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:46.867] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 2205, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:46 INFO 140624701892416] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=89.97837475042141\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:46 INFO 140624701892416] #quality_metric: host=algo-1, epoch=6, train mse <loss>=8096.107922727273\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:46 INFO 140624701892416] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=17.719223974609374\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634784.658807, \"EndTime\": 1614634786.86757, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2207.5371742248535, \"count\": 1, \"min\": 2207.5371742248535, \"max\": 2207.5371742248535}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:46 INFO 140624701892416] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634784.6600094, \"EndTime\": 1614634786.8677115, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1537633.0, \"count\": 1, \"min\": 1537633, \"max\": 1537633}, \"Total Batches Seen\": {\"sum\": 1541.0, \"count\": 1, \"min\": 1541, \"max\": 1541}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:46 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=99429.72763971714 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:46 INFO 140624701892416] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=69.21636367218376\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:46 INFO 140624701892416] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=4790.905\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:46 INFO 140624701892416] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=17.67858984375\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:49.106] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 2236, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:49 INFO 140624701892416] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=89.88232149183378\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:49 INFO 140624701892416] #quality_metric: host=algo-1, epoch=7, train mse <loss>=8078.831716761364\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:49 INFO 140624701892416] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=17.620570072798294\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634786.8676226, \"EndTime\": 1614634789.107106, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2238.3315563201904, \"count\": 1, \"min\": 2238.3315563201904, \"max\": 2238.3315563201904}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:49 INFO 140624701892416] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634786.8687513, \"EndTime\": 1614634789.1072483, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1757152.0, \"count\": 1, \"min\": 1757152, \"max\": 1757152}, \"Total Batches Seen\": {\"sum\": 1761.0, \"count\": 1, \"min\": 1761, \"max\": 1761}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:49 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=98061.66598070781 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:49 INFO 140624701892416] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=69.12171511182285\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:49 INFO 140624701892416] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=4777.8115\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:49 INFO 140624701892416] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=17.6106328125\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:51.444] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 2334, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:51 INFO 140624701892416] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=89.79394862019834\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:51 INFO 140624701892416] #quality_metric: host=algo-1, epoch=8, train mse <loss>=8062.953208806818\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:51 INFO 140624701892416] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=17.54735018643466\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634789.1071603, \"EndTime\": 1614634791.4444652, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2336.158514022827, \"count\": 1, \"min\": 2336.158514022827, \"max\": 2336.158514022827}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:51 INFO 140624701892416] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634789.108283, \"EndTime\": 1614634791.4446044, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1976671.0, \"count\": 1, \"min\": 1976671, \"max\": 1976671}, \"Total Batches Seen\": {\"sum\": 1981.0, \"count\": 1, \"min\": 1981, \"max\": 1981}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:51 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=93955.73025168657 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:51 INFO 140624701892416] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=69.03420166844838\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:51 INFO 140624701892416] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=4765.721\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:51 INFO 140624701892416] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=17.550154296875\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:53.763] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 2316, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:53 INFO 140624701892416] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=89.71251026638066\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:53 INFO 140624701892416] #quality_metric: host=algo-1, epoch=9, train mse <loss>=8048.334498295455\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:53 INFO 140624701892416] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=17.494896257990057\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634791.4445183, \"EndTime\": 1614634793.763948, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2318.2873725891113, \"count\": 1, \"min\": 2318.2873725891113, \"max\": 2318.2873725891113}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:53 INFO 140624701892416] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634791.445636, \"EndTime\": 1614634793.7641094, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2196190.0, \"count\": 1, \"min\": 2196190, \"max\": 2196190}, \"Total Batches Seen\": {\"sum\": 2201.0, \"count\": 1, \"min\": 2201, \"max\": 2201}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:53 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=94678.94198413656 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:53 INFO 140624701892416] #quality_metric: host=algo-1, epoch=10, batch=0 train rmse <loss>=68.95210294109962\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:53 INFO 140624701892416] #quality_metric: host=algo-1, epoch=10, batch=0 train mse <loss>=4754.3925\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:53 INFO 140624701892416] #quality_metric: host=algo-1, epoch=10, batch=0 train absolute_loss <loss>=17.490455078125\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:56.090] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 2323, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:56 INFO 140624701892416] #quality_metric: host=algo-1, epoch=10, train rmse <loss>=89.63715438616323\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:56 INFO 140624701892416] #quality_metric: host=algo-1, epoch=10, train mse <loss>=8034.819446448863\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:56 INFO 140624701892416] #quality_metric: host=algo-1, epoch=10, train absolute_loss <loss>=17.45398817471591\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634793.7640004, \"EndTime\": 1614634796.0906622, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2325.4711627960205, \"count\": 1, \"min\": 2325.4711627960205, \"max\": 2325.4711627960205}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:56 INFO 140624701892416] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634793.7651677, \"EndTime\": 1614634796.0908437, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2415709.0, \"count\": 1, \"min\": 2415709, \"max\": 2415709}, \"Total Batches Seen\": {\"sum\": 2421.0, \"count\": 1, \"min\": 2421, \"max\": 2421}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:56 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=94385.90636749809 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:56 INFO 140624701892416] #quality_metric: host=algo-1, epoch=11, batch=0 train rmse <loss>=68.87503901995265\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:56 INFO 140624701892416] #quality_metric: host=algo-1, epoch=11, batch=0 train mse <loss>=4743.771\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:56 INFO 140624701892416] #quality_metric: host=algo-1, epoch=11, batch=0 train absolute_loss <loss>=17.436017578125\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:39:58.357] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 2264, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:58 INFO 140624701892416] #quality_metric: host=algo-1, epoch=11, train rmse <loss>=89.56761664950523\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:58 INFO 140624701892416] #quality_metric: host=algo-1, epoch=11, train mse <loss>=8022.357952272727\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:58 INFO 140624701892416] #quality_metric: host=algo-1, epoch=11, train absolute_loss <loss>=17.42097587890625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634796.090717, \"EndTime\": 1614634798.3580625, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2266.273260116577, \"count\": 1, \"min\": 2266.273260116577, \"max\": 2266.273260116577}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:58 INFO 140624701892416] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634796.0917637, \"EndTime\": 1614634798.358345, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2635228.0, \"count\": 1, \"min\": 2635228, \"max\": 2635228}, \"Total Batches Seen\": {\"sum\": 2641.0, \"count\": 1, \"min\": 2641, \"max\": 2641}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:58 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=96845.59965895282 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:58 INFO 140624701892416] #quality_metric: host=algo-1, epoch=12, batch=0 train rmse <loss>=68.80243091635644\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:58 INFO 140624701892416] #quality_metric: host=algo-1, epoch=12, batch=0 train mse <loss>=4733.7745\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:39:58 INFO 140624701892416] #quality_metric: host=algo-1, epoch=12, batch=0 train absolute_loss <loss>=17.39027734375\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:40:00.800] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 2440, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:00 INFO 140624701892416] #quality_metric: host=algo-1, epoch=12, train rmse <loss>=89.502190183175\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:00 INFO 140624701892416] #quality_metric: host=algo-1, epoch=12, train mse <loss>=8010.642047585227\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:00 INFO 140624701892416] #quality_metric: host=algo-1, epoch=12, train absolute_loss <loss>=17.38904547674006\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634798.3581343, \"EndTime\": 1614634800.8012183, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2441.8938159942627, \"count\": 1, \"min\": 2441.8938159942627, \"max\": 2441.8938159942627}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:00 INFO 140624701892416] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634798.3593023, \"EndTime\": 1614634800.8013601, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2854747.0, \"count\": 1, \"min\": 2854747, \"max\": 2854747}, \"Total Batches Seen\": {\"sum\": 2861.0, \"count\": 1, \"min\": 2861, \"max\": 2861}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:00 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=89888.20028095145 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:00 INFO 140624701892416] #quality_metric: host=algo-1, epoch=13, batch=0 train rmse <loss>=68.73474376179779\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:00 INFO 140624701892416] #quality_metric: host=algo-1, epoch=13, batch=0 train mse <loss>=4724.465\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:00 INFO 140624701892416] #quality_metric: host=algo-1, epoch=13, batch=0 train absolute_loss <loss>=17.34413671875\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:40:03.148] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 2344, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:03 INFO 140624701892416] #quality_metric: host=algo-1, epoch=13, train rmse <loss>=89.44082671374765\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:03 INFO 140624701892416] #quality_metric: host=algo-1, epoch=13, train mse <loss>=7999.661483238637\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:03 INFO 140624701892416] #quality_metric: host=algo-1, epoch=13, train absolute_loss <loss>=17.362610808771308\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634800.8012712, \"EndTime\": 1614634803.1489902, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2346.5237617492676, \"count\": 1, \"min\": 2346.5237617492676, \"max\": 2346.5237617492676}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:03 INFO 140624701892416] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634800.802444, \"EndTime\": 1614634803.1491597, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3074266.0, \"count\": 1, \"min\": 3074266, \"max\": 3074266}, \"Total Batches Seen\": {\"sum\": 3081.0, \"count\": 1, \"min\": 3081, \"max\": 3081}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:03 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=93539.46303297732 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:03 INFO 140624701892416] #quality_metric: host=algo-1, epoch=14, batch=0 train rmse <loss>=68.67233795350207\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:03 INFO 140624701892416] #quality_metric: host=algo-1, epoch=14, batch=0 train mse <loss>=4715.89\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:03 INFO 140624701892416] #quality_metric: host=algo-1, epoch=14, batch=0 train absolute_loss <loss>=17.292935546875\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:40:06.039] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 2888, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:06 INFO 140624701892416] #quality_metric: host=algo-1, epoch=14, train rmse <loss>=89.38316585868087\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:06 INFO 140624701892416] #quality_metric: host=algo-1, epoch=14, train mse <loss>=7989.350338920454\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:06 INFO 140624701892416] #quality_metric: host=algo-1, epoch=14, train absolute_loss <loss>=17.333876198508523\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634803.1490428, \"EndTime\": 1614634806.0405354, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2890.2058601379395, \"count\": 1, \"min\": 2890.2058601379395, \"max\": 2890.2058601379395}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:06 INFO 140624701892416] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634803.1503053, \"EndTime\": 1614634806.0406723, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3293785.0, \"count\": 1, \"min\": 3293785, \"max\": 3293785}, \"Total Batches Seen\": {\"sum\": 3301.0, \"count\": 1, \"min\": 3301, \"max\": 3301}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:06 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=75946.3810916628 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:06 INFO 140624701892416] #quality_metric: host=algo-1, epoch=15, batch=0 train rmse <loss>=68.61341705526696\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:06 INFO 140624701892416] #quality_metric: host=algo-1, epoch=15, batch=0 train mse <loss>=4707.801\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:06 INFO 140624701892416] #quality_metric: host=algo-1, epoch=15, batch=0 train absolute_loss <loss>=17.2486875\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:40:08.338] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 2295, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:08 INFO 140624701892416] #quality_metric: host=algo-1, epoch=15, train rmse <loss>=89.32885107497465\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:08 INFO 140624701892416] #quality_metric: host=algo-1, epoch=15, train mse <loss>=7979.643634375\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:08 INFO 140624701892416] #quality_metric: host=algo-1, epoch=15, train absolute_loss <loss>=17.30803719815341\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634806.0405865, \"EndTime\": 1614634808.339467, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2297.6326942443848, \"count\": 1, \"min\": 2297.6326942443848, \"max\": 2297.6326942443848}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:08 INFO 140624701892416] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634806.0418117, \"EndTime\": 1614634808.339609, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3513304.0, \"count\": 1, \"min\": 3513304, \"max\": 3513304}, \"Total Batches Seen\": {\"sum\": 3521.0, \"count\": 1, \"min\": 3521, \"max\": 3521}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:08 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=95531.25955149782 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:08 INFO 140624701892416] #quality_metric: host=algo-1, epoch=16, batch=0 train rmse <loss>=68.55855161830652\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:08 INFO 140624701892416] #quality_metric: host=algo-1, epoch=16, batch=0 train mse <loss>=4700.275\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:08 INFO 140624701892416] #quality_metric: host=algo-1, epoch=16, batch=0 train absolute_loss <loss>=17.207400390625\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:40:10.605] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 2263, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:10 INFO 140624701892416] #quality_metric: host=algo-1, epoch=16, train rmse <loss>=89.2773943729471\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:10 INFO 140624701892416] #quality_metric: host=algo-1, epoch=16, train mse <loss>=7970.453146022727\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:10 INFO 140624701892416] #quality_metric: host=algo-1, epoch=16, train absolute_loss <loss>=17.283455996981534\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634808.3395202, \"EndTime\": 1614634810.6058848, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2265.0609016418457, \"count\": 1, \"min\": 2265.0609016418457, \"max\": 2265.0609016418457}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:10 INFO 140624701892416] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634808.3408003, \"EndTime\": 1614634810.606014, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3732823.0, \"count\": 1, \"min\": 3732823, \"max\": 3732823}, \"Total Batches Seen\": {\"sum\": 3741.0, \"count\": 1, \"min\": 3741, \"max\": 3741}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:10 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=96905.42153890197 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:10 INFO 140624701892416] #quality_metric: host=algo-1, epoch=17, batch=0 train rmse <loss>=68.50716750822501\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:10 INFO 140624701892416] #quality_metric: host=algo-1, epoch=17, batch=0 train mse <loss>=4693.232\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:10 INFO 140624701892416] #quality_metric: host=algo-1, epoch=17, batch=0 train absolute_loss <loss>=17.1710078125\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:40:12.891] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 2282, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:12 INFO 140624701892416] #quality_metric: host=algo-1, epoch=17, train rmse <loss>=89.22836755916096\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:12 INFO 140624701892416] #quality_metric: host=algo-1, epoch=17, train mse <loss>=7961.701577272727\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:12 INFO 140624701892416] #quality_metric: host=algo-1, epoch=17, train absolute_loss <loss>=17.25918828568892\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634810.6059291, \"EndTime\": 1614634812.8918672, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2284.83247756958, \"count\": 1, \"min\": 2284.83247756958, \"max\": 2284.83247756958}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:12 INFO 140624701892416] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634810.6070096, \"EndTime\": 1614634812.8920078, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3952342.0, \"count\": 1, \"min\": 3952342, \"max\": 3952342}, \"Total Batches Seen\": {\"sum\": 3961.0, \"count\": 1, \"min\": 3961, \"max\": 3961}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:12 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=96064.5889586324 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:12 INFO 140624701892416] #quality_metric: host=algo-1, epoch=18, batch=0 train rmse <loss>=68.45944785053412\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:12 INFO 140624701892416] #quality_metric: host=algo-1, epoch=18, batch=0 train mse <loss>=4686.696\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:12 INFO 140624701892416] #quality_metric: host=algo-1, epoch=18, batch=0 train absolute_loss <loss>=17.1378046875\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:40:15.127] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 2233, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:15 INFO 140624701892416] #quality_metric: host=algo-1, epoch=18, train rmse <loss>=89.18173190559723\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:15 INFO 140624701892416] #quality_metric: host=algo-1, epoch=18, train mse <loss>=7953.381305681818\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:15 INFO 140624701892416] #quality_metric: host=algo-1, epoch=18, train absolute_loss <loss>=17.23677677112926\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634812.8919203, \"EndTime\": 1614634815.128224, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2234.917640686035, \"count\": 1, \"min\": 2234.917640686035, \"max\": 2234.917640686035}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:15 INFO 140624701892416] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634812.8932798, \"EndTime\": 1614634815.1284282, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4171861.0, \"count\": 1, \"min\": 4171861, \"max\": 4171861}, \"Total Batches Seen\": {\"sum\": 4181.0, \"count\": 1, \"min\": 4181, \"max\": 4181}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:15 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=98208.18400726507 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:15 INFO 140624701892416] #quality_metric: host=algo-1, epoch=19, batch=0 train rmse <loss>=68.41493988888685\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:15 INFO 140624701892416] #quality_metric: host=algo-1, epoch=19, batch=0 train mse <loss>=4680.604\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:15 INFO 140624701892416] #quality_metric: host=algo-1, epoch=19, batch=0 train absolute_loss <loss>=17.10520703125\u001b[0m\n",
      "\n",
      "2021-03-01 21:40:21 Uploading - Uploading generated training model\u001b[34m[2021-03-01 21:40:17.503] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 2372, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #quality_metric: host=algo-1, epoch=19, train rmse <loss>=89.13712189336056\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #quality_metric: host=algo-1, epoch=19, train mse <loss>=7945.426499431818\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #quality_metric: host=algo-1, epoch=19, train absolute_loss <loss>=17.212976438210227\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #quality_metric: host=algo-1, train rmse <loss>=89.13712189336056\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #quality_metric: host=algo-1, train mse <loss>=7945.426499431818\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #quality_metric: host=algo-1, train absolute_loss <loss>=17.212976438210227\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634815.128288, \"EndTime\": 1614634817.5039854, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2374.480724334717, \"count\": 1, \"min\": 2374.480724334717, \"max\": 2374.480724334717}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634815.1294842, \"EndTime\": 1614634817.5041585, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4391380.0, \"count\": 1, \"min\": 4391380, \"max\": 4391380}, \"Total Batches Seen\": {\"sum\": 4401.0, \"count\": 1, \"min\": 4401, \"max\": 4401}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #throughput_metric: host=algo-1, train throughput=92438.22085111527 records/second\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 WARNING 140624701892416] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634817.504038, \"EndTime\": 1614634817.506973, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 2.5970935821533203, \"count\": 1, \"min\": 2.5970935821533203, \"max\": 2.5970935821533203}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] Saved checkpoint to \"/tmp/tmptq9rbgos/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:40:17.519] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 47414, \"num_examples\": 1, \"num_bytes\": 99820}\u001b[0m\n",
      "\u001b[34m[2021-03-01 21:40:17.724] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 204, \"num_examples\": 55, \"num_bytes\": 5462876}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634817.5193787, \"EndTime\": 1614634817.724915, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Total Batches Seen\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}, \"Max Records Seen Between Resets\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Max Batches Seen Between Resets\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Number of Batches Since Last Reset\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #test_score (algo-1) : ('rmse', 68.74431145296938)\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #test_score (algo-1) : ('mse', 4725.780357142857)\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #test_score (algo-1) : ('absolute_loss', 17.043618124914587)\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #quality_metric: host=algo-1, test rmse <loss>=68.74431145296938\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #quality_metric: host=algo-1, test mse <loss>=4725.780357142857\u001b[0m\n",
      "\u001b[34m[03/01/2021 21:40:17 INFO 140624701892416] #quality_metric: host=algo-1, test absolute_loss <loss>=17.043618124914587\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1614634817.5070434, \"EndTime\": 1614634817.725863, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 16.52979850769043, \"count\": 1, \"min\": 16.52979850769043, \"max\": 16.52979850769043}, \"totaltime\": {\"sum\": 47645.47252655029, \"count\": 1, \"min\": 47645.47252655029, \"max\": 47645.47252655029}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-03-01 21:40:42 Completed - Training job completed\n",
      "Training seconds: 80\n",
      "Billable seconds: 80\n",
      "Stored 'training_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "if 'training_job_name' not in locals():\n",
    "    \n",
    "    fm.fit({'train': train_data_location, 'test': test_data_location})\n",
    "    training_job_name = fm.latest_training_job.job_name\n",
    "    %store training_job_name\n",
    "    \n",
    "else:\n",
    "    print(f'Using previous training job: {training_job_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_info = sagemaker_boto_client.describe_training_job(TrainingJobName=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:us-east-2:645431112437:artifact/cdd7fbecb4eefa22c43b2ad48140acc2\n"
     ]
    }
   ],
   "source": [
    "training_data_s3_uri = training_job_info['InputDataConfig'][0]['DataSource']['S3DataSource']['S3Uri']\n",
    "\n",
    "matching_artifacts = list(artifact.Artifact.list(\n",
    "    source_uri=training_data_s3_uri,\n",
    "    sagemaker_session=sagemaker_session))\n",
    "\n",
    "if matching_artifacts:\n",
    "    training_data_artifact = matching_artifacts[0]\n",
    "    print(f'Using existing artifact: {training_data_artifact.artifact_arn}')\n",
    "else:\n",
    "    training_data_artifact = artifact.Artifact.create(\n",
    "        artifact_name='TrainingData',\n",
    "        source_uri=training_data_s3_uri,\n",
    "        artifact_type='Dataset',\n",
    "        sagemaker_session=sagemaker_session)\n",
    "    print(f'Create artifact {training_data_artifact.artifact_arn}: SUCCESSFUL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Artifact\n",
    "We do not need a code artifact because we are using a built-in SageMaker Algorithm called Factorization Machines. The Factorization Machines container contains all of the code and, by default, our model training stores the Factorization Machines image for tracking purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:us-east-2:645431112437:artifact/3acde2fc029adeff9c767be68feac3a7\n"
     ]
    }
   ],
   "source": [
    "trained_model_s3_uri = training_job_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "matching_artifacts = list(artifact.Artifact.list(\n",
    "    source_uri=trained_model_s3_uri,\n",
    "    sagemaker_session=sagemaker_session))\n",
    "\n",
    "if matching_artifacts:\n",
    "    model_artifact = matching_artifacts[0]\n",
    "    print(f'Using existing artifact: {model_artifact.artifact_arn}')\n",
    "else:\n",
    "    model_artifact = artifact.Artifact.create(\n",
    "        artifact_name='TrainedModel',\n",
    "        source_uri=trained_model_s3_uri,\n",
    "        artifact_type='Model',\n",
    "        sagemaker_session=sagemaker_session)\n",
    "    print(f'Create artifact {model_artifact.artifact_arn}: SUCCESSFUL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set artifact associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component = sagemaker_boto_client.describe_trial_component(TrialComponentName=training_job_name+'-aws-training-job')\n",
    "trial_component_arn = trial_component['TrialComponentArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association already exists with DataSet\n",
      "Association with Model: SUCCEESFUL\n"
     ]
    }
   ],
   "source": [
    "artifact_list = [\n",
    "                 [training_data_artifact, 'ContributedTo'],\n",
    "                 [model_artifact, 'Produced']\n",
    "]\n",
    "\n",
    "for art, assoc in artifact_list:\n",
    "    try:\n",
    "        association.Association.create(\n",
    "            source_arn=art.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type=assoc,\n",
    "            sagemaker_session=sagemaker_session)\n",
    "        print(f\"Association with {art.artifact_type}: SUCCEESFUL\")\n",
    "    except:\n",
    "        print(f\"Association already exists with {art.artifact_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model retail-recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: retail-recommendations\n"
     ]
    }
   ],
   "source": [
    "model_name = 'retail-recommendations'\n",
    "model_matches = sagemaker_boto_client.list_models(NameContains=model_name)['Models']\n",
    "\n",
    "if not model_matches:\n",
    "    print(f'Creating model {model_name}')\n",
    "    model = sagemaker_session.create_model_from_job(\n",
    "        name=model_name,\n",
    "        training_job_name=training_job_info['TrainingJobName'],\n",
    "        role=sagemaker_role,\n",
    "        image_uri=training_job_info['AlgorithmSpecification']['TrainingImage'])\n",
    "else:\n",
    "    print(f\"Model {model_name} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model Registry\n",
    "\n",
    "Once a useful model has been trained and its artifacts properly associated, the next step is to register the model for future reference and possible deployment.\n",
    "\n",
    "### Create Model Package Group\n",
    "\n",
    "A Model Package Groups holds multiple versions or iterations of a model. Though it is not required to create them for every model in the registry, they help organize various models which all have the same purpose and provide autiomatic versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mpg_name' (str)\n",
      "Model Package Group name: retail-recommendation-2021-03-01-21-41\n"
     ]
    }
   ],
   "source": [
    "if 'mpg_name' not in locals():\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "    mpg_name = f'retail-recommendation-{timestamp}'\n",
    "    %store mpg_name\n",
    "\n",
    "print(f'Model Package Group name: {mpg_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_input_dict = {\n",
    "    'ModelPackageGroupName': mpg_name,\n",
    "    'ModelPackageGroupDescription': 'Recommendation for Online Retail Sales'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model Package Group retail-recommendation-2021-03-01-21-41: SUCCESSFUL\n"
     ]
    }
   ],
   "source": [
    "matching_mpg = sagemaker_boto_client.list_model_package_groups(NameContains=mpg_name)['ModelPackageGroupSummaryList']\n",
    "\n",
    "if matching_mpg:\n",
    "    print(f'Using existing Model Package Group: {mpg_name}')\n",
    "else:\n",
    "    mpg_response = sagemaker_boto_client.create_model_package_group(**mpg_input_dict)\n",
    "    print(f'Create Model Package Group {mpg_name}: SUCCESSFUL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_report = {\n",
    "    'regression_metrics': {}\n",
    "}\n",
    "\n",
    "for metric in training_job_info['FinalMetricDataList']:\n",
    "    stat = {\n",
    "        metric['MetricName']: {\n",
    "            'value': metric['Value']\n",
    "        }\n",
    "    }\n",
    "    model_metrics_report['regression_metrics'].update(stat)\n",
    "    \n",
    "with open('training_metrics.json', 'w') as f:\n",
    "    json.dump(model_metrics_report, f)\n",
    "    \n",
    "metrics_s3_key = f\"training_jobs/{training_job_info['TrainingJobName']}/training_metrics.json\"\n",
    "s3_client.upload_file(Filename='training_metrics.json', Bucket=bucket, Key=metrics_s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the inference spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_inference_spec = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=training_job_info['AlgorithmSpecification']['TrainingImage'],\n",
    "    supports_gpu=False,\n",
    "    supported_content_types=['application/x-recordio-protobuf', 'application/json'],\n",
    "    supported_mime_types=['text/csv'])\n",
    "\n",
    "mp_inference_spec['InferenceSpecification']['Containers'][0]['ModelDataUrl'] = training_job_info['ModelArtifacts']['S3ModelArtifacts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model metrics\n",
    "Metrics other than model quality can be defined. See the Boto3 documentation for [creating a model package](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model_package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {\n",
    "    'ModelQuality': {\n",
    "        'Statistics': {\n",
    "            'ContentType': 'application/json',\n",
    "            'S3Uri': f's3://{bucket}/{metrics_s3_key}'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_input_dict = {\n",
    "    'ModelPackageGroupName': mpg_name,\n",
    "    'ModelPackageDescription': 'Factorization Machine Model to create personalized retail recommendations',\n",
    "    'ModelApprovalStatus': 'PendingManualApproval',\n",
    "    'ModelMetrics': model_metrics\n",
    "}\n",
    "\n",
    "mp_input_dict.update(mp_inference_spec)\n",
    "mp_response = sagemaker_boto_client.create_model_package(**mp_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait until model package is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model package status: Completed\n"
     ]
    }
   ],
   "source": [
    "mp_info = sagemaker_boto_client.describe_model_package(ModelPackageName=mp_response['ModelPackageArn'])\n",
    "mp_status = mp_info['ModelPackageStatus']\n",
    "\n",
    "while mp_status not in ['Completed', 'Failed']:\n",
    "    time.sleep(5)\n",
    "    mp_info = sagemaker_boto_client.describe_model_package(ModelPackageName=mp_response['ModelPackageArn'])\n",
    "    mp_status = mp_info['ModelPackageStatus']\n",
    "    print(f'model package status: {mp_status}')\n",
    "print(f'model package status: {mp_status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package = sagemaker_boto_client.list_model_packages(ModelPackageGroupName=mpg_name)['ModelPackageSummaryList'][0]\n",
    "model_package_update = {\n",
    "    'ModelPackageArn': model_package['ModelPackageArn'],\n",
    "    'ModelApprovalStatus': 'Approved'\n",
    "}\n",
    "\n",
    "update_response = sagemaker_boto_client.update_model_package(**model_package_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...1-03-01-21-36-56-437/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...12437/personalization/test/test.protobuf</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...437/personalization/train/train.protobuf</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40461...2.amazonaws.com/factorization-machines:1</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...1-03-01-21-36-56-437/output/model.tar.gz</td>\n",
       "      <td>Output</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...1-03-01-21-36-56-437/output/model.tar.gz     Input    Model   \n",
       "1  s3://...12437/personalization/test/test.protobuf     Input  DataSet   \n",
       "2  s3://...437/personalization/train/train.protobuf     Input  DataSet   \n",
       "3  40461...2.amazonaws.com/factorization-machines:1     Input    Image   \n",
       "4  s3://...1-03-01-21-36-56-437/output/model.tar.gz    Output    Model   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0         Produced     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker_session)\n",
    "display(viz.show(training_job_name=training_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint. This will allow out to make predictions (or inference) from the model dyanamically.\n",
    "\n",
    "Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, Amazon Redshift, Amazon Athena, or other deployment target.\n",
    "\n",
    "Here we will take the top customer, the customer who spent the most money, and try to find which items to recommend to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "class FMSerializer(JSONSerializer):\n",
    "    def serialize(self, data):\n",
    "        js = {'instances': []}\n",
    "        for row in data:\n",
    "              js['instances'].append({'features': row.tolist()})\n",
    "        return json.dumps(js)\n",
    "\n",
    "fm_predictor = fm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    serializer=FMSerializer(),\n",
    "    deserializer= JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find customer who spent the most money \n",
    "df = pd.read_csv('data/online_retail_preprocessed.csv')\n",
    "\n",
    "df['invoice_amount'] = df['Quantity'] * df['UnitPrice']\n",
    "top_customer = df.groupby('CustomerID').sum()['invoice_amount'].sort_values(ascending=False).index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(df, customer_id, n_recommendations, n_ranks = 100):\n",
    "    popular_items = df.groupby(['StockCode', 'UnitPrice']).nunique()['CustomerID'].sort_values(ascending=False).reset_index()\n",
    "    top_n_items = popular_items['StockCode'].iloc[:n_ranks].values\n",
    "    top_n_prices = popular_items['UnitPrice'].iloc[:n_ranks].values\n",
    "    \n",
    "    # stock codes can have multiple descriptions, so we will choose whichever description is most common\n",
    "    item_map = df.groupby('StockCode').agg(lambda x: x.value_counts().index[0])['Description']\n",
    "    \n",
    "    # find customer's country\n",
    "    df_subset = df.loc[df['CustomerID'] == customer_id]\n",
    "    country = df_subset['Country'].value_counts().index[0]\n",
    "\n",
    "    data = {'StockCode': top_n_items,\n",
    "            'Description': [item_map[i] for i in top_n_items],\n",
    "            'CustomerID': customer_id,\n",
    "            'Country': country,\n",
    "            'UnitPrice': top_n_prices\n",
    "           }\n",
    "\n",
    "    df_inference = pd.DataFrame(data)\n",
    "    \n",
    "    # we need to build the data set similar to how we built it for training\n",
    "    # it should have the same number of features as the training data\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    onehot_cols = ['StockCode', 'CustomerID', 'Country']\n",
    "    enc.fit(df[onehot_cols])\n",
    "    onehot_output = enc.transform(df_inference[onehot_cols])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=2)\n",
    "    unique_descriptions = df['Description'].unique()\n",
    "    vectorizer.fit(unique_descriptions)\n",
    "    tfidf_output = vectorizer.transform(df_inference['Description'])\n",
    "\n",
    "    row = range(len(df_inference))\n",
    "    col = [0] * len(df_inference)\n",
    "    unit_price = csr_matrix((df_inference['UnitPrice'].values, (row, col)), dtype='float32')\n",
    "\n",
    "    X_inference = hstack([onehot_output, tfidf_output, unit_price], format='csr')\n",
    "    \n",
    "    result = fm_predictor.predict(X_inference.toarray())\n",
    "    preds = [i['score'] for i in result['predictions']]\n",
    "    index_array = np.array(preds).argsort()\n",
    "    items = enc.inverse_transform(onehot_output)[:,0]\n",
    "    top_recs = np.take_along_axis(items, index_array, axis=0)[:-n_recommendations-1:-1]\n",
    "    recommendations = [[i, item_map[i]] for i in top_recs]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended products:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['22423', 'REGENCY CAKESTAND 3 TIER'],\n",
       " ['22776', 'SWEETHEART CAKESTAND 3 TIER'],\n",
       " ['22624', 'IVORY KITCHEN SCALES'],\n",
       " ['85123A', 'WHITE HANGING HEART T-LIGHT HOLDER'],\n",
       " ['85099B', 'JUMBO BAG RED RETROSPOT']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Top 5 recommended products:')\n",
    "get_recommendations(df, top_customer, n_recommendations=5, n_ranks=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
