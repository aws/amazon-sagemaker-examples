{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615b38ad-e8e5-49f4-a66a-036534c62798",
   "metadata": {},
   "source": [
    "# SageMaker Real-time Dynamic Batching Inference with Torchserve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1647aa0-0140-40fc-bf58-bd8cf786d7a4",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of dynamic batching on SageMaker with [torchserve](https://github.com/pytorch/serve/) as a model server. It demonstrates the following\n",
    "1. Batch inference using DLC i.e. SageMaker's default backend container. This is done by using SageMaker python sdk in script-mode.\n",
    "2. Specifying inference parameters for torchserve using environment variables.\n",
    "3. Option to use a custom container with config file for torchserve baked-in the container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc0e4a-ebb3-4b13-b4a1-e18f3307bc0a",
   "metadata": {},
   "source": [
    "**Installs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33d9b7-101f-4684-8820-e2d19a4afbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-model-archiver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7434c-2d73-41dc-a56c-7db10b9f552f",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce290e30-dadc-4fa8-bd15-7db4bafe4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import boto3, time, json\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d5ef87-09c4-44ac-9dc4-748cc90960e4",
   "metadata": {},
   "source": [
    "**Initiate session and retrieve region, account details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ea986-41bc-413b-88e6-470f7258d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227d6a8-3db5-4938-86db-59f2deec2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "region = sess.region_name\n",
    "account = boto3.client(\"sts\").get_caller_identity().get(\"Account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839ef82-28d3-4032-adc3-005ac140fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sm_sess.default_bucket()\n",
    "prefix = \"ts-dynamic-batching\"\n",
    "model_name = \"BERTSeqClassification\"\n",
    "mar_file = f\"{model_name}.mar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbed613-c3cd-4d10-95a6-93eecc4a7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = f\"s3://{bucket}/{prefix}/models/{model_name}.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735eefe5-3844-4f5a-b840-869c17c707cb",
   "metadata": {},
   "source": [
    "## Build a Custom Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55f197-8a28-4bcb-bcc5-80cc7f73cbc2",
   "metadata": {},
   "source": [
    "#### This approach uses a custom model config written to config.properties built-in with the container. This model config includes the batch_size, max_batch_delay and other properties to set the batching for the model\n",
    "### Refer docker/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f690e0d-a641-4751-b0fc-090a47d850dc",
   "metadata": {},
   "source": [
    "#### The following script builds a container and pushes it to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493f7cd-5b73-43e6-a974-ec9151337bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "container_name=custom-dynamic-torchserve\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${container_name}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${container_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${container_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "docker build --no-cache -t ${container_name} docker/\n",
    "docker tag ${container_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50bc873-b0aa-4269-9aad-2f08ef4fd976",
   "metadata": {},
   "source": [
    "**Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d678de3-52f8-4263-bdb4-96fa8e59e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://torchserve/mar_files/{mar_file} .\n",
    "!tar -cvzf {model_name}.tar.gz {mar_file}\n",
    "!aws s3 cp {model_name}.tar.gz s3://{bucket}/{prefix}/models/\n",
    "!rm {mar_file} {model_name}.tar.gz\n",
    "\n",
    "f\"s3://{bucket}/{prefix}/models/{model_name}.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a79b1-c9ab-484e-b6fc-7f552c43f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = \"custom-dynamic-torchserve\"\n",
    "image_uri = f\"{account}.dkr.ecr.{region}.amazonaws.com/{container_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa1f4c-a4f5-4ec8-a3d1-5a4ecde1ab9e",
   "metadata": {},
   "source": [
    "#### Create SageMaker model, deploy and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca6130-4e34-4c51-a1a9-2d4af8467d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "pytorch_model = Model(\n",
    "    model_data=model_artifact,\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    predictor_cls=Predictor,\n",
    ")\n",
    "\n",
    "endpoint_name = 'torchserve-endpoint-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# Change the instance type as necessary, or use 'local' for executing in Sagemaker local mode\n",
    "instance_type = \"ml.c5.9xlarge\"\n",
    "# instance_type = \"local\"\n",
    "\n",
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.BytesDeserializer(),\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "# Wait for model to load in case of local mode\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f396b2e-134a-45dc-a6eb-4dae91acc7c1",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1721c0-5215-4399-9068-26f5ea51f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "def invoke(num_request):\n",
    "    return predictor.predict(\n",
    "        data=\"{Bloomberg has decided to publish a new report on global economic situation.}\"\n",
    "    )\n",
    "\n",
    "pool = multiprocessing.Pool(3)\n",
    "results = pool.map(invoke, range(10))\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669e317-0f7c-4c93-9a89-b2e0f43afd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "predictor.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8119e33-fed7-4685-aa4c-caa468262e13",
   "metadata": {},
   "source": [
    "## Use AWS Deep Learning Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6fa176-5c10-44ee-be70-235a310ef00b",
   "metadata": {},
   "source": [
    "#### The AWS DLCs use sagemaker-pytorch-inference-toolkit to set-up and start the model server. Currently, the model-artifacts need to be archived into a *.tar.gz along with a manifest (model metadata) as required by TorchServe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fd0a9-c5da-4282-84d6-650b18162325",
   "metadata": {},
   "source": [
    "**Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46fe0f-0879-4adb-ad4b-0db704bf35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://torchserve/mar_files/{mar_file} .\n",
    "!unzip {mar_file}\n",
    "# Use torch-model-archiver (following command can be used as a reference when using custom models and handlers). Note using option 'no-archive' only generates\n",
    "# the metadata manifest inside MAR-INF/. This command creates a folder {model_name} i.e. BERTSeqClassification/\n",
    "!torch-model-archiver --version 1.0 --model-name {model_name} --handler Transformer_handler_generalized.py --serialized-file pytorch_model.bin --extra-files setup_config.json,index_to_name.json,config.json --archive-format no-archive -f\n",
    "# Sagemaker requires that the models be stored *.tar.gz archive\n",
    "!tar -cvzf {model_name}.tar.gz -C {model_name}/ .\n",
    "!aws s3 cp {model_name}.tar.gz s3://{bucket}/{prefix}/models/\n",
    "!rm -rf {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12a538-f55d-4613-ab66-ab0cb31df042",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    py_version=\"py38\",\n",
    "    image_scope=\"inference\",\n",
    "    version=\"1.10\",\n",
    "    instance_type=\"ml.c5.9xlarge\",\n",
    ")\n",
    "\n",
    "# We'll use a pytorch inference DLC image that ships with sagemaker-pytorch-inference-toolkit v2.0.10. This version includes support for Torchserve environment variables used below\n",
    "# PT 1.11 image is released, but not part of python sdk yet\n",
    "image_uri = image_uri.replace(\"1.10\", \"1.11\")\n",
    "print(f\"Using image: {image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d34fb-980e-402a-9773-e86d03c7bb1a",
   "metadata": {},
   "source": [
    "#### Create SageMaker model, deploy and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d847dd-ab32-4313-b616-32f166bfacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "env_variables_dict = {\n",
    "    \"SAGEMAKER_TS_BATCH_SIZE\": \"3\",\n",
    "    \"SAGEMAKER_TS_MAX_BATCH_DELAY\": \"100000\",\n",
    "    \"SAGEMAKER_TS_MIN_WORKERS\": \"1\",\n",
    "    \"SAGEMAKER_TS_MAX_WORKERS\": \"1\",\n",
    "}\n",
    "\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_artifact,\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"1.11\",\n",
    "    env=env_variables_dict,\n",
    "    entry_point=\"inference.py\",\n",
    ")\n",
    "\n",
    "# Change the instance type as necessary, or use 'local' for executing in Sagemaker local mode\n",
    "instance_type = \"ml.c5.9xlarge\"\n",
    "#instance_type = \"local\"\n",
    "\n",
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.BytesDeserializer(),\n",
    ")\n",
    "\n",
    "# Wait for model to load in case of local mode\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274615f4-5247-4bff-accc-f66112730a16",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d63f34-f14a-4a2e-8db7-9bc2307df40a",
   "metadata": {},
   "source": [
    "#### By spawning a pool of 3 processes we're able to simulate requests from multiple clients and verify inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd983997-038d-4e1e-8540-e34ba5ffd0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "def invoke(endpoint_name):\n",
    "    predictor = sagemaker.predictor.Predictor(\n",
    "        endpoint_name,\n",
    "        sm_sess,\n",
    "        serializer=sagemaker.serializers.JSONSerializer(),\n",
    "        deserializer=sagemaker.deserializers.BytesDeserializer(),\n",
    "    )\n",
    "    return predictor.predict(\n",
    "        \"{Bloomberg has decided to publish a new report on global economic situation.}\"\n",
    "    )\n",
    "\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "pool = multiprocessing.Pool(3)\n",
    "results = pool.map(invoke, 10 * [endpoint_name])\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59b939-744a-45cf-82fa-efbbf1cac2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "predictor.delete_endpoint(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d981a-46de-46af-9e96-fcd66e3f057a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this exercise, we were able to understand the basics of batch inference using torchserve on Amazon SageMaker. We learnt that we can have several inference requests from different processes/users batched together, and the results will be processed as a batch of inputs. We also learnt that we could either use SageMaker's default DLC container as the base environment, or create a custom container that can be used with SageMaker for more involved workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
