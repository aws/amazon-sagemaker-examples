{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "martial-parish",
   "metadata": {
    "papermill": {
     "duration": 0.008883,
     "end_time": "2021-06-01T00:17:46.948261",
     "exception": false,
     "start_time": "2021-06-01T00:17:46.939378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MNIST Training using MXNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-pasta",
   "metadata": {
    "papermill": {
     "duration": 0.008859,
     "end_time": "2021-06-01T00:17:46.965963",
     "exception": false,
     "start_time": "2021-06-01T00:17:46.957104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and MXNet. This notebook example shows how to use Horovod with MXNet in SageMaker using MNIST dataset.\n",
    "\n",
    "For more information about the MXNet in SageMaker, please visit following github repositories:\n",
    "1. [sagemaker-mxnet-training-toolkit](https://github.com/aws/sagemaker-mxnet-training-toolkit/)\n",
    "2. [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) \n",
    "3. [sagemaker-training-toolkit](https://github.com/aws/sagemaker-training-toolkit)\n",
    "4. [deep-learning-containers](https://github.com/aws/deep-learning-containers)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p2.xlarge notebook instance._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the [Amazon SageMaker Roles](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "still-excuse",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T00:17:46.987262Z",
     "iopub.status.busy": "2021-06-01T00:17:46.986756Z",
     "iopub.status.idle": "2021-06-01T00:17:48.146283Z",
     "shell.execute_reply": "2021-06-01T00:17:48.146800Z"
    },
    "papermill": {
     "duration": 1.172127,
     "end_time": "2021-06-01T00:17:48.146977",
     "exception": false,
     "start_time": "2021-06-01T00:17:46.974850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-mxnet-mnist-horovod\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-pharmacology",
   "metadata": {
    "papermill": {
     "duration": 0.008833,
     "end_time": "2021-06-01T00:17:48.165019",
     "exception": false,
     "start_time": "2021-06-01T00:17:48.156186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "\n",
    "In this example, we will ues MNIST dataset. MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "elder-karma",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T00:17:48.186969Z",
     "iopub.status.busy": "2021-06-01T00:17:48.186509Z",
     "iopub.status.idle": "2021-06-01T00:17:48.265140Z",
     "shell.execute_reply": "2021-06-01T00:17:48.264269Z"
    },
    "papermill": {
     "duration": 0.091542,
     "end_time": "2021-06-01T00:17:48.265327",
     "exception": true,
     "start_time": "2021-06-01T00:17:48.173785",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mxnet.gluon.data.vision.datasets.MNIST at 0x7f5bb3836710>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet.gluon.data.vision import datasets, transforms\n",
    "\n",
    "datasets.MNIST(\n",
    "    \"data\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-classification",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Uploading the data to S3\n",
    "\n",
    "We are going to use the sagemaker.Session.upload_data function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "atmospheric-final",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-688520471316/sagemaker/DEMO-mxnet-mnist-horovod\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=\"data\", bucket=bucket, key_prefix=prefix)\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-coach",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Train\n",
    "\n",
    "Define hyperparameters of training job. Note, that `entry_point` param defines training script which will be executed on Horovod distributed cluster. Additionally, you can also define any parameters of your training script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-appearance",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Training Scipt\n",
    "\n",
    "The mnist.py script provides the code we need for training a SageMaker model. The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "- `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to. These artifacts are uploaded to S3 for model hosting.\n",
    "- `SM_NUM_GPUS`: The number of gpus available in the current container.\n",
    "- `SM_CURRENT_HOST`: The name of the current container on the container network.\n",
    "- `SM_HOSTS`: JSON encoded list containing all the hosts .\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's fit() method, the following will be set, following the format SM_CHANNEL_[channel_name]:\n",
    "\n",
    "- `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit SageMaker Containers.\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "This script uses Horovod framework for distributed training. \n",
    "\n",
    "You can run the following command to view the script run by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-questionnaire",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize horovod_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-toyota",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Run training in SageMaker\n",
    "\n",
    "The `MXNet` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters. In this case we are going to run our training job on 2 `ml.p2.8xlarge` instances. But this example can be ran on one or multiple, cpu or gpu instances ([full list of available instances](https://aws.amazon.com/sagemaker/pricing/instance-types/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-armor",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### SageMaker MXNet Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-association",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Estimator API in Sagemaker Python SDK supports distributed training functionality via the distributions parameter.\n",
    "To leverage Horovod, we specify `mpi` dictionary in the distributions parameter. The dictionary can contain following keys\n",
    "- `enabled`: True/False\n",
    "- `custom_mpi_options`: string\n",
    "- `processes_per_host`: integer\n",
    "\n",
    "Note: `train_instance_type` and `processes_per_host` are interlinked. Make sure that `processes_per_host` doesn't exceed the number of available GPUs in the instance.\n",
    "\n",
    "\n",
    "For further details on various AWS EC2 instances & available GPUs refer:\n",
    "- P3 (https://aws.amazon.com/ec2/instance-types/p3/)\n",
    "- G4 (https://aws.amazon.com/ec2/instance-types/g4/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "funny-franchise",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpi_options = \"-verbose -x orte_base_help_aggregate=0\"\n",
    "distributions = {\n",
    "    \"mpi\": {\"enabled\": True, \"custom_mpi_options\": mpi_options, \"processes_per_host\": 2}\n",
    "}\n",
    "hyperparameters = {\n",
    "    \"batch_size\": 64,\n",
    "    \"dtype\": 16,\n",
    "    \"epochs\": 1, # use a more realistic number for real training\n",
    "    \"lr\": 0.02,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "radio-sequence",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.mxnet.estimator import MXNet\n",
    "\n",
    "estimator = MXNet(\n",
    "    entry_point='horovod_mnist.py',\n",
    "    source_dir='code',\n",
    "    role=role,\n",
    "    instance_type='ml.p3.8xlarge',\n",
    "    instance_count=2,\n",
    "    #image_name=image,\n",
    "    framework_version='1.6.0',\n",
    "    py_version='py3',\n",
    "    distribution=distributions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-heating",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "After we've constructed our `MXNet` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "political-vietnamese",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-01 22:03:28 Starting - Starting the training job...\n",
      "2021-06-01 22:03:30 Starting - Launching requested ML instances.........\n",
      "2021-06-01 22:05:10 Starting - Preparing the instances for training......\n",
      "2021-06-01 22:06:19 Downloading - Downloading input data...\n",
      "2021-06-01 22:06:56 Training - Downloading the training image......\n",
      "2021-06-01 22:07:46 Training - Training image download completed. Training in progress.\u001b[35m2021-06-01 22:07:46,871 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:46,916 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\",\"algo-2\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"batch_size\":64,\"dtype\":16,\"epochs\":1,\"lr\":0.02}', 'SM_USER_ENTRY_POINT': 'horovod_mnist.py', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"training\"]', 'SM_CURRENT_HOST': 'algo-2', 'SM_MODULE_NAME': 'horovod_mnist', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '32', 'SM_NUM_GPUS': '4', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-688520471316/mxnet-training-2021-06-01-22-03-11-954/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch_size\":64,\"dtype\":16,\"epochs\":1,\"lr\":0.02},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"mxnet-training-2021-06-01-22-03-11-954\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-688520471316/mxnet-training-2021-06-01-22-03-11-954/source/sourcedir.tar.gz\",\"module_name\":\"horovod_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"horovod_mnist.py\"}', 'SM_USER_ARGS': '[\"--batch_size\",\"64\",\"--dtype\",\"16\",\"--epochs\",\"1\",\"--lr\",\"0.02\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'SM_HP_BATCH_SIZE': '64', 'SM_HP_LR': '0.02', 'SM_HP_DTYPE': '16', 'SM_HP_EPOCHS': '1'}\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:43,476 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:43,520 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\",\"algo-2\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"batch_size\":64,\"dtype\":16,\"epochs\":1,\"lr\":0.02}', 'SM_USER_ENTRY_POINT': 'horovod_mnist.py', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"training\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'horovod_mnist', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '32', 'SM_NUM_GPUS': '4', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-688520471316/mxnet-training-2021-06-01-22-03-11-954/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch_size\":64,\"dtype\":16,\"epochs\":1,\"lr\":0.02},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2021-06-01-22-03-11-954\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-688520471316/mxnet-training-2021-06-01-22-03-11-954/source/sourcedir.tar.gz\",\"module_name\":\"horovod_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"horovod_mnist.py\"}', 'SM_USER_ARGS': '[\"--batch_size\",\"64\",\"--dtype\",\"16\",\"--epochs\",\"1\",\"--lr\",\"0.02\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'SM_HP_BATCH_SIZE': '64', 'SM_HP_LR': '0.02', 'SM_HP_DTYPE': '16', 'SM_HP_EPOCHS': '1'}\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:51,047 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:51,047 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:51,048 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:51,048 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.236.7\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:51,493 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:51,494 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:51,497 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:51,498 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:51,499 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.236.236\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:52,056 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.2p2)\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:52,169 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:52,169 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:52,169 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:52,169 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:52,175 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:52,506 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.2p2)\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:52,618 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:52,618 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:52,618 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:52,618 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:2', 'algo-2:2'] process_per_hosts: 2 num_processes: 4\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:52,620 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:52,665 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 2,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-verbose -x orte_base_help_aggregate=0\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 64,\n",
      "        \"lr\": 0.02,\n",
      "        \"dtype\": 16,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"mxnet-training-2021-06-01-22-03-11-954\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-688520471316/mxnet-training-2021-06-01-22-03-11-954/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"horovod_mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"horovod_mnist.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":64,\"dtype\":16,\"epochs\":1,\"lr\":0.02}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=horovod_mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=horovod_mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-688520471316/mxnet-training-2021-06-01-22-03-11-954/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch_size\":64,\"dtype\":16,\"epochs\":1,\"lr\":0.02},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2021-06-01-22-03-11-954\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-688520471316/mxnet-training-2021-06-01-22-03-11-954/source/sourcedir.tar.gz\",\"module_name\":\"horovod_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"horovod_mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"64\",\"--dtype\",\"16\",\"--epochs\",\"1\",\"--lr\",\"0.02\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.02\u001b[0m\n",
      "\u001b[34mSM_HP_DTYPE=16\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:2,algo-2:2 -np 4 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose -x orte_base_help_aggregate=0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAINING -x SM_HP_BATCH_SIZE -x SM_HP_LR -x SM_HP_DTYPE -x SM_HP_EPOCHS -x PYTHONPATH /usr/local/bin/python3.6 -m mpi4py horovod_mnist.py --batch_size 64 --dtype 16 --epochs 1 --lr 0.02\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.236.236' (ECDSA) to the list of known hosts.#015\n",
      " Data for JOB [41211,1] offset 0 Total slots allocated 4\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-236-7#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [41211,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41211,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\n",
      " Data for node: algo-2#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [41211,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41211,1] App: 0 Process rank: 3 Bound: N/A\n",
      "\n",
      " =============================================================\n",
      " Data for JOB [41211,1] offset 0 Total slots allocated 4\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-236-7#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [41211,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41211,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\n",
      " Data for node: algo-2#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [41211,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41211,1] App: 0 Process rank: 3 Bound: N/A\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:54,180 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=42, name='orted', status='sleeping', started='22:07:52')]\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:54,180 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=42, name='orted', status='sleeping', started='22:07:52')]\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:54,180 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=42, name='orted', status='sleeping', started='22:07:52')]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:usage: horovod_mnist.py [-h] [--batch-size BATCH_SIZE] [--dtype DTYPE]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                        [--epochs EPOCHS] [--lr LR] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                        [--no-cuda NO_CUDA] [--hosts HOSTS]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                        [--current-host CURRENT_HOST] [--model-dir MODEL_DIR]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:                        [--data-dir DATA_DIR] [--num-gpus NUM_GPUS]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:usage: horovod_mnist.py [-h] [--batch-size BATCH_SIZE] [--dtype DTYPE]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                        [--epochs EPOCHS] [--lr LR] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                        [--no-cuda NO_CUDA] [--hosts HOSTS]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                        [--current-host CURRENT_HOST] [--model-dir MODEL_DIR]\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:                        [--data-dir DATA_DIR] [--num-gpus NUM_GPUS]\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:horovod_mnist.py: error: unrecognized arguments: --batch_size 64\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:horovod_mnist.py: error: unrecognized arguments: --batch_size 64\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mPrimary job  terminated normally, but 1 process returned\u001b[0m\n",
      "\u001b[34ma non-zero exit code. Per user-direction, the job has been aborted.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:usage: horovod_mnist.py [-h] [--batch-size BATCH_SIZE] [--dtype DTYPE]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                        [--epochs EPOCHS] [--lr LR] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                        [--no-cuda NO_CUDA] [--hosts HOSTS]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                        [--current-host CURRENT_HOST] [--model-dir MODEL_DIR]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:                        [--data-dir DATA_DIR] [--num-gpus NUM_GPUS]\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:horovod_mnist.py: error: unrecognized arguments: --batch_size 64\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:usage: horovod_mnist.py [-h] [--batch-size BATCH_SIZE] [--dtype DTYPE]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                        [--epochs EPOCHS] [--lr LR] [--momentum MOMENTUM]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                        [--no-cuda NO_CUDA] [--hosts HOSTS]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                        [--current-host CURRENT_HOST] [--model-dir MODEL_DIR]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:                        [--data-dir DATA_DIR] [--num-gpus NUM_GPUS]\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:horovod_mnist.py: error: unrecognized arguments: --batch_size 64\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real detected that one or more processes exited with non-zero status, thus causing\u001b[0m\n",
      "\u001b[34mthe job to be terminated. The first process to do so was:\n",
      "\n",
      "  Process name: [[41211,1],3]\n",
      "  Exit code:    2\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m2021-06-01 22:07:59,881 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1:2,algo-2:2 -np 4 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose -x orte_base_help_aggregate=0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAINING -x SM_HP_BATCH_SIZE -x SM_HP_LR -x SM_HP_DTYPE -x SM_HP_EPOCHS -x PYTHONPATH /usr/local/bin/python3.6 -m mpi4py horovod_mnist.py --batch_size 64 --dtype 16 --epochs 1 --lr 0.02\"\u001b[0m\n",
      "\u001b[35m2021-06-01 22:07:59,865 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\n",
      "2021-06-01 22:08:10 Uploading - Uploading generated training model\n",
      "2021-06-01 22:08:10 Failed - Training job failed\n",
      "\u001b[35m2021-06-01 22:08:29,885 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2021-06-01 22:08:29,885 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job mxnet-training-2021-06-01-22-03-11-954: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1:2,algo-2:2 -np 4 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose -x orte_base_help_aggregate=0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9b31b1f68c20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3366\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3367\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2955\u001b[0m                 ),\n\u001b[1;32m   2956\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2957\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2958\u001b[0m             )\n\u001b[1;32m   2959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job mxnet-training-2021-06-01-22-03-11-954: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1:2,algo-2:2 -np 4 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose -x orte_base_help_aggregate=0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_D"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs={\"training\": inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-clerk",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Host\n",
    "### Create an inference endpoint\n",
    "\n",
    "After training, we use the MXNet estimator object to build and deploy an MXNetPredictor. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "This allows us to perform inference on json encoded multi-dimensional arrays.\n",
    "\n",
    "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances. Here we will deploy the model to a single `ml.m4.xlarge` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-girlfriend",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-writer",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Cleanup\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-shopper",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.664404,
   "end_time": "2021-06-01T00:17:48.771894",
   "environment_variables": {},
   "exception": true,
   "input_path": "mxnet_mnist_horovod.ipynb",
   "output_path": "/opt/ml/processing/output/mxnet_mnist_horovod-2021-06-01-00-13-33.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-06-01T00:17:46.107490",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
