{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7221eff8-ecb8-44e0-a481-3ed401560096",
   "metadata": {},
   "source": [
    "# Train a MNIST model with PyTorch and deploy to Azure Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab9281-8473-4ee5-aa7e-ed81a38f2c74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d993f-43e1-4566-86c9-cc13cf07d46e",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "  - [Overview](#Overview)\n",
    "  - [Prerequisites](#Prerequisites)\n",
    "  - [Setup](#Setup)\n",
    "    - [Install Dependencies](#Install-Dependencies)\n",
    "  - [Parameters](#Parameters)\n",
    "  - [Training](#Training)\n",
    "    - [The Training Script (train.py)](#The-Training-Script-(train.py))\n",
    "    - [Dataset](#Dataset)\n",
    "    - [The PyTorch Estimator Class](#The-PyTorch-Estimator-Class)\n",
    "    - [Calling the Fit method](#Calling-the-Fit-method)\n",
    "  - [Export Model to ONNX](#Export-Model-to-ONNX)\n",
    "  - [Package the Model](#Package-the-Model)\n",
    "  - [Deploy the model](#Deploy-the-model)\n",
    "    - [Install the Azure CLI and utility libraries](#Install-the-Azure-CLI-and-utility-libraries)\n",
    "    - [Sign in to your Azure Account](#Sign-in-to-your-Azure-Account)\n",
    "    - [Setup](#Setup)\n",
    "    - [Create a Resource Group](#Create-a-Resource-Group)\n",
    "    - [Create Storage Account](#Create-Storage-Account)\n",
    "    - [Create the function app](#Create-the-function-app)\n",
    "    - [Deploy our zip package to the function app](#Deploy-our-zip-package-to-the-function-app)\n",
    "  - [Test Inference](#Test-Inference)\n",
    "    - [Normalize and Visualize a random set of test images](#Normalize-and-Visualize-a-random-set-of-test-images)\n",
    "    - [Run inference by invoking the Azure function URL](#Run-inference-by-invoking-the-Azure-function-URL)\n",
    "  - [Clean Up](#Clean-Up)\n",
    "  - [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827e563-8547-4dbc-a8d6-b38951695186",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to train a model using Amazon SageMaker and deploy it to Azure Functions. This approach is beneficial if you use AWS services for ML for its most comprehensive set of features, yet you need to run your model in another cloud provider in situations for example, you might have acquired a company that was already running on a different cloud provider, or you may have a workload that generates value from unique capabilities provided by AWS. Another example is independent software vendors (ISV) that make their products and services available in different cloud platforms to benefit their end customers. Or an organization may be operating in a Region where a primary cloud provider is not available, and in order to meet the data sovereignty or data residency requirements, they can use a secondary cloud provider.\n",
    "\n",
    "In this notebook, we use PyTorch with Amazon SageMaker to train a model to classify handwritten digits. Once trained, we export the model to an ONNX format and deploy it to Azure functions. To train the model, we use the popular MNIST dataset for training the model. \n",
    "MNIST is a subset of a larger set available from NIST. It contains 70000 labelled grayscale images each of size 28x28 pixels. The dataset is split into sets of 60000 training images and 10000 test images.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06c4f1-abdc-455f-8a9c-0e24a87d3348",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Access to Azure and credentials for a service principal that has permissions to create and manage Azure Functions and associate resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a9caa1-2a2d-4588-87f0-13ef60c99d0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4839d90-15f8-4165-a9a7-a18b535eb196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install torchvision onnx onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bcda6-fa57-479a-ac47-f8aaa230ff4a",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f086fe39-3cd1-4bec-8dfe-1cac879ea9d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Start with setting up basic configuration we would use throughout this notebook. This includes - \n",
    "* The Execution role that provides SageMaker permissions to access the input training and test data in the Amazon S3 bucket in your account.\n",
    "* The default region for SageMaker\n",
    "* The bucket and prefix where would be store the input dataset and where SageMaker would store the output model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc61dfb0-8d12-44c2-a24c-ac45a1d87dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "execution_role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"sagemaker/mnist-pytorch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135d055-b178-4902-8f2d-37917911a3ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34bd04-c33d-41a1-ba21-85ab188e7fb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Amazon SageMaker provides pre-built Docker images for most common machine learning frameworks, such as PyTorch, TensorFlow, PyTorch, and Chainer. These images include the deep learning framework and any other dependencies needed to run training and inference. In this example we use the pre-built image for PyTorch framework to train our model.\n",
    "\n",
    "The [Amazon SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk#installing-the-sagemaker-python-sdk\") makes it easier to train and deploy models with these deep learning frameworks.\n",
    "\n",
    "\n",
    "Training a model with PyTorch involves the following steps - \n",
    ">1. Prepare a Training Script - A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to artifacts to a specified output location so that it can be deployed for inference later.\n",
    ">\n",
    ">2. Create an Estimator - To run our training script on Amazon SageMaker, we create a PyTorch estimator\n",
    ">\n",
    ">3. Start training job using the fit method on the estimator - Start your training script by calling fit on an PyTorch Estimator. For what arguments can be passed into fit, see the [API reference](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Framework)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44eaf3-3a41-4811-9fa9-897e85cffef8",
   "metadata": {},
   "source": [
    "### The Training Script (train.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b90995c-7896-4765-843b-51a34dfca007",
   "metadata": {
    "tags": []
   },
   "source": [
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves the model artifact to location specified in the environment variable `SM_MODEL_DIR` (which default to path `/opt/ml/model` in the training container) so that it can be deployed for inference later. Hyperparameters are passed to your script as arguments and can be retrieved using`argparse.ArgumentParser`. \n",
    "\n",
    "Our script is adapted from the PyTorch MNIST example [here](https://github.com/apache/PyTorch/blob/master/example/gluon/mnist/mnist.py). \n",
    "\n",
    "In the training script we use the `export` function to export both the model architecture and the model parameters. We write these files to the `/opt/ml/model` directory of the container. When training completes, SageMaker copies these files as a single object in compressed tar format to the Amazon S3 output location that we specify when we define the estimator.\n",
    "\n",
    "Because the container imports your training script, always put your training code in a main guard `(if __name__=='__main__':)` so that the container does not inadvertently run your training code at the wrong point in execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29bdde2-f2d5-449e-b89f-bb8b5f97790a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize 'code/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb31665-f5af-4efe-bd37-2b40e1f158fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b7aa8-5b5b-4ad7-8192-e25b466dfcf3",
   "metadata": {},
   "source": [
    "Download the data using `torchvision.datasets` module and upload it to our Amazon S3 location. We pass this location to the Estimator class when we start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b123bfa-8cb6-4ad6-9e34-114ff7f288fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "MNIST.mirrors = [\n",
    "    f\"https://sagemaker-example-files-prod-{region}.s3.amazonaws.com/datasets/image/MNIST/\"\n",
    "]\n",
    "\n",
    "MNIST(\n",
    "    \"data\",\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3856a2d-d83e-428d-b3d8-a3229ab42a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = session.upload_data(path=\"data\", bucket=bucket, key_prefix=prefix)\n",
    "print(f\"Dataset uploaded to {inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433d566-bc3f-47d5-af41-0239480b1dce",
   "metadata": {},
   "source": [
    "### The PyTorch Estimator Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005b9cf-6f72-421b-9350-4b6fc82ebb87",
   "metadata": {
    "tags": []
   },
   "source": [
    "The [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/\") includes the PyTorch Estimator class that makes running training job using the SageMaker open source PyTorch container easier. \n",
    "\n",
    "We define the Estimator providing the following key inputs - \n",
    "\n",
    "* The name and location of our training script.\n",
    "* The IAM Role that grants SageMaker permissions to access our data in our input S3 bucket\n",
    "* The version of python and PyTorch framework - SageMaker uses this information to get the pre-built image from the Elastic Container registry (ECR)\n",
    "* Compute resources that we want SageMaker to use for model training. Compute resources are machine learning (ML) compute instances that are managed by SageMaker.\n",
    "* The URL of the S3 bucket where we store the output of the job.\n",
    "\n",
    "We also provide our training script as the entry point to the training estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ea10b-ac1f-4fe7-ad76-94ae87832da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "output_location = f\"s3://{bucket}/{prefix}/output\"\n",
    "print(f\"training artifacts will be uploaded to: {output_location}\")\n",
    "\n",
    "hyperparameters = {\n",
    "    \"batch-size\": 100,\n",
    "    \"epochs\": 1,\n",
    "    \"lr\": 0.1,\n",
    "    \"gamma\": 0.9,\n",
    "    \"log-interval\": 100,\n",
    "    \"save-model\": True,\n",
    "}\n",
    "\n",
    "\n",
    "instance_type = \"ml.c4.xlarge\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",  # directory of your training script\n",
    "    role=execution_role,\n",
    "    framework_version=\"1.12\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    volume_size=250,\n",
    "    output_path=output_location,\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8729ef-547f-46d4-b526-12409c12ae84",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calling the Fit method\n",
    "Once we have defined the estimator, we can start training by calling the `fit()` method on the estimator, providing the inputs. When we call the `fit` method Amazon SageMaker starts a training job using our script as training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e399d-d94b-45d5-aadc-642d01f29e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs={\"training\": f\"{inputs}\", \"testing\": f\"{inputs}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511c9eb-c14a-4211-a0e8-8c258beae922",
   "metadata": {},
   "source": [
    "Once the PyTorch model has been trained, the output would be available in the designated output bucket. The model artifacts are store in a file with a name `model.tar.gz`. We can get the exact location of the model output using `estimator.model_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00545802-4ed9-425e-9a50-8c017741cf17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Export Model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986520c9-705e-4bed-acc1-46fbc4f2ccd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Open Neural Network Exchange (ONNX) is an open format used to represent machine learning models. The PyTorch module `torch.onnx` can be used to export our model to ONNX. A model is ONNX format can be consumed by many [runtimes that support ONNX](https://onnx.ai/supported-tools.html#deployModel). The benefit of ONNX models is that they can be moved between frameworks with ease.\n",
    "\n",
    "\n",
    "We included the below code snippet that exports our trained model into ONNX format in our training script. We can this function at the end of training. The code uses the `export` function in the `torch.onnx` module. The function expects along with the model `state_dict` the input and output size and shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520dcab1-0dd3-41b6-b3ff-e10303b98c6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "def export_to_onnx(model, model_dir, device):\n",
    "    logger.info(\"Exporting the model to onnx.\")\n",
    "    dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "    input_names = [ \"input_0\" ]\n",
    "    output_names = [ \"output_0\" ]\n",
    "    path = os.path.join(model_dir, 'mnist-pytorch.onnx')\n",
    "    torch.onnx.export(model, dummy_input, path, verbose=True, input_names=input_names, output_names=output_names)\n",
    "```\n",
    "When the training job finishes, Amazon SageMaker copies the exported file from the location specified the environment variable `SM_MODEL_DIR` (which default to path `/opt/ml/model` in the training container) to the S3 Bucket path specified in the output location.\n",
    "\n",
    "We download the model archive from the S3 location to a local directory on our SageMaker Studio Notebook instance and unpack it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f4ea3-1089-4abe-af0e-5ed511ae274e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "model_dir = \"model\"\n",
    "model_zip = \"model.tar.gz\"\n",
    "model_onnx_file = \"mnist-pytorch.onnx\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "local_model_file = f\"{model_dir}/{model_zip}\"\n",
    "model_bucket, model_key = estimator.model_data.split(\"/\", 2)[-1].split(\"/\", 1)\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(model_bucket, model_key, local_model_file)\n",
    "\n",
    "model_tar = tarfile.open(local_model_file)\n",
    "model_file_name = model_tar.next().name\n",
    "model_tar.extractall(model_dir)\n",
    "model_tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec674f-713e-4f51-a23c-0bdd1e453aac",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our PyTorch model archive contains the following two files our training script saved during the training process. \n",
    "* The PyTorch model file - `model.pth`\n",
    "* The Exported ONNX model file - `mnist-pytorch.onnx`\n",
    "\n",
    "After extracting the ONNX model from our model archive, we can check the consistency of the ONNX model using the `check_model` function in the `onnx.checker` module.\n",
    "\n",
    "Once validated, we use the exported ONNX model file in the subsequent steps where we package and deploy to Azure functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55448da9-f93a-4758-9277-6afcc375f1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(f\"{model_dir}/{model_onnx_file}\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb8fb3-ccc7-4003-a18b-db90d5bde687",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Package the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0160b80-7a30-4827-afbe-c6503cb41425",
   "metadata": {},
   "source": [
    "We use zip deployment method to publish our code to Azure Functions. In order to do that we need to package our ONNX model file created above along with Azure Function code into a zip file. The artifacts required to deploy the function code are in the `functionapp` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76b42d4",
   "metadata": {},
   "source": [
    "### Review Azure FunctionApp Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f908a8b-8563-4789-8900-935e71b296a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize functionapp/mnist-onnx/function_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3386f",
   "metadata": {},
   "source": [
    "### Create zip file with functionapp code for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3cc0b5-8918-46a1-a1e9-b3f2948b5b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "import pathlib\n",
    "\n",
    "\n",
    "onnx_model = f\"{model_dir}/{model_onnx_file}\"\n",
    "\n",
    "os.makedirs(f\"functionapp/mnist-onnx/{model_dir}\", exist_ok=True)\n",
    "shutil.copyfile(onnx_model, f\"functionapp/mnist-onnx/{onnx_model}\")\n",
    "src_path = \"functionapp/mnist-onnx/\"\n",
    "function_archive = \"functionapp/mnist-onnx.zip\"\n",
    "with ZipFile(function_archive, \"w\") as archive_file:\n",
    "    for dirpath, dirnames, filenames in os.walk(src_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            archive_file_path = os.path.relpath(file_path, src_path)\n",
    "            archive_file.write(file_path, archive_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a50e4-d5c2-4a7a-98f8-f2160effcc47",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04128655-fdec-4064-a02c-ef1547d982f0",
   "metadata": {},
   "source": [
    "### Install the Azure CLI and utility libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b731e32-49ef-44a4-8d37-962716674c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this section, we create an Azure function app (along with prerequisite resources). We then publish our model and our inference code to the function app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9290f8e-d8ea-44b0-8760-ad6dad48a137",
   "metadata": {
    "tags": []
   },
   "source": [
    "As a first step, we install the Azure CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91eeb9-be63-458d-a858-7a6d86116c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q azure-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1796d623-9ed8-4f33-a507-6683d8338e51",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Important Note:</b> We use variable `AZURE_CONNECTED` to control if following notebook code connects and perform necessary operations in your Azure subscription. By default, this variable is set to False meaning the code would not attempt to connect to Azure and would not deploy our model to Azure Functions. In order to enable the Notebook to connect to Azure and deploy the model to Azure Functions we need to explicitly set AZURE_CONNECTED to <b>True</b> in the cell below</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e25769-723d-4082-b599-0d8e222371c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AZURE_CONNECTED = False\n",
    "# AZURE_CONNECTED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b2a07-cd0c-43fa-8c20-738d802fce08",
   "metadata": {},
   "source": [
    "### Sign in to your Azure Account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5cb8d-5a28-406c-a7fc-c7ebfad51a80",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before we can start using Azure CLI, we need to sign in to Azure using the `az login` command. The Azure CLI supports several authentication methods. Restrict sign-in permissions for your use case to keep your Azure resources secure.\n",
    "\n",
    "The Azure CLI's default authentication method for logins uses a web browser and access token to sign in. This is a good option when learning Azure CLI commands and running the Azure CLI locally. See [Authenticate to Azure using Azure CLI](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli#sign-into-azure-with-azure-cli) for supported authentication methods.\n",
    "\n",
    "We use Azure CLI command `az login` that initiates the [device code flow](https://learn.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-device-code) and instructs us to open a browser page at https://login.microsoftonline.com/common/oauth2/deviceauth. Then, enter the code displayed in your terminal.\n",
    "\n",
    "This will allow us to interactively log in to our Azure account. Once we entered the code into the `devicelogin` URL, we are directed to Microsoft login website where we can enter our Azure account credentials to log in to Azure. When the authentication is successful you see a message like below - \n",
    "\n",
    "![ Azure-CLI-Success](success.jpg)\n",
    "\n",
    "Once you see the message above, return to the Notebook and verify that the Azure login has been successful. Once successful, proceed to run the subsequent cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e52488-7ec9-4757-a5a0-b97b7bd29ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if AZURE_CONNECTED:\n",
    "    !az config set core.login_experience_v2=off\n",
    "    !az login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288adf29-da2f-4b51-bf02-295c2088d192",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f0922-0093-4831-bd48-d07a8b3c4e59",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we configure a few variables that we use with Azure CLI commands to create the Azure function app and the prerequisites resources. We use a random suffix to ensure unique names for resources wherever necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c8685-99ad-4876-a54b-b69de7796c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_suffix = str(random.randint(10000, 99999))\n",
    "resource_group_name = f\"multicloud-{random_suffix}-rg\"\n",
    "storage_account_name = f\"multicloud{random_suffix}\"\n",
    "location = \"ukwest\"\n",
    "sku_storage = \"Standard_LRS\"\n",
    "functions_version = \"4\"\n",
    "python_version = \"3.9\"\n",
    "function_app = f\"multicloud-mnist-{random_suffix}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7b77e-4848-460f-abce-bef21bbecbc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once our environment is set up, we proceed to issue commands to create the necessary resources as below - \n",
    "\n",
    ">1. A Resource group that acts as a container for related resources\n",
    ">2. A Storage account for the function app that would be used to maintain state and other information about your functions\n",
    ">3. An Azure function app that provides the environment for executing our code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59f77d-1c87-4e4b-80e2-f1f32e617dc6",
   "metadata": {},
   "source": [
    "### Create a Resource Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1490ecd-b173-4dcc-a327-583f502adbec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if AZURE_CONNECTED:\n",
    "    !az group create --name {resource_group_name} --location {location}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b9c0f0-e0cb-4936-b0d5-c2682b229ce3",
   "metadata": {},
   "source": [
    "### Create Storage Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec2a87-ee2a-43b6-8db7-4821e93bd302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if AZURE_CONNECTED:\n",
    "    !az storage account create --name {storage_account_name} --resource-group {resource_group_name} --location {location} --sku {sku_storage}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff59c68-4eac-46d3-8b29-55792b02aea5",
   "metadata": {},
   "source": [
    "### Create the function app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a77bc4-26c4-4824-95ea-eb01e4c6dbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if AZURE_CONNECTED:\n",
    "    !az functionapp create --name {function_app} --resource-group {resource_group_name} --storage-account {storage_account_name}  --consumption-plan-location \"{location}\" --os-type Linux --runtime python --runtime-version {python_version} --functions-version {functions_version}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba65ca-0182-41cf-a5d1-af3d8e985851",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before we deploy our function code we are going to set a few configurations on the Azure Function. One of the key configuration is to set `SCM_DO_BUILD_DURING_DEPLOYMENT` to `true` to tell the function app to perform a build during deployment. This ensures that Azure Function uses our requirements.txt to make the dependencies available to our code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3458e58-976a-43e1-b3ed-ecc73ebbbeaf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> In case you see a `Resource Not Found` error wait for a few minutes and try again.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc35bf-a0ec-4588-9f62-4900c803414c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if AZURE_CONNECTED:\n",
    "    !az functionapp config appsettings set --name {function_app} --resource-group {resource_group_name} --settings @./functionapp/settings.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654b11e-ea0e-445d-a5af-4f84558ce6a0",
   "metadata": {},
   "source": [
    "### Deploy our zip package to the function app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6faa56-e3ae-4af6-9d65-becc88a07109",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that our function app is created and configured, we can deploy our model package to the Azure function. Once we do this the model would be available for inference through a function URL exposed by Azure Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c6971-9d81-419d-907e-ea5cfb6195d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if AZURE_CONNECTED:\n",
    "    !az functionapp deployment source config-zip -g {resource_group_name} -n {function_app} --src {function_archive} --build-remote true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed103e47-c356-4e79-8e2f-e76264473fc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da75c75d-96db-4d13-9cb3-987712468422",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once our model packaged as Azure function code is published to the Azure function app we can use the endpoint URL of the function to invoke our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2571cdf-e633-4d04-9449-281d3ed1baca",
   "metadata": {
    "tags": []
   },
   "source": [
    "The MNIST database of handwritten digits has a test set of 10,000 examples. The test data is available as two files, test set images and test set labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9316e4-17a8-405c-bc77-c11afecec423",
   "metadata": {},
   "source": [
    "### Normalize and Visualize a random set of test images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476080a-0043-4e66-befb-31d0d6f6b797",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to test inference using Azure functions endpoint, we sample a random selection of 16 images from the test dataset and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7bc30-b17b-44e7-bd4f-4da09e6631d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "test_dataset = datasets.MNIST(root=\"../data\", download=True, train=False, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_features, test_labels = next(iter(test_loader))\n",
    "\n",
    "# plot the images\n",
    "fig, axs = plt.subplots(nrows=1, ncols=16, figsize=(16, 1))\n",
    "\n",
    "for i, splt in enumerate(axs):\n",
    "    splt.imshow(test_features[i].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025af34-e887-4dd7-915f-6ff4675dc1ad",
   "metadata": {},
   "source": [
    "### Run inference by invoking the Azure function URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c223c-36ed-4087-bf0a-a0022e69a521",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Azure function endpoint URL is of the format `function_app.azurewebsites.net`. We send the input to the function in the json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529ab2f-136f-4aa5-80e4-4823d1c0e0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "\n",
    "url = f\"https://{function_app}.azurewebsites.net/api/classify\"\n",
    "if AZURE_CONNECTED:\n",
    "    response = requests.post(url, json.dumps({\"data\": to_numpy(test_features).tolist()}))\n",
    "    predictions = json.loads(response.text)[\"digits\"]\n",
    "else:\n",
    "    predictions = np.zeros(test_features.size(0))\n",
    "# plot the images\n",
    "fig_out, axs_out = plt.subplots(nrows=1, ncols=16, figsize=(16, 1))\n",
    "\n",
    "for i, splt in enumerate(axs_out):\n",
    "    splt.imshow(test_features[i].reshape(28, 28))\n",
    "    splt.set_title(predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4160114e-fb02-4bd6-8fe0-75e111a2d2a7",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533a1a7-c952-4853-88d2-6032acf1204a",
   "metadata": {
    "tags": []
   },
   "source": [
    "After we have tested that our model is running successfully on Azure function app, we delete the resources to avoid incurring unnecessary costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f79f3-baa2-49c9-ba97-a16077ccf879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if AZURE_CONNECTED:\n",
    "    !az group delete --name {resource_group_name} --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2dded-bacf-4d6d-8b29-32102f7cb6a9",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa6e73-72a3-4c41-a5e8-b09ed7754e15",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this notebook, we used prebuilt docker images with Amazon SageMaker to train an PyTorch model and deploy it to Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74028e08-a69c-43aa-beb5-b5f5fdb691f4",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/deploy_and_monitor|sm-multi_cloud_deployment_with_onnx|pytorch|mnist-train-using-pytorch.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
