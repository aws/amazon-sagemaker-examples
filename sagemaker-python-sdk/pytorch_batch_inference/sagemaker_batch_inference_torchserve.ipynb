{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615b38ad-e8e5-49f4-a66a-036534c62798",
   "metadata": {},
   "source": [
    "# SageMaker Real-time Dynamic Batching Inference with Torchserve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1647aa0-0140-40fc-bf58-bd8cf786d7a4",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of dynamic batching on SageMaker with [torchserve](https://github.com/pytorch/serve/) as a model server. It demonstrates the following\n",
    "1. Batch inference using DLC i.e. SageMaker's default backend container. This is done by using sagemaker python sdk in script-mode.\n",
    "2. Specifying inference parameters for torchserve using environment variables.\n",
    "3. Option to use a custom container with config file for torchserve baked-in the container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7434c-2d73-41dc-a56c-7db10b9f552f",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce290e30-dadc-4fa8-bd15-7db4bafe4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import boto3, time, json\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d5ef87-09c4-44ac-9dc4-748cc90960e4",
   "metadata": {},
   "source": [
    "**Initiate session and retrieve region, account details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474ea986-41bc-413b-88e6-470f7258d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3227d6a8-3db5-4938-86db-59f2deec2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "region = sess.region_name\n",
    "account = boto3.client(\"sts\").get_caller_identity().get(\"Account\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa306e3b-0f01-46d3-b35e-4700ff7c38ad",
   "metadata": {},
   "source": [
    "**Prepare model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac74bc9-8463-45fe-b090-3a3a4bff6f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-10 17:05:02--  https://torchserve.s3.amazonaws.com/tar_gz_files/BERTSeqClassification.tar.gz\n",
      "Resolving torchserve.s3.amazonaws.com (torchserve.s3.amazonaws.com)... 52.216.179.99\n",
      "Connecting to torchserve.s3.amazonaws.com (torchserve.s3.amazonaws.com)|52.216.179.99|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 405264576 (386M) [application/x-tar]\n",
      "Saving to: ‘BERTSeqClassification.tar.gz.2’\n",
      "\n",
      "BERTSeqClassificati 100%[===================>] 386.49M  9.98MB/s    in 26s     \n",
      "\n",
      "2021-11-10 17:05:28 (14.9 MB/s) - ‘BERTSeqClassification.tar.gz.2’ saved [405264576/405264576]\n",
      "\n",
      "upload: ./BERTSeqClassification.tar.gz to s3://sagemaker-us-west-2-850464037171/ts-dynamic-batching/models/BERTSeqClassification.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-850464037171/ts-dynamic-batching/models/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = sm_sess.default_bucket()\n",
    "prefix = \"ts-dynamic-batching\"\n",
    "model_file_name = \"BERTSeqClassification\"\n",
    "\n",
    "!wget https://torchserve.s3.amazonaws.com/tar_gz_files/BERTSeqClassification.tar.gz\n",
    "!aws s3 cp BERTSeqClassification.tar.gz s3://{bucket}/{prefix}/models/\n",
    "\n",
    "f\"s3://{bucket}/{prefix}/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fbed613-c3cd-4d10-95a6-93eecc4a7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = f\"s3://{bucket}/{prefix}/models/{model_file_name}.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdb7075-f6d4-4d6e-8192-ebe436746d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"hf-dynamic-torchserve-sagemaker\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8119e33-fed7-4685-aa4c-caa468262e13",
   "metadata": {},
   "source": [
    "## Using AWS Deep Learning Container\n",
    "`Note: See end of notebook for using a custom container`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "930811ed-b740-4852-9671-37f797400cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a pytorch inference DLC image that ships with sagemaker-pytorch-inference-toolkit v2.0.6. This version includes support for Torchserve environment variables used below.\n",
    "image_uri = sagemaker.image_uris.retrieve(framework=\"pytorch\", region=\"us-west-2\", py_version=\"py38\", image_scope=\"inference\", version=\"1.9\", instance_type=\"ml.c5.4xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d34fb-980e-402a-9773-e86d03c7bb1a",
   "metadata": {},
   "source": [
    "#### Create Sagemaker model, deploy and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66d847dd-ab32-4313-b616-32f166bfacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "env_variables_dict = {\"SAGEMAKER_TS_BATCH_SIZE\": \"3\", \"SAGEMAKER_TS_MAX_BATCH_DELAY\": \"100000\"}\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_artifact,\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"1.9\",\n",
    "    entry_point=\"inference.py\",\n",
    "    env=env_variables_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bff4c42-9b5f-43dc-80d9-6d0e96082cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# Change the instance type as necessary, or use 'local' for executing in Sagemaker local mode\n",
    "instance_type = \"ml.c5.9xlarge\"\n",
    "\n",
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.BytesDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274615f4-5247-4bff-accc-f66112730a16",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee9d5b9-435c-4b3b-9f0f-c49207738b01",
   "metadata": {},
   "source": [
    "#### The following prediction call could timeout for certain instance types (SageMaker 60 second limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6100ea7-9f25-429b-a6f0-2ac7713549b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-inference-2021-11-10-17-06-07-915 in account 850464037171 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-27be0a6aadad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m result = predictor.predict(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m\"{Bloomberg has decided to publish a new report on global economic situation.}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TIME:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/blog/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/blog/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/blog/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-inference-2021-11-10-17-06-07-915 in account 850464037171 for more information."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "result = predictor.predict(\n",
    "    \"{Bloomberg has decided to publish a new report on global economic situation.}\"\n",
    ")\n",
    "print(\"TIME:\", time.time() - start)\n",
    "print(\"ENDPOINT RESULT:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d63f34-f14a-4a2e-8db7-9bc2307df40a",
   "metadata": {},
   "source": [
    "#### By spawning a pool of 3 processes we're able to simulate requests from multiple clients and verify inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd983997-038d-4e1e-8540-e34ba5ffd0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "def invoke(endpoint_name):\n",
    "    predictor = sagemaker.predictor.Predictor(\n",
    "        endpoint_name,\n",
    "        sm_sess,\n",
    "        serializer=sagemaker.serializers.JSONSerializer(),\n",
    "        deserializer=sagemaker.deserializers.BytesDeserializer(),\n",
    "    )\n",
    "    return predictor.predict(\n",
    "        \"{Bloomberg has decided to publish a new report on global economic situation.}\"\n",
    "    )\n",
    "\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "pool = multiprocessing.Pool(3)\n",
    "results = pool.map(invoke, 3 * [endpoint_name])\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59b939-744a-45cf-82fa-efbbf1cac2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fcf207-bf4e-4600-80b4-6375f1e14eb5",
   "metadata": {},
   "source": [
    "## Using a custom container\n",
    "\n",
    "#### Details (Also see ./docker/)\n",
    "* Prebaked config.properties file included\n",
    "  * 1.66 Minute Batch Delay (Longer than SageMaker 60s Timeout)\n",
    "  * Batch size of 3\n",
    "  * Note: the image needs to be built and pushed only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909803b-d35b-4d13-a085-08025b006987",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "container_name=custom-dynamic-torchserve\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${container_name}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${container_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${container_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "docker build  -t ${container_name} docker/\n",
    "docker tag ${container_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d981a-46de-46af-9e96-fcd66e3f057a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this exercise, we were able to understand the basics of batch inference using torchserve on Amazon SageMaker. We learnt that we can have several inference requests from different processes/users batched together, and the results will be processed as a batch of inputs. We also learnt that we could either use SageMaker's default DLC container as the base environment, and supply an inference.py script with the model, or create a custom container that can be used with SageMaker for more involved workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
