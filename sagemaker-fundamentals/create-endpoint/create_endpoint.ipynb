{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  60.42kB\r",
      "\r\n",
      "Step 1/11 : FROM ubuntu:18.04\n",
      " ---> c090eaba6b94\n",
      "Step 2/11 : MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
      " ---> Using cache\n",
      " ---> dd1348b3a447\n",
      "Step 3/11 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          python3-pip          python3-setuptools          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 5d88b1be1abc\n",
      "Step 4/11 : RUN ln -s /usr/bin/python3 /usr/bin/python\n",
      " ---> Using cache\n",
      " ---> 6417ac44ded6\n",
      "Step 5/11 : RUN ln -s /usr/bin/pip3 /usr/bin/pip\n",
      " ---> Using cache\n",
      " ---> 09ada8830cc5\n",
      "Step 6/11 : RUN pip --no-cache-dir install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gunicorn\n",
      " ---> Using cache\n",
      " ---> 2885b6a01bd3\n",
      "Step 7/11 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> ab2910812f0d\n",
      "Step 8/11 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 0c0e511ac354\n",
      "Step 9/11 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 10fcf3254c9d\n",
      "Step 10/11 : COPY decision_trees /opt/program\n",
      " ---> Using cache\n",
      " ---> eb1b28c0c5c9\n",
      "Step 11/11 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> 2cb200fbc0a2\n",
      "Successfully built 2cb200fbc0a2\n",
      "Successfully tagged example-inference:latest\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build -t example-inference:latest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/sagemaker-notes/scikit_bring_your_own/container\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run the inference container\n",
    "docker run -v /home/ubuntu/sagemaker-notes/scikit_bring_your_own/containerlocal_test/test_dir:/opt/ml -p 8080:8080 --rm example-inference:latest serve \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 127.0.0.1:8080...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8080 (#0)\n",
      "> GET /ping HTTP/1.1\r\n",
      "> Host: localhost:8080\r\n",
      "> User-Agent: curl/7.68.0\r\n",
      "> Accept: */*\r\n",
      "> Content-Type: text/csv\r\n",
      "> \r\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\r\n",
      "< Server: nginx/1.14.0 (Ubuntu)\r\n",
      "< Date: Fri, 05 Mar 2021 00:20:23 GMT\r\n",
      "< Content-Type: application/json\r\n",
      "< Content-Length: 1\r\n",
      "< Connection: keep-alive\r\n",
      "< \r\n",
      "{ [1 bytes data]\n",
      "\r",
      "100     1  100     1    0     0   1000      0 --:--:-- --:--:-- --:--:--  1000\n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -H \"Content-Type: text/csv\" -v http://localhost:8080/ping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a wonderful potato"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 127.0.0.1:8080...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8080 (#0)\n",
      "> POST /invocations HTTP/1.1\r\n",
      "> Host: localhost:8080\r\n",
      "> User-Agent: curl/7.68.0\r\n",
      "> Accept: */*\r\n",
      "> Content-Type: text/csv\r\n",
      "> Content-Length: 2900\r\n",
      "> Expect: 100-continue\r\n",
      "> \r\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 100 Continue\r\n",
      "} [2900 bytes data]\n",
      "* We are completely uploaded and fine\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\r\n",
      "< Server: nginx/1.14.0 (Ubuntu)\r\n",
      "< Date: Fri, 05 Mar 2021 00:20:26 GMT\r\n",
      "< Content-Type: text/csv; charset=utf-8\r\n",
      "< Content-Length: 26\r\n",
      "< Connection: keep-alive\r\n",
      "< \r\n",
      "{ [26 bytes data]\n",
      "\r",
      "100  2926  100    26  100  2900   5200   566k --:--:-- --:--:-- --:--:--  571k\n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl --data-binary @local_test/payload.csv -H \"Content-Type: text/csv\" -v http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'example-inference' already exists in the registry with id '688520471316'\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import pprint\n",
    "import os\n",
    "import time\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "iam = boto3.client('iam')\n",
    "ecr = boto3.client('ecr')\n",
    "\n",
    "image_name=\"example-inference\"\n",
    "\n",
    "try:\n",
    "    # The repository might already exist\n",
    "    # in your ECR\n",
    "    cr_res = ecr.create_repository(\n",
    "        repositoryName=image_name)\n",
    "    pp.pprint(cr_res)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "The push refers to repository [688520471316.dkr.ecr.us-west-2.amazonaws.com/example-inference]\n",
      "f4cd793a4ebc: Preparing\n",
      "b520e3bd5eba: Preparing\n",
      "3a3b4090fe28: Preparing\n",
      "5b850ff3c508: Preparing\n",
      "408c63ea099b: Preparing\n",
      "9f10818f1f96: Preparing\n",
      "27502392e386: Preparing\n",
      "c95d2191d777: Preparing\n",
      "27502392e386: Waiting\n",
      "c95d2191d777: Waiting\n",
      "3a3b4090fe28: Pushed\n",
      "f4cd793a4ebc: Pushed\n",
      "5b850ff3c508: Pushed\n",
      "9f10818f1f96: Pushed\n",
      "27502392e386: Pushed\n",
      "c95d2191d777: Pushed\n",
      "408c63ea099b: Pushed\n",
      "b520e3bd5eba: Pushed\n",
      "latest: digest: sha256:72907655e9cc8dd5a940f7c04932e66aee7890356d6df4bad7380979a362ae2c size: 1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "account=$(aws sts get-caller-identity --query Account | sed -e 's/^\"//' -e 's/\"$//')\n",
    "region=$(aws configure get region)\n",
    "ecr_account=${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Give docker your ECR login password\n",
    "aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $ecr_account\n",
    "\n",
    "# Fullname of the repo\n",
    "fullname=$ecr_account/example-inference:latest\n",
    "\n",
    "#echo $fullname\n",
    "# Tag the image with the fullname\n",
    "docker tag example-inference:latest $fullname\n",
    "\n",
    "# Push to ECR\n",
    "docker push $fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'HTTPHeaders': {'content-length': '404',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 00:42:52 GMT',\n",
      "                                      'x-amzn-requestid': '3073c552-5b39-4217-9895-6dee8f9260e3'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '3073c552-5b39-4217-9895-6dee8f9260e3',\n",
      "                      'RetryAttempts': 0},\n",
      " 'imageDetails': [{'artifactMediaType': 'application/vnd.docker.container.image.v1+json',\n",
      "                   'imageDigest': 'sha256:72907655e9cc8dd5a940f7c04932e66aee7890356d6df4bad7380979a362ae2c',\n",
      "                   'imageManifestMediaType': 'application/vnd.docker.distribution.manifest.v2+json',\n",
      "                   'imagePushedAt': datetime.datetime(2021, 3, 5, 0, 31, 31, tzinfo=tzlocal()),\n",
      "                   'imageSizeInBytes': 132463690,\n",
      "                   'imageTags': ['latest'],\n",
      "                   'registryId': '688520471316',\n",
      "                   'repositoryName': 'example-inference'}]}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the ECR repository\n",
    "repo_res = ecr.describe_images(\n",
    "    repositoryName='example-inference')\n",
    "pp.pprint(repo_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri=\"688520471316.dkr.ecr.us-west-2.amazonaws.com/example-inference:latest\"\n",
    "role_arn = 'arn:aws:iam::688520471316:role/sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelArn': 'arn:aws:sagemaker:us-west-2:688520471316:model/example-inference',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '79',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 00:46:16 GMT',\n",
      "                                      'x-amzn-requestid': '88ceba30-6a67-4c52-b2d9-71fef75121af'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '88ceba30-6a67-4c52-b2d9-71fef75121af',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "sm_boto3 = boto3.client('sagemaker')\n",
    "\n",
    "cm_res = sm_boto3.create_model(\n",
    "    ModelName='example-inference',\n",
    "    Containers=[\n",
    "        {\n",
    "            'Image': image_uri,\n",
    "   \n",
    "        },\n",
    "    ],\n",
    "    ExecutionRoleArn=role_arn,\n",
    "    EnableNetworkIsolation=False\n",
    ")\n",
    "\n",
    "pp.pprint(cm_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create endpoint config\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint\n",
    "model_name='example-inference'\n",
    "initial_instance_count=1\n",
    "instance_type='ml.t2.medium'\n",
    "variant_name = \"AMeaningfulProdVarName\" #^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n",
    "\n",
    "# why do we need it\n",
    "# think about a use case, where we need many variants\n",
    "production_variants = [\n",
    "    {\n",
    "        \"VariantName\": variant_name,\n",
    "        \"ModelName\": model_name,\n",
    "        \"InitialInstanceCount\": initial_instance_count,\n",
    "        \"InstanceType\": instance_type\n",
    "    }\n",
    "]\n",
    "\n",
    "endpoint_config_name = \"ExampleInferenceConfig\" #^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n",
    "\n",
    "endpoint_config = {\n",
    "    \"EndpointConfigName\": endpoint_config_name,\n",
    "    \"ProductionVariants\": production_variants,\n",
    "}\n",
    "\n",
    "ep_conf_res = sm_boto3.create_endpoint_config(**endpoint_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:688520471316:endpoint-config/exampleinferenceconfig',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '103',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 01:44:02 GMT',\n",
      "                                      'x-amzn-requestid': 'fd66804f-380a-478e-b3fb-b3f6296f0639'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'fd66804f-380a-478e-b3fb-b3f6296f0639',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(ep_conf_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create endpoint\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint\n",
    "\n",
    "\n",
    "endpoint_name='exmaple-endpoint'\n",
    "ep_res = sm_boto3.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndpointArn': 'arn:aws:sagemaker:us-west-2:688520471316:endpoint/exmaple-endpoint',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '84',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 01:53:40 GMT',\n",
      "                                      'x-amzn-requestid': '641e22b1-dc16-4d50-b615-86a857180def'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '641e22b1-dc16-4d50-b615-86a857180def',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(ep_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2021, 3, 5, 1, 53, 41, 572000, tzinfo=tzlocal()),\n",
      " 'EndpointArn': 'arn:aws:sagemaker:us-west-2:688520471316:endpoint/exmaple-endpoint',\n",
      " 'EndpointConfigName': 'ExampleInferenceConfig',\n",
      " 'EndpointName': 'exmaple-endpoint',\n",
      " 'EndpointStatus': 'Creating',\n",
      " 'LastModifiedTime': datetime.datetime(2021, 3, 5, 1, 53, 41, 572000, tzinfo=tzlocal()),\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '260',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 01:56:13 GMT',\n",
      "                                      'x-amzn-requestid': '285fb80a-96dc-4adc-aceb-385d729a0199'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '285fb80a-96dc-4adc-aceb-385d729a0199',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# describe endpoint\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.describe_endpoint\n",
    "\n",
    "ep_des_res = sm_boto3.describe_endpoint(\n",
    "    EndpointName=endpoint_name\n",
    ")\n",
    "\n",
    "pp.pprint(ep_des_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke endpoint \n",
    "\n",
    "```sh\n",
    "grep -rwn .  -e predict\n",
    "```\n",
    "\n",
    "```python\n",
    "def predict(\n",
    "        self, data, initial_args=None, target_model=None, target_variant=None, inference_id=None\n",
    "    ):\n",
    "        \"\"\"Return the inference from the specified endpoint.\n",
    "        Args:\n",
    "            data (object): Input data for which you want the model to provide\n",
    "                inference. If a serializer was specified when creating the\n",
    "                Predictor, the result of the serializer is sent as input\n",
    "                data. Otherwise the data must be sequence of bytes, and the\n",
    "                predict method then sends the bytes in the request body as is.\n",
    "            initial_args (dict[str,str]): Optional. Default arguments for boto3\n",
    "                ``invoke_endpoint`` call. Default is None (no default\n",
    "                arguments).\n",
    "            target_model (str): S3 model artifact path to run an inference request on,\n",
    "                in case of a multi model endpoint. Does not apply to endpoints hosting\n",
    "                single model (Default: None)\n",
    "            target_variant (str): The name of the production variant to run an inference\n",
    "                request on (Default: None). Note that the ProductionVariant identifies the\n",
    "                model you want to host and the resources you want to deploy for hosting it.\n",
    "            inference_id (str): If you provide a value, it is added to the captured data\n",
    "                when you enable data capture on the endpoint (Default: None).\n",
    "        Returns:\n",
    "            object: Inference for the given input. If a deserializer was specified when creating\n",
    "                the Predictor, the result of the deserializer is\n",
    "                returned. Otherwise the response returns the sequence of bytes\n",
    "                as is.\n",
    "        \"\"\"\n",
    "\n",
    "        request_args = self._create_request_args(\n",
    "            data, initial_args, target_model, target_variant, inference_id\n",
    "        )\n",
    "        response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n",
    "        return self._handle_response(response)\n",
    "```\n",
    "\n",
    "so `sagemaker_session.sagemaker_runtime_client` invokes the endpint\n",
    "\n",
    "I searched `sagemaker_runtime` in session.py\n",
    "\n",
    "```python\n",
    "def _initialize(\n",
    "        self,\n",
    "        boto_session,\n",
    "        sagemaker_client,\n",
    "        sagemaker_runtime_client,\n",
    "        sagemaker_featurestore_runtime_client,\n",
    "    ):\n",
    "        \"\"\"Initialize this SageMaker Session.\n",
    "        Creates or uses a boto_session, sagemaker_client and sagemaker_runtime_client.\n",
    "        Sets the region_name.\n",
    "        \"\"\"\n",
    "        self.boto_session = boto_session or boto3.DEFAULT_SESSION or boto3.Session()\n",
    "\n",
    "        self._region_name = self.boto_session.region_name\n",
    "        if self._region_name is None:\n",
    "            raise ValueError(\n",
    "                \"Must setup local AWS configuration with a region supported by SageMaker.\"\n",
    "            )\n",
    "\n",
    "        self.sagemaker_client = sagemaker_client or self.boto_session.client(\"sagemaker\")\n",
    "        prepend_user_agent(self.sagemaker_client)\n",
    "\n",
    "        if sagemaker_runtime_client is not None:\n",
    "            self.sagemaker_runtime_client = sagemaker_runtime_client\n",
    "        else:\n",
    "            config = botocore.config.Config(read_timeout=80)\n",
    "            self.sagemaker_runtime_client = self.boto_session.client(\n",
    "                \"runtime.sagemaker\", config=config\n",
    "            )\n",
    "\n",
    "        prepend_user_agent(self.sagemaker_runtime_client)\n",
    "\n",
    "        if sagemaker_featurestore_runtime_client:\n",
    "            self.sagemaker_featurestore_runtime_client = sagemaker_featurestore_runtime_client\n",
    "        else:\n",
    "            self.sagemaker_featurestore_runtime_client = self.boto_session.client(\n",
    "                \"sagemaker-featurestore-runtime\"\n",
    "            )\n",
    "\n",
    "        self.local_mode = False\n",
    " \n",
    "```\n",
    "\n",
    "This led to investigate [`SageMakerRuntime` client in boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html)\n",
    "\n",
    "\n",
    "```\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName='string',\n",
    "    Body=b'bytes'|file,\n",
    "    ContentType='string',\n",
    "    Accept='string',\n",
    "    CustomAttributes='string',\n",
    "    TargetModel='string',\n",
    "    TargetVariant='string',\n",
    "    TargetContainerHostname='string',\n",
    "    InferenceId='string'\n",
    ")\n",
    "```\n",
    "\n",
    "To figure out how content type looks like, I looked up [`serializers.py`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/serializers.py) and [`deserializers.py`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/deserializers.py)\n",
    "\n",
    "\n",
    "Why is it called SageMaker runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to serialize csv\n",
    "\n",
    "import csv\n",
    "import io\n",
    "\n",
    "def _serialize_row(data):\n",
    "    \"\"\"Serialize data as a CSV-formatted row.\n",
    "    Args:\n",
    "        data (sting): Data to be serialized in a row.\n",
    "    Returns:\n",
    "        str: The data serialized as a CSV-formatted row.\n",
    "    \"\"\"\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = np.ndarray.flatten(data)\n",
    "\n",
    "    if hasattr(data, \"__len__\"):\n",
    "        if len(data) == 0:\n",
    "            raise ValueError(\"Cannot serialize empty array\")\n",
    "        csv_buffer = io.StringIO()\n",
    "        csv_writer = csv.writer(csv_buffer, delimiter=\",\")\n",
    "        csv_writer.writerow(data)\n",
    "        return csv_buffer.getvalue().rstrip(\"\\r\\n\")\n",
    "        \n",
    "csv_buffer = io.StringIO()\n",
    "csv_writer = csv.writer(csv_buffer, delimiter=',')\n",
    "csv_writer.writerow('xyze')\n",
    "v = csv_buffer.getvalue().rstrip('\\r\\n')\n",
    "\n",
    "print(v.rstrip(\"\\r\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '94053c6a-3290-4562-9089-d78dd65e9a77', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '94053c6a-3290-4562-9089-d78dd65e9a77', 'x-amzn-invoked-production-variant': 'AMeaningfulProdVarName', 'date': 'Fri, 05 Mar 2021 03:10:19 GMT', 'content-type': 'text/csv; charset=utf-8', 'content-length': '26'}, 'RetryAttempts': 0}, 'ContentType': 'text/csv; charset=utf-8', 'InvokedProductionVariant': 'AMeaningfulProdVarName', 'Body': <botocore.response.StreamingBody object at 0x7f46e9c84d68>}\n"
     ]
    }
   ],
   "source": [
    "# invoke endpoint\n",
    "import json\n",
    "\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "body=json.dumps('a json string')\n",
    "\n",
    "# the model only supports csv data (look at how the model is defined in container/predictor.py)\n",
    "content_type='text/csv'\n",
    "# see the cell below for serializing a string into csv format\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "csv_writer = csv.writer(csv_buffer, delimiter=',')\n",
    "csv_writer.writerow('xyze')\n",
    "body = csv_buffer.getvalue().rstrip('\\r\\n')\n",
    "\n",
    "# respnse type is also text/csv (look at decision_tree/predictor.py)\n",
    "accept='text/csv'\n",
    "\n",
    "res=sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=body,                # encoded input data\n",
    "    ContentType=content_type, # I told the endpoint what's the encode\n",
    "    Accept=accept             # I told the endpoint how I wish to decode its response\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to decode json string\n",
    "\n",
    "import codecs\n",
    "\n",
    "class SimpleBaseDeserializer:\n",
    "    pass\n",
    "\n",
    "class JSONDeserializer(SimpleBaseDeserializer):\n",
    "    \"\"\"Deserialize JSON data from an inference endpoint into a Python object.\"\"\"\n",
    "\n",
    "    def __init__(self, accept=\"application/json\"):\n",
    "        \"\"\"Initialize a ``JSONDeserializer`` instance.\n",
    "        Args:\n",
    "            accept (union[str, tuple[str]]): The MIME type (or tuple of allowable MIME types) that\n",
    "                is expected from the inference endpoint (default: \"application/json\").\n",
    "        \"\"\"\n",
    "        super(JSONDeserializer, self).__init__(accept=accept)\n",
    "\n",
    "    def deserialize(self, stream, content_type):\n",
    "        \"\"\"Deserialize JSON data from an inference endpoint into a Python object.\n",
    "        Args:\n",
    "            stream (botocore.response.StreamingBody): Data to be deserialized.\n",
    "            content_type (str): The MIME type of the data.\n",
    "        Returns:\n",
    "            object: The JSON-formatted data deserialized into a Python object.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json.load(codecs.getreader(\"utf-8\")(stream))\n",
    "        finally:\n",
    "            stream.close()\n",
    "            \n",
    "            \n",
    "\n",
    "class CSVDeserializer(SimpleBaseDeserializer):\n",
    "    \"\"\"Deserialize a stream of bytes into a list of lists.\n",
    "    Consider using :class:~`sagemaker.deserializers.NumpyDeserializer` or\n",
    "    :class:~`sagemaker.deserializers.PandasDeserializer` instead, if you'd like to convert text/csv\n",
    "    responses directly into other data types.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding=\"utf-8\", accept=\"text/csv\"):\n",
    "        \"\"\"Initialize a ``CSVDeserializer`` instance.\n",
    "        Args:\n",
    "            encoding (str): The string encoding to use (default: \"utf-8\").\n",
    "            accept (union[str, tuple[str]]): The MIME type (or tuple of allowable MIME types) that\n",
    "                is expected from the inference endpoint (default: \"text/csv\").\n",
    "        \"\"\"\n",
    "        super(CSVDeserializer, self).__init__(accept=accept)\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def deserialize(self, stream, content_type):\n",
    "        \"\"\"Deserialize data from an inference endpoint into a list of lists.\n",
    "        Args:\n",
    "            stream (botocore.response.StreamingBody): Data to be deserialized.\n",
    "            content_type (str): The MIME type of the data.\n",
    "        Returns:\n",
    "            list: The data deserialized into a list of lists representing the\n",
    "                contents of a CSV file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            decoded_string = stream.read().decode(self.encoding)\n",
    "            return list(csv.reader(decoded_string.splitlines()))\n",
    "        finally:\n",
    "            stream.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a wonderful potato'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode response\n",
    "res_body = res['Body']\n",
    "res_body.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'HTTPHeaders': {'content-length': '0',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 03:16:19 GMT',\n",
      "                                      'x-amzn-requestid': '42acf2c7-6c9e-4228-99cf-21e7bf684c4e'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '42acf2c7-6c9e-4228-99cf-21e7bf684c4e',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "del_res = sm_boto3.delete_endpoint(\n",
    "    EndpointName=endpoint_name)\n",
    "pp.pprint(del_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Endpoint \n",
    "\n",
    "In this notebook, you will learn basics about hosting your trained model on Amazon SageMaker for inference. There are two ways you can use Amazon SageMaker for inference:\n",
    "1. Set up persistent endpoint for real-time online inference\n",
    "2. Gather data to be predicted in batch and use SageMaker batch transform for offline inference. \n",
    "\n",
    "In this notebook, we focus on the first option and we will discuss batch transform in another notebook. \n",
    "\n",
    "You are highly recommeneded to go through [the section on model deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html) in the official docs before moving on.\n",
    "\n",
    "\n",
    "The pricing for setting up an endpoint can be found [here](https://aws.amazon.com/sagemaker/pricing/)\n",
    "\n",
    "Like a [CreateTrainingJob](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create-training-job.ipynb), Amazon SageMaker interacts with your inference logic via a containerized enviornment. \n",
    "\n",
    "The following APIs are relavent:\n",
    "* [`CreateModel`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model)\n",
    "* [`CreateEndpointConfig`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config)\n",
    "* [`CreateEndpoint`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint)\n",
    "\n",
    "You are highly recommended to go through them. It's okay if you don't understand everything, we will go through them in detail in this notebook. \n",
    "\n",
    "The outline of this notebook is:\n",
    "1. Build an inference image\n",
    "2. Test the inference image / container locally and push it to ECR\n",
    "3. Use the ECR address of the inference container to define a model by calling `CreateModel`\n",
    "4. Specify configuration of an endpoint by calling `CreateEndpointConfig`\n",
    "5. Use model definition from 3 and endpoint configuration from 4 to create an endpoint by calling `CreateEndpoint`\n",
    "6. Invoke the endpoint by using SageMaker runtime client "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an inference image\n",
    "\n",
    "You inference image must be a self-contained web server. When you run your inference container locally, it should listen on port 8080 and accept POST requests to the `/invocations` endpoint. The payload of the POST requests is the content of the data that you want your model to predict. Since the inference container is essentially a web server, you should expect it to look differently from the container we used for [`CreateTrainingJob`](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create-training-job.ipynb). \n",
    "\n",
    "In this notebook, we use a minimal python stack to build our web server:\n",
    "![Request serving stack](stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@TODO \n",
    "Go through this tutorial\n",
    "https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-uswgi-and-nginx-on-ubuntu-18-04\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an overview of ngnix, WSGI and Flask server checkout their homepages\n",
    "* [nginx](https://www.nginx.com/resources/wiki/start/)\n",
    "* [WSGI](https://gunicorn.org/)\n",
    "* [Flask](https://flask.palletsprojects.com/en/1.1.x/)\n",
    "\n",
    "SageMaker runs your container like\n",
    "\n",
    "```sh\n",
    "docker run <image> serve\n",
    "```\n",
    "\n",
    "This means you need to have an executable called `serve` in the `PATH`. In this notebook, we will create a python script as an **executable** and put it in the working directory of the docker image. \n",
    "        \n",
    "The folder `container/src` contains the configs and entry point of the web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nginx.conf  predictor.py  serve  wsgi.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls  container/src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`serve` is a python executable that is intended to be used as the entrypoint for the inference image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\r\n",
      "# This file implements the scoring service shell. You don't necessarily need to modify it for various\r\n",
      "# algorithms. It starts nginx and gunicorn with the correct configurations and then simply waits until\r\n",
      "# gunicorn exits.\r\n",
      "#\r\n",
      "# The flask server is specified to be the app object in wsgi.py\r\n",
      "#\r\n",
      "# We set the following parameters:\r\n",
      "#\r\n",
      "# Parameter                Environment Variable              Default Value\r\n",
      "# ---------                --------------------              -------------\r\n",
      "# number of workers        MODEL_SERVER_WORKERS              the number of CPU cores\r\n",
      "# timeout                  MODEL_SERVER_TIMEOUT              60 seconds\r\n",
      "\r\n",
      "import multiprocessing\r\n",
      "import os\r\n",
      "import signal\r\n",
      "import subprocess\r\n",
      "import sys\r\n",
      "\r\n",
      "cpu_count = multiprocessing.cpu_count()\r\n",
      "\r\n",
      "model_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\r\n",
      "model_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\r\n",
      "\r\n",
      "def sigterm_handler(nginx_pid, gunicorn_pid):\r\n",
      "    try:\r\n",
      "        os.kill(nginx_pid, signal.SIGQUIT)\r\n",
      "    except OSError:\r\n",
      "        pass\r\n",
      "    try:\r\n",
      "        os.kill(gunicorn_pid, signal.SIGTERM)\r\n",
      "    except OSError:\r\n",
      "        pass\r\n",
      "\r\n",
      "    sys.exit(0)\r\n",
      "\r\n",
      "def start_server():\r\n",
      "    print('Starting the inference server with {} workers.'.format(model_server_workers))\r\n",
      "\r\n",
      "    # link the log streams to stdout/err so they will be logged to the container logs\r\n",
      "    subprocess.check_call(['ln', '-sf', '/dev/stdout', '/var/log/nginx/access.log'])\r\n",
      "    subprocess.check_call(['ln', '-sf', '/dev/stderr', '/var/log/nginx/error.log'])\r\n",
      "\r\n",
      "    nginx = subprocess.Popen(['nginx', '-c', '/opt/program/nginx.conf'])\r\n",
      "    gunicorn = subprocess.Popen(['gunicorn',\r\n",
      "                                 '--timeout', str(model_server_timeout),\r\n",
      "                                 '-k', 'sync',\r\n",
      "                                 '-b', 'unix:/tmp/gunicorn.sock',\r\n",
      "                                 '-w', str(model_server_workers),\r\n",
      "                                 'wsgi:app'])\r\n",
      "\r\n",
      "    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\r\n",
      "\r\n",
      "    # If either subprocess exits, so do we.\r\n",
      "    pids = set([nginx.pid, gunicorn.pid])\r\n",
      "    while True:\r\n",
      "        pid, _ = os.wait()\r\n",
      "        if pid in pids:\r\n",
      "            break\r\n",
      "\r\n",
      "    sigterm_handler(nginx.pid, gunicorn.pid)\r\n",
      "    print('Inference server exiting')\r\n",
      "\r\n",
      "# The main routine just invokes the start function.\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    start_server()\r\n"
     ]
    }
   ],
   "source": [
    "!cat container/src/serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nginx.conf` is the config file for the nginx server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker_processes 1;\r\n",
      "daemon off; # Prevent forking\r\n",
      "\r\n",
      "\r\n",
      "pid /tmp/nginx.pid;\r\n",
      "error_log /var/log/nginx/error.log;\r\n",
      "\r\n",
      "events {\r\n",
      "  # defaults\r\n",
      "}\r\n",
      "\r\n",
      "http {\r\n",
      "  include /etc/nginx/mime.types;\r\n",
      "  default_type application/octet-stream;\r\n",
      "  access_log /var/log/nginx/access.log combined;\r\n",
      "  \r\n",
      "  upstream gunicorn {\r\n",
      "    server unix:/tmp/gunicorn.sock;\r\n",
      "  }\r\n",
      "\r\n",
      "  server {\r\n",
      "    listen 8080 deferred;\r\n",
      "    client_max_body_size 5m;\r\n",
      "\r\n",
      "    keepalive_timeout 5;\r\n",
      "    proxy_read_timeout 1200s;\r\n",
      "\r\n",
      "    location ~ ^/(ping|invocations) {\r\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\r\n",
      "      proxy_set_header Host $http_host;\r\n",
      "      proxy_redirect off;\r\n",
      "      proxy_pass http://gunicorn;\r\n",
      "    }\r\n",
      "\r\n",
      "    location / {\r\n",
      "      return 404 \"{}\";\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat container/src/nginx.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source code "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
