{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST training with MXNet and Gluon\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train and test an MNIST model on SageMaker using MXNet and the Gluon API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "output_path='s3://' + sess.default_bucket() + '/mxnet/mnist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXNet Estimator\n",
    "\n",
    "The `MXNet` class allows you to run your training script on SageMaker\n",
    "infrastracture in a containerized environment. In this notebook, we\n",
    "refer to this container as *training container*. \n",
    "\n",
    "You need to configure\n",
    "it with the following parameters to set up the environment:\n",
    "\n",
    "- entry_point: A user defined python file to be used by the training container as the \n",
    "instructions for training. We further discuss this file in the next subsection.\n",
    "\n",
    "- role: An IAM role to make AWS service requests\n",
    "\n",
    "- instance_type: The type of SageMaker instance to run your training script. \n",
    "Set it to `local` if you want to run the training job on \n",
    "the SageMaker instance you are using to run this notebook\n",
    "\n",
    "- instance count: The number of instances you need to run your training job. \n",
    "Multiple instances are needed for distributed training.\n",
    "\n",
    "- output_path: \n",
    "S3 bucket URI to save training output (model artifacts and output files)\n",
    "\n",
    "- framework_version: The version of MXNet you need to use.\n",
    "\n",
    "- py_version: The python version you need to use\n",
    "\n",
    "For more information, see [the API reference](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the entry point for training\n",
    "\n",
    "The entry point for training is a python script that provides all \n",
    "the code for training a MXNet model. It is used by the SageMaker \n",
    "MXNet Estimator (`MXNet` class above) as the entry point for running the training job.\n",
    "\n",
    "Under the hood, SageMaker MXNet Estimator downloads a docker image\n",
    "with runtime environemnts \n",
    "specified by the parameters you used to initiated the\n",
    "estimator class and it injects the training script into the \n",
    "docker image to be used as the entry point to run the container.\n",
    "\n",
    "In the rest of the notebook, we use *training image* to refer to the \n",
    "docker image specified by the MXNet Estimator and *training container*\n",
    "to refer to the container that runs the training image. \n",
    "\n",
    "This means your training script is very similar to a training script\n",
    "you might run outside Amazon SageMaker, but it can access the useful environment \n",
    "variables provided by the training image. Checkout [the short list of environment variables provided by the SageMaker service](https://sagemaker.readthedocs.io/en/stable/frameworks/mxnet/using_mxnet.html?highlight=entry%20point) to see some common environment \n",
    "variables you might used. Checkout [the complete list of environment variables](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md) for a complete \n",
    "description of all environment variables your training script\n",
    "can access to. \n",
    "\n",
    "In this example, we use the training script `code/train.py`\n",
    "as the entry point for our MXNet Estimator.\n",
    "\n",
    "The script here is an adaptation of the [Gluon MNIST example](https://github.com/apache/incubator-mxnet/blob/master/example/gluon/mnist.py) provided by the [Apache MXNet](https://mxnet.incubator.apache.org/) project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize 'code/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters\n",
    "\n",
    "In addition, MXNet estimator allows you to parse command line arguments\n",
    "to your training script via `hyperparameters`.\n",
    "\n",
    "<span style=\"color:red\"> Note: local mode is not supported in SageMaker Studio </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set local_mode to be True if you want to run the training script\n",
    "# on the machine that runs this notebook\n",
    "\n",
    "local_mode=True\n",
    "\n",
    "if local_mode:\n",
    "    instance_type='local'\n",
    "else:\n",
    "    instance_type='ml.c4.xlarge'\n",
    "    \n",
    "est = MXNet(\n",
    "    entry_point='train.py',\n",
    "    source_dir='code', # directory of your training script\n",
    "    role=role,\n",
    "    framework_version='1.7.0',\n",
    "    py_version='py3',\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        'batch-size':100,\n",
    "        'epochs':20,\n",
    "        'learning-rate': 0.1,\n",
    "        'momentum': 0.9,\n",
    "        'log-interval':100\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training container executes your training script like\n",
    "\n",
    "```\n",
    "python train.py --batch-size 100 --epochs 20 --learning-rate 0.1\n",
    "    --momentum 0.9 --log-interval 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up channels for training and testing data\n",
    "\n",
    "You need to tell `MXNet` estimator where to find your training and \n",
    "testing data. It can be a link to an S3 bucket or it can be a path\n",
    "in your local file system if you use local mode. In this example,\n",
    "we download the MNIST data from a public S3 bucket and upload it \n",
    "to your default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Download training and testing data from a public S3 bucket\n",
    "\n",
    "\n",
    "def download_from_s3(data_dir='/tmp/data', train=True):\n",
    "    \"\"\"Download MNIST dataset and convert it to numpy array\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): directory to save the data\n",
    "        train (bool): download training set\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get global config\n",
    "    with open('code/config.json', 'r') as f:\n",
    "        CONFIG=json.load(f)\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    if train:\n",
    "        images_file = \"train-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "    else:\n",
    "        images_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    # download objects\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket = CONFIG['public_bucket']\n",
    "    for obj in [images_file, labels_file]:\n",
    "        key = os.path.join(\"datasets/image/MNIST\", obj)\n",
    "        dest = os.path.join(data_dir, obj)\n",
    "        if not os.path.exists(dest):\n",
    "            s3.download_file(bucket, key, dest)\n",
    "    return\n",
    "\n",
    "\n",
    "download_from_s3('/tmp/data', True)\n",
    "download_from_s3('/tmp/data', False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to the default bucket\n",
    "\n",
    "prefix = 'mnist'\n",
    "bucket = sess.default_bucket()\n",
    "loc = sess.upload_data(path='/tmp/data', bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "channels = {\n",
    "    \"training\": loc,\n",
    "    \"testing\": loc\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys of the dictionary `channels` are parsed to the training image\n",
    "and it creates the environment variable `SM_CHANNEL_<key name>`. \n",
    "\n",
    "In this example, `SM_CHANNEL_TRAINING` and `SM_CHANNEL_TESTING` are created in the training image (checkout \n",
    "how `code/train.py` access these variables). For more information,\n",
    "see: [SM_CHANNEL_{channel_name}](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name)\n",
    "\n",
    "If you want, you can create a channel for validation:\n",
    "```\n",
    "channels = {\n",
    "    'training': train_data_loc,\n",
    "    'validation': val_data_loc,\n",
    "    'test': test_data_loc\n",
    "    }\n",
    "```\n",
    "You can then access this channel within your training script via\n",
    "`SM_CHANNEL_VALIDATION`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training script on SageMaker\n",
    "Now, the training container has everything to execute your training\n",
    "script. You can start the container by calling `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the short-lived AWS credentials found in session. They might expire while running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpst7ygajt_algo-1-ullhf_1 ... \n",
      "\u001b[1BAttaching to tmpst7ygajt_algo-1-ullhf_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m 2020-11-24 22:55:57,590 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m 2020-11-24 22:55:57,592 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m 2020-11-24 22:55:57,603 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1-ullhf\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9}', 'SM_USER_ENTRY_POINT': 'train.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1-ullhf\",\"hosts\":[\"algo-1-ullhf\"]}', 'SM_INPUT_DATA_CONFIG': '{\"testing\":{\"TrainingInputMode\":\"File\"},\"training\":{\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"testing\",\"training\"]', 'SM_CURRENT_HOST': 'algo-1-ullhf', 'SM_MODULE_NAME': 'train', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '8', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-688520471316/mxnet-training-2020-11-24-22-55-49-625/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-ullhf\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1-ullhf\"],\"hyperparameters\":{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"TrainingInputMode\":\"File\"},\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-11-24-22-55-49-625\",\"log_level\":20,\"master_hostname\":\"algo-1-ullhf\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-688520471316/mxnet-training-2020-11-24-22-55-49-625/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-ullhf\",\"hosts\":[\"algo-1-ullhf\"]},\"user_entry_point\":\"train.py\"}', 'SM_USER_ARGS': '[\"--batch-size\",\"100\",\"--epochs\",\"20\",\"--learning-rate\",\"0.1\",\"--log-interval\",\"100\",\"--momentum\",\"0.9\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'SM_CHANNEL_TESTING': '/opt/ml/input/data/testing', 'SM_HP_BATCH-SIZE': '100', 'SM_HP_EPOCHS': '20', 'SM_HP_LEARNING-RATE': '0.1', 'SM_HP_MOMENTUM': '0.9', 'SM_HP_LOG-INTERVAL': '100'}\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m 2020-11-24 22:55:57,629 botocore.credentials INFO     Found credentials in environment variables.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m 2020-11-24 22:55:57,752 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m 2020-11-24 22:55:57,765 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m 2020-11-24 22:55:57,778 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m 2020-11-24 22:55:57,789 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"training\": \"/opt/ml/input/data/training\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"testing\": \"/opt/ml/input/data/testing\"\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"current_host\": \"algo-1-ullhf\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"algo-1-ullhf\"\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"batch-size\": 100,\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"epochs\": 20,\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"learning-rate\": 0.1,\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"momentum\": 0.9,\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"log-interval\": 100\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"training\": {\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"testing\": {\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"job_name\": \"mxnet-training-2020-11-24-22-55-49-625\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"master_hostname\": \"algo-1-ullhf\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-west-2-688520471316/mxnet-training-2020-11-24-22-55-49-625/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"num_cpus\": 8,\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"current_host\": \"algo-1-ullhf\",\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m             \"algo-1-ullhf\"\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_HOSTS=[\"algo-1-ullhf\"]\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_HPS={\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9}\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-ullhf\",\"hosts\":[\"algo-1-ullhf\"]}\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"testing\":{\"TrainingInputMode\":\"File\"},\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_CHANNELS=[\"testing\",\"training\"]\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_CURRENT_HOST=algo-1-ullhf\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_NUM_CPUS=8\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-west-2-688520471316/mxnet-training-2020-11-24-22-55-49-625/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-ullhf\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1-ullhf\"],\"hyperparameters\":{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"TrainingInputMode\":\"File\"},\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-11-24-22-55-49-625\",\"log_level\":20,\"master_hostname\":\"algo-1-ullhf\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-688520471316/mxnet-training-2020-11-24-22-55-49-625/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-ullhf\",\"hosts\":[\"algo-1-ullhf\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_USER_ARGS=[\"--batch-size\",\"100\",\"--epochs\",\"20\",\"--learning-rate\",\"0.1\",\"--log-interval\",\"100\",\"--momentum\",\"0.9\"]\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_CHANNEL_TESTING=/opt/ml/input/data/testing\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_HP_BATCH-SIZE=100\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_HP_EPOCHS=20\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_HP_LEARNING-RATE=0.1\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_HP_MOMENTUM=0.9\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m SM_HP_LOG-INTERVAL=100\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m /usr/local/bin/python3.6 train.py --batch-size 100 --epochs 20 --learning-rate 0.1 --log-interval 100 --momentum 0.9\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 0 Batch 100] Training: accuracy=0.797525, 31020.664152 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 0 Batch 200] Training: accuracy=0.860697, 30961.127925 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 0 Batch 300] Training: accuracy=0.885183, 30610.888921 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 0 Batch 400] Training: accuracy=0.899302, 31025.253347 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 0 Batch 500] Training: accuracy=0.909880, 31607.415222 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 0] Training: accuracy=0.917167\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 0] Validation: accuracy=0.961717\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m INFO:root:Saving the model, params and optimizer state.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 1 Batch 100] Training: accuracy=0.963366, 35848.752137 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 1 Batch 200] Training: accuracy=0.964080, 30466.361589 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 1 Batch 300] Training: accuracy=0.963721, 30364.902628 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 1 Batch 400] Training: accuracy=0.964738, 30137.989509 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 1 Batch 500] Training: accuracy=0.965389, 28803.076500 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 1] Training: accuracy=0.966133\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 1] Validation: accuracy=0.977650\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m INFO:root:Saving the model, params and optimizer state.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 2 Batch 100] Training: accuracy=0.975842, 30135.824113 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 2 Batch 200] Training: accuracy=0.974826, 30402.319513 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 2 Batch 300] Training: accuracy=0.974419, 30244.476493 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 2 Batch 400] Training: accuracy=0.974065, 29950.756927 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 2 Batch 500] Training: accuracy=0.973972, 29818.740225 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 2] Training: accuracy=0.974333\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 2] Validation: accuracy=0.979017\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m INFO:root:Saving the model, params and optimizer state.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 3 Batch 100] Training: accuracy=0.979802, 30466.361589 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 3 Batch 200] Training: accuracy=0.981741, 28435.959322 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 3 Batch 300] Training: accuracy=0.980864, 35424.864865 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 3 Batch 400] Training: accuracy=0.980249, 35391.983799 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 3 Batch 500] Training: accuracy=0.979182, 30248.838886 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 3] Training: accuracy=0.979300\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 3] Validation: accuracy=0.985317\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m INFO:root:Saving the model, params and optimizer state.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 4 Batch 100] Training: accuracy=0.984752, 34365.456780 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 4 Batch 200] Training: accuracy=0.984328, 29114.979869 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 4 Batch 300] Training: accuracy=0.983389, 34441.648875 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 4 Batch 400] Training: accuracy=0.982369, 33280.203126 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 4 Batch 500] Training: accuracy=0.982255, 33835.947080 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 4] Training: accuracy=0.981733\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 4] Validation: accuracy=0.987083\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m INFO:root:Saving the model, params and optimizer state.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 5 Batch 100] Training: accuracy=0.986139, 34949.620865 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 5 Batch 200] Training: accuracy=0.987960, 35054.776431 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 5 Batch 300] Training: accuracy=0.987508, 29751.056888 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 5 Batch 400] Training: accuracy=0.985761, 31314.797671 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 5 Batch 500] Training: accuracy=0.985309, 35216.658270 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 5] Training: accuracy=0.985033\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 5] Validation: accuracy=0.986633\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 6 Batch 100] Training: accuracy=0.988812, 28424.396856 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 6 Batch 200] Training: accuracy=0.988856, 28119.495843 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 6 Batch 300] Training: accuracy=0.988837, 28343.722125 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 6 Batch 400] Training: accuracy=0.988479, 29448.178052 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 6 Batch 500] Training: accuracy=0.988443, 29721.541950 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 6] Training: accuracy=0.988033\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 6] Validation: accuracy=0.986900\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 7 Batch 100] Training: accuracy=0.989505, 34868.268351 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 7 Batch 200] Training: accuracy=0.988905, 34549.456343 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 7 Batch 300] Training: accuracy=0.988937, 30497.375118 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 7 Batch 400] Training: accuracy=0.988229, 30484.075878 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 7 Batch 500] Training: accuracy=0.988343, 30506.247727 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 7] Training: accuracy=0.988150\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 7] Validation: accuracy=0.989350\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m INFO:root:Saving the model, params and optimizer state.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 8 Batch 100] Training: accuracy=0.991485, 29558.167724 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 8 Batch 200] Training: accuracy=0.990498, 29535.272164 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 8 Batch 300] Training: accuracy=0.990033, 29627.067882 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 8 Batch 400] Training: accuracy=0.990299, 29566.502185 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 8 Batch 500] Training: accuracy=0.989960, 26007.961803 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 8] Training: accuracy=0.989417\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 8] Validation: accuracy=0.991600\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m INFO:root:Saving the model, params and optimizer state.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 9 Batch 100] Training: accuracy=0.989703, 30373.698313 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 9 Batch 200] Training: accuracy=0.988955, 34643.627653 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 9 Batch 300] Training: accuracy=0.989468, 35380.042176 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 9 Batch 400] Training: accuracy=0.989227, 29711.015088 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 9 Batch 500] Training: accuracy=0.989621, 35054.776431 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 9] Training: accuracy=0.989633\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 9] Validation: accuracy=0.991767\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m INFO:root:Saving the model, params and optimizer state.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 10 Batch 100] Training: accuracy=0.994158, 33565.172855 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 10 Batch 200] Training: accuracy=0.993781, 33589.364940 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 10 Batch 300] Training: accuracy=0.993389, 34337.322964 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 10 Batch 400] Training: accuracy=0.993117, 34538.076416 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 10 Batch 500] Training: accuracy=0.992575, 35261.067675 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 10] Training: accuracy=0.992233\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 10] Validation: accuracy=0.994133\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m INFO:root:Saving the model, params and optimizer state.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 11 Batch 100] Training: accuracy=0.995347, 35472.801083 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 11 Batch 200] Training: accuracy=0.994328, 30068.850814 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 11 Batch 300] Training: accuracy=0.993588, 28029.296979 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 11 Batch 400] Training: accuracy=0.992269, 29308.252393 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 11 Batch 500] Training: accuracy=0.992754, 34958.359727 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 11] Training: accuracy=0.992383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 11] Validation: accuracy=0.991233\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 12 Batch 100] Training: accuracy=0.991980, 34515.339039 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 12 Batch 200] Training: accuracy=0.991343, 34993.358919 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 12 Batch 300] Training: accuracy=0.991561, 34787.293688 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 12 Batch 400] Training: accuracy=0.991746, 35234.408602 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 12 Batch 500] Training: accuracy=0.991936, 35022.578490 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 12] Training: accuracy=0.991650\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 12] Validation: accuracy=0.992250\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 13 Batch 100] Training: accuracy=0.991386, 34669.399901 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 13 Batch 200] Training: accuracy=0.992239, 29110.938368 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 13 Batch 300] Training: accuracy=0.993256, 33500.830671 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 13 Batch 400] Training: accuracy=0.993591, 29343.108997 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 13 Batch 500] Training: accuracy=0.993693, 30240.115357 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 13] Training: accuracy=0.993783\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 13] Validation: accuracy=0.995900\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m INFO:root:Saving the model, params and optimizer state.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 14 Batch 100] Training: accuracy=0.995941, 30290.344479 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 14 Batch 200] Training: accuracy=0.995124, 34052.967443 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 14 Batch 300] Training: accuracy=0.995083, 31312.459873 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 14 Batch 400] Training: accuracy=0.994738, 34595.051138 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 14 Batch 500] Training: accuracy=0.994431, 35098.778243 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 14] Training: accuracy=0.994600\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 14] Validation: accuracy=0.997367\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m INFO:root:Saving the model, params and optimizer state.\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 15 Batch 100] Training: accuracy=0.998416, 35882.487809 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 15 Batch 200] Training: accuracy=0.997463, 30466.361589 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 15 Batch 300] Training: accuracy=0.996877, 30510.685968 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 15 Batch 400] Training: accuracy=0.996534, 29931.520731 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 15 Batch 500] Training: accuracy=0.996048, 30116.349537 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 15] Training: accuracy=0.995800\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 15] Validation: accuracy=0.993683\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 16 Batch 100] Training: accuracy=0.996238, 29721.541950 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 16 Batch 200] Training: accuracy=0.995970, 34075.099521 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 16 Batch 300] Training: accuracy=0.995282, 33522.250639 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 16 Batch 400] Training: accuracy=0.995112, 33664.852717 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 16 Batch 500] Training: accuracy=0.994950, 34261.591243 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 16] Training: accuracy=0.994317\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 16] Validation: accuracy=0.994067\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 17 Batch 100] Training: accuracy=0.992673, 30380.298421 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 17 Batch 200] Training: accuracy=0.993483, 27819.221331 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 17 Batch 300] Training: accuracy=0.993455, 33466.081545 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 17 Batch 400] Training: accuracy=0.993117, 34937.975843 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 17 Batch 500] Training: accuracy=0.992874, 34552.302496 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 17] Training: accuracy=0.992650\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 17] Validation: accuracy=0.990700\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 18 Batch 100] Training: accuracy=0.991782, 34940.886371 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 18 Batch 200] Training: accuracy=0.992587, 34706.694249 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 18 Batch 300] Training: accuracy=0.993621, 34635.045417 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 18 Batch 400] Training: accuracy=0.992868, 34683.734392 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 18 Batch 500] Training: accuracy=0.992615, 29664.785345 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 18] Training: accuracy=0.992433\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 18] Validation: accuracy=0.994550\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 19 Batch 100] Training: accuracy=0.993366, 32088.623671 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 19 Batch 200] Training: accuracy=0.993682, 34663.669421 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 19 Batch 300] Training: accuracy=0.993920, 29890.992018 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 19 Batch 400] Training: accuracy=0.994040, 30010.761305 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 19 Batch 500] Training: accuracy=0.994052, 29402.762005 samples/s\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 19] Training: accuracy=0.994133\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m [Epoch 19] Validation: accuracy=0.994950\n",
      "\u001b[36malgo-1-ullhf_1  |\u001b[0m 2020-11-24 22:56:57,174 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmpst7ygajt_algo-1-ullhf_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "est.fit(inputs=channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and store model data\n",
    "\n",
    "Now, the training is finished, the model artifact has been saved in \n",
    "the `output_path`. We "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact saved at:\n",
      " s3://sagemaker-us-west-2-688520471316/mxnet/mnist/mxnet-training-2020-11-24-22-55-49-625/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "mx_mnist_model_data = est.model_data\n",
    "print(\"Model artifact saved at:\\n\", mx_mnist_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the variable `model_data` in the current notebook kernel. \n",
    "In the [next notebook](get_started_with_mnist_deploy.ipynb), you will learn how to retrieve the model artifact and deploy to a SageMaker\n",
    "endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store mx_mnist_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and debug the entry point before executing the training container\n",
    "\n",
    "The entry point `code/train.py` provided here has been tested and it can be executed in the training container. \n",
    "When you do develop your own training script, it is a good practice to simulate the container environment \n",
    "in the local shell and test it before sending it to SageMaker, because debugging in a containerized environment\n",
    "is rather cumbersome. The following script shows how you can test your training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize code/test_train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
