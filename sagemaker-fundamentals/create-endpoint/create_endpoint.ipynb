{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  57.34kB\r",
      "\r\n",
      "Step 1/11 : FROM ubuntu:18.04\n",
      " ---> c090eaba6b94\n",
      "Step 2/11 : MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
      " ---> Using cache\n",
      " ---> dd1348b3a447\n",
      "Step 3/11 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          python3-pip          python3-setuptools          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 5d88b1be1abc\n",
      "Step 4/11 : RUN ln -s /usr/bin/python3 /usr/bin/python\n",
      " ---> Using cache\n",
      " ---> 6417ac44ded6\n",
      "Step 5/11 : RUN ln -s /usr/bin/pip3 /usr/bin/pip\n",
      " ---> Using cache\n",
      " ---> 09ada8830cc5\n",
      "Step 6/11 : RUN pip --no-cache-dir install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gunicorn\n",
      " ---> Using cache\n",
      " ---> 2885b6a01bd3\n",
      "Step 7/11 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> ab2910812f0d\n",
      "Step 8/11 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 0c0e511ac354\n",
      "Step 9/11 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 10fcf3254c9d\n",
      "Step 10/11 : COPY src /opt/program\n",
      " ---> 19cc7b5228a8\n",
      "Step 11/11 : WORKDIR /opt/program\n",
      " ---> Running in 087f86171bb4\n",
      "Removing intermediate container 087f86171bb4\n",
      " ---> 24252e02ab8b\n",
      "Successfully built 24252e02ab8b\n",
      "Successfully tagged example-serve:latest\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# build the image\n",
    "cd container/\n",
    "\n",
    "# tag it as example-image:latest\n",
    "docker build -t example-serve:latest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-64b3a2c76d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# wait for the process to terminate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout, endtime)\u001b[0m\n\u001b[1;32m   1475\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1477\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1478\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1422\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from subprocess import Popen\n",
    "import subprocess\n",
    "\n",
    "cmd = \"docker run -p 8080:8080 --rm example-serve:latest serve\"\n",
    "\n",
    "\n",
    "process = subprocess.Popen(cmd, shell=True,\n",
    "                           stdout=subprocess.PIPE, \n",
    "                           stderr=subprocess.PIPE,\n",
    "                           start_new_session=True\n",
    "                          )\n",
    "\n",
    "\n",
    "process.wait()\n",
    "\n",
    "# wait for the process to terminate\n",
    "out, err = process.communicate()\n",
    "errcode = process.returncode\n",
    "\n",
    "print(out.decode('utf-8'))\n",
    "print(process.pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system('ls .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "yield from wasn't used with future",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-36127ea21b0c>\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# must call decode because stdout is a bytes object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/asyncio/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_noop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         stdin, stdout, stderr = yield from tasks.gather(stdin, stdout, stderr,\n\u001b[0;32m--> 195\u001b[0;31m                                                         loop=self._loop)\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: yield from wasn't used with future"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "proc = await asyncio.create_subprocess_exec(\n",
    "    'ls','.',\n",
    "    stdout=asyncio.subprocess.PIPE,\n",
    "    stderr=asyncio.subprocess.PIPE)\n",
    "\n",
    "\n",
    "# if proc takes very long to complete, the CPUs are free to use cycles for \n",
    "# other processes\n",
    "#stdout, stderr = await proc.communicate()\n",
    "stdout, stderr = proc.communicate()\n",
    "proc.returncode\n",
    "# 0\n",
    "\n",
    "# must call decode because stdout is a bytes object\n",
    "stdout.decode()\n",
    "# total 24K\n",
    "# drwxrwxr-x  3 felipe felipe 4,0K Nov  4 17:52 .\n",
    "# drwxrwxr-x 39 felipe felipe 4,0K Nov  3 18:31 ..\n",
    "# drwxrwxr-x  2 felipe felipe 4,0K Nov  3 19:32 .ipynb_checkpoints\n",
    "# -rw-rw-r--  1 felipe felipe  11K Nov  4 17:52 main.ipynb\n",
    "\n",
    "stderr.decode()\n",
    "# ''  empty string   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decode() argument 1 must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-8dfcb4eecd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: decode() argument 1 must be str, not int"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(Popen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# run the inference container\n",
    "docker run -p 8080:8080 --rm example-serve:latest serve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 127.0.0.1:8080...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8080 (#0)\n",
      "> GET /ping HTTP/1.1\r\n",
      "> Host: localhost:8080\r\n",
      "> User-Agent: curl/7.68.0\r\n",
      "> Accept: */*\r\n",
      "> Content-Type: text/csv\r\n",
      "> \r\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\r\n",
      "< Server: nginx/1.14.0 (Ubuntu)\r\n",
      "< Date: Wed, 10 Mar 2021 02:11:47 GMT\r\n",
      "< Content-Type: application/json\r\n",
      "< Content-Length: 1\r\n",
      "< Connection: keep-alive\r\n",
      "< \r\n",
      "{ [1 bytes data]\n",
      "\r",
      "100     1  100     1    0     0   1000      0 --:--:-- --:--:-- --:--:--  1000\n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -H \"Content-Type: text/csv\" -v http://localhost:8080/ping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just take json, and I am fed with text/csv"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 127.0.0.1:8080...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8080 (#0)\n",
      "> POST /invocations HTTP/1.1\r\n",
      "> Host: localhost:8080\r\n",
      "> User-Agent: curl/7.68.0\r\n",
      "> Accept: */*\r\n",
      "> Content-Type: text/csv\r\n",
      "> Content-Length: 8\r\n",
      "> \r\n",
      "} [8 bytes data]\n",
      "* upload completely sent off: 8 out of 8 bytes\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\r\n",
      "< Server: nginx/1.14.0 (Ubuntu)\r\n",
      "< Date: Wed, 10 Mar 2021 02:15:02 GMT\r\n",
      "< Content-Type: text/plain; charset=utf-8\r\n",
      "< Content-Length: 44\r\n",
      "< Connection: keep-alive\r\n",
      "< \r\n",
      "{ [44 bytes data]\n",
      "\r",
      "100    52  100    44  100     8  14666   2666 --:--:-- --:--:-- --:--:-- 17333\n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl --data-binary @container/local_test/payload.csv -H \"Content-Type: text/csv\" -v http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'example-inference' already exists in the registry with id '688520471316'\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import pprint\n",
    "import os\n",
    "import time\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "iam = boto3.client('iam')\n",
    "ecr = boto3.client('ecr')\n",
    "\n",
    "image_name=\"example-inference\"\n",
    "\n",
    "try:\n",
    "    # The repository might already exist\n",
    "    # in your ECR\n",
    "    cr_res = ecr.create_repository(\n",
    "        repositoryName=image_name)\n",
    "    pp.pprint(cr_res)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "The push refers to repository [688520471316.dkr.ecr.us-west-2.amazonaws.com/example-inference]\n",
      "f4cd793a4ebc: Preparing\n",
      "b520e3bd5eba: Preparing\n",
      "3a3b4090fe28: Preparing\n",
      "5b850ff3c508: Preparing\n",
      "408c63ea099b: Preparing\n",
      "9f10818f1f96: Preparing\n",
      "27502392e386: Preparing\n",
      "c95d2191d777: Preparing\n",
      "27502392e386: Waiting\n",
      "c95d2191d777: Waiting\n",
      "3a3b4090fe28: Pushed\n",
      "f4cd793a4ebc: Pushed\n",
      "5b850ff3c508: Pushed\n",
      "9f10818f1f96: Pushed\n",
      "27502392e386: Pushed\n",
      "c95d2191d777: Pushed\n",
      "408c63ea099b: Pushed\n",
      "b520e3bd5eba: Pushed\n",
      "latest: digest: sha256:72907655e9cc8dd5a940f7c04932e66aee7890356d6df4bad7380979a362ae2c size: 1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "account=$(aws sts get-caller-identity --query Account | sed -e 's/^\"//' -e 's/\"$//')\n",
    "region=$(aws configure get region)\n",
    "ecr_account=${account}.dkr.ecr.${region}.amazonaws.com\n",
    "\n",
    "# Give docker your ECR login password\n",
    "aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $ecr_account\n",
    "\n",
    "# Fullname of the repo\n",
    "fullname=$ecr_account/example-inference:latest\n",
    "\n",
    "#echo $fullname\n",
    "# Tag the image with the fullname\n",
    "docker tag example-inference:latest $fullname\n",
    "\n",
    "# Push to ECR\n",
    "docker push $fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'HTTPHeaders': {'content-length': '404',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 00:42:52 GMT',\n",
      "                                      'x-amzn-requestid': '3073c552-5b39-4217-9895-6dee8f9260e3'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '3073c552-5b39-4217-9895-6dee8f9260e3',\n",
      "                      'RetryAttempts': 0},\n",
      " 'imageDetails': [{'artifactMediaType': 'application/vnd.docker.container.image.v1+json',\n",
      "                   'imageDigest': 'sha256:72907655e9cc8dd5a940f7c04932e66aee7890356d6df4bad7380979a362ae2c',\n",
      "                   'imageManifestMediaType': 'application/vnd.docker.distribution.manifest.v2+json',\n",
      "                   'imagePushedAt': datetime.datetime(2021, 3, 5, 0, 31, 31, tzinfo=tzlocal()),\n",
      "                   'imageSizeInBytes': 132463690,\n",
      "                   'imageTags': ['latest'],\n",
      "                   'registryId': '688520471316',\n",
      "                   'repositoryName': 'example-inference'}]}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the ECR repository\n",
    "repo_res = ecr.describe_images(\n",
    "    repositoryName='example-inference')\n",
    "pp.pprint(repo_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri=\"688520471316.dkr.ecr.us-west-2.amazonaws.com/example-inference:latest\"\n",
    "role_arn = 'arn:aws:iam::688520471316:role/sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelArn': 'arn:aws:sagemaker:us-west-2:688520471316:model/example-inference',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '79',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 00:46:16 GMT',\n",
      "                                      'x-amzn-requestid': '88ceba30-6a67-4c52-b2d9-71fef75121af'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '88ceba30-6a67-4c52-b2d9-71fef75121af',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "sm_boto3 = boto3.client('sagemaker')\n",
    "\n",
    "cm_res = sm_boto3.create_model(\n",
    "    ModelName='example-inference',\n",
    "    Containers=[\n",
    "        {\n",
    "            'Image': image_uri,\n",
    "   \n",
    "        },\n",
    "    ],\n",
    "    ExecutionRoleArn=role_arn,\n",
    "    EnableNetworkIsolation=False\n",
    ")\n",
    "\n",
    "pp.pprint(cm_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create endpoint config\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint\n",
    "model_name='example-inference'\n",
    "initial_instance_count=1\n",
    "instance_type='ml.t2.medium'\n",
    "variant_name = \"AMeaningfulProdVarName\" #^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n",
    "\n",
    "# why do we need it\n",
    "# think about a use case, where we need many variants\n",
    "production_variants = [\n",
    "    {\n",
    "        \"VariantName\": variant_name,\n",
    "        \"ModelName\": model_name,\n",
    "        \"InitialInstanceCount\": initial_instance_count,\n",
    "        \"InstanceType\": instance_type\n",
    "    }\n",
    "]\n",
    "\n",
    "endpoint_config_name = \"ExampleInferenceConfig\" #^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n",
    "\n",
    "endpoint_config = {\n",
    "    \"EndpointConfigName\": endpoint_config_name,\n",
    "    \"ProductionVariants\": production_variants,\n",
    "}\n",
    "\n",
    "ep_conf_res = sm_boto3.create_endpoint_config(**endpoint_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:688520471316:endpoint-config/exampleinferenceconfig',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '103',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 01:44:02 GMT',\n",
      "                                      'x-amzn-requestid': 'fd66804f-380a-478e-b3fb-b3f6296f0639'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'fd66804f-380a-478e-b3fb-b3f6296f0639',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(ep_conf_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create endpoint\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint\n",
    "\n",
    "\n",
    "endpoint_name='exmaple-endpoint'\n",
    "ep_res = sm_boto3.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndpointArn': 'arn:aws:sagemaker:us-west-2:688520471316:endpoint/exmaple-endpoint',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '84',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 01:53:40 GMT',\n",
      "                                      'x-amzn-requestid': '641e22b1-dc16-4d50-b615-86a857180def'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '641e22b1-dc16-4d50-b615-86a857180def',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(ep_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2021, 3, 5, 1, 53, 41, 572000, tzinfo=tzlocal()),\n",
      " 'EndpointArn': 'arn:aws:sagemaker:us-west-2:688520471316:endpoint/exmaple-endpoint',\n",
      " 'EndpointConfigName': 'ExampleInferenceConfig',\n",
      " 'EndpointName': 'exmaple-endpoint',\n",
      " 'EndpointStatus': 'Creating',\n",
      " 'LastModifiedTime': datetime.datetime(2021, 3, 5, 1, 53, 41, 572000, tzinfo=tzlocal()),\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '260',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 01:56:13 GMT',\n",
      "                                      'x-amzn-requestid': '285fb80a-96dc-4adc-aceb-385d729a0199'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '285fb80a-96dc-4adc-aceb-385d729a0199',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# describe endpoint\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.describe_endpoint\n",
    "\n",
    "ep_des_res = sm_boto3.describe_endpoint(\n",
    "    EndpointName=endpoint_name\n",
    ")\n",
    "\n",
    "pp.pprint(ep_des_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke endpoint \n",
    "\n",
    "```sh\n",
    "grep -rwn .  -e predict\n",
    "```\n",
    "\n",
    "```python\n",
    "def predict(\n",
    "        self, data, initial_args=None, target_model=None, target_variant=None, inference_id=None\n",
    "    ):\n",
    "        \"\"\"Return the inference from the specified endpoint.\n",
    "        Args:\n",
    "            data (object): Input data for which you want the model to provide\n",
    "                inference. If a serializer was specified when creating the\n",
    "                Predictor, the result of the serializer is sent as input\n",
    "                data. Otherwise the data must be sequence of bytes, and the\n",
    "                predict method then sends the bytes in the request body as is.\n",
    "            initial_args (dict[str,str]): Optional. Default arguments for boto3\n",
    "                ``invoke_endpoint`` call. Default is None (no default\n",
    "                arguments).\n",
    "            target_model (str): S3 model artifact path to run an inference request on,\n",
    "                in case of a multi model endpoint. Does not apply to endpoints hosting\n",
    "                single model (Default: None)\n",
    "            target_variant (str): The name of the production variant to run an inference\n",
    "                request on (Default: None). Note that the ProductionVariant identifies the\n",
    "                model you want to host and the resources you want to deploy for hosting it.\n",
    "            inference_id (str): If you provide a value, it is added to the captured data\n",
    "                when you enable data capture on the endpoint (Default: None).\n",
    "        Returns:\n",
    "            object: Inference for the given input. If a deserializer was specified when creating\n",
    "                the Predictor, the result of the deserializer is\n",
    "                returned. Otherwise the response returns the sequence of bytes\n",
    "                as is.\n",
    "        \"\"\"\n",
    "\n",
    "        request_args = self._create_request_args(\n",
    "            data, initial_args, target_model, target_variant, inference_id\n",
    "        )\n",
    "        response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n",
    "        return self._handle_response(response)\n",
    "```\n",
    "\n",
    "so `sagemaker_session.sagemaker_runtime_client` invokes the endpint\n",
    "\n",
    "I searched `sagemaker_runtime` in session.py\n",
    "\n",
    "```python\n",
    "def _initialize(\n",
    "        self,\n",
    "        boto_session,\n",
    "        sagemaker_client,\n",
    "        sagemaker_runtime_client,\n",
    "        sagemaker_featurestore_runtime_client,\n",
    "    ):\n",
    "        \"\"\"Initialize this SageMaker Session.\n",
    "        Creates or uses a boto_session, sagemaker_client and sagemaker_runtime_client.\n",
    "        Sets the region_name.\n",
    "        \"\"\"\n",
    "        self.boto_session = boto_session or boto3.DEFAULT_SESSION or boto3.Session()\n",
    "\n",
    "        self._region_name = self.boto_session.region_name\n",
    "        if self._region_name is None:\n",
    "            raise ValueError(\n",
    "                \"Must setup local AWS configuration with a region supported by SageMaker.\"\n",
    "            )\n",
    "\n",
    "        self.sagemaker_client = sagemaker_client or self.boto_session.client(\"sagemaker\")\n",
    "        prepend_user_agent(self.sagemaker_client)\n",
    "\n",
    "        if sagemaker_runtime_client is not None:\n",
    "            self.sagemaker_runtime_client = sagemaker_runtime_client\n",
    "        else:\n",
    "            config = botocore.config.Config(read_timeout=80)\n",
    "            self.sagemaker_runtime_client = self.boto_session.client(\n",
    "                \"runtime.sagemaker\", config=config\n",
    "            )\n",
    "\n",
    "        prepend_user_agent(self.sagemaker_runtime_client)\n",
    "\n",
    "        if sagemaker_featurestore_runtime_client:\n",
    "            self.sagemaker_featurestore_runtime_client = sagemaker_featurestore_runtime_client\n",
    "        else:\n",
    "            self.sagemaker_featurestore_runtime_client = self.boto_session.client(\n",
    "                \"sagemaker-featurestore-runtime\"\n",
    "            )\n",
    "\n",
    "        self.local_mode = False\n",
    " \n",
    "```\n",
    "\n",
    "This led to investigate [`SageMakerRuntime` client in boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html)\n",
    "\n",
    "\n",
    "```\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName='string',\n",
    "    Body=b'bytes'|file,\n",
    "    ContentType='string',\n",
    "    Accept='string',\n",
    "    CustomAttributes='string',\n",
    "    TargetModel='string',\n",
    "    TargetVariant='string',\n",
    "    TargetContainerHostname='string',\n",
    "    InferenceId='string'\n",
    ")\n",
    "```\n",
    "\n",
    "To figure out how content type looks like, I looked up [`serializers.py`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/serializers.py) and [`deserializers.py`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/deserializers.py)\n",
    "\n",
    "\n",
    "Why is it called SageMaker runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to serialize csv\n",
    "\n",
    "import csv\n",
    "import io\n",
    "\n",
    "def _serialize_row(data):\n",
    "    \"\"\"Serialize data as a CSV-formatted row.\n",
    "    Args:\n",
    "        data (sting): Data to be serialized in a row.\n",
    "    Returns:\n",
    "        str: The data serialized as a CSV-formatted row.\n",
    "    \"\"\"\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = np.ndarray.flatten(data)\n",
    "\n",
    "    if hasattr(data, \"__len__\"):\n",
    "        if len(data) == 0:\n",
    "            raise ValueError(\"Cannot serialize empty array\")\n",
    "        csv_buffer = io.StringIO()\n",
    "        csv_writer = csv.writer(csv_buffer, delimiter=\",\")\n",
    "        csv_writer.writerow(data)\n",
    "        return csv_buffer.getvalue().rstrip(\"\\r\\n\")\n",
    "        \n",
    "csv_buffer = io.StringIO()\n",
    "csv_writer = csv.writer(csv_buffer, delimiter=',')\n",
    "csv_writer.writerow('xyze')\n",
    "v = csv_buffer.getvalue().rstrip('\\r\\n')\n",
    "\n",
    "print(v.rstrip(\"\\r\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '94053c6a-3290-4562-9089-d78dd65e9a77', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '94053c6a-3290-4562-9089-d78dd65e9a77', 'x-amzn-invoked-production-variant': 'AMeaningfulProdVarName', 'date': 'Fri, 05 Mar 2021 03:10:19 GMT', 'content-type': 'text/csv; charset=utf-8', 'content-length': '26'}, 'RetryAttempts': 0}, 'ContentType': 'text/csv; charset=utf-8', 'InvokedProductionVariant': 'AMeaningfulProdVarName', 'Body': <botocore.response.StreamingBody object at 0x7f46e9c84d68>}\n"
     ]
    }
   ],
   "source": [
    "# invoke endpoint\n",
    "import json\n",
    "\n",
    "sm_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "body=json.dumps('a json string')\n",
    "\n",
    "# the model only supports csv data (look at how the model is defined in container/predictor.py)\n",
    "content_type='text/csv'\n",
    "# see the cell below for serializing a string into csv format\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "csv_writer = csv.writer(csv_buffer, delimiter=',')\n",
    "csv_writer.writerow('xyze')\n",
    "body = csv_buffer.getvalue().rstrip('\\r\\n')\n",
    "\n",
    "# respnse type is also text/csv (look at decision_tree/predictor.py)\n",
    "accept='text/csv'\n",
    "\n",
    "res=sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=body,                # encoded input data\n",
    "    ContentType=content_type, # I told the endpoint what's the encode\n",
    "    Accept=accept             # I told the endpoint how I wish to decode its response\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to decode json string\n",
    "\n",
    "import codecs\n",
    "\n",
    "class SimpleBaseDeserializer:\n",
    "    pass\n",
    "\n",
    "class JSONDeserializer(SimpleBaseDeserializer):\n",
    "    \"\"\"Deserialize JSON data from an inference endpoint into a Python object.\"\"\"\n",
    "\n",
    "    def __init__(self, accept=\"application/json\"):\n",
    "        \"\"\"Initialize a ``JSONDeserializer`` instance.\n",
    "        Args:\n",
    "            accept (union[str, tuple[str]]): The MIME type (or tuple of allowable MIME types) that\n",
    "                is expected from the inference endpoint (default: \"application/json\").\n",
    "        \"\"\"\n",
    "        super(JSONDeserializer, self).__init__(accept=accept)\n",
    "\n",
    "    def deserialize(self, stream, content_type):\n",
    "        \"\"\"Deserialize JSON data from an inference endpoint into a Python object.\n",
    "        Args:\n",
    "            stream (botocore.response.StreamingBody): Data to be deserialized.\n",
    "            content_type (str): The MIME type of the data.\n",
    "        Returns:\n",
    "            object: The JSON-formatted data deserialized into a Python object.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json.load(codecs.getreader(\"utf-8\")(stream))\n",
    "        finally:\n",
    "            stream.close()\n",
    "            \n",
    "            \n",
    "\n",
    "class CSVDeserializer(SimpleBaseDeserializer):\n",
    "    \"\"\"Deserialize a stream of bytes into a list of lists.\n",
    "    Consider using :class:~`sagemaker.deserializers.NumpyDeserializer` or\n",
    "    :class:~`sagemaker.deserializers.PandasDeserializer` instead, if you'd like to convert text/csv\n",
    "    responses directly into other data types.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding=\"utf-8\", accept=\"text/csv\"):\n",
    "        \"\"\"Initialize a ``CSVDeserializer`` instance.\n",
    "        Args:\n",
    "            encoding (str): The string encoding to use (default: \"utf-8\").\n",
    "            accept (union[str, tuple[str]]): The MIME type (or tuple of allowable MIME types) that\n",
    "                is expected from the inference endpoint (default: \"text/csv\").\n",
    "        \"\"\"\n",
    "        super(CSVDeserializer, self).__init__(accept=accept)\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def deserialize(self, stream, content_type):\n",
    "        \"\"\"Deserialize data from an inference endpoint into a list of lists.\n",
    "        Args:\n",
    "            stream (botocore.response.StreamingBody): Data to be deserialized.\n",
    "            content_type (str): The MIME type of the data.\n",
    "        Returns:\n",
    "            list: The data deserialized into a list of lists representing the\n",
    "                contents of a CSV file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            decoded_string = stream.read().decode(self.encoding)\n",
    "            return list(csv.reader(decoded_string.splitlines()))\n",
    "        finally:\n",
    "            stream.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a wonderful potato'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode response\n",
    "res_body = res['Body']\n",
    "res_body.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'HTTPHeaders': {'content-length': '0',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Fri, 05 Mar 2021 03:16:19 GMT',\n",
      "                                      'x-amzn-requestid': '42acf2c7-6c9e-4228-99cf-21e7bf684c4e'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '42acf2c7-6c9e-4228-99cf-21e7bf684c4e',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "del_res = sm_boto3.delete_endpoint(\n",
    "    EndpointName=endpoint_name)\n",
    "pp.pprint(del_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Endpoint \n",
    "\n",
    "In this notebook, you will learn basics about hosting your trained model on Amazon SageMaker for inference. There are two ways you can use Amazon SageMaker for inference:\n",
    "1. Set up persistent endpoint for real-time online inference\n",
    "2. Gather data to be predicted in batch and use SageMaker batch transform for offline inference. \n",
    "\n",
    "In this notebook, we focus on the first option and we will discuss batch transform in another notebook. \n",
    "\n",
    "You are highly recommeneded to go through [the section on model deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html) in the official docs before moving on.\n",
    "\n",
    "\n",
    "The pricing for setting up an endpoint can be found [here](https://aws.amazon.com/sagemaker/pricing/)\n",
    "\n",
    "Like a [CreateTrainingJob](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create-training-job.ipynb), Amazon SageMaker interacts with your inference logic via a containerized enviornment. \n",
    "\n",
    "The following APIs are relavent:\n",
    "* [`CreateModel`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model)\n",
    "* [`CreateEndpointConfig`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config)\n",
    "* [`CreateEndpoint`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint)\n",
    "\n",
    "You are highly recommended to go through them. It's okay if you don't understand everything, we will go through them in detail in this notebook. \n",
    "\n",
    "The outline of this notebook is:\n",
    "1. Build an inference image\n",
    "2. Test the inference image / container locally and push it to ECR\n",
    "3. Use the ECR address of the inference container to define a model by calling `CreateModel`\n",
    "4. Specify configuration of an endpoint by calling `CreateEndpointConfig`\n",
    "5. Use model definition from 3 and endpoint configuration from 4 to create an endpoint by calling `CreateEndpoint`\n",
    "6. Invoke the endpoint by using SageMaker runtime client "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an inference image\n",
    "\n",
    "You inference image must be a self-contained web server. When you run your inference container locally, it should listen on port 8080 and accept POST requests to the `/invocations` endpoint. The payload of the POST requests is the content of the data that you want your model to predict. Since the inference container is essentially a web server, you should expect it to look differently from the container we used for [`CreateTrainingJob`](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create-training-job.ipynb). \n",
    "\n",
    "In this notebook, we use a minimal python stack to build our web server:\n",
    "![Request serving stack](stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further readings on the serving stack\n",
    "\n",
    "* [Overview of the stack](https://flask.palletsprojects.com/en/1.1.x/deploying/uwsgi/)\n",
    "* [Ngnix homepage](https://www.nginx.com/resources/wiki/start/) \n",
    "* [WSGI homepage](https://gunicorn.org/)\n",
    "* [Flask homepage](https://flask.palletsprojects.com/en/1.1.x/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How SageMaker runs your container\n",
    "\n",
    "SageMaker runs your container like\n",
    "\n",
    "```sh\n",
    "docker run <image> serve\n",
    "```\n",
    "\n",
    "This means you need to have an executable called `serve` in the `PATH`. In this notebook, we will create a python script as an **executable** and put it in the working directory of the docker image. \n",
    "        \n",
    "The folder `container/src` contains the configs and entry point of the web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nginx.conf  predictor.py  serve  wsgi.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls  container/src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrypoint for Ngnixs server\n",
    "\n",
    "`serve` is a python executable that is intended to be used as the entrypoint for the inference image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "\r\n",
      "# This file implements the scoring service shell. You don't necessarily need to modify it for various\r\n",
      "# algorithms. It starts nginx and gunicorn with the correct configurations and then simply waits until\r\n",
      "# gunicorn exits.\r\n",
      "#\r\n",
      "# The flask server is specified to be the app object in wsgi.py\r\n",
      "#\r\n",
      "# We set the following parameters:\r\n",
      "#\r\n",
      "# Parameter                Environment Variable              Default Value\r\n",
      "# ---------                --------------------              -------------\r\n",
      "# number of workers        MODEL_SERVER_WORKERS              the number of CPU cores\r\n",
      "# timeout                  MODEL_SERVER_TIMEOUT              60 seconds\r\n",
      "\r\n",
      "import multiprocessing\r\n",
      "import os\r\n",
      "import signal\r\n",
      "import subprocess\r\n",
      "import sys\r\n",
      "\r\n",
      "cpu_count = multiprocessing.cpu_count()\r\n",
      "\r\n",
      "model_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\r\n",
      "model_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\r\n",
      "\r\n",
      "def sigterm_handler(nginx_pid, gunicorn_pid):\r\n",
      "    try:\r\n",
      "        os.kill(nginx_pid, signal.SIGQUIT)\r\n",
      "    except OSError:\r\n",
      "        pass\r\n",
      "    try:\r\n",
      "        os.kill(gunicorn_pid, signal.SIGTERM)\r\n",
      "    except OSError:\r\n",
      "        pass\r\n",
      "\r\n",
      "    sys.exit(0)\r\n",
      "\r\n",
      "def start_server():\r\n",
      "    print('Starting the inference server with {} workers.'.format(model_server_workers))\r\n",
      "\r\n",
      "    # link the log streams to stdout/err so they will be logged to the container logs\r\n",
      "    subprocess.check_call(['ln', '-sf', '/dev/stdout', '/var/log/nginx/access.log'])\r\n",
      "    subprocess.check_call(['ln', '-sf', '/dev/stderr', '/var/log/nginx/error.log'])\r\n",
      "\r\n",
      "    nginx = subprocess.Popen(['nginx', '-c', '/opt/program/nginx.conf'])\r\n",
      "    gunicorn = subprocess.Popen(['gunicorn',\r\n",
      "                                 '--timeout', str(model_server_timeout),\r\n",
      "                                 '-k', 'sync',\r\n",
      "                                 '-b', 'unix:/tmp/gunicorn.sock',\r\n",
      "                                 '-w', str(model_server_workers),\r\n",
      "                                 'wsgi:app'])\r\n",
      "\r\n",
      "    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\r\n",
      "\r\n",
      "    # If either subprocess exits, so do we.\r\n",
      "    pids = set([nginx.pid, gunicorn.pid])\r\n",
      "    while True:\r\n",
      "        pid, _ = os.wait()\r\n",
      "        if pid in pids:\r\n",
      "            break\r\n",
      "\r\n",
      "    sigterm_handler(nginx.pid, gunicorn.pid)\r\n",
      "    print('Inference server exiting')\r\n",
      "\r\n",
      "# The main routine just invokes the start function.\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    start_server()\r\n"
     ]
    }
   ],
   "source": [
    "!cat container/src/serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config file for Ngnix server\n",
    "`nginx.conf` is the config file for the nginx server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker_processes 1;\r\n",
      "daemon off; # Prevent forking\r\n",
      "\r\n",
      "\r\n",
      "pid /tmp/nginx.pid;\r\n",
      "error_log /var/log/nginx/error.log;\r\n",
      "\r\n",
      "events {\r\n",
      "  # defaults\r\n",
      "}\r\n",
      "\r\n",
      "http {\r\n",
      "  include /etc/nginx/mime.types;\r\n",
      "  default_type application/octet-stream;\r\n",
      "  access_log /var/log/nginx/access.log combined;\r\n",
      "  \r\n",
      "  upstream gunicorn {\r\n",
      "    server unix:/tmp/gunicorn.sock;\r\n",
      "  }\r\n",
      "\r\n",
      "  server {\r\n",
      "    listen 8080 deferred;\r\n",
      "    client_max_body_size 5m;\r\n",
      "\r\n",
      "    keepalive_timeout 5;\r\n",
      "    proxy_read_timeout 1200s;\r\n",
      "\r\n",
      "    location ~ ^/(ping|invocations) {\r\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\r\n",
      "      proxy_set_header Host $http_host;\r\n",
      "      proxy_redirect off;\r\n",
      "      proxy_pass http://gunicorn;\r\n",
      "    }\r\n",
      "\r\n",
      "    location / {\r\n",
      "      return 404 \"{}\";\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat container/src/nginx.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WSGI config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import predictor as myapp\r\n",
      "\r\n",
      "# This is just a simple wrapper for gunicorn to find your app.\r\n",
      "# If you want to change the algorithm file, simply change \"predictor\" above to the\r\n",
      "# new file.\r\n",
      "\r\n",
      "app = myapp.app\r\n"
     ]
    }
   ],
   "source": [
    "!cat container/src/wsgi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference logic\n",
    "\n",
    "The most important file in `container/src` is `predictor.py`. It contains the inference logic. Other files in the `container/src` can be used **as it**. But you will need to customize `predictor.py` to implement your own inference logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# This is the file that implements a flask server to do inferences. It's the file that you will modify to adapt to your own inference logic.\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mflask\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# The flask app for serving predictions\u001b[39;49;00m\r\n",
      "app = flask.Flask(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[90m@app\u001b[39;49;00m.route(\u001b[33m'\u001b[39;49;00m\u001b[33m/ping\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, methods=[\u001b[33m'\u001b[39;49;00m\u001b[33mGET\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mping\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"Determine if the container is working and healthy. \u001b[39;49;00m\r\n",
      "\u001b[33m    In this sample container, we declare\u001b[39;49;00m\r\n",
      "\u001b[33m    it healthy if we can load the model successfully.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    status = \u001b[34m200\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m flask.Response(response=\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "        status=status, mimetype=\u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[90m@app\u001b[39;49;00m.route(\u001b[33m'\u001b[39;49;00m\u001b[33m/invocations\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, methods=[\u001b[33m'\u001b[39;49;00m\u001b[33mPOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minference\u001b[39;49;00m():\r\n",
      "    \u001b[33m\"\"\"Do an inference on incoming data. \u001b[39;49;00m\r\n",
      "\u001b[33m    In this sample server, we take data as application/json,\u001b[39;49;00m\r\n",
      "\u001b[33m    print it out to confirm that the server received it.  \u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    content_type = flask.request.content_type\r\n",
      "    \u001b[34mif\u001b[39;49;00m flask.request.content_type != \u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "        msg = \u001b[33m\"\u001b[39;49;00m\u001b[33mI just take json, and I am fed with \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "            content_type)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        msg = \u001b[33m\"\u001b[39;49;00m\u001b[33mI am fed with json. Therefore, I am happy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m== The entire request object ==\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(request)\r\n",
      "    \r\n",
      "    \u001b[37m# define response header \u001b[39;49;00m\r\n",
      "    header = {} \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m flask.Response(\r\n",
      "        response=msg,\r\n",
      "        status=\u001b[34m200\u001b[39;49;00m,\r\n",
      "        header = header,\r\n",
      "        mimetype=\u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize container/src/predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the container\n",
    "\n",
    "We build the container from `container/Dockderfile`. And let's call this image `example-serve`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Build an image that can do training and inference in SageMaker\r\n",
      "# This is a Python 3 image that uses the nginx, gunicorn, flask stack\r\n",
      "# for serving inferences in a stable way.\r\n",
      "\r\n",
      "FROM ubuntu:18.04\r\n",
      "\r\n",
      "MAINTAINER Amazon AI <sage-learner@amazon.com>\r\n",
      "\r\n",
      "\r\n",
      "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\r\n",
      "         wget \\\r\n",
      "         python3-pip \\\r\n",
      "         python3-setuptools \\\r\n",
      "         nginx \\\r\n",
      "         ca-certificates \\\r\n",
      "    && rm -rf /var/lib/apt/lists/*\r\n",
      "\r\n",
      "RUN ln -s /usr/bin/python3 /usr/bin/python\r\n",
      "RUN ln -s /usr/bin/pip3 /usr/bin/pip\r\n",
      "\r\n",
      "# Here we get all python packages.\r\n",
      "# There's substantial overlap between scipy and numpy that we eliminate by\r\n",
      "# linking them together. Likewise, pip leaves the install caches populated which uses\r\n",
      "# a significant amount of space. These optimizations save a fair amount of space in the\r\n",
      "# image, which reduces start up time.\r\n",
      "RUN pip --no-cache-dir install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gunicorn\r\n",
      "\r\n",
      "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\r\n",
      "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\r\n",
      "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\r\n",
      "# PATH so that the train and serve programs are found when the container is invoked.\r\n",
      "\r\n",
      "ENV PYTHONUNBUFFERED=TRUE\r\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\r\n",
      "ENV PATH=\"/opt/program:${PATH}\"\r\n",
      "\r\n",
      "# Set up the program in the image\r\n",
      "COPY src /opt/program\r\n",
      "WORKDIR /opt/program\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  77.82kB\r",
      "\r\n",
      "Step 1/11 : FROM ubuntu:18.04\n",
      " ---> c090eaba6b94\n",
      "Step 2/11 : MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
      " ---> Using cache\n",
      " ---> dd1348b3a447\n",
      "Step 3/11 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          python3-pip          python3-setuptools          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 5d88b1be1abc\n",
      "Step 4/11 : RUN ln -s /usr/bin/python3 /usr/bin/python\n",
      " ---> Using cache\n",
      " ---> 6417ac44ded6\n",
      "Step 5/11 : RUN ln -s /usr/bin/pip3 /usr/bin/pip\n",
      " ---> Using cache\n",
      " ---> 09ada8830cc5\n",
      "Step 6/11 : RUN pip --no-cache-dir install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gunicorn\n",
      " ---> Using cache\n",
      " ---> 2885b6a01bd3\n",
      "Step 7/11 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> ab2910812f0d\n",
      "Step 8/11 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 0c0e511ac354\n",
      "Step 9/11 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 10fcf3254c9d\n",
      "Step 10/11 : COPY src /opt/program\n",
      " ---> 3d03fa67f272\n",
      "Step 11/11 : WORKDIR /opt/program\n",
      " ---> Running in 6f308f57ad87\n",
      "Removing intermediate container 6f308f57ad87\n",
      " ---> 12c6861aff3a\n",
      "Successfully built 12c6861aff3a\n",
      "Successfully tagged example-serve:latest\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# build the image\n",
    "cd container/\n",
    "\n",
    "# tag it as example-image:latest\n",
    "docker build -t example-serve:latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your image\n",
    "\n",
    "Like in the [notebook for CreateTrainingJob](https://github.com/hsl89/amazon-sagemaker-examples/blob/sagemaker-fundamentals/sagemaker-fundamentals/create-training-job/create-training-job.ipynb), we replicate the Amazon SageMaker hosting environment and test your image locally before serving in production. You are encouraged to read through the section on [Use Your Own Inference Code with Hosting Services](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html) and think about how would you replicate SageMaker hosting environment before moving on. \n",
    "\n",
    "Like for `CreateTrainingJob`, SageMaker reserves `/opt/ml` directory in your image to inject ML-related info for `CreateEndpoint`. In particular, it downloads your trained model artifact and inject it in the directory `/opt/ml/model`. When calling `CreateEndpoint` you will need to tell SageMaker the S3 URI of your model artifact. SageMaker will use then pull the artifact and inject it into `/opt/ml/model`. This means when defining your own inference logic, you should load your trained model from `/opt/ml/model`. \n",
    "\n",
    "We will use docker python client to run your image and we will mount `container/local_test/ml` to `/opt/ml` as docker volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\r\n"
     ]
    }
   ],
   "source": [
    "# look at what's inside `container/ml`\n",
    "!ls container/local_test/ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference logic we implemented in `container/src/predictor.py` under `def inference():` does not require a real ML model. Therefore we do not need to inject anything for the purpose of local test. We will discuss how to load a real model in a more advanced notebook. \n",
    "\n",
    "<span style=\"color:red\"> TODO for Dev:  add link to the advanced notebook when it is ready</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the container\n",
    "\n",
    "To run the container `example-serve`, open a terminal in the current directory and go to `container/local_test`\n",
    "\n",
    "```sh\n",
    "cd container/local_test\n",
    "```\n",
    "\n",
    "Then run the following command\n",
    "\n",
    "```sh\n",
    "docker run -v ml:/opt/ml -p 8080:8080 --rm example-serve:latest serve \n",
    "```\n",
    "\n",
    "`-v ml:/opt/ml` binds the directory `ml` (in `container/local_test`) to `/opt/ml` in the image as a docker volume.\n",
    "\n",
    "`-p 8080:8080` exposes port 8080 inside container as port 8080 on the hos\n",
    "\n",
    "`--rm` removes the container from daemon when it is stopped. \n",
    "\n",
    "We suggest you to run the image from the shell instead of within the notebook because when you are debugging your own container, you can more easily stdout from the container when you have a shell process running it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ping your container\n",
    "Once your container is up, you can ping it at `http://localhost:8080`. \n",
    "\n",
    "To trigger the logic under `def ping():` in `container/src/predictor.py`, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100     1  100     1    0     0   1000      0 --:--:-- --:--:-- --:--:--  1000\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "curl localhost:8080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To trigger the logic under `def inference():` in `container/src/predictor.py` with a json string, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am fed with json. Therefore, I am happy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100    56  100    41  100    15  13666   5000 --:--:-- --:--:-- --:--:-- 18666\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "curl --header \"Content-Type: application/json\" \\\n",
    "  --request POST \\\n",
    "  --data '{\"key\":\"value\"}' \\\n",
    "  http://localhost:8080/invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the container "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
