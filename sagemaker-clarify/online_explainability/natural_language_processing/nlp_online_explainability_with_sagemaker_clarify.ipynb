{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9947f913",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NLP Online Explainability with SageMaker Clarify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80d5f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff305d4b",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "1. [General Setup](#General-Setup)\n",
    "    1. [Install dependencies](#Install-dependencies)\n",
    "    1. [Import libraries](#Import-libraries)\n",
    "    1. [Set configurations](#Set-configurations)\n",
    "    1. [Create serializer and deserializer](#Create-serializer-and-deserializer)\n",
    "    1. [For visualization](#For-visualization)\n",
    "1. [Prepare data](#Prepare-data)\n",
    "    1. [Download data](#Download-data)\n",
    "    1. [Loading the data](#Loading-the-data)\n",
    "    1. [Data preparation for model training](#Data-preparation-for-model-training)\n",
    "    1. [Upload the dataset](#Upload-the-dataset)\n",
    "1. [Train and Deploy Hugging Face Model](#Train-and-Deploy-Hugging-Face-Model)\n",
    "    1. [Train model with Hugging Face estimator](#Train-model-with-Hugging-Face-estimator)\n",
    "    1. [Download the trained model files](#Download-the-trained-model-files)\n",
    "    1. [Prepare model container definition](#Prepare-model-container-definition)\n",
    "1. [Create endpoint](#Create-endpoint)\n",
    "    1. [Create model](#Create-model)\n",
    "    1. [Create endpoint config](#Create-endpoint-config)\n",
    "    1. [Create endpoint](#Create-endpoint)\n",
    "1. [Invoke endpoint](#Invoke-endpoint)\n",
    "    1. [Single record request](#Single-record-request)\n",
    "    1. [Single record request, no explanation](#Single-record-request,-no-explanation)\n",
    "    1. [Batch request, explain both](#Batch-request,-explain-both)\n",
    "    1. [Batch request with more records, explain some of the records](#Batch-request-with-more-records,-explain-some-of-the-records)\n",
    "1. [Cleanup](#Cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e481e8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Amazon SageMaker Clarify helps improve your machine learning models by detecting potential bias and helping explain how these models make predictions. The fairness and explainability functionality provided by SageMaker Clarify takes a step towards enabling AWS customers to build trustworthy and understandable machine learning models. \n",
    "\n",
    "SageMaker Clarify currently supports explainability for SageMaker models as an offline processing job. This example notebook showcases a new feature for explainability on a [SageMaker real-time inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html) endpoint, a.k.a. [Online Explainability](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-online-explainability.html).\n",
    "\n",
    "This example notebook walks you through:  \n",
    "1. Key terms and concepts needed to understand SageMaker Clarify\n",
    "1. Trained the model on the Women's ecommerce clothing reviews dataset.\n",
    "1. Create a model from trained model artifacts, create an endpoint configuration with the new SageMaker Clarify explainer configuration, and create an endpoint using the same explainer configuration.\n",
    "1. Invoke the endpoint with single and batch request with different `EnableExplanations` query.\n",
    "1. Explaining the importance of the various input features on the model's decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a9f9b",
   "metadata": {},
   "source": [
    "## General Setup\n",
    "\n",
    "We recommend you use `Python 3 (Data Science)` kernel on SageMaker Studio or `conda_python3` kernel on SageMaker Notebook Instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af597f",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "Install required dependencies. `datasets[s3]` and `transformers` are used for data preparation and training, `captum` is used to visualize the feature attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8163e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7375b3",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f759dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import tarfile\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from datasets import Dataset\n",
    "from datasets.filesystems import S3FileSystem\n",
    "from captum.attr import visualization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.utils import unique_name_from_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a7b53d",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a747240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-west-2\n",
      "Role: arn:aws:iam::000000000000:role/service-role/SMClarifySageMaker-ExecutionRole\n",
      "Demo S3 key: s3://sagemaker-us-west-2-000000000000/sagemaker/DEMO-NLP-Women-Clothing-1687464029-bfff\n",
      "Demo model name: DEMO-NLP-Women-Clothing-1687464029-bfff-model\n",
      "Demo endpoint config name: DEMO-NLP-Women-Clothing-1687464029-bfff-endpoint-config\n",
      "Demo endpoint name: DEMO-NLP-Women-Clothing-1687464029-bfff-endpoint\n"
     ]
    }
   ],
   "source": [
    "boto3_session = boto3.session.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Initialize sagemaker session\n",
    "sagemaker_session = Session(\n",
    "    boto_session=boto3_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_runtime_client=sagemaker_runtime_client,\n",
    ")\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "role = get_execution_role()\n",
    "print(f\"Role: {role}\")\n",
    "\n",
    "prefix = unique_name_from_base(\"DEMO-NLP-Women-Clothing\")\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = f\"sagemaker/{prefix}\"\n",
    "s3_key = f\"s3://{s3_bucket}/{s3_prefix}\"\n",
    "print(f\"Demo S3 key: {s3_key}\")\n",
    "\n",
    "model_name = f\"{prefix}-model\"\n",
    "print(f\"Demo model name: {model_name}\")\n",
    "endpoint_config_name = f\"{prefix}-endpoint-config\"\n",
    "print(f\"Demo endpoint config name: {endpoint_config_name}\")\n",
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "print(f\"Demo endpoint name: {endpoint_name}\")\n",
    "\n",
    "# SageMaker Clarify model directory name\n",
    "model_path = \"model/\"\n",
    "\n",
    "# Instance type for training and hosting\n",
    "instance_type = \"ml.m5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a3bf2b",
   "metadata": {},
   "source": [
    "### Create serializer and deserializer\n",
    "\n",
    "CSV serializer to serialize test data to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4394a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_serializer = CSVSerializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38990b",
   "metadata": {},
   "source": [
    "JSON deserializer to deserialize invoke endpoint response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3033a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70baa91",
   "metadata": {},
   "source": [
    "### For visualization\n",
    "We have some methods implemented for visualization in `visualization_utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34126440",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run visualization_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6468da5",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef1b76",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Data Source: `https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews/`\n",
    "\n",
    "The Women’s E-Commerce Clothing Reviews dataset has been made available under a Creative Commons Public Domain license. A copy of the dataset has been saved in a sample data Amazon S3 bucket. In the first section of the notebook, we’ll walk through how to download the data and get started with building the ML workflow as a SageMaker pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1911a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    f\"sagemaker-example-files-prod-{region}\",\n",
    "    \"datasets/tabular/womens_clothing_ecommerce/Womens_Clothing_E-Commerce_Reviews.csv\",\n",
    "    \"womens_clothing_reviews_dataset.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928470ca",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57b9977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title   \n",
       "0          767   33                      NaN  \\\n",
       "1         1080   34                      NaN   \n",
       "2         1077   60  Some major design flaws   \n",
       "3         1049   50         My favorite buy!   \n",
       "4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND   \n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1  \\\n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0       Initmates        Intimate  Intimates  \n",
       "1                        4         General         Dresses    Dresses  \n",
       "2                        0         General         Dresses    Dresses  \n",
       "3                        0  General Petite         Bottoms      Pants  \n",
       "4                        6         General            Tops    Blouses  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"womens_clothing_reviews_dataset.csv\", index_col=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf7bbd",
   "metadata": {},
   "source": [
    "**Context**\n",
    "\n",
    "The Women’s Clothing E-Commerce dataset contains reviews written by customers. Because the dataset contains real commercial data, it has been anonymized, and any references to the company in the review text and body have been replaced with “retailer”.\n",
    "\n",
    "\n",
    "\n",
    "**Content**\n",
    "\n",
    "The dataset contains 23486 rows and 10 columns. Each row corresponds to a customer review.\n",
    "\n",
    "The columns include:\n",
    "\n",
    "* Clothing ID: Integer Categorical variable that refers to the specific piece being reviewed.\n",
    "* Age: Positive Integer variable of the reviewer's age.\n",
    "* Title: String variable for the title of the review.\n",
    "* Review Text: String variable for the review body.\n",
    "* Rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n",
    "* Recommended IND: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n",
    "* Positive Feedback Count: Positive Integer documenting the number of other customers who found this review positive.\n",
    "* Division Name: Categorical name of the product high level division.\n",
    "* Department Name: Categorical name of the product department name.\n",
    "* Class Name: Categorical name of the product class name.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "To predict the sentiment of a review based on the text, and then explain the predictions using SageMaker Clarify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5abd81",
   "metadata": {},
   "source": [
    "### Data preparation for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57ff2d",
   "metadata": {},
   "source": [
    "#### Target Variable Creation\n",
    "Since the dataset does not contain a column that indicates the sentiment of the customer reviews, lets create one. To do this, let's assume that reviews with a `Rating` of 4 or higher indicate positive sentiment and reviews with a `Rating` of 2 or lower indicate negative sentiment. Let's also assume that a `Rating` of 3 indicates neutral sentiment and exclude these rows from the dataset. Additionally, to predict the sentiment of a review, we are going to use the `Review Text` column; therefore let's remove rows that are empty in the `Review Text` column of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_column(df, min_positive_score, max_negative_score):\n",
    "    neutral_values = [i for i in range(max_negative_score + 1, min_positive_score)]\n",
    "    for neutral_value in neutral_values:\n",
    "        df = df[df[\"Rating\"] != neutral_value]\n",
    "    df[\"Sentiment\"] = df[\"Rating\"] >= min_positive_score\n",
    "    return df.replace({\"Sentiment\": {True: 1, False: 0}})\n",
    "\n",
    "\n",
    "df = create_target_column(df, 4, 2)\n",
    "df = df[~df[\"Review Text\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349c218",
   "metadata": {},
   "source": [
    "#### Train-Validation-Test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2fbf13",
   "metadata": {},
   "source": [
    "The most common approach for model evaluation is using the train/validation/test split. Although this approach can be very effective in general, it can result in misleading results and potentially fail when used on classification problems with a severe class imbalance. Instead, the technique must be modified to stratify the sampling by the class label as below. Stratification ensures that all classes are well represented across the train, validation and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "296aa13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train  (15874,) (15874,) {1: 0.8804334131283861, 0: 0.11956658687161396}\n",
      "Dataset: validation  (1962,) (1962,) {1: 0.8802242609582059, 0: 0.11977573904179409}\n",
      "Dataset: test  (1982,) (1982,) {1: 0.8804238143289607, 0: 0.11957618567103935}\n"
     ]
    }
   ],
   "source": [
    "target = \"Sentiment\"\n",
    "cols = \"Review Text\"\n",
    "\n",
    "X = df[cols]\n",
    "y = df[target]\n",
    "\n",
    "# Data split: 11%(val) of the 90% (train and test) of the dataset ~ 10%; resulting in 80:10:10split\n",
    "test_dataset_size = 0.10\n",
    "val_dataset_size = 0.11\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Stratified train-val-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_dataset_size, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=val_dataset_size,\n",
    "    stratify=y_train,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Dataset: train \",\n",
    "    X_train.shape,\n",
    "    y_train.shape,\n",
    "    y_train.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "print(\n",
    "    \"Dataset: validation \",\n",
    "    X_val.shape,\n",
    "    y_val.shape,\n",
    "    y_val.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "print(\n",
    "    \"Dataset: test \",\n",
    "    X_test.shape,\n",
    "    y_test.shape,\n",
    "    y_test.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "\n",
    "# Combine the independent columns with the label\n",
    "df_train = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "df_test = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "df_val = pd.concat([X_val, y_val], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e0f262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: Review Text\n",
      "Label name: Sentiment\n",
      "Test data (without label column):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am 5'6\", 130 lbs with an athletic body type ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The design on the blue sweater is actually a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The colors are so much brighter than pictured....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A very versatile and cozy top. would look grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just not cute. i don't know how else to descri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>As soon as i opened the package, i knew that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>As the title suggests, i am very skeptical and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>I love this dress. i'm 6' so it's a tad bit sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>I love the concept of this dress. i love the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>Bought this is the blue, which is actually a v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1982 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Review Text\n",
       "0     I am 5'6\", 130 lbs with an athletic body type ...\n",
       "1     The design on the blue sweater is actually a d...\n",
       "2     The colors are so much brighter than pictured....\n",
       "3     A very versatile and cozy top. would look grea...\n",
       "4     Just not cute. i don't know how else to descri...\n",
       "...                                                 ...\n",
       "1977  As soon as i opened the package, i knew that t...\n",
       "1978  As the title suggests, i am very skeptical and...\n",
       "1979  I love this dress. i'm 6' so it's a tad bit sh...\n",
       "1980  I love the concept of this dress. i love the s...\n",
       "1981  Bought this is the blue, which is actually a v...\n",
       "\n",
       "[1982 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = df_test.columns.to_list()\n",
    "feature_headers = headers[0]\n",
    "label_header = headers[1]\n",
    "print(f\"Feature names: {feature_headers}\")\n",
    "print(f\"Label name: {label_header}\")\n",
    "print(f\"Test data (without label column):\")\n",
    "test_data = df_test.iloc[:, :1]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609ec0ce",
   "metadata": {},
   "source": [
    "We have split the dataset into train, test, and validation datasets. We use the train and validation datasets during training process, and run Clarify on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc56b95",
   "metadata": {},
   "source": [
    "### Upload the dataset\n",
    "Here, we upload the prepared datasets to S3 buckets so that we can train the model with the Hugging Face Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cd912e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train.csv\", index=False, header=False)\n",
    "df_val.to_csv(\"test.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c68dce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training input path: s3://sagemaker-us-west-2-000000000000/sagemaker/DEMO-NLP-Women-Clothing-1687464029-bfff/train\n",
      "validation input path: s3://sagemaker-us-west-2-000000000000/sagemaker/DEMO-NLP-Women-Clothing-1687464029-bfff/test\n"
     ]
    }
   ],
   "source": [
    "training_input_path = f\"{s3_key}/train\"\n",
    "print(f\"training input path: {training_input_path}\")\n",
    "val_input_path = f\"{s3_key}/test\"\n",
    "print(f\"validation input path: {val_input_path}\")\n",
    "\n",
    "train_uri = S3Uploader.upload(\"train.csv\", training_input_path)\n",
    "test_uri = S3Uploader.upload(\"test.csv\", val_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950f117",
   "metadata": {},
   "source": [
    "## Train and Deploy Hugging Face Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19ba35",
   "metadata": {},
   "source": [
    "In this step of the workflow, we use the [Hugging Face Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) to load the pre-trained `distilbert-base-uncased` model and fine-tune the model on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad509be",
   "metadata": {},
   "source": [
    "### Train model with Hugging Face estimator\n",
    "The hyperparameters defined below are parameters that are passed to the custom PyTorch code in [`scripts/train.py`](./scripts/train.py). The only required parameter is `model_name`. The other parameters like `epoch`, `train_batch_size` all have default values which can be overridden by setting their values here.\n",
    "\n",
    "The training job requires GPU instance type. Here, we use `ml.g4dn.xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d120f31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-000000000000/sagemaker/DEMO-NLP-Women-Clothing-1687464029-bfff/train'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bc2f340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-06-22-20-00-31-761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-06-22 20:00:32 Starting - Starting the training job...\n",
      "2023-06-22 20:00:47 Starting - Preparing the instances for training......\n",
      "2023-06-22 20:01:51 Downloading - Downloading input data...\n",
      "2023-06-22 20:02:16 Training - Downloading the training image...............\n",
      "2023-06-22 20:04:42 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:55,537 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:55,567 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:55,571 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:55,825 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"test_file\": \"test.csv\",\n",
      "        \"train_file\": \"train.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-06-22-20-00-31-761\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-000000000000/huggingface-pytorch-training-2023-06-22-20-00-31-761/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"test_file\":\"test.csv\",\"train_file\":\"train.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-000000000000/huggingface-pytorch-training-2023-06-22-20-00-31-761/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"test_file\":\"test.csv\",\"train_file\":\"train.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2023-06-22-20-00-31-761\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-000000000000/huggingface-pytorch-training-2023-06-22-20-00-31-761/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--test_file\",\"test.csv\",\"--train_file\",\"train.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_FILE=test.csv\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=train.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --epochs 1 --model_name distilbert-base-uncased --test_file test.csv --train_file train.csv\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:58,268 - datasets.builder - WARNING - Using custom data configuration default-dd9cff78fda6d41f\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-dd9cff78fda6d41f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-dd9cff78fda6d41f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:58,638 - datasets.builder - WARNING - Using custom data configuration default-6e3ab9f4794794bf\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-6e3ab9f4794794bf/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-6e3ab9f4794794bf/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:58,655 - __main__ - INFO -  loaded train_dataset length is: 15874\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:58,655 - __main__ - INFO -  loaded test_dataset length is: 1962\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:58,761 - filelock - INFO - Lock 140303718168840 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:58,875 - filelock - INFO - Lock 140303718168840 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:58,990 - filelock - INFO - Lock 140300681038760 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.cf47717d443acbff3940da39f5ddd0b17179607321d46f2c0a5060d2264eefd0.lock\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:59,236 - filelock - INFO - Lock 140300681038760 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.cf47717d443acbff3940da39f5ddd0b17179607321d46f2c0a5060d2264eefd0.lock\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:59,351 - filelock - INFO - Lock 140300681038760 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.53241bddd84f83cd6f1881886465d84bbf4f27be795658add74bee2568ac4587.lock\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:59,566 - filelock - INFO - Lock 140300681038760 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.53241bddd84f83cd6f1881886465d84bbf4f27be795658add74bee2568ac4587.lock\u001b[0m\n",
      "\u001b[34m2023-06-22 20:04:59,907 - filelock - INFO - Lock 140300681038760 acquired on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m2023-06-22 20:05:00,018 - filelock - INFO - Lock 140300681038760 released on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m2023-06-22 20:05:02,959 - filelock - INFO - Lock 140300647498416 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2023-06-22 20:05:06,178 - filelock - INFO - Lock 140300647498416 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[2023-06-22 20:05:08.910 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-06-22 20:05:08.965 algo-1:27 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.14932341873645782, 'eval_accuracy': 0.936289500509684, 'eval_f1': 0.9643163003140165, 'eval_precision': 0.9510135135135135, 'eval_recall': 0.9779965257672264, 'eval_runtime': 10.8827, 'eval_samples_per_second': 180.287, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 251.0795, 'train_samples_per_second': 1.979, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m#0150 tables [00:00, ? tables/s]#015                            #015#0150 tables [00:00, ? tables/s]#015                            #015#015Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 483/483 [00:00<00:00, 428kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading: 0.00B [00:00, ?B/s]#015Downloading: 219kB [00:00, 1.70MB/s]#015Downloading: 232kB [00:00, 1.78MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading: 0.00B [00:00, ?B/s]#015Downloading: 466kB [00:00, 4.60MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 23.5kB/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/16 [00:00<?, ?ba/s]#015  6%|▋         | 1/16 [00:00<00:02,  6.25ba/s]#015 12%|█▎        | 2/16 [00:00<00:02,  6.62ba/s]#015 19%|█▉        | 3/16 [00:00<00:01,  6.94ba/s]#015 25%|██▌       | 4/16 [00:00<00:01,  7.10ba/s]#015 31%|███▏      | 5/16 [00:00<00:01,  7.25ba/s]#015 38%|███▊      | 6/16 [00:00<00:01,  7.38ba/s]#015 44%|████▍     | 7/16 [00:00<00:01,  7.46ba/s]#015 50%|█████     | 8/16 [00:01<00:01,  7.40ba/s]#015 56%|█████▋    | 9/16 [00:01<00:00,  7.08ba/s]#015 62%|██████▎   | 10/16 [00:01<00:00,  7.10ba/s]#015 69%|██████▉   | 11/16 [00:01<00:00,  6.75ba/s]#015 75%|███████▌  | 12/16 [00:01<00:00,  6.62ba/s]#015 81%|████████▏ | 13/16 [00:01<00:00,  6.45ba/s]#015 88%|████████▊ | 14/16 [00:02<00:00,  6.22ba/s]#015 94%|█████████▍| 15/16 [00:02<00:00,  6.11ba/s]#015100%|██████████| 16/16 [00:02<00:00,  6.51ba/s]#015100%|██████████| 16/16 [00:02<00:00,  6.84ba/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/2 [00:00<?, ?ba/s]#015 50%|█████     | 1/2 [00:00<00:00,  6.20ba/s]#015100%|██████████| 2/2 [00:00<00:00,  6.23ba/s]#015100%|██████████| 2/2 [00:00<00:00,  6.24ba/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   2%|▏         | 5.01M/268M [00:00<00:05, 50.1MB/s]#015Downloading:   4%|▍         | 12.0M/268M [00:00<00:04, 54.8MB/s]#015Downloading:   8%|▊         | 20.4M/268M [00:00<00:04, 61.1MB/s]#015Downloading:  11%|█         | 29.1M/268M [00:00<00:03, 67.1MB/s]#015Downloading:  14%|█▍        | 37.8M/268M [00:00<00:03, 72.1MB/s]#015Downloading:  17%|█▋        | 46.6M/268M [00:00<00:02, 76.1MB/s]#015Downloading:  21%|██        | 55.2M/268M [00:00<00:02, 79.1MB/s]#015Downloading:  24%|██▍       | 63.6M/268M [00:00<00:02, 80.5MB/s]#015Downloading:  27%|██▋       | 71.6M/268M [00:00<00:02, 75.9MB/s]#015Downloading:  30%|██▉       | 79.9M/268M [00:01<00:02, 77.9MB/s]#015Downloading:  33%|███▎      | 88.7M/268M [00:01<00:02, 80.6MB/s]#015Downloading:  36%|███▋      | 97.4M/268M [00:01<00:02, 82.4MB/s]#015Downloading:  40%|███▉      | 106M/268M [00:01<00:01, 83.9MB/s] #015Downloading:  43%|████▎     | 115M/268M [00:01<00:01, 85.1MB/s]#015Downloading:  46%|████▌     | 124M/268M [00:01<00:01, 86.0MB/s]#015Downloading:  49%|████▉     | 133M/268M [00:01<00:01, 86.7MB/s]#015Downloading:  53%|█████▎    | 141M/268M [00:01<00:01, 87.0MB/s]#015Downloading:  56%|█████▌    | 150M/268M [00:01<00:01, 87.2MB/s]#015Downloading:  59%|█████▉    | 159M/268M [00:01<00:01, 87.3MB/s]#015Downloading:  63%|██████▎   | 168M/268M [00:02<00:01, 87.4MB/s]#015Downloading:  66%|██████▌   | 176M/268M [00:02<00:01, 87.4MB/s]#015Downloading:  69%|██████▉   | 185M/268M [00:02<00:00, 86.6MB/s]#015Downloading:  72%|███████▏  | 194M/268M [00:02<00:00, 87.0MB/s]#015Downloading:  76%|███████▌  | 203M/268M [00:02<00:00, 87.2MB/s]#015Downloading:  79%|███████▉  | 211M/268M [00:02<00:00, 87.4MB/s]#015Downloading:  82%|████████▏ | 220M/268M [00:02<00:00, 87.5MB/s]#015Downloading:  85%|████████▌ | 229M/268M [00:02<00:00, 87.4MB/s]#015Downloading:  89%|████████▊ | 238M/268M [00:02<00:00, 87.4MB/s]#015Downloading:  92%|█████████▏| 247M/268M [00:02<00:00, 87.7MB/s]#015Downloading:  95%|█████████▌| 255M/268M [00:03<00:00, 87.8MB/s]#015Downloading:  99%|█████████▊| 264M/268M [00:03<00:00, 87.7MB/s]#015Downloading: 100%|██████████| 268M/268M [00:03<00:00, 84.6MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/497 [00:00<?, ?it/s]#015  0%|          | 1/497 [00:01<10:21,  1.25s/it]#015  0%|          | 2/497 [00:01<08:21,  1.01s/it]#015  1%|          | 3/497 [00:02<06:58,  1.18it/s]#015  1%|          | 4/497 [00:02<06:00,  1.37it/s]#015  1%|          | 5/497 [00:03<05:19,  1.54it/s]#015  1%|          | 6/497 [00:03<04:51,  1.69it/s]#015  1%|▏         | 7/497 [00:03<04:28,  1.83it/s]#015  2%|▏         | 8/497 [00:04<04:15,  1.91it/s]#015  2%|▏         | 9/497 [00:04<04:02,  2.01it/s]#015  2%|▏         | 10/497 [00:05<03:54,  2.08it/s]#015  2%|▏         | 11/497 [00:05<03:50,  2.11it/s]#015  2%|▏         | 12/497 [00:06<03:49,  2.12it/s]#015  3%|▎         | 13/497 [00:06<03:47,  2.13it/s]#015  3%|▎         | 14/497 [00:07<03:46,  2.14it/s]#015  3%|▎         | 15/497 [00:07<03:44,  2.15it/s]#015  3%|▎         | 16/497 [00:08<03:40,  2.18it/s]#015  3%|▎         | 17/497 [00:08<03:41,  2.17it/s]#015  4%|▎         | 18/497 [00:09<03:40,  2.17it/s]#015  4%|▍         | 19/497 [00:09<03:40,  2.17it/s]#015  4%|▍         | 20/497 [00:09<03:40,  2.16it/s]#015  4%|▍         | 21/497 [00:10<03:39,  2.16it/s]#015  4%|▍         | 22/497 [00:10<03:39,  2.16it/s]#015  5%|▍         | 23/497 [00:11<03:38,  2.17it/s]#015  5%|▍         | 24/497 [00:11<03:38,  2.16it/s]#015  5%|▌         | 25/497 [00:12<03:39,  2.15it/s]#015  5%|▌         | 26/497 [00:12<03:38,  2.16it/s]#015  5%|▌         | 27/497 [00:13<03:39,  2.15it/s]#015  6%|▌         | 28/497 [00:13<03:39,  2.14it/s]#015  6%|▌         | 29/497 [00:14<03:38,  2.14it/s]#015  6%|▌         | 30/497 [00:14<03:37,  2.15it/s]#015  6%|▌         | 31/497 [00:15<03:38,  2.14it/s]#015  6%|▋         | 32/497 [00:15<03:36,  2.15it/s]#015  7%|▋         | 33/497 [00:15<03:36,  2.14it/s]#015  7%|▋         | 34/497 [00:16<03:36,  2.14it/s]#015  7%|▋         | 35/497 [00:16<03:31,  2.18it/s]#015  7%|▋         | 36/497 [00:17<03:30,  2.19it/s]#015  7%|▋         | 37/497 [00:17<03:30,  2.19it/s]#015  8%|▊         | 38/497 [00:18<03:31,  2.17it/s]#015  8%|▊         | 39/497 [00:18<03:31,  2.16it/s]#015  8%|▊         | 40/497 [00:19<03:30,  2.17it/s]#015  8%|▊         | 41/497 [00:19<03:29,  2.17it/s]#015  8%|▊         | 42/497 [00:20<03:30,  2.16it/s]#015  9%|▊         | 43/497 [00:20<03:29,  2.17it/s]#015  9%|▉         | 44/497 [00:21<03:28,  2.17it/s]#015  9%|▉         | 45/497 [00:21<03:27,  2.17it/s]#015  9%|▉         | 46/497 [00:21<03:28,  2.17it/s]#015  9%|▉         | 47/497 [00:22<03:27,  2.16it/s]#015 10%|▉         | 48/497 [00:22<03:27,  2.16it/s]#015 10%|▉         | 49/497 [00:23<03:26,  2.16it/s]#015 10%|█         | 50/497 [00:23<03:27,  2.16it/s]#015 10%|█         | 51/497 [00:24<03:28,  2.14it/s]#015 10%|█         | 52/497 [00:24<03:24,  2.18it/s]#015 11%|█         | 53/497 [00:25<03:21,  2.20it/s]#015 11%|█         | 54/497 [00:25<03:23,  2.18it/s]#015 11%|█         | 55/497 [00:26<03:23,  2.17it/s]#015 11%|█▏        | 56/497 [00:26<03:17,  2.24it/s]#015 11%|█▏        | 57/497 [00:27<03:18,  2.21it/s]#015 12%|█▏        | 58/497 [00:27<03:19,  2.20it/s]#015 12%|█▏        | 59/497 [00:27<03:19,  2.19it/s]#015 12%|█▏        | 60/497 [00:28<03:19,  2.19it/s]#015 12%|█▏        | 61/497 [00:28<03:19,  2.19it/s]#015 12%|█▏        | 62/497 [00:29<03:19,  2.18it/s]#015 13%|█▎        | 63/497 [00:29<03:17,  2.20it/s]#015 13%|█▎        | 64/497 [00:30<03:18,  2.18it/s]#015 13%|█▎        | 65/497 [00:30<03:19,  2.16it/s]#015 13%|█▎        | 66/497 [00:31<03:18,  2.17it/s]#015 13%|█▎        | 67/497 [00:31<03:17,  2.18it/s]#015 14%|█▎        | 68/497 [00:32<03:18,  2.17it/s]#015 14%|█▍        | 69/497 [00:32<03:17,  2.16it/s]#015 14%|█▍        | 70/497 [00:32<03:17,  2.16it/s]#015 14%|█▍        | 71/497 [00:33<03:18,  2.15it/s]#015 14%|█▍        | 72/497 [00:33<03:18,  2.14it/s]#015 15%|█▍        | 73/497 [00:34<03:17,  2.15it/s]#015 15%|█▍        | 74/497 [00:34<03:16,  2.16it/s]#015 15%|█▌        | 75/497 [00:35<03:15,  2.16it/s]#015 15%|█▌        | 76/497 [00:35<03:14,  2.16it/s]#015 15%|█▌        | 77/497 [00:36<03:13,  2.18it/s]#015 16%|█▌        | 78/497 [00:36<03:13,  2.16it/s]#015 16%|█▌        | 79/497 [00:37<03:12,  2.17it/s]#015 16%|█▌        | 80/497 [00:37<03:12,  2.17it/s]#015 16%|█▋        | 81/497 [00:38<03:12,  2.16it/s]#015 16%|█▋        | 82/497 [00:38<03:12,  2.16it/s]#015 17%|█▋        | 83/497 [00:39<03:12,  2.15it/s]#015 17%|█▋        | 84/497 [00:39<03:15,  2.12it/s]#015 17%|█▋        | 85/497 [00:40<03:17,  2.09it/s]#015 17%|█▋        | 86/497 [00:40<03:15,  2.11it/s]#015 18%|█▊        | 87/497 [00:40<03:13,  2.12it/s]#015 18%|█▊        | 88/497 [00:41<03:11,  2.13it/s]#015 18%|█▊        | 89/497 [00:41<03:13,  2.11it/s]#015 18%|█▊        | 90/497 [00:42<03:11,  2.12it/s]#015 18%|█▊        | 91/497 [00:42<03:11,  2.12it/s]#015 19%|█▊        | 92/497 [00:43<03:12,  2.10it/s]#015 19%|█▊        | 93/497 [00:43<03:17,  2.05it/s]#015 19%|█▉        | 94/497 [00:44<03:15,  2.06it/s]#015 19%|█▉        | 95/497 [00:44<03:15,  2.06it/s]#015 19%|█▉        | 96/497 [00:45<03:13,  2.07it/s]#015 20%|█▉        | 97/497 [00:45<03:13,  2.07it/s]#015 20%|█▉        | 98/497 [00:46<03:11,  2.09it/s]#015 20%|█▉        | 99/497 [00:46<03:12,  2.06it/s]#015 20%|██        | 100/497 [00:47<03:13,  2.05it/s]#015 20%|██        | 101/497 [00:47<03:13,  2.04it/s]#015 21%|██        | 102/497 [00:48<03:12,  2.06it/s]#015 21%|██        | 103/497 [00:48<03:08,  2.09it/s]#015 21%|██        | 104/497 [00:49<03:06,  2.10it/s]#015 21%|██        | 105/497 [00:49<03:06,  2.10it/s]#015 21%|██▏       | 106/497 [00:50<03:09,  2.06it/s]#015 22%|██▏       | 107/497 [00:50<03:06,  2.09it/s]#015 22%|██▏       | 108/497 [00:51<03:09,  2.06it/s]#015 22%|██▏       | 109/497 [00:51<03:09,  2.05it/s]#015 22%|██▏       | 110/497 [00:52<03:04,  2.10it/s]#015 22%|██▏       | 111/497 [00:52<03:00,  2.13it/s]#015 23%|██▎       | 112/497 [00:52<03:04,  2.08it/s]#015 23%|██▎       | 113/497 [00:53<03:05,  2.07it/s]#015 23%|██▎       | 114/497 [00:53<03:03,  2.09it/s]#015 23%|██▎       | 115/497 [00:54<03:03,  2.08it/s]#015 23%|██▎       | 116/497 [00:54<03:00,  2.11it/s]#015 24%|██▎       | 117/497 [00:55<02:58,  2.13it/s]#015 24%|██▎       | 118/497 [00:55<02:57,  2.13it/s]#015 24%|██▍       | 119/497 [00:56<02:56,  2.14it/s]#015 24%|██▍       | 120/497 [00:56<02:54,  2.16it/s]#015 24%|██▍       | 121/497 [00:57<02:55,  2.15it/s]#015 25%|██▍       | 122/497 [00:57<02:52,  2.18it/s]#015 25%|██▍       | 123/497 [00:58<02:51,  2.18it/s]#015 25%|██▍       | 124/497 [00:58<02:53,  2.15it/s]#015 25%|██▌       | 125/497 [00:59<02:52,  2.15it/s]#015 25%|██▌       | 126/497 [00:59<02:53,  2.14it/s]#015 26%|██▌       | 127/497 [00:59<02:53,  2.13it/s]#015 26%|██▌       | 128/497 [01:00<02:54,  2.12it/s]#015 26%|██▌       | 129/497 [01:00<02:53,  2.12it/s]#015 26%|██▌       | 130/497 [01:01<02:54,  2.10it/s]#015 26%|██▋       | 131/497 [01:01<02:54,  2.10it/s]#015 27%|██▋       | 132/497 [01:02<02:53,  2.11it/s]#015 27%|██▋       | 133/497 [01:02<02:52,  2.11it/s]#015 27%|██▋       | 134/497 [01:03<02:52,  2.11it/s]#015 27%|██▋       | 135/497 [01:03<02:51,  2.10it/s]#015 27%|██▋       | 136/497 [01:04<02:48,  2.14it/s]#015 28%|██▊       | 137/497 [01:04<02:49,  2.12it/s]#015 28%|██▊       | 138/497 [01:05<02:49,  2.12it/s]#015 28%|██▊       | 139/497 [01:05<02:48,  2.12it/s]#015 28%|██▊       | 140/497 [01:06<02:48,  2.12it/s]#015 28%|██▊       | 141/497 [01:06<02:47,  2.13it/s]#015 29%|██▊       | 142/497 [01:07<02:47,  2.12it/s]#015 29%|██▉       | 143/497 [01:07<02:47,  2.11it/s]#015 29%|██▉       | 144/497 [01:08<02:47,  2.10it/s]#015 29%|██▉       | 145/497 [01:08<02:44,  2.14it/s]#015 29%|██▉       | 146/497 [01:08<02:44,  2.13it/s]#015 30%|██▉       | 147/497 [01:09<02:44,  2.13it/s]#015 30%|██▉       | 148/497 [01:09<02:45,  2.11it/s]#015 30%|██▉       | 149/497 [01:10<02:43,  2.12it/s]#015 30%|███       | 150/497 [01:10<02:43,  2.12it/s]#015 30%|███       | 151/497 [01:11<02:42,  2.12it/s]#015 31%|███       | 152/497 [01:11<02:44,  2.10it/s]#015 31%|███       | 153/497 [01:12<02:43,  2.11it/s]#015 31%|███       | 154/497 [01:12<02:43,  2.10it/s]#015 31%|███       | 155/497 [01:13<02:43,  2.10it/s]#015 31%|███▏      | 156/497 [01:13<02:42,  2.10it/s]#015 32%|███▏      | 157/497 [01:14<02:41,  2.10it/s]#015 32%|███▏      | 158/497 [01:14<02:40,  2.11it/s]#015 32%|███▏      | 159/497 [01:15<02:40,  2.10it/s]#015 32%|███▏      | 160/497 [01:15<02:39,  2.11it/s]#015 32%|███▏      | 161/497 [01:16<02:40,  2.09it/s]#015 33%|███▎      | 162/497 [01:16<02:39,  2.10it/s]#015 33%|███▎      | 163/497 [01:17<02:39,  2.10it/s]#015 33%|███▎      | 164/497 [01:17<02:38,  2.10it/s]#015 33%|███▎      | 165/497 [01:17<02:35,  2.13it/s]#015 33%|███▎      | 166/497 [01:18<02:36,  2.12it/s]#015 34%|███▎      | 167/497 [01:18<02:35,  2.12it/s]#015 34%|███▍      | 168/497 [01:19<02:35,  2.12it/s]#015 34%|███▍      | 169/497 [01:19<02:35,  2.11it/s]#015 34%|███▍      | 170/497 [01:20<02:35,  2.10it/s]#015 34%|███▍      | 171/497 [01:20<02:35,  2.09it/s]#015 35%|███▍      | 172/497 [01:21<02:35,  2.09it/s]#015 35%|███▍      | 173/497 [01:21<02:35,  2.08it/s]#015 35%|███▌      | 174/497 [01:22<02:34,  2.09it/s]#015 35%|███▌      | 175/497 [01:22<02:31,  2.12it/s]#015 35%|███▌      | 176/497 [01:23<02:32,  2.10it/s]#015 36%|███▌      | 177/497 [01:23<02:32,  2.10it/s]#015 36%|███▌      | 178/497 [01:24<02:31,  2.10it/s]#015 36%|███▌      | 179/497 [01:24<02:31,  2.11it/s]#015 36%|███▌      | 180/497 [01:25<02:32,  2.08it/s]#015 36%|███▋      | 181/497 [01:25<02:31,  2.09it/s]#015 37%|███▋      | 182/497 [01:26<02:30,  2.09it/s]#015 37%|███▋      | 183/497 [01:26<02:30,  2.09it/s]#015 37%|███▋      | 184/497 [01:27<02:30,  2.09it/s]#015 37%|███▋      | 185/497 [01:27<02:29,  2.09it/s]#015 37%|███▋      | 186/497 [01:27<02:26,  2.12it/s]#015 38%|███▊      | 187/497 [01:28<02:26,  2.12it/s]#015 38%|███▊      | 188/497 [01:28<02:26,  2.11it/s]#015 38%|███▊      | 189/497 [01:29<02:27,  2.09it/s]#015 38%|███▊      | 190/497 [01:29<02:26,  2.09it/s]#015 38%|███▊      | 191/497 [01:30<02:26,  2.09it/s]#015 39%|███▊      | 192/497 [01:30<02:26,  2.09it/s]#015 39%|███▉      | 193/497 [01:31<02:26,  2.07it/s]#015 39%|███▉      | 194/497 [01:31<02:26,  2.07it/s]#015 39%|███▉      | 195/497 [01:32<02:25,  2.08it/s]#015 39%|███▉      | 196/497 [01:32<02:24,  2.09it/s]#015 40%|███▉      | 197/497 [01:33<02:23,  2.09it/s]#015 40%|███▉      | 198/497 [01:33<02:23,  2.08it/s]#015 40%|████      | 199/497 [01:34<02:23,  2.07it/s]#015 40%|████      | 200/497 [01:34<02:22,  2.09it/s]#015 40%|████      | 201/497 [01:35<02:22,  2.07it/s]#015 41%|████      | 202/497 [01:35<02:22,  2.06it/s]#015 41%|████      | 203/497 [01:36<02:22,  2.07it/s]#015 41%|████      | 204/497 [01:36<02:22,  2.06it/s]#015 41%|████      | 205/497 [01:37<02:21,  2.07it/s]#015 41%|████▏     | 206/497 [01:37<02:20,  2.07it/s]#015 42%|████▏     | 207/497 [01:38<02:20,  2.07it/s]#015 42%|████▏     | 208/497 [01:38<02:19,  2.08it/s]#015 42%|████▏     | 209/497 [01:39<02:17,  2.10it/s]#015 42%|████▏     | 210/497 [01:39<02:17,  2.09it/s]#015 42%|████▏     | 211/497 [01:39<02:15,  2.12it/s]#015 43%|████▎     | 212/497 [01:40<02:15,  2.10it/s]#015 43%|████▎     | 213/497 [01:40<02:13,  2.12it/s]#015 43%|████▎     | 214/497 [01:41<02:15,  2.09it/s]#015 43%|████▎     | 215/497 [01:41<02:14,  2.09it/s]#015 43%|████▎     | 216/497 [01:42<02:14,  2.08it/s]#015 44%|████▎     | 217/497 [01:42<02:15,  2.07it/s]#015 44%|████▍     | 218/497 [01:43<02:14,  2.08it/s]#015 44%|████▍     | 219/497 [01:43<02:14,  2.06it/s]#015 44%|████▍     | 220/497 [01:44<02:14,  2.06it/s]#015 44%|████▍     | 221/497 [01:44<02:14,  2.06it/s]#015 45%|████▍     | 222/497 [01:45<02:14,  2.05it/s]#015 45%|████▍     | 223/497 [01:45<02:13,  2.06it/s]#015 45%|████▌     | 224/497 [01:46<02:12,  2.05it/s]#015 45%|████▌     | 225/497 [01:46<02:12,  2.05it/s]#015 45%|████▌     | 226/497 [01:47<02:12,  2.05it/s]#015 46%|████▌     | 227/497 [01:47<02:11,  2.06it/s]#015 46%|████▌     | 228/497 [01:48<02:10,  2.06it/s]#015 46%|████▌     | 229/497 [01:48<02:08,  2.09it/s]#015 46%|████▋     | 230/497 [01:49<02:08,  2.08it/s]#015 46%|████▋     | 231/497 [01:49<02:07,  2.08it/s]#015 47%|████▋     | 232/497 [01:50<02:06,  2.10it/s]#015 47%|████▋     | 233/497 [01:50<02:06,  2.09it/s]#015 47%|████▋     | 234/497 [01:51<02:07,  2.06it/s]#015 47%|████▋     | 235/497 [01:51<02:07,  2.06it/s]#015 47%|████▋     | 236/497 [01:52<02:06,  2.06it/s]#015 48%|████▊     | 237/497 [01:52<02:04,  2.08it/s]#015 48%|████▊     | 238/497 [01:53<02:07,  2.02it/s]#015 48%|████▊     | 239/497 [01:53<02:07,  2.02it/s]#015 48%|████▊     | 240/497 [01:54<02:06,  2.02it/s]#015 48%|████▊     | 241/497 [01:54<02:06,  2.02it/s]#015 49%|████▊     | 242/497 [01:55<02:06,  2.02it/s]#015 49%|████▉     | 243/497 [01:55<02:05,  2.03it/s]#015 49%|████▉     | 244/497 [01:56<02:03,  2.04it/s]#015 49%|████▉     | 245/497 [01:56<02:02,  2.05it/s]#015 49%|████▉     | 246/497 [01:56<02:01,  2.06it/s]#015 50%|████▉     | 247/497 [01:57<02:01,  2.06it/s]#015 50%|████▉     | 248/497 [01:57<02:01,  2.06it/s]#015 50%|█████     | 249/497 [01:58<02:00,  2.06it/s]#015 50%|█████     | 250/497 [01:58<02:00,  2.05it/s]#015 51%|█████     | 251/497 [01:59<01:59,  2.05it/s]#015 51%|█████     | 252/497 [01:59<01:59,  2.05it/s]#015 51%|█████     | 253/497 [02:00<01:58,  2.05it/s]#015 51%|█████     | 254/497 [02:00<01:57,  2.07it/s]#015 51%|█████▏    | 255/497 [02:01<01:57,  2.06it/s]#015 52%|█████▏    | 256/497 [02:01<01:57,  2.06it/s]#015 52%|█████▏    | 257/497 [02:02<01:54,  2.09it/s]#015 52%|█████▏    | 258/497 [02:02<01:54,  2.08it/s]#015 52%|█████▏    | 259/497 [02:03<01:55,  2.07it/s]#015 52%|█████▏    | 260/497 [02:03<01:54,  2.07it/s]#015 53%|█████▎    | 261/497 [02:04<01:54,  2.06it/s]#015 53%|█████▎    | 262/497 [02:04<01:53,  2.06it/s]#015 53%|█████▎    | 263/497 [02:05<01:53,  2.06it/s]#015 53%|█████▎    | 264/497 [02:05<01:53,  2.06it/s]#015 53%|█████▎    | 265/497 [02:06<01:53,  2.04it/s]#015 54%|█████▎    | 266/497 [02:06<01:52,  2.05it/s]#015 54%|█████▎    | 267/497 [02:07<01:51,  2.06it/s]#015 54%|█████▍    | 268/497 [02:07<01:51,  2.06it/s]#015 54%|█████▍    | 269/497 [02:08<01:51,  2.05it/s]#015 54%|█████▍    | 270/497 [02:08<01:49,  2.07it/s]#015 55%|█████▍    | 271/497 [02:09<01:49,  2.06it/s]#015 55%|█████▍    | 272/497 [02:09<01:47,  2.09it/s]#015 55%|█████▍    | 273/497 [02:10<01:47,  2.08it/s]#015 55%|█████▌    | 274/497 [02:10<01:47,  2.07it/s]#015 55%|█████▌    | 275/497 [02:11<01:48,  2.05it/s]#015 56%|█████▌    | 276/497 [02:11<01:48,  2.04it/s]#015 56%|█████▌    | 277/497 [02:12<01:48,  2.03it/s]#015 56%|█████▌    | 278/497 [02:12<01:45,  2.07it/s]#015 56%|█████▌    | 279/497 [02:12<01:46,  2.05it/s]#015 56%|█████▋    | 280/497 [02:13<01:45,  2.05it/s]#015 57%|█████▋    | 281/497 [02:13<01:45,  2.05it/s]#015 57%|█████▋    | 282/497 [02:14<01:45,  2.04it/s]#015 57%|█████▋    | 283/497 [02:14<01:45,  2.04it/s]#015 57%|█████▋    | 284/497 [02:15<01:43,  2.06it/s]#015 57%|█████▋    | 285/497 [02:15<01:43,  2.05it/s]#015 58%|█████▊    | 286/497 [02:16<01:42,  2.05it/s]#015 58%|█████▊    | 287/497 [02:16<01:42,  2.06it/s]#015 58%|█████▊    | 288/497 [02:17<01:40,  2.08it/s]#015 58%|█████▊    | 289/497 [02:17<01:41,  2.05it/s]#015 58%|█████▊    | 290/497 [02:18<01:40,  2.05it/s]#015 59%|█████▊    | 29\u001b[0m\n",
      "\u001b[34m2023-06-22 20:09:32,262 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m1/497 [02:18<01:40,  2.05it/s]#015 59%|█████▉    | 292/497 [02:19<01:40,  2.05it/s]#015 59%|█████▉    | 293/497 [02:19<01:39,  2.05it/s]#015 59%|█████▉    | 294/497 [02:20<01:39,  2.03it/s]#015 59%|█████▉    | 295/497 [02:20<01:40,  2.02it/s]#015 60%|█████▉    | 296/497 [02:21<01:38,  2.05it/s]#015 60%|█████▉    | 297/497 [02:21<01:36,  2.07it/s]#015 60%|█████▉    | 298/497 [02:22<01:36,  2.07it/s]#015 60%|██████    | 299/497 [02:22<01:36,  2.06it/s]#015 60%|██████    | 300/497 [02:23<01:34,  2.08it/s]#015 61%|██████    | 301/497 [02:23<01:34,  2.07it/s]#015 61%|██████    | 302/497 [02:24<01:34,  2.06it/s]#015 61%|██████    | 303/497 [02:24<01:34,  2.06it/s]#015 61%|██████    | 304/497 [02:25<01:33,  2.06it/s]#015 61%|██████▏   | 305/497 [02:25<01:34,  2.04it/s]#015 62%|██████▏   | 306/497 [02:26<01:32,  2.07it/s]#015 62%|██████▏   | 307/497 [02:26<01:32,  2.05it/s]#015 62%|██████▏   | 308/497 [02:27<01:32,  2.04it/s]#015 62%|██████▏   | 309/497 [02:27<01:31,  2.05it/s]#015 62%|██████▏   | 310/497 [02:28<01:31,  2.05it/s]#015 63%|██████▎   | 311/497 [02:28<01:31,  2.04it/s]#015 63%|██████▎   | 312/497 [02:29<01:30,  2.04it/s]#015 63%|██████▎   | 313/497 [02:29<01:28,  2.07it/s]#015 63%|██████▎   | 314/497 [02:30<01:28,  2.06it/s]#015 63%|██████▎   | 315/497 [02:30<01:28,  2.05it/s]#015 64%|██████▎   | 316/497 [02:31<01:28,  2.04it/s]#015 64%|██████▍   | 317/497 [02:31<01:28,  2.03it/s]#015 64%|██████▍   | 318/497 [02:32<01:28,  2.03it/s]#015 64%|██████▍   | 319/497 [02:32<01:27,  2.03it/s]#015 64%|██████▍   | 320/497 [02:33<01:27,  2.02it/s]#015 65%|██████▍   | 321/497 [02:33<01:26,  2.03it/s]#015 65%|██████▍   | 322/497 [02:34<01:26,  2.02it/s]#015 65%|██████▍   | 323/497 [02:34<01:26,  2.02it/s]#015 65%|██████▌   | 324/497 [02:34<01:24,  2.04it/s]#015 65%|██████▌   | 325/497 [02:35<01:24,  2.03it/s]#015 66%|██████▌   | 326/497 [02:35<01:24,  2.03it/s]#015 66%|██████▌   | 327/497 [02:36<01:23,  2.03it/s]#015 66%|██████▌   | 328/497 [02:36<01:23,  2.03it/s]#015 66%|██████▌   | 329/497 [02:37<01:22,  2.03it/s]#015 66%|██████▋   | 330/497 [02:37<01:22,  2.03it/s]#015 67%|██████▋   | 331/497 [02:38<01:22,  2.01it/s]#015 67%|██████▋   | 332/497 [02:38<01:21,  2.02it/s]#015 67%|██████▋   | 333/497 [02:39<01:20,  2.03it/s]#015 67%|██████▋   | 334/497 [02:39<01:20,  2.03it/s]#015 67%|██████▋   | 335/497 [02:40<01:20,  2.02it/s]#015 68%|██████▊   | 336/497 [02:40<01:20,  2.01it/s]#015 68%|██████▊   | 337/497 [02:41<01:18,  2.04it/s]#015 68%|██████▊   | 338/497 [02:41<01:18,  2.04it/s]#015 68%|██████▊   | 339/497 [02:42<01:17,  2.03it/s]#015 68%|██████▊   | 340/497 [02:42<01:17,  2.04it/s]#015 69%|██████▊   | 341/497 [02:43<01:16,  2.04it/s]#015 69%|██████▉   | 342/497 [02:43<01:14,  2.07it/s]#015 69%|██████▉   | 343/497 [02:44<01:14,  2.07it/s]#015 69%|██████▉   | 344/497 [02:44<01:14,  2.06it/s]#015 69%|██████▉   | 345/497 [02:45<01:14,  2.05it/s]#015 70%|██████▉   | 346/497 [02:45<01:12,  2.07it/s]#015 70%|██████▉   | 347/497 [02:46<01:12,  2.06it/s]#015 70%|███████   | 348/497 [02:46<01:11,  2.08it/s]#015 70%|███████   | 349/497 [02:47<01:11,  2.07it/s]#015 70%|███████   | 350/497 [02:47<01:11,  2.05it/s]#015 71%|███████   | 351/497 [02:48<01:11,  2.04it/s]#015 71%|███████   | 352/497 [02:48<01:11,  2.04it/s]#015 71%|███████   | 353/497 [02:49<01:10,  2.03it/s]#015 71%|███████   | 354/497 [02:49<01:10,  2.03it/s]#015 71%|███████▏  | 355/497 [02:50<01:10,  2.02it/s]#015 72%|███████▏  | 356/497 [02:50<01:09,  2.03it/s]#015 72%|███████▏  | 357/497 [02:51<01:08,  2.03it/s]#015 72%|███████▏  | 358/497 [02:51<01:08,  2.02it/s]#015 72%|███████▏  | 359/497 [02:52<01:08,  2.01it/s]#015 72%|███████▏  | 360/497 [02:52<01:07,  2.02it/s]#015 73%|███████▎  | 361/497 [02:53<01:07,  2.01it/s]#015 73%|███████▎  | 362/497 [02:53<01:07,  2.00it/s]#015 73%|███████▎  | 363/497 [02:54<01:06,  2.01it/s]#015 73%|███████▎  | 364/497 [02:54<01:06,  2.00it/s]#015 73%|███████▎  | 365/497 [02:55<01:05,  2.01it/s]#015 74%|███████▎  | 366/497 [02:55<01:05,  2.01it/s]#015 74%|███████▍  | 367/497 [02:56<01:04,  2.01it/s]#015 74%|███████▍  | 368/497 [02:56<01:04,  2.01it/s]#015 74%|███████▍  | 369/497 [02:57<01:03,  2.01it/s]#015 74%|███████▍  | 370/497 [02:57<01:03,  2.01it/s]#015 75%|███████▍  | 371/497 [02:58<01:02,  2.02it/s]#015 75%|███████▍  | 372/497 [02:58<01:01,  2.02it/s]#015 75%|███████▌  | 373/497 [02:59<01:00,  2.06it/s]#015 75%|███████▌  | 374/497 [02:59<00:59,  2.05it/s]#015 75%|███████▌  | 375/497 [03:00<00:59,  2.04it/s]#015 76%|███████▌  | 376/497 [03:00<00:59,  2.03it/s]#015 76%|███████▌  | 377/497 [03:01<00:59,  2.02it/s]#015 76%|███████▌  | 378/497 [03:01<00:58,  2.02it/s]#015 76%|███████▋  | 379/497 [03:02<00:58,  2.02it/s]#015 76%|███████▋  | 380/497 [03:02<00:57,  2.02it/s]#015 77%|███████▋  | 381/497 [03:03<00:56,  2.05it/s]#015 77%|███████▋  | 382/497 [03:03<00:56,  2.04it/s]#015 77%|███████▋  | 383/497 [03:04<00:55,  2.04it/s]#015 77%|███████▋  | 384/497 [03:04<00:55,  2.04it/s]#015 77%|███████▋  | 385/497 [03:05<00:55,  2.03it/s]#015 78%|███████▊  | 386/497 [03:05<00:54,  2.03it/s]#015 78%|███████▊  | 387/497 [03:06<00:54,  2.02it/s]#015 78%|███████▊  | 388/497 [03:06<00:54,  2.01it/s]#015 78%|███████▊  | 389/497 [03:07<00:53,  2.01it/s]#015 78%|███████▊  | 390/497 [03:07<00:53,  2.01it/s]#015 79%|███████▊  | 391/497 [03:07<00:52,  2.01it/s]#015 79%|███████▉  | 392/497 [03:08<00:51,  2.05it/s]#015 79%|███████▉  | 393/497 [03:08<00:50,  2.04it/s]#015 79%|███████▉  | 394/497 [03:09<00:50,  2.04it/s]#015 79%|███████▉  | 395/497 [03:09<00:50,  2.02it/s]#015 80%|███████▉  | 396/497 [03:10<00:50,  2.01it/s]#015 80%|███████▉  | 397/497 [03:10<00:49,  2.01it/s]#015 80%|████████  | 398/497 [03:11<00:49,  2.00it/s]#015 80%|████████  | 399/497 [03:11<00:48,  2.01it/s]#015 80%|████████  | 400/497 [03:12<00:48,  2.01it/s]#015 81%|████████  | 401/497 [03:12<00:47,  2.00it/s]#015 81%|████████  | 402/497 [03:13<00:46,  2.04it/s]#015 81%|████████  | 403/497 [03:13<00:46,  2.04it/s]#015 81%|████████▏ | 404/497 [03:14<00:46,  2.02it/s]#015 81%|████████▏ | 405/497 [03:14<00:45,  2.01it/s]#015 82%|████████▏ | 406/497 [03:15<00:45,  2.02it/s]#015 82%|████████▏ | 407/497 [03:15<00:44,  2.01it/s]#015 82%|████████▏ | 408/497 [03:16<00:44,  2.00it/s]#015 82%|████████▏ | 409/497 [03:16<00:43,  2.01it/s]#015 82%|████████▏ | 410/497 [03:17<00:43,  2.01it/s]#015 83%|████████▎ | 411/497 [03:17<00:42,  2.01it/s]#015 83%|████████▎ | 412/497 [03:18<00:41,  2.05it/s]#015 83%|████████▎ | 413/497 [03:18<00:40,  2.07it/s]#015 83%|████████▎ | 414/497 [03:19<00:40,  2.06it/s]#015 84%|████████▎ | 415/497 [03:19<00:40,  2.05it/s]#015 84%|████████▎ | 416/497 [03:20<00:39,  2.04it/s]#015 84%|████████▍ | 417/497 [03:20<00:39,  2.03it/s]#015 84%|████████▍ | 418/497 [03:21<00:39,  2.02it/s]#015 84%|████████▍ | 419/497 [03:21<00:38,  2.01it/s]#015 85%|████████▍ | 420/497 [03:22<00:38,  2.01it/s]#015 85%|████████▍ | 421/497 [03:22<00:37,  2.00it/s]#015 85%|████████▍ | 422/497 [03:23<00:37,  2.00it/s]#015 85%|████████▌ | 423/497 [03:23<00:36,  2.01it/s]#015 85%|████████▌ | 424/497 [03:24<00:36,  2.00it/s]#015 86%|████████▌ | 425/497 [03:24<00:35,  2.04it/s]#015 86%|████████▌ | 426/497 [03:25<00:35,  2.02it/s]#015 86%|████████▌ | 427/497 [03:25<00:34,  2.02it/s]#015 86%|████████▌ | 428/497 [03:26<00:34,  2.02it/s]#015 86%|████████▋ | 429/497 [03:26<00:33,  2.00it/s]#015 87%|████████▋ | 430/497 [03:27<00:33,  2.00it/s]#015 87%|████████▋ | 431/497 [03:27<00:32,  2.01it/s]#015 87%|████████▋ | 432/497 [03:28<00:31,  2.04it/s]#015 87%|████████▋ | 433/497 [03:28<00:31,  2.04it/s]#015 87%|████████▋ | 434/497 [03:29<00:30,  2.03it/s]#015 88%|████████▊ | 435/497 [03:29<00:30,  2.02it/s]#015 88%|████████▊ | 436/497 [03:30<00:30,  2.01it/s]#015 88%|████████▊ | 437/497 [03:30<00:29,  2.02it/s]#015 88%|████████▊ | 438/497 [03:31<00:29,  2.02it/s]#015 88%|████████▊ | 439/497 [03:31<00:28,  2.02it/s]#015 89%|████████▊ | 440/497 [03:32<00:28,  2.00it/s]#015 89%|████████▊ | 441/497 [03:32<00:27,  2.00it/s]#015 89%|████████▉ | 442/497 [03:33<00:27,  1.99it/s]#015 89%|████████▉ | 443/497 [03:33<00:26,  2.03it/s]#015 89%|████████▉ | 444/497 [03:34<00:26,  2.02it/s]#015 90%|████████▉ | 445/497 [03:34<00:25,  2.03it/s]#015 90%|████████▉ | 446/497 [03:35<00:24,  2.06it/s]#015 90%|████████▉ | 447/497 [03:35<00:24,  2.02it/s]#015 90%|█████████ | 448/497 [03:36<00:24,  2.02it/s]#015 90%|█████████ | 449/497 [03:36<00:23,  2.01it/s]#015 91%|█████████ | 450/497 [03:37<00:23,  2.01it/s]#015 91%|█████████ | 451/497 [03:37<00:22,  2.02it/s]#015 91%|█████████ | 452/497 [03:38<00:22,  2.02it/s]#015 91%|█████████ | 453/497 [03:38<00:21,  2.04it/s]#015 91%|█████████▏| 454/497 [03:39<00:21,  2.04it/s]#015 92%|█████████▏| 455/497 [03:39<00:20,  2.06it/s]#015 92%|█████████▏| 456/497 [03:40<00:20,  2.05it/s]#015 92%|█████████▏| 457/497 [03:40<00:19,  2.04it/s]#015 92%|█████████▏| 458/497 [03:41<00:19,  2.02it/s]#015 92%|█████████▏| 459/497 [03:41<00:18,  2.02it/s]#015 93%|█████████▎| 460/497 [03:42<00:18,  2.00it/s]#015 93%|█████████▎| 461/497 [03:42<00:17,  2.01it/s]#015 93%|█████████▎| 462/497 [03:43<00:17,  2.01it/s]#015 93%|█████████▎| 463/497 [03:43<00:16,  2.01it/s]#015 93%|█████████▎| 464/497 [03:44<00:16,  2.01it/s]#015 94%|█████████▎| 465/497 [03:44<00:15,  2.04it/s]#015 94%|█████████▍| 466/497 [03:45<00:15,  2.03it/s]#015 94%|█████████▍| 467/497 [03:45<00:14,  2.03it/s]#015 94%|█████████▍| 468/497 [03:46<00:14,  2.02it/s]#015 94%|█████████▍| 469/497 [03:46<00:13,  2.01it/s]#015 95%|█████████▍| 470/497 [03:47<00:13,  2.00it/s]#015 95%|█████████▍| 471/497 [03:47<00:13,  2.00it/s]#015 95%|█████████▍| 472/497 [03:48<00:12,  2.00it/s]#015 95%|█████████▌| 473/497 [03:48<00:12,  1.99it/s]#015 95%|█████████▌| 474/497 [03:49<00:11,  2.00it/s]#015 96%|█████████▌| 475/497 [03:49<00:11,  1.99it/s]#015 96%|█████████▌| 476/497 [03:50<00:10,  2.00it/s]#015 96%|█████████▌| 477/497 [03:50<00:09,  2.00it/s]#015 96%|█████████▌| 478/497 [03:51<00:09,  2.00it/s]#015 96%|█████████▋| 479/497 [03:51<00:09,  1.99it/s]#015 97%|█████████▋| 480/497 [03:52<00:08,  2.00it/s]#015 97%|█████████▋| 481/497 [03:52<00:07,  2.01it/s]#015 97%|█████████▋| 482/497 [03:53<00:07,  1.97it/s]#015 97%|█████████▋| 483/497 [03:53<00:07,  1.96it/s]#015 97%|█████████▋| 484/497 [03:54<00:06,  1.98it/s]#015 98%|█████████▊| 485/497 [03:54<00:06,  1.95it/s]#015 98%|█████████▊| 486/497 [03:55<00:05,  1.96it/s]#015 98%|█████████▊| 487/497 [03:55<00:05,  1.98it/s]#015 98%|█████████▊| 488/497 [03:56<00:04,  1.99it/s]#015 98%|█████████▊| 489/497 [03:56<00:04,  1.99it/s]#015 99%|█████████▊| 490/497 [03:57<00:03,  2.00it/s]#015 99%|█████████▉| 491/497 [03:57<00:02,  2.03it/s]#015 99%|█████████▉| 492/497 [03:58<00:02,  2.02it/s]#015 99%|█████████▉| 493/497 [03:58<00:01,  2.01it/s]#015 99%|█████████▉| 494/497 [03:59<00:01,  2.04it/s]#015100%|█████████▉| 495/497 [03:59<00:00,  2.02it/s]#015100%|█████████▉| 496/497 [04:00<00:00,  2.02it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/31 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  6%|▋         | 2/31 [00:00<00:05,  5.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 10%|▉         | 3/31 [00:00<00:06,  4.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 13%|█▎        | 4/31 [00:01<00:07,  3.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 16%|█▌        | 5/31 [00:01<00:07,  3.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 19%|█▉        | 6/31 [00:01<00:08,  3.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 23%|██▎       | 7/31 [00:02<00:08,  2.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 26%|██▌       | 8/31 [00:02<00:07,  2.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 29%|██▉       | 9/31 [00:02<00:07,  2.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 32%|███▏      | 10/31 [00:03<00:07,  2.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 35%|███▌      | 11/31 [00:03<00:07,  2.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 39%|███▊      | 12/31 [00:04<00:06,  2.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 42%|████▏     | 13/31 [00:04<00:06,  2.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 45%|████▌     | 14/31 [00:04<00:06,  2.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 48%|████▊     | 15/31 [00:05<00:05,  2.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 52%|█████▏    | 16/31 [00:05<00:05,  2.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 55%|█████▍    | 17/31 [00:05<00:04,  2.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 58%|█████▊    | 18/31 [00:06<00:04,  2.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 61%|██████▏   | 19/31 [00:06<00:04,  2.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 65%|██████▍   | 20/31 [00:06<00:03,  2.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 68%|██████▊   | 21/31 [00:07<00:03,  2.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 71%|███████   | 22/31 [00:07<00:03,  2.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 74%|███████▍  | 23/31 [00:07<00:02,  2.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 77%|███████▋  | 24/31 [00:08<00:02,  2.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 81%|████████  | 25/31 [00:08<00:02,  2.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 84%|████████▍ | 26/31 [00:08<00:01,  2.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 87%|████████▋ | 27/31 [00:09<00:01,  2.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 90%|█████████ | 28/31 [00:09<00:01,  2.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 94%|█████████▎| 29/31 [00:09<00:00,  2.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 97%|█████████▋| 30/31 [00:10<00:00,  2.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 31/31 [00:10<00:00,  3.21it/s]#033[A#015                                                 #015\u001b[0m\n",
      "\u001b[34m#015                                               #015#033[A#015100%|██████████| 497/497 [04:11<00:00,  2.02it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 31/31 [00:10<00:00,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                               #033[A#015                                                 #015#015100%|██████████| 497/497 [04:11<00:00,  2.02it/s]#015100%|██████████| 497/497 [04:11<00:00,  1.98it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/31 [00:00<?, ?it/s]#015  6%|▋         | 2/31 [00:00<00:05,  5.56it/s]#015 10%|▉         | 3/31 [00:00<00:06,  4.25it/s]#015 13%|█▎        | 4/31 [00:01<00:07,  3.66it/s]#015 16%|█▌        | 5/31 [00:01<00:07,  3.34it/s]#015 19%|█▉        | 6/31 [00:01<00:07,  3.14it/s]#015 23%|██▎       | 7/31 [00:02<00:07,  3.01it/s]#015 26%|██▌       | 8/31 [00:02<00:07,  2.93it/s]#015 29%|██▉       | 9/31 [00:02<00:07,  2.89it/s]#015 32%|███▏      | 10/31 [00:03<00:07,  2.85it/s]#015 35%|███▌      | 11/31 [00:03<00:07,  2.83it/s]#015 39%|███▊      | 12/31 [00:03<00:06,  2.81it/s]#015 42%|████▏     | 13/31 [00:04<00:06,  2.81it/s]#015 45%|████▌     | 14/31 [00:04<00:06,  2.80it/s]#015 48%|████▊     | 15/31 [00:05<00:05,  2.79it/s]#015 52%|█████▏    | 16/31 [00:05<00:05,  2.79it/s]#015 55%|█████▍    | 17/31 [00:05<00:04,  2.83it/s]#015 58%|█████▊    | 18/31 [00:06<00:04,  2.85it/s]#015 61%|██████▏   | 19/31 [00:06<00:04,  2.87it/s]#015 65%|██████▍   | 20/31 [00:06<00:03,  2.87it/s]#015 68%|██████▊   | 21/31 [00:07<00:03,  2.87it/s]#015 71%|███████   | 22/31 [00:07<00:03,  2.89it/s]#015 74%|███████▍  | 23/31 [00:07<00:02,  2.90it/s]#015 77%|███████▋  | 24/31 [00:08<00:02,  2.91it/s]#015 81%|████████  | 25/31 [00:08<00:02,  2.91it/s]#015 84%|████████▍ | 26/31 [00:08<00:01,  2.92it/s]#015 87%|████████▋ | 27/31 [00:09<00:01,  2.92it/s]#015 90%|█████████ | 28/31 [00:09<00:01,  2.90it/s]#015 94%|█████████▎| 29/31 [00:09<00:00,  2.91it/s]#015 97%|█████████▋| 30/31 [00:10<00:00,  2.90it/s]#015100%|██████████| 31/31 [00:10<00:00,  3.21it/s]#015100%|██████████| 31/31 [00:10<00:00,  2.96it/s]\u001b[0m\n",
      "\n",
      "2023-06-22 20:09:38 Uploading - Uploading generated training model\n",
      "2023-06-22 20:10:04 Completed - Training job completed\n",
      "Training seconds: 493\n",
      "Billable seconds: 493\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters passed into the training job\n",
    "hyperparameters = {\n",
    "    \"epochs\": 1,\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"train_file\": \"train.csv\",\n",
    "    \"test_file\": \"test.csv\",\n",
    "}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"scripts\",\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    instance_count=1,\n",
    "    transformers_version=\"4.6.1\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    ")\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": val_input_path}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf80375",
   "metadata": {},
   "source": [
    "### Download the trained model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d9b089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-000000000000/huggingface-pytorch-training-2023-06-22-20-00-31-761/output/model.tar.gz to ./model.tar.gz\n",
      "tokenizer.json\n",
      "training_args.bin\n",
      "tokenizer_config.json\n",
      "special_tokens_map.json\n",
      "config.json\n",
      "vocab.txt\n",
      "pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp {huggingface_estimator.model_data} model.tar.gz\n",
    "! mkdir -p {model_path}\n",
    "! tar -xvf model.tar.gz -C  {model_path}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df9a4e",
   "metadata": {},
   "source": [
    "### Prepare model container definition\n",
    "\n",
    "We are going to use the trained model files along with the HuggingFace Inference container to deploy the model to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e68292cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-000000000000/DEMO-NLP-Women-Clothing-1687464029-bfff/hf-model-sm/hf_model.tar.gz'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tarfile.open(\"hf_model.tar.gz\", mode=\"w:gz\") as archive:\n",
    "    archive.add(model_path, recursive=True)\n",
    "    archive.add(\"code/\")\n",
    "directory_name = s3_prefix.split(\"/\")[-1]\n",
    "zipped_model_path = sagemaker_session.upload_data(\n",
    "    path=\"hf_model.tar.gz\", key_prefix=directory_name + \"/hf-model-sm\"\n",
    ")\n",
    "zipped_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb673fa",
   "metadata": {},
   "source": [
    "Create a new model object and then update its model artifact and inference script. The model object will be used to create the SageMaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c957b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Environment': {'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
      "                 'SAGEMAKER_PROGRAM': 'inference.py',\n",
      "                 'SAGEMAKER_REGION': 'us-west-2',\n",
      "                 'SAGEMAKER_SUBMIT_DIRECTORY': ''},\n",
      " 'Image': '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:1.7.1-transformers4.6.1-cpu-py36-ubuntu18.04',\n",
      " 'ModelDataUrl': 's3://sagemaker-us-west-2-000000000000/DEMO-NLP-Women-Clothing-1687464029-bfff/hf-model-sm/hf_model.tar.gz'}\n"
     ]
    }
   ],
   "source": [
    "model = huggingface_estimator.create_model(name=model_name)\n",
    "container_def = model.prepare_container_def(instance_type=instance_type)\n",
    "container_def[\"ModelDataUrl\"] = zipped_model_path\n",
    "container_def[\"Environment\"][\"SAGEMAKER_PROGRAM\"] = \"inference.py\"\n",
    "pprint.pprint(container_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69846253",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088fe56",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "The following parameters are required to create a SageMaker model:\n",
    "\n",
    "* `ExecutionRoleArn`: The ARN of the IAM role that Amazon SageMaker can assume to access the model artifacts/ docker images for deployment\n",
    "\n",
    "* `ModelName`: name of the SageMaker model.\n",
    "\n",
    "* `PrimaryContainer`: The location of the primary docker image containing inference code, associated artifacts, and custom environment map that the inference code uses when the model is deployed for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3cd1bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: DEMO-NLP-Women-Clothing-1687464029-bfff-model\n"
     ]
    }
   ],
   "source": [
    "sagemaker_client.create_model(\n",
    "    ExecutionRoleArn=role,\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer=container_def,\n",
    ")\n",
    "print(f\"Model created: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98431adc",
   "metadata": {},
   "source": [
    "### Create endpoint config\n",
    "\n",
    "Create an endpoint configuration by calling the `create_endpoint_config` API. Here, supply the same `model_name` used in the `create_model` API call. The `create_endpoint_config` now supports the additional parameter `ClarifyExplainerConfig` to enable the Clarify explainer. The SHAP baseline is mandatory, it can be provided either as inline baseline data (the `ShapBaseline` parameter) or by a S3 baseline file (the `ShapBaselineUri` parameter). Baseline dataset type shall be the same as input dataset type, and baseline samples shall only include features. For more details on baseline selection please [refer this documentation](https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/clarify-feature-attribute-shap-baselines.html).\n",
    "\n",
    "Please see [the API documentation](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html) for details on other config parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca7fe6",
   "metadata": {},
   "source": [
    "Here we use a special token as the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91e75947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP baseline: [['<UNK>']]\n"
     ]
    }
   ],
   "source": [
    "baseline = [[\"<UNK>\"]]\n",
    "print(f\"SHAP baseline: {baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808453f8",
   "metadata": {},
   "source": [
    "The `TextConfig` configured with `sentence` level granularity (When granularity is `sentence`, each sentence is a feature, and we need a few sentences per review for good visualization) and the language as English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d988fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:000000000000:endpoint-config/demo-nlp-women-clothing-1687464029-bfff-endpoint-config',\n",
       " 'ResponseMetadata': {'RequestId': 'ad8d98fe-ac16-4227-b80b-570abb94ac58',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ad8d98fe-ac16-4227-b80b-570abb94ac58',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '136',\n",
       "   'date': 'Thu, 22 Jun 2023 20:10:54 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"TestVariant\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InstanceType\": instance_type,\n",
    "        }\n",
    "    ],\n",
    "    ExplainerConfig={\n",
    "        \"ClarifyExplainerConfig\": {\n",
    "            \"InferenceConfig\": {\"FeatureTypes\": [\"text\"]},\n",
    "            \"ShapConfig\": {\n",
    "                \"ShapBaselineConfig\": {\"ShapBaseline\": csv_serializer.serialize(baseline)},\n",
    "                \"TextConfig\": {\"Granularity\": \"sentence\", \"Language\": \"en\"},\n",
    "            },\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8eee82",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "Once you have your model and endpoint configuration ready, use the `create_endpoint` API to create your endpoint. The `endpoint_name` must be unique within an AWS Region in your AWS account. The `create_endpoint` API is synchronous in nature and returns an immediate response with the endpoint status being `Creating` state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c02382a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-west-2:000000000000:endpoint/demo-nlp-women-clothing-1687464029-bfff-endpoint',\n",
       " 'ResponseMetadata': {'RequestId': 'd8f02d0a-b221-4dc3-8a58-317eb9077844',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd8f02d0a-b221-4dc3-8a58-317eb9077844',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '116',\n",
       "   'date': 'Thu, 22 Jun 2023 20:10:55 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7444cb",
   "metadata": {},
   "source": [
    "Wait for the endpoint to be in \"InService\" state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "675d0447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'DEMO-NLP-Women-Clothing-1687464029-bfff-endpoint',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-west-2:000000000000:endpoint/demo-nlp-women-clothing-1687464029-bfff-endpoint',\n",
       " 'EndpointConfigName': 'DEMO-NLP-Women-Clothing-1687464029-bfff-endpoint-config',\n",
       " 'ProductionVariants': [{'VariantName': 'TestVariant',\n",
       "   'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:1.7.1-transformers4.6.1-cpu-py36-ubuntu18.04',\n",
       "     'ResolvedImage': '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference@sha256:97cdf11484b82818b195579c7b5d8f16bc97d600ae352f47667e0587de7ae7f0',\n",
       "     'ResolutionTime': datetime.datetime(2023, 6, 22, 20, 10, 56, 696000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2023, 6, 22, 20, 10, 56, 158000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 6, 22, 20, 13, 15, 436000, tzinfo=tzlocal()),\n",
       " 'ExplainerConfig': {'ClarifyExplainerConfig': {'InferenceConfig': {'FeatureTypes': ['text']},\n",
       "   'ShapConfig': {'ShapBaselineConfig': {'ShapBaseline': '<UNK>'},\n",
       "    'TextConfig': {'Language': 'en', 'Granularity': 'sentence'}}}},\n",
       " 'ResponseMetadata': {'RequestId': 'c3d17eaf-1043-4f7b-bf54-0c7361ab6693',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c3d17eaf-1043-4f7b-bf54-0c7361ab6693',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1068',\n",
       "   'date': 'Thu, 22 Jun 2023 20:13:26 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d668226",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke endpoint\n",
    "\n",
    "There are expanding business needs and legislative regulations that require explanations of _why_ a model made the decision it did. SageMaker Clarify uses SHAP to explain the contribution that each input feature makes to the final decision.\n",
    "\n",
    "Kernel SHAP algorithm requires a baseline (also known as background dataset). By definition, `baseline` should either be a S3 URI to the baseline dataset file, or an in-place list of records. Baseline dataset type shall be the same as the original request data type, and baseline records shall only include features. \n",
    "\n",
    "Below are the several different combination of endpoint invocation, call them one by one and visualize the explanations by running the subsequent cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89102478",
   "metadata": {},
   "source": [
    "### Single record request\n",
    "\n",
    "Put only one record in the request body, and then send the request to the endpoint to get its predictions and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f13b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85f73d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Body': <botocore.response.StreamingBody object at 0x7fbc8bb4ffd0>,\n",
      " 'ContentType': 'application/json',\n",
      " 'InvokedProductionVariant': 'TestVariant',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',\n",
      "                                      'content-length': '809',\n",
      "                                      'content-type': 'application/json',\n",
      "                                      'date': 'Thu, 22 Jun 2023 20:13:28 GMT',\n",
      "                                      'x-amzn-invoked-production-variant': 'TestVariant',\n",
      "                                      'x-amzn-requestid': '3acca534-1feb-42dc-b322-9f8d64f27e75'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '3acca534-1feb-42dc-b322-9f8d64f27e75',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",\n",
    "    Accept=\"text/csv\",\n",
    "    Body=csv_serializer.serialize(test_data.iloc[:num_records, :].to_numpy()),\n",
    ")\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ec37533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanations': {'kernel_shap': [[{'attributions': [{'attribution': [0.06300842799999996],\n",
      "                                                      'description': {'partial_text': 'I '\n",
      "                                                                                      'am '\n",
      "                                                                                      '5\\'6\", '\n",
      "                                                                                      '130 '\n",
      "                                                                                      'lbs '\n",
      "                                                                                      'with '\n",
      "                                                                                      'an '\n",
      "                                                                                      'athletic '\n",
      "                                                                                      'body '\n",
      "                                                                                      'type '\n",
      "                                                                                      'and '\n",
      "                                                                                      'i '\n",
      "                                                                                      'ordered '\n",
      "                                                                                      'a '\n",
      "                                                                                      'size '\n",
      "                                                                                      'small.',\n",
      "                                                                      'start_idx': 0}},\n",
      "                                                     {'attribution': [-0.17682224599999996],\n",
      "                                                      'description': {'partial_text': 'these '\n",
      "                                                                                      'were '\n",
      "                                                                                      'really '\n",
      "                                                                                      'baggy '\n",
      "                                                                                      'in '\n",
      "                                                                                      'the '\n",
      "                                                                                      'thigh/quadricep '\n",
      "                                                                                      'area '\n",
      "                                                                                      'and '\n",
      "                                                                                      'made '\n",
      "                                                                                      'my '\n",
      "                                                                                      'thighs '\n",
      "                                                                                      'look '\n",
      "                                                                                      'bulky.',\n",
      "                                                                      'start_idx': 74}},\n",
      "                                                     {'attribution': [0.19618576600000012],\n",
      "                                                      'description': {'partial_text': 'the '\n",
      "                                                                                      'fabric '\n",
      "                                                                                      'quality '\n",
      "                                                                                      'is '\n",
      "                                                                                      'very '\n",
      "                                                                                      'nice '\n",
      "                                                                                      'and '\n",
      "                                                                                      'i '\n",
      "                                                                                      'like '\n",
      "                                                                                      'the '\n",
      "                                                                                      'idea '\n",
      "                                                                                      'of '\n",
      "                                                                                      'them '\n",
      "                                                                                      'for '\n",
      "                                                                                      'curvier '\n",
      "                                                                                      'body '\n",
      "                                                                                      'types.',\n",
      "                                                                      'start_idx': 157}},\n",
      "                                                     {'attribution': [0.0976928519999999],\n",
      "                                                      'description': {'partial_text': 'my '\n",
      "                                                                                      'son '\n",
      "                                                                                      'commented '\n",
      "                                                                                      'that '\n",
      "                                                                                      'they '\n",
      "                                                                                      'looked '\n",
      "                                                                                      'like '\n",
      "                                                                                      'pajama '\n",
      "                                                                                      'pants '\n",
      "                                                                                      'and '\n",
      "                                                                                      'i '\n",
      "                                                                                      'agreed.',\n",
      "                                                                      'start_idx': 241}}],\n",
      "                                    'feature_type': 'text'}]]},\n",
      " 'predictions': {'content_type': 'text/csv', 'data': '0.9713779\\n'},\n",
      " 'version': '1.0'}\n"
     ]
    }
   ],
   "source": [
    "result = json_deserializer.deserialize(response[\"Body\"], content_type=response[\"ContentType\"])\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8ac31c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>True</b></text></td><td><text style=\"padding-right:2em\"><b>0.92</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I am 5'6\", 130 lbs with an athletic body type and i ordered a size small.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> these were really baggy in the thigh/quadricep area and made my thighs look bulky.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the fabric quality is very nice and i like the idea of them for curvier body types.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> my son commented that they looked like pajama pants and i agreed.                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_result(result, df_test[label_header][:num_records])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4b2f5",
   "metadata": {},
   "source": [
    "### Single record request, no explanation\n",
    "\n",
    "Use the `EnableExplanations` parameter to disable the explanations for this request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf1fc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74c78fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Body': <botocore.response.StreamingBody object at 0x7fbc8cd62440>,\n",
      " 'ContentType': 'application/json',\n",
      " 'InvokedProductionVariant': 'TestVariant',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',\n",
      "                                      'content-length': '98',\n",
      "                                      'content-type': 'application/json',\n",
      "                                      'date': 'Thu, 22 Jun 2023 20:13:28 GMT',\n",
      "                                      'x-amzn-invoked-production-variant': 'TestVariant',\n",
      "                                      'x-amzn-requestid': '268abfc9-870c-4423-8e38-30a86add6a20'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '268abfc9-870c-4423-8e38-30a86add6a20',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",\n",
    "    Accept=\"text/csv\",\n",
    "    Body=csv_serializer.serialize(test_data.iloc[:num_records, :].to_numpy()),\n",
    "    EnableExplanations=\"`false`\",  # Do not provide explanations\n",
    ")\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0346b17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanations': {},\n",
      " 'predictions': {'content_type': 'text/csv', 'data': '0.9713779\\n'},\n",
      " 'version': '1.0'}\n"
     ]
    }
   ],
   "source": [
    "result = json_deserializer.deserialize(response[\"Body\"], content_type=response[\"ContentType\"])\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59e4b8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Clarify explanations for the record(s)\n"
     ]
    }
   ],
   "source": [
    "visualize_result(result, df_test[label_header][:num_records])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d80c4f6",
   "metadata": {},
   "source": [
    "### Batch request, explain both\n",
    "\n",
    "Put two records in the request body, and then send the request to the endpoint to get their predictions and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f306d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13e3a94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Body': <botocore.response.StreamingBody object at 0x7fbc8bb4ee30>,\n",
      " 'ContentType': 'application/json',\n",
      " 'InvokedProductionVariant': 'TestVariant',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',\n",
      "                                      'content-length': '1574',\n",
      "                                      'content-type': 'application/json',\n",
      "                                      'date': 'Thu, 22 Jun 2023 20:13:31 GMT',\n",
      "                                      'x-amzn-invoked-production-variant': 'TestVariant',\n",
      "                                      'x-amzn-requestid': '168b10a5-27d2-479a-a5cc-bb25d41e7030'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '168b10a5-27d2-479a-a5cc-bb25d41e7030',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",\n",
    "    Accept=\"text/csv\",\n",
    "    Body=csv_serializer.serialize(test_data.iloc[:num_records, :].to_numpy()),\n",
    ")\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f1fa15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanations': {'kernel_shap': [[{'attributions': [{'attribution': [0.06300842799999996],\n",
      "                                                      'description': {'partial_text': 'I '\n",
      "                                                                                      'am '\n",
      "                                                                                      '5\\'6\", '\n",
      "                                                                                      '130 '\n",
      "                                                                                      'lbs '\n",
      "                                                                                      'with '\n",
      "                                                                                      'an '\n",
      "                                                                                      'athletic '\n",
      "                                                                                      'body '\n",
      "                                                                                      'type '\n",
      "                                                                                      'and '\n",
      "                                                                                      'i '\n",
      "                                                                                      'ordered '\n",
      "                                                                                      'a '\n",
      "                                                                                      'size '\n",
      "                                                                                      'small.',\n",
      "                                                                      'start_idx': 0}},\n",
      "                                                     {'attribution': [-0.17682224599999996],\n",
      "                                                      'description': {'partial_text': 'these '\n",
      "                                                                                      'were '\n",
      "                                                                                      'really '\n",
      "                                                                                      'baggy '\n",
      "                                                                                      'in '\n",
      "                                                                                      'the '\n",
      "                                                                                      'thigh/quadricep '\n",
      "                                                                                      'area '\n",
      "                                                                                      'and '\n",
      "                                                                                      'made '\n",
      "                                                                                      'my '\n",
      "                                                                                      'thighs '\n",
      "                                                                                      'look '\n",
      "                                                                                      'bulky.',\n",
      "                                                                      'start_idx': 74}},\n",
      "                                                     {'attribution': [0.19618576600000012],\n",
      "                                                      'description': {'partial_text': 'the '\n",
      "                                                                                      'fabric '\n",
      "                                                                                      'quality '\n",
      "                                                                                      'is '\n",
      "                                                                                      'very '\n",
      "                                                                                      'nice '\n",
      "                                                                                      'and '\n",
      "                                                                                      'i '\n",
      "                                                                                      'like '\n",
      "                                                                                      'the '\n",
      "                                                                                      'idea '\n",
      "                                                                                      'of '\n",
      "                                                                                      'them '\n",
      "                                                                                      'for '\n",
      "                                                                                      'curvier '\n",
      "                                                                                      'body '\n",
      "                                                                                      'types.',\n",
      "                                                                      'start_idx': 157}},\n",
      "                                                     {'attribution': [0.0976928519999999],\n",
      "                                                      'description': {'partial_text': 'my '\n",
      "                                                                                      'son '\n",
      "                                                                                      'commented '\n",
      "                                                                                      'that '\n",
      "                                                                                      'they '\n",
      "                                                                                      'looked '\n",
      "                                                                                      'like '\n",
      "                                                                                      'pajama '\n",
      "                                                                                      'pants '\n",
      "                                                                                      'and '\n",
      "                                                                                      'i '\n",
      "                                                                                      'agreed.',\n",
      "                                                                      'start_idx': 241}}],\n",
      "                                    'feature_type': 'text'}],\n",
      "                                  [{'attributions': [{'attribution': [0.0625544215],\n",
      "                                                      'description': {'partial_text': 'The '\n",
      "                                                                                      'design '\n",
      "                                                                                      'on '\n",
      "                                                                                      'the '\n",
      "                                                                                      'blue '\n",
      "                                                                                      'sweater '\n",
      "                                                                                      'is '\n",
      "                                                                                      'actually '\n",
      "                                                                                      'a '\n",
      "                                                                                      'dark '\n",
      "                                                                                      'navy '\n",
      "                                                                                      '(not '\n",
      "                                                                                      'black, '\n",
      "                                                                                      'as '\n",
      "                                                                                      'i '\n",
      "                                                                                      'thought '\n",
      "                                                                                      'it '\n",
      "                                                                                      'was), '\n",
      "                                                                                      'but '\n",
      "                                                                                      'it '\n",
      "                                                                                      'still '\n",
      "                                                                                      'looks '\n",
      "                                                                                      'beautiful '\n",
      "                                                                                      'with '\n",
      "                                                                                      'black '\n",
      "                                                                                      'underneath.',\n",
      "                                                                      'start_idx': 0}},\n",
      "                                                     {'attribution': [0.0713343185],\n",
      "                                                      'description': {'partial_text': 'rather '\n",
      "                                                                                      'than '\n",
      "                                                                                      'jeans '\n",
      "                                                                                      'like '\n",
      "                                                                                      \"it's \"\n",
      "                                                                                      'shown, '\n",
      "                                                                                      'the '\n",
      "                                                                                      'v-neck '\n",
      "                                                                                      'is '\n",
      "                                                                                      'a '\n",
      "                                                                                      'nice '\n",
      "                                                                                      'change '\n",
      "                                                                                      'from '\n",
      "                                                                                      'other '\n",
      "                                                                                      'cardigans '\n",
      "                                                                                      'i '\n",
      "                                                                                      'have '\n",
      "                                                                                      '- '\n",
      "                                                                                      'looks '\n",
      "                                                                                      'great '\n",
      "                                                                                      'with '\n",
      "                                                                                      'a '\n",
      "                                                                                      'cami '\n",
      "                                                                                      'or '\n",
      "                                                                                      'another '\n",
      "                                                                                      'v-neck '\n",
      "                                                                                      'underneath '\n",
      "                                                                                      'it.',\n",
      "                                                                      'start_idx': 141}},\n",
      "                                                     {'attribution': [-0.007584672500000035],\n",
      "                                                      'description': {'partial_text': 'soft, '\n",
      "                                                                                      'nice '\n",
      "                                                                                      'medium-weight '\n",
      "                                                                                      'and '\n",
      "                                                                                      'not '\n",
      "                                                                                      'at '\n",
      "                                                                                      'all '\n",
      "                                                                                      'itchy.',\n",
      "                                                                      'start_idx': 291}},\n",
      "                                                     {'attribution': [0.0749991925],\n",
      "                                                      'description': {'partial_text': 'happy '\n",
      "                                                                                      'with '\n",
      "                                                                                      'this '\n",
      "                                                                                      'as '\n",
      "                                                                                      'an '\n",
      "                                                                                      'easy '\n",
      "                                                                                      'everyday '\n",
      "                                                                                      'sweater.',\n",
      "                                                                      'start_idx': 338}}],\n",
      "                                    'feature_type': 'text'}]]},\n",
      " 'predictions': {'content_type': 'text/csv', 'data': '0.9713779\\n0.99261636\\n'},\n",
      " 'version': '1.0'}\n"
     ]
    }
   ],
   "source": [
    "result = json_deserializer.deserialize(response[\"Body\"], content_type=response[\"ContentType\"])\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95f9005e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>True</b></text></td><td><text style=\"padding-right:2em\"><b>0.92</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I am 5'6\", 130 lbs with an athletic body type and i ordered a size small.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> these were really baggy in the thigh/quadricep area and made my thighs look bulky.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the fabric quality is very nice and i like the idea of them for curvier body types.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> my son commented that they looked like pajama pants and i agreed.                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.99)</b></text></td><td><text style=\"padding-right:2em\"><b>True</b></text></td><td><text style=\"padding-right:2em\"><b>2.68</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 59%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The design on the blue sweater is actually a dark navy (not black, as i thought it was), but it still looks beautiful with black underneath.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 53%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rather than jeans like it's shown, the v-neck is a nice change from other cardigans i have - looks great with a cami or another v-neck underneath it.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> soft, nice medium-weight and not at all itchy.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> happy with this as an easy everyday sweater.                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_result(result, df_test[label_header][:num_records])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb60531",
   "metadata": {},
   "source": [
    "### Batch request with more records, explain some of the records\n",
    "\n",
    "Put a few more records to the request body, and then use the `EnableExplanations` expression to filter the records to be explained according to their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71a2f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d038336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Body': <botocore.response.StreamingBody object at 0x7fbc8bb4db70>,\n",
      " 'ContentType': 'application/json',\n",
      " 'InvokedProductionVariant': 'TestVariant',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',\n",
      "                                      'content-length': '1340',\n",
      "                                      'content-type': 'application/json',\n",
      "                                      'date': 'Thu, 22 Jun 2023 20:13:33 GMT',\n",
      "                                      'x-amzn-invoked-production-variant': 'TestVariant',\n",
      "                                      'x-amzn-requestid': 'd3ddf9ed-042d-41de-ad26-71a77fc90d0b'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'd3ddf9ed-042d-41de-ad26-71a77fc90d0b',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",\n",
    "    Accept=\"text/csv\",\n",
    "    Body=csv_serializer.serialize(test_data.iloc[:num_records, :].to_numpy()),\n",
    "    EnableExplanations=\"[0]>`0.99`\",  # Explain a record only when its prediction meets the condition\n",
    ")\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a331c536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanations': {'kernel_shap': [None,\n",
      "                                  [{'attributions': [{'attribution': [0.0625544215],\n",
      "                                                      'description': {'partial_text': 'The '\n",
      "                                                                                      'design '\n",
      "                                                                                      'on '\n",
      "                                                                                      'the '\n",
      "                                                                                      'blue '\n",
      "                                                                                      'sweater '\n",
      "                                                                                      'is '\n",
      "                                                                                      'actually '\n",
      "                                                                                      'a '\n",
      "                                                                                      'dark '\n",
      "                                                                                      'navy '\n",
      "                                                                                      '(not '\n",
      "                                                                                      'black, '\n",
      "                                                                                      'as '\n",
      "                                                                                      'i '\n",
      "                                                                                      'thought '\n",
      "                                                                                      'it '\n",
      "                                                                                      'was), '\n",
      "                                                                                      'but '\n",
      "                                                                                      'it '\n",
      "                                                                                      'still '\n",
      "                                                                                      'looks '\n",
      "                                                                                      'beautiful '\n",
      "                                                                                      'with '\n",
      "                                                                                      'black '\n",
      "                                                                                      'underneath.',\n",
      "                                                                      'start_idx': 0}},\n",
      "                                                     {'attribution': [0.0713343185],\n",
      "                                                      'description': {'partial_text': 'rather '\n",
      "                                                                                      'than '\n",
      "                                                                                      'jeans '\n",
      "                                                                                      'like '\n",
      "                                                                                      \"it's \"\n",
      "                                                                                      'shown, '\n",
      "                                                                                      'the '\n",
      "                                                                                      'v-neck '\n",
      "                                                                                      'is '\n",
      "                                                                                      'a '\n",
      "                                                                                      'nice '\n",
      "                                                                                      'change '\n",
      "                                                                                      'from '\n",
      "                                                                                      'other '\n",
      "                                                                                      'cardigans '\n",
      "                                                                                      'i '\n",
      "                                                                                      'have '\n",
      "                                                                                      '- '\n",
      "                                                                                      'looks '\n",
      "                                                                                      'great '\n",
      "                                                                                      'with '\n",
      "                                                                                      'a '\n",
      "                                                                                      'cami '\n",
      "                                                                                      'or '\n",
      "                                                                                      'another '\n",
      "                                                                                      'v-neck '\n",
      "                                                                                      'underneath '\n",
      "                                                                                      'it.',\n",
      "                                                                      'start_idx': 141}},\n",
      "                                                     {'attribution': [-0.007584672500000035],\n",
      "                                                      'description': {'partial_text': 'soft, '\n",
      "                                                                                      'nice '\n",
      "                                                                                      'medium-weight '\n",
      "                                                                                      'and '\n",
      "                                                                                      'not '\n",
      "                                                                                      'at '\n",
      "                                                                                      'all '\n",
      "                                                                                      'itchy.',\n",
      "                                                                      'start_idx': 291}},\n",
      "                                                     {'attribution': [0.0749991925],\n",
      "                                                      'description': {'partial_text': 'happy '\n",
      "                                                                                      'with '\n",
      "                                                                                      'this '\n",
      "                                                                                      'as '\n",
      "                                                                                      'an '\n",
      "                                                                                      'easy '\n",
      "                                                                                      'everyday '\n",
      "                                                                                      'sweater.',\n",
      "                                                                      'start_idx': 338}}],\n",
      "                                    'feature_type': 'text'}],\n",
      "                                  None,\n",
      "                                  [{'attributions': [{'attribution': [0.0685000183333333],\n",
      "                                                      'description': {'partial_text': 'A '\n",
      "                                                                                      'very '\n",
      "                                                                                      'versatile '\n",
      "                                                                                      'and '\n",
      "                                                                                      'cozy '\n",
      "                                                                                      'top.',\n",
      "                                                                      'start_idx': 0}},\n",
      "                                                     {'attribution': [0.06710124333333331],\n",
      "                                                      'description': {'partial_text': 'would '\n",
      "                                                                                      'look '\n",
      "                                                                                      'great '\n",
      "                                                                                      'dressed '\n",
      "                                                                                      'up '\n",
      "                                                                                      'or '\n",
      "                                                                                      'down '\n",
      "                                                                                      'for '\n",
      "                                                                                      'a '\n",
      "                                                                                      'casual '\n",
      "                                                                                      'comfy '\n",
      "                                                                                      'fall '\n",
      "                                                                                      'day.',\n",
      "                                                                      'start_idx': 31}},\n",
      "                                                     {'attribution': [0.06915193833333336],\n",
      "                                                      'description': {'partial_text': 'what '\n",
      "                                                                                      'a '\n",
      "                                                                                      'fun '\n",
      "                                                                                      'piece '\n",
      "                                                                                      'for '\n",
      "                                                                                      'my '\n",
      "                                                                                      'wardrobe!',\n",
      "                                                                      'start_idx': 96}}],\n",
      "                                    'feature_type': 'text'}]]},\n",
      " 'predictions': {'content_type': 'text/csv',\n",
      "                 'data': '0.9713779\\n0.99261636\\n0.29229787\\n0.9960663\\n'},\n",
      " 'version': '1.0'}\n"
     ]
    }
   ],
   "source": [
    "result = json_deserializer.deserialize(response[\"Body\"], content_type=response[\"ContentType\"])\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4488886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.99)</b></text></td><td><text style=\"padding-right:2em\"><b>True</b></text></td><td><text style=\"padding-right:2em\"><b>2.68</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 59%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The design on the blue sweater is actually a dark navy (not black, as i thought it was), but it still looks beautiful with black underneath.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 53%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rather than jeans like it's shown, the v-neck is a nice change from other cardigans i have - looks great with a cami or another v-neck underneath it.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> soft, nice medium-weight and not at all itchy.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> happy with this as an easy everyday sweater.                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>True</b></text></td><td><text style=\"padding-right:2em\"><b>2.96</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> A very versatile and cozy top.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 52%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> would look great dressed up or down for a casual comfy fall day.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what a fun piece for my wardrobe!                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_result(result, df_test[label_header][:num_records])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59483bc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Finally, don’t forget to clean up the resources we set up and used for this demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e06f3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b72a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbd832e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_model(ModelName=model_name);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910062b8",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/sagemaker-clarify|online_explainability|natural_language_processing|nlp_online_explainability_with_sagemaker_clarify.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
