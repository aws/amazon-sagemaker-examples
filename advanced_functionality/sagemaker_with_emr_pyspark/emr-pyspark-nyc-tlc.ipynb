{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation in Spark cluster for ML in Amazon SageMaker\n",
    "\n",
    "This example prepares [New York City Taxi and Limousine Commission Trip Record Data](https://registry.opendata.aws/nyc-tlc-trip-records-pds/) dataset for machine learning in [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "\n",
    "This example requires that this Jupyter notebook is running in an [Amazon SageMaker Notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html) attached to a Spark cluster. So, if you have not already done so, let us first create an Amazon SageMaker Notebook instance either attached to an AWS Glue Development Endpoint (Option 1), or  attached to an Amazon EMR cluster (Option 2) as described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Create Amazon SageMaker Notebook instance attached to AWS Glue Development Endpoint\n",
    "\n",
    "The first option is to create an Amazon SageMaker Notebook instance attached to an [AWS Glue Development Endpoint](https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint.html), as described below:\n",
    "\n",
    "  -  [Add an AWS Glue Development Endpoint](https://docs.aws.amazon.com/glue/latest/dg/add-dev-endpoint.html)\n",
    "  -  [Use an Amazon SageMaker Notebook with your AWS Glue Development Endpoint](https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-sage.html)\n",
    "\n",
    "### Option 2: Create Amazon SageMaker Notebook instance attached to Amazon EMR\n",
    "\n",
    "The second option is to create an [Amazon EMR Cluster](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-launch.html) using <b>Advanced Options</b> to include <b>Spark and Livy</b> in the software for the EMR cluster. Create Amazon SageMaker notebook instance attached to the Amazon EMR cluster you just created following this [Amazon blog post](https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/). \n",
    "\n",
    "After you create the Amazon SageMaker Notebook instance using one of the two options described above, reopen this Jupyter notebook from the Amazon SageMaker notebook instance you just created, so this Juypyter notebook can use the attached Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure PySpark\n",
    "\n",
    "Below we configure PySpark to use Python 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\":{\n",
    "          \"spark.pyspark.python\": \"python3\",\n",
    "          \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "          \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "          \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into a DataFrame \n",
    "Next, we create a Resilient Distributed Dataset (RDD) from a CSV file stored in S3 bucket as part of [New York City Taxi and Limousine Commission (TLC) Trip Record Data](https://registry.opendata.aws/nyc-tlc-trip-records-pds/) dataset in [Registry of Open Data on AWS](https://registry.opendata.aws/). We convert the RDD to a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads RDD\n",
    "lines = sc.textFile(\"s3://nyc-tlc/misc/uber_nyc_data.csv\")\n",
    "# Split lines into columns; change split() argument depending on deliminiter e.g. '\\t'\n",
    "parts = lines.map(lambda l: l.split(','))\n",
    "# Convert RDD into DataFrame\n",
    "uber_df = spark.createDataFrame(parts, ['id','origin_taz','destination_taz','pickup_datetime','trip_distance','trip_duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the schema of the data we just loaded and also show 10 rows to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uber_df.printSchema())\n",
    "uber_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data ###\n",
    "Next, we clean the data. We drop any rows with any NULL or NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up data \n",
    "# remove id column as we don't need it\n",
    "uber_df1=uber_df.drop(uber_df.id)\n",
    "\n",
    "# drop all rows with any null value\n",
    "uber_df1=uber_df1.dropna(how='any')\n",
    "\n",
    "# filter rows where destnation, orign and trip duration are not set to NULL\n",
    "uber_df1=uber_df1.filter((uber_df1.destination_taz != 'NULL')  & \n",
    "    (uber_df1.origin_taz != 'NULL')  & \n",
    "    (uber_df1.trip_duration != 'NULL')  & \n",
    "    (uber_df1.destination_taz != 'destination_taz'))\n",
    "\n",
    "# show 10 rows\n",
    "uber_df1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PySpark user-defined functions ###\n",
    "Below, we import relevant Python clasess for defining PySpark user-defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, to_timestamp\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function for extracting ordinal day of the week from pickup date timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define UDF for extracting pickup day of the week from datetime\n",
    "\n",
    "def weekday(x):\n",
    "    pickup=datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "    return int(pickup.date().weekday())\n",
    "    \n",
    "pickup_day_udf = udf(weekday, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function for extracting month from the pickup date timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define month udf for extracting pickup month from datetime\n",
    "def month(x):\n",
    "    pickup=datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "    return int(pickup.date().month)\n",
    "    \n",
    "pickup_month_udf = udf(month, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function for extracting hour of the day from the pickup date timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pickup_time udf for extracting pickup hour from datetime\n",
    "\n",
    "def pickup_time(x):\n",
    "    ptime = datetime.strptime(x, '%Y-%m-%d %H:%M:%S').time()\n",
    "    return int(ptime.hour)\n",
    "    \n",
    "pickup_time_udf = udf(pickup_time, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a PySpark user-defined function that parses source and target zones as hexadecimal integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_taz(x):\n",
    "   return int(x, 16)\n",
    "\n",
    "taz_udf=udf(encode_taz, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function that computes duration of the trip in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define duration udf for extracting duration in minutes\n",
    "def duration(x):\n",
    "    time=x.split(':')\n",
    "    duration = int(time[0]*60) + int(time[1])\n",
    "    return duration\n",
    "\n",
    "duration_udf = udf(duration, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for SageMaker XGBoost algorithm ###\n",
    "SageMaker XGBoost algorithm expects the label to be the first column. So, we transform the PySpark DataFrame to make `duration` as the first column, because we want to train the model to predict duration of an Uber ride, given ride pickup source and target zones, the month of the year, the day of the week, and hour of the day. Columns of the DataFrame are transformed using PySpark user-defined functions defined above. \n",
    "\n",
    "We also drop any rows with Null or NaN values as a result of transformations. We filter rows to keep rows with duration greater than 0 but less than 120 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data frame\n",
    "# we want trip duration (minutes) in the first column as label for the row\n",
    "# our feature vector includes origin, desination, and pickup month, day, and hour\n",
    "# we will discard other columms\n",
    "uber_df2 = uber_df1.select(duration_udf(uber_df1.trip_duration).alias('duration'),\n",
    "    taz_udf(uber_df1.origin_taz).alias('origin'), \n",
    "    taz_udf(uber_df1.destination_taz).alias('destination'), \n",
    "    pickup_month_udf(uber_df1.pickup_datetime).alias('month'), \n",
    "    pickup_day_udf(uber_df1.pickup_datetime).alias('day'), \n",
    "    pickup_time_udf(uber_df1.pickup_datetime).alias('pickup_time'))\n",
    "\n",
    "uber_df3 = uber_df2.dropna(how='any')\n",
    "uber_df4 = uber_df3.filter((uber_df3.duration > 0) & (uber_df3.duration < 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show \n",
    "uber_df4.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save prepared data in S3 bucket ###\n",
    "Finally, we save the transformed PySpark DataFrame in S3 bucket, so we set the S3 bucket name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket name for saving PySpark output\n",
    "bucket_name="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a PySpark DataFrame is a RDD, data will be saved in S3 in multiple files in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prepared data frame S3 bucket\n",
    "uber_df4.write.save(f\"s3://{bucket_name}/emr/output/uber_nyc/v1\", format='csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
