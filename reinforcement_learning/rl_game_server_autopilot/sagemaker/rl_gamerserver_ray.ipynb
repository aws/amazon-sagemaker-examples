{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game servers autopilot\n",
    "Multiplayer game publishers often need to either over-provision resources or manually manage compute resource allocation when launching a large-scale worldwide game, to avoid the long player-wait in the game lobby. Game publishers need to develop, config, and deploy tools that helped them to monitor and control the compute allocation.\n",
    "\n",
    "This notebook demonstrates Game server autopilot, a new machine learning-based example tool that makes it easy for game publishers to reduce the time players wait for compute to spawn, while still avoiding compute over-provisioning. It also eliminates manual configuration decisions and changes publishers need to make and reduces the opportunity for human errors.\n",
    "\n",
    "We heard from customers that optimizing compute resource allocation is not trivial. This is because it often takes substantial time to allocate and prepare EC2 instances. The time needed to spin up an EC2 instance and install game binaries and other assets must be learned and accounted for in the allocation algorithm. Ever-changing usage patterns require a model that is adaptive to emerging player habits. Finally, the system also performs scale down in concert with new server allocation as needed.\n",
    "\n",
    "We describe a reinforcement learning-based system that learns to allocate resources in response to player usage patterns. The hosted model directly predicts the required number of game-servers so as to allow EKS the time to allocate instances to reduce player wait time. The training process integrates with the game eco-system, and requires minimal manual configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites \n",
    "\n",
    "### Imports\n",
    "\n",
    "To get started, we'll import the Python libraries we need, set up the environment with a few prerequisites for permissions and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "sys.path.append(\"common\")\n",
    "from misc import get_execution_role, wait_for_s3_object\n",
    "from docker_utils import build_and_push_docker_image\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup S3 bucket\n",
    "\n",
    "Set up the linkage and authentication to the S3 bucket that you want to use for checkpoint and the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://sagemaker-us-west-2-356566070122/\n"
     ]
    }
   ],
   "source": [
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = sage_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Adding new parameters for the job require update in the training section that invokes the RLEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix = 'rl-game-server-autopilot'\n",
    "job_duration_in_seconds = 60 * 60 * 24 * 5\n",
    "train_instance_count = 1\n",
    "cloudwatch_namespace = 'rl-game-server-autopilot'\n",
    "min_servers=10\n",
    "max_servers=100\n",
    "# over provisionning factor. use 5 for optimal. \n",
    "over_prov_factor=5\n",
    "#gamma is the discount factor\n",
    "gamma=0.9\n",
    "# if local inference is set gs_inventory_url=local and populate learning_freq\n",
    "gs_inventory_url = 'https://4bfiebw6ui.execute-api.us-west-2.amazonaws.com/api/currsine1h/'\n",
    "#gs_inventory_url = 'local'\n",
    "# sleep time in seconds between step() executions\n",
    "learning_freq = 65\n",
    "# actions are normelized between 0 and 1, action factor the number of game servers needed e.g. 100 will be 100*action and clipped to the min and max servers parameters above\n",
    "action_factor = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pick the instance type\n",
    "instance_type = \"ml.c5.xlarge\" #4 cpus\n",
    "#     instance_type = \"ml.c5.4xlarge\" #16 cpus\n",
    "#      instance_type = \"ml.c5.2xlarge\" #8 cpus\n",
    "#      instance_type = \"ml.c4.4xlarge\"\n",
    "#     instance_type = \"ml.p2.8xlarge\" #32 cpus\n",
    "#     instance_type = \"ml.p3.2xlarge\" #8 cpus\n",
    "#    instance_type = \"ml.p3.8xlarge\" #32 cpus\n",
    "#     instance_type = \"ml.p3.16xlarge\" #96 cpus\n",
    "#     instance_type = \"ml.c5.18xlarge\" #72 cpus\n",
    "\n",
    "num_cpus_per_instance = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an IAM role\n",
    "\n",
    "Either get the execution role when running from a SageMaker notebook instance `role = sagemaker.get_execution_role()` or, when running from local notebook instance, use utils method `role = get_execution_role()` to create an execution role. In this example, the env thru the training job, publishes cloudwatch custom metrics as well as put values in DynamoDB table. Therefore, an appropriate role is required to be set to the role arn below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using IAM role arn: arn:aws:iam::356566070122:role/service-role/AmazonSageMaker-ExecutionRole-20181024T210472\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    role = get_execution_role()\n",
    "\n",
    "print(\"Using IAM role arn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment\n",
    "\n",
    "The environment is defined in a Python file called gameserver_env.py and the file is uploaded on /src directory.\n",
    "The environment also implements the init(), step() and reset() functions that describe how the environment behaves. This is consistent with Open AI Gym interfaces for defining an environment. It also implements help functions for custom CloudWatch metrics (populate_cloudwatch_metric()) and a simple sine demand simulator (get_curr_sine1h())\n",
    "\n",
    "1. init() - initialize the environment in a pre-defined state\n",
    "2. step() - take an action on the environment\n",
    "3. reset()- restart the environment on a new episode\n",
    "4. get_curr_sine1h() - return the sine value based on the current second.\n",
    "5. populate_cloudwatch_metric(namespace,metric_value,metric_name) - populate the metric_name with metric_value in namespace. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrequests\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgym\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gmtime,strftime\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgym.spaces\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Discrete, Box\r\n",
      "\r\n",
      "cloudwatch_cli = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33mcloudwatch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,region_name=\u001b[33m'\u001b[39;49;00m\u001b[33mus-west-2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      " \r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mGameServerEnv\u001b[39;49;00m(gym.Env):\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, env_config={}):\r\n",
      "        \u001b[34mprint\u001b[39;49;00m (\u001b[33m\"\u001b[39;49;00m\u001b[33min __init__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m (\u001b[33m\"\u001b[39;49;00m\u001b[33menv_config {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(env_config))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.namespace = env_config[\u001b[33m'\u001b[39;49;00m\u001b[33mcloudwatch_namespace\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.gs_inventory_url = env_config[\u001b[33m'\u001b[39;49;00m\u001b[33mgs_inventory_url\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.learning_freq = env_config[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_freq\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.min_servers = \u001b[36mint\u001b[39;49;00m(env_config[\u001b[33m'\u001b[39;49;00m\u001b[33mmin_servers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "        \u001b[36mself\u001b[39;49;00m.max_servers = \u001b[36mint\u001b[39;49;00m(env_config[\u001b[33m'\u001b[39;49;00m\u001b[33mmax_servers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "        \u001b[36mself\u001b[39;49;00m.action_factor = \u001b[36mint\u001b[39;49;00m(env_config[\u001b[33m'\u001b[39;49;00m\u001b[33maction_factor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "        \u001b[36mself\u001b[39;49;00m.over_prov_factor = \u001b[36mint\u001b[39;49;00m(env_config[\u001b[33m'\u001b[39;49;00m\u001b[33mover_prov_factor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "        \u001b[36mself\u001b[39;49;00m.num_steps = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.max_num_steps = \u001b[34m301\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.history_len = \u001b[34m5\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.total_num_of_obs = \u001b[34m1\u001b[39;49;00m\r\n",
      "        \u001b[37m# we have two observation array, allocation and demand. allocation is alloc_observation, demand is observation hence *2\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.observation_space = Box(low=np.array([\u001b[36mself\u001b[39;49;00m.min_servers]*\u001b[36mself\u001b[39;49;00m.history_len*\u001b[34m2\u001b[39;49;00m),\r\n",
      "                                           high=np.array([\u001b[36mself\u001b[39;49;00m.max_servers]*\u001b[36mself\u001b[39;49;00m.history_len*\u001b[34m2\u001b[39;49;00m),\r\n",
      "                                           dtype=np.uint32)\r\n",
      "        \r\n",
      "        \u001b[37m# How many servers should the agent spin up at each time step \u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.action_space = Box(low=np.array([\u001b[34m0\u001b[39;49;00m]),\r\n",
      "                                     high=np.array([\u001b[34m1\u001b[39;49;00m]),\r\n",
      "                                     dtype=np.float32)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mreset\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mprint\u001b[39;49;00m (\u001b[33m\"\u001b[39;49;00m\u001b[33min reset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[37m#self.populate_cloudwatch_metric(self.namespace,1,'reset')\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.num_steps = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.current_min = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.demand_observation = np.array([\u001b[36mself\u001b[39;49;00m.min_servers]*\u001b[36mself\u001b[39;49;00m.history_len)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.alloc_observation = np.array([\u001b[36mself\u001b[39;49;00m.min_servers]*\u001b[36mself\u001b[39;49;00m.history_len)\r\n",
      "        \u001b[37m#self.action_observation = np.array([self.min_servers]*self.history_len)\u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[34mprint\u001b[39;49;00m (\u001b[33m'\u001b[39;49;00m\u001b[33mself.demand_observation \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.demand_observation))\r\n",
      "        \u001b[34mprint\u001b[39;49;00m (\u001b[33m'\u001b[39;49;00m\u001b[33mself.alloc_observation \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.alloc_observation))\r\n",
      "        \u001b[37m#return np.concatenate((self.demand_observation, self.alloc_observation,self.action_observation))\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m np.concatenate((\u001b[36mself\u001b[39;49;00m.demand_observation, \u001b[36mself\u001b[39;49;00m.alloc_observation))\r\n",
      "\r\n",
      "   \r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mstep\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, action):\r\n",
      "        \u001b[34mprint\u001b[39;49;00m (\u001b[33m'\u001b[39;49;00m\u001b[33min step - action recieved from model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(action))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.num_steps+=\u001b[34m1\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.total_num_of_obs+=\u001b[34m1\u001b[39;49;00m\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtotal_num_of_obs={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mself\u001b[39;49;00m.total_num_of_obs))\r\n",
      "\r\n",
      "        raw_action=\u001b[36mfloat\u001b[39;49;00m(action)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.curr_action = raw_action*\u001b[36mself\u001b[39;49;00m.action_factor\r\n",
      "        \u001b[36mself\u001b[39;49;00m.curr_action = np.clip(\u001b[36mself\u001b[39;49;00m.curr_action, \u001b[36mself\u001b[39;49;00m.min_servers, \u001b[36mself\u001b[39;49;00m.max_servers)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mself.curr_action={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mself\u001b[39;49;00m.curr_action))\r\n",
      "        \r\n",
      "               \r\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[36mself\u001b[39;49;00m.gs_inventory_url!=\u001b[33m'\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "          \u001b[37m#get the demand from the matchmaking service\u001b[39;49;00m\r\n",
      "          \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mquering matchmaking service for current demand, curr_demand\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "          \u001b[34mtry\u001b[39;49;00m:\r\n",
      "           gs_url=\u001b[36mself\u001b[39;49;00m.gs_inventory_url\r\n",
      "           req=requests.get(url=gs_url)\r\n",
      "           data=req.json()\r\n",
      "           \u001b[36mself\u001b[39;49;00m.curr_demand = \u001b[36mfloat\u001b[39;49;00m(data[\u001b[33m'\u001b[39;49;00m\u001b[33mPrediction\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mnum_of_gameservers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])            \r\n",
      "            \r\n",
      "          \u001b[34mexcept\u001b[39;49;00m requests.exceptions.RequestException \u001b[34mas\u001b[39;49;00m e:\r\n",
      "           \u001b[34mprint\u001b[39;49;00m(e)\r\n",
      "           \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mif matchmaking did not respond just randomized curr_demand between limit, reward will correct\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "           \u001b[36mself\u001b[39;49;00m.curr_demand = \u001b[36mfloat\u001b[39;49;00m(np.random.randint(\u001b[36mself\u001b[39;49;00m.min_servers,\u001b[36mself\u001b[39;49;00m.max_servers))\r\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[36mself\u001b[39;49;00m.gs_inventory_url==\u001b[33m'\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "          \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlocal matchmaking service for current demand, curr_demand\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "          data=\u001b[36mself\u001b[39;49;00m.get_curr_sine1h()\r\n",
      "          \u001b[36mself\u001b[39;49;00m.curr_demand = \u001b[36mfloat\u001b[39;49;00m(data[\u001b[33m'\u001b[39;49;00m\u001b[33mPrediction\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mnum_of_gameservers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])       \r\n",
      "        \u001b[37m# clip the demand to the allowed range\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.curr_demand = np.clip(\u001b[36mself\u001b[39;49;00m.curr_demand, \u001b[36mself\u001b[39;49;00m.min_servers, \u001b[36mself\u001b[39;49;00m.max_servers)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mself.curr_demand={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mself\u001b[39;49;00m.curr_demand)) \r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.curr_alloc = \u001b[36mself\u001b[39;49;00m.alloc_observation[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mself.curr_alloc={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mself\u001b[39;49;00m.curr_alloc)) \r\n",
      "            \r\n",
      "        \u001b[37m# Assumes it takes history_len time steps to create or delete \u001b[39;49;00m\r\n",
      "        \u001b[37m# the game server from allocation\u001b[39;49;00m\r\n",
      "        \u001b[37m# self.action_observation = self.action_observation[1:]\u001b[39;49;00m\r\n",
      "        \u001b[37m# self.action_observation = np.append(self.action_observation, self.curr_action)\u001b[39;49;00m\r\n",
      "        \u001b[37m# print('self.action_observation={}'.format(self.action_observation))\u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[37m# store the current demand in the history array demand_observation\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.demand_observation = \u001b[36mself\u001b[39;49;00m.demand_observation[\u001b[34m1\u001b[39;49;00m:] \u001b[37m# shift the observation by one to remove one history point\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.demand_observation=np.append(\u001b[36mself\u001b[39;49;00m.demand_observation,\u001b[36mself\u001b[39;49;00m.curr_demand)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mself.demand_observation={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mself\u001b[39;49;00m.demand_observation))\r\n",
      "        \r\n",
      "        \u001b[37m# store the current demand in the history array demand_observation\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.alloc_observation = \u001b[36mself\u001b[39;49;00m.alloc_observation[\u001b[34m1\u001b[39;49;00m:] \r\n",
      "        \u001b[36mself\u001b[39;49;00m.alloc_observation=np.append(\u001b[36mself\u001b[39;49;00m.alloc_observation,\u001b[36mself\u001b[39;49;00m.curr_action)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mself.alloc_observation={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mself\u001b[39;49;00m.alloc_observation))\r\n",
      " \r\n",
      "        \r\n",
      "        \u001b[37m#reward calculation - in case of over provision just 1-ratio. under provision is more severe so 500% more negative reward\u001b[39;49;00m\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcalculate the reward, calculate the ratio between allocation and demand, we use the first allocation in the series of history of five, first_alloc/curr_demand\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mhistory of previous predictions made by the model ={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mself\u001b[39;49;00m.alloc_observation))\r\n",
      "        \r\n",
      "        ratio=\u001b[36mself\u001b[39;49;00m.curr_alloc/\u001b[36mself\u001b[39;49;00m.curr_demand\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mratio={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(ratio))\r\n",
      "        \u001b[34mif\u001b[39;49;00m (ratio>\u001b[34m1\u001b[39;49;00m):\r\n",
      "           \u001b[37m#reward=1-ratio\u001b[39;49;00m\r\n",
      "           reward = -\u001b[34m1\u001b[39;49;00m * (\u001b[36mself\u001b[39;49;00m.curr_alloc - \u001b[36mself\u001b[39;49;00m.curr_demand)\r\n",
      "           \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mover provision - ratio>1 - {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(reward))\r\n",
      "        \u001b[34mif\u001b[39;49;00m (ratio<\u001b[34m1\u001b[39;49;00m):\r\n",
      "           \u001b[37m#reward=-50*ratio\u001b[39;49;00m\r\n",
      "           reward = -\u001b[34m5\u001b[39;49;00m * (\u001b[36mself\u001b[39;49;00m.curr_demand - \u001b[36mself\u001b[39;49;00m.curr_alloc) \r\n",
      "           \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33munder provision - ratio<1 - {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(reward))\r\n",
      "        \u001b[34mif\u001b[39;49;00m (ratio==\u001b[34m1\u001b[39;49;00m):\r\n",
      "           reward=\u001b[34m1\u001b[39;49;00m\r\n",
      "           \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mratio=1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        reward -= (\u001b[36mself\u001b[39;49;00m.curr_demand - \u001b[36mself\u001b[39;49;00m.curr_alloc)*\u001b[36mself\u001b[39;49;00m.over_prov_factor\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mratio={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(ratio))\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mreward={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(reward))\r\n",
      "                \r\n",
      "         \r\n",
      "        \u001b[37m#Instrumnet the supply and demand in cloudwatch\u001b[39;49;00m\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mpopulating cloudwatch - self.curr_demand={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mself\u001b[39;49;00m.curr_demand))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.populate_cloudwatch_metric(\u001b[36mself\u001b[39;49;00m.namespace,\u001b[36mself\u001b[39;49;00m.curr_demand,\u001b[33m'\u001b[39;49;00m\u001b[33mcurr_demand\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mpopulating cloudwatch - self.curr_alloc={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mself\u001b[39;49;00m.curr_action))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.populate_cloudwatch_metric(\u001b[36mself\u001b[39;49;00m.namespace,\u001b[36mself\u001b[39;49;00m.curr_action,\u001b[33m'\u001b[39;49;00m\u001b[33mcurr_alloc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mpopulating cloudwatch - reward={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(reward))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.populate_cloudwatch_metric(\u001b[36mself\u001b[39;49;00m.namespace,reward,\u001b[33m'\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[36mself\u001b[39;49;00m.num_steps >= \u001b[36mself\u001b[39;49;00m.max_num_steps):\r\n",
      "          done = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "          \u001b[34mprint\u001b[39;49;00m (\u001b[33m\"\u001b[39;49;00m\u001b[33mself.num_steps \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.num_steps))\r\n",
      "          \u001b[34mprint\u001b[39;49;00m (\u001b[33m\"\u001b[39;49;00m\u001b[33mself.max_num_steps \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.max_num_steps))\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "          done = \u001b[36mFalse\u001b[39;49;00m\r\n",
      "        \r\n",
      "        \u001b[34mprint\u001b[39;49;00m (\u001b[33m'\u001b[39;49;00m\u001b[33mtime.sleep() for {} before next iteration\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mself\u001b[39;49;00m.learning_freq))\r\n",
      "        time.sleep(\u001b[36mint\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.learning_freq)) \r\n",
      "        \r\n",
      "        extra_info = {}\r\n",
      "        \u001b[37m#the next state includes the demand and allocation history. \u001b[39;49;00m\r\n",
      "        \u001b[37m#next_state=np.concatenate((self.demand_observation,self.alloc_observation,self.action_observation))\u001b[39;49;00m\r\n",
      "        next_state=np.concatenate((\u001b[36mself\u001b[39;49;00m.demand_observation,\u001b[36mself\u001b[39;49;00m.alloc_observation))\r\n",
      "        \u001b[34mprint\u001b[39;49;00m (\u001b[33m'\u001b[39;49;00m\u001b[33mnext_state={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(next_state))\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m next_state, reward, done, extra_info\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mrender\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, mode):\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33min render\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[34mpass\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpopulate_cloudwatch_metric\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,namespace,metric_value,metric_name):\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33min populate_cloudwatch_metric metric_value=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(metric_value)+\u001b[33m\"\u001b[39;49;00m\u001b[33m metric_name=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m+metric_name)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        response = cloudwatch_cli.put_metric_data(\r\n",
      "    \tNamespace=namespace,\r\n",
      "    \tMetricData=[\r\n",
      "           {\r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33mMetricName\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: metric_name,\r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33mUnit\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33mValue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: metric_value,\r\n",
      "           },\r\n",
      "        ]\r\n",
      "        )\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mresponse from cloud watch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(response))\r\n",
      "        \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_curr_sine1h\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        max_servers=\u001b[36mself\u001b[39;49;00m.max_servers*\u001b[34m0.9\u001b[39;49;00m\r\n",
      "        \u001b[34mprint\u001b[39;49;00m (\u001b[33m'\u001b[39;49;00m\u001b[33min get_curr_sine1h\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        cycle_arr=np.linspace(\u001b[34m0.2\u001b[39;49;00m,\u001b[34m3.1\u001b[39;49;00m,\u001b[34m61\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.current_min = (\u001b[36mself\u001b[39;49;00m.current_min + \u001b[34m1\u001b[39;49;00m) % \u001b[34m60\u001b[39;49;00m\r\n",
      "        current_min = \u001b[36mself\u001b[39;49;00m.current_min\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_min={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(current_min))\r\n",
      "        current_point=cycle_arr[\u001b[36mint\u001b[39;49;00m(current_min)]\r\n",
      "        sine=max_servers*np.sin(current_point)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33msine({})={}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(current_point,sine))\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mPrediction\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:{\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_of_gameservers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: sine}}\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/gameserver_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the presets for RL algorithm\n",
    "\n",
    "The presets that configure the RL training jobs are defined in the train_gameserver_ppo.py file which is also uploaded on the /src directory. Using the preset file, you can define agent parameters to select the specific agent algorithm. You can also set the environment parameters, define the schedule and visualization parameters, and define the graph manager. The schedule presets will define the number of heat up steps, periodic evaluation steps, training steps between evaluations.\n",
    "It can be used to define custom hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgym\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mray\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mray.tune\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m run_experiments\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mray.tune.registry\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m register_env\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_rl.ray_launcher\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SageMakerRayLauncher\r\n",
      "\r\n",
      "env_config={}\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMyLauncher\u001b[39;49;00m(SageMakerRayLauncher):\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mregister_env_creator\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgameserver_env\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GameServerEnv\r\n",
      "        register_env(\u001b[33m\"\u001b[39;49;00m\u001b[33mGameServers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34mlambda\u001b[39;49;00m env_config: GameServerEnv(env_config))\r\n",
      "        \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_save_tf_model\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33min _save_tf_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        ckpt_dir = \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/output/data/checkpoint\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "        model_dir = \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# Re-Initialize from the checkpoint so that you will have the latest models up.\u001b[39;49;00m\r\n",
      "        tf.train.init_from_checkpoint(ckpt_dir,\r\n",
      "                                      {\u001b[33m'\u001b[39;49;00m\u001b[33mmain_level/agent/online/network_0/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mmain_level/agent/online/network_0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m})\r\n",
      "        tf.train.init_from_checkpoint(ckpt_dir,\r\n",
      "                                      {\u001b[33m'\u001b[39;49;00m\u001b[33mmain_level/agent/online/network_1/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mmain_level/agent/online/network_1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m})\r\n",
      "\r\n",
      "        \u001b[37m# Create a new session with a new tf graph.\u001b[39;49;00m\r\n",
      "        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=\u001b[36mTrue\u001b[39;49;00m))\r\n",
      "        sess.run(tf.global_variables_initializer())  \u001b[37m# initialize the checkpoint.\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# This is the node that will accept the input.\u001b[39;49;00m\r\n",
      "        input_nodes = tf.get_default_graph().get_tensor_by_name(\u001b[33m'\u001b[39;49;00m\u001b[33mmain_level/agent/main/online/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + \\\r\n",
      "                                                                \u001b[33m'\u001b[39;49;00m\u001b[33mnetwork_0/observation/observation:0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[37m# This is the node that will produce the output.\u001b[39;49;00m\r\n",
      "        output_nodes = tf.get_default_graph().get_operation_by_name(\u001b[33m'\u001b[39;49;00m\u001b[33mmain_level/agent/main/online/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + \\\r\n",
      "                                                                    \u001b[33m'\u001b[39;49;00m\u001b[33mnetwork_1/ppo_head_0/policy_mean/BiasAdd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[37m# Save the model as a servable model.\u001b[39;49;00m\r\n",
      "        tf.saved_model.simple_save(session=sess,\r\n",
      "                                   export_dir=\u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                   inputs={\u001b[33m\"\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: input_nodes},\r\n",
      "                                   outputs={\u001b[33m\"\u001b[39;49;00m\u001b[33mpolicy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: output_nodes.outputs[\u001b[34m0\u001b[39;49;00m]})\r\n",
      "        \u001b[37m# Move to the appropriate folder. \u001b[39;49;00m\r\n",
      "        shutil.move(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, model_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/model/tf-model/00000001/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[37m# SageMaker will pick it up and upload to the right path.\u001b[39;49;00m\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33min _save_tf_model Success\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_experiment_config\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mget_experiment_config\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)       \r\n",
      "        \u001b[34mprint\u001b[39;49;00m(env_config)\r\n",
      "        \u001b[37m# allowing 1600 seconds to the job toto stop and save the model\u001b[39;49;00m\r\n",
      "        time_total_s=\u001b[36mfloat\u001b[39;49;00m(env_config[\u001b[33m\"\u001b[39;49;00m\u001b[33mtime_total_s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])-\u001b[34m4600\u001b[39;49;00m\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtime_total_s=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(time_total_s))\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\r\n",
      "          \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33menv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mGameServers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mrun\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mPPO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "             \u001b[33m\"\u001b[39;49;00m\u001b[33mstop\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33mtime_total_s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: time_total_s\r\n",
      "             },\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33mignore_worker_failures\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mTrue\u001b[39;49;00m,\r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33mgamma\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33mkl_coeff\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m1.0\u001b[39;49;00m,\r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33mnum_sgd_iter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m10\u001b[39;49;00m,\r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33mlr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m0.0001\u001b[39;49;00m,\r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33msgd_minibatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m32\u001b[39;49;00m, \r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m128\u001b[39;49;00m,\r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "\u001b[37m#                 \"free_log_std\": True,\u001b[39;49;00m\r\n",
      "\u001b[37m#                  \"fcnet_hiddens\": [512, 512],\u001b[39;49;00m\r\n",
      "                },\r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33muse_gae\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mTrue\u001b[39;49;00m,\r\n",
      "               \u001b[37m#\"num_workers\": (self.num_cpus-1),\u001b[39;49;00m\r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mself\u001b[39;49;00m.num_gpus,\r\n",
      "               \u001b[37m#\"batch_mode\": \"complete_episodes\",\u001b[39;49;00m\r\n",
      "               \u001b[33m\"\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\r\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33menv_config\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: env_config,\r\n",
      "               \u001b[37m#'observation_filter': 'MeanStdFilter',\u001b[39;49;00m\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(sys.argv)):\r\n",
      "      \u001b[34mif\u001b[39;49;00m i==\u001b[34m0\u001b[39;49;00m:\r\n",
      "         \u001b[34mcontinue\u001b[39;49;00m\r\n",
      "      \u001b[34mif\u001b[39;49;00m i % \u001b[34m2\u001b[39;49;00m > \u001b[34m0\u001b[39;49;00m:\r\n",
      "         env_config[sys.argv[i].split(\u001b[33m'\u001b[39;49;00m\u001b[33m--\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[34m1\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m]]=sys.argv[i+\u001b[34m1\u001b[39;49;00m]\r\n",
      "    MyLauncher().train_main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/train_gameserver_ppo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the RL model using the Python SDK Script mode\n",
    "\n",
    "The RLEstimator is used for training RL jobs. \n",
    "\n",
    "1. The entry_point value indicates the script that invokes the GameServer RL environment.\n",
    "2. source_dir indicates the location of environment code which currently includes train-gameserver-ppo.py and game_server_env.py. \n",
    "3. Specify the choice of RL toolkit and framework. This automatically resolves to the ECR path for the RL Container. \n",
    "4. Define the training parameters such as the instance count, job name, S3 path for output and job name. \n",
    "5. Specify the hyperparameters for the RL agent algorithm. The RLCOACH_PRESET or the RLRAY_PRESET can be used to specify the RL agent algorithm you want to use. \n",
    "6. Define the metrics definitions that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'episode_reward_mean',\n",
       "  'Regex': 'episode_reward_mean: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
       " {'Name': 'episode_reward_max',\n",
       "  'Regex': 'episode_reward_max: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
       " {'Name': 'episode_len_mean',\n",
       "  'Regex': 'episode_len_mean: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
       " {'Name': 'entropy',\n",
       "  'Regex': 'entropy: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
       " {'Name': 'episode_reward_min',\n",
       "  'Regex': 'episode_reward_min: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
       " {'Name': 'vf_loss',\n",
       "  'Regex': 'vf_loss: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
       " {'Name': 'policy_loss',\n",
       "  'Regex': 'policy_loss: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_definitions = [{'Name': 'episode_reward_mean',\n",
    "  'Regex': 'episode_reward_mean: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    " {'Name': 'episode_reward_max',\n",
    "  'Regex': 'episode_reward_max: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    " {'Name': 'episode_len_mean',\n",
    "  'Regex': 'episode_len_mean: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    " {'Name': 'entropy',\n",
    "  'Regex': 'entropy: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    " {'Name': 'episode_reward_min',\n",
    "  'Regex': 'episode_reward_min: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    " {'Name': 'vf_loss',\n",
    "  'Regex': 'vf_loss: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    " {'Name': 'policy_loss',\n",
    "  'Regex': 'policy_loss: ([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?)'},                                            \n",
    "]\n",
    "\n",
    "metric_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job: rl-game-server-autopilot-2019-12-25-06-03-34-742\n",
      "CPU times: user 118 ms, sys: 0 ns, total: 118 ms\n",
      "Wall time: 315 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)\n",
    "    \n",
    "estimator = RLEstimator(\n",
    "                        entry_point=\"train_gameserver_ppo.py\",\n",
    "                        source_dir='src',\n",
    "                        dependencies=[\"common/sagemaker_rl\"],\n",
    "                        toolkit=RLToolkit.RAY,\n",
    "                        toolkit_version='0.6.5',\n",
    "                        framework=RLFramework.TENSORFLOW,\n",
    "                        role=role,\n",
    "                        train_instance_type=instance_type,\n",
    "                        train_instance_count=train_instance_count,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        metric_definitions=metric_definitions,\n",
    "                        train_max_run=job_duration_in_seconds,\n",
    "                        hyperparameters={\n",
    "                           \"cloudwatch_namespace\":cloudwatch_namespace,\n",
    "                          \"gs_inventory_url\":gs_inventory_url,\n",
    "                          \"learning_freq\":learning_freq,\n",
    "                          \"time_total_s\":job_duration_in_seconds,\n",
    "                          \"min_servers\":min_servers,\n",
    "                          \"max_servers\":max_servers,\n",
    "                          \"gamma\":gamma,\n",
    "                          \"action_factor\":action_factor,\n",
    "                          \"over_prov_factor\":over_prov_factor,\n",
    "                          \"save_model\": 1\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "estimator.fit(wait=False)\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.45.0.dev0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store intermediate training output and model checkpoints\n",
    "\n",
    "The output from the training job above is stored in a S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "job_name=estimator._current_job_name\n",
    "print(\"Job name: {}\".format(job_name))\n",
    "\n",
    "s3_url = \"s3://{}/{}\".format(s3_bucket,job_name)\n",
    "\n",
    "output_tar_key = \"{}/output/output.tar.gz\".format(job_name)\n",
    "\n",
    "intermediate_folder_key = \"{}/output/intermediate/\".format(job_name)\n",
    "output_url = \"s3://{}/{}\".format(s3_bucket, output_tar_key)\n",
    "intermediate_url = \"s3://{}/{}\".format(s3_bucket, intermediate_folder_key)\n",
    "\n",
    "print(\"S3 job path: {}\".format(s3_url))\n",
    "print(\"Output.tar.gz location: {}\".format(output_url))\n",
    "print(\"Intermediate folder path: {}\".format(intermediate_url))\n",
    "    \n",
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "print(\"Create local folder {}\".format(tmp_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of RL models\n",
    "We use the latest checkpointed model to run evaluation for the RL Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpointed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpointed data from the previously trained models will be passed on for evaluation / inference in the checkpoint channel. \n",
    "Since TensorFlow stores ckeckpoint file containes absolute paths from when they were generated (see issue), we need to replace the absolute paths to relative paths. This is implemented within evaluate-game-server.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wait_for_s3_object(s3_bucket, output_tar_key, tmp_dir)  \n",
    "\n",
    "if not os.path.isfile(\"{}/output.tar.gz\".format(tmp_dir)):\n",
    "    raise FileNotFoundError(\"File output.tar.gz not found\")\n",
    "os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir))\n",
    "\n",
    "checkpoint_dir = \"{}/checkpoint\".format(tmp_dir)\n",
    "\n",
    "print(\"Checkpoint directory {}\".format(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "checkpoint_path = \"s3://{}/{}/checkpoint/\".format(s3_bucket, job_name)\n",
    "if not os.listdir(checkpoint_dir):\n",
    "     raise FileNotFoundError(\"Checkpoint files not found under the path\")\n",
    "os.system(\"aws s3 cp --recursive {} {}\".format(checkpoint_dir, checkpoint_path))\n",
    "print(\"S3 checkpoint file path: {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evaluation step\n",
    "Use the checkpointed model to run the evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_name: 5obs-local-sine-2019-08-18-21-13-45-314\n",
      "\u001b[31min __init__\u001b[0m\n",
      "\u001b[31menv_config\u001b[0m\n",
      "\u001b[31m{'cloudwatch_namespace': '5obs-local-sine', 'gs_inventory_url': 'https://4bfiebw6ui.execute-api.us-west-2.amazonaws.com/api/currsine1h/', 'learning_freq': '5', 'max_servers': '100', 'min_servers': '10', 'save_model': '1', 'time_total_s': '32400'}\u001b[0m\n",
      "\u001b[31mself.curr_demand=63.138143498979936\u001b[0m\n",
      "\u001b[31mcalculate the reward, calculate the ratio between allocation and demand, curr_alloc/curr_demand\u001b[0m\n",
      "\u001b[31minterm ratio=1.0151651067081289\u001b[0m\n",
      "\u001b[31mover provision - ratio>1 - -0.9574966835151812\u001b[0m\n",
      "\u001b[31mhttps://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[31m2019-08-19 06:24:38,987 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-08-19 06:24:43 Uploading - Uploading generated training model\n",
      "2019-08-19 06:24:43 Completed - Training job completed\n",
      "Billable seconds: 60\n",
      "Evaluation job: 5obs-local-sine-evaluation-2019-08-19-06-21-59-623\n",
      "CPU times: user 1.65 s, sys: 129 ms, total: 1.78 s\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "job_name = \"5obs-local-sine-2019-08-18-21-13-45-314\"\n",
    "print(\"job_name: %s\" % job_name)\n",
    "estimator_eval = RLEstimator(entry_point=\"evaluate_gameserver_ppo.py\",\n",
    "                        source_dir='src',\n",
    "                        dependencies=[\"common/sagemaker_rl\"],\n",
    "                        role=role,\n",
    "                        toolkit=RLToolkit.RAY,\n",
    "                        toolkit_version='0.6.5',\n",
    "                        framework=RLFramework.TENSORFLOW,\n",
    "                        train_instance_type=instance_type,\n",
    "                        train_instance_count=1,\n",
    "                        base_job_name=job_name_prefix + \"-evaluation\",\n",
    "                        hyperparameters={\n",
    "                          \"cloudwatch_namespace\":cloudwatch_namespace,\n",
    "                          \"gs_inventory_url\":gs_inventory_url,\n",
    "                          \"learning_freq\":learning_freq,\n",
    "                          \"time_total_s\":job_duration_in_seconds,\n",
    "                          \"min_servers\":min_servers,\n",
    "                          \"max_servers\":max_servers,\n",
    "                          \"gamma\":gamma,\n",
    "                          \"action_factor\":action_factor,\n",
    "                          \"over_prov_factor\":over_prov_factor,\n",
    "                          \"save_model\": 1\n",
    "                        }     \n",
    "                    )\n",
    "estimator_eval.fit({'model': checkpoint_path})\n",
    "job_name = estimator_eval.latest_training_job.job_name\n",
    "print(\"Evaluation job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same insantance (or type of instance) that we used to train. The endpoint deployment can be accomplished as follows:\n",
    "\n",
    "### Model deployment\n",
    "\n",
    "Now let us deploy the RL policy so that we can get the optimal action, given an environment observation.\n",
    "In case the notebook restarted and lost its previous estimator object, populate the estimator.model_data with the full s3 link to the model.tar.gz. e.g., s3://sagemaker-us-west-2-356566070122/rl-gameserver-autopilot-2019-07-19-19-36-32-926/output/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: s3://sagemaker-us-west-2-356566070122/rl-gs-training-2019-09-23-15-41-40-260/output/model.tar.gz\n",
      "-------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "print (\"model name: %s\" % estimator.model_data)\n",
    "model_data='s3://sagemaker-us-west-2-356566070122/rl-gs-training-2019-09-23-15-41-40-260/output/model.tar.gz'\n",
    "model = Model(model_data=model_data,\n",
    "              role=role)\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Now that the trained model is deployed at an endpoint that is up-and-running, we can use this endpoint for inference. The format of input should match that of observation_space in the defined environment. In this example, the observation space is a 15 dimensional vector formulated from previous and current observations. For the sake of space, this demo doesn't include the non-trivial construction process. Instead, we provide a dummy input below. For more details, please check src/gameserver_env.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_region = 'us-west-2'\n",
    "sagemaker_client = boto3.client('sagemaker-runtime',region_name=sagemaker_region)\n",
    "#populate the correct endpoint_name\n",
    "endpoint_name =\"sagemaker-tensorflow-serving-2019-09-23-20-53-20-237\"\n",
    "content_type = \"application/json\"\n",
    "accept = \"Accept\"\n",
    "last_observations = np.arange(1, 16)\n",
    "\n",
    "response = sagemaker_client.invoke_endpoint(\n",
    "      EndpointName=endpoint_name,\n",
    "      ContentType=content_type,\n",
    "      Accept=accept,\n",
    "      Body=last_observations\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Endpoint\n",
    "\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
