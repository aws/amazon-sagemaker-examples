{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c859c24",
   "metadata": {},
   "source": [
    "# Compiling HuggingFace models for AWS Inferentia with SageMaker Neo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c3fb9",
   "metadata": {},
   "source": [
    "AWS Inferentia is Amazon's first custom silicon designed to accelerate deep learning workloads and is part of a long-term strategy to deliver on this vision. AWS Inferentia is designed to provide high performance inference in the cloud, to drive down the total cost of inference, and to make it easy for developers to integrate machine learning into their business applications. AWS Inferentia chips deliver up 2.3x higher throughput and up to 70% lower cost per inference than comparable current generation GPU-based Amazon EC2 instances, as we will confirm in the example notebook.\n",
    "\n",
    "[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/) is a software development kit (SDK) for running machine learning inference using AWS Inferentia chips. It consists of a compiler, run-time, and profiling tools that enable developers to run high-performance and low latency inference using AWS Inferentia-based Amazon EC2 Inf1 instances. Using Neuron, you can bring your models that have been trained on any popular framework (PyTorch, TensorFlow, MXNet), and run them optimally on Inferentia. There is excellent support for Vision and NLP models especially, and on top of that we have released great features to help you make the most efficient use of the hardware, such as [dynamic batching](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/appnotes/perf/torch-neuron-dataparallel-app-note.html#dynamic-batching-description) or [Data Parallel](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/api-torch-neuron-dataparallel-api.html) inferencing.\n",
    "\n",
    "SageMaker Neo saves you the effort of DIY model compilation, extending familiar SageMaker SDK API's to enable easy compilation for a [wide range](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html#API_OutputConfig_Contents) of platforms. This includes CPU and GPU-based instances, but also Inf1 instances; in this case, SageMaker Neo uses the Neuron SDK to compile your model.\n",
    "\n",
    "In this example notebook, we will deploy 2 HuggingFace NLP models for the task of paraphrase classification on SageMaker endpoints. One will be deployed on a GPU-accelerated instance, with no changes to the model; the other will be compiled and deployed to an Inf1 instance on SageMaker. Finally, we will perform a simple benchmark to compare the performance of both endpoints in terms of latency and throughput. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe9169",
   "metadata": {},
   "source": [
    "## Setting up our environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fa1d81",
   "metadata": {},
   "source": [
    "We first install some required Python packages, including `transformers`.\n",
    "We also create a default sagemaker session, get our sagemaker role and default bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U sagemaker\n",
    "!pip install -U torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import sagemaker\n",
    "import torch\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sess_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c312843",
   "metadata": {},
   "source": [
    "## Getting model from HuggingFace Model Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1c309",
   "metadata": {},
   "source": [
    "We choose one of the most downloaded models from the HuggingFace Model Hub for our experiments - `distilbert-base-uncased`. [DistilBERT](https://huggingface.co/distilbert-base-uncased) is a transformer model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher in a knowledge distillation process. It is important to set the `return_dict` parameter to `False` when instantiating the model. In `transformers` v4.x, this parameter is `True` by default and it enables the return of dict-like python objects containing the model outputs, instead of the standard tuples. Neuron compilation does not support dictionary-based model ouputs, and compilation would fail if we didn't explictly set it to `False`.\n",
    "\n",
    "We also get the tokenizer corresponding to this same model, in order to create a sample input to trace our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee838f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", return_dict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc2ec2",
   "metadata": {},
   "source": [
    "## Tracing model with `torch.jit` and uploading to S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de41996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create directory for model artifacts\n",
    "Path(\"traced_model/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce8f9d",
   "metadata": {},
   "source": [
    "We will create a sample input to `jit.trace` our model with PyTorch; this is a required step to have SageMaker Neo compile our model artifact, which will take a `tar.gz` file containing the traced model.\n",
    "\n",
    "The `.pth` extension when saving our model is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample input for jit model tracing\n",
    "seq_0 = \"This is just sample text for model tracing, the length of the sequence does not matter because we will pad to the max length that Bert accepts.\"\n",
    "seq_1 = seq_0\n",
    "max_length = 512\n",
    "\n",
    "tokenized_sequence_pair = tokenizer.encode_plus(\n",
    "    seq_0, seq_1, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "example = tokenized_sequence_pair[\"input_ids\"], tokenized_sequence_pair[\"attention_mask\"]\n",
    "\n",
    "traced_model = torch.jit.trace(model.eval(), example)\n",
    "traced_model.save(\"traced_model/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a48098",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf traced_model.tar.gz -C traced_model . && mv traced_model.tar.gz traced_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a1e917",
   "metadata": {},
   "source": [
    "We upload the traced model `tar.gz` file to Amazon S3, where our compilation job will download it from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model_url = sagemaker_session.upload_data(\n",
    "    path=\"traced_model/traced_model.tar.gz\",\n",
    "    key_prefix=\"neuron-experiments/bert-seq-classification/traced-model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b92b0c",
   "metadata": {},
   "source": [
    "## Understanding our inference code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a1646",
   "metadata": {},
   "source": [
    "Before we deploy any model, let's check out the code we have written to do inference on a SageMaker endpoint, with a default uncompiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc53c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize code/inference_normal.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a30b2",
   "metadata": {},
   "source": [
    "As usual, we have a `model_fn` - receives the model directory, is responsible for loading and returning the model -, an `input_fn` and `output_fn` - in charge of pre-processing/checking content types of input and output to the endpoint - and a `predict_fn`, which receives the outputs of `model_fn` and `input_fn` (meaning, the loaded model and the deserialized/pre-processed input data) and defines how the model will run inference.\n",
    "\n",
    "In this case, notice that we will load the model directly from the HuggingFace Model Hub for simplicity. `model_fn` will return a tuple containing both the model and its corresponding tokenizer. Both the model and the input data will be sent `.to(device)`, which can be a CPU or GPU, as we can see in line 7 of the file.\n",
    "\n",
    "#### Now, lets see what changes in the inference code when we want to do inference with a model that has been compiled for Inferentia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command outputs what you see in the cell after this one: only the model_fn in the inference_inf1.py file\n",
    "%load -s model_fn code/inference_inf1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s model_fn code/inference_inf1.py\n",
    "def model_fn(model_dir):\n",
    "\n",
    "    dir_contents = os.listdir(model_dir)\n",
    "    model_path = next(filter(lambda item: \"model\" in item, dir_contents), None)\n",
    "\n",
    "    tokenizer_init = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = torch.jit.load(os.path.join(model_dir, model_path))\n",
    "\n",
    "    return (model, tokenizer_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a19508",
   "metadata": {},
   "source": [
    "In this case, within the `model_fn` we first grab the model artifact located in `model_dir` (the compilation step will name the artifact `model_neuron.pt`, but we just get the first file containing `model` in its name for script flexibility). Then, **we load the Neuron compiled model with `torch.jit.load`**. \n",
    "\n",
    "Other than this change to `model_fn`, we only need to add an extra import `import torch_neuron` to the beginning of the script, and get rid of all `.to(device)` calls, since the Neuron runtime will take care of loading our model to the NeuronCores on our Inferentia instance. All other functions are unchanged. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41028d00",
   "metadata": {},
   "source": [
    "## Deploying default model to GPU-backed endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e90e652",
   "metadata": {},
   "source": [
    "Now that we understand how we will do inference, we will first deploy a normal uncompiled model to a GPU-backed g4dn instance. Typically, this is a great instance type in terms of price-performance ratio that still provides GPU-acceleration.\n",
    "\n",
    "Although we will be passing the `traced_model_url` as the `model_data` parameter to the `PyTorchModel` API, as we saw we will be pulling the model directly from the HuggingFace Model Hub directly in the inference script; this won't affect our benchmark in any way, since `model_fn` gets executed before any request even reaches the endpoint. We are using `PyTorchModel` here instead of the [HuggingFace specific](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model) (and optimized) [`HuggingFaceModel`](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model) for the simple reason that the latter is not integrated with SageMaker Neo at the time of writing, and we want to ensure a similar, standard setup for deploying both models. Anyhow, you will definitely benefit from using HuggingFace specific SageMaker API's if you are working with HuggingFace Models, but are not looking for model compilation.\n",
    "\n",
    "Notice that we are passing `inference_normal.py` as our entry point script; also, the packages defined in the requirements file within our `source_dir` will automatically be installed on our endpoint instance. In this case we only need the latest version of the `transformers` library that is good to go on Inferentia instances, v. 4.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0415ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from datetime import datetime\n",
    "\n",
    "prefix = \"neuron-experiments/bert-seq-classification\"\n",
    "flavour = \"normal\"\n",
    "date_string = datetime.now().strftime(\"%Y%m-%d%H-%M%S\")\n",
    "\n",
    "normal_sm_model = PyTorchModel(\n",
    "    model_data=traced_model_url,\n",
    "    predictor_cls=Predictor,\n",
    "    framework_version=\"1.8.1\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    entry_point=\"inference_normal.py\",\n",
    "    source_dir=\"code\",\n",
    "    py_version=\"py3\",\n",
    "    name=f\"{flavour}-distilbert-pt181-{date_string}\",\n",
    "    env={\"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"10\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bcc47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "hardware = \"g4dn\"\n",
    "\n",
    "normal_predictor = normal_sm_model.deploy(\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=f\"distilbert-{flavour}-{hardware}-{date_string}\",\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f179ff5b",
   "metadata": {},
   "source": [
    "A quick test that our endpoint is responding as expected, using the sequences built further up in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = seq_0, seq_1\n",
    "normal_predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870da4fb",
   "metadata": {},
   "source": [
    "## Compiling and deploying model on Inferentia instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40263ec",
   "metadata": {},
   "source": [
    "We now create a new `PyTorchModel` that will use `inference_inf1.py` as its entry point script. PyTorch version 1.5.1 is the latest that supports Neo compilation to Inferentia, as you can see from the warning in the compilation cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"neuron-experiments/bert-seq-classification\"\n",
    "flavour = \"normal\"\n",
    "date_string = datetime.now().strftime(\"%Y%m-%d%H-%M%S\")\n",
    "\n",
    "compiled_sm_model = PyTorchModel(\n",
    "    model_data=traced_model_url,\n",
    "    predictor_cls=Predictor,\n",
    "    framework_version=\"1.5.1\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    entry_point=\"inference_inf1.py\",\n",
    "    source_dir=\"code\",\n",
    "    py_version=\"py3\",\n",
    "    name=f\"{flavour}-distilbert-pt181-{date_string}\",\n",
    "    env={\"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"10\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924439ff",
   "metadata": {},
   "source": [
    "Finally, we are ready to compile the model. Two notes here:\n",
    "* HuggingFace models should be compiled to `dtype` `int64`\n",
    "* the format for `compiler_options` differs from the standard Python `dict` that you can use when compiling for \"normal\" instance types; for inferentia, you must provide a JSON string with CLI arguments, which correspond to the ones supported by the [Neuron Compiler](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-cc/command-line-reference.html) (read more about `compiler_options` [here](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html#API_OutputConfig_Contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb41ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "hardware = \"inf1\"\n",
    "flavour = \"compiled-inf\"\n",
    "compilation_job_name = f\"distilbert-{flavour}-{hardware}-\" + date_string\n",
    "\n",
    "compiled_inf1_model = compiled_sm_model.compile(\n",
    "    target_instance_family=f\"ml_{hardware}\",\n",
    "    input_shape={\"input_ids\": [1, 512], \"attention_mask\": [1, 512]},\n",
    "    job_name=compilation_job_name,\n",
    "    role=role,\n",
    "    framework=\"pytorch\",\n",
    "    framework_version=\"1.5.1\",\n",
    "    output_path=f\"s3://{sess_bucket}/{prefix}/neo-compilations/{flavour}-model\",\n",
    "    compiler_options=json.dumps(\"--dtype int64\"),\n",
    "    #     compiler_options={'dtype': 'int64'},    # For compiling to \"normal\" instance types, cpu or gpu-based\n",
    "    compile_max_run=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30b194",
   "metadata": {},
   "source": [
    "After successful compilation, we deploy our model to an inf1.xlarge instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "date_string = datetime.now().strftime(\"%Y%m-%d%H-%M%S\")\n",
    "\n",
    "compiled_inf1_predictor = compiled_inf1_model.deploy(\n",
    "    instance_type=\"ml.inf1.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=f\"test-neo-{hardware}-{date_string}\",\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64542253",
   "metadata": {},
   "source": [
    "Again, we test if everything is running smoothly in our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e754e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with model endpoint\n",
    "payload = seq_0, seq_1\n",
    "compiled_inf1_predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258d4c5f",
   "metadata": {},
   "source": [
    "## Benchmark and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7818b88",
   "metadata": {},
   "source": [
    "We will now perform a simple benchmark of both endpoints, using Python's `threading` module. In each benchmark, we start 5 threads that will each make 300 requests to the model endpoint. We measure the inference latency for each request, and we also measure the total time to finish the task, so that we can get an estimate of the request throughput/second.\n",
    "\n",
    "**We first benchmark the uncompiled endpoint.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3fa700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "num_preds = 300\n",
    "num_threads = 5\n",
    "\n",
    "times = []\n",
    "\n",
    "\n",
    "def predict():\n",
    "    thread_id = threading.get_ident()\n",
    "    print(f\"Thread {thread_id} started\")\n",
    "\n",
    "    for i in range(num_preds):\n",
    "        tick = time.time()\n",
    "        response = normal_predictor.predict(payload)\n",
    "        tock = time.time()\n",
    "        times.append((thread_id, tock - tick))\n",
    "\n",
    "\n",
    "threads = []\n",
    "[threads.append(threading.Thread(target=predict, daemon=False)) for i in range(num_threads)]\n",
    "[t.start() for t in threads]\n",
    "\n",
    "# Wait for threads, get an estimate of total time\n",
    "start = time.time()\n",
    "[t.join() for t in threads]\n",
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0a9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import hist, title, show, xlim\n",
    "import numpy as np\n",
    "\n",
    "TPS = (num_preds * num_threads) / end\n",
    "\n",
    "t = [duration for thread__id, duration in times]\n",
    "latency_percentiles = np.percentile(t, q=[50, 90, 95, 99])\n",
    "\n",
    "hist(t, bins=100)\n",
    "title(\"Request latency histogram on GPU\")\n",
    "xlim(0, 0.2)\n",
    "show()\n",
    "\n",
    "print(\"==== Default HuggingFace model on GPU benchmark ====\\n\")\n",
    "print(f\"95 % of requests take less than {latency_percentiles[2]*1000} ms\")\n",
    "print(f\"Rough request throughput/second is {TPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d7a3f",
   "metadata": {},
   "source": [
    "![Default Benchmark](images/default_benchmark.jpg)\n",
    "\n",
    "We can see that request latency is pretty concentrated around the 85-90 millisecond range, and throughput is around ~60 TPS.\n",
    "\n",
    "**Now, we benchmark our compiled model running on Inferentia**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a21382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "num_preds = 300\n",
    "num_threads = 5\n",
    "\n",
    "times = []\n",
    "\n",
    "\n",
    "def predict():\n",
    "    thread_id = threading.get_ident()\n",
    "    print(f\"Thread {thread_id} started\")\n",
    "\n",
    "    for i in range(num_preds):\n",
    "        tick = time.time()\n",
    "        response = compiled_inf1_predictor.predict(payload)\n",
    "        tock = time.time()\n",
    "        times.append((thread_id, tock - tick))\n",
    "\n",
    "\n",
    "threads = []\n",
    "[threads.append(threading.Thread(target=predict, daemon=False)) for i in range(num_threads)]\n",
    "[t.start() for t in threads]\n",
    "\n",
    "# Make a rough estimate of total time, wait for threads\n",
    "start = time.time()\n",
    "[t.join() for t in threads]\n",
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ee4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import hist, title, show, savefig, xlim\n",
    "import numpy as np\n",
    "\n",
    "TPS = (num_preds * num_threads) / end\n",
    "\n",
    "t = [duration for thread__id, duration in times]\n",
    "latency_percentiles = np.percentile(t, q=[50, 90, 95, 99])\n",
    "\n",
    "hist(t, bins=100)\n",
    "title(\"Request latency histogram for Inferentia\")\n",
    "xlim(0, 0.2)\n",
    "show()\n",
    "\n",
    "print(\"==== HuggingFace model compiled for Inferentia benchmark ====\\n\")\n",
    "print(f\"95 % of requests take less than {latency_percentiles[2]*1000} ms\")\n",
    "print(f\"Rough request throughput/second is {TPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d683ce",
   "metadata": {},
   "source": [
    "![Compiled Benchmark](images/compiled_inf1_benchmark.jpg)\n",
    "\n",
    "#### In this case, we can see that latency has dropped to a staggering 25-30 millisecond range - **around a 70% latency decrease** - while throughput has increased to 220 TPS - **almost a 400% increase**! ðŸ¤¯ðŸ¤¯ðŸ¤¯\n",
    "\n",
    "#### Best of all, the on-demand price of the Inferentia instance type we have used (ml.inf1.xlarge) for SageMaker Real Time Inference is **around 60% lower than ml.g4dn.xlarge, already the lowest-cost GPU instance option**  (Ireland region at the time of writing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be5de89",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71812e",
   "metadata": {},
   "source": [
    "#### The potential of Inferentia is obvious to anyone who has dipped their toe in to experiment with it; the increase in performance obtained from using it, paired with the steep cost reduction when compared to even the most cost efficient GPU-accelerated instances is extremely compelling.\n",
    "\n",
    "#### Now, with a clear-cut way of compiling your models for Inferentia using the same familiar SageMaker SDK API's you already know (and love?), you can more easily take advantage of these benefits with little development effort or learning curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e83d889",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11682cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_predictor.delete_model()\n",
    "normal_predictor.delete_endpoint()\n",
    "compiled_inf1_predictor.delete_model()\n",
    "compiled_inf1_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
