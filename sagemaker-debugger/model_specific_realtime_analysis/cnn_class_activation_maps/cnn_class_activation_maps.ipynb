{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SageMaker debugger to visualize class activation maps in CNNs\n",
    "\n",
    "This notebook will demonstrate how to use SageMaker debugger to plot class activations maps for image classification models. A class activation map (saliency map) is a heatmap that highlights the regions in the image that lead the model to make a certain prediction. This is especially useful:  \n",
    "\n",
    "1. if the model makes a misclassification and it is not clear why; \n",
    "\n",
    "2. or to determine if the model takes all important features of an object into account \n",
    "\n",
    "In this notebook we will train a [ResNet](https://arxiv.org/abs/1512.03385) model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=news) and we will use SageMaker debugger to plot class activation maps in real-time.\n",
    "\n",
    "The following animation shows the saliency map for a particular traffic sign as training progresses. Red highlights the regions with high activation leading to the prediction, blue indicates low activation that are less relevant for the prediction. \n",
    "\n",
    "In the beginning the model will do a lot of mis-classifications as it focuses on the wrong image regions e.g. the obstacle in the lower left corner. As training progresses the focus shifts to the center of the image, and the model becomes more and more confident in predicting the class 3 (which is the correct class).\n",
    "\n",
    "![](images/example.gif)\n",
    "\n",
    "There exist several methods to generate saliency maps e.g. [CAM](http://cnnlocalization.csail.mit.edu/), [GradCAM](https://arxiv.org/abs/1610.02391). The paper [Full-Gradient Representation for Neural Network Visualization [1]](https://arxiv.org/abs/1905.00780) proposes a new method which produces state of the art results. It requires intermediate features and their biases. With SageMaker debugger we can easily retrieve these tensors.\n",
    "\n",
    "[1] *Full-Gradient Representation for Neural Network Visualization*: Suraj Srinivas and Francois Fleuret, 2019, 1905.00780, arXiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize the smdebug hook\n",
    "\n",
    "To create saliency maps, the gradients of the prediction with respect to the intermediate features need to be computed. To obtain this information, we have to customize the [smdebug hook](https://github.com/awslabs/sagemaker-debugger/blob/master/smdebug/pytorch/hook.py). The custom hook is defined in [entry_point/custom_hook.py](entry_point/custom_hook.py) During the forward pass, we register a backward hook on the outputs. We also need to get gradients of the input image, so we provide an additional function that registers a backward hook on the input tensor. \n",
    "\n",
    "The paper [Full-Gradient Representation for Neural Network Visualization [1]](https://arxiv.org/abs/1905.00780) distinguishes between implicit and explicit biases. Implicit biases include running mean and variance from BatchNorm layers. With SageMaker debugger we only get the explicit biases which equals the beta paramater in the case of BatchNorm layers. We extend the hook to also record running averages and variances for BatchNorm layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import smdebug.pytorch as smd\n",
    "       \n",
    "class CustomHook(smd.Hook):\n",
    "    \n",
    "    #register input image for backward pass, to get image gradients\n",
    "    def image_gradients(self, image):\n",
    "        image.register_hook(self.backward_hook(\"image\"))\n",
    "        \n",
    "    def forward_hook(self, module, inputs, outputs):\n",
    "        module_name = self.module_maps[module]   \n",
    "        self._write_inputs(module_name, inputs)\n",
    "        \n",
    "        #register outputs for backward pass. this is expensive, so we will only do it during EVAL mode\n",
    "        if self.mode == ModeKeys.EVAL:\n",
    "            outputs.register_hook(self.backward_hook(module_name + \"_output\"))\n",
    "            \n",
    "            #record running mean and var of BatchNorm layers\n",
    "            if isinstance(module, torch.nn.BatchNorm2d):\n",
    "                self._write_outputs(module_name + \".running_mean\", module.running_mean)\n",
    "                self._write_outputs(module_name + \".running_var\", module.running_var)\n",
    "            \n",
    "        self._write_outputs(module_name, outputs)\n",
    "        self.last_saved_step = self.step\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace in-place operations\n",
    "Additionally we need to convert inplace operations, as they can potentially overwrite values that are required to compute gradients. In the case of PyTorch pre-trained ResNet model, ReLU activatons are per default executed inplace. The following code sets `inplace=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_inplace(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, torch.nn.ReLU):\n",
    "            setattr(model, child_name, torch.nn.ReLU(inplace=False))\n",
    "        else:\n",
    "            relu_inplace(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset and upload it to Amazon S3\n",
    "\n",
    "Now we download the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=news) and upload it to Amazon S3. The training dataset consists of 43 image classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB-Training_fixed.zip\n",
    "! unzip -q GTSRB-Training_fixed.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip\n",
    "! unzip -q GTSRB_Final_Test_Images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we upload the datasets to the SageMaker default bucket in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "def upload_to_s3(path, directory_name, bucket, counter=-1):\n",
    "    \n",
    "    print(\"Upload files from\" + path + \" to \" + bucket)\n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    for path, subdirs, files in os.walk(path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        print(path)\n",
    "        for file in files[0:counter]:\n",
    "            client.upload_file(os.path.join(path, file), bucket, directory_name+'/'+path.split(\"/\")[-1]+'/'+file)\n",
    "            \n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "upload_to_s3(\"GTSRB/Training\", directory_name=\"train\",  bucket=bucket)\n",
    "\n",
    "#we will compute saliency maps for all images in the test dataset, so we will only upload 4 images \n",
    "upload_to_s3(\"GTSRB/Final_Test\", directory_name=\"test\", bucket=bucket, counter=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the SageMaker training job, we need to install some libraries. We will use `smdebug` library to read, filter and analyze raw tensors that are stored in Amazon S3. We will use `opencv-python` library to plot saliency maps as heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install smdebug\n",
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker training\n",
    "Following code defines the SageMaker estimator. The entry point script [train.py](entry_point/train.py) defines the model training. It downloads a pre-trained ResNet model and performs transfer learning on the German traffic sign dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "LOCATION_IN_BUCKET = 'smdebug-output'\n",
    "s3_bucket_for_tensors = 's3://{BUCKET_NAME}/{LOCATION_IN_BUCKET}'.format(BUCKET_NAME=BUCKET_NAME, LOCATION_IN_BUCKET=LOCATION_IN_BUCKET)\n",
    "\n",
    "pytorch_estimator = PyTorch(entry_point='train.py',\n",
    "                            source_dir='entry_point',\n",
    "                            role=role,\n",
    "                            train_instance_type='ml.p3.2xlarge',\n",
    "                            train_instance_count=1,\n",
    "                            framework_version='1.3.1',\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 10, \n",
    "                                               'batch_size_train': 64,\n",
    "                                               'batch_size_val': 4,\n",
    "                                               'learning_rate': 0.001, \n",
    "                                               'smdebug_dir': s3_bucket_for_tensors})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script `train.py` also creates the custom smdebug hook and overwrites the forward hook as described in the beginning of this notebook. The hook is configured to save every iteration during the validation phase and only the first iteration of the training phase. The hook configuration is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "save_config = smd.SaveConfig(mode_save_configs={\n",
    "    smd.modes.TRAIN: smd.SaveConfigMode(save_steps=[0]),\n",
    "    smd.modes.EVAL: smd.SaveConfigMode(save_interval=1)\n",
    "})\n",
    "\n",
    "hook = CustomHook(args.smdebug_dir, save_config=save_config, include_regex='.*bn|.*bias|.*downsample|.*ResNet_input|.*image|.*fc_output' )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the estimator we can call `fit`, which will start the training job on a `ml.p3.2xlarge` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator.fit(inputs={'train': 's3://{}/train'.format(bucket), 'test': 's3://{}/test'.format(bucket)}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize saliency maps in real-time\n",
    "Once the training job has started, SageMaker debugger will upload the tensors of our model into S3. We can check the status of our training job, by executing `describe_training_job`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = pytorch_estimator.latest_training_job.name\n",
    "print('Training job name: {}'.format(job_name))\n",
    "\n",
    "client = pytorch_estimator.sagemaker_session.sagemaker_client\n",
    "\n",
    "description = client.describe_training_job(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the tensors from S3 once the training job is in status `Training` or `Completed`. In the following code cell we check the job status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if description['TrainingJobStatus'] != 'Completed':\n",
    "    while description['SecondaryStatus'] not in {'Training', 'Completed'}:\n",
    "        description = client.describe_training_job(TrainingJobName=job_name)\n",
    "        primary_status = description['TrainingJobStatus']\n",
    "        secondary_status = description['SecondaryStatus']\n",
    "        print('Current job status: [PrimaryStatus: {}, SecondaryStatus: {}]'.format(primary_status, secondary_status))\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is in status `Training` or `Completed`, we can create the trial: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "trial = create_trial(s3_bucket_for_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the saliency maps. The method described in [Full-Gradient Representation for Neural Network Visualization [1]](https://arxiv.org/abs/1905.00780) requires all intermediate features and their biases. The following cell retrieves the gradients for the outputs of batchnorm and downsampling layers and the corresponding biases. If you use a model other than ResNet you may need to adjust the regular expressions in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases, gradients = [], []\n",
    "\n",
    "for tname in trial.tensor_names(regex='.*gradient.*bn.*output|.*gradient.*downsample.1.*output'):\n",
    "    gradients.append(tname)\n",
    "    \n",
    "for tname in trial.tensor_names(regex='^(?=.*bias)(?:(?!fc).)*$'):\n",
    "    biases.append(tname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the beginning of the notebook, in the case of BatchNorm layers, we need to compute the implicit biases. In the following code cell we retrieve the necessary tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_weights, running_vars, running_means = [], [], []\n",
    "\n",
    "for tname in trial.tensor_names(regex='.*running_mean'):\n",
    "    running_means.append(tname)\n",
    "    \n",
    "for tname in trial.tensor_names(regex='.*running_var'):\n",
    "    running_vars.append(tname)\n",
    "\n",
    "for tname in trial.tensor_names(regex='.*bn.*weight|.*downsample.1.*weight'):\n",
    "    bn_weights.append(tname)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ensure that the tensors in the list are in order, e.g. bias vector and gradients need to be for the same layer. Let's have a look on the tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bias, gradient, weight, running_var, running_mean in zip(biases, gradients, bn_weights, running_vars, running_means):\n",
    "    print(bias, gradient, weight, running_var, running_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a helper function that is used later on to normalize tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor):\n",
    "    tensor = tensor - np.min(tensor)\n",
    "    tensor = tensor / np.max(tensor)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to plot saliency maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(saliency_map, image, predicted_class, propability): \n",
    "    \n",
    "    #clear matplotlib figure\n",
    "    plt.clf()\n",
    "    \n",
    "    #revert normalization\n",
    "    mean = [[[0.485]], [[0.456]], [[0.406]]]\n",
    "    std = [[[0.229]], [[0.224]], [[0.225]]]\n",
    "    image = image * std + mean\n",
    "\n",
    "    #transpose image: color channel in last dimension\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = (image * 255).astype(np.uint8) \n",
    "    \n",
    "    #create heatmap: we multiply it with -1 because we use\n",
    "    #matplotlib to plot output results which inverts the colormap\n",
    "    saliency_map = - saliency_map * 255\n",
    "    saliency_map = saliency_map.astype(np.uint8)\n",
    "    heatmap = cv2.applyColorMap(saliency_map, cv2.COLORMAP_JET)\n",
    "    \n",
    "    #overlay original image with heatmap\n",
    "    output_image = heatmap.astype(np.float32) + image.astype(np.float32)\n",
    "    \n",
    "    #normalize\n",
    "    output_image = output_image / np.max(output_image)\n",
    "    \n",
    "    #plot\n",
    "    fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))    \n",
    "    ax0.imshow(image)\n",
    "    ax1.imshow(output_image)\n",
    "    ax0.set_axis_off()\n",
    "    ax1.set_axis_off()\n",
    "    ax0.set_title(\"Input image\")\n",
    "    ax1.set_title(\"Predicted class \" + predicted_class + \" with propability \" + propability + \"%\")\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to compute implicit biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_implicit_biases(bn_weights, running_vars, running_means, step):\n",
    "    implicit_biases = []\n",
    "    for weight_name, running_var_name, running_mean_name in zip(bn_weights, running_vars, running_means):\n",
    "        weight = trial.tensor(weight_name).value(step_num=step, mode=modes.EVAL)\n",
    "        running_var = trial.tensor(running_var_name).value(step_num=step, mode=modes.EVAL)\n",
    "        running_mean = trial.tensor(running_mean_name).value(step_num=step, mode=modes.EVAL)\n",
    "        implicit_biases.append(- running_mean / np.sqrt(running_var) * weight)\n",
    "    return implicit_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get available steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "steps = 0\n",
    "while steps == 0:\n",
    "    steps = trial.steps()\n",
    "    print('Waiting for tensors to become available...')\n",
    "    time.sleep(3)\n",
    "print('\\nDone')\n",
    "\n",
    "print('Getting tensors...')\n",
    "rendered_steps = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate over the tensors from the validation steps and compute the saliency map for each item in the batch. To compute the saliency map, we perform the following steps:\n",
    "\n",
    "1. compute the implicit bias\n",
    "2. multiply gradients and bias (sum of explicit and implicit bias)\n",
    "3. normalize result \n",
    "4. interpolate tensor to the input size of the original input image\n",
    "5. create heatmap and overlay it with the original input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from smdebug import modes\n",
    "from smdebug.core.modes import ModeKeys\n",
    "from smdebug.exceptions import TensorUnavailableForStep\n",
    "import os\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "loaded_all_steps = False\n",
    "\n",
    "while not loaded_all_steps:\n",
    "    \n",
    "    # get available steps\n",
    "    loaded_all_steps = trial.loaded_all_steps\n",
    "    steps = trial.steps(mode=modes.EVAL)\n",
    "    \n",
    "    # quick way to get diff between two lists\n",
    "    steps_to_render = list(set(steps).symmetric_difference(set(rendered_steps)))\n",
    "\n",
    "    #iterate over available steps\n",
    "    for step in sorted(steps_to_render):\n",
    "        try:\n",
    "\n",
    "            #get original input image\n",
    "            image_batch = trial.tensor(\"ResNet_input_0\").value(step_num=step, mode=modes.EVAL)\n",
    "\n",
    "            #compute implicit biases from batchnorm layers\n",
    "            implicit_biases = compute_implicit_biases(bn_weights, running_vars, running_means, step)\n",
    "            \n",
    "            for item in range(image_batch.shape[0]):\n",
    "\n",
    "                #input image\n",
    "                image = image_batch[item,:,:,:]\n",
    "\n",
    "                #get gradients of input image\n",
    "                image_gradient = trial.tensor(\"gradient/image\").value(step_num=step, mode=modes.EVAL)[item,:]  \n",
    "                image_gradient = np.sum(normalize(np.abs(image_gradient * image)), axis=0)\n",
    "                saliency_map = image_gradient\n",
    "\n",
    "                for gradient_name, bias_name, implicit_bias in zip(gradients, biases, implicit_biases):\n",
    "\n",
    "                    #get gradients and bias vectors for corresponding step\n",
    "                    gradient = trial.tensor(gradient_name).value(step_num=step, mode=modes.EVAL)[item:item+1,:,:,:]\n",
    "                    bias = trial.tensor(bias_name).value(step_num=step, mode=modes.EVAL) \n",
    "                    bias = bias + implicit_bias\n",
    "\n",
    "                    #compute full gradient\n",
    "                    bias = bias.reshape((1,bias.shape[0],1,1))\n",
    "                    bias = np.broadcast_to(bias, gradient.shape)\n",
    "                    bias_gradient = normalize(np.abs(bias * gradient))\n",
    "\n",
    "                    #interpolate to original image size\n",
    "                    for channel in range(bias_gradient.shape[1]):\n",
    "                        interpolated = scipy.ndimage.zoom(bias_gradient[0,channel,:,:], image_size/bias_gradient.shape[2], order=1)\n",
    "                        saliency_map += interpolated \n",
    "\n",
    "\n",
    "                #normalize\n",
    "                saliency_map = normalize(saliency_map) \n",
    "                \n",
    "                #predicted class and propability\n",
    "                predicted_class = trial.tensor(\"fc_output_0\").value(step_num=step, mode=modes.EVAL)[item,:]\n",
    "                print(\"Predicted class:\", np.argmax(predicted_class))\n",
    "                scores = np.exp(np.asarray(predicted_class))\n",
    "                scores = scores / scores.sum(0)\n",
    "                \n",
    "                #plot image and heatmap\n",
    "                plot(saliency_map, image, str(np.argmax(predicted_class)), str(int(np.max(scores) * 100)) )\n",
    "                \n",
    "        except TensorUnavailableForStep:\n",
    "            print(\"Tensor unavailable for step {}\".format(step))\n",
    "            \n",
    "    rendered_steps.extend(steps_to_render)\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "print('\\nDone')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
