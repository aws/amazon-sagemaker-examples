{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27811923-bb05-4bae-a8dc-eaf6887a72ab",
   "metadata": {},
   "source": [
    "\n",
    "# Triton on SageMaker - this notebook shows how you can take a RoBERTA model and create a traced model and leverage the Pytorch back end for Triton\n",
    "\n",
    "Amazon SageMaker is a fully managed service for data science and machine learning workflows. It helps data scientists and developers to prepare, build, train, and deploy high-quality ML models quickly by bringing together a broad set of capabilities purpose-built for ML.\n",
    "\n",
    "Now, NVIDIA Triton Inference Server can be used to serve models for inference in Amazon SageMaker. Thanks to the new NVIDIA Triton container image, you can easily serve ML models and benefit from the performance optimizations, dynamic batching, and multi-framework support provided by NVIDIA Triton. Triton helps maximize the utilization of GPU and CPU, further lowering the cost of inference.\n",
    "\n",
    "This notebook was tested on Studio with ml.g4dn.xlarge which comes with 1 GPU and with ml.m5.large which is a CPU based machine only Contents\n",
    "\n",
    "Introduction to NVIDIA Triton Server\n",
    "Set up the environment\n",
    "Basic: RoBERTA Model\n",
    "* PyTorch: JIT Trace the model and create a Scripted model\n",
    "* PyTorch: Testing the JIT Traced model \n",
    "* PyTorch: Packaging model files and uploading to s3\n",
    "* PyTorch: Create SageMaker Endpoint\n",
    "* PyTorch: Run inference\n",
    "* PyTorch: Leverage the Predictions to view the results for Object detection\n",
    "* PyTorch: Terminate endpoint and clean up artifacts\n",
    "\n",
    "\n",
    "### Introduction to NVIDIA Triton Server\n",
    "\n",
    "NVIDIA Triton Inference Server was developed specifically to enable scalable, cost-effective, and easy deployment of models in production. NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance.\n",
    "\n",
    "Some key features of Triton are:\n",
    "\n",
    "* Support for Multiple frameworks: Triton can be used to deploy models from all major frameworks. Triton supports TensorFlow GraphDef, TensorFlow SavedModel, ONNX, PyTorch TorchScript, TensorRT, RAPIDS FIL for tree based models, and OpenVINO model formats.\n",
    "* Model pipelines: Triton model ensemble represents a pipeline of one or more models or pre/post-processing logic and the connection of input and output tensors between them. A single inference request to an ensemble will trigger the execution of the entire pipeline.\n",
    "* Concurrent model execution: Multiple models (or multiple instances of the same model) can run simultaneously on the same GPU or on multiple GPUs for different model management needs.\n",
    "* Dynamic batching: For models that support batching, Triton has multiple built-in scheduling and batching algorithms that combine individual inference requests together to improve inference throughput. These scheduling and batching decisions are transparent to the client requesting inference.\n",
    "* Diverse CPUs and GPUs: The models can be executed on CPUs or GPUs for maximum flexibility and to support heterogeneous computing requirements.\n",
    "\n",
    "Note: This initial release of NVIDIA Triton on SageMaker will only support a single model. Future releases will have multi-model support. A minimal config.pbtxt configuration file is required in the model artifacts. This release doesn't support inferring the model config automatically. Set up the environment\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image.\n",
    "\n",
    "The purpose of this file is to show the ability to take a pytorch computer vision model and create a scripted model which can then be leveraged by Triton using the pytorch back end.\n",
    "\n",
    "The other option is to build using a python back end but in that we loose some performance gains by compilation to native format\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2064a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "627f8f60",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers[torch] in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (4.26.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers[torch]) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers[torch]) (2022.10.31)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers[torch]) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers[torch]) (5.4.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers[torch]) (4.11.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers[torch]) (3.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers[torch]) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers[torch]) (0.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers[torch]) (1.21.6)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.7 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[torch]) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from torch!=1.12.0,>=1.7->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.7->transformers[torch]) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.7->transformers[torch]) (65.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->transformers[torch]) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers[torch]) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers[torch]) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae037f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting nvidia-pyindex\n",
      "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-pyindex\n",
      "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8413 sha256=03d4e5f8b678c8e0714d13efba42d37b3c32e94b7bdefdbcf6d41ea087df0add\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/e0/c2/fb/5cf4e1cfaf28007238362cb746fb38fc2dd76348331a748d54\n",
      "Successfully built nvidia-pyindex\n",
      "Installing collected packages: nvidia-pyindex\n",
      "Successfully installed nvidia-pyindex-1.0.9\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://pypi.ngc.nvidia.com\n",
      "Collecting tritonclient[http]\n",
      "  Downloading tritonclient-2.25.0-py3-none-manylinux1_x86_64.whl (11.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m341.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-rapidjson>=0.9.1\n",
      "  Downloading python_rapidjson-1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m386.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tritonclient[http]) (1.21.2)\n",
      "Collecting geventhttpclient>=1.4.4\n",
      "  Downloading geventhttpclient-2.0.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 KB\u001b[0m \u001b[31m289.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp>=3.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tritonclient[http]) (3.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.7.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (2.0.7)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (4.0.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (2021.10.8)\n",
      "Requirement already satisfied: gevent>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (21.8.0)\n",
      "Collecting brotli\n",
      "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 KB\u001b[0m \u001b[31m353.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp>=3.8.1->tritonclient[http]) (4.0.0)\n",
      "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (59.2.0)\n",
      "Requirement already satisfied: zope.event in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (4.5.0)\n",
      "Requirement already satisfied: zope.interface in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (5.4.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.1->tritonclient[http]) (3.1)\n",
      "Installing collected packages: brotli, python-rapidjson, tritonclient, geventhttpclient\n",
      "Successfully installed brotli-1.0.9 geventhttpclient-2.0.2 python-rapidjson-1.8 tritonclient-2.25.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.0.1 requires botocore<1.22.9,>=1.22.8, but you have botocore 1.27.73 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]\n",
    "\n",
    "!pip install -qU pip awscli boto3 sagemaker transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa9ef5",
   "metadata": {},
   "source": [
    "### Start RoBERTA Base for Triton\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2bd0a228-31e1-42fb-b553-8bf3bde5c69c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p triton-serve-pt/roberta-traced\n",
    "!mkdir -p triton-serve-pt/roberta-traced/1\n",
    "\n",
    "\n",
    "!cd triton-serve-pt/roberta-traced/1 && rm -rf \".ipynb_checkpoints\"\n",
    "!cd triton-serve-pt/roberta-traced && rm -rf \".ipynb_checkpoints\"\n",
    "!cd triton-serve-pt && rm -rf \".ipynb_checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17611bb9-d1a0-4e96-8da8-a95b30de5ad6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "total 487364\n",
      "drwxrwxr-x 2 ec2-user ec2-user      4096 Feb 21 16:22 .\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 499050915 Feb 21 16:41 model.pt\n",
      "drwxrwxr-x 3 ec2-user ec2-user      4096 Feb 21 18:03 ..\n"
     ]
    }
   ],
   "source": [
    "!ls -alrt triton-serve-pt/roberta-traced/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee6cd9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing triton-serve-pt/roberta-traced/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile triton-serve-pt/roberta-traced/config.pbtxt\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [512, 768]\n",
    "  },\n",
    "  {\n",
    "    name: \"1634__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: 32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4a981",
   "metadata": {},
   "source": [
    "### Run for Triton server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec079557",
   "metadata": {},
   "source": [
    "**Note**: Amazon SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`. Below is the sample model directory structure\n",
    "\n",
    "```\n",
    "roberta-large\n",
    "├── 1\n",
    "│   └── model.pt\n",
    "└── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a576b1e",
   "metadata": {},
   "source": [
    "**Have to use the same Tokenizer to generate the input to test as BERT uncased**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d635d",
   "metadata": {},
   "source": [
    "### Create the RoBERTA Model in Torch Script mode -- .pt model\n",
    "use the ore trained and use torchscript flag here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736daf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPTJModel\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a3446e",
   "metadata": {},
   "source": [
    "### Run a simple test for RoBERTA base \n",
    "\n",
    "    * We run multiple tests\n",
    "        * First we token ize and then de tokenize to make sure the vaues match\n",
    "        * Then we use the model and run predictions to get values\n",
    "        * Then we run on the traced Model and run predictions to get values \n",
    "        * Check to make sure they match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692ec25-a715-4363-a0fc-94ba87f0fb50",
   "metadata": {},
   "source": [
    "### Prepare some dummy inputs for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e5a3980-ea0e-4331-aa24-f3a5e5689ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT:Tokenized:Text=['[', 'CL', 'S', ']', 'ĠWho', 'Ġwas', 'ĠJim', 'ĠH', 'enson', 'Ġ?', 'Ġ[', 'SE', 'P', ']', 'ĠJim', 'ĠH', 'enson', 'Ġwas', 'Ġa', 'Ġpupp', 'ete', 'er', 'Ġ[', 'SE', 'P', ']']:::\n",
      "BERT:indexed_tokens:=[10975, 7454, 104, 742, 3394, 21, 2488, 289, 3, 17487, 646, 3388, 510, 742, 2488, 289, 13919, 21, 10, 32986, 9306, 254, 646, 3388, 510, 742]::\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing input text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(f\"BERT:Tokenized:Text={tokenized_text}:::\")\n",
    "\n",
    "# Masking one of the input tokens\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = \"[MASK]\"\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(f\"BERT:indexed_tokens:={indexed_tokens}::\")\n",
    "\n",
    "# -- segments id's\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Creating a dummy input\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f5c6d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_inputs[\"input_ids\"], dummy_inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49e33dd8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "Saved RobertaModel(\n",
      "  original_name=RobertaModel\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    original_name=RobertaEmbeddings\n",
      "    (word_embeddings): Embedding(original_name=Embedding)\n",
      "    (position_embeddings): Embedding(original_name=Embedding)\n",
      "    (token_type_embeddings): Embedding(original_name=Embedding)\n",
      "    (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "    (dropout): Dropout(original_name=Dropout)\n",
      "  )\n",
      "  (encoder): RobertaEncoder(\n",
      "    original_name=RobertaEncoder\n",
      "    (layer): ModuleList(\n",
      "      original_name=ModuleList\n",
      "      (0): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "      (1): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "      (2): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "      (3): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "      (4): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "      (5): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "      (6): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "      (7): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "      (8): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "      (9): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "      (10): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "      (11): RobertaLayer(\n",
      "        original_name=RobertaLayer\n",
      "        (attention): RobertaAttention(\n",
      "          original_name=RobertaAttention\n",
      "          (self): RobertaSelfAttention(\n",
      "            original_name=RobertaSelfAttention\n",
      "            (query): Linear(original_name=Linear)\n",
      "            (key): Linear(original_name=Linear)\n",
      "            (value): Linear(original_name=Linear)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            original_name=RobertaSelfOutput\n",
      "            (dense): Linear(original_name=Linear)\n",
      "            (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "            (dropout): Dropout(original_name=Dropout)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          original_name=RobertaIntermediate\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (intermediate_act_fn): GELUActivation(original_name=GELUActivation)\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          original_name=RobertaOutput\n",
      "          (dense): Linear(original_name=Linear)\n",
      "          (LayerNorm): LayerNorm(original_name=LayerNorm)\n",
      "          (dropout): Dropout(original_name=Dropout)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): RobertaPooler(\n",
      "    original_name=RobertaPooler\n",
      "    (dense): Linear(original_name=Linear)\n",
      "    (activation): Tanh(original_name=Tanh)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Roberta -\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# -- IF you use from bert it comes without HEAD\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")  # roberta-large\n",
    "model = AutoModel.from_pretrained(\"roberta-base\", torchscript=True)  # roberta-large\n",
    "model = model.eval()\n",
    "\n",
    "\n",
    "bs = 1\n",
    "seq_len = 512\n",
    "dummy_inputs = [\n",
    "    torch.randint(1000, (bs, seq_len)).to(\"cpu\"),  # to(device),\n",
    "    torch.zeros(bs, seq_len, dtype=torch.int).to(\"cpu\"),  # to(device),\n",
    "]\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "dummy_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=seq_len,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "print(dummy_inputs.keys())\n",
    "\n",
    "\n",
    "# Creating the trace\n",
    "# traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n",
    "traced_model = torch.jit.trace(\n",
    "    model, [dummy_inputs[\"input_ids\"], dummy_inputs[\"attention_mask\"]]\n",
    ")\n",
    "\n",
    "model = model.eval()\n",
    "# model.to(device)\n",
    "torch.jit.save(traced_model, \"./triton-serve-pt/roberta-traced/1/model.pt\")\n",
    "\n",
    "print(\"Saved {}\".format(traced_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bd95803",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85246e13",
   "metadata": {},
   "source": [
    "#### Test encoders various methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0824610a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 565, 3961, 261, 96, 23861, 30472, 1639, 10, 3613, 8, 3543, 4047, 8663, 11162, 2472, 29854, 13, 258, 39076, 8, 37658, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d85b28f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2345: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "encoded_tokens = tokenizer.encode_plus(\n",
    "    \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\",\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=512,\n",
    "    pad_to_max_length=True,  # Pad & truncate all sentences\n",
    ")\n",
    "# encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b91d085e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101, 13012,  2669, 28937,  8241,  3640,  1037,  6112,  1998,  3341,\n",
       "           1999,  7512,  2368,  6129,  5576, 23569, 27605,  5422,  2005,  2119,\n",
       "          17368,  2015,  1998, 14246,  2271,  1012,   102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1]])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoded_input[\"input_ids\"], encoded_input[\"attention_mask\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d075e-5585-45f0-ac22-1571e9373820",
   "metadata": {},
   "source": [
    "### Test the HuggingFace and then the scripted model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80984984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\",\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=512,  # -- this model has max length set to 100 -- not to 512,\n",
    "    pad_to_max_length=True,  # Pad & truncate all sentences\n",
    ")\n",
    "# unscripted_output = model.generate( # --\n",
    "unscripted_output = model(  # -- both work the same way\n",
    "    **encoded_input,\n",
    "    # inputs=encoded_input['attention_mask']],\n",
    "    return_dict=True,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    # do_sample=True,\n",
    "    # temperature=0.9,\n",
    "    # max_length=128,\n",
    ")  # -- BaseModelOutputWithPoolingAndCrossAttentions\n",
    "\n",
    "# tokenizer.decode(unscripted_output[0])\n",
    "unscripted_output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226688a9-61de-4e27-aa9d-e6e4fa0a369c",
   "metadata": {},
   "source": [
    "#### Now test the Scripted model -- Scripted model gives us tensors back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ccbc136-88d7-48c3-a191-495654f557be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\",\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=512,  # -- this model has max length set to 100 -- not to 512,\n",
    "    pad_to_max_length=True,  # Pad & truncate all sentences\n",
    ")\n",
    "# Traced Model expects ONLY the INPUT ID's\n",
    "unscripted_traced_output = traced_model(  # -- both work the same way\n",
    "    encoded_input[\"input_ids\"], encoded_input[\"attention_mask\"]\n",
    ")\n",
    "\n",
    "# tokenizer.decode(unscripted_output[0])\n",
    "print(unscripted_traced_output[0].shape)\n",
    "print(unscripted_traced_output[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dedcb78c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0381,  0.1347, -0.0798,  ..., -0.0984, -0.0382, -0.0533],\n",
       "         [-0.0080, -0.0298,  0.0424,  ...,  0.1234,  0.0331,  0.1715],\n",
       "         [-0.1365,  0.1032,  0.0484,  ..., -0.0449, -0.1060, -0.0534],\n",
       "         ...,\n",
       "         [ 0.0220,  0.1821, -0.0217,  ..., -0.1113, -0.0644, -0.0289],\n",
       "         [ 0.0220,  0.1821, -0.0217,  ..., -0.1113, -0.0644, -0.0289],\n",
       "         [ 0.0220,  0.1821, -0.0217,  ..., -0.1113, -0.0644, -0.0289]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscripted_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b878a8fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20568/919986660.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munscripted_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3437\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3438\u001b[0m             )\n\u001b[0;32m-> 3439\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3440\u001b[0m         ]\n\u001b[1;32m   3441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3437\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3438\u001b[0m             )\n\u001b[0;32m-> 3439\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3440\u001b[0m         ]\n\u001b[1;32m   3441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3473\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3474\u001b[0m             \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3475\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3476\u001b[0m         )\n\u001b[1;32m   3477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "tokenizer.batch_decode(unscripted_output[1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34790ced",
   "metadata": {},
   "source": [
    "### Upload the Model.tar after it has been created correctly by the above scripted and the config.pbtxt files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58abbcb6-98d5-4ad6-9a5c-84dcdf9b63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_file_name = \"roberta-traced-v1.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f924882-5e88-430e-8631-be48b7588062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "roberta-traced/\n",
      "roberta-traced/config.pbtxt\n",
      "roberta-traced/1/\n",
      "roberta-traced/1/model.pt\n"
     ]
    }
   ],
   "source": [
    "!cd triton-serve-pt && tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" --exclude \"*.tar\" --exclude \"*.ipynb_checkpoints\"  -zcvf {tar_file_name} roberta-traced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feed1b55",
   "metadata": {},
   "source": [
    "**Upload the model.tar.gz to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e9ba035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86f0d03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-425576326687/mme-roberta-benchmark/roberta-large/roberta-traced-v1.tar.gz\n",
      "s3://sagemaker-us-east-1-425576326687/mme-roberta-benchmark/roberta-large/\n"
     ]
    }
   ],
   "source": [
    "s3_model_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=f\"./triton-serve-pt/{tar_file_name}\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-425576326687/mme-roberta-benchmark/roberta-large\",\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "s3_mme_model_path = (\n",
    "    \"s3://sagemaker-us-east-1-425576326687/mme-roberta-benchmark/roberta-large/\"\n",
    ")\n",
    "print(s3_model_path_triton)\n",
    "print(s3_mme_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2b009",
   "metadata": {},
   "source": [
    "#### Start Single Model Triton for starting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741da45c",
   "metadata": {},
   "source": [
    "**Triton Image download and sagemaker variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d863f79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.10-py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.10-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")\n",
    "print(triton_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82661f3",
   "metadata": {},
   "source": [
    "**Model creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f05affc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-base--2023-02-21-18-14-38-943\n",
      "{'ModelArn': 'arn:aws:sagemaker:us-east-1:425576326687:model/roberta-base--2023-02-21-18-14-38-943', 'ResponseMetadata': {'RequestId': 'f4c48a74-64b0-4dfa-8a19-fea9d8d27007', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f4c48a74-64b0-4dfa-8a19-fea9d8d27007', 'content-type': 'application/x-amz-json-1.1', 'content-length': '99', 'date': 'Tue, 21 Feb 2023 18:14:39 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = name_from_base(f\"roberta-base-\")\n",
    "print(endpoint_name)\n",
    "\n",
    "container_p5 = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": s3_mme_model_path,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "    \"Environment\": {\n",
    "        #'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        #'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        #'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bert-uc',\n",
    "        # \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\": \"16777216000\",  # \"16777216000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"104857600\",\n",
    "    },\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ba56d7",
   "metadata": {},
   "source": [
    "**Endpoint config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8e83ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:425576326687:endpoint-config/roberta-base--2023-02-21-18-14-38-943\n"
     ]
    }
   ],
   "source": [
    "# Sampling percentage. Choose an integer value between 0 and 100\n",
    "initial_sampling_percentage = 10\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.8xlarge\",  # \"ml.g4dn.xlarge\", \"ml.g4dn.4xlarge\"\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe59ec7",
   "metadata": {},
   "source": [
    "**Endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0d85217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:425576326687:endpoint/roberta-base--2023-02-21-18-14-38-943\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "81f5e1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SINGLE:Model:endpoint:Triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:425576326687:endpoint/roberta-base--2023-02-21-18-14-38-943\n",
      "Single:model:triton:Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"SINGLE:Model:endpoint:Triton:Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Single:model:triton:Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Single:model:triton:Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb90ea",
   "metadata": {},
   "source": [
    "**Now Invoke The endpoint**\n",
    "<li>First option is JSON</li>\n",
    "<li>Second is native binary headers</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b8b44b4-8c91-4840-8f25-367937aba784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from tritonclient.utils import np_to_triton_dtype\n",
    "\n",
    "\n",
    "def tokenize_text(text, enc, max_length=512):\n",
    "    # enc = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    print(f\"Tokenize:text:why??::max_length={max_length}::Tokenizer={enc}\")\n",
    "    encoded_text = enc(text, padding=\"max_length\", max_length=max_length)\n",
    "    return encoded_text[\"input_ids\"], encoded_text[\"attention_mask\"]\n",
    "\n",
    "\n",
    "# Inference hyperparameters\n",
    "def prepare_tensor(name, input_d):\n",
    "    tensor = httpclient.InferInput(\n",
    "        name, input_d.shape, np_to_triton_dtype(input_d.dtype)\n",
    "    )\n",
    "    tensor.set_data_from_numpy(input_d)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# explanation\n",
    "def prepare_roberta_2_inputs(input0, attention_0):\n",
    "    input0_data = np.array(\n",
    "        input0, dtype=np.int32\n",
    "    )  # - convert to Numpy from PyTorch tensors\n",
    "    input_attention_data = np.array(attention_0, dtype=np.int32)\n",
    "\n",
    "    inputs = [  # - match the config.pbtxt\n",
    "        prepare_tensor(\"INPUT__0\", input0_data),\n",
    "        prepare_tensor(\"INPUT__1\", input_attention_data),\n",
    "    ]\n",
    "\n",
    "    outputs = []\n",
    "    outputs.append(httpclient.InferRequestedOutput(\"OUTPUT__0\", binary_data=True))\n",
    "    outputs.append(httpclient.InferRequestedOutput(\"1634__1\", binary_data=True))\n",
    "    (\n",
    "        request_body,\n",
    "        header_length,\n",
    "    ) = httpclient.InferenceServerClient.generate_request_body(inputs, outputs=outputs)\n",
    "    return request_body, header_length\n",
    "\n",
    "\n",
    "def get_decoded_text(tensors_tokens, enc):\n",
    "    return_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    return return_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f4bb8-f52f-4ee6-903c-c9acc1005765",
   "metadata": {},
   "source": [
    "**Run the JSON invocation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "32d674e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leverage the Tokenizer=BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})::max_seq_length=512:: create above when creating the model \n",
      "Tokenize:text:why??::max_length=512::Tokenizer=RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n",
      "dict_keys(['model_name', 'model_version', 'outputs'])\n",
      "CPU times: user 182 ms, sys: 21.1 ms, total: 203 ms\n",
      "Wall time: 7.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "max_seq_length = 512\n",
    "text_triton = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\"\n",
    "print(\n",
    "    f\"Leverage the Tokenizer={enc}::max_seq_length={max_seq_length}:: create above when creating the model \"\n",
    ")\n",
    "\n",
    "input_ids, attention_mask = tokenize_text(\n",
    "    text_triton, tokenizer, max_length=max_seq_length\n",
    ")\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"shape\": [1, max_seq_length],\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"data\": input_ids,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"INPUT__1\",\n",
    "            \"shape\": [1, max_seq_length],\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"data\": attention_mask,\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"roberta-traced-v1.tar.gz\",\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed92b242",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.038062676787376404,\n",
       " 0.13473781943321228,\n",
       " -0.0798252746462822,\n",
       " -0.07597241550683975,\n",
       " 0.07719936966896057,\n",
       " -0.05807043984532356,\n",
       " -0.060758646577596664,\n",
       " -0.008639145642518997,\n",
       " 0.050245169550180435,\n",
       " -0.06938910484313965,\n",
       " -0.035817358642816544,\n",
       " -0.012312027625739574,\n",
       " 0.027301158756017685,\n",
       " -0.029667401686310768,\n",
       " 0.04819143936038017,\n",
       " -0.006481475196778774,\n",
       " -0.07737714797258377,\n",
       " -0.02181418053805828,\n",
       " -0.017108870670199394,\n",
       " -0.05121561512351036,\n",
       " -0.09797798097133636,\n",
       " 0.05809696018695831,\n",
       " -0.036770690232515335,\n",
       " 0.10161531716585159,\n",
       " -0.021414315328001976,\n",
       " 0.08224329352378845,\n",
       " 0.13604256510734558,\n",
       " 0.1126348003745079,\n",
       " -0.009445725940167904,\n",
       " 0.014347495511174202,\n",
       " 0.004373329691588879,\n",
       " -0.06697972118854523,\n",
       " 0.06279001384973526,\n",
       " -0.004792243242263794,\n",
       " 0.030412670224905014,\n",
       " 0.0675869733095169,\n",
       " -0.012920498847961426,\n",
       " -0.01566997542977333,\n",
       " -0.02670985646545887,\n",
       " -0.015797285363078117,\n",
       " 0.018117519095540047,\n",
       " 0.2294064313173294,\n",
       " 0.0520874485373497,\n",
       " -0.025243427604436874,\n",
       " 0.06764113157987595,\n",
       " 0.030497683212161064,\n",
       " 0.035585105419158936,\n",
       " 0.0142144113779068,\n",
       " -0.05949646979570389,\n",
       " -0.01812520995736122,\n",
       " -0.04705926030874252,\n",
       " 0.09608938544988632,\n",
       " -0.025992896407842636,\n",
       " 0.0029603729490190744,\n",
       " -0.1575116366147995,\n",
       " 0.026906797662377357,\n",
       " 0.03210260346531868,\n",
       " 0.017748137935996056,\n",
       " 0.0744231715798378,\n",
       " -0.1346973031759262,\n",
       " -0.09343066811561584,\n",
       " -0.1713627129793167,\n",
       " -0.07058940827846527,\n",
       " -0.1240488588809967,\n",
       " 0.07055283337831497,\n",
       " -0.05698372796177864,\n",
       " -0.03177075833082199,\n",
       " 0.05233728140592575,\n",
       " 0.0055142296478152275,\n",
       " 0.036517806351184845,\n",
       " 0.03603959456086159,\n",
       " -0.07015316188335419,\n",
       " 0.0014290118124336004,\n",
       " -0.019718587398529053,\n",
       " 0.017913829535245895,\n",
       " -0.00902856420725584,\n",
       " -0.007364591117948294,\n",
       " 0.5346779227256775,\n",
       " -0.020812328904867172,\n",
       " 0.06740715354681015,\n",
       " 0.015824589878320694,\n",
       " -0.06839541345834732,\n",
       " 0.527599036693573,\n",
       " -0.016190147027373314,\n",
       " 0.0063167233020067215,\n",
       " -0.0785910040140152,\n",
       " 0.11792892962694168,\n",
       " 0.028411315754055977,\n",
       " 0.039690591394901276,\n",
       " 0.019681276753544807,\n",
       " 0.023761525750160217,\n",
       " 0.056554682552814484,\n",
       " -0.043755706399679184,\n",
       " -0.01194735523313284,\n",
       " 0.03915725275874138,\n",
       " -0.031595658510923386,\n",
       " -0.0378192663192749,\n",
       " 0.01029955130070448,\n",
       " -0.05309472605586052,\n",
       " -0.05366159975528717,\n",
       " -0.05122452974319458,\n",
       " -0.06552679091691971,\n",
       " 0.0888291522860527,\n",
       " 0.05598081275820732,\n",
       " 0.018703924492001534,\n",
       " 0.03671707585453987,\n",
       " 0.05562654882669449,\n",
       " -0.02498466707766056,\n",
       " -0.009063457138836384,\n",
       " -0.049256328493356705,\n",
       " -0.03996885195374489,\n",
       " 0.01570838876068592,\n",
       " 0.07439684867858887,\n",
       " -0.005835651885718107,\n",
       " 0.021658847108483315,\n",
       " -0.04843791201710701,\n",
       " -0.02300291880965233,\n",
       " 0.050568219274282455,\n",
       " 0.0013220531400293112,\n",
       " 0.04533703625202179,\n",
       " -0.034725386649370193,\n",
       " 0.07390841841697693,\n",
       " 0.04484959691762924,\n",
       " -0.11058564484119415,\n",
       " -0.09526387602090836,\n",
       " -0.05403417721390724,\n",
       " -0.07485190778970718,\n",
       " -0.06578676402568817,\n",
       " -0.05534779280424118,\n",
       " -0.0005576154217123985,\n",
       " -0.02944624051451683,\n",
       " -0.1839808225631714,\n",
       " 0.005189490970224142,\n",
       " 0.09667443484067917,\n",
       " 0.023311099037528038,\n",
       " 0.0003720805107150227,\n",
       " 0.023432066664099693,\n",
       " -0.06303749978542328,\n",
       " 0.0047907172702252865,\n",
       " 0.00024638400645926595,\n",
       " -0.020649516955018044,\n",
       " 0.0185630414634943,\n",
       " -0.0037150338757783175,\n",
       " 0.042722854763269424,\n",
       " 0.12250134348869324,\n",
       " 0.05748818442225456,\n",
       " -0.04554203152656555,\n",
       " -0.039375800639390945,\n",
       " 0.04082142561674118,\n",
       " 0.011343582533299923,\n",
       " 0.08676032721996307,\n",
       " -0.018865935504436493,\n",
       " -0.0019268598407506943,\n",
       " -0.01909697614610195,\n",
       " -0.08565337210893631,\n",
       " 0.5681220293045044,\n",
       " 0.12685710191726685,\n",
       " 0.1538267880678177,\n",
       " -0.01036332082003355,\n",
       " -0.012174157425761223,\n",
       " 0.1860111951828003,\n",
       " 0.031817857176065445,\n",
       " 0.04173360764980316,\n",
       " 0.02896464429795742,\n",
       " -0.03100358135998249,\n",
       " -0.017424605786800385,\n",
       " -0.01913810335099697,\n",
       " -0.02735772542655468,\n",
       " 0.06488393247127533,\n",
       " 0.01760532148182392,\n",
       " 0.04398100823163986,\n",
       " 0.019485486671328545,\n",
       " 0.043921925127506256,\n",
       " -0.010812162421643734,\n",
       " -0.06587809324264526,\n",
       " -0.022739097476005554,\n",
       " 0.09540416300296783,\n",
       " 0.0199970044195652,\n",
       " -0.04957873374223709,\n",
       " 0.013258825056254864,\n",
       " 0.006176680326461792,\n",
       " 0.09874878823757172,\n",
       " -0.08049360662698746,\n",
       " 0.02438499964773655,\n",
       " -0.05227994546294212,\n",
       " 0.0037910884711891413,\n",
       " 0.05346455052495003,\n",
       " 0.030204541981220245,\n",
       " 0.0036801465321332216,\n",
       " 0.0852692574262619,\n",
       " 0.036603379994630814,\n",
       " -0.06278350949287415,\n",
       " -0.012592775747179985,\n",
       " -0.05507664382457733,\n",
       " -0.04281105101108551,\n",
       " 0.08831626921892166,\n",
       " -0.030260780826210976,\n",
       " -0.015744592994451523,\n",
       " 0.045561425387859344,\n",
       " -0.04332415387034416,\n",
       " 0.04361937940120697,\n",
       " -0.07989231497049332,\n",
       " 0.15830379724502563,\n",
       " -0.08085374534130096,\n",
       " 0.05456532910466194,\n",
       " -0.034028310328722,\n",
       " 0.0427996851503849,\n",
       " 0.006386348977684975,\n",
       " 0.008409847505390644,\n",
       " -0.057998642325401306,\n",
       " -0.04392186924815178,\n",
       " 0.007197361905127764,\n",
       " -0.03929002955555916,\n",
       " 0.0876951739192009,\n",
       " 0.08204694092273712,\n",
       " 0.009655938483774662,\n",
       " 0.047638021409511566,\n",
       " 0.11134974658489227,\n",
       " -0.0004493028682190925,\n",
       " -0.007669809740036726,\n",
       " 0.024716325104236603,\n",
       " -0.005858960561454296,\n",
       " 0.03291698172688484,\n",
       " 0.050265923142433167,\n",
       " -0.010900157503783703,\n",
       " 0.013786545023322105,\n",
       " -0.014463771134614944,\n",
       " -0.01607048325240612,\n",
       " 0.022054413333535194,\n",
       " -0.09125374257564545,\n",
       " -0.008699669502675533,\n",
       " 0.043739207088947296,\n",
       " 0.05398291349411011,\n",
       " 0.029299259185791016,\n",
       " 0.06195994094014168,\n",
       " -0.07822859287261963,\n",
       " -0.07724865525960922,\n",
       " 0.00392907066270709,\n",
       " -0.044448111206293106,\n",
       " 0.07622642070055008,\n",
       " 0.020842351019382477,\n",
       " 0.07939551025629044,\n",
       " 0.08059144020080566,\n",
       " 0.06360308825969696,\n",
       " 0.02039271965622902,\n",
       " 0.06407324224710464,\n",
       " 0.025655847042798996,\n",
       " 0.018022073432803154,\n",
       " -0.018611500039696693,\n",
       " 0.03548961505293846,\n",
       " -0.05896716192364693,\n",
       " -0.11532154679298401,\n",
       " 0.03440726548433304,\n",
       " 0.05165988206863403,\n",
       " 0.04237472265958786,\n",
       " -0.0003363762516528368,\n",
       " -0.09496097266674042,\n",
       " -0.013269400224089622,\n",
       " -0.017047278583049774,\n",
       " -0.05686311796307564,\n",
       " -0.07341665774583817,\n",
       " -0.028907248750329018,\n",
       " -0.018737375736236572,\n",
       " 0.015613753348588943,\n",
       " -0.06316672265529633,\n",
       " -0.018389945849776268,\n",
       " -0.05794689431786537,\n",
       " -0.010699617676436901,\n",
       " 0.03181305527687073,\n",
       " -0.006989739369601011,\n",
       " -0.031923986971378326,\n",
       " -0.09256148338317871,\n",
       " -0.01694360189139843,\n",
       " -0.04218195378780365,\n",
       " 0.061736151576042175,\n",
       " 0.07721000164747238,\n",
       " 0.025981178507208824,\n",
       " 0.004840520676225424,\n",
       " -0.04854142665863037,\n",
       " 0.00865749642252922,\n",
       " 0.022213349118828773,\n",
       " -0.0545722059905529,\n",
       " -0.09488201141357422,\n",
       " -0.10204578936100006,\n",
       " 0.0009987519588321447,\n",
       " -0.05593698471784592,\n",
       " -0.054508522152900696,\n",
       " -0.00390974385663867,\n",
       " 0.04677275940775871,\n",
       " 0.04859362170100212,\n",
       " 0.062093473970890045,\n",
       " -0.039874810725450516,\n",
       " -0.10397965461015701,\n",
       " 0.05156955495476723,\n",
       " 0.010237840935587883,\n",
       " -0.030324162915349007,\n",
       " 0.04944578930735588,\n",
       " -0.030490107834339142,\n",
       " 0.04338084161281586,\n",
       " 0.017261339351534843,\n",
       " -0.07643527537584305,\n",
       " -0.06428395956754684,\n",
       " -0.012855255976319313,\n",
       " -0.026030894368886948,\n",
       " -0.04035571962594986,\n",
       " -0.05386709049344063,\n",
       " 0.029590509831905365,\n",
       " -0.05103456601500511,\n",
       " 0.02396935224533081,\n",
       " 0.0039991335943341255,\n",
       " -0.049736008048057556,\n",
       " -0.04707171395421028,\n",
       " -0.13003455102443695,\n",
       " 0.1368493139743805,\n",
       " -0.007810996845364571,\n",
       " -0.06390342861413956,\n",
       " 0.059647466987371445,\n",
       " 0.05745909363031387,\n",
       " 0.022465508431196213,\n",
       " -0.03589126095175743,\n",
       " -0.01250979769974947,\n",
       " 0.021464701741933823,\n",
       " 0.06428521871566772,\n",
       " 0.002638831501826644,\n",
       " 0.017883799970149994,\n",
       " 0.03244632109999657,\n",
       " 0.009045527316629887,\n",
       " 0.02195475436747074,\n",
       " 0.05979452282190323,\n",
       " 0.28377753496170044,\n",
       " -0.29178062081336975,\n",
       " 0.0042207553051412106,\n",
       " 0.04338517040014267,\n",
       " 0.03882697969675064,\n",
       " 0.04975847899913788,\n",
       " -0.03389803320169449,\n",
       " 0.06374555081129074,\n",
       " 0.09143979847431183,\n",
       " 0.15626834332942963,\n",
       " 0.1716933697462082,\n",
       " 0.0011715429136529565,\n",
       " 0.08486900478601456,\n",
       " -0.0471964105963707,\n",
       " 0.020415062084794044,\n",
       " 0.014842678792774677,\n",
       " 0.02994662895798683,\n",
       " 0.04855243116617203,\n",
       " -0.08807455003261566,\n",
       " 0.026721464470028877,\n",
       " 0.07164724171161652,\n",
       " -0.022720715031027794,\n",
       " -0.01521777082234621,\n",
       " -0.020494449883699417,\n",
       " -0.07109569013118744,\n",
       " -0.029362838715314865,\n",
       " 0.02821795828640461,\n",
       " 0.005584718659520149,\n",
       " -0.03133520856499672,\n",
       " 0.03331910818815231,\n",
       " -0.07658013701438904,\n",
       " 0.043279558420181274,\n",
       " -0.01279791072010994,\n",
       " -0.043217986822128296,\n",
       " -0.01911541447043419,\n",
       " 0.06538340449333191,\n",
       " -0.012679509818553925,\n",
       " -0.16901512444019318,\n",
       " 0.07878188043832779,\n",
       " -0.01588393747806549,\n",
       " 0.0255796667188406,\n",
       " 0.04191390797495842,\n",
       " -0.105067677795887,\n",
       " 0.010842859745025635,\n",
       " -0.007146937306970358,\n",
       " 0.022380029782652855,\n",
       " 0.006385620683431625,\n",
       " 0.08058314770460129,\n",
       " 0.012615731917321682,\n",
       " 0.032798465341329575,\n",
       " 0.11583392322063446,\n",
       " -0.03525421768426895,\n",
       " 0.061941757798194885,\n",
       " -0.06488144397735596,\n",
       " 0.05057995393872261,\n",
       " 0.12602001428604126,\n",
       " -0.0352032333612442,\n",
       " 0.047709256410598755,\n",
       " -0.02475137636065483,\n",
       " 0.01626514084637165,\n",
       " 0.024270249530673027,\n",
       " -0.030532190576195717,\n",
       " -0.07010092586278915,\n",
       " -0.036652494221925735,\n",
       " -0.04502687230706215,\n",
       " 0.07746629416942596,\n",
       " 0.004565645474940538,\n",
       " -0.016937144100666046,\n",
       " -0.11376567929983139,\n",
       " 0.00013339334691409022,\n",
       " -0.059659697115421295,\n",
       " -0.03200555592775345,\n",
       " 0.05939992889761925,\n",
       " -0.07425146549940109,\n",
       " 0.05857817083597183,\n",
       " 0.023894192650914192,\n",
       " 0.005960592068731785,\n",
       " 0.00349485338665545,\n",
       " -0.0695904865860939,\n",
       " 0.023362012580037117,\n",
       " -0.048779670149087906,\n",
       " -0.07996194064617157,\n",
       " 0.05314182490110397,\n",
       " 0.10189821571111679,\n",
       " 0.004038946237415075,\n",
       " 0.08912108838558197,\n",
       " -0.02936684712767601,\n",
       " 0.06338205188512802,\n",
       " 0.009796184487640858,\n",
       " -0.03674556314945221,\n",
       " -0.09753814339637756,\n",
       " -0.09440194070339203,\n",
       " 0.053885381668806076,\n",
       " -0.01746513321995735,\n",
       " -0.01840152218937874,\n",
       " -0.02344001643359661,\n",
       " -0.004448411054909229,\n",
       " -0.0061114151030778885,\n",
       " -0.09149478375911713,\n",
       " -0.061142634600400925,\n",
       " -0.02291332371532917,\n",
       " -0.08462906628847122,\n",
       " 0.11318814754486084,\n",
       " -0.042709119617938995,\n",
       " 0.018003657460212708,\n",
       " 0.0014426667476072907,\n",
       " -0.06940514594316483,\n",
       " 0.0018921316368505359,\n",
       " -0.04636333882808685,\n",
       " 0.008294692263007164,\n",
       " -0.06928307563066483,\n",
       " 0.010659489780664444,\n",
       " 0.006476887501776218,\n",
       " 0.03897649794816971,\n",
       " 0.052113477140665054,\n",
       " 0.024359451606869698,\n",
       " -0.02125515230000019,\n",
       " 0.0685599148273468,\n",
       " 0.014134584926068783,\n",
       " -0.03226615861058235,\n",
       " 0.08869607746601105,\n",
       " -0.06965454667806625,\n",
       " 0.029907604679465294,\n",
       " -0.05313265323638916,\n",
       " -0.28591370582580566,\n",
       " 0.016919689252972603,\n",
       " 0.0028306778986006975,\n",
       " 0.02289368025958538,\n",
       " -0.040519870817661285,\n",
       " -0.06034257262945175,\n",
       " 0.038116369396448135,\n",
       " 0.056554075330495834,\n",
       " 0.022087693214416504,\n",
       " 0.030488157644867897,\n",
       " -0.08357119560241699,\n",
       " 0.04008189216256142,\n",
       " -0.027379771694540977,\n",
       " -0.13153357803821564,\n",
       " 0.016587093472480774,\n",
       " -0.0067968182265758514,\n",
       " -0.05068743973970413,\n",
       " 0.10648276656866074,\n",
       " -0.044305142015218735,\n",
       " -0.026507332921028137,\n",
       " -0.16269546747207642,\n",
       " 0.03109557181596756,\n",
       " -0.07176941633224487,\n",
       " -0.010327800177037716,\n",
       " 0.027418484911322594,\n",
       " -0.00743482168763876,\n",
       " -0.08213834464550018,\n",
       " -0.09672925621271133,\n",
       " 0.020676417276263237,\n",
       " 0.07342265546321869,\n",
       " -0.00045200224849395454,\n",
       " -0.09070739150047302,\n",
       " -0.006364182569086552,\n",
       " 0.0048161162994802,\n",
       " 0.02816316857933998,\n",
       " 0.11356022953987122,\n",
       " -0.03309959918260574,\n",
       " -0.024598468095064163,\n",
       " -0.019606607034802437,\n",
       " 0.10225055366754532,\n",
       " 0.07021399587392807,\n",
       " 0.21269984543323517,\n",
       " 0.011341619305312634,\n",
       " 0.1704384684562683,\n",
       " 0.017053255811333656,\n",
       " 0.10086142271757126,\n",
       " 0.04652076214551926,\n",
       " 0.015906887128949165,\n",
       " -0.02313523180782795,\n",
       " -0.07362227141857147,\n",
       " -0.02058606781065464,\n",
       " -0.010983615182340145,\n",
       " 0.0469818189740181,\n",
       " -0.06907662749290466,\n",
       " -0.052717190235853195,\n",
       " -0.001253766124136746,\n",
       " -0.0034297264646738768,\n",
       " 0.013104453682899475,\n",
       " -0.023851221427321434,\n",
       " 0.10813648253679276,\n",
       " 0.028742291033267975,\n",
       " 0.06120465695858002,\n",
       " -0.03862231224775314,\n",
       " -0.015909122303128242,\n",
       " 0.03489195182919502,\n",
       " -0.0583132840692997,\n",
       " -0.024759475141763687,\n",
       " 0.0032649070490151644,\n",
       " -0.00693918252363801,\n",
       " -0.022041458636522293,\n",
       " -0.01191727351397276,\n",
       " -0.0826391726732254,\n",
       " -0.00032920949161052704,\n",
       " -0.023384999483823776,\n",
       " 0.013053189031779766,\n",
       " 0.0933789536356926,\n",
       " -0.0398193821310997,\n",
       " 0.06226425617933273,\n",
       " -0.002655437681823969,\n",
       " 0.11333725601434708,\n",
       " 0.009487118571996689,\n",
       " 0.024863244965672493,\n",
       " 0.04566824063658714,\n",
       " 0.06881234049797058,\n",
       " -0.014578747563064098,\n",
       " 0.023214969784021378,\n",
       " 0.030217524617910385,\n",
       " -0.08874916285276413,\n",
       " 0.03631175309419632,\n",
       " 0.11903325468301773,\n",
       " -0.01862489804625511,\n",
       " 0.06801699846982956,\n",
       " 0.07027295231819153,\n",
       " 0.027321290224790573,\n",
       " -0.020377997308969498,\n",
       " 0.00406067306175828,\n",
       " 0.05224758759140968,\n",
       " 0.05247519537806511,\n",
       " -0.5734594464302063,\n",
       " -0.015109115280210972,\n",
       " 0.1345064640045166,\n",
       " 0.03992572799324989,\n",
       " -0.0295004453510046,\n",
       " 0.02595839649438858,\n",
       " 0.04929359257221222,\n",
       " -0.045755449682474136,\n",
       " -0.01843084953725338,\n",
       " -0.025081465020775795,\n",
       " 0.10087119042873383,\n",
       " 0.009139600209891796,\n",
       " 0.11501334607601166,\n",
       " -0.007322863209992647,\n",
       " -0.015711747109889984,\n",
       " 0.046489253640174866,\n",
       " -0.044804058969020844,\n",
       " 0.010291974060237408,\n",
       " 0.021825604140758514,\n",
       " -0.21987125277519226,\n",
       " -0.021733228117227554,\n",
       " -0.11206070333719254,\n",
       " 0.07989270985126495,\n",
       " -0.04700859263539314,\n",
       " 0.015147753059864044,\n",
       " 0.08549347519874573,\n",
       " -0.0626719743013382,\n",
       " 0.09461146593093872,\n",
       " 0.061161212623119354,\n",
       " -0.022204138338565826,\n",
       " 0.056696441024541855,\n",
       " 0.0636654868721962,\n",
       " 0.005457155406475067,\n",
       " 0.06973742693662643,\n",
       " 0.06697166711091995,\n",
       " 0.07666701823472977,\n",
       " 0.03500848636031151,\n",
       " 11.502883911132812,\n",
       " -0.038649071007966995,\n",
       " 0.017855392768979073,\n",
       " -0.025054702535271645,\n",
       " 0.013574724085628986,\n",
       " -0.058831837028265,\n",
       " 0.0431445874273777,\n",
       " -0.09132145345211029,\n",
       " -0.051583584398031235,\n",
       " 0.15845341980457306,\n",
       " -0.024876290932297707,\n",
       " -0.08595984429121017,\n",
       " -0.07616337388753891,\n",
       " -0.11288295686244965,\n",
       " 0.04585438221693039,\n",
       " 0.018757978454232216,\n",
       " -0.08364702761173248,\n",
       " -0.04122171550989151,\n",
       " 0.030344819650053978,\n",
       " -0.033089008182287216,\n",
       " 0.03701826184988022,\n",
       " 0.019094448536634445,\n",
       " 0.03930468112230301,\n",
       " 0.0023256801068782806,\n",
       " -0.05760044977068901,\n",
       " 0.061465486884117126,\n",
       " 0.07180633395910263,\n",
       " -0.027842281386256218,\n",
       " -0.04418038949370384,\n",
       " 0.0671076774597168,\n",
       " 0.060721125453710556,\n",
       " 0.025487855076789856,\n",
       " 0.0454440712928772,\n",
       " -0.03267376124858856,\n",
       " 0.1354934275150299,\n",
       " 0.00531325489282608,\n",
       " 0.011833170428872108,\n",
       " 0.08188292384147644,\n",
       " 0.013812984339892864,\n",
       " 0.034124791622161865,\n",
       " -0.02207571640610695,\n",
       " -0.009056279435753822,\n",
       " 0.0833369567990303,\n",
       " 0.04745468869805336,\n",
       " 0.055765245109796524,\n",
       " -0.02738385833799839,\n",
       " 0.024983715265989304,\n",
       " 0.12011256814002991,\n",
       " 0.006824173498898745,\n",
       " 0.07812938094139099,\n",
       " 0.07735370099544525,\n",
       " -0.04141386225819588,\n",
       " 0.1366695612668991,\n",
       " -0.001683016773313284,\n",
       " -0.012972323223948479,\n",
       " -0.04506340250372887,\n",
       " 0.022106768563389778,\n",
       " -0.06929007917642593,\n",
       " 0.08827989548444748,\n",
       " 0.09623853862285614,\n",
       " -0.061553508043289185,\n",
       " 0.12154621630907059,\n",
       " 0.02075239084661007,\n",
       " 0.02163536846637726,\n",
       " 0.01843789406120777,\n",
       " 0.15026940405368805,\n",
       " 0.07890885323286057,\n",
       " 0.09265021234750748,\n",
       " -0.11559667438268661,\n",
       " -0.014978872612118721,\n",
       " 0.008619587868452072,\n",
       " -0.045299429446458817,\n",
       " -0.0648534819483757,\n",
       " 0.0781092420220375,\n",
       " 0.0679531916975975,\n",
       " -0.07666154205799103,\n",
       " 0.023236535489559174,\n",
       " -0.015692556276917458,\n",
       " -0.0454709567129612,\n",
       " -0.06698892265558243,\n",
       " -0.0008972917567007244,\n",
       " 0.01307358406484127,\n",
       " 0.02396148256957531,\n",
       " -0.014634532853960991,\n",
       " 0.11450386792421341,\n",
       " 0.05838967487215996,\n",
       " 0.038238346576690674,\n",
       " 0.09263566136360168,\n",
       " -0.04120352491736412,\n",
       " -0.04964567720890045,\n",
       " -0.010928643867373466,\n",
       " 0.04319054260849953,\n",
       " 0.019505904987454414,\n",
       " -0.07743918895721436,\n",
       " -0.01683695800602436,\n",
       " -0.013378112576901913,\n",
       " 0.06996569037437439,\n",
       " -0.09066999703645706,\n",
       " 0.040897779166698456,\n",
       " 0.06009334325790405,\n",
       " -0.10796041041612625,\n",
       " -0.022694304585456848,\n",
       " -0.01953183300793171,\n",
       " 0.09239064157009125,\n",
       " -0.0017576012760400772,\n",
       " 0.0008986530010588467,\n",
       " -0.030193470418453217,\n",
       " -0.012293539941310883,\n",
       " 0.04549875110387802,\n",
       " 0.002026076428592205,\n",
       " -0.03984478861093521,\n",
       " 0.028428446501493454,\n",
       " 0.02476208284497261,\n",
       " -0.07435756176710129,\n",
       " 0.02761857584118843,\n",
       " 0.03746093809604645,\n",
       " 0.014198848977684975,\n",
       " 0.05976441130042076,\n",
       " 0.04968716576695442,\n",
       " 0.11628363281488419,\n",
       " -0.10596533864736557,\n",
       " -0.002865568967536092,\n",
       " -0.0432717390358448,\n",
       " 0.0023935509379953146,\n",
       " -0.009407097473740578,\n",
       " 0.005160569213330746,\n",
       " -0.07371285557746887,\n",
       " -0.012129388749599457,\n",
       " -0.013813975267112255,\n",
       " 0.01603071577847004,\n",
       " -0.03517588973045349,\n",
       " 0.05555066093802452,\n",
       " -0.006245662923902273,\n",
       " -0.0434948168694973,\n",
       " 0.0897572860121727,\n",
       " -0.004589611664414406,\n",
       " -0.0016217493684962392,\n",
       " 0.07306087762117386,\n",
       " 0.03651055693626404,\n",
       " 0.010066233575344086,\n",
       " -0.04253173992037773,\n",
       " 0.0017063973937183619,\n",
       " -0.07850777357816696,\n",
       " -0.06507332623004913,\n",
       " 0.0222178902477026,\n",
       " 0.09490349143743515,\n",
       " 0.07518456876277924,\n",
       " 0.0375962071120739,\n",
       " 0.05611230805516243,\n",
       " -0.12092214822769165,\n",
       " -0.06788058578968048,\n",
       " 0.07741507142782211,\n",
       " 0.07873570173978806,\n",
       " 0.024624573066830635,\n",
       " 0.04158011078834534,\n",
       " -0.03443353995680809,\n",
       " -0.04642781242728233,\n",
       " 0.0739564448595047,\n",
       " -0.06537658721208572,\n",
       " 0.03934003412723541,\n",
       " 0.024388834834098816,\n",
       " -0.030850261449813843,\n",
       " 0.04003036394715309,\n",
       " 0.10731695592403412,\n",
       " 0.057732824236154556,\n",
       " 0.04928296431899071,\n",
       " -0.0013270609779283404,\n",
       " 0.006960338447242975,\n",
       " 0.049719784408807755,\n",
       " -0.03975554183125496,\n",
       " 0.06165838986635208,\n",
       " 0.02330659329891205,\n",
       " -0.08261353522539139,\n",
       " -0.04609210789203644,\n",
       " 0.018808448687195778,\n",
       " 0.1316077560186386,\n",
       " 0.03442014381289482,\n",
       " -0.09837204217910767,\n",
       " -0.038243357092142105,\n",
       " -0.053313225507736206,\n",
       " -0.007968751713633537,\n",
       " -0.029836352914571762,\n",
       " 0.04242327809333801,\n",
       " -0.05710526183247566,\n",
       " 0.0766967311501503,\n",
       " 0.35191047191619873,\n",
       " -0.13628575205802917,\n",
       " -0.06061777472496033,\n",
       " 0.02563885785639286,\n",
       " -0.07988099008798599,\n",
       " -0.1424689143896103,\n",
       " -0.07648669183254242,\n",
       " 0.0008841360104270279,\n",
       " -0.0508430041372776,\n",
       " 0.05591076612472534,\n",
       " 0.349307656288147,\n",
       " 0.024572663009166718,\n",
       " 0.023797137662768364,\n",
       " -0.033455006778240204,\n",
       " -0.20904269814491272,\n",
       " -0.05710186809301376,\n",
       " -0.013867393136024475,\n",
       " -0.12203245609998703,\n",
       " -0.1550283282995224,\n",
       " -0.14516837894916534,\n",
       " -0.08335801213979721,\n",
       " 0.1934582144021988,\n",
       " -0.040169645100831985,\n",
       " 0.1241743192076683,\n",
       " 0.054147616028785706,\n",
       " -0.19576846063137054,\n",
       " -0.15868616104125977,\n",
       " 0.001185862347483635,\n",
       " 0.05327204242348671,\n",
       " -0.025177136063575745,\n",
       " -0.054988209158182144,\n",
       " 0.16926637291908264,\n",
       " -0.07013201713562012,\n",
       " -0.41425931453704834,\n",
       " 0.014493491500616074,\n",
       " -0.14078712463378906,\n",
       " -0.15480715036392212,\n",
       " -0.0749775692820549,\n",
       " 0.13233603537082672,\n",
       " 0.004709000699222088,\n",
       " -0.06496863067150116,\n",
       " 0.04866655543446541,\n",
       " 0.07849720865488052,\n",
       " -0.030931493267416954,\n",
       " -0.21346276998519897,\n",
       " -0.2234608381986618,\n",
       " -0.02811252884566784,\n",
       " -0.0706445723772049,\n",
       " -0.15118145942687988,\n",
       " 0.09238441288471222,\n",
       " -0.06673503667116165,\n",
       " 0.11090018600225449,\n",
       " -0.48454615473747253,\n",
       " -0.035993728786706924,\n",
       " -0.15726567804813385,\n",
       " -0.18853792548179626,\n",
       " 0.873434841632843,\n",
       " 0.02355528622865677,\n",
       " -0.22319090366363525,\n",
       " -0.15044823288917542,\n",
       " -0.07209020853042603,\n",
       " 0.06686617434024811,\n",
       " 0.046508193016052246,\n",
       " 0.014638888649642467,\n",
       " -0.0315166600048542,\n",
       " -0.028929470106959343,\n",
       " -0.11000522971153259,\n",
       " 0.01743188686668873,\n",
       " 0.26178693771362305,\n",
       " -0.17235922813415527,\n",
       " 0.1552484631538391,\n",
       " 0.06599922478199005,\n",
       " -5.262625217437744,\n",
       " 0.07757453620433807,\n",
       " 0.3275645673274994,\n",
       " -0.0366484671831131,\n",
       " 0.12626025080680847,\n",
       " 1.5960445404052734,\n",
       " -0.01290128007531166,\n",
       " 0.05003543943166733,\n",
       " 0.01710168644785881,\n",
       " 0.06414182484149933,\n",
       " 0.23671744763851166,\n",
       " 0.15491624176502228,\n",
       " -0.08503058552742004,\n",
       " 0.3759176433086395,\n",
       " -0.02975630573928356,\n",
       " -0.04740818589925766,\n",
       " -0.4242144823074341,\n",
       " 0.09042807668447495,\n",
       " 0.10380513966083527,\n",
       " -0.018176373094320297,\n",
       " 0.5809704065322876,\n",
       " 0.026353463530540466,\n",
       " -0.15154936909675598,\n",
       " -0.14061228930950165,\n",
       " 0.08579585701227188,\n",
       " -0.0014842470409348607,\n",
       " 0.14041967689990997,\n",
       " 0.05071541294455528,\n",
       " 0.02640281617641449,\n",
       " -0.09427572786808014,\n",
       " -0.022059880197048187,\n",
       " -0.2055608630180359,\n",
       " -0.03333176672458649,\n",
       " -0.08741393685340881,\n",
       " -0.19896718859672546,\n",
       " -0.07252504676580429,\n",
       " 0.030611248686909676,\n",
       " -0.08470124006271362,\n",
       " 0.14021767675876617,\n",
       " -0.0017326551023870707,\n",
       " 0.12251195311546326,\n",
       " 0.12580518424510956,\n",
       " -0.26584944128990173,\n",
       " 0.05621788278222084,\n",
       " 0.020984241738915443,\n",
       " -0.01200797688215971,\n",
       " 0.2316180020570755,\n",
       " -0.14587022364139557,\n",
       " 0.023360664024949074,\n",
       " 0.21471460163593292,\n",
       " -0.11146242916584015,\n",
       " -0.035230230540037155,\n",
       " 0.1621367186307907,\n",
       " -0.1736876368522644,\n",
       " -0.9872607588768005,\n",
       " 0.04983051121234894,\n",
       " 0.38329216837882996,\n",
       " 0.019343508407473564,\n",
       " -0.02372334711253643,\n",
       " 0.11351364850997925,\n",
       " -0.0853319764137268,\n",
       " -0.08008723706007004,\n",
       " 0.15486468374729156,\n",
       " 0.02559899538755417,\n",
       " 0.1644570678472519,\n",
       " -0.10268828272819519,\n",
       " -0.014317984692752361,\n",
       " -0.014955843798816204,\n",
       " -0.009174899198114872,\n",
       " 0.023342695087194443,\n",
       " -0.08334430307149887,\n",
       " -0.020869199186563492,\n",
       " 0.23376278579235077,\n",
       " 0.1769980788230896,\n",
       " -0.04214867204427719,\n",
       " -0.08757857978343964,\n",
       " -0.07680834084749222,\n",
       " -0.18494217097759247,\n",
       " 0.10622934252023697,\n",
       " 0.08116908371448517,\n",
       " 0.3990500867366791,\n",
       " 0.0051521919667720795,\n",
       " -0.06205201894044876,\n",
       " 0.18757577240467072,\n",
       " 0.11021791398525238,\n",
       " 0.3160672187805176,\n",
       " -0.059313561767339706,\n",
       " -0.012148461304605007,\n",
       " -0.1416197121143341,\n",
       " 0.11636901646852493,\n",
       " -0.030718065798282623,\n",
       " 0.07227780669927597,\n",
       " -0.16770371794700623,\n",
       " -0.16023553907871246,\n",
       " -0.01903146505355835,\n",
       " 0.009913153015077114,\n",
       " -0.06176203116774559,\n",
       " 0.06261416524648666,\n",
       " 0.007917392067611217,\n",
       " -0.06206667795777321,\n",
       " 0.009026747196912766,\n",
       " -0.035594698041677475,\n",
       " 0.14766916632652283,\n",
       " 0.011597694829106331,\n",
       " 0.04018216207623482,\n",
       " -0.005928026046603918,\n",
       " 0.03261668607592583,\n",
       " 0.11636406183242798,\n",
       " 0.026188142597675323,\n",
       " 0.09873206913471222,\n",
       " -0.23003612458705902,\n",
       " 0.11208179593086243,\n",
       " 0.12794123589992523,\n",
       " 0.09611763805150986,\n",
       " 0.11069627851247787,\n",
       " -0.12703996896743774,\n",
       " 0.3015213906764984,\n",
       " 0.22113455832004547,\n",
       " 0.026225334033370018,\n",
       " -0.11028434336185455,\n",
       " 0.015028283931314945,\n",
       " 0.03737661987543106,\n",
       " 0.13865627348423004,\n",
       " -0.013302210718393326,\n",
       " 0.028000952675938606,\n",
       " 0.3399127721786499,\n",
       " 0.033563561737537384,\n",
       " 0.09327061474323273,\n",
       " 0.042566340416669846,\n",
       " 0.1176266074180603,\n",
       " 0.05950639396905899,\n",
       " -0.02641058899462223,\n",
       " -0.1916523277759552,\n",
       " -0.09170790761709213,\n",
       " 0.6058546900749207,\n",
       " 0.010981856845319271,\n",
       " 0.039774708449840546,\n",
       " 0.06680653244256973,\n",
       " -0.0695718303322792,\n",
       " -0.15890663862228394,\n",
       " -1.7691893577575684,\n",
       " 0.057207655161619186,\n",
       " -0.45157402753829956,\n",
       " 0.10063028335571289,\n",
       " -0.14498789608478546,\n",
       " 0.007992211729288101,\n",
       " 0.3381873071193695,\n",
       " -0.1693730354309082,\n",
       " 0.0038567143492400646,\n",
       " 0.03709704801440239,\n",
       " 0.03362458944320679,\n",
       " -0.08427221328020096,\n",
       " -0.19130562245845795,\n",
       " -0.023714259266853333,\n",
       " -0.4228726625442505,\n",
       " ...]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"outputs\"][0][\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(\n",
    "    torch.tensor(output[\"outputs\"][0][\"data\"], dtype=torch.int8).type(\n",
    "        torch.int\n",
    "    ),  # tokenizer.decode(unscripted_output[0])\n",
    "    skip_special_tokens=True,\n",
    "    clean_up=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e413c074-0fdf-4850-ad78-4ec5f1819660",
   "metadata": {},
   "source": [
    "**Invoke using the Binary Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c3fe2c5-5a5d-46f6-bb13-451af0cbc99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\n",
    "    \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\",\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=512,  #\n",
    "    pad_to_max_length=True,  # Pad & truncate all sentences\n",
    ")\n",
    "# Traced Model expects ONLY the INPUT ID's\n",
    "input_ids = encoded_input[\"input_ids\"]\n",
    "attention_mask = encoded_input[\"attention_mask\"]\n",
    "\n",
    "triton_request_body, triton_header_length = prepare_roberta_2_inputs(\n",
    "    input_ids, attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ee35125a-4655-4bd5-b0b3-14f091b45611",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '62cb3f1a-f045-43a0-8e32-3337dd447d8d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '62cb3f1a-f045-43a0-8e32-3337dd447d8d', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Tue, 21 Feb 2023 18:35:01 GMT', 'content-type': 'application/vnd.sagemaker-triton.binary+json;json-header-size=274', 'content-length': '1576210'}, 'RetryAttempts': 0}, 'ContentType': 'application/vnd.sagemaker-triton.binary+json;json-header-size=274', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7f97e0c2ddd0>}\n",
      "Error in parsing response -- \n"
     ]
    }
   ],
   "source": [
    "response_binary = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/vnd.sagemaker-triton.binary+json;json-header-size={}\".format(\n",
    "        triton_header_length\n",
    "    ),\n",
    "    Body=triton_request_body,\n",
    "    TargetModel=f\"{tar_file_name}\",\n",
    ")\n",
    "print(response_binary)\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "# print(response_binary[\"Body\"].read())\n",
    "\n",
    "try:\n",
    "    # Read response body\n",
    "    result = httpclient.InferenceServerClient.parse_response_body(\n",
    "        response_binary[\"Body\"].read()  # , header_length=int(header_length_str)\n",
    "    )\n",
    "    output0_data = result.as_numpy(\"1634__1\")\n",
    "    output1_data = result.as_numpy(\"OUTPUT__0\")\n",
    "    print(output0_data)\n",
    "    print(output1_data)\n",
    "except:\n",
    "    print(\"Error in parsing response -- \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1745887a",
   "metadata": {},
   "source": [
    "### Stress Test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7e2ff26-d79e-4061-b702-900753a408f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-425576326687/mme-roberta-benchmark/roberta-large/roberta-traced-v1.tar.gz\n",
      "s3://sagemaker-us-east-1-425576326687/mme-roberta-benchmark/roberta-large/\n",
      "roberta-base\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "print(s3_model_path_triton)\n",
    "print(s3_mme_model_path)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "56358ec4-eeba-4cbb-9977-7a981d9fdb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leverage the Tokenizer=RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})::max_seq_length=512:: create above when creating the model \n",
      "Tokenize:text:why??::max_length=512::Tokenizer=RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n"
     ]
    }
   ],
   "source": [
    "text_triton = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\"\n",
    "print(\n",
    "    f\"Leverage the Tokenizer={tokenizer}::max_seq_length={max_seq_length}:: create above when creating the model \"\n",
    ")\n",
    "\n",
    "input_ids, attention_mask = tokenize_text(\n",
    "    text_triton, tokenizer, max_length=max_seq_length\n",
    ")\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"shape\": [1, max_seq_length],\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"data\": input_ids,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"INPUT__1\",\n",
    "            \"shape\": [1, max_seq_length],\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"data\": attention_mask,\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_loaded = 0\n",
    "memory_utilization_threshold = 0.9\n",
    "memory_utilization_history = []\n",
    "max_models_test = 10\n",
    "while models_loaded < max_models_test:\n",
    "    # make a copy of the model\n",
    "    !aws s3 cp {s3_model_path_triton} {s3_mme_model_path}/{model_name}-v{models_loaded}.tar.gz\n",
    "    \n",
    "    # make a inference request to load model into memory\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        ContentType=\"application/octet-stream\", \n",
    "        Body=json.dumps(payload),\n",
    "        TargetModel=f\"{model_name}-v{models_loaded}.tar.gz\", \n",
    "    )\n",
    "    \n",
    "    models_loaded+=1\n",
    "    \n",
    "        \n",
    "    print(f\"loaded {models_loaded} models with memory utilzation of {memory_utilization:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9aaaa",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a412ed75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '02dbb082-eb76-417e-bd6a-9ed782b95f18',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '02dbb082-eb76-417e-bd6a-9ed782b95f18',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 21 Feb 2023 18:13:34 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "sm_client.delete_model(ModelName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72583cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
