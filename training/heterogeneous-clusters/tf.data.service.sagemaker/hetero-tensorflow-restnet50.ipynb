{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow's tf.data.service with Amazon SageMaker Training Heterogeneous Clusters\n",
    "\n",
    "---\n",
    "### Introduction\n",
    "\n",
    "Heterogeneous clusters enable launching training jobs that use multiple instance types in a single job. This capability can improve your training cost and speed by running different parts of the model training on the most suitable instance type. This use case typically happens in computer vision (CV) deep learning (DL) training, where training is bottleneck on CPU resources needed for data augmentation, leaving the expensive GPU underutilized. Heterogeneous clusters enable you to add more CPU resources to fully utilize GPUs to increase training speed and cost-efficiency. For more details, you can find the documentation of this feature [here](https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html).\n",
    "\n",
    "This notebook demonstrates how to use Heterogeneous Clusters with TensorFlow's [tf.data.service](https://www.TensorFlow.org/api_docs/python/tf/data/experimental/service). It includes training a CPU intensive DL CV workload. Comparing cost and performance between homogeneous and heterogeneous training configurations.  \n",
    "\n",
    "üí°To get started quickly with heterogeneous clusters, we suggest you'll reuse the provided code as a quick way to migrate your workload from a local tf.data pipeline to a distributed tf.data.service pipeline. You'll need to change [code/train_dnn.py](./code/train_dnn.py), while keeping [code/train_data.py](./code/train_data.py) and [code/launcher.py](code/launcher.py) intact. This is explained below in the [Workload Details] section.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers:\n",
    "- A guide to switching from a homogeneous job (single instance type) to a heterogeneous job (multiple instance types)\n",
    "- Explaining to use Heterogeneous clusters with TensorFlow's tf.data.service\n",
    "- Set up Amazon SageMaker Studio Notebook \n",
    "- Run homogeneous cluster training job \n",
    "- Run heterogeneous cluster training job \n",
    "- Compare time and cost to train between homogeneous and heterogeneous clusters\n",
    "- Conclusion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A guide to switching from a homogeneous to a heterogeneous job\n",
    "\n",
    "This notebook runs and compares these two workloads:\n",
    "\n",
    "Homogeneous Training Job - The image shows a ml.p4d.24xlarge instance GPUs is under-utilized due to a CPU bottleneck.  \n",
    "<img src=images/basic-homogeneous-job.png alt=\"homogeneous training job\" />\n",
    "  \n",
    "Heterogeneous Training Job - The image shows two ml.c5.18xlarge instances with extra CPU cores, to reduce the CPU bottleneck and increase GPU usage, to improve training speed cost-efficiency.  \n",
    " <img src=images/basic-heterogeneous-job.png alt=\"Heterogeneous training job\" />\n",
    "\n",
    "In each workload: Training data is an artificially generated dataset consisting of 32x32x3 images with random pixel values, and a corresponding random label representing 10 different classes. As this dataset is randomly generated, you should not expect the model to converge in a meaningful way. This shouldn't matter as our intent is only to measure data pipeline and neural network optimization throughput expressed in epoch/step time.  \n",
    "The model we used is [Resnet50](https://www.TensorFlow.org/api_docs/python/tf/keras/applications/ResNet50). The workloads uses an 8 GPUs instance, ml.p4d.24xlarge, and uses Horovod for data parallelization. \n",
    "\n",
    "The heterogeneous job will include two instance groups:\n",
    "- **data_group** - A group of CPU instances that will run data pre-processing code.\n",
    "- **dnn_group** - A group of GPU instances that will run Deep Neural Network training code.\n",
    "\n",
    "In this example, the inter-node communication between CPU and GPU instance groups is implemented using [TensorFlow data service feature](https://www.TensorFlow.org/api_docs/python/tf/data/experimental/service). This feature allows offloading a configurable amount of preprocessing work to worker machines. Note that SageMaker's Heterogeneous cluster does not provide out-of-the-box support for inter-instance_group communication, and it is up to the user to implement (we provide reference implementation here).\n",
    "\n",
    "This notebook refers following files and folders:\n",
    "- [code/train_dnn.py](./code/train_dnn.py) - this is standard TF training script, it has a single reference to tf.data.service when setting up the tf.data pipeline. This script will be executed on GPU instances belonging to the dnn_group.\n",
    "- [code/train_data.py](./code/train_data.py) - this script starts tf.data.service services like a tf.data.service Dispatcher and tf.data.service Worker processes. You shouldn't edit this script when adjusting to your workload.\n",
    "- [code/launcher.py](./code/launcher.py) - Entry point training script. This is the script that SageMaker Training will start on all instances (all instances groups must share the same entry point script in heterogeneous clusters). `launcher.py` is responsible for detecting the instance group the instance belong to, and start `train_dnn.py` and `train_data.py` accordingly. It is also responsible for shutting down tf.data.services the training script completes (`train_dnn.py`) so all instances exit allowing the SageMaker training job to complete. \n",
    "In every instance `luncher.py` will use `train_data.py` to start a tf.data.service worker server (As all instance types have CPUs that could be used for preprocessing). `luncher.py` will start a single tf.data.service dispatcher server (on the first instance of the `data_group`).  \n",
    "`luncher.py` will start the `train_dnn.py` script in all GPU instances (`dnn_group` instances).\n",
    "\n",
    "#### Learn more about tf.data.service processes\n",
    "`tf.data.service Dispatcher` - The dispatcher server acts as the control plain for tf.data.service; Being responsible for registering worker servers and assigning preprocessing tasks to them. Each training job has a single Dispatcher running in the first instance of the `data_group` and listens on port 6000.\n",
    "`tf.data.service Workers` - Worker servers carry out the data processing. Each instance could have one or more workers (listen on port 6001/6002/...).\n",
    "\n",
    "##### Defining what part of your pipeline runs in which instance group\n",
    " When you apply `tf.data.experimental.service.distribute()` on your dataset, all preprocessing operations defined up to the apply will run on the tf.data.service workers, and all dataset operations defined afterwords will run on the local process. All instances will need access to a dataset you'll make available through a SageMaker training data channel. You do have the option of limiting which instance group will see which training data channel.\n",
    "\n",
    "The below figure shows sequence of events of setting up and running in a tf.data.service based heterogeneous cluster training job.\n",
    "\n",
    "<img src=images/tf.data.service-diagram.png width=600px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### security groups update if running in private VPC\n",
    "This section is relevant if you plan to [run in a private VPC](https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html) (passing `subnets` and `security_group_ids` parameters when defining an Estimator).  \n",
    "SageMaker documentation recommends you [add](https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html#train-vpc-vpc) a rule for your security group that allows inbound connections between members of the same security group, for all TCP communication. This will also cover for the tf.data.service related traffic between instances:\n",
    "- tf.data.service Dispatcher node will listen for incoming connections on port 6000 (configurable) from all nodes.\n",
    "- tf.data.service Workers will listen on ports 6001-6006 from all nodes.\n",
    "- Each node listens on port 16000 for a tf.data.service shutdown signal from all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Set up SageMaker Studio notebook\n",
    "#### Before you start\n",
    "Ensure you have selected Python 3 (_TensorFlow 2.6 Python 3.8 CPU Optimized_) image for your SageMaker Studio Notebook instance, and running on _ml.t3.medium_ instance type.\n",
    "\n",
    "#### Step 1 - Upgrade SageMaker SDK and dependent packages \n",
    "Heterogeneous Clusters for Amazon SageMaker model training was [announced](https://aws.amazon.com/about-aws/whats-new/2022/07/announcing-heterogeneous-clusters-amazon-sagemaker-model-training) on 07/08/2022. This feature release requires you to have updated SageMaker SDK, Boto3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (1.24.72)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.24.80-py3-none-any.whl (132 kB)\n",
      "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 132.5/132.5 kB 925.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: botocore in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (1.27.72)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.27.80-py3-none-any.whl (9.1 MB)\n",
      "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9.1/9.1 MB 16.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: awscli in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (1.25.73)\n",
      "Collecting awscli\n",
      "  Downloading awscli-1.25.81-py3-none-any.whl (3.9 MB)\n",
      "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.9/3.9 MB 38.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sagemaker in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (2.109.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/jmespath-1.0.0-py3.9.egg (from boto3) (1.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/urllib3-1.26.9-py3.9.egg (from botocore) (1.26.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/python_dateutil-2.8.2-py3.9.egg (from botocore) (2.8.2)\n",
      "Requirement already satisfied: PyYAML<5.5,>=3.10 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from awscli) (5.4.1)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from awscli) (0.16)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from awscli) (0.4.4)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/importlib_metadata-4.11.3-py3.9.egg (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: pathos in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pathos-0.2.8-py3.9.egg (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/protobuf3_to_dict-0.1.5-py3.9.egg (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: pandas in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pandas-1.4.2-py3.9-macosx-10.9-x86_64.egg (from sagemaker) (1.4.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from sagemaker) (1.22.4)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/attrs-20.3.0-py3.9.egg (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/smdebug_rulesconfig-1.0.1-py3.9.egg (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: google-pasta in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/google_pasta-0.2.0-py3.9.egg (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from sagemaker) (3.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/packaging-21.3-py3.9.egg (from sagemaker) (21.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/zipp-3.7.0-py3.9.egg (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pyparsing-3.0.7-py3.9.egg (from packaging>=20.0->sagemaker) (3.0.7)\n",
      "Requirement already satisfied: six in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pytz-2022.1-py3.9.egg (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: dill>=0.3.4 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/dill-0.3.4-py3.9.egg (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/multiprocess-0.70.12.2-py3.9.egg (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/pox-0.3.0-py3.9.egg (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /Users/gili/dev/hetro-training/.venv/lib/python3.9/site-packages/ppft-1.6.6.4-py3.9.egg (from pathos->sagemaker) (1.6.6.4)\n",
      "Installing collected packages: botocore, boto3, awscli\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.27.72\n",
      "    Uninstalling botocore-1.27.72:\n",
      "      Successfully uninstalled botocore-1.27.72\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.24.72\n",
      "    Uninstalling boto3-1.24.72:\n",
      "      Successfully uninstalled boto3-1.24.72\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.25.73\n",
      "    Uninstalling awscli-1.25.73:\n",
      "      Successfully uninstalled awscli-1.25.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-training 4.2.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed awscli-1.25.81 boto3-1.24.80 botocore-1.27.80\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m pip install --upgrade boto3 botocore awscli sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Restart the notebook kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import IPython\n",
    "#IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Validate SageMaker Python SDK and TensorFlow versions\n",
    "Ensure the output of the cell below reflects:\n",
    "\n",
    "- SageMaker Python SDK version 2.98.0 or above, \n",
    "- boto3 1.24 or above \n",
    "- botocore 1.27 or above \n",
    "- TensorFlow 2.6 or above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sagemaker\n",
      "Version: 2.109.0\n",
      "---\n",
      "Name: boto3\n",
      "Version: 1.24.80\n",
      "---\n",
      "Name: botocore\n",
      "Version: 1.27.80\n",
      "---\n",
      "Name: tensorflow\n",
      "Version: 2.8.0\n",
      "---\n",
      "Name: protobuf\n",
      "Version: 3.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip show sagemaker boto3 botocore tensorflow protobuf |egrep 'Name|Version|---'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Run a homogeneous training job\n",
    "#### Step 1: Set up the training environment\n",
    "In this step, we define and submit a homogeneous training job. It uses a single instance type (p4d.24xlarge) with 8 GPUs. The analysis of the job will shows that it is CPU bound and therefore its GPUs are underutilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "import os\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": 10,\n",
    "    \"steps_per_epoch\": 500,\n",
    "    \"batch_size\": 1024,\n",
    "    \"tf_data_mode\": \"local\",  # We won't be using tf.data.service ('service') for this homogeneous job\n",
    "    \"num_of_data_workers\": 0,  # We won't be using tf.data.service ('service') for this homogeneous job\n",
    "}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"launcher.py\",\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"2.9.1\",\n",
    "    py_version=\"py39\",\n",
    "    role=role,\n",
    "    volume_size=10,\n",
    "    max_run=1800,  # 30 minutes\n",
    "    disable_profiler=True,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    instance_count=1,\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": 8,  # 8 GPUs per host\n",
    "            \"custom_mpi_options\": \"--NCCL_DEBUG WARN\",\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Submit the training job\n",
    "\n",
    "Note: For the logs, click on **View logs** from the **Training Jobs** node in **Amazon SageMaker Console**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-24 11:28:23 Starting - Starting the training job......\n",
      "2022-09-24 11:29:08 Starting - Preparing the instances for training........................\n",
      "2022-09-24 11:33:34 Downloading - Downloading input data\n",
      "2022-09-24 11:33:34 Training - Downloading the training image..................\n",
      "2022-09-24 11:37:00 Training - Training image download completed. Training in progress..2022-09-24 11:37:05.792579: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2022-09-24 11:37:05.801314: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2022-09-24 11:37:06.269740: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2022-09-24 11:37:13,412 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "2022-09-24 11:37:14,075 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/usr/local/bin/python3.9 -m pip install -r requirements.txt\n",
      "Collecting protobuf==3.20.1\n",
      "Downloading protobuf-3.20.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.0/1.0 MB 39.6 MB/s eta 0:00:00\n",
      "Collecting tensorflow-addons==0.17.0\n",
      "Downloading tensorflow_addons-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.1/1.1 MB 51.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from tensorflow-addons==0.17.0->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.9/site-packages (from tensorflow-addons==0.17.0->-r requirements.txt (line 2)) (2.13.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging->tensorflow-addons==0.17.0->-r requirements.txt (line 2)) (3.0.9)\n",
      "Installing collected packages: protobuf, tensorflow-addons\n",
      "Attempting uninstall: protobuf\n",
      "Found existing installation: protobuf 3.19.4\n",
      "Uninstalling protobuf-3.19.4:\n",
      "Successfully uninstalled protobuf-3.19.4\n",
      "Attempting uninstall: tensorflow-addons\n",
      "Found existing installation: tensorflow-addons 0.17.1\n",
      "Uninstalling tensorflow-addons-0.17.1:\n",
      "Successfully uninstalled tensorflow-addons-0.17.1\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-models-official 2.9.1 requires tensorflow~=2.9.0, which is not installed.\n",
      "tensorflow-gpu 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
      "sagemaker-training 4.1.4.dev0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
      "Successfully installed protobuf-3.20.1 tensorflow-addons-0.17.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2022-09-24 11:37:24,079 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2022-09-24 11:37:24,079 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2022-09-24 11:37:24,258 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "2022-09-24 11:37:24,258 sagemaker-training-toolkit INFO     Creating SSH daemon.\n",
      "2022-09-24 11:37:24,274 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\n",
      "2022-09-24 11:37:24,274 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:8'] process_per_hosts: 8 num_processes: 8\n",
      "2022-09-24 11:37:24,276 sagemaker-training-toolkit INFO     Network interface name: eth0\n",
      "2022-09-24 11:37:24,368 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"--NCCL_DEBUG WARN\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 1024,\n",
      "        \"epochs\": 10,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"num_of_data_workers\": 0,\n",
      "        \"steps_per_epoch\": 500,\n",
      "        \"tf_data_mode\": \"local\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"homogeneous-20220924T112821Z-1\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-331113010199/homogeneous-20220924T112821Z-1/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"launcher.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"batch_size\":1024,\"epochs\":10,\"model_dir\":\"/opt/ml/model\",\"num_of_data_workers\":0,\"steps_per_epoch\":500,\"tf_data_mode\":\"local\"}\n",
      "SM_USER_ENTRY_POINT=launcher.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"--NCCL_DEBUG WARN\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=launcher\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=96\n",
      "SM_NUM_GPUS=8\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-331113010199/homogeneous-20220924T112821Z-1/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"--NCCL_DEBUG WARN\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":1024,\"epochs\":10,\"model_dir\":\"/opt/ml/model\",\"num_of_data_workers\":0,\"steps_per_epoch\":500,\"tf_data_mode\":\"local\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"job_name\":\"homogeneous-20220924T112821Z-1\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-331113010199/homogeneous-20220924T112821Z-1/source/sourcedir.tar.gz\",\"module_name\":\"launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"launcher.py\"}\n",
      "SM_USER_ARGS=[\"--batch_size\",\"1024\",\"--epochs\",\"10\",\"--model_dir\",\"/opt/ml/model\",\"--num_of_data_workers\",\"0\",\"--steps_per_epoch\",\"500\",\"--tf_data_mode\",\"local\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_HP_BATCH_SIZE=1024\n",
      "SM_HP_EPOCHS=10\n",
      "SM_HP_MODEL_DIR=/opt/ml/model\n",
      "SM_HP_NUM_OF_DATA_WORKERS=0\n",
      "SM_HP_STEPS_PER_EPOCH=500\n",
      "SM_HP_TF_DATA_MODE=local\n",
      "PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python39.zip:/usr/local/lib/python3.9:/usr/local/lib/python3.9/lib-dynload:/usr/local/lib/python3.9/site-packages:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg:/usr/local/lib/python3.9/site-packages/pyinstrument-3.4.2-py3.9.egg:/usr/local/lib/python3.9/site-packages/pyinstrument_cext-0.2.4-py3.9-linux-x86_64.egg\n",
      "Invoking script with the following command:\n",
      "mpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=WARN -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_HP_BATCH_SIZE -x SM_HP_EPOCHS -x SM_HP_MODEL_DIR -x SM_HP_NUM_OF_DATA_WORKERS -x SM_HP_STEPS_PER_EPOCH -x SM_HP_TF_DATA_MODE -x PYTHONPATH /usr/local/bin/python3.9 -m mpi4py launcher.py --batch_size 1024 --epochs 10 --model_dir /opt/ml/model --num_of_data_workers 0 --steps_per_epoch 500 --tf_data_mode local\n",
      "Data for JOB [7555,1] offset 0 Total slots allocated 8\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: ip-10-0-215-180#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [7555,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [7555,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [7555,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [7555,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [7555,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [7555,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [7555,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [7555,1] App: 0 Process rank: 7 Bound: N/A\n",
      " =============================================================\n",
      "[1,mpirank:1,algo-1]<stdout>:env.is_hetero=False\n",
      "[1,mpirank:1,algo-1]<stdout>:current_host=algo-1\n",
      "[1,mpirank:1,algo-1]<stdout>:Opening process: ['python', './train_dnn.py', '--batch_size', '1024', '--epochs', '10', '--model_dir', '/opt/ml/model', '--num_of_data_workers', '0', '--steps_per_epoch', '500', '--tf_data_mode', 'local']\n",
      "[1,mpirank:4,algo-1]<stdout>:env.is_hetero=False\n",
      "[1,mpirank:4,algo-1]<stdout>:current_host=algo-1\n",
      "[1,mpirank:4,algo-1]<stdout>:Opening process: ['python', './train_dnn.py', '--batch_size', '1024', '--epochs', '10', '--model_dir', '/opt/ml/model', '--num_of_data_workers', '0', '--steps_per_epoch', '500', '--tf_data_mode', 'local']\n",
      "[1,mpirank:5,algo-1]<stdout>:env.is_hetero=False\n",
      "[1,mpirank:5,algo-1]<stdout>:current_host=algo-1\n",
      "[1,mpirank:5,algo-1]<stdout>:Opening process: ['python', './train_dnn.py', '--batch_size', '1024', '--epochs', '10', '--model_dir', '/opt/ml/model', '--num_of_data_workers', '0', '--steps_per_epoch', '500', '--tf_data_mode', 'local']\n",
      "[1,mpirank:7,algo-1]<stdout>:env.is_hetero=False\n",
      "[1,mpirank:7,algo-1]<stdout>:current_host=algo-1\n",
      "[1,mpirank:7,algo-1]<stdout>:Opening process: ['python', './train_dnn.py', '--batch_size', '1024', '--epochs', '10', '--model_dir', '/opt/ml/model', '--num_of_data_workers', '0', '--steps_per_epoch', '500', '--tf_data_mode', 'local']\n",
      "[1,mpirank:0,algo-1]<stdout>:env.is_hetero=False\n",
      "[1,mpirank:0,algo-1]<stdout>:current_host=algo-1\n",
      "[1,mpirank:0,algo-1]<stdout>:Opening process: ['python', './train_dnn.py', '--batch_size', '1024', '--epochs', '10', '--model_dir', '/opt/ml/model', '--num_of_data_workers', '0', '--steps_per_epoch', '500', '--tf_data_mode', 'local']\n",
      "[1,mpirank:6,algo-1]<stdout>:env.is_hetero=False\n",
      "[1,mpirank:6,algo-1]<stdout>:current_host=algo-1\n",
      "[1,mpirank:6,algo-1]<stdout>:Opening process: ['python', './train_dnn.py', '--batch_size', '1024', '--epochs', '10', '--model_dir', '/opt/ml/model', '--num_of_data_workers', '0', '--steps_per_epoch', '500', '--tf_data_mode', 'local']\n",
      "[1,mpirank:3,algo-1]<stdout>:env.is_hetero=False\n",
      "[1,mpirank:3,algo-1]<stdout>:current_host=algo-1\n",
      "[1,mpirank:3,algo-1]<stdout>:Opening process: ['python', './train_dnn.py', '--batch_size', '1024', '--epochs', '10', '--model_dir', '/opt/ml/model', '--num_of_data_workers', '0', '--steps_per_epoch', '500', '--tf_data_mode', 'local']\n",
      "[1,mpirank:2,algo-1]<stdout>:env.is_hetero=False\n",
      "[1,mpirank:2,algo-1]<stdout>:current_host=algo-1[1,mpirank:2,algo-1]<stdout>:\n",
      "[1,mpirank:2,algo-1]<stdout>:Opening process: ['python', './train_dnn.py', '--batch_size', '1024', '--epochs', '10', '--model_dir', '/opt/ml/model', '--num_of_data_workers', '0', '--steps_per_epoch', '500', '--tf_data_mode', 'local']\n",
      "[1,mpirank:1,algo-1]<stderr>:2022-09-24 11:37:25.276381: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:2,algo-1]<stderr>:2022-09-24 11:37:25.276382: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:4,algo-1]<stderr>:2022-09-24 11:37:25.276384: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:1,algo-1]<stderr>:2022-09-24 11:37:25.276524: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "[1,mpirank:2,algo-1]<stderr>:2022-09-24 11:37:25.276524: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "[1,mpirank:4,algo-1]<stderr>:2022-09-24 11:37:25.276524: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "[1,mpirank:0,algo-1]<stderr>:2022-09-24 11:37:25.290991: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:7,algo-1]<stderr>:2022-09-24 11:37:25.290987: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:3,algo-1]<stderr>:2022-09-24 11:37:25.290990: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:5,algo-1]<stderr>:2022-09-24 11:37:25.290991: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:6,algo-1]<stderr>:2022-09-24 11:37:25.290990: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:0,algo-1]<stderr>:2022-09-24 11:37:25.291121: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "[1,mpirank:7,algo-1]<stderr>:2022-09-24 11:37:25.291122: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "[1,mpirank:3,algo-1]<stderr>:2022-09-24 11:37:25.291124: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "[1,mpirank:5,algo-1]<stderr>:2022-09-24 11:37:25.291124: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "[1,mpirank:6,algo-1]<stderr>:2022-09-24 11:37:25.291121: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "[1,mpirank:4,algo-1]<stderr>:2022-09-24 11:37:25.310966: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:2,algo-1]<stderr>:2022-09-24 11:37:25.310966: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:1,algo-1]<stderr>:2022-09-24 11:37:25.310966: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:0,algo-1]<stderr>:2022-09-24 11:37:25.325878: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:7,algo-1]<stderr>:2022-09-24 11:37:25.325873: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:3,algo-1]<stderr>:2022-09-24 11:37:25.325878: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:6,algo-1]<stderr>:2022-09-24 11:37:25.326012: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:5,algo-1]<stderr>:2022-09-24 11:37:25.326064: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "[1,mpirank:6,algo-1]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]\n",
      "[1,mpirank:0,algo-1]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]\n",
      "[1,mpirank:0,algo-1]<stdout>:hvd.local_rank() 0\n",
      "[1,mpirank:1,algo-1]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]\n",
      "[1,mpirank:6,algo-1]<stdout>:hvd.local_rank() 6\n",
      "[1,mpirank:1,algo-1]<stdout>:hvd.local_rank() 1\n",
      "[1,mpirank:5,algo-1]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')][1,mpirank:5,algo-1]<stdout>:\n",
      "[1,mpirank:5,algo-1]<stdout>:hvd.local_rank() 5\n",
      "[1,mpirank:2,algo-1]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]\n",
      "[1,mpirank:3,algo-1]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]\n",
      "[1,mpirank:4,algo-1]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]\n",
      "[1,mpirank:2,algo-1]<stdout>:hvd.local_rank() 2\n",
      "[1,mpirank:7,algo-1]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]\n",
      "[1,mpirank:3,algo-1]<stdout>:hvd.local_rank() 3\n",
      "[1,mpirank:4,algo-1]<stdout>:hvd.local_rank() 4\n",
      "[1,mpirank:7,algo-1]<stdout>:hvd.local_rank() 7\n",
      "[1,mpirank:3,algo-1]<stdout>:Running in local tf_data_mode.\n",
      "[1,mpirank:6,algo-1]<stdout>:Running in local tf_data_mode.\n",
      "[1,mpirank:5,algo-1]<stdout>:Running in local tf_data_mode.\n",
      "[1,mpirank:0,algo-1]<stdout>:Running in local tf_data_mode.\n",
      "[1,mpirank:7,algo-1]<stdout>:Running in local tf_data_mode.\n",
      "[1,mpirank:2,algo-1]<stdout>:Running in local tf_data_mode.\n",
      "[1,mpirank:1,algo-1]<stdout>:Running in local tf_data_mode.\n",
      "[1,mpirank:4,algo-1]<stdout>:Running in local tf_data_mode.\n",
      "[1,mpirank:1,algo-1]<stdout>:Epoch 1/10\n",
      "[1,mpirank:4,algo-1]<stdout>:Epoch 1/10\n",
      "[1,mpirank:2,algo-1]<stdout>:Epoch 1/10\n",
      "[1,mpirank:3,algo-1]<stdout>:Epoch 1/10\n",
      "[1,mpirank:5,algo-1]<stdout>:Epoch 1/10\n",
      "[1,mpirank:7,algo-1]<stdout>:Epoch 1/10\n",
      "[1,mpirank:0,algo-1]<stdout>:Epoch 1/10\n",
      "[1,mpirank:6,algo-1]<stdout>:Epoch 1/10\n",
      "[1,mpirank:5,algo-1]<stdout>:Extension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\n",
      "[1,mpirank:5,algo-1]<stdout>:If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "[1,mpirank:5,algo-1]<stdout>:Warning! MPI libs are missing, but python applications are still available.\n",
      "[1,mpirank:0,algo-1]<stdout>:Extension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\n",
      "[1,mpirank:0,algo-1]<stdout>:If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "[1,mpirank:0,algo-1]<stdout>:Warning! MPI libs are missing, but python applications are still available.\n",
      "[1,mpirank:7,algo-1]<stdout>:Extension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\n",
      "[1,mpirank:7,algo-1]<stdout>:If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "[1,mpirank:7,algo-1]<stdout>:Warning! MPI libs are missing, but python applications are still available.\n",
      "[1,mpirank:3,algo-1]<stdout>:Extension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\n",
      "[1,mpirank:3,algo-1]<stdout>:If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "[1,mpirank:3,algo-1]<stdout>:Warning! MPI libs are missing, but python applications are still available.\n",
      "[1,mpirank:1,algo-1]<stdout>:Extension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\n",
      "[1,mpirank:1,algo-1]<stdout>:If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "[1,mpirank:1,algo-1]<stdout>:Warning! MPI libs are missing, but python applications are still available.\n",
      "[1,mpirank:2,algo-1]<stdout>:Extension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\n",
      "[1,mpirank:2,algo-1]<stdout>:If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "[1,mpirank:2,algo-1]<stdout>:Warning! MPI libs are missing, but python applications are still available.\n",
      "[1,mpirank:4,algo-1]<stdout>:Extension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\n",
      "[1,mpirank:4,algo-1]<stdout>:If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "[1,mpirank:4,algo-1]<stdout>:Warning! MPI libs are missing, but python applications are still available.\n",
      "[1,mpirank:6,algo-1]<stdout>:Extension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\n",
      "[1,mpirank:6,algo-1]<stdout>:If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "[1,mpirank:6,algo-1]<stdout>:Warning! MPI libs are missing, but python applications are still available.\n",
      "[1,mpirank:1,algo-1]<stdout>:[2022-09-24 11:37:33.366 algo-1:177 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[1,mpirank:4,algo-1]<stdout>:[2022-09-24 11:37:33.366 algo-1:178 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[1,mpirank:5,algo-1]<stdout>:[2022-09-24 11:37:33.366 algo-1:179 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-09-24 11:37:33.366 algo-1:181 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[1,mpirank:3,algo-1]<stdout>:[2022-09-24 11:37:33.366 algo-1:183 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[1,mpirank:2,algo-1]<stdout>:[2022-09-24 11:37:33.366 algo-1:184 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[1,mpirank:6,algo-1]<stdout>:[2022-09-24 11:37:33.366 algo-1:182 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[1,mpirank:7,algo-1]<stdout>:[2022-09-24 11:37:33.366 algo-1:180 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[1,mpirank:1,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:2,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:4,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:5,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:6,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:0,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:3,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:1,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:2,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:5,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:4,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:6,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:7,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:0,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:3,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:7,algo-1]<stderr>:/usr/local/lib/python3.9/site-packages/smdebug-1.0.17b20220701-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "[1,mpirank:1,algo-1]<stdout>:[2022-09-24 11:37:33.581 algo-1:177 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[1,mpirank:2,algo-1]<stdout>:[2022-09-24 11:37:33.581 algo-1:184 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-09-24 11:37:33.582 algo-1:181 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[1,mpirank:5,algo-1]<stdout>:[2022-09-24 11:37:33.582 algo-1:179 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[1,mpirank:7,algo-1]<stdout>:[2022-09-24 11:37:33.582 algo-1:180 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[1,mpirank:6,algo-1]<stdout>:[2022-09-24 11:37:33.582 algo-1:182 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[1,mpirank:3,algo-1]<stdout>:[2022-09-24 11:37:33.582 algo-1:183 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[1,mpirank:4,algo-1]<stdout>:[2022-09-24 11:37:33.582 algo-1:178 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[1,mpirank:2,algo-1]<stdout>:[2022-09-24 11:37:33.639 algo-1:184 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[1,mpirank:1,algo-1]<stdout>:[2022-09-24 11:37:33.639 algo-1:177 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[1,mpirank:4,algo-1]<stdout>:[2022-09-24 11:37:33.639 algo-1:178 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[1,mpirank:3,algo-1]<stdout>:[2022-09-24 11:37:33.639 algo-1:183 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[1,mpirank:7,algo-1]<stdout>:[2022-09-24 11:37:33.639 algo-1:180 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[1,mpirank:6,algo-1]<stdout>:[2022-09-24 11:37:33.639 algo-1:182 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-09-24 11:37:33.639 algo-1:181 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[1,mpirank:5,algo-1]<stdout>:[2022-09-24 11:37:33.639 algo-1:179 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[1,mpirank:1,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:177 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[1,mpirank:4,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:178 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[1,mpirank:2,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:184 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[1,mpirank:3,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:183 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[1,mpirank:7,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:180 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[1,mpirank:6,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:182 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:181 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[1,mpirank:2,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:184 INFO hook.py:254] Saving to /opt/ml/output/tensors\n",
      "[1,mpirank:1,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:177 INFO hook.py:254] Saving to /opt/ml/output/tensors\n",
      "[1,mpirank:4,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:178 INFO hook.py:254] Saving to /opt/ml/output/tensors\n",
      "[1,mpirank:2,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:184 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[1,mpirank:4,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:178 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[1,mpirank:1,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:177 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[1,mpirank:5,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:179 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[1,mpirank:3,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:183 INFO hook.py:254] Saving to /opt/ml/output/tensors\n",
      "[1,mpirank:3,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:183 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[1,mpirank:2,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:184 INFO hook.py:421] Monitoring the collections: losses, sm_metrics, metrics\n",
      "[1,mpirank:7,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:180 INFO hook.py:254] Saving to /opt/ml/output/tensors\n",
      "[1,mpirank:6,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:182 INFO hook.py:254] Saving to /opt/ml/output/tensors\n",
      "[1,mpirank:1,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:177 INFO hook.py:421] Monitoring the collections: losses, metrics, sm_metrics\n",
      "[1,mpirank:4,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:178 INFO hook.py:421] Monitoring the collections: losses, metrics, sm_metrics\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:181 INFO hook.py:254] Saving to /opt/ml/output/tensors\n",
      "[1,mpirank:7,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:180 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[1,mpirank:6,algo-1]<stdout>:[2022-09-24 11:37:33.640 algo-1:182 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-09-24 11:37:33.641 algo-1:181 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[1,mpirank:3,algo-1]<stdout>:[2022-09-24 11:37:33.641 algo-1:183 INFO hook.py:421] Monitoring the collections: losses, sm_metrics, metrics\n",
      "[1,mpirank:6,algo-1]<stdout>:[2022-09-24 11:37:33.641 algo-1:182 INFO hook.py:421] Monitoring the collections: sm_metrics, metrics, losses\n",
      "[1,mpirank:7,algo-1]<stdout>:[2022-09-24 11:37:33.641 algo-1:180 INFO hook.py:421] Monitoring the collections: sm_metrics, losses, metrics\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-09-24 11:37:33.641 algo-1:181 INFO hook.py:421] Monitoring the collections: sm_metrics, metrics, losses\n",
      "[1,mpirank:5,algo-1]<stdout>:[2022-09-24 11:37:33.641 algo-1:179 INFO hook.py:254] Saving to /opt/ml/output/tensors\n",
      "[1,mpirank:5,algo-1]<stdout>:[2022-09-24 11:37:33.641 algo-1:179 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[1,mpirank:5,algo-1]<stdout>:[2022-09-24 11:37:33.641 algo-1:179 INFO hook.py:421] Monitoring the collections: metrics, losses, sm_metrics\n",
      "[1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.2\n",
      "[1,mpirank:2,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2246s vs `on_train_batch_end` time: 0.6465s). Check your callbacks.\n",
      "[1,mpirank:2,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2246s vs `on_train_batch_end` time: 0.6465s). Check your callbacks.\n",
      "[1,mpirank:3,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2247s vs `on_train_batch_end` time: 0.6464s). Check your callbacks.\n",
      "[1,mpirank:3,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2247s vs `on_train_batch_end` time: 0.6464s). Check your callbacks.\n",
      "[1,mpirank:0,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2237s vs `on_train_batch_end` time: 0.6464s). Check your callbacks.\n",
      "[1,mpirank:0,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2237s vs `on_train_batch_end` time: 0.6464s). Check your callbacks.\n",
      "[1,mpirank:5,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2247s vs `on_train_batch_end` time: 0.6464s). Check your callbacks.\n",
      "[1,mpirank:5,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2247s vs `on_train_batch_end` time: 0.6464s). Check your callbacks.\n",
      "[1,mpirank:7,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2241s vs `on_train_batch_end` time: 0.6464s). Check your callbacks.\n",
      "[1,mpirank:6,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2247s vs `on_train_batch_end` time: 0.6465s). Check your callbacks.\n",
      "[1,mpirank:7,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2241s vs `on_train_batch_end` time: 0.6464s). Check your callbacks.\n",
      "[1,mpirank:6,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2247s vs `on_train_batch_end` time: 0.6465s). Check your callbacks.\n",
      "[1,mpirank:1,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2255s vs `on_train_batch_end` time: 0.6463s). Check your callbacks.\n",
      "[1,mpirank:1,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2255s vs `on_train_batch_end` time: 0.6463s). Check your callbacks.\n",
      "[1,mpirank:4,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2250s vs `on_train_batch_end` time: 0.6464s). Check your callbacks.\n",
      "[1,mpirank:4,algo-1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2250s vs `on_train_batch_end` time: 0.6464s). Check your callbacks.\n",
      "[1,mpirank:7,algo-1]<stdout>:500/500 - 121s - loss: 2.4081 - lr: 0.0033 - 121s/epoch - 242ms/step\n",
      "[1,mpirank:1,algo-1]<stdout>:500/500 - 121s - loss: 2.4081 - lr: 0.0033 - 121s/epoch - 242ms/step\n",
      "[1,mpirank:2,algo-1]<stdout>:500/500 - 121s - loss: 2.4081 - lr: 0.0033 - 121s/epoch - 242ms/step\n",
      "[1,mpirank:4,algo-1]<stdout>:500/500 - 121s - loss: 2.4081 - lr: 0.0033 - 121s/epoch - 242ms/step\n",
      "[1,mpirank:6,algo-1]<stdout>:500/500 - 121s - loss: 2.4081 - lr: 0.0033 - 121s/epoch - 242ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:500/500 - 121s - loss: 2.4081 - lr: 0.0033 - 121s/epoch - 242ms/step\n",
      "[1,mpirank:1,algo-1]<stdout>:Epoch 2/10\n",
      "[1,mpirank:5,algo-1]<stdout>:Epoch 2/10\n",
      "[1,mpirank:7,algo-1]<stdout>:Epoch 2/10\n",
      "[1,mpirank:6,algo-1]<stdout>:Epoch 2/10\n",
      "[1,mpirank:2,algo-1]<stdout>:Epoch 2/10\n",
      "[1,mpirank:4,algo-1]<stdout>:Epoch 2/10\n",
      "[1,mpirank:3,algo-1]<stdout>:500/500 - 121s - loss: 2.4081 - lr: 0.0033 - 121s/epoch - 242ms/step\n",
      "[1,mpirank:3,algo-1]<stdout>:Epoch 2/10\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 122s - loss: 2.4081 - lr: 0.0033 - 122s/epoch - 245ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:Epoch 2/10\n",
      "[1,mpirank:1,algo-1]<stdout>:500/500 - 100s - loss: 2.3881 - lr: 0.0057 - 100s/epoch - 199ms/step\n",
      "[1,mpirank:4,algo-1]<stdout>:500/500 - 100s - loss: 2.3881 - lr: 0.0057 - 100s/epoch - 199ms/step\n",
      "[1,mpirank:7,algo-1]<stdout>:500/500 - 100s - loss: 2.3881 - lr: 0.0057 - 100s/epoch - 199ms/step\n",
      "[1,mpirank:6,algo-1]<stdout>:500/500 - 100s - loss: 2.3881 - lr: 0.0057 - 100s/epoch - 199ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:500/500 - 100s - loss: 2.3881 - lr: 0.0057 - 100s/epoch - 199ms/step\n",
      "[1,mpirank:3,algo-1]<stdout>:500/500 - 100s - loss: 2.3881 - lr: 0.0057 - 100s/epoch - 199ms/step\n",
      "[1,mpirank:2,algo-1]<stdout>:500/500 - 100s - loss: 2.3881 - lr: 0.0057 - 100s/epoch - 199ms/step\n",
      "[1,mpirank:7,algo-1]<stdout>:Epoch 3/10\n",
      "[1,mpirank:5,algo-1]<stdout>:Epoch 3/10\n",
      "[1,mpirank:2,algo-1]<stdout>:Epoch 3/10\n",
      "[1,mpirank:6,algo-1]<stdout>:Epoch 3/10\n",
      "[1,mpirank:4,algo-1]<stdout>:Epoch 3/10\n",
      "[1,mpirank:3,algo-1]<stdout>:Epoch 3/10\n",
      "[1,mpirank:1,algo-1]<stdout>:Epoch 3/10\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 99s - loss: 2.3881 - lr: 0.0057 - 99s/epoch - 199ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:Epoch 3/10\n",
      "[1,mpirank:6,algo-1]<stdout>:500/500 - 103s - loss: 2.3532 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:\n",
      "[1,mpirank:0,algo-1]<stdout>:Epoch 3: finished gradual learning rate warmup to 0.008.\n",
      "[1,mpirank:7,algo-1]<stdout>:500/500 - 103s - loss: 2.3532 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:500/500 - 103s - loss: 2.3532 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:2,algo-1]<stdout>:500/500 - 103s - loss: 2.3532 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:1,algo-1]<stdout>:500/500 - 103s - loss: 2.3532 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:3,algo-1]<stdout>:500/500 - 103s - loss: 2.3532 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:6,algo-1]<stdout>:Epoch 4/10\n",
      "[1,mpirank:5,algo-1]<stdout>:Epoch 4/10\n",
      "[1,mpirank:2,algo-1]<stdout>:Epoch 4/10\n",
      "[1,mpirank:7,algo-1]<stdout>:Epoch 4/10\n",
      "[1,mpirank:1,algo-1]<stdout>:Epoch 4/10\n",
      "[1,mpirank:3,algo-1]<stdout>:Epoch 4/10\n",
      "[1,mpirank:4,algo-1]<stdout>:500/500 - 103s - loss: 2.3532 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:4,algo-1]<stdout>:Epoch 4/10\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 103s - loss: 2.3532 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:Epoch 4/10\n",
      "[1,mpirank:7,algo-1]<stdout>:500/500 - 103s - loss: 2.3199 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:6,algo-1]<stdout>:500/500 - 103s - loss: 2.3199 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:4,algo-1]<stdout>:500/500 - 103s - loss: 2.3199 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:2,algo-1]<stdout>:500/500 - 103s - loss: 2.3199 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:500/500 - 103s - loss: 2.3199 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:1,algo-1]<stdout>:500/500 - 103s - loss: 2.3199 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:3,algo-1]<stdout>:500/500 - 103s - loss: 2.3199 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:6,algo-1]<stdout>:Epoch 5/10\n",
      "[1,mpirank:7,algo-1]<stdout>:Epoch 5/10\n",
      "[1,mpirank:4,algo-1]<stdout>:Epoch 5/10\n",
      "[1,mpirank:1,algo-1]<stdout>:Epoch 5/10\n",
      "[1,mpirank:2,algo-1]<stdout>:Epoch 5/10\n",
      "[1,mpirank:3,algo-1]<stdout>:Epoch 5/10\n",
      "[1,mpirank:5,algo-1]<stdout>:Epoch 5/10\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 103s - loss: 2.3199 - lr: 0.0080 - 103s/epoch - 206ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:Epoch 5/10\n",
      "[1,mpirank:2,algo-1]<stdout>:500/500 - 100s - loss: 2.3071 - lr: 0.0080 - 100s/epoch - 200ms/step\n",
      "[1,mpirank:6,algo-1]<stdout>:500/500 - 100s - loss: 2.3071 - lr: 0.0080 - 100s/epoch - 200ms/step\n",
      "[1,mpirank:2,algo-1]<stdout>:Epoch 6/10\n",
      "[1,mpirank:7,algo-1]<stdout>:500/500 - 100s - loss: 2.3071 - lr: 0.0080 - 100s/epoch - 200ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:500/500 - 100s - loss: 2.3071 - lr: 0.0080 - 100s/epoch - 200ms/step\n",
      "[1,mpirank:1,algo-1]<stdout>:500/500 - 100s - loss: 2.3071 - lr: 0.0080 - 100s/epoch - 200ms/step\n",
      "[1,mpirank:6,algo-1]<stdout>:Epoch 6/10\n",
      "[1,mpirank:3,algo-1]<stdout>:500/500 - 100s - loss: 2.3071 - lr: 0.0080 - 100s/epoch - 200ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:Epoch 6/10\n",
      "[1,mpirank:7,algo-1]<stdout>:Epoch 6/10\n",
      "[1,mpirank:1,algo-1]<stdout>:Epoch 6/10\n",
      "[1,mpirank:3,algo-1]<stdout>:Epoch 6/10\n",
      "[1,mpirank:4,algo-1]<stdout>:500/500 - 100s - loss: 2.3071 - lr: 0.0080 - 100s/epoch - 200ms/step\n",
      "[1,mpirank:4,algo-1]<stdout>:Epoch 6/10\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 100s - loss: 2.3071 - lr: 0.0080 - 100s/epoch - 200ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:Epoch 6/10\n",
      "[1,mpirank:7,algo-1]<stdout>:500/500 - 94s - loss: 2.3043 - lr: 0.0080 - 94s/epoch - 188ms/step\n",
      "[1,mpirank:4,algo-1]<stdout>:500/500 - 94s - loss: 2.3043 - lr: 0.0080 - 94s/epoch - 188ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:500/500 - 94s - loss: 2.3043 - lr: 0.0080 - 94s/epoch - 188ms/step\n",
      "[1,mpirank:2,algo-1]<stdout>:500/500 - 94s - loss: 2.3043 - lr: 0.0080 - 94s/epoch - 188ms/step\n",
      "[1,mpirank:3,algo-1]<stdout>:500/500 - 94s - loss: 2.3043 - lr: 0.0080 - 94s/epoch - 188ms/step\n",
      "[1,mpirank:4,algo-1]<stdout>:Epoch 7/10\n",
      "[1,mpirank:5,algo-1]<stdout>:Epoch 7/10\n",
      "[1,mpirank:3,algo-1]<stdout>:Epoch 7/10\n",
      "[1,mpirank:7,algo-1]<stdout>:Epoch 7/10\n",
      "[1,mpirank:1,algo-1]<stdout>:500/500 - 94s - loss: 2.3043 - lr: 0.0080 - 94s/epoch - 188ms/step\n",
      "[1,mpirank:2,algo-1]<stdout>:Epoch 7/10\n",
      "[1,mpirank:1,algo-1]<stdout>:Epoch 7/10\n",
      "[1,mpirank:6,algo-1]<stdout>:500/500 - 94s - loss: 2.3043 - lr: 0.0080 - 94s/epoch - 188ms/step\n",
      "[1,mpirank:6,algo-1]<stdout>:Epoch 7/10\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 94s - loss: 2.3043 - lr: 0.0080 - 94s/epoch - 189ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:Epoch 7/10\n",
      "[1,mpirank:3,algo-1]<stdout>:500/500 - 97s - loss: 2.3031 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:500/500 - 97s - loss: 2.3031 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:2,algo-1]<stdout>:500/500 - 97s - loss: 2.3031 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:1,algo-1]<stdout>:500/500 - 97s - loss: 2.3031 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:3,algo-1]<stdout>:Epoch 8/10\n",
      "[1,mpirank:5,algo-1]<stdout>:Epoch 8/10\n",
      "[1,mpirank:2,algo-1]<stdout>:Epoch 8/10\n",
      "[1,mpirank:6,algo-1]<stdout>:500/500 - 97s - loss: 2.3031 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:7,algo-1]<stdout>:500/500 - 97s - loss: 2.3031 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:6,algo-1]<stdout>:Epoch 8/10\n",
      "[1,mpirank:1,algo-1]<stdout>:Epoch 8/10\n",
      "[1,mpirank:7,algo-1]<stdout>:Epoch 8/10\n",
      "[1,mpirank:4,algo-1]<stdout>:500/500 - 97s - loss: 2.3031 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:4,algo-1]<stdout>:Epoch 8/10\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 97s - loss: 2.3031 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:Epoch 8/10\n",
      "[1,mpirank:3,algo-1]<stdout>:500/500 - 96s - loss: 2.3027 - lr: 0.0080 - 96s/epoch - 192ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:500/500 - 96s - loss: 2.3027 - lr: 0.0080 - 96s/epoch - 192ms/step\n",
      "[1,mpirank:4,algo-1]<stdout>:500/500 - 96s - loss: 2.3027 - lr: 0.0080 - 96s/epoch - 192ms/step\n",
      "[1,mpirank:7,algo-1]<stdout>:500/500 - 96s - loss: 2.3027 - lr: 0.0080 - 96s/epoch - 192ms/step\n",
      "[1,mpirank:2,algo-1]<stdout>:500/500 - 96s - loss: 2.3027 - lr: 0.0080 - 96s/epoch - 192ms/step\n",
      "[1,mpirank:6,algo-1]<stdout>:500/500 - 96s - loss: 2.3027 - lr: 0.0080 - 96s/epoch - 192ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:Epoch 9/10\n",
      "[1,mpirank:4,algo-1]<stdout>:Epoch 9/10\n",
      "[1,mpirank:7,algo-1]<stdout>:Epoch 9/10\n",
      "[1,mpirank:3,algo-1]<stdout>:Epoch 9/10\n",
      "[1,mpirank:2,algo-1]<stdout>:Epoch 9/10\n",
      "[1,mpirank:6,algo-1]<stdout>:Epoch 9/10\n",
      "[1,mpirank:1,algo-1]<stdout>:500/500 - 96s - loss: 2.3027 - lr: 0.0080 - 96s/epoch - 192ms/step\n",
      "[1,mpirank:1,algo-1]<stdout>:Epoch 9/10\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 96s - loss: 2.3027 - lr: 0.0080 - 96s/epoch - 192ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:Epoch 9/10\n",
      "[1,mpirank:2,algo-1]<stdout>:500/500 - 105s - loss: 2.3021 - lr: 0.0080 - 105s/epoch - 210ms/step\n",
      "[1,mpirank:3,algo-1]<stdout>:500/500 - 105s - loss: 2.3021 - lr: 0.0080 - 105s/epoch - 210ms/step\n",
      "[1,mpirank:1,algo-1]<stdout>:500/500 - 105s - loss: 2.3021 - lr: 0.0080 - 105s/epoch - 210ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:500/500 - 105s - loss: 2.3021 - lr: 0.0080 - 105s/epoch - 210ms/step\n",
      "[1,mpirank:2,algo-1]<stdout>:Epoch 10/10\n",
      "[1,mpirank:1,algo-1]<stdout>:Epoch 10/10\n",
      "[1,mpirank:4,algo-1]<stdout>:500/500 - 105s - loss: 2.3021 - lr: 0.0080 - 105s/epoch - 210ms/step\n",
      "[1,mpirank:6,algo-1]<stdout>:500/500 - 105s - loss: 2.3021 - lr: 0.0080 - 105s/epoch - 210ms/step\n",
      "[1,mpirank:3,algo-1]<stdout>:Epoch 10/10\n",
      "[1,mpirank:5,algo-1]<stdout>:Epoch 10/10\n",
      "[1,mpirank:6,algo-1]<stdout>:Epoch 10/10\n",
      "[1,mpirank:4,algo-1]<stdout>:Epoch 10/10\n",
      "[1,mpirank:7,algo-1]<stdout>:500/500 - 105s - loss: 2.3021 - lr: 0.0080 - 105s/epoch - 210ms/step\n",
      "[1,mpirank:7,algo-1]<stdout>:Epoch 10/10\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 105s - loss: 2.3021 - lr: 0.0080 - 105s/epoch - 209ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:Epoch 10/10\n",
      "[1,mpirank:6,algo-1]<stdout>:500/500 - 97s - loss: 2.3013 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:7,algo-1]<stdout>:500/500 - 97s - loss: 2.3013 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:3,algo-1]<stdout>:500/500 - 97s - loss: 2.3013 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:2,algo-1]<stdout>:500/500 - 97s - loss: 2.3013 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:5,algo-1]<stdout>:500/500 - 97s - loss: 2.3013 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:1,algo-1]<stdout>:500/500 - 97s - loss: 2.3013 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:4,algo-1]<stdout>:500/500 - 97s - loss: 2.3013 - lr: 0.0080 - 97s/epoch - 194ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 97s - loss: 2.3013 - lr: 0.0080 - 97s/epoch - 193ms/step\n",
      "[1,mpirank:4,algo-1]<stdout>:Process train_dnn.py closed with returncode=0\n",
      "[1,mpirank:6,algo-1]<stdout>:Process train_dnn.py closed with returncode=0\n",
      "[1,mpirank:3,algo-1]<stdout>:Process train_dnn.py closed with returncode=0\n",
      "[1,mpirank:1,algo-1]<stdout>:Process train_dnn.py closed with returncode=0\n",
      "[1,mpirank:5,algo-1]<stdout>:Process train_dnn.py closed with returncode=0\n",
      "[1,mpirank:7,algo-1]<stdout>:Process train_dnn.py closed with returncode=0\n",
      "[1,mpirank:2,algo-1]<stdout>:Process train_dnn.py closed with returncode=0\n",
      "[1,mpirank:0,algo-1]<stderr>:WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/000000001/assets\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/000000001/assets\n",
      "[1,mpirank:0,algo-1]<stdout>:Process train_dnn.py closed with returncode=0\n",
      "2022-09-24 11:54:50,061 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2022-09-24 11:54:50,061 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2022-09-24 11:54:50,062 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2022-09-24 11:55:05 Uploading - Uploading generated training model\n",
      "2022-09-24 11:55:36 Completed - Training job completed\n",
      "Training seconds: 1337\n",
      "Billable seconds: 1337\n"
     ]
    }
   ],
   "source": [
    "from start_job_utils import fit_with_retries\n",
    "fit_with_retries(5, estimator, \n",
    "    job_name=\"homogeneous-\" + datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Analyzing the homogeneous training job throughput and resource usage\n",
    "We'll examine: CPU and GPU usage. Epoch time and step time\n",
    "\n",
    "**CPU and GPU usage analysis** \n",
    "\n",
    "In the screenshot below we observe that close to all the 96 vCPU of the instance is utilized. While GPU utilization is only ~45%. Clearly if we had more vCPUs we could increase GPU usage significantly to increase job throughput\n",
    "\n",
    "Note: To view your own job Click on **View instance metrics** from the **Training jobs** in **Amazon SageMaker Console**. Then to rescale the CloudWatch Metrics to 100% on CPU utilization for algo-1 and algo-2, use CloudWatch \"Add Math\" feature and average it out by no. of vCPUs/GPUs on those instance types. We captured metrics definitions used to produce this graph [here](./cloudwatch-metric-definitions/homogenous-workload%20copy.json).  \n",
    "<img src=\"images/metrics homogeneous cpu and gpu usage.png\" width=75%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epoch time and step time analysis**\n",
    "\n",
    "For 2nd and 3rd epochs the below should print out: 105s/epoch - 209ms/step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture homogeneous_logs\n",
    "estimator.sagemaker_session.logs_for_job(estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing step time for epochs and steps for homogeneous-20220923T231801Z\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 117s - loss: 2.4153 - lr: 0.0033 - 117s/epoch - 234ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 92s - loss: 2.3755 - lr: 0.0057 - 92s/epoch - 184ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 92s - loss: 2.3472 - lr: 0.0080 - 92s/epoch - 184ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 92s - loss: 2.3175 - lr: 0.0080 - 92s/epoch - 184ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 92s - loss: 2.3066 - lr: 0.0080 - 92s/epoch - 183ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 90s - loss: 2.3043 - lr: 0.0080 - 90s/epoch - 181ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 94s - loss: 2.3028 - lr: 0.0080 - 94s/epoch - 189ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 92s - loss: 2.3024 - lr: 0.0080 - 92s/epoch - 184ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 93s - loss: 2.3021 - lr: 0.0080 - 93s/epoch - 185ms/step\n",
      "[1,mpirank:0,algo-1]<stdout>:500/500 - 89s - loss: 2.3018 - lr: 0.0080 - 89s/epoch - 177ms/step\n"
     ]
    }
   ],
   "source": [
    "print(f\"Printing step time for epochs and steps for {estimator.latest_training_job.name}\")\n",
    "for line in homogeneous_logs.stdout.split(\"\\n\"):\n",
    "    if \"mpirank:0\" in line and \"/epoch\" in line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Run a heterogeneous cluster training job\n",
    "\n",
    "#### Step 1: Set up training environment\n",
    "We'll now run a training job in heterogeneous cluster mode.  \n",
    "Note the changes from the homogeneous cluster job:  \n",
    "- We define two new instance groups that are provided to the `estimator` as the `instance_groups` parameter that replaces the homogeneous parameters `instance_type` and `instance_count`.\n",
    "- In the `distribution` parameter for Horovod we added a new parameter `instance_groups` that is used to limit the MPI cluster to run in the `dnn_group`. The MPI cluster should include only the GPU nodes that run Horovod (which needs MPI). The `data_group` instances should not be part of the MPI cluster, as they set up their on `tf.data.service` cluster.\n",
    "\n",
    "More on the two instance groups config we use:\n",
    "- `data_group` - two ml.c5.18xlarge instances, each with 72 vCPUs to handle data preprocessing. Reading data from S3, preprocessing it, and forwarding it to the `dnn_group`.\n",
    "- `dnn_group` - a single p4d.24xlarge instance, with 8 GPUs and 96 vCPUs to handle deep neural network optimization (forward backward passes). To fully utilize 96 vCPUs in the `dnn_group`, we'll be starting data workers on all the instances in both groups, therefore we have 240 vCPUs (96+72+72) in total available for preprocessing (minus vCPUs used for the neural network optimization process).\n",
    "\n",
    "There are three Python scripts to know about:\n",
    "The 1st is `train_dnn.py` - This is your training script for the neural network, you should edit it to match your own use case. Note that this script isn't aware of the Heterogeneous cluster set up, except when it initializes the tf.data dataset calling this line: `ds = ds.apply(tf.data.experimental.service.distribute(...)`.  \n",
    "The 2nd and 3rd scripts, which should not need editing when adapting to your own use case, do the heavy lifting required for using tf.data.service over the Heterogeneous cluster feature.  \n",
    "`train_data.py` include functions to start/stop tf.service.data process like a dispatcher and WorkerServer. \n",
    "`launcher.py` has several responsibilities: \n",
    "- A single entry point script for all instances in all instance groups (SageMaker will start the same script on all instances).\n",
    "- Identifies which instance group the node belong to, and start the relevant script accordingly (`train_dnn.py` or `train_data.py` or sometimes both).\n",
    "- Takes measures to ensure that tf.data.service processes shutdown when training completes, as the training job completes only when all instances exit.\n",
    "- Allow to start more than one process (for example, on the dnn_group instances we'll run both the `train_dnn.py` and a tf.data.service worker to utilize the instance CPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import os\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": 10,\n",
    "    \"steps_per_epoch\": 500,\n",
    "    \"batch_size\": 1024,\n",
    "    \"tf_data_mode\": \"service\",  # Using tf.data.service for this Heterogeneous cluster job\n",
    "    \"num_of_data_workers\": 1,  # One tf.data.service worker per node\n",
    "}\n",
    "\n",
    "# Group for CPU instances to run tf.data.service dispatcher/workers processes.\n",
    "data_group = InstanceGroup(\"data_group\", \"ml.c5.18xlarge\", 2)\n",
    "# Group for deep neural network (dnn) with accleartors (e.g., GPU, FPGA, etc.)\n",
    "dnn_group = InstanceGroup(\"dnn_group\", \"ml.p4d.24xlarge\", 1)\n",
    "\n",
    "estimator2 = TensorFlow(\n",
    "    entry_point=\"launcher.py\",\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"2.9.1\",\n",
    "    py_version=\"py39\",\n",
    "    role=role,\n",
    "    volume_size=10,\n",
    "    max_run=1800,  # 30 minutes\n",
    "    disable_profiler=True,\n",
    "    # instance_type='ml.p4d.24xlarge',\n",
    "    # instance_count=1,\n",
    "    instance_groups=[data_group, dnn_group],\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": 8,  # p4d.24xlarge has 8 GPUs per host\n",
    "            \"custom_mpi_options\": \"--NCCL_DEBUG WARN\",\n",
    "        },\n",
    "        \"instance_groups\": [dnn_group],  # Apply distribution strategy to the dnn_group only\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Submit the training job\n",
    "\n",
    "Note1: For the logs, click on **View logs** from the **Training Jobs** node in **Amazon SageMaker Console**. \n",
    "Note2: Ignore the 0 billable seconds shown below. See actual billable seconds in the AWS web console > SageMaker > Training Jobs > this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-23 23:52:33 Starting - Starting the training job......\n",
      "2022-09-23 23:53:17 Starting - Preparing the instances for training........................\n",
      "2022-09-23 23:57:33 Downloading - Downloading input data...\n",
      "2022-09-23 23:57:48 Training - Downloading the training image...........................\n",
      "2022-09-24 00:02:25 Training - Training image download completed. Training in progress....................................................\n",
      "2022-09-24 00:11:23 Uploading - Uploading generated training model...\n",
      "2022-09-24 00:11:59 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "from start_job_utils import fit_with_retries\n",
    "fit_with_retries(5, estimator2, \n",
    "    job_name=\"heterogeneous-\" + datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Analyze the heterogeneous cluster training job's throughput and resource usage\n",
    "We'll examine: CPU and GPU usage. Epoch time and step time.\n",
    "\n",
    "**CPU and GPU usage analysis** \n",
    "\n",
    " In the screenshot below we observe that GPU usage has increase to 74% (compared to ~45% in the homogeneous training run) which is what we were aiming for. The CPU usage on all 3 instances are close to 80% CPU usage.  \n",
    " \n",
    "Note: To view your own job Click on **View instance metrics** from the **Training jobs** node in **Amazon SageMaker Console**. Then to rescale the CloudWatch Metrics to 100% on CPU utilization for algo-1 and algo-2, use CloudWatch \"Add Math\" feature and average it out by no. of vCPUs/GPUs on those instance types.  We captured metrics definitions used to produce this graph [here](./cloudwatch-metric-definitions/heterogenenous-workload.json).  \n",
    "<img src=\"images/metrics Heterogeneous cpu and gpu usage.png\" width=75%/>\n",
    "\n",
    "**Epoch time and step time analysis** \n",
    "\n",
    "For 2nd epoch onwards you should see this print out in the logs of the dnn_group instance (p4d.24xlarge): 43s/epoch - 86ms/step.\n",
    "Note that the instances are named: Algo1, Algo2, Algo3 randomly on each execution, so you'll need to open all instances logs to find the dnn_group instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Comparing time-to-train and cost-to-train\n",
    "The table below summarizes both jobs. We can see that:\n",
    "- The Heterogeneous job is <b>2.2x faster to train</b> (86ms/step) than the homogeneous job (192ms/step).\n",
    "- The Heterogeneous job is <b>45% cheaper to train</b> than the homogeneous job. This is despite the heterogeneous costs more per hour ($45/hour vs $37/hour), due to the two extra c5.18xlarge instances included in the heterogeneous job `($45 = $37.7 + 2 * $3.67` \n",
    "The cost-to-train formula we used: change in hourly price `($45/$37.7) ` times `reduction-in-time-to-train (86ms/192ms)`  =  45% = `($45/$37.7) * (86ms/192ms)`. \n",
    "\n",
    "<img src=images/homogeneous-vs-heterogeneous-results-table.png alt=\"results table\" />\n",
    "\n",
    "## F. Conclusion\n",
    "In this notebook, we demonstrated how to leverage Heterogeneous cluster feature of SageMaker Training, with TensorFlow to achieve better price performance and increase training speed. To get started you can copy this example project and change `train_dnn.py` to match your workload. To run the job, you could use this notebook, or the `start_job.py`."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "77c0de85c2cb739aa5100af7b92fb9d2075368f0e653f4148499a56c989df5f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
