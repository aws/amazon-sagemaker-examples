{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package a machine learning model for listing on the AWS Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook provides scripts you can use to package and verify your ML model for listing on AWS Marketplace. This sample notebook shows you the end-to-end process by building a sample ML model based on the Iris plant dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram provides an overview of the ML model packaging process. As you can see, In [Step 1](#step1) you will train a simple model, and you will store model artifacts into a joblib file. In [Step 2](#step2) you will learn how to author scoring logic that loads the ML model, performs inference, and returns the prediction. In [Step 3](#step3) you will learn how to package the ML model into a Docker Image. In [Step 4](#step4) you will push this Docker image into Amazon ECR. In [Step 5](#step5) you will learn how to package the ML model into a Model Package. In [Step 6](#step6) you will validate this ML model by deploying it with Amazon SageMaker. In [Step 7](#step7) you will learn about resources that guide you on how to list the ML model in AWS Marketplace.\n",
    "\n",
    "<img src=\"images/ml-model-publishing-workflow.png\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-requisites** \n",
    "1. Before you start building an ML model, you are strongly recommended watching this [video](https://www.youtube.com/watch?v=npilyL5xvV4) to understand the overall end-to-end ML model building and listing process.\n",
    "2. You need to add the managed policy **[AmazonEC2ContainerRegistryFullAccess](https://docs.aws.amazon.com/AmazonECR/latest/userguide/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonEC2ContainerRegistryFullAccess)** to the role associated with your notebook instance.\n",
    "3. In order to create listings on AWS Marketplace you will need to register an AWS account to be a seller account by following the [seller registration process](https://docs.aws.amazon.com/marketplace/latest/userguide/seller-registration-process.html). This guide assumes that the notebook is to be run in the registered seller account.\n",
    "\n",
    "**Note** - This example shows how to package a simple Python example which showcases a decision tree model built with the scikit-learn machine learning package. You are recommended to follow the notebook once and then customize it for your own ML model. \n",
    "\n",
    "**Table of contents**\n",
    "1. [Step 1 - Build ML model](#step1): \n",
    "2. [Step 2 - Implement scoring logic](#step2): \n",
    "3. [Step 3 - Package model artifacts and scoring logic into a Docker image](#step3)\n",
    "    1. [Step 3.1: Build Docker image to be included in the ML model](#step31)\n",
    "    2. [Step 3.2 : Test Docker image](#step32)\n",
    "4. [Step 4 - Push the Docker image into Amazon ECR](#step4): \n",
    "5. [Step 5 - Create an ML Model Package](#step5): \n",
    "6. [Step 6 - Validate model in Amazon SageMaker environment](#step6):\n",
    "    1. [Step 6.1 Validate Real-time inference via Amazon SageMaker Endpoint](#step61)\n",
    "    2. [Step 6.2 Validate batch inference via batch transform job](#step61)\n",
    "7. [Step 7 - List ML model on AWS Marketplace](#step7):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import all of the libraries needed throughout the notebook to complete the model packaging process.  We also create the clients necessary to interact with the various services needed (e.g., ECR, SageMaker, and S3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import boto3\n",
    "import docker\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role, ModelPackage\n",
    "import socket\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Training specific imports\n",
    "from joblib import dump, load\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "# Common variables\n",
    "session = sage.Session()\n",
    "s3_bucket = session.default_bucket()\n",
    "region = session.boto_region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "role = get_execution_role()\n",
    "\n",
    "sagemaker = boto3.client(\"sagemaker\")\n",
    "s3_client = session.boto_session.client(\"s3\")\n",
    "ecr = boto3.client(\"ecr\")\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model name will be re-used through various parts of the packaging and publishing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "model_name = \"my-flower-detection-model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"step1\"></a> Step 1: Build ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this sample, this section builds a simple classification model using the [Iris plants dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset) and then serializes it using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"s3://sagemaker-sample-files/datasets/tabular/iris/iris.data\", header=None)\n",
    "\n",
    "features = iris.iloc[:, 0:4]\n",
    "label = iris.iloc[:, 4].apply(\n",
    "    lambda x: IrisLabel[x.replace(\"Iris-\", \"\")].value\n",
    ")  # Integer encode the labels\n",
    "\n",
    "classifier = tree.DecisionTreeClassifier(random_state=0)\n",
    "classifier = classifier.fit(features, label)\n",
    "\n",
    "# Store the model\n",
    "dump(classifier, \"src/model-artifacts.joblib\")\n",
    "\n",
    "# Show the model\n",
    "plt.figure(figsize=[15.4, 14.0])\n",
    "tree.plot_tree(classifier, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"step2\"></a>Step 2: Implement scoring logic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supported input and output content types are left to the scoring logic. It is recommended to follow the [SageMaker standards](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html) for request and response formats where possible to provide a consistent experience to end users. The sample scoring logic provided in this example follows this standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scoring_logic.py contains all the necessary logic to take the HTTP requests that arrive via the SageMaker endpoint, translate them as needed to perform an inference, and return a properly formatted response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/scoring_logic.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker uses two URLs in the container:\n",
    "\n",
    "* `/ping` will receive `GET` requests from the infrastructure. Your program returns 200 if the container is up and accepting requests.\n",
    "* `/invocations` is the endpoint that receives client inference `POST` requests. The format of the request and the response is up to the algorithm. If the client supplied `ContentType` and `Accept` headers, these will be passed in as well. For advanced usage like request tracing, `CustomAttributes` can be used (more [details](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-runtime-now-supports-the-customattribute-header/)).  All other headers will be stripped off by the SageMaker Endpoint.\n",
    "\n",
    "The container will have the model files in the same place they were written during training:\n",
    "\n",
    "    /opt/ml\n",
    "     -- model\n",
    "        -- <model files>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on Inference pricing\n",
    "\n",
    "When the buyer runs your software by hosting an endpoint to perform real-time inference, you can choose to set a price per inference or per hour that the endpoint is active. Batch transform processes always use hourly pricing.\n",
    "\n",
    "With inference pricing, AWS Marketplace charges your buyer for each invocation of your endpoint with an HTTP response code of 2XX. However, in some cases, your software may process a batch of inferences in a single invocation. For an endpoint deployment, you can indicate a custom number of inferences that AWS Marketplace should charge the buyer for that single invocation. To do this, include a custom metering header in the HTTP response headers of your invocation, as in the following example.\n",
    "\n",
    "```\n",
    "X-Amzn-Inference-Metering: {\"Dimension\": \"inference.count\", \"ConsumedUnits\": 3}\n",
    "```\n",
    "This example shows an invocation that charges the buyer for three inferences. You can find more information in the [documentation](https://docs.aws.amazon.com/marketplace/latest/userguide/machine-learning-pricing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"step3\"></a>Step 3: Package model artifacts and scoring logic into a Docker Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided Dockerfile packages the model artifacts and serving logic as well as installing all the dependencies needed at inference time (flask, gunicorn, sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are showing a minimal example for how to create an inference image for clarity. However, for models that use common machine learning frameworks such as Sklearn, TensorFlow, TensorFlow 2, PyTorch, and Apache MXNet, AWS provides [Deep Learning Containers](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/what-is-dlc.html) as well as [Scikit-learn and SparkML Containers](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-docker-containers-scikit-learn-spark.html), which are a set of optimized Docker images which greatly simplify the setup necessary for model serving. These images should be used as base images when possible as they are performance optimized for CPU, GPU, and Inferentia. [Detailed instructions](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) are available for using the Deep Learning Containers.\n",
    "\n",
    "Select the appropriate image (CPU/GPU/Inferentia/framework combination) and replace the ubuntu:18.04 base image when adapting this example notebook for your own model to take advantage of the prebuilt SageMaker containers. \n",
    "\n",
    "For additional performance optimization, [SageMaker Neo](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html) provides the ability to automatically optimize an existing model implemented in any common machine learning framework for deployment on cloud instances (including Inferentia).  To take advantage of SageMaker Neo, follow the instructions for [compilation](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-job-compilation.html) and [serving](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-deployment-hosting-services-prerequisites.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Serving application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`serve` is a minimal script for starting up an HTTP server to handle requests.  \n",
    "\n",
    "Here we use [gunicorn](https://gunicorn.org/) as it is appropriate for a production deployment of [Flask](https://flask.palletsprojects.com/) applications. For more complex deployments the prebuilt SageMaker containers include the [SageMaker Inference Toolkit](https://github.com/aws/sagemaker-inference-toolkit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize -l bash src/serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Amazon SageMaker runs your Docker container\n",
    "\n",
    "Amazon SageMaker runs your container with the argument `serve`. How your container processes this argument depends on the container:\n",
    "\n",
    "* In the example here, we don't define an `ENTRYPOINT` in the Dockerfile so Docker will run the command `train` at training time and `serve` at serving time. In this example, we define these as executable bash scripts, but they could be any program that we want to start in that environment.\n",
    "* If you specify a program as an `ENTRYPOINT` in the Dockerfile, that program will be run at startup and the first argument will be `train` or `serve`. The program can then look at that argument and decide what to do.\n",
    "* If you are building separate containers for training and hosting (or building only for one or the other), you can define a program as an `ENTRYPOINT` in the Dockerfile and ignore (or verify) the first argument passed in. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"step31\"></a>Step 3.1: Build Docker Image to be included in the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_client = docker.from_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, build_logs = docker_client.images.build(path=\"./src\", tag=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"step32\"></a>Step 3.2 : Run Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 8080\n",
    "SECONDS = 1000000000  # One second in nanoseconds\n",
    "\n",
    "container = docker_client.containers.run(\n",
    "    image,\n",
    "    detach=True,\n",
    "    name=model_name,\n",
    "    command=\"serve\",\n",
    "    healthcheck={\n",
    "        \"test\": f\"curl -f http://localhost:{port}/ping || exit 1\",\n",
    "        \"interval\": 1 * SECONDS,  # One second\n",
    "        \"timeout\": 1 * SECONDS,  # One second\n",
    "    },\n",
    "    ports={f\"{port}/tcp\": port},\n",
    ")\n",
    "\n",
    "# Wait until our server is ready\n",
    "while docker_client.api.inspect_container(container.name)[\"State\"][\"Health\"][\"Status\"] != \"healthy\":\n",
    "    print(\"Waiting for server to become ready...\")\n",
    "    time.sleep(1)\n",
    "    container.reload()\n",
    "    print(\n",
    "        f\"Container is {docker_client.api.inspect_container(container.name)['State']['Health']['Status']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Perform inference on the container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that we can send a single record in a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_invocation_url = f\"http://127.0.0.1:{port}/invocations\"\n",
    "\n",
    "r = requests.post(\n",
    "    container_invocation_url,\n",
    "    headers={\"Content-Type\": \"text/csv\"},\n",
    "    data=\"5.1, 3.5, 1.4, 0.2\",  # setosa labeled record from training set\n",
    ")\n",
    "\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try sending multiple records in a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three records from the training set corresponding to setosa, versicolor, and virginica labels respectively\n",
    "csv_input_data = \"\"\"\n",
    "5.1, 3.5, 1.4, 0.2\n",
    "6.5, 2.8, 4.6, 1.5\n",
    "6.3, 2.9, 5.6, 1.8\n",
    "\"\"\".strip()\n",
    "\n",
    "print(csv_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(\n",
    "    container_invocation_url, headers={\"Content-Type\": \"text/csv\"}, data=csv_input_data\n",
    ")\n",
    "\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try sending different supported input content types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JSON input Content-Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_input_data = json.dumps(\n",
    "    {\n",
    "        \"instances\": [\n",
    "            {\"features\": [5.1, 3.5, 1.4, 0.2]},  # setosa labeled record from training set\n",
    "            {\"features\": [6.5, 2.8, 4.6, 1.5]},  # versicolor\n",
    "            {\"features\": [6.3, 2.9, 5.6, 1.8]},  # virginica\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(\n",
    "    container_invocation_url,\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    data=json_input_data,\n",
    ")\n",
    "\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JSON Lines input Content-Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three records from the training set corresponding to setosa, versicolor, and virginica labels respectively\n",
    "jsonlines_input_data = \"\"\"\n",
    "{\\\"features\\\": [5.1, 3.5, 1.4, 0.2]}\n",
    "{\\\"features\\\": [6.5, 2.8, 4.6, 1.5]}\n",
    "{\\\"features\\\": [6.3, 2.9, 5.6, 1.8]}\n",
    "\"\"\".strip()\n",
    "\n",
    "print(jsonlines_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(\n",
    "    container_invocation_url,\n",
    "    headers={\"Content-Type\": \"application/jsonlines\"},\n",
    "    data=jsonlines_input_data,\n",
    ")\n",
    "\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSV output Content-Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the response types by setting the Accept header to the desired type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(\n",
    "    container_invocation_url,\n",
    "    headers={\"Content-Type\": \"application/jsonlines\", \"Accept\": \"text/csv\"},\n",
    "    data=jsonlines_input_data,\n",
    ")\n",
    "\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JSON Lines output Content-Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(\n",
    "    container_invocation_url,\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/jsonlines\",\n",
    "        \"Accept\": \"application/jsonlines\",\n",
    "    },\n",
    "    data=jsonlines_input_data,\n",
    ")\n",
    "\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - If the container did not return the expected response, run the following command to see the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(container.logs().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, now that you have successfully tested container locally you can remove the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.stop()\n",
    "container.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"step4\"></a>Step 4: Push the docker image into Amazon ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your docker image is ready, you are ready to push the docker image into the Amazon ECR repository. \n",
    "\n",
    "**NOTE:** The ECR repository must belong to the AWS account that is registered as a seller on the AWS Marketplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_image_arn = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{model_name}\"\n",
    "docker_image_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows how to build the container image and push the container image to ECR using the Docker python SDK. \n",
    "\n",
    "This code looks for an ECR repository in the account you're using and the current default region (if you're using an Amazon SageMaker notebook instance, this will be the region where the notebook instance was created). If the repository doesn't exist, the script will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_exists = model_name in [\n",
    "    repo[\"repositoryName\"] for repo in ecr.describe_repositories().get(\"repositories\")\n",
    "]\n",
    "\n",
    "if not repo_exists:\n",
    "    ecr.create_repository(repositoryName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_auth_data = ecr.get_authorization_token()[\"authorizationData\"][0]\n",
    "username, password = (\n",
    "    base64.b64decode(ecr_auth_data[\"authorizationToken\"]).decode(\"utf-8\").split(\":\")\n",
    ")\n",
    "\n",
    "docker_client.api.tag(model_name, docker_image_arn, tag=\"latest\")\n",
    "status = docker_client.api.push(\n",
    "    docker_image_arn,\n",
    "    tag=\"latest\",\n",
    "    auth_config={\"username\": username, \"password\": password},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"step5\"></a>Step 5: Create an ML Model Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will see how you can package your artifacts (ECR image and the trained artifact from your previous training job) into a ModelPackage. Once you complete this, you can list your product as a pretrained model in the AWS Marketplace.\n",
    "\n",
    "**NOTE:** If your model can be deployed on multiple hardware types (CPU/GPU/Inferentia) then a ModelPackage must be created for each and added to the MP listing as different versions as, in general, the container image used will be different for each.  \n",
    "\n",
    "#### Model Package Definition\n",
    "A Model Package is a reusable abstraction for model artifacts that packages all the ingredients necessary for inference. It consists of an inference specification that defines the inference image to use along with an optional model data location.\n",
    "\n",
    "The ModelPackage must be created in the AWS account that is registered to be a seller on the AWS Marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1 Define parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_description = \"This model accepts petal length, petal width, sepal length, sepal width and predicts whether flower is of type setosa, versicolor, or virginica\"\n",
    "\n",
    "supported_content_types = [\"text/csv\", \"application/json\", \"application/jsonlines\"]\n",
    "supported_response_MIME_types = [\n",
    "    \"application/json\",\n",
    "    \"text/csv\",\n",
    "    \"application/jsonlines\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Model Package creation process requires you to specify following:\n",
    "  1. Docker image\n",
    "  2. Model artifacts\n",
    "    - You can either package these inside the docker image, as we have done in this example, or provide them as a gzipped tarball.\n",
    "  3. Validation specification \n",
    "        \n",
    "In order to provide confidence to sellers (and buyers) that the products work in Amazon SageMaker, before listing them on AWS Marketplace SageMaker needs to perform basic validations. The product can be listed in AWS Marketplace only if this validation process succeeds. This validation process uses the validation profile and sample data provided by you to create a transform job in your account using the Model to verify your inference image works with SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to identify the right instance-sizes for your ML models. You can do so by running performance tests on top of your ML Model.\n",
    "A [sample notebook](https://github.com/aws-samples/aws-marketplace-machine-learning/blob/master/right_size_your_sagemaker_endpoints/Right-sizing%20your%20Amazon%20SageMaker%20Endpoints.ipynb) is available to identify minimum suggested instance types.\n",
    "\n",
    "**NOTE:** In addition to tuning, take into account the requirements of your model when identifying instance types.  If your model does not use GPU resources, then do not include GPU instance types.  Similarly, if your model does use GPU resources, but can only make use of a single GPU, do not include instance types that have multiple GPUs as it will lead to increased infrastructure charges for your customers with no performance benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_realtime_inference_instance_types = [\"ml.m4.xlarge\"]\n",
    "supported_batch_transform_instance_types = [\"ml.m4.xlarge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_file_name = \"input.csv\"\n",
    "validation_input_path = f\"s3://{s3_bucket}/validation-input-csv/\"\n",
    "validation_output_path = f\"s3://{s3_bucket}/validation-output-csv/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create sample data to be used in the validation stage of the ModelPackage creation and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_line = \"5.1, 3.5, 1.4, 0.2\"\n",
    "\n",
    "with open(\"input.csv\", \"w\") as f:\n",
    "    f.write(csv_line)\n",
    "\n",
    "s3_client.put_object(Bucket=s3_bucket, Key=\"validation-input-csv/input.csv\", Body=csv_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2 Create Model Package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package = sagemaker.create_model_package(\n",
    "    ModelPackageName=model_name,\n",
    "    ModelPackageDescription=model_description,\n",
    "    InferenceSpecification={\n",
    "        \"Containers\": [\n",
    "            {\n",
    "                \"Image\": f\"{docker_image_arn}:latest\",\n",
    "            }\n",
    "        ],\n",
    "        \"SupportedTransformInstanceTypes\": supported_batch_transform_instance_types,\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": supported_realtime_inference_instance_types,\n",
    "        \"SupportedContentTypes\": supported_content_types,\n",
    "        \"SupportedResponseMIMETypes\": supported_response_MIME_types,\n",
    "    },\n",
    "    CertifyForMarketplace=True,  # Make sure to set this to True for Marketplace models!\n",
    "    ValidationSpecification={\n",
    "        \"ValidationRole\": role,\n",
    "        \"ValidationProfiles\": [\n",
    "            {\n",
    "                \"ProfileName\": \"Validation-test\",\n",
    "                \"TransformJobDefinition\": {\n",
    "                    \"BatchStrategy\": \"SingleRecord\",\n",
    "                    \"TransformInput\": {\n",
    "                        \"DataSource\": {\n",
    "                            \"S3DataSource\": {\n",
    "                                \"S3DataType\": \"S3Prefix\",\n",
    "                                \"S3Uri\": validation_input_path,\n",
    "                            }\n",
    "                        },\n",
    "                        \"ContentType\": supported_content_types[0],\n",
    "                    },\n",
    "                    \"TransformOutput\": {\n",
    "                        \"S3OutputPath\": validation_output_path,\n",
    "                    },\n",
    "                    \"TransformResources\": {\n",
    "                        \"InstanceType\": supported_batch_transform_instance_types[0],\n",
    "                        \"InstanceCount\": 1,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.wait_for_model_package(model_package_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have executed the preceding cell, open the [Model Packages console from Amazon SageMaker](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/model-packages/my-resources) and check if model creation succeeded. \n",
    "\n",
    "Choose the Model and then open the **Validation** tab to see the validation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"step6\"></a>Step 6: Validate model in Amazon SageMaker environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a deployable model from the model package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package[\"ModelPackageArn\"],\n",
    "    sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name='step61'></a>Step 6.1 Validate Real-time inference via Amazon SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deploy the SageMaker model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=supported_realtime_inference_instance_types[0],\n",
    "    endpoint_name=model_name,\n",
    ")\n",
    "model.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_type = supported_content_types[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example invocation via boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=model.endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Accept=\"application/json\",\n",
    "    Body=csv_input_data,\n",
    ")\n",
    "\n",
    "json.load(response[\"Body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example invocation via the AWS CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "!aws sagemaker-runtime invoke-endpoint \\\n",
    "    --endpoint-name $model.endpoint_name \\\n",
    "    --body fileb://$validation_file_name \\\n",
    "    --content-type $content_type \\\n",
    "    --region $session.boto_region_name \\\n",
    "    out.out\n",
    "    \n",
    "    \n",
    "# Print inference\n",
    "!head out.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the endpoint and endpoint configuration created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model.endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name='step62'></a>Step 6.2 Validate batch inference via batch transform job "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run a batch-transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=supported_batch_transform_instance_types[0],\n",
    "    accept=\"application/jsonlines\",\n",
    ")\n",
    "transformer.transform(validation_input_path, content_type=content_type)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieve the results from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_url = urlparse(transformer.output_path)\n",
    "file_key = f\"{parsed_url.path[1:]}/{validation_file_name}.out\"\n",
    "response = s3_client.get_object(Bucket=s3_bucket, Key=file_key)\n",
    "\n",
    "print(response[\"Body\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You just verified that the batch transform job is working as expected. Since the model is not required, you can delete it. Note that you are deleting the deployable model. Not the model package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To publish the model to the AWS Marketplace, you will need to specify model package ARN. Copy the following Model Package ARN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package[\"ModelPackageArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"step7\"></a>Step 7: List ML Model on AWS Marketplace\n",
    "\n",
    "In the [Model Packages](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/model-packages/my-resources) section of the SageMaker console you'll find the entity you created in this notebook. If it was successfully created and validated, you should be able to select the entity and choose **Publish new ML Marketplace listing**.\n",
    "\n",
    "<img src=\"images/publish-to-marketplace-action.png\"/>\n",
    "\n",
    "You will be redirected to the [AWS Marketplace Management portal](https://aws.amazon.com/marketplace/management/ml-products/) where you will be able to build a listing.\n",
    "\n",
    "If your model targets multiple hardware types, remember to add each ModelPackage to the listing as separate versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a High-quality ML Model Listing\n",
    "\n",
    "Your AWS Marketplace model listing should appeal to both data scientists with deep expertise who are looking for an ML model because they don't have access to data they need to train an ML model from scratch and developers with little to no ML background who are looking to add powerful new features to their applications. \n",
    "\n",
    "You need to provide sample notebook and instructions your users can follow to interact with your model. [Sample notebook templates](https://github.com/aws/amazon-sagemaker-examples/tree/master/aws_marketplace/curating_aws_marketplace_listing_and_sample_notebook/ModelPackage/Sample_Notebook_Template) are available to assist in creating an effective sample notebook.\n",
    "\n",
    "To build an impressive listing, you need to stand out by providing information thoughtfully on your Marketplace listing. [Best practice recommendations](https://github.com/aws/amazon-sagemaker-examples/blob/master/aws_marketplace/curating_aws_marketplace_listing_and_sample_notebook/ModelPackage/curating_good_model_package_listing.md) are provided for curating your listing.\n",
    "\n",
    "\n",
    "**Resources**\n",
    "* [Publishing your product in AWS Marketplace](https://docs.aws.amazon.com/marketplace/latest/userguide/ml-publishing-your-product-in-aws-marketplace.html)\n",
    "\n",
    "Most importantly, once you have listed your listing in AWS Marketplace, do explore how you can spread the word about your cool new ML listing."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
