{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify a Network Bottleneck with Amazon SageMaker Debugger \n",
    "\n",
    "In this notebook we demonstrate how to identify a bottleneck in `tf.data` pipeline of a ResNet50 training session. To simulate the bottleneck, we have added a heavy data preprocessing task to the pipeline to modify the CIFAR-10 dataset during the training.\n",
    "\n",
    "### Tensorflow Datasets package\n",
    "\n",
    "First of all, set the notebook kernel to Tensorflow 2.x.\n",
    "\n",
    "We will use CIFAR-10 dataset for this experiment. To download CIFAR-10 datasets and convert it into TFRecord format, install `tensorflow-datasets` package, run `demo/generate_cifar10_tfrecords`, and upload tfrecord files to your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python demo/generate_cifar10_tfrecords.py --data-dir=./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "s3_bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "dataset_prefix='data/cifar10-tfrecords'\n",
    "desired_s3_uri = f's3://{s3_bucket}/{dataset_prefix}'\n",
    "\n",
    "dataset_location = sagemaker.s3.S3Uploader.upload(local_path='data', desired_s3_uri=desired_s3_uri)\n",
    "print(f'Dataset uploaded to {dataset_location}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a Training Job with Profiling Enabled<a class=\"anchor\" id=\"option-1\"></a>\n",
    "\n",
    "We will use the standard [SageMaker Estimator API for Tensorflow](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-estimator) to create a training job. To enable profiling, we create a `ProfilerConfig` object and pass it to the `profiler_config` parameter of the `TensorFlow` estimator. For this demo, we set the the profiler to probe the system once every 60 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a profiler configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "  system_monitor_interval_millis=500,\n",
    "  framework_profile_params=FrameworkProfile(start_step=5, num_steps=2)  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters\n",
    "\n",
    "The start-up script is set to [train_tf_bottleneck.py](./demo/train_tf_bottleneck). Define hyperparameters such as number of epochs, batch size, and data augmentation. `dataset_bottleneck` hyperparameter is to turn the data augmentation on or off. To add data preprocessing bottleneck, set `dataset_bottleneck` as `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epoch': 1, \n",
    "                   'batch_size': 1024,\n",
    "                   'dataset_bottleneck': True\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SageMaker Tensorflow Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "job_name = 'dataset-bottleneck'\n",
    "instance_count = 1\n",
    "instance_type = 'ml.p2.xlarge'\n",
    "entry_script = 'train_tf_bottleneck.py'\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    base_job_name=job_name,\n",
    "    instance_type=instance_count,\n",
    "    instance_count=instance_type,\n",
    "    entry_point=entry_script,\n",
    "    source_dir='demo',\n",
    "    framework_version='2.3.1',\n",
    "    py_version='py37',\n",
    "    profiler_config=profiler_config,\n",
    "    script_mode=True,\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_mode='Pipe'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you see an error, `TypeError: __init__() got an unexpected keyword argument 'instance_type'`, that means SageMaker Python SDK is out-dated. Please update your SageMaker Python SDK to 2.x by executing the below command and restart this notebook.\n",
    "\n",
    "```bash\n",
    "pip install --upgrade sagemaker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training job\n",
    "\n",
    "The following `estimator.fit()` with `wait=False` argument initiates the training job in the background. You can proceed to run the dashboard or analysis notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_inputs = {'train' : dataset_location+'/train'}\n",
    "\n",
    "estimator.fit(remote_inputs, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Monitor the system resource utilization using SageMaker Studio\n",
    "\n",
    "During the training is in progress or after the training job is completed, go to `Debugger` in SageMaker Studio. You will see GPU utilization stays low while CPU utilization is hitting 100% all the time. We know this is due to the change we made. But if you see this pattern in your own training job, what do you want to know? Definitely, we want to know what is being executed on CPU. Python profiling functionality of SageMaker Debugger will tell you what is happening or what happened. \n",
    "\n",
    "![Debugger-in-Studio](./images/datapipeline-bottleneck.png)\n",
    "\n",
    "## Step 4: Investigate the bottleneck interactively using Debugger analysis APIs\n",
    "\n",
    "In order to analyze the Python profiling data gathered by SageMaker Debugger, open a notebook, [interactive_analysis.ipynb](https://github.com/aws/amazon-sagemaker-examples/tree/master/sagemaker-debugger/profiling_analysis_tools/interactive_analysis.ipynb), which is in SageMaker Debugger example git repo. Then, set your training job and the AWS region where your training job exists in the notebook. \n",
    "\n",
    "```python\n",
    "training_job_name = '<PUT YOUR TRANING JOB NAME>'\n",
    "region = '<AWS REGION WHERE THE TRAINING JOB WAS EXECUTED>' \n",
    "```\n",
    "\n",
    "Execute the notebook code one by one, and you will meet a plot similar to this. This is a zoom-in version of the above plots. Follow the guide in the notebook to choose the time interval for dive deep investigation. \n",
    "\n",
    "![Debugger-in-Studio](./images/datapipeline-bottleneck-cpugpuutil.png)\n",
    "\n",
    "The list of function names executed during the selected period is given as below, and the execution time of each function is also printed. In our case, `GetNext` functions consumed CPU cycle mostly, and `GetNext` function is related to get the next example. \n",
    "\n",
    "```python\n",
    "view_timeline_charts.find_time_annotations([13527,13548])  \n",
    "```\n",
    "\n",
    "```\n",
    "Selected timerange: 1606920919.15031 to 1606920929.67799 \n",
    "Spent 0.079906123 ms (cumulative time) in Step:ModeKeys.TRAIN_79 \n",
    "Spent 3.202e-06 ms (cumulative time) in PipeModeDatasetOp::Dataset::Iterator::GetNext \n",
    "Spent 3.5277999999999976e-05 ms (cumulative time) in tensorflow::data::(anonymous namespace)::ParallelMapIterator::GetNext \n",
    "Spent 9.340299999999999e-05 ms (cumulative time) in tensorflow::data::RepeatDatasetOp::Dataset::ForeverIterator::GetNext \n",
    "Spent 9.553100000000004e-05 ms (cumulative time) in tensorflow::data::ShuffleDatasetOp::ReshufflingDatasetV2::Iterator::GetNext \n",
    "Spent 0.079633313 ms (cumulative time) in tensorflow::data::experimental::MapAndBatchDatasetOp::Dataset::Iterator::GetNext \n",
    "Spent 0.078741146 ms (cumulative time) in tensorflow::data::PrefetchDatasetOp::Dataset::Iterator::GetNext \n",
    "Spent 0.078741162 ms (cumulative time) in tensorflow::data::(anonymous namespace)::ModelDatasetOp::Dataset::Iterator::GetNext \n",
    "Spent 0.078741179 ms (cumulative time) in IteratorResource::GetNext \n",
    "Spent 0.078741203 ms (cumulative time) in IteratorGetNextOp::DoCompute \n",
    "```\n",
    "\n",
    "You can also download the generated timeline file, and open it using Chrome Tracing tool, which visualizes the timeline profiling data. Agsin, `GetNext` is the most heavy function call. \n",
    "\n",
    "![Debugger-in-Studio](./images/datapipeline-bottleneck-timeline.png)\n",
    "\n",
    "If you take a look at the data pipeline in the training code, there is a code to add Gaussian Blur filters in `data_augmentation` function which slow down the data pipelines to GPU. \n",
    "\n",
    "```python\n",
    "def data_augmentation(image, label): \n",
    "    import tensorflow_addons as tfa \n",
    "    for i in range(1): \n",
    "        image = tfa.image.gaussian_filter2d(image=image, filter_shape=(11, 11), sigma=0.8) \n",
    "    return image, label \n",
    "\n",
    "if dataset_bottleneck: \n",
    "    dataset = dataset.map(data_augmentation, num_parallel_calls=tf.data.experimental.AUTOTUNE) \n",
    "```\n",
    "\n",
    "This is the bottleneck you have to resolve either by removing it or applying this modification to the dataset in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
