{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Store Data Ingestion via SageMaker Model Building Pipeline\n",
    "\n",
    "This sample notebook demonstrates how one can leverage model building pipeline to: \n",
    "\n",
    "1. Ingest preprocessed data directly into a feature group.\n",
    "2. Transform and ingest data into a feature group.\n",
    "\n",
    "please choose `Python 3 (Data Science)` kernel to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "# get the latest sagemaker python SDK\n",
    "!{sys.executable} -m pip install \"sagemaker>=2.41.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.session import get_execution_role, Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup, FeatureDefinition, FeatureTypeEnum\n",
    "from sagemaker.wrangler.processing import DataWranglerProcessor\n",
    "from sagemaker.wrangler.ingestion import generate_data_ingestion_flow_from_s3_input\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.processing import FeatureStoreOutput\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "boto_session = boto3.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for SageMaker FeatureStore Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "featurestore_runtime_client = boto_session.client(\"sagemaker-featurestore-runtime\")\n",
    "featurestore_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_feature_group_create(feature_group: FeatureGroup):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        print(feature_group.describe())\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_to_clean_up = []\n",
    "feature_group_to_clean_up = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest the already transformed data directly into a feature group.\n",
    "\n",
    "This section shows how you can ingest a pre-processed dataset directly into a feature group via a pipeline execution. More specficially, we will run a pipeline with a single `Processing` step, this step will launch a DataWrangler processing job to ingest your data into a feature group directly, without any transformation.\n",
    "\n",
    "Let's use the `data/features.csv` as the sample pre-processed features. The dataset has in total 11 features, `f10` is the `event_time_feature_name`, and `f11` is the unique `record_identifier_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/features.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload dataset to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_uri = sagemaker_session.upload_data(\n",
    "    path=\"data/features.csv\", \n",
    "    bucket=sagemaker_session.default_bucket(), \n",
    "    key_prefix=\"data-ingestion-demo/inputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_group(\n",
    "    name, \n",
    "    feature_definition, \n",
    "    record_identifier_name, \n",
    "    event_time_feature_name, \n",
    "    offline_store_s3_uri, \n",
    "    featurestore_session, \n",
    "    role\n",
    "):\n",
    "    feature_group = FeatureGroup(\n",
    "        name=name, \n",
    "        feature_definitions=feature_definition, \n",
    "        sagemaker_session=featurestore_session\n",
    "    )\n",
    "    try:\n",
    "        feature_group.create(\n",
    "            s3_uri=offline_store_s3_uri,\n",
    "            record_identifier_name=record_identifier_name,\n",
    "            event_time_feature_name=event_time_feature_name,\n",
    "            role_arn=role,\n",
    "            # we will disable the online store for the purpose of this demo\n",
    "            # otherwise, the data will be only available after offline sync is done (SLA 15min)\n",
    "            enable_online_store=False,\n",
    "        )\n",
    "        wait_for_feature_group_create(feature_group)\n",
    "    except Exception as e:\n",
    "        if 'ResourceInUse' in str(e):\n",
    "            print(\"FeatureGroup already exists.\") \n",
    "        else:\n",
    "            raise e\n",
    "    return feature_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to first define the feature definition for our feature group based on the above dataset.\n",
    "feature_definition = [\n",
    "        FeatureDefinition(feature_name=\"f1\", feature_type=FeatureTypeEnum.STRING),\n",
    "        FeatureDefinition(feature_name=\"f2\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f3\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f4\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f5\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f6\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f7\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f8\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f9\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "        FeatureDefinition(feature_name=\"f10\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f11\", feature_type=FeatureTypeEnum.STRING),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_name = \"demo-1-1-ingestion-fg\"\n",
    "# define where feature store should offline sync the features into\n",
    "offline_store_s3_uri = os.path.join(\"s3://\", sagemaker_session.default_bucket(), feature_group_name)\n",
    "feature_group = create_feature_group(feature_group_name, feature_definition, \"f11\", \"f10\", offline_store_s3_uri, featurestore_session, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_to_clean_up.append(feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the ingestion flow\n",
    "\n",
    "Lets use a helper function to generate a ingestion only data wrangler flow, the flow will read your data only and output to the feature group you specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion_only_flow, output_name = generate_data_ingestion_flow_from_s3_input(\n",
    "    \"features.csv\",\n",
    "    input_data_uri,\n",
    "    s3_content_type=\"csv\",\n",
    "    s3_has_header=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('flows', exist_ok=True)\n",
    "json.dump(ingestion_only_flow, open(\"flows/ingestion_only.flow\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure DataWranglerProcessor\n",
    "\n",
    "Lets configure a `DataWranglerProcessor` as the base processor for our `Processing` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count = ParameterInteger(name=\"InstanceCount\", default_value=1)\n",
    "instance_type = ParameterString(name=\"InstanceType\", default_value=\"ml.m5.4xlarge\")\n",
    "    \n",
    "data_wrangler_processor = DataWranglerProcessor(\n",
    "    role=role,\n",
    "    data_wrangler_flow_source=\"flows/ingestion_only.flow\",\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    max_runtime_in_seconds=86400,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> 💡Note that when setting the ProcessingOutput, we need to set app_managed to True in order to ingest into the feature group we specify. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name=\"features.csv\",\n",
    "        source=input_data_uri,\n",
    "        destination=\"/opt/ml/processing/features.csv\",\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name=output_name,\n",
    "        app_managed=True, # this must be true in order to ingest data into a feature group\n",
    "        feature_store_output=FeatureStoreOutput(feature_group_name=feature_group_name),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrangler_step = ProcessingStep(\n",
    "    name=\"ingestion-step\", processor=data_wrangler_processor, inputs=inputs, outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=\"ingestion-only-pipeline\",\n",
    "    parameters=[instance_count, instance_type],\n",
    "    steps=[data_wrangler_step],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "pipeline.create(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_to_clean_up.append(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()\n",
    "response = execution.describe()\n",
    "execution.wait(delay=60, max_attempts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm features get ingested\n",
    "\n",
    "Let's use athena query to confirm the features are ingested into our feature group without any transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_group_data(feature_group, offline_store_s3_uri):\n",
    "    athena_query = feature_group.athena_query()\n",
    "    athena_query.run(query_string=f'SELECT * FROM \"{athena_query.table_name}\"', output_location=f\"{offline_store_s3_uri}/query_results\")\n",
    "    athena_query.wait()\n",
    "    assert \"SUCCEEDED\" == athena_query.get_query_execution().get(\"QueryExecution\").get(\n",
    "        \"Status\"\n",
    "    ).get(\"State\")\n",
    "    return athena_query.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_df = get_feature_group_data(feature_group, offline_store_s3_uri)\n",
    "fg_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(fg_df[df.columns] == df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo shows how to copy data from s3 into a feature group, there are other two helper functions that can help you generate flow to ingest from `Athena` or `RedShift`. You can find them at:\n",
    "\n",
    "[`sagemaker.wrangler.ingestion.generate_data_ingestion_flow_from_athena_dataset_definition`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/wrangler/ingestion.py#L76)\n",
    "\n",
    "[`sagemaker.wrangler.ingestion.generate_data_ingestion_flow_from_redshift_dataset_definition`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/wrangler/ingestion.py#L123)\n",
    "\n",
    "The usage is very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data via DataWrangler, and ingest the transformed data into a feature group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to go to the SageMaker DataWrangler console to generate a flow with a transformation. The below DataWrangler flow will import the same `data/features.csv` we have uploaded to s3 from the first scenario. \n",
    "\n",
    "1. generate a flow with input from s3\n",
    "\n",
    "<img src=\"./pics/input-flow.png\" style=\"width:800px;\"/>\n",
    "\n",
    "2. add transformation\n",
    "\n",
    "<img src=\"./pics/add-transform.png\" style=\"width:800px;\"/>\n",
    "\n",
    "3. add one-hot encoding to the categoritcal feature `f1`\n",
    "\n",
    "<img src=\"./pics/one-hot-transform.png\" style=\"width:800px;\"/>\n",
    "\n",
    "4. export to feature store\n",
    "\n",
    "<img src=\"./pics/export.png\" style=\"width:800px;\"/>\n",
    "\n",
    "once you click **Export to Feature Store** you will be directed to a notebook. Inside the notebook, you can upload the freshly baked `untitled.flow` to a s3 uri. The flow representing the steps shown above is donwloaded locally to `./flows/transformation.flow` for the purpose of this demo. \n",
    "\n",
    "<div class=\"alert alert-info\"> 💡Note that one important information inside the notebook is the output name, it is auto-generated from the select output node's ID + output name from the flow file.\n",
    "</div>\n",
    "\n",
    "In our case, the output name value is `2351bdcf-a2f6-499c-9665-1203f48eb3cd.default`, this value tells the DataWrangler container where to look up the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, similarly, lets create another feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to first define the feature definition for our feature group based on the above dataset.\n",
    "feature_definition = [\n",
    "        # one-hot encoded f1\n",
    "        FeatureDefinition(feature_name=\"f1_F\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f1_M\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f1_I\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f2\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f3\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f4\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f5\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f6\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f7\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f8\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f9\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "        FeatureDefinition(feature_name=\"f10\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "        FeatureDefinition(feature_name=\"f11\", feature_type=FeatureTypeEnum.STRING),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_name = \"demo-transformation-ingestion-fg\"\n",
    "# define where feature store should offline sync the features into\n",
    "offline_store_s3_uri = os.path.join(\"s3://\", sagemaker_session.default_bucket(), feature_group_name)\n",
    "feature_group = create_feature_group(feature_group_name, feature_definition, \"f11\", \"f10\", offline_store_s3_uri, featurestore_session, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_to_clean_up.append(feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, lets create another pipeline to execute the transformation and ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrangler_processor = DataWranglerProcessor(\n",
    "    role=role,\n",
    "    data_wrangler_flow_source=\"flows/transformation.flow\",\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    max_runtime_in_seconds=86400,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name=\"features.csv\",\n",
    "        source=input_data_uri,\n",
    "        destination=\"/opt/ml/processing/features.csv\",\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name=\"2351bdcf-a2f6-499c-9665-1203f48eb3cd.default\",\n",
    "        app_managed=True, # this must be true in order to ingest data into a feature group\n",
    "        feature_store_output=FeatureStoreOutput(feature_group_name=feature_group_name),\n",
    "    )\n",
    "]\n",
    "\n",
    "data_wrangler_step = ProcessingStep(\n",
    "    name=\"transformation-ingestion-step\", processor=data_wrangler_processor, inputs=inputs, outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=\"transformation-ingestion-pipeline\",\n",
    "    parameters=[instance_count, instance_type],\n",
    "    steps=[data_wrangler_step],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "pipeline.create(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_to_clean_up.append(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()\n",
    "response = execution.describe()\n",
    "execution.wait(delay=60, max_attempts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm features get transformed and ingested\n",
    "\n",
    "Let's use athena query to confirm the features are transformed and ingested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_df = get_feature_group_data(feature_group, offline_store_s3_uri)\n",
    "fg_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up():\n",
    "    for pipeline in pipeline_to_clean_up:\n",
    "        try:\n",
    "            pipeline.delete()\n",
    "        except Exception as e:\n",
    "            if 'ResourceNotFound' in str(e):\n",
    "                pass\n",
    "        \n",
    "    for fg in feature_group_to_clean_up:\n",
    "        try:\n",
    "            fg.delete()\n",
    "        except Exception as e:\n",
    "            if 'ResourceNotFound' in str(e):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
