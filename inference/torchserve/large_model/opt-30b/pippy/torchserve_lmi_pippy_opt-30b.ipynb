{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a93ca6",
   "metadata": {},
   "source": [
    "# Serve OPT-30B on SageMaker With PyTorch PiPPy Using TorchServe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446701de",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "This notebook demonstrates how to use PyTorch Native large model inference solution TorchServe+PiPPy on SageMaker. In this example, OPT-30B is loaded on g5.24xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc665186",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pillow\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7c6f24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account=084495728311, region=us-west-2, role=arn:aws:iam::084495728311:role/service-role/AmazonSageMaker-ExecutionRole-20230505T104760\n"
     ]
    }
   ],
   "source": [
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import sagemaker\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from sagemaker.model import Model\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "region = sess.region_name\n",
    "account = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "\n",
    "smsess = sagemaker.Session(boto_session=sess)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Configuration:\n",
    "bucket_name = smsess.default_bucket()\n",
    "prefix = \"torchserve\"\n",
    "output_path = f\"s3://{bucket_name}/{prefix}\"\n",
    "print(f\"account={account}, region={region}, role={role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761c4028",
   "metadata": {},
   "source": [
    "## Create Model Artifacts\n",
    "This example creates a tar.gz format TorchServe model artifact for each model.\n",
    "### Install torch-model-archiver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36bc81",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install torch-model-archiver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd98e09",
   "metadata": {},
   "source": [
    "### Download OPT-30B from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41318bc",
   "metadata": {},
   "source": [
    "#### Implement customized handler\n",
    "This [readme](https://github.com/pytorch/serve/blob/e205e6b9836a881dea6b5d2afee20570b1280f36/docs/large_model_inference.md?plain=1#L1) describes how to implement large model's handler in TorchServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4109a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cwd=$(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5edd0a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import logging\r\n",
      "import time\r\n",
      "from abc import ABC\r\n",
      "\r\n",
      "import packaging.version\r\n",
      "import requests\r\n",
      "import torch\r\n",
      "import transformers\r\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\r\n",
      "\r\n",
      "from ts.handler_utils.distributed.pt_pippy import get_pipeline_driver\r\n",
      "from ts.torch_handler.distributed.base_pippy_handler import BasePippyHandler\r\n",
      "\r\n",
      "logger = logging.getLogger(__name__)\r\n",
      "logger.info(\"Transformers version %s\", transformers.__version__)\r\n",
      "if packaging.version.parse(torch.__version__) >= packaging.version.parse(\"2.0.0\"):\r\n",
      "    logger.info(\"PyTorch version is 2.0.0 or greater\")\r\n",
      "else:\r\n",
      "    logger.info(\r\n",
      "        \"PyTorch version is less than 2.0.0, initializing with meta device needs PyTorch 2.0.0 and greater\"\r\n",
      "    )\r\n",
      "\r\n",
      "\r\n",
      "class TransformersSeqClassifierHandler(BasePippyHandler, ABC):\r\n",
      "    \"\"\"\r\n",
      "    Transformers handler class for sequence, token classification and question answering.\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    def __init__(self):\r\n",
      "        super(TransformersSeqClassifierHandler, self).__init__()\r\n",
      "        self.initialized = False\r\n",
      "\r\n",
      "    def initialize(self, ctx):\r\n",
      "        \"\"\"In this initialize function, the HF large model is loaded and\r\n",
      "        partitioned into multiple stages each on one device using PiPPy.\r\n",
      "        Args:\r\n",
      "            ctx (context): It is a JSON Object containing information\r\n",
      "            pertaining to the model artefacts parameters.\r\n",
      "        \"\"\"\r\n",
      "        super().initialize(ctx)\r\n",
      "        self.manifest = ctx.manifest\r\n",
      "        properties = ctx.system_properties\r\n",
      "        model_dir = properties.get(\"model_dir\")\r\n",
      "        self.device = self.local_rank\r\n",
      "\r\n",
      "        model_path = ctx.model_yaml_config[\"handler\"][\"model_path\"]\r\n",
      "        seed = ctx.model_yaml_config[\"handler\"][\"manual_seed\"]\r\n",
      "        dtype_str = ctx.model_yaml_config[\"handler\"][\"dtype\"]\r\n",
      "        torch.manual_seed(seed)\r\n",
      "\r\n",
      "        dtypes = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\r\n",
      "\r\n",
      "        dtype = dtypes.get(dtype_str, torch.float32)\r\n",
      "        if dtype != torch.float32 and dtype_str not in dtypes:\r\n",
      "            logger.info(\r\n",
      "                f\"Unsupported data type {dtype_str}, \"\r\n",
      "                \"please submit a PR to support it. Falling back to fp32 now.\"\r\n",
      "            )\r\n",
      "\r\n",
      "        skip_init_start = time.perf_counter()\r\n",
      "        with torch.device(\"meta\"):\r\n",
      "            self.model = AutoModelForCausalLM.from_pretrained(\r\n",
      "                model_path, use_cache=False, torch_dtype=dtype\r\n",
      "            )\r\n",
      "        skip_init_end = time.perf_counter()\r\n",
      "        logger.info(\r\n",
      "            f\" init model time on meta device took {skip_init_end - skip_init_start} seconds\"\r\n",
      "        )\r\n",
      "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, return_tensors=\"pt\")\r\n",
      "        self.tokenizer.pad_token = self.tokenizer.eos_token\r\n",
      "\r\n",
      "        self.max_length = ctx.model_yaml_config[\"handler\"][\"max_length\"]\r\n",
      "        self.max_new_tokens = ctx.model_yaml_config[\"handler\"][\"max_new_tokens\"]\r\n",
      "\r\n",
      "        logger.info(\"Instantiating model Pipeline\")\r\n",
      "        pippy_compile_time_start = time.perf_counter()\r\n",
      "        self.model = get_pipeline_driver(self.model, self.world_size, ctx)\r\n",
      "        pippy_compile_time_end = time.perf_counter()\r\n",
      "\r\n",
      "        logger.info(\r\n",
      "            f\" pippy compile time took {pippy_compile_time_end- pippy_compile_time_start} seconds on rank {self.local_rank}\"\r\n",
      "        )\r\n",
      "\r\n",
      "        logger.info(\"Transformer model from path %s loaded successfully\", model_dir)\r\n",
      "\r\n",
      "        self.initialized = True\r\n",
      "\r\n",
      "    def preprocess(self, requests):\r\n",
      "        \"\"\"\r\n",
      "        Basic text preprocessing, based on the user's choice of application mode.\r\n",
      "        Args:\r\n",
      "            requests (list): A list of dictionaries with a \"data\" or \"body\" field, each\r\n",
      "                            containing the input text to be processed.\r\n",
      "        Returns:\r\n",
      "            tuple: A tuple with two tensors: the batch of input ids and the batch of\r\n",
      "                attention masks.\r\n",
      "        \"\"\"\r\n",
      "        input_texts = [data.get(\"data\") or data.get(\"body\") for data in requests]\r\n",
      "        input_ids_batch = []\r\n",
      "        for input_text in input_texts:\r\n",
      "            input_ids = self.encode_input_text(input_text)\r\n",
      "            input_ids_batch.append(input_ids)\r\n",
      "        input_ids_batch = torch.cat(input_ids_batch, dim=0).to(self.device)\r\n",
      "        return input_ids_batch\r\n",
      "\r\n",
      "    def encode_input_text(self, input_text):\r\n",
      "        \"\"\"\r\n",
      "        Encodes a single input text using the tokenizer.\r\n",
      "        Args:\r\n",
      "            input_text (str): The input text to be encoded.\r\n",
      "        Returns:\r\n",
      "            tuple: A tuple with two tensors: the encoded input ids and the attention mask.\r\n",
      "        \"\"\"\r\n",
      "        if isinstance(input_text, (bytes, bytearray)):\r\n",
      "            input_text = input_text.decode(\"utf-8\")\r\n",
      "        logger.info(\"Received text: '%s'\", input_text)\r\n",
      "        inputs = self.tokenizer.encode_plus(\r\n",
      "            input_text,\r\n",
      "            max_length=self.max_length,\r\n",
      "            pad_to_max_length=True,\r\n",
      "            add_special_tokens=True,\r\n",
      "            return_tensors=\"pt\",\r\n",
      "        )\r\n",
      "        input_ids = inputs[\"input_ids\"]\r\n",
      "        return input_ids\r\n",
      "\r\n",
      "    def inference(self, input_batch):\r\n",
      "        \"\"\"\r\n",
      "        Predicts the class (or classes) of the received text using the serialized transformers\r\n",
      "        checkpoint.\r\n",
      "        Args:\r\n",
      "            input_batch (tuple): A tuple with two tensors: the batch of input ids and the batch\r\n",
      "                                of attention masks, as returned by the preprocess function.\r\n",
      "        Returns:\r\n",
      "            list: A list of strings with the predicted values for each input text in the batch.\r\n",
      "        \"\"\"\r\n",
      "        input_ids_batch = input_batch\r\n",
      "        input_ids_batch = input_ids_batch.to(self.device)\r\n",
      "        outputs = self.model.generate(\r\n",
      "            input_ids_batch,\r\n",
      "            max_length=self.max_new_tokens,\r\n",
      "        )\r\n",
      "        generated_text = self.tokenizer.batch_decode(\r\n",
      "            outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\r\n",
      "        )\r\n",
      "\r\n",
      "        logger.info(\"Generated text: %s\", generated_text)\r\n",
      "        return generated_text\r\n",
      "\r\n",
      "    def postprocess(self, inference_output):\r\n",
      "        \"\"\"Post Process Function converts the predicted response into Torchserve readable format.\r\n",
      "        Args:\r\n",
      "            inference_output (list): It contains the predicted response of the input text.\r\n",
      "        Returns:\r\n",
      "            (list): Returns a list of the Predictions and Explanations.\r\n",
      "        \"\"\"\r\n",
      "        return inference_output\r\n"
     ]
    }
   ],
   "source": [
    "!cat `pwd`/workspace/pippy_handler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042828b0",
   "metadata": {},
   "source": [
    "#### Download OPT-30B from HuggingFace, Create and Upload opt-30b.tar.gz file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57652cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ec2-user/SageMaker/amazon-sagemaker-examples/inference/torchserve/large_model/opt-30b/pippy/opt-30b/../../../utils/Download_HF_model.py\", line 4, in <module>\r\n",
      "    from huggingface_hub import HfApi, snapshot_download\r\n",
      "ModuleNotFoundError: No module named 'huggingface_hub'\r\n"
     ]
    }
   ],
   "source": [
    "# Create Model Manifest\n",
    "!torch-model-archiver --model-name opt-30b --version 1.0 --handler `pwd`/workspace/pippy_handler.py --config-file `pwd`/workspace/model-config.yaml --archive-format no-archive\n",
    "\n",
    "!cd opt-30b && cp -rp `pwd`/workspace/code/ .\n",
    "\n",
    "# Download OPT-30B from HuggingFace\n",
    "!cd opt-30b && mkdir model && python ../../../utils/Download_HF_model.py --model_name facebook/opt-30b\n",
    "# Replace symbolic link b/c SageMaker does not allow symbolic link in tgz file\n",
    "!cd opt-30b/model/models--facebook--opt-30b/snapshots/ceea0a90ac0f6fae7c2c34bcb40477438c152546 && for f in $(find -type l);do cp --remove-destination $(readlink $f) $f;done;\n",
    "\n",
    "# Create model tgz file\n",
    "!export GZIP='--fast'\n",
    "!cd opt-30b && tar  --exclude='model/models--facebook--opt-30b/blobs/*' -cvzf opt-30b.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11924275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "The user-provided path opt-30b.tar.gz does not exist.\r\n"
     ]
    }
   ],
   "source": [
    "# Upload model tgz to S3\n",
    "!cd opt-30b && aws s3 cp opt-30b.tar.gz {output_path}/opt-30b.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd33e1",
   "metadata": {},
   "source": [
    "## Create the Model Endpoint with the SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20294af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SageMaker PyTorch DLC as base image\n",
    "baseimage = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    py_version=\"py310\",\n",
    "    image_scope=\"inference\",\n",
    "    version=\"2.0\",\n",
    "    instance_type=\"ml.g5.24xlarge\",\n",
    ")\n",
    "print(baseimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50efe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where our endpoint will read models from on S3.\n",
    "model_s3uri = output_path\n",
    "print(model_s3uri)\n",
    "model = Model(\n",
    "    name=\"torchserve-opt-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n",
    "    model_data=f\"{model_s3uri}/opt-30b.tar.gz\",\n",
    "    image_uri=baseimage,\n",
    "    role=role,\n",
    "    sagemaker_session=smsess,\n",
    "    env={\"GZIP\": \"--fast\"},\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abb202",
   "metadata": {},
   "source": [
    "### Deploy the Endpoint\n",
    "\n",
    "You need to consider the appropriate instance type and number of instances for the projected prediction workload across all the models you plan to host behind your multi-model endpoint. The number and size of the individual models will also drive memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "    print(\"Deleting previous endpoint...\")\n",
    "    time.sleep(10)\n",
    "except (NameError, ClientError):\n",
    "    pass\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.24xlarge\",\n",
    "    endpoint_name=\"torchserve-opt-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()),\n",
    "    model_data_download_timeout=3600,\n",
    "    container_startup_health_check_timeout=1800,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d28e7",
   "metadata": {},
   "source": [
    "## Get predictions from the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=model.endpoint_name, sagemaker_session=smsess\n",
    ")\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeed467",
   "metadata": {},
   "source": [
    "### OPT Inference Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfe9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = json.dumps({\"data\": \"Hey, are you conscious? Can you talk to me?\"}).encode(\"utf-8\")\n",
    "\n",
    "\n",
    "response = predictor.predict(data=payload).decode(\"utf-8\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb05ed2",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Endpoints should be deleted when no longer in use, since (per the [SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/)) they're billed by time deployed. Here we'll also delete the endpoint configuration - to keep things tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623248bc",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/advanced_functionality|multi_model_pytorch|pytorch_multi_model_endpoint.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
