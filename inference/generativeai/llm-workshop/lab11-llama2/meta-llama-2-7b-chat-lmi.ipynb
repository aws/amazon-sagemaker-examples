{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# LLAMA 7B Chat inference using SageMaker LMI \n",
    "In this tutorial, you will use LMI container from DLC to SageMaker and run inference with it.\n",
    "\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "- S3 bucket push access\n",
    "- SageMaker access\n",
    "\n",
    "## Step 1: Let's bump up SageMaker and import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Step 2: Start preparing model artifacts\n",
    "In LMI contianer, we expect some artifacts to help setting up the model\n",
    "- serving.properties (required): Defines the model server settings\n",
    "- model.py (optional): A python file to define the core inference logic\n",
    "- requirements.txt (optional): Any additional pip wheel need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011bf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=MPI\n",
    "option.model_id=TheBloke/Llama-2-7B-Chat-fp16\n",
    "option.task=text-generation\n",
    "option.trust_remote_code=true\n",
    "option.tensor_parallel_degree=1\n",
    "option.max_rolling_batch_size=32\n",
    "option.rolling_batch=lmi-dist\n",
    "option.dtype=fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ab053",
   "metadata": {},
   "source": [
    "In this step, we will try to override the [default HuggingFace handler](https://github.com/deepjavalibrary/djl-serving/blob/0.25.0-dlc/engines/python/setup/djl_python/huggingface.py) provided by DJLServing. We will add an extra parameter checker called `password` to see if password is correct in the payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6798b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "from djl_python.huggingface import HuggingFaceService\n",
    "from djl_python import Output\n",
    "from djl_python.encode_decode import encode, decode\n",
    "from transformers import AutoTokenizer\n",
    "import logging\n",
    "import json\n",
    "import types\n",
    "\n",
    "_service = HuggingFaceService()\n",
    "\n",
    "def custom_parse_input(self, inputs):\n",
    "    input_data = []\n",
    "    input_size = []\n",
    "    parameters = []\n",
    "    errors = {}\n",
    "    # used for chat completion\n",
    "    if self.tokenizer is None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.hf_configs.model_id_or_path)\n",
    "    batch = inputs.get_batches()\n",
    "    for i, item in enumerate(batch):\n",
    "        try:\n",
    "            content_type = item.get_property(\"Content-Type\")\n",
    "            input_map = decode(item, content_type)\n",
    "        except Exception as e:  # pylint: disable=broad-except\n",
    "            logging.warning(f\"Parse input failed: {i}\")\n",
    "            input_size.append(0)\n",
    "            errors[i] = str(e)\n",
    "            continue\n",
    "        # Chat message masssaging\n",
    "        chat = input_map.pop(\"chat\", [])\n",
    "        if len(chat) != 0:\n",
    "            formatted_str = self.tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "            input_data.extend([formatted_str])\n",
    "        else:\n",
    "            input_data.extend([\"\"])\n",
    "        input_size.append(1)\n",
    "        # End of massaging\n",
    "        _param = input_map.pop(\"parameters\", {})\n",
    "        if not \"seed\" in _param:\n",
    "            # set server provided seed if seed is not part of request\n",
    "            if item.contains_key(\"seed\"):\n",
    "                _param[\"seed\"] = item.get_as_string(key=\"seed\")\n",
    "        for _ in range(input_size[i]):\n",
    "            parameters.append(_param)\n",
    "\n",
    "    return input_data, input_size, parameters, errors, batch\n",
    "\n",
    "\n",
    "def chat_output_formatter(token, first_token, last_token, details, generated_tokens):\n",
    "    \"\"\"\n",
    "    json output formatter\n",
    "\n",
    "    :return: formatted output\n",
    "    \"\"\"\n",
    "    json_encoded_str = f\"{{\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"\" if first_token else \"\"\n",
    "    json_encoded_str = f\"{json_encoded_str}{json.dumps(token.text, ensure_ascii=False)[1:-1]}\"\n",
    "    if last_token:\n",
    "        if details:\n",
    "            details_str = f\"\\\"details\\\": {json.dumps(details, ensure_ascii=False)}\"\n",
    "            json_encoded_str = f\"{json_encoded_str}\\\", {details_str}}}\"\n",
    "        else:\n",
    "            json_encoded_str = f\"{json_encoded_str}\\\"}}\"\n",
    "\n",
    "    return json_encoded_str\n",
    "\n",
    "\n",
    "def handle(inputs):\n",
    "    if not _service.initialized:\n",
    "        props = inputs.get_properties()\n",
    "        props[\"output_formatter\"] = chat_output_formatter\n",
    "        _service.initialize(inputs.get_properties())\n",
    "        # replace parse_input\n",
    "        _service.parse_input = types.MethodType(custom_parse_input, _service)\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # initialization request\n",
    "        return None\n",
    "\n",
    "    return _service.inference(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0142973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "mv model.py mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 3: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d955679",
   "metadata": {},
   "source": [
    "### Getting the container image URI\n",
    "\n",
    "[Large Model Inference available DLC](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a174b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11601839",
   "metadata": {},
   "source": [
    "### Upload artifact on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1e5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f39f6",
   "metadata": {},
   "source": [
    "### 4.2 Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e61cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.12xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-model\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "             # container_startup_health_check_timeout=3600\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63ee65",
   "metadata": {},
   "source": [
    "## Step 5: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69f9b5-d9c8-438a-9d7a-9ca37286465b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Tell me a story about Albert Einstein \"},\n",
    "]\n",
    "\n",
    "# NOTE: If you do not want the log probability to be printed in your output, please set, \"do_sample\":False\n",
    "\n",
    "result = predictor.predict(\n",
    "    {\"chat\": chat, \"parameters\": {\"do_sample\": True, \"max_new_tokens\": 600}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63852d39-63d5-47ae-8f77-4d2f35e73de7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assistant_response = {\"role\": result[\"role\"], \"content\": result[\"content\"]}\n",
    "\n",
    "print(f'{result[\"role\"]}: {result[\"content\"]}')\n",
    "\n",
    "chat.append(assistant_response)\n",
    "# next\n",
    "user_input = {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "print(f'{user_input[\"role\"]}: {user_input[\"content\"]}')\n",
    "chat.append(user_input)\n",
    "result = predictor.predict(\n",
    "    {\"chat\": chat, \"parameters\": {\"do_sample\": True, \"details\": True, \"max_new_tokens\": 512}}\n",
    ")\n",
    "print(f'{result[\"role\"]}: {result[\"content\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
