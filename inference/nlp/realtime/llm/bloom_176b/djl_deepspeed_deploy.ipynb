{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22007eff",
   "metadata": {},
   "source": [
    "\n",
    "### Serve large models on SageMaker with DeepSpeed Container\n",
    "\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the latest container launched using from DeepSpeed.\n",
    "\n",
    "Language models have recently exploded in both size and popularity. In 2018, BERT-large entered the scene and, with its 340M parameters and novel transformer architecture, set the standard on NLP task accuracy. Within just a few years, state-of-the-art NLP model size has grown by more than 500x with models such as OpenAI’s 175 billion parameter GPT-3 and similarly sized open source Bloom 176B raising the bar on NLP accuracy. This increase in the number of parameters is driven by the simple and empirically-demonstrated positive relationship between model size and accuracy: more is better. With easy access from models zoos such as Hugging Face and improved accuracy in NLP tasks such as classification and text generation, practitioners are increasingly reaching for these large models. However, deploying them can be a challenge because of their size.\n",
    "\n",
    "Model parallelism can help deploy large models that would normally be too large for a single GPU. With model parallelism, we partition and distribute a model across multiple GPUs. Each GPU holds a different part of the model, resolving the memory capacity issue for the largest deep learning models with billions of parameters. This notebook uses tensor parallelism techniques which allow GPUs to work simultaneously on the same layer of a model and achieve low latency inference relative to a pipeline parallel solution.\n",
    "\n",
    "SageMaker has rolled out DeepSpeed container which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting\n",
    "\n",
    "In this notebook, we deploy the open source Bloom 176B quantized model across GPU's on a ml.p4d.24xlarge instance. DeepSpeed is used for tensor parallelism inference while DJLServing handles inference requests and the distributed workers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c792ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instal boto3 library to create model and run inference workloads\n",
    "%pip install -Uqq boto3 awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6a015",
   "metadata": {},
   "source": [
    "## Setup Docker Image\n",
    "This section should be removed after DLC release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "docker pull deepjavalibrary/djl-serving:0.19.0-deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# The name of our algorithm\n",
    "image_id=$(docker images | grep 0.19.0-deepspeed | tr -s \" \" | cut -d \" \" -f 3)\n",
    "echo \"image_id=${image_id}\"\n",
    "\n",
    "\n",
    "repo_name='djl-ds' # same as algorithim name\n",
    "echo \"repo_name=$repo_name\"\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${repo_name}:latest\"\n",
    "echo \"Full_name=$fullname\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${repo_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${repo_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "image_id=$(docker images | grep 0.19.0-deepspeed | tr -s \" \" | cut -d \" \" -f 3)\n",
    "\n",
    "echo \"image_id=${image_id}\"\n",
    "\n",
    "#docker build -q -t ${algorithm_name} .\n",
    "\n",
    "docker tag $image_id ${fullname}\n",
    "#docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30461683",
   "metadata": {},
   "source": [
    "## Optional Section to Download Model from Hugging Face Hub\n",
    "\n",
    "Use this section of you are interested in downloading the model directly from Huggingface hub and storing in your own S3 bucket. \n",
    "\n",
    "**However this notebook currently leverages the model stored in AWS public S3 location for ease of use. So you can skip this step**\n",
    "\n",
    "The below step to download and then upload to S3 can take several minutes since the model size is extremely large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface-hub -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f57b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - This will download the model into the ./model directory where ever the jupyter file is running\n",
    "local_model_path = Path(\"./model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"microsoft/bloom-deepspeed-inference-int8\"\n",
    "commit_hash = \"aa00a6626f6484a2eef68e06d1e089e4e32aa571\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d98a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "snapshot_download(repo_id=model_name, \n",
    "                  revision=commit_hash,\n",
    "                  cache_dir=local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e564a",
   "metadata": {},
   "source": [
    "#### Upload to S3 using the awscli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_prefix = \"hf-large-model-djl-ds/model\" # folder where model checkpoint will go\n",
    "model_snapshot_path = list(local_model_path.glob(\"**/snapshots/*\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive {model_snapshot_path} s3://{bucket}/{s3_model_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e04811",
   "metadata": {},
   "source": [
    "## Create SageMaker compatible Model artifact and Upload Model to S3\n",
    "\n",
    "SageMaker needs the model to be in a Tarball format. In this notebook we are going to create the model with the Inference code to shorten the end point creation time. In the Inference code we kick of a multi threaded approach to download the model weights into the container using awscli\n",
    "\n",
    "The tarball is in the following format\n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── model.py\n",
    "│   └── requirements.txt\n",
    "│   └── serving.properties\n",
    "\n",
    "```\n",
    "\n",
    "The actual model is stored in S3 location and will be downloaded into the container directly when the endpoint is created. For that we will pass in 2 environment variables\n",
    "\n",
    "1.  \"MODEL_S3_BUCKET\" : Specify the S3 Bucket where the model artifact is\n",
    "2.  \"MODEL_S3_PREFIX\" : Specify the S3 prefix for where the model artifacts file are actually located\n",
    "\n",
    "This will be used in the model.py file to read in the actual model artifacts. \n",
    "\n",
    "- `model.py` is the key file which will handle any requests for serving. It is also responsible for loading the model from S3\n",
    "- `requirements.txt` has the awscli library needed to be installed when the container starts up.\n",
    "- `serving.properties` is the script that will have environment variables which can be used to customize model.py at run time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d0f11",
   "metadata": {},
   "source": [
    "#### Create required variables and initialize them to create the endpoint, we leverage boto3 for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()      # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()         # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()             # bucket to house artifacts\n",
    "s3_code_prefix = \"hf-large-model-djl-ds/code\"       # folder within bucket where code artifact will go\n",
    "s3_model_prefix = 'bloom-176B/raw_model_microsoft/' # folder where model checkpoint will go\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/djl-ds:latest\"\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n",
    "# 622343165275.dkr.ecr.us-east-1.amazonaws.com/djl-ds:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7697dd5",
   "metadata": {},
   "source": [
    "**Create the Tarball and then upload to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edeed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2605858",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c2b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_prefix = 'bloom-176B/raw_model_microsoft/'\n",
    "print(f\"S3 Model Prefix where the model files are -- > {s3_model_prefix}\")\n",
    "print(f\"S3 Model Bucket is -- > {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d497aeb",
   "metadata": {},
   "source": [
    "### This is optional in case you want to use VpcConfig to specify when creating the end points\n",
    "\n",
    "For more details you can refer to this link https://docs.aws.amazon.com/sagemaker/latest/dg/host-vpc.html\n",
    "\n",
    "The below is just an example to extract information about Security Groups and Subnets needed to configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 describe-security-groups --filter Name=vpc-id,Values=<use vpcId> | python3 -c \"import sys, json; print(json.load(sys.stdin)['SecurityGroups'])\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30880071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - provide networking configs if needed. \n",
    "security_group_ids = [] # add the security group id's\n",
    "subnets = [] # add the subnet id for this vpc\n",
    "privateVpcConfig={\n",
    "    'SecurityGroupIds': security_group_ids, \n",
    "    'Subnets': subnets\n",
    "}\n",
    "print(privateVpcConfig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7472147",
   "metadata": {},
   "source": [
    "### To create the end point the steps are:\n",
    "\n",
    "1. Create the Model using the Image container and the Model Tarball uploaded earlier\n",
    "2. Create the endpoint config using the following key parameters\n",
    "\n",
    "    a) Instance Type is ml.p4d.24xlarge \n",
    "    \n",
    "    b) ModelDataDownloadTimeoutInSeconds is 2400 which is needed to ensure the Model downloads from S3 successfully,\n",
    "    \n",
    "    c) ContainerStartupHealthCheckTimeoutInSeconds is 2400 to ensure health check starts after the model is ready\n",
    "    \n",
    "3. Create the end point using the endpoint config created    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b6394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "model_name = name_from_base(f\"bloom-djl-ds\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact,\n",
    "        \"Environment\": {\n",
    "            \"MODEL_S3_BUCKET\": bucket,\n",
    "            \"MODEL_S3_PREFIX\": s3_model_prefix,\n",
    "            \"TENSOR_PARALLEL_DEGREE\": \"8\"\n",
    "        },\n",
    "    },\n",
    "    # Uncomment if providing networking configs\n",
    "    #VpcConfig=privateVpcConfig\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01edbce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.p4d.24xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"VolumeSizeInGB\" : 200\n",
    "            'ModelDataDownloadTimeoutInSeconds': 2400,\n",
    "            'ContainerStartupHealthCheckTimeoutInSeconds': 2400\n",
    "        },\n",
    "    ],\n",
    "     \n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6583b468",
   "metadata": {},
   "source": [
    "#### Wait for the end point to be created.This can be take couple of minutes or longer. Please be patient\n",
    "However while that happens, let us look at the critical areas of the helper files we are using to load the model\n",
    "1. We will look at the code snippets for model.py to see the model downloading mechanism\n",
    "2. Requirements.txt to see the required libraries to be loaded\n",
    "3. Serving.properties to see the environment related properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7901353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code snippet which is responsible to load the model from S3\n",
    "! sed -n '26,34p' code/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb4cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code snippet which loads the libraries into the container needed for run\n",
    "! sed -n '1,3p' code/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d96ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code snippet which shows the environment variables being used to customize runtime\n",
    "! sed -n '1,3p' code/serving.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34055a69",
   "metadata": {},
   "source": [
    "#### Leverage the Boto3 to invoke the endpoint. \n",
    "\n",
    "This is a generative model so we pass in a Text as a prompt and Model will complete the sentence and return the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"input\": \"Amazon.com is the best \", \"gen_kwargs\": {\"min_length\":5, \"max_new_tokens\": 100, \"temperature\": 0.8, \"num_beams\": 5, \"no_repeat_ngram_size\": 2} }),\n",
    "    ContentType='application/json'\n",
    ")[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8836407",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db12c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Delete the end point \n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - In case the end point failed we still want to delete the model \n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442411f4",
   "metadata": {},
   "source": [
    "#### Optionally delete the model checkpoint from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b1c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive s3://{bucket}/{s3_model_prefix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b58b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s3_client.list_objects(Bucket=bucket, Prefix=f\"{s3_model_prefix}/\")[\"Contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6821df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
