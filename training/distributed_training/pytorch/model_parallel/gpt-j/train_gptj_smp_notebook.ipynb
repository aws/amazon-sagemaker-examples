{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train EleutherAI GPT-J with PyTorch 1.8.1 and Pipeline Parallelism Using the SageMaker Model Parallelism Library\n",
    "\n",
    "**Please run this notebook with Data Science-> Python 3 Kernel on SageMaker Studio Notebook**\n",
    "\n",
    "This notebook walks you through how to train the [EleutherAI's](https://www.eleuther.ai/) [GPT-J](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/) model with SageMaker's model parallelism.\n",
    "EleutherAI released GPT-J 6B, an open-source alternative to [OpenAIs GPT-3](https://openai.com/blog/gpt-3-apps/). [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6B) is the 6 billion parameter successor to EleutherAIs GPT-NEO family, a family of transformer-based language models based on the GPT architecture for text generation.\n",
    "\n",
    "EleutherAI's primary goal is to train a model that is equivalent in size to GPT⁠-⁠3 and make it available to the public under an open license.\n",
    "Over the last few months, GPT-J gained a lot of interest from Researchers, Data Scientists, and even Software Developers, but it remained very challenging to fine tune GPT-J.\n",
    "\n",
    "The weights of the 6 billion parameter model represent a ~24GB memory footprint. To load it in float32, one would need at least 2x model size CPU RAM: 1x for initial weights and another 1x to load the checkpoint. Apart from the model parameters, there are the gradients, optimizer states, and activations taking memory, so the actual memory usage might be significantly higher than 48GB. Just as an example, with Adam optimizer and FP32 training, the use from parameters, gradients and optimizer states might be 96GB+, and activation memory footprint would be even more than this, so the total memory usage might be easily larger than 200 GB.\n",
    "\n",
    "In this notebook, you will learn how to easily fine tune GPT-J using Amazon SageMaker and Hugging Face on NVIDIA GPU instances.\n",
    "\n",
    "This notebook depends on the following files and folders:\n",
    "\n",
    "1. `train_gptj_smp_script.py`: This is an entrypoint script that is passed to the PyTorch estimator in the notebook instructions. This script is responsible for end to end training of the GPT-J model with SMP. The script has additional comments at places where the SMP API is used.\n",
    "2. `fp16`: This folder is used for 16-bit float training, which contains a fp16 optimizer and various fp16 utilities.\n",
    "3. `learning_rates.py`: This contains the functions for learning rate schedule.\n",
    "4. `requirements.txt`: This will install the dependencies, like the right version of huggingface transformers.\n",
    "5. `preprocess.py`: This will download and preprocess the sst2/glue dataset.\n",
    "6. `args.py`: collection of difference arguments like training, data, SageMaker Model Parallel related args.\n",
    "7. `smp_trainer.py`.py: Defines the SageMaker Model Parallel Trainer class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Distributed Training \n",
    "\n",
    "SageMaker provides distributed training libraries for data parallelism and model parallelism. The libraries are optimized for the SageMaker training environment, help adapt your distributed training jobs to SageMaker, and improve training speed and throughput.\n",
    "\n",
    "### Approaches\n",
    "\n",
    "![SageMaker Distributed Training Approaches](TypesOfDistributedTraining.png)\n",
    "\n",
    "\n",
    "### SageMaker Model Parallel\n",
    "\n",
    "Model parallelism is the process of splitting a model up between multiple devices or nodes (such as GPU-equipped instances) and creating an efficient pipeline to train the model across these devices to maximize GPU utilization.\n",
    "\n",
    "Increasing deep learning model size (layers and parameters) can result in better accuracy. However, there is a limit to the maximum model size you can fit in a single GPU. When training deep learning models, GPU memory limitations can be a bottleneck in the following ways:\n",
    "\n",
    "1. They can limit the size of the model you train. Given that larger models tend to achieve higher accuracy, this directly translates to trained model accuracy.\n",
    "\n",
    "2. They can limit the batch size you train with, leading to lower GPU utilization and slower training.\n",
    "\n",
    "To overcome the limitations associated with training a model on a single GPU, you can use model parallelism to distribute and train your model on multiple computing devices.\n",
    "\n",
    "### Core features of SageMaker Model Parallel \n",
    "\n",
    "1. [Automated Model Splitting](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html): When you use SageMaker's model parallel library, you can take advantage of automated model splitting, also referred to as automated model partitioning. The library uses a partitioning algorithm that balances memory, minimizes communication between devices, and optimizes performance. You can configure the automated partitioning algorithm to optimize for speed or memory.\n",
    "\n",
    "2. [Pipeline Execution Schedule](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html): A core feature of SageMaker's distributed model parallel library is pipelined execution, which determines the order in which computations are made and data is processed across devices during model training. Pipelining is a technique to achieve true parallelization in model parallelism, by having the GPUs compute simultaneously on different data samples, and to overcome the performance loss due to sequential computation.\n",
    "\n",
    "Pipelining is based on splitting a mini-batch into microbatches, which are fed into the training pipeline one-by-one and follow an execution schedule defined by the library runtime. A microbatch is a smaller subset of a given training mini-batch. The pipeline schedule determines which microbatch is executed by which device for every time slot.\n",
    "\n",
    "In addition to its [core features](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html), the SageMaker distributed model parallel library offers [memory-saving features](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch.html) for training deep learning models with PyTorch: [tensor parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-tensor-parallelism.html), [optimizer state sharding](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-optimizer-state-sharding.html), [activation checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html), and [activation offloading](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-offloading.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Model Parallel configuration\n",
    "\n",
    "Please refer to all the [configuration parameters](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html) related to SageMaker Distributed Training.\n",
    "\n",
    "As we are going to use PyTorch and Hugging Face for training GPT-J, it is important to understand all the SageMaker Distributed configuration parameters specific to PyTorch [here](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#pytorch-specific-parameters).\n",
    "\n",
    "#### Important\n",
    "\n",
    "`process_per_host` must not be greater than the number of GPUs per instance and typically will be equal to the number of GPUs per instance.\n",
    "\n",
    "For example, if you use one instance with 4-way pipeline parallelism and 2-way data parallelism, then processes_per_host should be 2 x 4 = 8. Therefore, you must choose an instance that has at least 8 GPUs, such as an ml.p3.16xlarge.\n",
    "\n",
    "The following image illustrates how 4-way data parallelism and 2-way pipeline parallelism is distributed across 8 GPUs: the models is partitioned across 2 GPUs, and each partition is added to 4 GPUs.\n",
    "\n",
    "It is also important to understand how the [ranking mechanism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-ranking-mechanism.html) of model parallelism works with tensor parallelism. This is extended from the Ranking Basics for Core Features of the SageMaker Model Parallel Library.\n",
    "\n",
    "![SageMaker Distributed Training Approaches](SMP-Pipeline-Parallel-DDP.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Resources\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to learn more about SMP and using SageMaker with PyTorch.\n",
    "\n",
    "1. To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "2. To learn more about using the SageMaker Python SDK with PyTorch, see Using [PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "3. To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon SageMaker Initialization\n",
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment, such as your AWS account ID, the AWS Region, and the ARN of your Amazon SageMaker execution role.\n",
    "\n",
    "Upgrade SageMaker SDK to the latest version.\n",
    "\n",
    "NOTE: This step might require a kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.20.3)\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (2.81.0)\n",
      "Requirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.1.35)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
      "Requirement already satisfied: torchnet in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (0.0.4)\n",
      "Requirement already satisfied: transformers==4.15.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (4.15.0)\n",
      "Requirement already satisfied: smdebug in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (1.0.12)\n",
      "Requirement already satisfied: humanize in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (4.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (0.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (2022.3.15)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (4.63.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (0.0.49)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (2.26.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.12.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 1)) (2021.11.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.4)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.1.5)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: attrs==20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r requirements.txt (line 3)) (20.3.0)\n",
      "Requirement already satisfied: boto3>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.20.23)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.19.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.8)\n",
      "Requirement already satisfied: visdom in /opt/conda/lib/python3.7/site-packages (from torchnet->-r requirements.txt (line 6)) (0.1.8.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchnet->-r requirements.txt (line 6)) (1.14.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchnet->-r requirements.txt (line 6)) (1.11.0)\n",
      "Requirement already satisfied: pyinstrument==3.4.2 in /opt/conda/lib/python3.7/site-packages (from smdebug->-r requirements.txt (line 8)) (3.4.2)\n",
      "Requirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from pyinstrument==3.4.2->smdebug->-r requirements.txt (line 8)) (0.2.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.23 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker->-r requirements.txt (line 3)) (1.23.23)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker->-r requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0->-r requirements.txt (line 7)) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.15.0->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.15.0->-r requirements.txt (line 7)) (2.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2021.10.8)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.7.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.13.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (0.3.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.15.0->-r requirements.txt (line 7)) (0.14.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.15.0->-r requirements.txt (line 7)) (8.1.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (8.4.0)\n",
      "Requirement already satisfied: tornado in /opt/conda/lib/python3.7/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (6.1)\n",
      "Requirement already satisfied: jsonpatch in /opt/conda/lib/python3.7/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.32)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.7/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (0.59.0)\n",
      "Requirement already satisfied: pyzmq in /opt/conda/lib/python3.7/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (18.1.1)\n",
      "Requirement already satisfied: torchfile in /opt/conda/lib/python3.7/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (0.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.7/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 6)) (2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.81.0)\n",
      "Requirement already satisfied: boto3>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.20.23)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.19.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.20.3)\n",
      "Requirement already satisfied: attrs==20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.23 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.21->sagemaker) (1.23.23)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (2.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.20.21->sagemaker) (1.26.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker: 2.81.0\n",
      "transformers: 4.15.0\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import sagemaker\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "sagemaker_session = LocalSession()\n",
    "sagemaker_session.config = {\"local\": {\"local_code\": True}}\n",
    "\n",
    "print(f\"sagemaker: {sagemaker.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role: arn:aws:iam::232838030412:role/service-role/AmazonSageMaker-ExecutionRole-20211204T182243\n",
      "AWS account: 232838030412\n",
      "AWS region: us-west-2\n",
      "\n",
      "Default bucket for this session:  sagemaker-us-west-2-232838030412\n",
      "CPU times: user 560 ms, sys: 42.2 ms, total: 602 ms\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import boto3\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script fine-tunes GPT-J on the `sst2` dataset. \n",
    "\n",
    "#### DataLoader \n",
    "\n",
    "The DataLoader and Sampler is defined in `smp_trainer.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Hyperparameters\n",
    "We will train on 4-node p3.16xlarge cluster.  Total number of GPUs in the cluster will be 32.\n",
    "We will use 16-way pipeline parallelism and 2-way data parallel. Please note the `ddp=True` enables PyTorch's Distributed Data Parallel (DDP).\n",
    "\n",
    "`(pipeline parallelism degree) x (data parallelism degree) = total number of GPUs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"EleutherAI/gpt-j-6B\"\n",
    "pipeline_parallel_degree = 16\n",
    "ddp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"dataset_name\": \"glue\",\n",
    "    \"dataset_config_name\": \"sst2\",\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"output_dir\": \"./temp\",\n",
    "    \"model_name_or_path\": model_name_or_path,\n",
    "    \"max_steps\": 80,\n",
    "    \"seed\": 12345,\n",
    "    \"lr\": 2.0e-4,\n",
    "    \"lr_decay_iters\": 125000,\n",
    "    \"min_lr\": 0.00001,\n",
    "    \"warmup\": 0.01,\n",
    "    \"shard_optimizer_state\": 1,\n",
    "    \"activation_checkpointing\": 1,\n",
    "    \"activation_strategy\": \"each\",\n",
    "    \"optimize\": \"memory\",\n",
    "    \"pipeline_parallel_degree\": pipeline_parallel_degree,\n",
    "    \"ddp\": ddp,\n",
    "    \"prescaled_batch\": 0,\n",
    "    \"cache_dir\": \"/tmp\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SageMaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smp-ml-p3-16xlarge-EleutherAI-gpt-j-6B-2022-03-29-08-39-09\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import datetime\n",
    "\n",
    "instance_type = \"ml.p3.16xlarge\"\n",
    "volume_size = 900\n",
    "instance_count = 4\n",
    "\n",
    "cur_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "base_job_name = f\"smp-{instance_type}-{model_name_or_path}-{cur_time}\".replace(\".\", \"-\").replace(\n",
    "    \"/\", \"-\"\n",
    ")\n",
    "print(base_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify SageMaker Model Parallel Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_per_host = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "mpioptions += \"-x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    "mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\"\n",
    "\n",
    "\n",
    "mpi = {\n",
    "    \"enabled\": True,\n",
    "    \"processes_per_host\": process_per_host,\n",
    "    \"custom_mpi_options\": mpioptions,\n",
    "}\n",
    "\n",
    "smdistributed = {\n",
    "    \"modelparallel\": {\n",
    "        \"enabled\": True,\n",
    "        \"parameters\": {\n",
    "            \"ddp\": hyperparameters[\"ddp\"],\n",
    "            \"microbatches\": 2,\n",
    "            # partitions is a required param in the current SM SDK so it needs to be passed,\n",
    "            # these two map to the same config\n",
    "            \"partitions\": hyperparameters[\"pipeline_parallel_degree\"],\n",
    "            \"shard_optimizer_state\": hyperparameters[\"shard_optimizer_state\"] > 0,\n",
    "            \"prescaled_batch\": hyperparameters[\"prescaled_batch\"] > 0,\n",
    "            \"optimize\": hyperparameters[\"optimize\"],\n",
    "            \"auto_partition\": True,\n",
    "            \"default_partition\": 0,\n",
    "            \"offload_activations\": True,\n",
    "            \"active_microbatches\": 2,\n",
    "            \"optimize\": hyperparameters[\"optimize\"],\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "distribution = {\"mpi\": mpi, \"smdistributed\": smdistributed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell constructs a PyTorch estimator using the parameters defined above. To see how the SageMaker tensor parallelism modules and functions are applied to the script, see the `train_gptj_smp_script.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train_gptj_smp_script.py\",\n",
    "    source_dir=os.getcwd(),\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=volume_size,\n",
    "    instance_count=instance_count,\n",
    "    distribution=distribution,\n",
    "    framework_version=\"1.8.1\",\n",
    "    py_version=\"py36\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    base_job_name=base_job_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: smp-ml-p3-16xlarge-EleutherAI-gpt-j-6B--2022-03-29-08-39-26-552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-29 08:39:27 Starting - Starting the training job...\n",
      "2022-03-29 08:39:29 Starting - Launching requested ML instances.........\n",
      "2022-03-29 08:41:08 Starting - Preparing the instances for training.........\n",
      "2022-03-29 08:42:45 Downloading - Downloading input data\n",
      "2022-03-29 08:42:45 Training - Downloading the training image....................\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:06,954 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:07,030 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:12,067 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:12,144 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\n",
      "2022-03-29 08:46:15 Training - Training image download completed. Training in progress.\u001b[36mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[36mbash: no job control in this shell\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:13,777 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:13,852 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:16,921 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:16,618 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:16,694 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:16,703 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:17,248 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:17,510 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[36m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:18,376 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[36mCollecting datasets\n",
      "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (2.72.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.1.35)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.5.4)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: torchnet in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (0.0.4)\u001b[0m\n",
      "\u001b[34mCollecting datasets\n",
      "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (2.72.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.1.35)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchnet in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (0.0.4)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.15.0\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.0.9)\u001b[0m\n",
      "\u001b[34mCollecting humanize\n",
      "  Downloading humanize-3.14.0-py3-none-any.whl (98 kB)\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:18,878 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[36mCollecting transformers==4.15.0\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: smdebug in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.0.9)\u001b[0m\n",
      "\u001b[36mCollecting humanize\n",
      "  Downloading humanize-3.14.0-py3-none-any.whl (98 kB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (0.8)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (4.8.3)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (2.26.0)\u001b[0m\n",
      "\u001b[36mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (4.61.2)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.3.15-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (4.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (5.4.1)\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:19,681 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:20,192 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting datasets\n",
      "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (2.72.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.1.35)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.5.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torchnet in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (0.0.4)\u001b[0m\n",
      "\u001b[35mCollecting transformers==4.15.0\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: smdebug in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.0.9)\u001b[0m\n",
      "\u001b[35mCollecting humanize\n",
      "  Downloading humanize-3.14.0-py3-none-any.whl (98 kB)\u001b[0m\n",
      "\u001b[36mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.3.15-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (3.4.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (4.61.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (2.26.0)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (2021.7.0)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\n",
      "  Downloading responses-0.17.0-py2.py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.4)\u001b[0m\n",
      "\u001b[32mCollecting datasets\n",
      "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (2.72.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.1.35)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.5.4)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: torchnet in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (0.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (2.26.0)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (4.8.3)\u001b[0m\n",
      "\u001b[35mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[36mCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (21.3)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (5.4.1)\u001b[0m\n",
      "\u001b[36mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.1)\u001b[0m\n",
      "\u001b[36mCollecting responses<0.19\n",
      "  Downloading responses-0.17.0-py2.py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (2021.7.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.63.1-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3>=1.20.18 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.20.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: visdom in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (0.1.8.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument>=3.1.3 in /opt/conda/lib/python3.6/site-packages (from smdebug->-r requirements.txt (line 8)) (3.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.24.0,>=1.23.24 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (1.23.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0->-r requirements.txt (line 7)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.15.0->-r requirements.txt (line 7)) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers==4.15.0->-r requirements.txt (line 7)) (3.0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from pyinstrument>=3.1.3->smdebug->-r requirements.txt (line 8)) (0.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.6/site-packages (from tqdm>=4.27->transformers==4.15.0->-r requirements.txt (line 7)) (5.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (5.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna-ssl>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (1.6.6.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.15.0->-r requirements.txt (line 7)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.15.0->-r requirements.txt (line 7)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (8.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websocket-client in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.2.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyzmq in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (22.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchfile in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpatch in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.32)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.6/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 6)) (2.2)\u001b[0m\n",
      "\u001b[32mCollecting transformers==4.15.0\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: smdebug in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.0.9)\u001b[0m\n",
      "\u001b[32mCollecting humanize\n",
      "  Downloading humanize-3.14.0-py3-none-any.whl (98 kB)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.3.15-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (4.61.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (21.3)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (3.4.0)\u001b[0m\n",
      "\u001b[36mCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\u001b[0m\n",
      "\u001b[36mCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.63.1-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.4)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.19.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: boto3>=1.20.18 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.20.24)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: attrs in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (21.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.1.5)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (1.8.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: visdom in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (0.1.8.9)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyinstrument>=3.1.3 in /opt/conda/lib/python3.6/site-packages (from smdebug->-r requirements.txt (line 8)) (3.4.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (0.10.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: botocore<1.24.0,>=1.23.24 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (1.23.24)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (0.5.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0->-r requirements.txt (line 7)) (3.10.0.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.15.0->-r requirements.txt (line 7)) (3.6.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers==4.15.0->-r requirements.txt (line 7)) (3.0.6)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from pyinstrument>=3.1.3->smdebug->-r requirements.txt (line 8)) (0.2.4)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2021.5.30)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2.10)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2.0.4)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (1.26.6)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.6/site-packages (from tqdm>=4.27->transformers==4.15.0->-r requirements.txt (line 7)) (5.4.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.7.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (5.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.13.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: idna-ssl>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2021.3)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (1.6.6.4)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (0.3.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.15.0->-r requirements.txt (line 7)) (1.0.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.15.0->-r requirements.txt (line 7)) (8.0.3)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: tornado in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (6.1)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: jsonpatch in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.32)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: torchfile in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pyzmq in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (22.3.0)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (8.3.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: websocket-client in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.2.3)\u001b[0m\n",
      "\u001b[32mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.3.15-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (3.4.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (0.8)\u001b[0m\n",
      "\u001b[32mCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[32mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[35mCollecting responses<0.19\n",
      "  Downloading responses-0.17.0-py2.py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[35mCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.12.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (2021.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.4)\u001b[0m\n",
      "\u001b[35mCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.63.1-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: boto3>=1.20.18 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.20.24)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (21.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.1.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (1.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: visdom in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (0.1.8.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyinstrument>=3.1.3 in /opt/conda/lib/python3.6/site-packages (from smdebug->-r requirements.txt (line 8)) (3.4.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: botocore<1.24.0,>=1.23.24 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (1.23.24)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (0.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (0.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0->-r requirements.txt (line 7)) (3.10.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.15.0->-r requirements.txt (line 7)) (3.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers==4.15.0->-r requirements.txt (line 7)) (3.0.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from pyinstrument>=3.1.3->smdebug->-r requirements.txt (line 8)) (0.2.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2021.5.30)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (1.26.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.6/site-packages (from tqdm>=4.27->transformers==4.15.0->-r requirements.txt (line 7)) (5.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (5.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.13.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna-ssl>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.7.2)\u001b[0m\n",
      "\u001b[36mRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.6/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 6)) (2.2)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tqdm, regex, xxhash, tokenizers, sacremoses, responses, huggingface-hub, transformers, humanize, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[32mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (4.61.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (2.26.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (4.8.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (5.4.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers==4.15.0->-r requirements.txt (line 7)) (21.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[32mCollecting responses<0.19\n",
      "  Downloading responses-0.17.0-py2.py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.4)\u001b[0m\n",
      "\u001b[32mCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (2021.7.0)\u001b[0m\n",
      "\u001b[32mCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.63.1-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.12.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: boto3>=1.20.18 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.20.24)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: attrs in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (21.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.19.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.1.5)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: visdom in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (0.1.8.9)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchnet->-r requirements.txt (line 6)) (1.8.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyinstrument>=3.1.3 in /opt/conda/lib/python3.6/site-packages (from smdebug->-r requirements.txt (line 8)) (3.4.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2021.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (1.6.6.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (0.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.15.0->-r requirements.txt (line 7)) (8.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.15.0->-r requirements.txt (line 7)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tornado in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (6.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: websocket-client in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.2.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (8.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonpatch in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.32)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyzmq in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (22.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torchfile in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.6/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 6)) (2.2)\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tqdm, regex, xxhash, tokenizers, sacremoses, responses, huggingface-hub, transformers, humanize, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: botocore<1.24.0,>=1.23.24 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (1.23.24)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (0.5.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker->-r requirements.txt (line 3)) (0.10.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0->-r requirements.txt (line 7)) (3.10.0.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.15.0->-r requirements.txt (line 7)) (3.6.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers==4.15.0->-r requirements.txt (line 7)) (3.0.6)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from pyinstrument>=3.1.3->smdebug->-r requirements.txt (line 8)) (0.2.4)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2.10)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2.0.4)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (2021.5.30)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.15.0->-r requirements.txt (line 7)) (1.26.6)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.6/site-packages (from tqdm>=4.27->transformers==4.15.0->-r requirements.txt (line 7)) (5.4.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: idna-ssl>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (5.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.13.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.7.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2021.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (0.3.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (1.6.6.4)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.15.0->-r requirements.txt (line 7)) (1.0.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.15.0->-r requirements.txt (line 7)) (8.0.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: torchfile in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: websocket-client in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.2.3)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: jsonpatch in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.32)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: tornado in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (6.1)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (8.3.2)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: pyzmq in /opt/conda/lib/python3.6/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (22.3.0)\u001b[0m\n",
      "\u001b[32mRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.6/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 6)) (2.2)\u001b[0m\n",
      "\u001b[36mInstalling collected packages: tqdm, regex, xxhash, tokenizers, sacremoses, responses, huggingface-hub, transformers, humanize, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-2.0.0 huggingface-hub-0.4.0 humanize-3.14.0 regex-2022.3.15 responses-0.17.0 sacremoses-0.0.49 tokenizers-0.10.3 tqdm-4.63.1 transformers-4.15.0 xxhash-3.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:27,130 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:27,131 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:27,133 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:27,135 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:27,135 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.162.81\u001b[0m\n",
      "\u001b[32mInstalling collected packages: tqdm, regex, xxhash, tokenizers, sacremoses, responses, huggingface-hub, transformers, humanize, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[36mSuccessfully installed datasets-2.0.0 huggingface-hub-0.4.0 humanize-3.14.0 regex-2022.3.15 responses-0.17.0 sacremoses-0.0.49 tokenizers-0.10.3 tqdm-4.63.1 transformers-4.15.0 xxhash-3.0.0\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:28,137 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:28,137 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.162.81\u001b[0m\n",
      "\u001b[36mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:28,406 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:28,406 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:28,414 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:28,485 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:28,485 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:28,485 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:28,485 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:28,490 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:29,145 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:29,217 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:29,218 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:29,218 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:29,218 sagemaker-training-toolkit INFO     Cannot connect to host algo-3\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:29,218 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.179.176\u001b[0m\n",
      "\u001b[35mSuccessfully installed datasets-2.0.0 huggingface-hub-0.4.0 humanize-3.14.0 regex-2022.3.15 responses-0.17.0 sacremoses-0.0.49 tokenizers-0.10.3 tqdm-4.63.1 transformers-4.15.0 xxhash-3.0.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:28,899 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:28,899 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:28,907 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:28,963 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:28,964 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:28,964 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:28,964 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:28,967 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:30,220 sagemaker-training-toolkit INFO     Cannot connect to host algo-3\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:30,220 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.179.176\u001b[0m\n",
      "\u001b[32mSuccessfully installed datasets-2.0.0 huggingface-hub-0.4.0 humanize-3.14.0 regex-2022.3.15 responses-0.17.0 sacremoses-0.0.49 tokenizers-0.10.3 tqdm-4.63.1 transformers-4.15.0 xxhash-3.0.0\u001b[0m\n",
      "\u001b[32mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:30,148 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:30,148 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:30,155 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:30,214 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:30,214 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:30,214 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:30,215 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:30,218 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:31,229 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:31,296 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:31,296 sagemaker-training-toolkit INFO     Can connect to host algo-3\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:31,296 sagemaker-training-toolkit INFO     Worker algo-3 available for communication\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:31,304 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:31,377 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:31,377 sagemaker-training-toolkit INFO     Can connect to host algo-4\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:31,377 sagemaker-training-toolkit INFO     Worker algo-4 available for communication\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:31,377 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4'] Hosts: ['algo-1:8', 'algo-2:8', 'algo-3:8', 'algo-4:8'] process_per_hosts: 8 num_processes: 32\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:31,378 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2022-03-29 08:46:31,457 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8,\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"max_steps\": 80,\n",
      "        \"seed\": 12345,\n",
      "        \"lr\": 0.0002,\n",
      "        \"lr_decay_iters\": 125000,\n",
      "        \"activation_checkpointing\": 1,\n",
      "        \"do_eval\": false,\n",
      "        \"optimize\": \"memory\",\n",
      "        \"cache_dir\": \"/tmp\",\n",
      "        \"dataset_config_name\": \"sst2\",\n",
      "        \"output_dir\": \"./temp\",\n",
      "        \"shard_optimizer_state\": 1,\n",
      "        \"prescaled_batch\": 0,\n",
      "        \"activation_strategy\": \"each\",\n",
      "        \"min_lr\": 1e-05,\n",
      "        \"do_train\": true,\n",
      "        \"dataset_name\": \"glue\",\n",
      "        \"ddp\": true,\n",
      "        \"per_device_train_batch_size\": 2,\n",
      "        \"warmup\": 0.01,\n",
      "        \"model_name_or_path\": \"EleutherAI/gpt-j-6B\",\n",
      "        \"mp_parameters\": {\n",
      "            \"ddp\": true,\n",
      "            \"microbatches\": 2,\n",
      "            \"partitions\": 16,\n",
      "            \"shard_optimizer_state\": true,\n",
      "            \"prescaled_batch\": false,\n",
      "            \"optimize\": \"memory\",\n",
      "            \"auto_partition\": true,\n",
      "            \"default_partition\": 0,\n",
      "            \"offload_activations\": true,\n",
      "            \"active_microbatches\": 2\n",
      "        },\n",
      "        \"pipeline_parallel_degree\": 16\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"smp-ml-p3-16xlarge-EleutherAI-gpt-j-6B--2022-03-29-08-39-26-552\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-232838030412/smp-ml-p3-16xlarge-EleutherAI-gpt-j-6B--2022-03-29-08-39-26-552/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_gptj_smp_script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_gptj_smp_script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"activation_checkpointing\":1,\"activation_strategy\":\"each\",\"cache_dir\":\"/tmp\",\"dataset_config_name\":\"sst2\",\"dataset_name\":\"glue\",\"ddp\":true,\"do_eval\":false,\"do_train\":true,\"lr\":0.0002,\"lr_decay_iters\":125000,\"max_steps\":80,\"min_lr\":1e-05,\"model_name_or_path\":\"EleutherAI/gpt-j-6B\",\"mp_parameters\":{\"active_microbatches\":2,\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"microbatches\":2,\"offload_activations\":true,\"optimize\":\"memory\",\"partitions\":16,\"prescaled_batch\":false,\"shard_optimizer_state\":true},\"optimize\":\"memory\",\"output_dir\":\"./temp\",\"per_device_train_batch_size\":2,\"pipeline_parallel_degree\":16,\"prescaled_batch\":0,\"seed\":12345,\"shard_optimizer_state\":1,\"warmup\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_gptj_smp_script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_gptj_smp_script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-232838030412/smp-ml-p3-16xlarge-EleutherAI-gpt-j-6B--2022-03-29-08-39-26-552/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"hyperparameters\":{\"activation_checkpointing\":1,\"activation_strategy\":\"each\",\"cache_dir\":\"/tmp\",\"dataset_config_name\":\"sst2\",\"dataset_name\":\"glue\",\"ddp\":true,\"do_eval\":false,\"do_train\":true,\"lr\":0.0002,\"lr_decay_iters\":125000,\"max_steps\":80,\"min_lr\":1e-05,\"model_name_or_path\":\"EleutherAI/gpt-j-6B\",\"mp_parameters\":{\"active_microbatches\":2,\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"microbatches\":2,\"offload_activations\":true,\"optimize\":\"memory\",\"partitions\":16,\"prescaled_batch\":false,\"shard_optimizer_state\":true},\"optimize\":\"memory\",\"output_dir\":\"./temp\",\"per_device_train_batch_size\":2,\"pipeline_parallel_degree\":16,\"prescaled_batch\":0,\"seed\":12345,\"shard_optimizer_state\":1,\"warmup\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"smp-ml-p3-16xlarge-EleutherAI-gpt-j-6B--2022-03-29-08-39-26-552\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-232838030412/smp-ml-p3-16xlarge-EleutherAI-gpt-j-6B--2022-03-29-08-39-26-552/source/sourcedir.tar.gz\",\"module_name\":\"train_gptj_smp_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_gptj_smp_script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--activation_checkpointing\",\"1\",\"--activation_strategy\",\"each\",\"--cache_dir\",\"/tmp\",\"--dataset_config_name\",\"sst2\",\"--dataset_name\",\"glue\",\"--ddp\",\"True\",\"--do_eval\",\"False\",\"--do_train\",\"True\",\"--lr\",\"0.0002\",\"--lr_decay_iters\",\"125000\",\"--max_steps\",\"80\",\"--min_lr\",\"1e-05\",\"--model_name_or_path\",\"EleutherAI/gpt-j-6B\",\"--mp_parameters\",\"active_microbatches=2,auto_partition=True,ddp=True,default_partition=0,microbatches=2,offload_activations=True,optimize=memory,partitions=16,prescaled_batch=False,shard_optimizer_state=True\",\"--optimize\",\"memory\",\"--output_dir\",\"./temp\",\"--per_device_train_batch_size\",\"2\",\"--pipeline_parallel_degree\",\"16\",\"--prescaled_batch\",\"0\",\"--seed\",\"12345\",\"--shard_optimizer_state\",\"1\",\"--warmup\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=80\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=12345\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LR_DECAY_ITERS=125000\u001b[0m\n",
      "\u001b[34mSM_HP_ACTIVATION_CHECKPOINTING=1\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=false\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZE=memory\u001b[0m\n",
      "\u001b[34mSM_HP_CACHE_DIR=/tmp\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_CONFIG_NAME=sst2\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=./temp\u001b[0m\n",
      "\u001b[34mSM_HP_SHARD_OPTIMIZER_STATE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PRESCALED_BATCH=0\u001b[0m\n",
      "\u001b[34mSM_HP_ACTIVATION_STRATEGY=each\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_LR=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=glue\u001b[0m\n",
      "\u001b[34mSM_HP_DDP=true\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=EleutherAI/gpt-j-6B\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"active_microbatches\":2,\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"microbatches\":2,\"offload_activations\":true,\"optimize\":\"memory\",\"partitions\":16,\"prescaled_batch\":false,\"shard_optimizer_state\":true}\u001b[0m\n",
      "\u001b[34mSM_HP_PIPELINE_PARALLEL_DEGREE=16\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8,algo-3:8,algo-4:8 -np 32 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_HP_MAX_STEPS -x SM_HP_SEED -x SM_HP_LR -x SM_HP_LR_DECAY_ITERS -x SM_HP_ACTIVATION_CHECKPOINTING -x SM_HP_DO_EVAL -x SM_HP_OPTIMIZE -x SM_HP_CACHE_DIR -x SM_HP_DATASET_CONFIG_NAME -x SM_HP_OUTPUT_DIR -x SM_HP_SHARD_OPTIMIZER_STATE -x SM_HP_PRESCALED_BATCH -x SM_HP_ACTIVATION_STRATEGY -x SM_HP_MIN_LR -x SM_HP_DO_TRAIN -x SM_HP_DATASET_NAME -x SM_HP_DDP -x SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE -x SM_HP_WARMUP -x SM_HP_MODEL_NAME_OR_PATH -x SM_HP_MP_PARAMETERS -x SM_HP_PIPELINE_PARALLEL_DEGREE -x PYTHONPATH /opt/conda/bin/python3.6 -m mpi4py train_gptj_smp_script.py --activation_checkpointing 1 --activation_strategy each --cache_dir /tmp --dataset_config_name sst2 --dataset_name glue --ddp True --do_eval False --do_train True --lr 0.0002 --lr_decay_iters 125000 --max_steps 80 --min_lr 1e-05 --model_name_or_path EleutherAI/gpt-j-6B --mp_parameters active_microbatches=2,auto_partition=True,ddp=True,default_partition=0,microbatches=2,offload_activations=True,optimize=memory,partitions=16,prescaled_batch=False,shard_optimizer_state=True --optimize memory --output_dir ./temp --per_device_train_batch_size 2 --pipeline_parallel_degree 16 --prescaled_batch 0 --seed 12345 --shard_optimizer_state 1 --warmup 0.01\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:32,498 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=58, name='orted', status='disk-sleep', started='08:46:32')]\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:32,498 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=58, name='orted', status='disk-sleep', started='08:46:32')]\u001b[0m\n",
      "\u001b[36m2022-03-29 08:46:32,498 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=58, name='orted', status='disk-sleep', started='08:46:32')]\u001b[0m\n",
      "\u001b[34m Data for JOB [41135,1] offset 0 Total slots allocated 32\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 15 Bound: N/A\n",
      " Data for node: algo-3#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 16 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 17 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 18 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 19 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 20 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 21 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 22 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 23 Bound: N/A\n",
      " Data for node: algo-4#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 24 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 25 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 26 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 27 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 28 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 29 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 30 Bound: N/A\n",
      " #011Process OMPI jobid: [41135,1] App: 0 Process rank: 31 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:33,226 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=58, name='orted', status='sleeping', started='08:46:31')]\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:33,226 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=58, name='orted', status='sleeping', started='08:46:31')]\u001b[0m\n",
      "\u001b[32m2022-03-29 08:46:33,226 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=58, name='orted', status='sleeping', started='08:46:31')]\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:32,977 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=58, name='orted', status='sleeping', started='08:46:31')]\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:32,977 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=58, name='orted', status='sleeping', started='08:46:31')]\u001b[0m\n",
      "\u001b[35m2022-03-29 08:46:32,978 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=58, name='orted', status='sleeping', started='08:46:31')]\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2022-03-29 08:46:39.382: I smdistributed/modelparallel/torch/state_mod.py:163] [6] Finished initializing torch distributed process groups. pp_rank: 6, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2022-03-29 08:46:39.382: I smdistributed/modelparallel/torch/state_mod.py:163] [7] Finished initializing torch distributed process groups. pp_rank: 7, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,26]<stdout>:[2022-03-29 08:46:39.382: I smdistributed/modelparallel/torch/state_mod.py:163] [26] Finished initializing torch distributed process groups. pp_rank: 10, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,31]<stdout>:[2022-03-29 08:46:39.382: I smdistributed/modelparallel/torch/state_mod.py:163] [31] Finished initializing torch distributed process groups. pp_rank: 15, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2022-03-29 08:46:39.383: I smdistributed/modelparallel/torch/state_mod.py:163] [2] Finished initializing torch distributed process groups. pp_rank: 2, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2022-03-29 08:46:39.383: I smdistributed/modelparallel/torch/state_mod.py:163] [3] Finished initializing torch distributed process groups. pp_rank: 3, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,28]<stdout>:[2022-03-29 08:46:39.383: I smdistributed/modelparallel/torch/state_mod.py:163] [28] Finished initializing torch distributed process groups. pp_rank: 12, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,30]<stdout>:[2022-03-29 08:46:39.384: I smdistributed/modelparallel/torch/state_mod.py:163] [30] Finished initializing torch distributed process groups. pp_rank: 14, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,25]<stdout>:[2022-03-29 08:46:39.384: I smdistributed/modelparallel/torch/state_mod.py:163] [25] Finished initializing torch distributed process groups. pp_rank: 9, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:39.384: I smdistributed/modelparallel/torch/state_mod.py:163] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,22]<stdout>:[2022-03-29 08:46:39.386: I smdistributed/modelparallel/torch/state_mod.py:163] [22] Finished initializing torch distributed process groups. pp_rank: 6, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:39.384: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 1.\u001b[0m\n",
      "\u001b[34m[1,24]<stdout>:[2022-03-29 08:46:39.384: I smdistributed/modelparallel/torch/state_mod.py:163] [24] Finished initializing torch distributed process groups. pp_rank: 8, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2022-03-29 08:46:39.384: I smdistributed/modelparallel/torch/state_mod.py:163] [4] Finished initializing torch distributed process groups. pp_rank: 4, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,29]<stdout>:[2022-03-29 08:46:39.386: I smdistributed/modelparallel/torch/state_mod.py:163] [29] Finished initializing torch distributed process groups. pp_rank: 13, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,17]<stdout>:[2022-03-29 08:46:39.387: I smdistributed/modelparallel/torch/state_mod.py:163] [17] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,18]<stdout>:[2022-03-29 08:46:39.387: I smdistributed/modelparallel/torch/state_mod.py:163] [18] Finished initializing torch distributed process groups. pp_rank: 2, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,27]<stdout>:[2022-03-29 08:46:39.386: I smdistributed/modelparallel/torch/state_mod.py:163] [27] Finished initializing torch distributed process groups. pp_rank: 11, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2022-03-29 08:46:39.386: I smdistributed/modelparallel/torch/state_mod.py:163] [10] Finished initializing torch distributed process groups. pp_rank: 10, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2022-03-29 08:46:39.386: I smdistributed/modelparallel/torch/state_mod.py:163] [12] Finished initializing torch distributed process groups. pp_rank: 12, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,23]<stdout>:[2022-03-29 08:46:39.388: I smdistributed/modelparallel/torch/state_mod.py:163] [23] Finished initializing torch distributed process groups. pp_rank: 7, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,19]<stdout>:[2022-03-29 08:46:39.388: I smdistributed/modelparallel/torch/state_mod.py:163] [19] Finished initializing torch distributed process groups. pp_rank: 3, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2022-03-29 08:46:39.386: I smdistributed/modelparallel/torch/state_mod.py:163] [14] Finished initializing torch distributed process groups. pp_rank: 14, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,16]<stdout>:[2022-03-29 08:46:39.388: I smdistributed/modelparallel/torch/state_mod.py:163] [16] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,20]<stdout>:[2022-03-29 08:46:39.388: I smdistributed/modelparallel/torch/state_mod.py:163] [20] Finished initializing torch distributed process groups. pp_rank: 4, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,21]<stdout>:[2022-03-29 08:46:39.389: I smdistributed/modelparallel/torch/state_mod.py:163] [21] Finished initializing torch distributed process groups. pp_rank: 5, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2022-03-29 08:46:39.387: I smdistributed/modelparallel/torch/state_mod.py:163] [11] Finished initializing torch distributed process groups. pp_rank: 11, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2022-03-29 08:46:39.387: I smdistributed/modelparallel/torch/state_mod.py:163] [9] Finished initializing torch distributed process groups. pp_rank: 9, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2022-03-29 08:46:39.389: I smdistributed/modelparallel/torch/state_mod.py:163] [1] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2022-03-29 08:46:39.389: I smdistributed/modelparallel/torch/state_mod.py:163] [8] Finished initializing torch distributed process groups. pp_rank: 8, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2022-03-29 08:46:39.390: I smdistributed/modelparallel/torch/state_mod.py:163] [13] Finished initializing torch distributed process groups. pp_rank: 13, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2022-03-29 08:46:39.391: I smdistributed/modelparallel/torch/state_mod.py:163] [15] Finished initializing torch distributed process groups. pp_rank: 15, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2022-03-29 08:46:39.392: I smdistributed/modelparallel/torch/state_mod.py:163] [5] Finished initializing torch distributed process groups. pp_rank: 5, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.591: I smdistributed/modelparallel/backend/config.py:217] Configuration parameters:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.592: I smdistributed/modelparallel/backend/config.py:220]   pipeline_parallel_degree: 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.592: I smdistributed/modelparallel/backend/config.py:220]   microbatches: 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.592: I smdistributed/modelparallel/backend/config.py:220]   pipeline: interleaved\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.592: I smdistributed/modelparallel/backend/config.py:220]   horovod: False\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.592: I smdistributed/modelparallel/backend/config.py:220]   ddp: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.592: I smdistributed/modelparallel/backend/config.py:220]   tensor_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.592: I smdistributed/modelparallel/backend/config.py:220]   ddp_port: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.592: I smdistributed/modelparallel/backend/config.py:220]   ddp_dist_backend: nccl\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.592: I smdistributed/modelparallel/backend/config.py:220]   contiguous: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.593: I smdistributed/modelparallel/backend/config.py:220]   placement_strategy: cluster\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.593: I smdistributed/modelparallel/backend/config.py:220]   optimize: memory\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.593: I smdistributed/modelparallel/backend/config.py:220]   default_partition: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.593: I smdistributed/modelparallel/backend/config.py:220]   auto_partition: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.593: I smdistributed/modelparallel/backend/config.py:220]   prescaled_batch: False\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.593: I smdistributed/modelparallel/backend/config.py:220]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.593: I smdistributed/modelparallel/backend/config.py:220]   active_microbatches: 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.593: I smdistributed/modelparallel/backend/config.py:220]   fp16_params: False\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.593: I smdistributed/modelparallel/backend/config.py:220]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.593: I smdistributed/modelparallel/backend/config.py:220]   offload_activations: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.594: I smdistributed/modelparallel/backend/config.py:220]   shard_optimizer_state: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.594: I smdistributed/modelparallel/backend/config.py:220]   skip_tracing: False\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:46:42.594: I smdistributed/modelparallel/backend/config.py:220]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[34m[1,25]<stdout>:Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34m[1,25]<stdout>:Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,24]<stdout>:max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,19]<stdout>:Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34m[1,19]<stdout>:Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.137: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 1.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.137: I smdistributed/modelparallel/backend/config.py:217] Configuration parameters:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.137: I smdistributed/modelparallel/backend/config.py:220]   pipeline_parallel_degree: 16\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.137: I smdistributed/modelparallel/backend/config.py:220]   microbatches: 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.137: I smdistributed/modelparallel/backend/config.py:220]   pipeline: interleaved\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.137: I smdistributed/modelparallel/backend/config.py:220]   horovod: False\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.138: I smdistributed/modelparallel/backend/config.py:220]   ddp: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.138: I smdistributed/modelparallel/backend/config.py:220]   tensor_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.138: I smdistributed/modelparallel/backend/config.py:220]   ddp_port: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.138: I smdistributed/modelparallel/backend/config.py:220]   ddp_dist_backend: nccl\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.138: I smdistributed/modelparallel/backend/config.py:220]   contiguous: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.138: I smdistributed/modelparallel/backend/config.py:220]   placement_strategy: cluster\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.138: I smdistributed/modelparallel/backend/config.py:220]   optimize: memory\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.138: I smdistributed/modelparallel/backend/config.py:220]   default_partition: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.139: I smdistributed/modelparallel/backend/config.py:220]   auto_partition: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.139: I smdistributed/modelparallel/backend/config.py:220]   prescaled_batch: False\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.139: I smdistributed/modelparallel/backend/config.py:220]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.139: I smdistributed/modelparallel/backend/config.py:220]   active_microbatches: 2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.139: I smdistributed/modelparallel/backend/config.py:220]   fp16_params: False\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.139: I smdistributed/modelparallel/backend/config.py:220]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.139: I smdistributed/modelparallel/backend/config.py:220]   offload_activations: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.139: I smdistributed/modelparallel/backend/config.py:220]   shard_optimizer_state: True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.140: I smdistributed/modelparallel/backend/config.py:220]   skip_tracing: False\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.140: I smdistributed/modelparallel/backend/config.py:220]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Arguments: {'pipeline_parallel_degree': 16, 'microbatches': 1, 'active_microbatches': None, 'optimize': 'memory', 'activation_strategy': 'each', 'shard_optimizer_state': 1, 'offload_activations': 0, 'fast_mode': 0, 'static_mode': 0, 'delayed_param': 0, 'same_partition_load': 0, 'attention_in_fp32': 1, 'ddp': True, 'activation_checkpointing': 1, 'prescaled_batch': 0, 'trace_device': 'cpu', 'match_weights': 0}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Transformers version: 4.15.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:smdistributed.modelparallel version: 1.6.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:smdistributed config: {'ddp': True, 'pipeline_parallel_degree': 16, 'microbatches': 2, 'shard_optimizer_state': True, 'prescaled_batch': False, '_match_weights': False, 'offload_activations': True, 'optimize': 'memory', 'auto_partition': True, 'default_partition': 0, 'static_mode': False, 'fast_mode': False, 'active_microbatches': 2}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# total parameters: 6050882784\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Learning rate decay style: linear\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.194: W smdistributed/modelparallel/backend/split.py:171] Object of type <class 'torch.optim.adam.Adam'> passed to smp.step. In normal use of SMP, this object should be used outside of smp.step. This might *potentially* be a bug.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.196: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'args.CustomTrainingArguments'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2022-03-29 08:55:17.198: I smdistributed/modelparallel/torch/worker.py:293] Tracing on CPU. For models whose parameters fit in a single device, setting trace_device to `gpu` is recommended for better partitioning decisions.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,16]<stdout>:max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,16]<stdout>:[2022-03-29 08:55:24.670: W smdistributed/modelparallel/backend/split.py:171] Object of type <class 'torch.optim.adam.Adam'> passed to smp.step. In normal use of SMP, this object should be used outside of smp.step. This might *potentially* be a bug.\u001b[0m\n",
      "\u001b[34m[1,16]<stdout>:[2022-03-29 08:55:24.672: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'args.CustomTrainingArguments'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "smp_estimator.fit(logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Training Logs\n",
    "\n",
    "You can access the training logs from [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html). Make sure to look at the logs of **algo-1** because that is the main node whose output stream will have the training job logs.\n",
    "\n",
    "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see [SageMaker Jobs and Endpoint Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-jobs) in the Amazon SageMaker Developer Guide.\n",
    "\n",
    "If you are a new user of CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html). \n",
    "\n",
    "For additional information on monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html).\n",
    "\n",
    "## Deploying Trained Model for Inference\n",
    "\n",
    "In most cases, a trained model can be deployed on a single device for inference because inference only requires a small amount of memory. You can use the SMP API to create a single, unified model after training: the [smp.DistributedModel.save_model()](https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/latest/smd_model_parallel_tensorflow.html#smp.DistributedModel.save_model) method for TensorFlow, and the [smp.save()](https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/latest/smd_model_parallel_pytorch.html#apis-for-saving-and-loading) function for PyTorch.\n",
    "\n",
    "After you build and train your models, you can deploy them to get predictions in one of two ways:\n",
    "\n",
    "* To set up a persistent endpoint to get predictions from your models, use SageMaker hosting services. For an overview on deploying a single model or multiple models with SageMaker hosting services, see [Deploy a Model on SageMaker Hosting Services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html#how-it-works-hosting).\n",
    "* To get predictions for an entire dataset, use SageMaker batch transform. For an overview on deploying a model with SageMaker Batch Transform, see [Get Inferences for an Entire Dataset with Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html).\n",
    "\n",
    "To learn more about deploying models for inference using SageMaker, see [Deploy Models for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html). \n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
