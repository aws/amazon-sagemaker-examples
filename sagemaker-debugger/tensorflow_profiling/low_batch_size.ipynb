{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to identify low GPU utilization due to small batch size\n",
    "\n",
    "In this notebook, we demonstrate how the profiling functionality of Amazon SageMaker Debugger can be used to identify under-utilization of the GPU resource, resulting from a low training batch size. We will demonstrate this using TensorFlow, on a ResNet50 model, and the CIFAR-10 dataset.\n",
    "The training script for this example is [demo/train_tf_bottleneck.py](./demo/train_tf_bottleneck.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install sagemaker\n",
    "To use the new Debugger profiling features, ensure that you have the latest version of SageMaker SDK installed. The following cell updates the library and restarts the Jupyter kernel to apply the updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "install_needed = True  # should only be True once\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install -U sagemaker\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare training dataset\n",
    "\n",
    "### Tensorflow Datasets package\n",
    "\n",
    "First of all, set the notebook kernel to Tensorflow 2.x.\n",
    "\n",
    "We will use CIFAR-10 dataset for this experiment. To download CIFAR-10 datasets and convert it into TFRecord format, run `demo/generate_cifar10_tfrecords`, and upload tfrecord files to your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python demo/generate_cifar10_tfrecords.py --data-dir=./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "s3_bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "dataset_prefix = \"data/cifar10-tfrecords\"\n",
    "desired_s3_uri = f\"s3://{s3_bucket}/{dataset_prefix}\"\n",
    "\n",
    "dataset_location = sagemaker.s3.S3Uploader.upload(local_path=\"data\", desired_s3_uri=desired_s3_uri)\n",
    "print(f\"Dataset uploaded to {dataset_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Training Job with Profiling Enabled<a class=\"anchor\" id=\"option-1\"></a>\n",
    "\n",
    "We will use the standard [SageMaker Estimator API for Tensorflow](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-estimator) to create a training job. To enable profiling, create a `ProfilerConfig` object and pass it to the `profiler_config` parameter of the `TensorFlow` estimator. In this case we set the profiling interval to be 500 miliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a profiler configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    "    framework_profile_params=FrameworkProfile(\n",
    "        local_path=\"/opt/ml/output/profiler/\", start_step=5, num_steps=2\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters\n",
    "\n",
    "The start_up script, [train_tf_bottleneck.py](./demo/train_tf_bottleneck), accepts a number of parameters. Here we set the batch_size to 64, and the number of epochs to 3 to keep the training short for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epoch\": 3,\n",
    "    \"batch_size\": batch_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the image URI\n",
    "The image that we will is dependent on the region that you are running this notebook in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SageMaker Tensorflow Estimator\n",
    "To enable profiling, you need to pass the Debugger profiling configuration (`profiler_config`), a list of Debugger rules (`rules`), and the image URI (`image_uri`) to the estimator. Debugger enables monitoring and profiling while the SageMaker estimator requests a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "job_name = f\"lowbatchsize-{batch_size}\"\n",
    "instance_count = 1\n",
    "instance_type = \"ml.p2.xlarge\"\n",
    "entry_script = \"train_tf_bottleneck.py\"\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    image_uri=image_uri,\n",
    "    base_job_name=job_name,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    entry_point=entry_script,\n",
    "    source_dir=\"demo\",\n",
    "    profiler_config=profiler_config,\n",
    "    script_mode=True,\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_mode=\"Pipe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you see an error, `TypeError: __init__() got an unexpected keyword argument 'instance_type'`, that means SageMaker Python SDK is out-dated. Please update your SageMaker Python SDK to 2.x by executing the below command and restart this notebook.\n",
    "\n",
    "```bash\n",
    "pip install --upgrade sagemaker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training job\n",
    "\n",
    "The following `estimator.fit()` with `wait=False` argument initiates the training job in the background. You can proceed to run the dashboard or analysis notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_inputs = {\"train\": dataset_location + \"/train\"}\n",
    "\n",
    "estimator.fit(remote_inputs, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monitor the system resource utilization using SageMaker Studio\n",
    "\n",
    "SageMaker Studio provides the visualization tool for Sagemaker Debugger, where you can find the analysis report and plots of the system and framework performance metrics.\n",
    "\n",
    "To access this information in SageMaker Studio, click on the last icon on the left to open `SageMaker Components and registries` and choose `Experiments and trials`. You will see the list of training jobs. Right click on the job you want to investigate shows a pop-up menu, then click on `Open Debugger for insights` which opens a new tap for SageMaker Debugger as below.\n",
    "\n",
    "![Debugger-in-Studio](./images/Debugger-in-Studio.png)\n",
    "\n",
    "There are two tabs, `Overview` and `Nodes`. `Overview` gives profiling summaries for quick review, and `Nodes` gives a detailed utilization information on all nodes.\n",
    "\n",
    "GPU and system utilization history found in `Nodes`, indicate that our GPU was under-utilized. GPU utilization was 60% and GPU Memory utilization was 20%.\n",
    "\n",
    "![low-GPU-utilization](./images/low-GPU-utilization.png)\n",
    "\n",
    "The first action to be taken in this case is to increase the batch size to push more examples to GPU. In this example, you can increase the batch size by changing a value of a hyperparemter and run the training job again. For example, change `batch_size` from 64 to 1024.\n",
    "\n",
    "```python\n",
    "hyperparameters = {'epoch': 20, \n",
    "                   'batch_size': 1024\n",
    "                  }\n",
    "```\n",
    "\n",
    "The system resouce utilization with batch size 1024 shows fully utilized GPU as in the following plot.\n",
    "\n",
    "![low-GPU-utilization-fixed](./images/low-GPU-utilization-fixed.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
