{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5ab391",
   "metadata": {},
   "source": [
    "# Serve large models on SageMaker with model parallel inference and DJLServing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b43ca5",
   "metadata": {},
   "source": [
    "In this notebook, we explore how to host a large language model on SageMaker using model parallelism from DeepSpeed and DJLServing.\n",
    "\n",
    "Language models have recently exploded in both size and popularity. In 2018, BERT-large entered the scene and, with its 340M parameters and novel transformer architecture, set the standard on NLP task accuracy. Within just a few years, state-of-the-art NLP model size has grown by more than 500x with models such as OpenAIâ€™s 175 billion parameter GPT-3 and similarly sized open source Bloom 176B raising the bar on NLP accuracy. This increase in the number of parameters is driven by the simple and empirically-demonstrated positive relationship between model size and accuracy: more is better. With easy access from models zoos such as Hugging Face and improved accuracy in NLP tasks such as classification and text generation, practitioners are increasingly reaching for these large models. However, deploying them can be a challenge because of their size.\n",
    "\n",
    "Model parallelism can help deploy large models that would normally be too large for a single GPU. With model parallelism, we partition and distribute a model across multiple GPUs. Each GPU holds a different part of the model, resolving the memory capacity issue for the largest deep learning models with billions of parameters. This notebook uses tensor parallelism techniques which allow GPUs to work simultaneously on the same layer of a model and achieve low latency inference relative to a pipeline parallel solution.\n",
    "\n",
    "In this notebook, we deploy a PyTorch GPT-J model from Hugging Face with 6 billion parameters across two GPUs on an Amazon SageMaker ml.g5.48xlarge instance. DeepSpeed is used for tensor parallelism inference while DJLServing handles inference requests and the distributed workers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c2bdf4",
   "metadata": {},
   "source": [
    "## Step 1: Creating image for SageMaker endpoint\n",
    "We first pull the docker image djl-serving:0.18.0-deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "docker pull deepjavalibrary/djl-serving:0.18.0-deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "73d0ff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                                   TAG                IMAGE ID       CREATED       SIZE\n",
      "084313272408.dkr.ecr.us-east-1.amazonaws.com/djl_deepspeed   latest             c2edba9c6d73   3 weeks ago   12.7GB\n",
      "deepjavalibrary/djl-serving                                  0.18.0-deepspeed   c2edba9c6d73   3 weeks ago   12.7GB\n"
     ]
    }
   ],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822977b",
   "metadata": {},
   "source": [
    "You should see the image `djl-serving` listed from running the code above. Please note the `IMAGE ID`. We will need it for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c695144",
   "metadata": {},
   "source": [
    "### Push image to ECR\n",
    "The following code pushes the `djl-serving` image, downloaded from previous step, to ECR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "47ab31d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "The push refers to repository [084313272408.dkr.ecr.us-east-1.amazonaws.com/djl_deepspeed]\n",
      "42737ae997a9: Preparing\n",
      "4e257e38b819: Preparing\n",
      "b16fc1914e14: Preparing\n",
      "7e20e12fff52: Preparing\n",
      "01b6a2323efe: Preparing\n",
      "55df8d287560: Preparing\n",
      "61d781c1452a: Preparing\n",
      "dbc3bf935e02: Preparing\n",
      "1a5fac543081: Preparing\n",
      "a8d0c4c62eef: Preparing\n",
      "7ed9a71261c7: Preparing\n",
      "a1eeba43cdbe: Preparing\n",
      "6127942867a5: Preparing\n",
      "e592fe6d10a9: Preparing\n",
      "f42691182163: Preparing\n",
      "68016c5bb65c: Preparing\n",
      "8034550a3bbe: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "61d781c1452a: Waiting\n",
      "dbc3bf935e02: Waiting\n",
      "1a5fac543081: Waiting\n",
      "a8d0c4c62eef: Waiting\n",
      "7ed9a71261c7: Waiting\n",
      "a1eeba43cdbe: Waiting\n",
      "6127942867a5: Waiting\n",
      "e592fe6d10a9: Waiting\n",
      "f42691182163: Waiting\n",
      "55df8d287560: Waiting\n",
      "68016c5bb65c: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "01b6a2323efe: Layer already exists\n",
      "4e257e38b819: Layer already exists\n",
      "42737ae997a9: Layer already exists\n",
      "7e20e12fff52: Layer already exists\n",
      "b16fc1914e14: Layer already exists\n",
      "55df8d287560: Layer already exists\n",
      "dbc3bf935e02: Layer already exists\n",
      "61d781c1452a: Layer already exists\n",
      "1a5fac543081: Layer already exists\n",
      "a8d0c4c62eef: Layer already exists\n",
      "7ed9a71261c7: Layer already exists\n",
      "a1eeba43cdbe: Layer already exists\n",
      "6127942867a5: Layer already exists\n",
      "e592fe6d10a9: Layer already exists\n",
      "f42691182163: Layer already exists\n",
      "68016c5bb65c: Layer already exists\n",
      "8034550a3bbe: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "latest: digest: sha256:41848dffa70483c3af8b2420135c0e0d6b5e32c6cd21d2cef94e10b0bae6fe47 size: 4098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our container\n",
    "img=djl_deepspeed\n",
    "\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration\n",
    "region=$(aws configure get region)\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${img}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${img}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${img}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "\n",
    "# # Build the docker image locally with the image name and then push it to ECR\n",
    "image_id=$(docker images -q | head -n1)\n",
    "docker tag $image_id ${fullname}\n",
    "\n",
    "docker push $fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac32e96",
   "metadata": {},
   "source": [
    "## Step 2: Create a `model.py` and `serving.properties`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f4864eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "predictor = None\n",
    "\n",
    "def get_model():\n",
    "    model_name = 'EleutherAI/gpt-j-6B'\n",
    "    tensor_parallel = int(os.getenv('TENSOR_PARALLEL_DEGREE', '2'))\n",
    "    local_rank = int(os.getenv('LOCAL_RANK', '0'))\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, revision=\"float32\", torch_dtype=torch.float32)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    model = deepspeed.init_inference(model,\n",
    "                                           mp_size=tensor_parallel,\n",
    "                                           dtype=model.dtype,\n",
    "                                           replace_method='auto',\n",
    "                       replace_with_kernel_inject=True)\n",
    "    generator = pipeline(task='text-generation', model=model, tokenizer=tokenizer, device=local_rank)\n",
    "    return generator\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global predictor\n",
    "    if not predictor:\n",
    "        predictor = get_model()\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_string()\n",
    "    result = predictor(data, do_sample=True, min_tokens=200, max_new_tokens=256)\n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b6929",
   "metadata": {},
   "source": [
    "### Setup serving.properties\n",
    "\n",
    "User needs to add engine Rubikon as shown below. If you would like to control how many worker groups, you can set\n",
    "\n",
    "```\n",
    "gpu.minWorkers=1\n",
    "gpu.maxWorkers=1\n",
    "```\n",
    "by adding these lines in the below file. By default, we will create as much worker group as possible based on `gpu_numbers/tensor_parallel_degree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c5ea96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile serving.properties\n",
    "\n",
    "engine=Rubikon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44488e6",
   "metadata": {},
   "source": [
    "The code below creates the SageMaker model file (`model.tar.gz`) and upload it to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5a536439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "account = session.account_id()\n",
    "region = session.boto_region_name\n",
    "img = 'djl_deepspeed'\n",
    "fullname = account+'.dkr.ecr.'+region+'amazonaws.com/'+img+':latest'\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "path = 's3://' + bucket + '/DEMO-djl-big-model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9965dd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-j/\n",
      "gpt-j/model.py\n",
      "gpt-j/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "if [ -d gpt-j ]; then\n",
    "  rm -d -r gpt-j\n",
    "fi #always start fresh\n",
    "\n",
    "mkdir -p gpt-j\n",
    "mv model.py gpt-j\n",
    "mv serving.properties gpt-j\n",
    "tar -czvf gpt-j.tar.gz gpt-j/\n",
    "#aws s3 cp gpt-j.tar.gz {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db47f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./gpt-j.tar.gz to s3://sagemaker-us-east-1-084313272408/DEMO-djl-big-model/gpt-j.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp gpt-j.tar.gz {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507e3ef",
   "metadata": {},
   "source": [
    "## Step 3: Create SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32589338",
   "metadata": {},
   "source": [
    "You should see two images from code above. Please note the image name similar to`<AWS_account_ID>.dkr.ecr.us-east-1.amazonaws.com/djl_deepspeed`. This is the ECR image URL that we need for later use. \n",
    "\n",
    "Now we create our [SageMaker model](https://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-model.html). Make sure you provide an IAM role that SageMaker can assume to access model artifacts and docker image for deployment on ML compute hosting instances. In addition, you also use the IAM role to manage permissions the inference code needs. Please check out our SageMaker Roles [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for more details. \n",
    "\n",
    " <span style=\"color:red\"> You must enter ECR image name, S3 path for the model file, and an execution-role-arn</span> in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "026d27d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
      "/bin/bash: -c: line 0: `aws sagemaker create-model --model-name gpt-j --primary-container Image={fullname},ModelDataUrl={path},Environment={TENSOR_PARALLEL_DEGREE=2} --execution-role-arn <your execution-role-arn>'\n"
     ]
    }
   ],
   "source": [
    "!aws sagemaker create-model \\\n",
    "--model-name gpt-j \\\n",
    "--primary-container \\\n",
    "Image=<ECR image>,ModelDataUrl={path},Environment={TENSOR_PARALLEL_DEGREE=2} \\\n",
    "--execution-role-arn <your execution-role-arn>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2fc2b",
   "metadata": {},
   "source": [
    "Note that we configured `ModelDataDownloadTimeoutInSeconds` and `ContainerStartupHealthCheckTimeoutInSeconds` to acommodate the large size of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "84e25dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter validation failed:\n",
      "Unknown parameter in ProductionVariants[0]: \"ModelDataDownloadTimeoutInSeconds\", must be one of: VariantName, ModelName, InitialInstanceCount, InstanceType, InitialVariantWeight, AcceleratorType, CoreDumpConfig, ServerlessConfig\n",
      "Unknown parameter in ProductionVariants[0]: \"ContainerStartupHealthCheckTimeoutInSeconds\", must be one of: VariantName, ModelName, InitialInstanceCount, InstanceType, InitialVariantWeight, AcceleratorType, CoreDumpConfig, ServerlessConfig\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'aws sagemaker create-endpoint-config \\\\\\n    --region $(aws configure get region) \\\\\\n    --endpoint-config-name gpt-j-config \\\\\\n    --production-variants \\'[\\n      {\\n        \"ModelName\": \"gpt-j\",\\n        \"VariantName\": \"AllTraffic\",\\n        \"InstanceType\": \"ml.g5.48xlarge\",\\n        \"InitialInstanceCount\": 1,\\n        \"ModelDataDownloadTimeoutInSeconds\": 1800,\\n        \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600\\n        }\\n    ]\\'\\n'' returned non-zero exit status 255.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3754/3348682523.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aws sagemaker create-endpoint-config \\\\\\n    --region $(aws configure get region) \\\\\\n    --endpoint-config-name gpt-j-config \\\\\\n    --production-variants \\'[\\n      {\\n        \"ModelName\": \"gpt-j\",\\n        \"VariantName\": \"AllTraffic\",\\n        \"InstanceType\": \"ml.g5.48xlarge\",\\n        \"InitialInstanceCount\": 1,\\n        \"ModelDataDownloadTimeoutInSeconds\": 1800,\\n        \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600\\n        }\\n    ]\\'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2460\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2461\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2462\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'aws sagemaker create-endpoint-config \\\\\\n    --region $(aws configure get region) \\\\\\n    --endpoint-config-name gpt-j-config \\\\\\n    --production-variants \\'[\\n      {\\n        \"ModelName\": \"gpt-j\",\\n        \"VariantName\": \"AllTraffic\",\\n        \"InstanceType\": \"ml.g5.48xlarge\",\\n        \"InitialInstanceCount\": 1,\\n        \"ModelDataDownloadTimeoutInSeconds\": 1800,\\n        \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600\\n        }\\n    ]\\'\\n'' returned non-zero exit status 255."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "aws sagemaker create-endpoint-config \\\n",
    "    --region $(aws configure get region) \\\n",
    "    --endpoint-config-name gpt-j-config \\\n",
    "    --production-variants '[\n",
    "      {\n",
    "        \"ModelName\": \"gpt-j\",\n",
    "        \"VariantName\": \"AllTraffic\",\n",
    "        \"InstanceType\": \"ml.g5.48xlarge\",\n",
    "        \"InitialInstanceCount\": 1,\n",
    "        \"ModelDataDownloadTimeoutInSeconds\": 1800,\n",
    "        \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600\n",
    "        }\n",
    "    ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws sagemaker create-endpoint \\\n",
    "--endpoint-name gpt-j \\\n",
    "--endpoint-config-name gpt-j-config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2a85a",
   "metadata": {},
   "source": [
    "The creation of the SageMaker endpoint might take a while. After the endpoint is created, you can test it out using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "endpoint_name = \"gpt-j\"                                       # Your endpoint name.\n",
    "content_type = \"text/plain\"                                        # The MIME type of the input data in the request body.\n",
    "# accept = \"...\"                                              # The desired MIME type of the inference in the response.\n",
    "payload = \"Amazon.com is the best\"                                             # Payload for inference.\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=content_type,\n",
    "    Body=payload\n",
    "    )\n",
    "print(response['Body'].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e83c91",
   "metadata": {},
   "source": [
    "## Step 4: Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15980a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws sagemaker delete-endpoint --endpoint-name gpt-j"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
