{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In the world of AI, there is an ever increasing set of DeepLearning frameworks and tool kits to make the life of a DL practitioner easy. While this gives flexibility and ease of building new models for ML/DL practitioners, it also brings new hurdles. One of the major hurdles for a ML/DL practitioner is taking the models that they build to production. To spell it more clearly, building a highly scalable, highly performant, production ready serving system to host the model is not an easy hurdle to pass. In this blogpost, we will showcase how anyone could use MMS (a Model Server for Apach MXNet) to host their model written using any ML/DL framework or tool kit in production. We chose AWS Sagemaker service as production hosting - PaaS solution that does a lot of heavy lifting to provide infrastructure and allows users to focus on their use cases. Specifically we will be using Bring You Own Container approach [also known as BYOC](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html) where users could bring their models together with all necessary dependencies, libraries, frameworks and other components compiled inside of a single custom-built docker container and host it on Sagemaker.  To showcase the true “ML/DL framework agnostic architecture” of MMS, we chose to launch a model written in “PaddlePaddle” framework into production.\n",
    "\n",
    "In order to bring your own ML/DL framework to Sagemaker using MMS BYO container, you need two main components\n",
    "\n",
    "1. **Model artifacts**: These are all the artifacts required to run your model on a given host. This usually contains the following:\n",
    "    1. **Model files**, which are usually symbols and weights.\n",
    "    2. **Custom Service File**: This file contains the entrypoint which gets called every time when inference request is received and served by MMS. This file generally contains the logic to initialize the model in a particular ML/DL framework, preprocess the incoming request, run inference in a particular ML/DL framework and post-process logic which takes the data coming out of framework's inference method and converts it to end-user consumable data.\n",
    "    3. **MANIFEST File**: This is the interface between custom service file and the MMS. This file is generated by running a tool that comes as part of MMS distribution, called “model-archiver”.\n",
    "2. **Container artifact**: To load and run a model written in a custom DL framework on Sagemaker, you need to bring a container that will be run on Sagemaker platform. In this document we will show how to use MMS base container and extend it to support custom DL frameworks and other model dependencies. The MMS base container is a docker container that comes with a highly scalable and performant model-server which is readily launchable onto Sagemaker platform.\n",
    "\n",
    "In the following sections, we will see each of the above components in detail.\n",
    "\n",
    "## Preparing a Model\n",
    "MMS container is completely ML/DL framework agnostic. Users can write models in a ML/DL framework of their choice and bring it to Sagemaker with MMS BYO container to get the features of scalability and performance. In this blogpost, we chose to showcase this by bringing in a model written for PaddlePaddle framework. Lets look at how to prepare a PaddlePaddle model in the following sections. The model artifact is readily available at <*TODO: Update this with the S3 link with model.tar.gz*>.\n",
    "\n",
    "### Preparing Model Artifacts\n",
    "We are going to use [Understand Sentiment](https://github.com/PaddlePaddle/book/tree/develop/06.understand_sentiment) example that is available and published in examples section of PaddlePaddle repository. First of all we need to create a model. In order to do that we followed instructions provided in [PaddlePaddle/book](https://github.com/PaddlePaddle/book) repository: downloaded  container and ran training by the notebook that is provided as part of the example. We used “Stacked Bidirectional LSTM” network for our training and trained the model for 100 epochs. At the end of this training exercise, we get the following list of trained model artifacts.\n",
    "\n",
    "```bash\n",
    "!ls\n",
    "embedding_0.w_0    fc_2.w_0    fc_5.w_0    learning_rate_0    lstm_3.b_0    moment_10    moment_18    moment_25    moment_32    moment_8\n",
    "embedding_1.w_0    fc_2.w_1    fc_5.w_1    learning_rate_1    lstm_3.w_0    moment_11    moment_19    moment_26    moment_33    moment_9\n",
    "fc_0.b_0    fc_3.b_0    fc_6.b_0    lstm_0.b_0    lstm_4.b_0    moment_12    moment_2    moment_27    moment_34\n",
    "fc_0.w_0    fc_3.w_0    fc_6.w_0    lstm_0.w_0    lstm_4.w_0    moment_13    moment_20    moment_28    moment_35\n",
    "fc_1.b_0    fc_3.w_1    fc_6.w_1    lstm_1.b_0    lstm_5.b_0    moment_14    moment_21    moment_29    moment_4\n",
    "fc_1.w_0    fc_4.b_0    fc_7.b_0    lstm_1.w_0    lstm_5.w_0    moment_15    moment_22    moment_3    moment_5\n",
    "fc_1.w_1    fc_4.w_0    fc_7.w_0    lstm_2.b_0    moment_0    moment_16    moment_23    moment_30    moment_6\n",
    "fc_2.b_0    fc_5.b_0    fc_7.w_1    lstm_2.w_0    moment_1    moment_17    moment_24    moment_31    moment_7\n",
    "```\n",
    "\n",
    "These artifacts constitute a PaddlePaddle model. We copy these artifacts from within training container to localhost so that it will be easier to begin preparation of the model for production hosting. To learn more on how to copy files from inside a docker container to location outside of it please refer to [Docker CLI](https://docs.docker.com/engine/reference/commandline/cp/).\n",
    "\n",
    "### Writing Custom Service Code\n",
    "We now have model files required to host the model in production. We can now define a custom service file which knows how to use these files and also knows how to “preprocess” the raw request coming into the server and how to “postprocess” the responses coming out of the PaddlePaddle framework's “infer” method. For this, we modified the notebook example written to test the trained model *<TODO: supply link to what was the source of the example notebook>*. Let's look at some code. \n",
    "\n",
    "We created a custom service file called “paddle_sentiment_analysis.py”. Here, we first define a class called “PaddleSentimentAnalysis” which contains methods to initialize the model and also defines pre-processing, post-processing and inference methods. Refer [Custom Service Code](https://github.com/awslabs/mxnet-model-server/blob/master/docs/custom_service.md) document to learn how to write your custom-service code. The skeleton of this file is as follows:\n",
    "\n",
    "```bash\n",
    "$ cat paddle_sentiment_analysis.py\n",
    "```\n",
    "```python\n",
    "\n",
    "from __future__ import print_function\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "import paddle.dataset as dataset\n",
    "from functools import partial\n",
    "\n",
    "  \n",
    "class PaddleSentimentAnalysis(object):\n",
    "    def __init__(self):\n",
    "    ...\n",
    "\n",
    "    def initialize(self, context):\n",
    "    \"\"\"\n",
    "    This method is used to initialize the network and read other artifacts.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    def preprocess(self, data):\n",
    "    \"\"\"\n",
    "    This method is used to convert the string requests coming from client \n",
    "    into tensors. \n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "    def inference(self, input):\n",
    "    \"\"\"\n",
    "    This method runs the tensors created in preprocess method through the \n",
    "    DL framework's infer method.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "    def postprocess(self, output, data):\n",
    "    \"\"\"\n",
    "    Here the values returned from the inference method is converted to a \n",
    "    human understandable response.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "\n",
    "_service = PaddleSentimentAnalysis()\n",
    "\n",
    "\n",
    "def handle(data, context):\n",
    "\"\"\"\n",
    "This method is the entrypoint \\\"handler\\\" method that is used by MMS.\n",
    "Any request coming in for this model will be sent to this method.\n",
    "\"\"\"\n",
    "    if not _service.initialized:\n",
    "        _service.initialize(context)\n",
    "\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    pre = _service.preprocess(data)\n",
    "    inf = _service.inference(pre)\n",
    "    ret = _service.postprocess(inf, data)\n",
    "    return ret\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model artifact file to be hosted on sagemaker\n",
    "In order to load this model onto Sagemaker platform with MMS BYO container, we need to do the following:\n",
    "\n",
    "1. Create a MANIFEST file, which is used by MMS as a model's metadata to load and run the model.\n",
    "2. Add the above custom-service file and the trained model-artifacts, along with the MANIFEST file, to a .tar.gz file.\n",
    "\n",
    "Let's use “model-archiver” tool, to accomplish the above points. Before we use the tool to create a “.tar.gz” artifact, we need to collect all the model artifacts, including the custom-service-file mentioned above, into a separate folder. For ease of getting started, we have uploaded all the model artifacts onto an [S3 bucket](https://s3.amazonaws.com/model-server/blog_artifacts/PaddlePaddle_blog/sentiment.tar.gz). Lets run the following commands to get this artifact onto your host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://s3.amazonaws.com/model-server/blog_artifacts/PaddlePaddle_blog/artifacts.tgz\n",
    "!tar zxvf artifacts.tgz    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -R artifacts/sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model artifacts, let's convert this to a model artifact that can be hosted on Sagemaker. \n",
    "\n",
    "#### Prerequisites\n",
    "Before we proceed with preparing a Sagemaker model-artifact, we need the following. \n",
    "1. Model-archiver tool\n",
    "2. Sagemaker SDK\n",
    "3. Boto3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create a sagemaker model artifact. For this, we use the \"model-archiver\" tool to create a Sagemaker model artifact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U boto3 sagemaker awscli model-archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!model-archiver -f --model-name paddle_sentiment \\\n",
    "--handler paddle_sentiment_analysis:handle\\\n",
    "--model-path artifacts/sentiment --export-path . --archive-format amazon_sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command would create an model artifact called `paddle_sentiment.tar.gz`, which we will use to host our endpoint. Let's verify if this model artifact is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's take a look at how to build a container with it and bring it into Sagemaker.\n",
    "\n",
    "### Building your own BYO container with MMS\n",
    "\n",
    "In this section, we build our own MMS based container which can be brought onto Sagemaker (also known as BYO Container).\n",
    "\n",
    "To help with this process, every released version of MMS comes with a corresponding MMS base container, hosted on [DockerHub](https://hub.docker.com/r/awsdeeplearningteam/mxnet-model-server) [**TODO: Publish the base container after finalizing the blog**] which can be hosted on the Sagemaker platform.\n",
    "\n",
    "For this example, we will use container tagged mxnet-model-server:base_cpu_py3 [**TODO: Update this to point to MMS-BYO tag**]. To host the model created in the above section, we need to install “PaddlePaddle” and “numpy” packages in the container. This can be done by creating a Dockerfile which extends from the base MMS image and installs the above python packages. Here is how its content should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat artifacts/Dockerfile.paddle.mms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have Dockerfile that describes our BYO container let's build it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd artifacts && docker build -t paddle-mms -f Dockerfile.paddle.mms ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sagemaker endpoint with PaddlePaddle model\n",
    "Before we go on and create a Sagemaker endpoint for our model, we need to do some preparations:\n",
    "\n",
    "### Upload the Sagemaker model artifact to a S3 bucket\n",
    "Upload the model archive **sentiment.tar.gz** created above to a S3 bucket. Here we uploaded it to the S3 bucket called paddle_paddle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, os\n",
    "s3 = boto3.resource('s3')\n",
    "s3_bucket_name = 'paddle-sentiment-model'\n",
    "local_model_artifact = s3_model_artifact = 'paddle_sentiment.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a bucket called **paddle-sentiment-model**. Here is where we will copy the model, **paddle_sentiment.tar.gz**, that we had created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.create_bucket(Bucket=s3_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.meta.client.upload_file(local_model_artifact, s3_bucket_name, s3_model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have **paddle_sentiment.tar.gz** on S3 in our account. Now let's look at having the container that we built on ECR, so that we can go ahead and set up our Sagemaker Endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the container image to ECR\n",
    "We had built an image called **paddle-mms** above. We need to upload this to a Amazon ECR in our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, os\n",
    "client = boto3.client('ecr')\n",
    "account = sagemaker.Session().boto_session.client('sts').get_caller_identity()['Account']\n",
    "os.environ[\"Account\"] = account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_repository(repositoryName='paddle-mms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag paddle-mms:latest ${Account}.dkr.ecr.us-east-1.amazonaws.com/paddle-mms:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$(aws ecr get-login --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push ${Account}.dkr.ecr.us-east-1.amazonaws.com/paddle-mms:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pushes the \"paddle-mms\" container to Amazon ECR in your account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sagemaker Endpoint\n",
    "Now that the model and container artifacts are uploaded onto S3 and ECR respectively, we can go ahead and create Sagemaker endpoint. To do that we need to complete following steps\n",
    "\n",
    "\n",
    "#### Prerequisite\n",
    "\n",
    "Before we go onto create an Sagemaker endpoint, we need to setup an IAM role which has *AmazonSageMakerFullAccess* policy attached to it. If you already have one, use it. Else, create a role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "def get_execution_role(role_name=\"sagemaker\", aws_account=None, aws_region=None):\n",
    "    \"\"\"\n",
    "    Create sagemaker execution role to perform sagemaker task\n",
    "    Args:\n",
    "        role_name (string): name of the role to be created\n",
    "        aws_account (string): aws account of the ECR repo\n",
    "        aws_region (string): aws region where the repo is located\n",
    "    \"\"\"\n",
    "    session = boto3.Session()\n",
    "    aws_account = session.client(\"sts\").get_caller_identity()['Account'] if aws_account is None else aws_account\n",
    "    aws_region = session.region_name if aws_region is None else aws_region\n",
    "    \n",
    "prin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "def get_execution_role(role_name=\"sagemaker\", aws_account=None, aws_region=None):\n",
    "    \"\"\"\n",
    "    Create sagemaker execution role to perform sagemaker task\n",
    "    Args:\n",
    "        role_name (string): name of the role to be created\n",
    "        aws_account (string): aws account of the ECR repo\n",
    "        aws_region (string): aws region where the repo is located\n",
    "    \"\"\"\n",
    "    session = boto3.Session()\n",
    "    aws_account = session.client(\"sts\").get_caller_identity()['Account'] if aws_account is None else aws_account\n",
    "    aws_region = session.region_name if aws_region is None else aws_region\n",
    "    \n",
    "    assume_role_policy_document = json.dumps({\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": [\"sagemaker.amazonaws.com\"]\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    client = session.client('iam')\n",
    "    try:\n",
    "        client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=str(assume_role_policy_document)\n",
    "        )\n",
    "        print(\"Created new sagemaker execution role: %s\" % role_name)\n",
    "    except ClientError as e:\n",
    "        if 'EntityAlreadyExistsException' == e.__class__.__name__:\n",
    "            pass\n",
    "        \n",
    "    client.attach_role_policy(\n",
    "        PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    "        RoleName=role_name\n",
    "    )\n",
    "        \n",
    "    client.attach_role_policy(\n",
    "        PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "        RoleName=role_name\n",
    "        )\n",
    "\n",
    "    return client.get_role(RoleName=role_name)['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "sm_role = get_execution_role(aws_account=account, aws_region=region)\n",
    "inference_image = '{}.dkr.ecr.{}.amazonaws.com/paddle-mms:latest'.format(account, region)\n",
    "s3_url = 's3://{}/{}'.format(s3_bucket_name, s3_model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the role required to launch our Sagemaker endpoint above. Now let's use the Sagemaker SDK to launch an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_handler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "endpoint = 'PaddleSentiment'\n",
    "paddle_model = Model(model_data=s3_url, image=inference_image, role=sm_role)\n",
    "try:\n",
    "    inf_handler = paddle_model.deploy(1, 'ml.m4.xlarge', endpoint_name=endpoint)\n",
    "except ClientError as e:\n",
    "    if 'ValidationException' == e.response['Error']['Code']:\n",
    "        print(\"The endpoint \\\"{}\\\"already exists\".format(endpoint))\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creats an sagemaker endpoint using the model artifact \"paddle_sentiment.tar.gz\".\n",
    "\n",
    "### Testing the endpoint\n",
    "Let's test the endpoint. To do this, we will send a movie review to the endpoint \"paddle-sentiment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "predictor = RealTimePredictor(endpoint=endpoint, sagemaker_session=sess)\n",
    "\n",
    "message = \"This is an amazing movie.\"\n",
    "print(predictor.predict(message).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would get a response showing that the review was positive and the probability of it being positive was \"56.124%\".\n",
    "\n",
    "### Conclusion\n",
    "We have just shown how to build and host PaddlePaddle model on Sagemaker using MMS BYO container. This flow can be reused with minor modifications in order to build BYO containers serving inference traffic on Sagemaker endpoints with MMS for models built using many ML/DL frameworks, not just PaddlePaddle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
