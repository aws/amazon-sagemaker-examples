{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167a5e66",
   "metadata": {},
   "source": [
    "## Training Amazon SageMaker models for binding affinity prediction by using the DGL-LifeSci with PyTorch backend\n",
    "The **Amazon SageMaker Python SDK** makes it easy to train DGL-LifeSci models. In this example, you train an Atomic Convolutional Networks (ACNN) [1] or PotentialNet [2] model using the [PDBBind](http://www.pdbbind.org.cn/) dataset. For detailed information of them please see [the example page of DGL-Lifesci](https://github.com/yoheigon/dgl-lifesci/tree/master/examples/binding_affinity_prediction).\n",
    "\n",
    "[1] Gomes et al. (2017) Atomic Convolutional Networks for Predicting Protein-Ligand Binding Affinity. *arXiv preprint arXiv:1703.10603*.\n",
    "\n",
    "[2] Feinberg et al. (2018) PotentialNet for molecular property prediction. *ACS central science* 4.11: 1520-1530."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7434f763",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Define a few variables that are needed later in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749a9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# Setup session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here.\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "# IAM execution role that gives Amazon SageMaker access to resources in your AWS account.\n",
    "# You can use the Amazon SageMaker Python SDK to get the role from the notebook environment.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d13fe5",
   "metadata": {},
   "source": [
    "### The training script\n",
    "The main.py script provides all the code you need for training an Amazon SageMaker model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e379cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e857a586",
   "metadata": {},
   "source": [
    "### SageMaker's Estimator class\n",
    "The Amazon SageMaker Estimator allows you to run single machine in Amazon SageMaker, using CPU or GPU-based instances.\n",
    "\n",
    "When you create the estimator, pass in the filename of the training script and the name of the IAM execution role. You can also provide a few other parameters. instance_count and instance_type determine the number and type of Amazon SageMaker instances that are used for the training job. The hyperparameters parameter is a dictionary of values that is passed to your training script as parameters so that you can use argparse to parse them. You can see how to access these values in the main.py script above.\n",
    "\n",
    "For this example, we use ml.p3.2xlarge for training instance, PDBBind(v2015) core for dataset, and PDBBind for model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffcb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"mae\", \"Regex\": \"mae ([0-9.]+).*$\"},\n",
    "    {\"Name\": \"r2\", \"Regex\": \"r2 ([0-9.]+).*$\"},\n",
    "]\n",
    "\n",
    "# Create estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"main.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    "    framework_version=\"1.6.0\",\n",
    "    py_version=\"py3\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    hyperparameters={\n",
    "        \"model\": \"PotentialNet\",\n",
    "        \"dataset_option\": \"PDBBind_core_pocket_random\",\n",
    "        \"version\": \"v2015\",\n",
    "    },\n",
    "    metric_definitions=metric_definitions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8e60b",
   "metadata": {},
   "source": [
    "### Running the Training Job\n",
    "After you construct the Estimator object, fit it by using Amazon SageMaker. The [PDBBind](http://www.pdbbind.org.cn/) dataset is automatically downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df739893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Launch SageMaker training job\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27113078",
   "metadata": {},
   "source": [
    "### (Optional) Hyperparameter tuning\n",
    "\n",
    "SageMaker offers hyperparameter tuning to kick off multiple training jobs with different hyperparameter combinations, to find the set with best model performance. You can specify the hyperparameters you want to tune and their possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bf6418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.001, 0.01),\n",
    "    \"num_epochs\": IntegerParameter(100, 200),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be27bc0",
   "metadata": {},
   "source": [
    "Next, specify the objective metric that you want to tune and its definition. This includes the regular expression (regex) needed to extract that metric from the Amazon CloudWatch logs of the training job. For this example, \"mae\" and \"r2\" is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35923062",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = \"mae\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8acc1",
   "metadata": {},
   "source": [
    "Now, create a HyperparameterTuner object, which you pass:\n",
    "\n",
    " * The training estimator you created above\n",
    " * The hyperparameter ranges\n",
    " * Objective metric name and definition\n",
    " * Number of training jobs to run in total and how many training jobs should be run simultaneously. More parallel jobs will finish tuning sooner, but may sacrifice accuracy. We recommend you set the parallel jobs value to less than 10% of the total number of training jobs (we'll set it higher just for this example to keep it short).\n",
    " * Whether you should maximize or minimize the objective metric. You haven't specified here since it defaults to 'Maximize', which is what you want for validation accuracy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_tags = [{\"Key\": \"ML Task\", \"Value\": \"DGL-Lifesci\"}]\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    tags=task_tags,\n",
    "    max_jobs=2,\n",
    "    max_parallel_jobs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca982ead",
   "metadata": {},
   "source": [
    "And finally, start the tuning job by calling .fit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9556ca",
   "metadata": {},
   "source": [
    "Let's just run a quick check of the hyperparameter tuning jobs status to make sure it started successfully and is InProgress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc8a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "sagemaker_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)[\"HyperParameterTuningJobStatus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46652e",
   "metadata": {},
   "source": [
    "### Output\n",
    "After the hyperparameter tuning job is finished. You can get the best model of the turning job as below. Through the SageMaker hyperparameter turning job, you can easily manage computationally heavy training jobs and find the best-performing model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab7de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "tuner_status = sagemaker_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)[\"HyperParameterTuningJobStatus\"]\n",
    "\n",
    "if tuner_status == \"Completed\":\n",
    "    best_training_job_summary = sagemaker_boto_client.describe_hyper_parameter_tuning_job(\n",
    "        HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)[\"BestTrainingJob\"]\n",
    "    best_training_job_details = sagemaker_boto_client.describe_training_job(\n",
    "        TrainingJobName=best_training_job_summary[\"TrainingJobName\"])\n",
    "\n",
    "    trained_model_s3_uri = best_training_job_details[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "    s3_client.download_file(trained_model_s3_uri.split('/')[2], ('/').join(trained_model_s3_uri.split('/')[3:]), \"model.tar.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
