{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e909f87b-0738-4aea-9388-6ffe258db9f2",
   "metadata": {},
   "source": [
    "# Deploy Stable Cascade for Real-Time Image Generation on SageMaker\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    " ![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "For further reading & reference materials:\n",
    "\n",
    "Sources: https://www.philschmid.de/sagemaker-stable-diffusion#2-create-sagemaker-modeltargz-artifact\n",
    "\n",
    "Further Reading: https://huggingface.co/stabilityai/stable-cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7a9e0a-6325-4639-85c1-ca3b4083feb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "746.96s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "757.78s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "767.09s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker huggingface_hub --upgrade --q\n",
    "!pip install python-dotenv --upgrade --q\n",
    "!pip install diffusers --upgrade --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30738e7b-cb07-4366-84af-026bee55bdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 19:10:16.346758: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sagemaker bucket: sagemaker-us-east-1-329542461890\n",
      "Sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import boto3\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import sagemaker\n",
    "import tarfile\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from diffusers import (\n",
    "    StableCascadePriorPipeline,\n",
    "    StableCascadeDecoderPipeline,\n",
    "    StableCascadeUNet,\n",
    ")\n",
    "from distutils.dir_util import copy_tree\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import snapshot_download\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.async_inference import AsyncInferenceConfig\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "print(f\"Sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"Sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18f7eef1-b2cd-4137-ba57-974f025dc1dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fa69b01a3c484fab9fa89bfcf477c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad35a4b9154b48b9b2069f101cacf65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101bc228e6b7445abede68f391e2451d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "image_encoder/config.json:   0%|          | 0.00/575 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46be738b478744cc98cb7b61d36af3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ature_extractor/preprocessor_config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e04e4c6c8b1448eab121cacf4b4bfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler/scheduler_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee4009dc67c48b8870855c5b24d942b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f946786dd7de4bcbbd850d55a6390810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model.bf16.safetensors:   0%|          | 0.00/1.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44834d71b320440cb275ef1a3faa31b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "image_encoder/model.bf16.safetensors:   0%|          | 0.00/608M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ab4994c3dd4e2387045c7c8b2cf60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)diffusion_pytorch_model.bf16.safetensors:   0%|          | 0.00/2.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0c756f43ae488b91f06b4a2425ab1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/special_tokens_map.json:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48d06ce5d904470a23927969a6835d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c732c9a8d734c9982c89bef0077b4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer_config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8feb2b2d2e7f467c8325a90c9281a22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b679a598c344bb692d70749ef3d2a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c602e57de143a39f0e8da2a07ba2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f0732729cb417abada1f8368228643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc3d54be7d24b359ac269f2c3f5018f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724c0e912258420b80c7b5eeb705334f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)diffusion_pytorch_model.bf16.safetensors:   0%|          | 0.00/1.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b13c195e13410eaf0fcefaa92e4cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fe23086e0c4addbe1e4cc8c61b36b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/special_tokens_map.json:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a478d4cb8ac540ebacba3e5208019c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ab94582edd4c3aba0140b2b054c4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6b57dc96534ac792f19099a56248f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler/scheduler_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a3e1a39fb84108a873776b03ec6979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer_config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ac2c0bcb364fcaa8ea5f1641f6aa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model.bf16.safetensors:   0%|          | 0.00/1.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872727cad73e4edba22b9805ada9189c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vqgan/config.json:   0%|          | 0.00/328 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464cf488a6994ddc9df3499294da9d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)diffusion_pytorch_model.bf16.safetensors:   0%|          | 0.00/36.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400528864cb648888f452b200f63dd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e171e2ed8a4e12b49b3fd6a39a5409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HF_PRIOR_ID = \"stabilityai/stable-cascade-prior\"\n",
    "HF_DECODER_ID = \"stabilityai/stable-cascade\"\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\", \"cache_dir\")\n",
    "\n",
    "prior_unet = StableCascadeUNet.from_pretrained(HF_PRIOR_ID, subfolder=\"prior_lite\")\n",
    "decoder_unet = StableCascadeUNet.from_pretrained(\n",
    "    HF_DECODER_ID, subfolder=\"decoder_lite\"\n",
    ")\n",
    "\n",
    "prior = StableCascadePriorPipeline.from_pretrained(\n",
    "    HF_PRIOR_ID,\n",
    "    variant=\"bf16\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    prior=prior_unet,\n",
    ")\n",
    "decoder = StableCascadeDecoderPipeline.from_pretrained(\n",
    "    HF_DECODER_ID,\n",
    "    variant=\"bf16\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    decoder=decoder_unet,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b15454b-7449-4d06-b10a-276181734a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model/\"\n",
    "prior_path = \"model/prior/\"\n",
    "decoder_path = \"model/decoder/\"\n",
    "code_path = \"code/\"\n",
    "cache_dir = \"cache_dir/\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "if not os.path.exists(code_path):\n",
    "    os.mkdir(code_path)\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.mkdir(cache_dir)\n",
    "if not os.path.exists(prior_path):\n",
    "    os.mkdir(prior_path)\n",
    "if not os.path.exists(decoder_path):\n",
    "    os.mkdir(decoder_path)\n",
    "\n",
    "prior.save_pretrained(save_directory=prior_path)\n",
    "decoder.save_pretrained(save_directory=decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ff336e-7aee-40a5-a1e2-3e57ac542511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc5952c55ec4127864c093a8ca2a059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8593bdfa4b44ff928b2e752e88b051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e43849c66c84e7288ca26ca7d063e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fba21ee0654962bd1a44d1f36fb783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform local inference in notebook to validate model loading and inference call\n",
    "\n",
    "prior = StableCascadePriorPipeline.from_pretrained(prior_path, local_files_only=True)\n",
    "decoder = StableCascadeDecoderPipeline.from_pretrained(decoder_path, local_files_only=True)\n",
    "prompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "prior.enable_model_cpu_offload()\n",
    "prior_output = prior(\n",
    "    prompt=prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    negative_prompt=negative_prompt,\n",
    "    guidance_scale=4.0,\n",
    "    num_images_per_prompt=1,\n",
    "    num_inference_steps=20\n",
    ")\n",
    "\n",
    "decoder.enable_model_cpu_offload()\n",
    "decoder_output = decoder(\n",
    "    image_embeddings=prior_output.image_embeddings,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    guidance_scale=0.0,\n",
    "    output_type=\"pil\",\n",
    "    num_inference_steps=10\n",
    ").images[0]\n",
    "decoder_output.save(\"cascade.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d76117ef-f131-48bf-bc83-c2f2a6652a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/requirements.txt\n",
    "--find-links https://download.pytorch.org/whl/torch_stable.html\n",
    "accelerate>=0.25.0\n",
    "torch==2.1.2+cu118\n",
    "torchvision==0.16.2+cu118\n",
    "transformers>=4.30.0\n",
    "numpy>=1.23.5\n",
    "kornia>=0.7.0\n",
    "insightface>=0.7.3\n",
    "opencv-python>=4.8.1.78\n",
    "tqdm>=4.66.1\n",
    "matplotlib>=3.7.4\n",
    "webdataset>=0.2.79\n",
    "wandb>=0.16.2\n",
    "munch>=4.0.0\n",
    "onnxruntime>=1.16.3\n",
    "einops>=0.7.0\n",
    "onnx2torch>=1.5.13\n",
    "warmup-scheduler @ git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
    "torchtools @ git+https://github.com/pabloppp/pytorch-tools\n",
    "diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5802c2fa-f333-4137-ac1e-1f902c2ad382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from diffusers import StableCascadePriorPipeline, StableCascadeDecoderPipeline\n",
    "from io import BytesIO\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the model for inference\n",
    "    \"\"\"\n",
    "    print(\"Entering model_fn...\")\n",
    "    print(f\"Model Directory is {model_dir}\")\n",
    "    \n",
    "    prior = StableCascadePriorPipeline.from_pretrained(f\"{model_dir}/prior\", local_files_only=True)\n",
    "    decoder = StableCascadeDecoderPipeline.from_pretrained(f\"{model_dir}/decoder\", local_files_only=True)\n",
    "    \n",
    "    model_dict = {\"prior\": prior, \"decoder\": decoder}\n",
    "    print(f\"model dictionary: {model_dict}\")\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model_dict):\n",
    "    \"\"\"\n",
    "    Apply model to the incoming request\n",
    "    \"\"\"\n",
    "    print(\"Entering predict_fn...\")\n",
    "    prior = model_dict[\"prior\"]\n",
    "    decoder = model_dict[\"decoder\"]\n",
    "    \n",
    "    print(f\"Processing input_data {input_data}\")\n",
    "    prompt = input_data[\"prompt\"]\n",
    "    negative_prompt = input_data[\"negative_prompt\"]\n",
    "    print(f\"Prompt = {prompt}\")    \n",
    "    print(f\"Negative Prompt = {negative_prompt}\")    \n",
    "    \n",
    "    prior.enable_model_cpu_offload()\n",
    "    prior_output = prior(\n",
    "        prompt=prompt,\n",
    "        height=1024,\n",
    "        width=1024,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=4.0,\n",
    "        num_images_per_prompt=1,\n",
    "        num_inference_steps=20\n",
    "    )\n",
    "    \n",
    "    decoder.enable_model_cpu_offload()\n",
    "    decoder_output = decoder(\n",
    "        image_embeddings=prior_output.image_embeddings,\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=0.0,\n",
    "        output_type=\"pil\",\n",
    "        num_inference_steps=10\n",
    "    ).images[0]\n",
    "\n",
    "    encoded_images = []\n",
    "    buffered = BytesIO()\n",
    "    decoder_output.save(buffered, format=\"JPEG\")\n",
    "    encoded_images.append(base64.b64encode(buffered.getvalue()).decode())\n",
    "    print(\"Finished encodeing returned images.\")\n",
    "    return {\"generated_images\": encoded_images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9c1554b-71e0-4755-9661-c0df74fc91ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model-63921/code/requirements.txt', 'model-63921/code/inference.py']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assemble model package\n",
    "model_tar = Path(f\"model-{random.getrandbits(16)}\")\n",
    "model_tar.mkdir(exist_ok=True)\n",
    "\n",
    "copy_tree(prior_path, str(model_tar.joinpath(\"prior\")))\n",
    "copy_tree(decoder_path, str(model_tar.joinpath(\"decoder\")))\n",
    "copy_tree(code_path, str(model_tar.joinpath(\"code\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f83a4-e0c8-46ed-b477-3fe3d617e365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior\n"
     ]
    }
   ],
   "source": [
    "# helper to create the model.tar.gz\n",
    "def compress(tar_dir=None, output_file=\"model.tar.gz\"):\n",
    "    parent_dir = os.getcwd()\n",
    "    os.chdir(tar_dir)\n",
    "    with tarfile.open(os.path.join(parent_dir, output_file), \"w:gz\") as tar:\n",
    "        for item in os.listdir(\".\"):\n",
    "            print(item)\n",
    "            tar.add(item, arcname=item)\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "\n",
    "compress(str(model_tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08218d56-f750-4414-83a7-db36068234ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload model.tar.gz to s3\n",
    "s3_model_uri = S3Uploader.upload(\n",
    "    local_path=\"model.tar.gz\",\n",
    "    desired_s3_uri=f\"s3://{sess.default_bucket()}/stable-cascade\",\n",
    ")\n",
    "print(f\"model uploaded to: {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc684183-8da4-4eed-9e6a-e30c2bc5471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40ece5-67bf-4bc6-987b-7306310f9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper decoder\n",
    "def decode_base64_image(image_string):\n",
    "    base64_image = base64.b64decode(image_string)\n",
    "    buffer = BytesIO(base64_image)\n",
    "    return Image.open(buffer)\n",
    "\n",
    "\n",
    "# display PIL images as grid\n",
    "def display_images(images=None, columns=3, width=100, height=100):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228d7a7-497d-40ff-aecd-1b135813e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=s3_model_uri,\n",
    "    role=get_execution_role(sess),\n",
    "    transformers_version=\"4.17\",\n",
    "    pytorch_version=\"1.10\",\n",
    "    py_version=\"py38\",\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.48xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d06a2e-12b8-4774-83d1-9a014be56faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# invoke_endpoint_async API call\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "prompt = \"A dog trying to catch a flying pizza art\"\n",
    "num_images_per_prompt = 1\n",
    "payload = {\"prompt\": prompt, \"negative_prompt\": \"\"}\n",
    "\n",
    "serialized_payload = json.dumps(payload, indent=4)\n",
    "with open(\"payload.json\", \"w\") as outfile:\n",
    "    outfile.write(serialized_payload)\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    ContentType=\"application/json\",  # Specify the format of the payload\n",
    "    Accept=\"application/json\",\n",
    "    Body=serialized_payload,\n",
    ")\n",
    "print(f\"inference response: {response}\")\n",
    "\n",
    "response_payload = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "\n",
    "# decode images\n",
    "decoded_images = [\n",
    "    decode_base64_image(image) for image in response_payload[\"generated_images\"]\n",
    "]\n",
    "\n",
    "# visualize generation\n",
    "display_images(decoded_images)\n",
    "\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "print(f\"total inference time = {inference_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ac143-dfaf-4944-8b57-e1c043153d59",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|generativeai|huggingface-multimodal|stability-cascade|DeployStableCascade.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
