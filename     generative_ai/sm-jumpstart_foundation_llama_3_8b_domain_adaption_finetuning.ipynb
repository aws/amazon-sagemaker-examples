{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b408436-c1fa-41b6-9f0f-ffc5b37ae773",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart Foundation Models - Fine-tuning text generation Llama-3 8B model on domain specific dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcb6b4-120b-4147-b8e1-73dbccfccb20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeec833-b3cc-4429-8cd3-6bf6200e71df",
   "metadata": {},
   "source": [
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK for finetuning Llama-3 8B Foundation Models and deploying the trained model for inference. The Foundation models perform Text Generation task. It takes a text string as input and predicts next words in the sequence.\n",
    "\n",
    "Additionally, this notebook will demonstrate how you can use [Amamzon SageMaker Jumpstart Industry SDK](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart-industry.html) to prepare training data from US SEC filings. \n",
    "\n",
    "Below is the content of the notebook.\n",
    "\n",
    "1. [Domain adaptation fine-tuning](#1.-Domain-adaptation-fine-tuning)\n",
    "   * [1.1. Set up](#1.1.-Set-up)\n",
    "   * [1.2. Preparing training data](#1.2.-Preparing-training-data)\n",
    "   * [1.3. Starting training](#1.4.-Starting-training)\n",
    "   * [1.4. Deploying inference endpoints](#1.5.-Deploying-inference-endpoints)\n",
    "   * [1.5. Running inference queries and compare model performances](#1.5.-Running-inference-queries-and-compare-model-performances)\n",
    "   * [1.6. Clean up](#1.6.-Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498cd74-82e5-4d67-96fb-d6be69662377",
   "metadata": {},
   "source": [
    "## 1. Domain adaptation fine-tuning\n",
    "The Text Generation model can also be fine-tuned on any domain specific dataset. After being fine-tuned on the domain specific dataset, the model\n",
    "is expected to generate domain specific text and solve various NLP tasks in that specific domain with **few shot prompting**.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file. \n",
    "  - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "  - The number of files under train and validation (if provided) should equal to one, respectively. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "Below is an example of a TXT file for fine-tuning the Text Generation model. The TXT file is SEC filings of Amazon from year 2024.\n",
    "\n",
    "```Note About Forward-Looking Statements\n",
    "This Annual Report on Form 10-K includes forward-looking statements within the\n",
    "meaning of the Private Securities Litigation Reform Act of 1995. All\n",
    "statements other than statements of historical fact, including statements\n",
    "regarding guidance, industry prospects, or future results of operations or\n",
    "financial position, made in this Annual Report on Form 10-K are forward-\n",
    "looking. We use words such as anticipates, believes, expects, future, intends,\n",
    "and similar expressions to identify forward-looking statements. Forward-\n",
    "looking statements reflect management’s current expectations and are\n",
    "inherently uncertain. Actual results and outcomes could differ materially for\n",
    "a variety of reasons, including, among others, fluctuations in foreign\n",
    "exchange rates, changes in global economic conditions and customer demand and\n",
    "spending, inflation, interest rates, regional labor market constraints, world\n",
    "events, the rate of growth of the internet, online commerce, cloud services,\n",
    "and new and emerging technologies, the amount that Amazon.com invests in new\n",
    "business opportunities and the timing of those investments, the mix of\n",
    "products and services sold to customers, the mix of net sales derived from\n",
    "products as compared with services, the extent to which we owe income or other\n",
    "taxes, competition, management of growth, potential fluctuations in operating\n",
    "results, international growth and expansion, the outcomes of claims,\n",
    "litigation, government investigations, and other proceedings, fulfillment,\n",
    "sortation, delivery, and data center optimization, risks of inventory\n",
    "management, variability in demand, the degree to which we enter into,\n",
    "maintain, and develop commercial agreements, proposed and completed\n",
    "acquisitions and strategic transactions, payments risks, and risks of\n",
    "fulfillment throughput and productivity. In addition, global economic and\n",
    "geopolitical conditions and additional or unforeseen circumstances,\n",
    "developments, or events may give rise to or amplify many of these risks. These\n",
    "risks and uncertainties, as well as other risks and uncertainties that could\n",
    "cause our actual results or outcomes to differ significantly from management’s\n",
    "expectations, are described in greater detail in Item 1A of Part I, “Risk\n",
    "Factors.”\n",
    "\n",
    "GENERAL\n",
    "Embracing Our Future ...\n",
    "```\n",
    "\n",
    "\n",
    "#### 2.2. Instruction fine-tuning\n",
    "The Text generation model can be instruction-tuned on any text data provided that the data \n",
    "is in the expected format. The instruction-tuned model can be further deployed for inference. \n",
    "Below are the instructions for how the training data should be formatted for input to the \n",
    "model.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Train and validation directories should contain one or multiple JSON lines (`.jsonl`) formatted files. In particular, train directory can also contain an optional `*.json` file describing the input and output formats. \n",
    "  - The best model is selected according to the validation loss, calculated at the end of each epoch.\n",
    "  If a validation set is not given, an (adjustable) percentage of the training data is\n",
    "  automatically split and used for validation.\n",
    "  - The training data must be formatted in a JSON lines (`.jsonl`) format, where each line is a dictionary\n",
    "representing a single data sample. All training data must be in a single folder, however\n",
    "it can be saved in multiple jsonl files. The `.jsonl` file extension is mandatory. The training\n",
    "folder can also contain a `template.json` file describing the input and output formats. If no\n",
    "template file is given, the following template will be used:\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\",\n",
    "    \"completion\": \"{response}\"\n",
    "  }\n",
    "  ```\n",
    "  - In this case, the data in the JSON lines entries must include `instruction`, `context` and `response` fields. If a custom template is provided it must also use `prompt` and `completion` keys to define\n",
    "  the input and output templates.\n",
    "  Below is a sample custom template:\n",
    "\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"question: {question} context: {context}\",\n",
    "    \"completion\": \"{answer}\"\n",
    "  }\n",
    "  ```\n",
    "Here, the data in the JSON lines entries must include `question`, `context` and `answer` fields. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d4114-6a6e-4bfe-b735-ffc5227041f9",
   "metadata": {},
   "source": [
    "### 1.1 Set Up\n",
    "Before executing the notebook, there are some initial steps required for setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357bca8-fa89-44e3-bcdd-f7002f4f7170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install sagemaker smjsindustry sec-edgar-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf077a-5084-4b84-9260-7bf34362a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    modelid,\n",
    "    modelversion,\n",
    ") = (\n",
    "    \"meta-textgeneration-llama-3-8b\",\n",
    "    \"2.*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adaf237-64eb-48f8-923b-d4880f2b84c4",
   "metadata": {},
   "source": [
    "### 1.2 Preparing training data\n",
    "\n",
    "We will go step by step to prepare domain dataset for fine tuning using filed 10-K reports by Amazon from year 2024 and 2023.\n",
    "\n",
    "We provide a subset of SEC filings data of Amazon in domain adaptation dataset format. It is downloaded from publicly available [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch). Instruction of accessing the data is shown [here](https://www.sec.gov/os/accessing-edgar-data).\n",
    "\n",
    "License: [Creative Commons Attribution-ShareAlike License (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/legalcode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba86b2-3709-482b-8e9e-472b11f80c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import smjsindustry\n",
    "import shutil\n",
    "from smjsindustry.finance.processor import DataLoader, SECXMLFilingParser\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b85aca3-8eeb-4ffa-bf1f-073d49b52f77",
   "metadata": {},
   "source": [
    "#### Prepare the SageMaker session's default S3 bucket and a folder to store processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06dd98-6d80-4d1e-a78f-1d5c46f49d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "sec_processed_folder = \"amazon_sec_filing_data\"\n",
    "default_bucket_prefix = session.default_bucket_prefix\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if default_bucket_prefix:\n",
    "    sec_processed_folder = f\"{default_bucket_prefix}/{sec_processed_folder}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739bc8e8-de98-40e1-b35e-5fa337974f73",
   "metadata": {},
   "source": [
    "#### Create local directories for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2486c1b-a910-460c-81b1-8fcd7b0cf17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local directories for data preprocessing\n",
    "if not os.path.exists(\"./rawfiles\"):\n",
    "    os.makedirs(\"./rawfiles\")\n",
    "if not os.path.exists(\"./parsedata\"):\n",
    "    os.makedirs(\"./parsedata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ebe1c-6c34-4efc-9d3a-b22c259968de",
   "metadata": {},
   "source": [
    "#### Download 10-K reports from SEC database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c860ad-b382-4c6f-b5da-604823dd3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_edgar_downloader import Downloader\n",
    "\n",
    "dl = Downloader(\"Amazon\", \"companyinfo@amazon.com\")\n",
    "# Get the latest 10-K filing for Amazon\n",
    "dl.get(\"10-K\", \"AMZN\", limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168297b7-c500-49f3-a1a0-61a0331a169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Move the downloaded files into 'rawfiles' folder\n",
    "\n",
    "\n",
    "def rename_files_in_directory(directorypath):\n",
    "    # Get the directory name\n",
    "    dir_name = os.path.basename(directorypath)\n",
    "\n",
    "    # Iterate over files in the directory\n",
    "    for count, filename in enumerate(os.listdir(directorypath), start=1):\n",
    "        # Construct the new file name\n",
    "        new_filename = f\"{dir_name}_{count:03d}{os.path.splitext(filename)[1]}\"\n",
    "\n",
    "        # Construct the old and new file paths\n",
    "        old_path = os.path.join(directorypath, filename)\n",
    "        new_path = os.path.join(directorypath, new_filename)\n",
    "\n",
    "        # Rename the file\n",
    "        os.rename(old_path, new_path)\n",
    "\n",
    "\n",
    "def looplistvalues(listvalues, tuplevalue):\n",
    "    for item in listvalues:\n",
    "        if isinstance(item, str):\n",
    "            if item.endswith(\".txt\"):\n",
    "                oldname = item\n",
    "                filenames = list(tuplevalue)[0].split(\"/\")\n",
    "                ## rename files in the directory\n",
    "                rename_files_in_directory(list(tuplevalue)[0])\n",
    "                break\n",
    "        if isinstance(item, list):\n",
    "            looplistvalues(item, tuplevalue)\n",
    "\n",
    "\n",
    "# Get the directory name and rename file for merge\n",
    "directory = \"./sec-edgar-filings/AMZN/10-K\"\n",
    "for dirs in os.walk(directory):\n",
    "    tuplevalue = dirs\n",
    "    listvalues = list(dirs)\n",
    "    looplistvalues(listvalues, tuplevalue)\n",
    "\n",
    "# Move the files\n",
    "for root, dirs, files in os.walk(\"./sec-edgar-filings/AMZN/10-K\"):\n",
    "    for file in files:\n",
    "        path_file = os.path.join(root, file)\n",
    "        shutil.copy2(path_file, \"./rawfiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65370120-7492-4899-8964-0bfbbaf32ce9",
   "metadata": {},
   "source": [
    "#### Parse the raw files using SEC Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2e893-159b-41f3-9190-7c31563e0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parser = SECXMLFilingParser(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.2xlarge\",\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    ")\n",
    "parser.parse(\n",
    "    \"rawfiles\",\n",
    "    \"s3://{}/{}/{}\".format(bucket, sec_processed_folder, \"output\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da3b09-43da-4b18-9168-3b56022a4998",
   "metadata": {},
   "source": [
    "#### Collate parsed data in S3 and download for local view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14e807-6228-4b6e-ad45-804f2db5e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3client = boto3.client(\"s3\")\n",
    "\n",
    "file_contents = []\n",
    "response = s3client.list_objects_v2(\n",
    "    Bucket=bucket, Prefix=\"{}/{}/\".format(sec_processed_folder, \"output\")\n",
    ")\n",
    "\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    file = obj[\"Key\"]\n",
    "    # print(file)\n",
    "    # Read the file contents from Amazon S3\n",
    "    try:\n",
    "        response = s3client.get_object(Bucket=bucket, Key=file)\n",
    "        file_content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "        file_contents.append(file_content)\n",
    "        file_contents.append(\"\\n\\n\")\n",
    "    except s3client.exceptions.NoSuchKey:\n",
    "        print(f\"The file {file} does not exist in the bucket {bucket}.\")\n",
    "\n",
    "# convert string data into bytes\n",
    "file_contents = \"\".join(file_contents).encode(\"utf-8\")\n",
    "\n",
    "# put file object to Amazon s3 with collated content\n",
    "filename = \"combined.txt\"\n",
    "try:\n",
    "    s3client.put_object(\n",
    "        Body=file_contents,\n",
    "        Bucket=bucket,\n",
    "        Key=\"{}/{}/{}\".format(sec_processed_folder, \"output\", filename),\n",
    "    )\n",
    "    file_contents = \"\"\n",
    "except Exception as e:\n",
    "    print(\"Fail to write\")\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    s3client.download_file(\n",
    "        bucket,\n",
    "        \"{}/{}/{}\".format(sec_processed_folder, \"output\", filename),\n",
    "        \"./parsedata/{}\".format(filename),\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Fail to download\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ce017-b057-481f-bfc8-0748ad896e55",
   "metadata": {},
   "source": [
    "### 1.3 Starting Training\n",
    "***\n",
    "We start by creating the estimator object with all the required assets and then launch the training job.  Since default hyperparameter values are model-specific, inspect estimator.hyperparameters() to view default values for your selected model.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c963f-ce48-40f7-84f2-daa646ba4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_training_data_location = \"s3://{}/{}/{}/{}\".format(\n",
    "    bucket, sec_processed_folder, \"output\", \"combined.txt\"\n",
    ")\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=modelid,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    hyperparameters={\"epoch\": \"5\", \"per_device_train_batch_size\": \"4\"},\n",
    ")\n",
    "\n",
    "estimator.fit({\"training\": domain_training_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e2fc8-a8a2-4dd3-a796-b30548fb1d4c",
   "metadata": {},
   "source": [
    "### 1.4 Deploying Inference Endpoints\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a9f16-d4c3-4090-a936-e701cde56ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can deploy the fine-tuned model to an endpoint directly from the estimator.\n",
    "domain_fine_tuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042e987-13fb-4f30-8045-df95b661c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## If kernel dies out or active reference to training estimator is not available\n",
    "# ## Then, retrieve the reference for estimator\n",
    "# from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "# training_job_name = \"Your estimator training job name\"\n",
    "# model_id = modelid\n",
    "# attached_estimator = JumpStartEstimator.attach(training_job_name, model_id)\n",
    "# attached_estimator.logs()\n",
    "# domain_adaption_predictor = attached_estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa92dd1-7d0f-4ee6-b19a-50e4ee480a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "payload = {\"inputs\": \"Risk factors highlighted in this 10-K report\", \"parameters\": parameters}\n",
    "domain_fine_tuned_predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c9e69-bf6f-48e2-9a6a-ad5d353f9d91",
   "metadata": {},
   "source": [
    "### 1.5 Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1038a49-daa5-4d18-b24d-efd9a9a8adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove local directories\n",
    "!rm -rf ./rawfiles/\n",
    "!rm -rf ./parsedata/\n",
    "!rm -rf ./sec-edgar-filings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0728f8c0-26b4-4801-9adf-82e622bbfca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Model & Endpoints\n",
    "domain_fine_tuned_predictor.delete_model()\n",
    "domain_fine_tuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369d9a3",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/generative_ai|sm-jumpstart_foundation_llama_3_8b_domain_adaption_finetuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e86bc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
