{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Tensor Parallelism using the SageMaker Model Parallelism Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through how to use the tensor parallelism feature provided by the SageMaker model parallelism library. You'll learn how to train the GPT-J model with tensor parallelism on the GLUE sst2 dataset.\n",
    "\n",
    "**Note**: To run this example training job, you must be in `us-west-2`.\n",
    "\n",
    "## Install and Upgrade Libraries\n",
    "\n",
    "The SageMaker model parallelism library's tensor parallelism feature requires the SageMaker Python SDK and the SageMaker Experiments library. Run the following cell to install or upgrade the libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To finish applying the changes, you must restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# run once, restart kernel, then comment out this cell\n",
    "# update sagemaker to the latest 2.x version\n",
    "! pip install -qU pip\n",
    "! pip install -qU \"sagemaker>=2,<3\"\n",
    "! pip install -qU sagemaker-experiments\n",
    "\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and check if the SageMaker Python SDK version is successfully set to the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.87.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Throughout this example, you'll use a training script of GPTJ model and a text dataset.\n",
    "\n",
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment: your AWS account ID, the AWS Region you are using to run the notebook, and the ARN of your Amazon SageMaker execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role:arn:aws:iam::232838030412:role/service-role/AmazonSageMaker-ExecutionRole-20211204T182243\n",
      "AWS account:232838030412\n",
      "AWS region:us-west-2\n",
      "CPU times: user 218 ms, sys: 25.5 ms, total: 243 ms\n",
      "Wall time: 575 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import boto3\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Amazon S3 Bucket Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need to specify the paths for training data to be used by your job. The bucket used must be in the same region as where training will run. On the `10_prepare_dataset` notebook you downloaded the glue-sst2 training and validation split datasets and uploaded the json files in an S3 bucket in your account. This example will train on those json files.\n",
    "\n",
    "After you successfully run this example tensor parallel training job, you can modify the S3 bucket to where your own dataset is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve locations from 10_prepare_dataset notebook\n",
    "%store -r training_dataset_location\n",
    "%store -r validation_dataset_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-232838030412/dataset/train/'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if region == \"us-west-2\":\n",
    "    s3_train_bucket = training_dataset_location\n",
    "    s3_test_bucket = validation_dataset_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below bucket will store output artifacts of the training job. You can modify this as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = f\"s3://sagemaker-{region}-{account}/smp-tensorparallel-outputdir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Channels for SageMaker Training\n",
    "\n",
    "In this step, you define SageMaker training data channels using the above buckets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set use_fsx to False by default\n",
    "# Set below var to True if you want to use fsx (see next cell)\n",
    "use_fsx = False\n",
    "if not use_fsx:\n",
    "    train = sagemaker.inputs.TrainingInput(\n",
    "        s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "    )\n",
    "    test = sagemaker.inputs.TrainingInput(\n",
    "        s3_test_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "    )\n",
    "    data_channels = {\"train\": train, \"test\": test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup fsx and use fsx for data channels and checkpoints\n",
    "\n",
    "While the above option is easier to setup, using an FSX can be beneficial for performance when dealing with large input sizes and large model sizes. If you are using models above 13B, checkpointing should be done using FSX. \n",
    "\n",
    "Please see the instructions [here](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-scriptmode-fsx.ipynb), to create the FSx lustre filesystem and import the dataset from the S3 bucket to your fsx filesystem. Note that the FSX must be created in a private subnet with internet gateway to ensure that training job has access to the internet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions obtained from:\n",
    "# https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-scriptmode-fsx.ipynb\n",
    "\n",
    "if use_fsx:\n",
    "    from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "    # Specify FSx Lustre file system id.\n",
    "    file_system_id = \"fs-id\"\n",
    "\n",
    "    # Specify the SG and subnet used by the FSX, these are passed to SM Estimator so jobs use this as well\n",
    "    fsx_security_group_id = \"sg-id\"\n",
    "    fsx_subnet = \"subnet-id\"\n",
    "\n",
    "    # Specify directory path for input data on the file system.\n",
    "    # You need to provide normalized and absolute path below.\n",
    "    # Your mount name can be provided by you when creating fsx, or generated automatically.\n",
    "    # You can find this mount_name on the FSX page in console.\n",
    "    # Example of fsx generated mount_name: \"3x8abcde\"\n",
    "    base_path = \"/3x8abcde\"\n",
    "\n",
    "    # Specify your file system type.\n",
    "    file_system_type = \"FSxLustre\"\n",
    "\n",
    "    train = FileSystemInput(\n",
    "        file_system_id=file_system_id,\n",
    "        file_system_type=file_system_type,\n",
    "        directory_path=base_path,\n",
    "        file_system_access_mode=\"rw\",\n",
    "    )\n",
    "\n",
    "    data_channels = {\"train\": train, \"test\": train}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Hyperparameters, Metric Definitions, and MPI Options\n",
    "The following `hyperparameters` dictionary is to pass arguments to the training script (`train_gptj_smp_tesnor_parallel_script.py`) and set the model parallel configuration when creating the training job.\n",
    "\n",
    "You can also add custom mpi flags. By default, we have `--mca btl_vader_single_copy_mechanism none` to remove unnecessary logs.\n",
    "\n",
    "Next we add a base metric definitions to enable the metric upload in SageMaker. You can add any further metric definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_steps\": 1000,\n",
    "    \"seed\": 12345,\n",
    "    \"fp16\": 1,\n",
    "    \"lr\": 2.0e-4,\n",
    "    \"lr_decay_iters\": 125000,\n",
    "    \"min_lr\": 0.00001,\n",
    "    \"lr-decay-style\": \"linear\",\n",
    "    \"warmup\": 0.01,\n",
    "    \"num_kept_checkpoints\": 5,\n",
    "    \"checkpoint_freq\": 200,\n",
    "    \"validation_freq\": 1000,\n",
    "    \"logging_freq\": 10,\n",
    "    \"save_final_full_model\": 1,\n",
    "    \"manual_partition\": 0,\n",
    "    \"skip_full_optimizer\": 1,\n",
    "    \"shard_optimizer_state\": 1,\n",
    "    \"activation_checkpointing\": 1,\n",
    "    \"activation_strategy\": \"each\",\n",
    "    \"optimize\": \"speed\",\n",
    "    # below flag loads model and optimizer state from checkpoint_s3_uri\n",
    "    # 'load_partial': 1,\n",
    "}\n",
    "\n",
    "\n",
    "if use_fsx:\n",
    "    # make sure to update paths for training-dir and test-dir based on the paths of datasets in fsx\n",
    "    # If you want to resume training, set checkpoint-dir to the same path as a previous job.\n",
    "    SM_TRAIN_DIR = \"/opt/ml/input/data/train\"\n",
    "    hyperparameters[\"checkpoint-dir\"] = f\"{SM_TRAIN_DIR}/checkpointdir-job2\"\n",
    "    hyperparameters[\"model-dir\"] = f\"{SM_TRAIN_DIR}/modeldir-job2\"\n",
    "    hyperparameters[\"training-dir\"] = f\"{SM_TRAIN_DIR}/datasets/pytorch_gpt2/train_synthetic\"\n",
    "    hyperparameters[\"test-dir\"] = f\"{SM_TRAIN_DIR}/datasets/pytorch_gpt2/val_synthetic\"\n",
    "\n",
    "# The checkpoint path (hyperparameters['checkpoint-dir'] or checkpoint_s3_uri) is not unique per job.\n",
    "# You need to modify as needed for different runs.\n",
    "# If same path is used for unrelated runs, this may increase time when downloading unnecessary checkpoints,\n",
    "# and cause conflicts when loading checkpoints.\n",
    "\n",
    "\n",
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "mpioptions += (\n",
    "    \"-x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    ")\n",
    "mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\"\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
    "]  # Add your custom metric definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model configuration below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \"gpt-j-6B\"\n",
    "\n",
    "if model_config == \"gpt-j-6B\":\n",
    "    model_params = {\n",
    "        \"tensor_parallel_degree\": 4,\n",
    "        \"pipeline_parallel_degree\": 1,\n",
    "        \"train_batch_size\": 8,\n",
    "        \"val_batch_size\": 8,\n",
    "        \"prescaled_batch\": 1,\n",
    "    }\n",
    "\n",
    "for k, v in model_params.items():\n",
    "    hyperparameters[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up SageMaker Studio Experiment\n",
    "Create or load [SageMaker Experiment](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) for the example training job. This will create an experiment trial object in SageMaker Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "# Specify your experiment name\n",
    "experiment_name = \"smp-gptj-tensor-parallel\"\n",
    "# Specify your trial name\n",
    "trial_name = f\"{experiment_name}-trial1\"\n",
    "\n",
    "all_experiment_names = [exp.experiment_name for exp in Experiment.list()]\n",
    "# Load the experiment if it exists, otherwise create\n",
    "if experiment_name not in all_experiment_names:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client\n",
    "    )\n",
    "else:\n",
    "    experiment = Experiment.load(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=sm_boto_client\n",
    "    )\n",
    "\n",
    "# Create the trial\n",
    "trial = Trial.create(\n",
    "    trial_name=\"smp-{}-{}\".format(trial_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())),\n",
    "    experiment_name=experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm_boto_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Essential Parameters for a SageMaker Training Job\n",
    "\n",
    "Next, you will use the [`SageMaker Estimator API`](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to define a SageMaker Training Job, passing values through the following parameters for training job name, the number of EC2 instances, the instance type, and the size of the volume attached to the instances. \n",
    "\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "* `base_job_name`\n",
    "\n",
    "### Update the Type and Number of EC2 Instance to Use\n",
    "\n",
    "The instance type and the number of instances you specify to the `instance_type` and `instance_count` parameters, respectively, will determine the total number of GPUs (world size).\n",
    "\n",
    "$$ \\text{(world size) = (the number of GPUs on a single instance)}\\times\\text{(the number of instance)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "# alternatively you can use the following instance type\n",
    "# instance_type = 'ml.p3.16xlarge'\n",
    "\n",
    "instance_count = 1\n",
    "\n",
    "# set to the number of GPUs on that instance\n",
    "processes_per_host = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look up the number of GPUs of different instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that, for example, a given instance type `p4d.24xlarge` has a corresponding instance type `ml.p4d.24xlarge` in SageMaker.\n",
    "For SageMaker supported `ml` instances and cost information, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach an EBS Volume to the Training Instance\n",
    "The volume size you specify in `volume_size` must be larger than your input data size. In this example, the volume size is set to 500GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a Base Job Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "pp_degree = hyperparameters[\"pipeline_parallel_degree\"]\n",
    "tp_degree = hyperparameters[\"tensor_parallel_degree\"]\n",
    "base_job_name = f'smp-{model_config}-{machine_str}-tp{tp_degree}-pp{pp_degree}-bs{hyperparameters[\"train_batch_size\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_fsx:\n",
    "    # If you want to resume training, set checkpoint_s3_uri to the same path as a previous job.\n",
    "    # Previous checkpoint to load must have same model config.\n",
    "    checkpoint_bucket = f\"s3://sagemaker-{region}-{account}/\"\n",
    "    checkpoint_s3_uri = (\n",
    "        f\"{checkpoint_bucket}/experiments/gptj_synthetic_simpletrainer_checkpoints/{base_job_name}/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker HuggingFace 🤗 Estimator\n",
    "\n",
    "The following cell constructs a PyTorch estimator using the parameters defined above. To see how the SageMaker tensor parallelism modules and functions are applied to the script, see the `train_gptj_smp_tensor_parallel_script.py` file and the private preview documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "if use_fsx:\n",
    "    # Use the security group and subnet that was used to create the fsx filesystem\n",
    "    kwargs[\"security_group_ids\"] = [fsx_security_group_id]\n",
    "    kwargs[\"subnets\"] = [fsx_subnet]\n",
    "\n",
    "smp_estimator = HuggingFace(\n",
    "    entry_point=\"train_gptj_smp_tensor_parallel_script.py\",\n",
    "    source_dir=os.getcwd(),\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=volume_size,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": processes_per_host,\n",
    "            \"custom_mpi_options\": mpioptions,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"ddp\": True,\n",
    "                    \"tensor_parallel_degree\": hyperparameters[\"tensor_parallel_degree\"],\n",
    "                    # partitions is a required param in the current SM SDK so it needs to be passed,\n",
    "                    # these two map to the same config\n",
    "                    \"partitions\": hyperparameters[\"pipeline_parallel_degree\"],\n",
    "                    \"shard_optimizer_state\": hyperparameters[\"shard_optimizer_state\"] > 0,\n",
    "                    \"prescaled_batch\": hyperparameters[\"prescaled_batch\"] > 0,\n",
    "                    \"fp16_params\": hyperparameters[\"fp16\"] > 0,\n",
    "                    \"optimize\": hyperparameters[\"optimize\"],\n",
    "                    \"auto_partition\": False if hyperparameters[\"manual_partition\"] else True,\n",
    "                    \"default_partition\": 0,\n",
    "                    \"fp16_params\": hyperparameters[\"fp16\"] > 0,\n",
    "                    \"optimize\": hyperparameters[\"optimize\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    pytorch_version=\"1.10\",\n",
    "    transformers_version=\"4.17\",\n",
    "    py_version=\"py38\",\n",
    "    output_path=s3_output_bucket,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri if not use_fsx else None,\n",
    "    checkpoint_local_path=hyperparameters[\"checkpoint-dir\"] if use_fsx else None,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    base_job_name=base_job_name,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the estimator to launch the SageMaker training job of GPTJ model with tensor parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: smp-gpt-j-6B-p4d24x-tp4-pp1-bs8-2022-04-21-13-21-30-847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-21 13:21:32 Starting - Starting the training job......\n",
      "2022-04-21 13:22:21 Starting - Preparing the instances for training................................................\n",
      "2022-04-21 13:30:16 Downloading - Downloading input data...\n",
      "2022-04-21 13:31:01 Training - Downloading the training image..........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-04-21 13:35:21,058 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-04-21 13:35:21,138 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-04-21 13:35:21,143 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-04-21 13:35:21,850 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.18.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.77.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.1.35)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchnet in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (0.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (4.17.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug in /opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg (from -r requirements.txt (line 8)) (1.0.13b20220304)\u001b[0m\n",
      "\u001b[34mCollecting humanize\u001b[0m\n",
      "\u001b[34mDownloading humanize-4.0.0-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.7/97.7 KB 4.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting smart-open>=5.2.1\u001b[0m\n",
      "\u001b[34mDownloading smart_open-5.2.1-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.6/58.6 KB 2.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3>=1.23 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.26.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (2022.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (7.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (0.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (2.27.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 1)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (3.19.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3>=1.20.21 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.21.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs==20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (20.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 3)) (4.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from torchnet->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: visdom in /opt/conda/lib/python3.8/site-packages (from torchnet->-r requirements.txt (line 6)) (0.1.8.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from torchnet->-r requirements.txt (line 6)) (1.10.2+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (0.11.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (0.0.49)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers->-r requirements.txt (line 7)) (2022.3.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument==3.4.2 in /opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg (from smdebug->-r requirements.txt (line 8)) (3.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg (from pyinstrument==3.4.2->smdebug->-r requirements.txt (line 8)) (0.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.20.21->sagemaker->-r requirements.txt (line 3)) (0.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.20.21->sagemaker->-r requirements.txt (line 3)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.25.0,>=1.24.13 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.20.21->sagemaker->-r requirements.txt (line 3)) (1.24.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->-r requirements.txt (line 1)) (4.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=1.4.0->sagemaker->-r requirements.txt (line 3)) (3.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets->-r requirements.txt (line 1)) (3.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker->-r requirements.txt (line 3)) (1.6.6.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers->-r requirements.txt (line 7)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers->-r requirements.txt (line 7)) (8.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (9.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websocket-client in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyzmq in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (22.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchfile in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpatch in /opt/conda/lib/python3.8/site-packages (from visdom->torchnet->-r requirements.txt (line 6)) (1.32)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.8/site-packages (from jsonpatch->visdom->torchnet->-r requirements.txt (line 6)) (2.2)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: smart-open, humanize\u001b[0m\n",
      "\u001b[34mSuccessfully installed humanize-4.0.0 smart-open-5.2.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-04-21 13:35:24,641 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-04-21 13:35:24,642 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-04-21 13:35:24,646 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2022-04-21 13:35:24,646 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:8'] process_per_hosts: 8 num_processes: 8\u001b[0m\n",
      "\u001b[34m2022-04-21 13:35:24,647 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\n",
      "2022-04-21 13:35:18 Training - Training image download completed. Training in progress.\u001b[34m2022-04-21 13:35:24,757 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"activation_checkpointing\": 1,\n",
      "        \"activation_strategy\": \"each\",\n",
      "        \"checkpoint_freq\": 200,\n",
      "        \"fp16\": 1,\n",
      "        \"logging_freq\": 10,\n",
      "        \"lr\": 0.0002,\n",
      "        \"lr-decay-style\": \"linear\",\n",
      "        \"lr_decay_iters\": 125000,\n",
      "        \"manual_partition\": 0,\n",
      "        \"max_steps\": 1000,\n",
      "        \"min_lr\": 1e-05,\n",
      "        \"mp_parameters\": {\n",
      "            \"ddp\": true,\n",
      "            \"tensor_parallel_degree\": 4,\n",
      "            \"partitions\": 1,\n",
      "            \"shard_optimizer_state\": true,\n",
      "            \"prescaled_batch\": true,\n",
      "            \"fp16_params\": true,\n",
      "            \"optimize\": \"speed\",\n",
      "            \"auto_partition\": true,\n",
      "            \"default_partition\": 0\n",
      "        },\n",
      "        \"num_kept_checkpoints\": 5,\n",
      "        \"optimize\": \"speed\",\n",
      "        \"pipeline_parallel_degree\": 1,\n",
      "        \"prescaled_batch\": 1,\n",
      "        \"save_final_full_model\": 1,\n",
      "        \"seed\": 12345,\n",
      "        \"shard_optimizer_state\": 1,\n",
      "        \"skip_full_optimizer\": 1,\n",
      "        \"tensor_parallel_degree\": 4,\n",
      "        \"train_batch_size\": 8,\n",
      "        \"val_batch_size\": 8,\n",
      "        \"validation_freq\": 1000,\n",
      "        \"warmup\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"smp-gpt-j-6B-p4d24x-tp4-pp1-bs8-2022-04-21-13-21-30-847\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-232838030412/smp-gpt-j-6B-p4d24x-tp4-pp1-bs8-2022-04-21-13-21-30-847/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_gptj_smp_tensor_parallel_script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_gptj_smp_tensor_parallel_script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"activation_checkpointing\":1,\"activation_strategy\":\"each\",\"checkpoint_freq\":200,\"fp16\":1,\"logging_freq\":10,\"lr\":0.0002,\"lr-decay-style\":\"linear\",\"lr_decay_iters\":125000,\"manual_partition\":0,\"max_steps\":1000,\"min_lr\":1e-05,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"fp16_params\":true,\"optimize\":\"speed\",\"partitions\":1,\"prescaled_batch\":true,\"shard_optimizer_state\":true,\"tensor_parallel_degree\":4},\"num_kept_checkpoints\":5,\"optimize\":\"speed\",\"pipeline_parallel_degree\":1,\"prescaled_batch\":1,\"save_final_full_model\":1,\"seed\":12345,\"shard_optimizer_state\":1,\"skip_full_optimizer\":1,\"tensor_parallel_degree\":4,\"train_batch_size\":8,\"val_batch_size\":8,\"validation_freq\":1000,\"warmup\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_gptj_smp_tensor_parallel_script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_gptj_smp_tensor_parallel_script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-232838030412/smp-gpt-j-6B-p4d24x-tp4-pp1-bs8-2022-04-21-13-21-30-847/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"activation_checkpointing\":1,\"activation_strategy\":\"each\",\"checkpoint_freq\":200,\"fp16\":1,\"logging_freq\":10,\"lr\":0.0002,\"lr-decay-style\":\"linear\",\"lr_decay_iters\":125000,\"manual_partition\":0,\"max_steps\":1000,\"min_lr\":1e-05,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"fp16_params\":true,\"optimize\":\"speed\",\"partitions\":1,\"prescaled_batch\":true,\"shard_optimizer_state\":true,\"tensor_parallel_degree\":4},\"num_kept_checkpoints\":5,\"optimize\":\"speed\",\"pipeline_parallel_degree\":1,\"prescaled_batch\":1,\"save_final_full_model\":1,\"seed\":12345,\"shard_optimizer_state\":1,\"skip_full_optimizer\":1,\"tensor_parallel_degree\":4,\"train_batch_size\":8,\"val_batch_size\":8,\"validation_freq\":1000,\"warmup\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"smp-gpt-j-6B-p4d24x-tp4-pp1-bs8-2022-04-21-13-21-30-847\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-232838030412/smp-gpt-j-6B-p4d24x-tp4-pp1-bs8-2022-04-21-13-21-30-847/source/sourcedir.tar.gz\",\"module_name\":\"train_gptj_smp_tensor_parallel_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_gptj_smp_tensor_parallel_script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--activation_checkpointing\",\"1\",\"--activation_strategy\",\"each\",\"--checkpoint_freq\",\"200\",\"--fp16\",\"1\",\"--logging_freq\",\"10\",\"--lr\",\"0.0002\",\"--lr-decay-style\",\"linear\",\"--lr_decay_iters\",\"125000\",\"--manual_partition\",\"0\",\"--max_steps\",\"1000\",\"--min_lr\",\"1e-05\",\"--mp_parameters\",\"auto_partition=True,ddp=True,default_partition=0,fp16_params=True,optimize=speed,partitions=1,prescaled_batch=True,shard_optimizer_state=True,tensor_parallel_degree=4\",\"--num_kept_checkpoints\",\"5\",\"--optimize\",\"speed\",\"--pipeline_parallel_degree\",\"1\",\"--prescaled_batch\",\"1\",\"--save_final_full_model\",\"1\",\"--seed\",\"12345\",\"--shard_optimizer_state\",\"1\",\"--skip_full_optimizer\",\"1\",\"--tensor_parallel_degree\",\"4\",\"--train_batch_size\",\"8\",\"--val_batch_size\",\"8\",\"--validation_freq\",\"1000\",\"--warmup\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_ACTIVATION_CHECKPOINTING=1\u001b[0m\n",
      "\u001b[34mSM_HP_ACTIVATION_STRATEGY=each\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINT_FREQ=200\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=1\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_FREQ=10\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LR-DECAY-STYLE=linear\u001b[0m\n",
      "\u001b[34mSM_HP_LR_DECAY_ITERS=125000\u001b[0m\n",
      "\u001b[34mSM_HP_MANUAL_PARTITION=0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=1000\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_LR=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"auto_partition\":true,\"ddp\":true,\"default_partition\":0,\"fp16_params\":true,\"optimize\":\"speed\",\"partitions\":1,\"prescaled_batch\":true,\"shard_optimizer_state\":true,\"tensor_parallel_degree\":4}\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_KEPT_CHECKPOINTS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZE=speed\u001b[0m\n",
      "\u001b[34mSM_HP_PIPELINE_PARALLEL_DEGREE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PRESCALED_BATCH=1\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_FINAL_FULL_MODEL=1\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=12345\u001b[0m\n",
      "\u001b[34mSM_HP_SHARD_OPTIMIZER_STATE=1\u001b[0m\n",
      "\u001b[34mSM_HP_SKIP_FULL_OPTIMIZER=1\u001b[0m\n",
      "\u001b[34mSM_HP_TENSOR_PARALLEL_DEGREE=4\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_VAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FREQ=1000\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/urllib3-1.26.8-py3.8.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TEST -x SM_CHANNEL_TRAIN -x SM_HP_ACTIVATION_CHECKPOINTING -x SM_HP_ACTIVATION_STRATEGY -x SM_HP_CHECKPOINT_FREQ -x SM_HP_FP16 -x SM_HP_LOGGING_FREQ -x SM_HP_LR -x SM_HP_LR-DECAY-STYLE -x SM_HP_LR_DECAY_ITERS -x SM_HP_MANUAL_PARTITION -x SM_HP_MAX_STEPS -x SM_HP_MIN_LR -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_KEPT_CHECKPOINTS -x SM_HP_OPTIMIZE -x SM_HP_PIPELINE_PARALLEL_DEGREE -x SM_HP_PRESCALED_BATCH -x SM_HP_SAVE_FINAL_FULL_MODEL -x SM_HP_SEED -x SM_HP_SHARD_OPTIMIZER_STATE -x SM_HP_SKIP_FULL_OPTIMIZER -x SM_HP_TENSOR_PARALLEL_DEGREE -x SM_HP_TRAIN_BATCH_SIZE -x SM_HP_VAL_BATCH_SIZE -x SM_HP_VALIDATION_FREQ -x SM_HP_WARMUP -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py train_gptj_smp_tensor_parallel_script.py --activation_checkpointing 1 --activation_strategy each --checkpoint_freq 200 --fp16 1 --logging_freq 10 --lr 0.0002 --lr-decay-style linear --lr_decay_iters 125000 --manual_partition 0 --max_steps 1000 --min_lr 1e-05 --mp_parameters auto_partition=True,ddp=True,default_partition=0,fp16_params=True,optimize=speed,partitions=1,prescaled_batch=True,shard_optimizer_state=True,tensor_parallel_degree=4 --num_kept_checkpoints 5 --optimize speed --pipeline_parallel_degree 1 --prescaled_batch 1 --save_final_full_model 1 --seed 12345 --shard_optimizer_state 1 --skip_full_optimizer 1 --tensor_parallel_degree 4 --train_batch_size 8 --val_batch_size 8 --validation_freq 1000 --warmup 0.01\u001b[0m\n",
      "\u001b[34m[algo-1:00036] Warning: could not find environment variable \"SM_HP_LR-DECAY-STYLE\"\u001b[0m\n",
      "\u001b[34mData for JOB [41154,1] offset 0 Total slots allocated 8\u001b[0m\n",
      "\u001b[34m========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41154,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41154,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41154,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41154,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41154,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41154,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41154,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41154,1] App: 0 Process rank: 7 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-04-21 13:35:31.518: I smdistributed/modelparallel/torch/state_mod.py:161] [5] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 1, dp_rank: 5, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-04-21 13:35:31.518: I smdistributed/modelparallel/torch/state_mod.py:161] [6] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 2, dp_rank: 6, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:35:31.519: I smdistributed/modelparallel/torch/state_mod.py:161] [1] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 1, dp_rank: 1, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:35:31.519: I smdistributed/modelparallel/torch/state_mod.py:161] [2] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 2, dp_rank: 2, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.519: I smdistributed/modelparallel/torch/state_mod.py:161] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.519: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:35:31.519: I smdistributed/modelparallel/torch/state_mod.py:161] [3] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 3, dp_rank: 3, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-04-21 13:35:31.528: I smdistributed/modelparallel/torch/state_mod.py:161] [4] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 4, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-04-21 13:35:31.528: I smdistributed/modelparallel/torch/state_mod.py:161] [7] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 3, dp_rank: 7, rdp_rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.652: I smdistributed/modelparallel/backend/config.py:230] Configuration parameters:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.652: I smdistributed/modelparallel/backend/config.py:233]   pipeline_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.652: I smdistributed/modelparallel/backend/config.py:233]   microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   pipeline: interleaved\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   horovod: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   ddp: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   tensor_parallel_degree: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   ddp_port: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   ddp_dist_backend: nccl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   contiguous: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   placement_strategy: cluster\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   optimize: speed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   default_partition: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   auto_partition: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   prescaled_batch: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   active_microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   fp16_params: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.653: I smdistributed/modelparallel/backend/config.py:233]   offload_activations: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.654: I smdistributed/modelparallel/backend/config.py:233]   shard_optimizer_state: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.654: I smdistributed/modelparallel/backend/config.py:233]   skip_tracing: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:31.654: I smdistributed/modelparallel/backend/config.py:233]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Arguments:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'train_batch_size': 8, 'val_batch_size': 8, 'max_steps': 1000, 'seed': 12345, 'same_seed': 0, 'n_gpus': '8', 'fp16': 1, 'fp32_grad_accumulation': 0, 'megatron': 0, 'grad_clip': 1.0, 'weight_decay': 0.01, 'beta1': 0.9, 'beta2': 0.95, 'activation_checkpointing': 1, 'logging_freq': 10, 'use_bert_data': 0, 'zipped_data': 0, 'epochs': 30, 'output_data_dir': '/opt/ml/output/data', 'checkpoint_dir': '/opt/ml/checkpoints', 'model_dir': '/opt/ml/model', 'training_dir': '/opt/ml/input/data/train', 'test_dir': '/opt/ml/input/data/test', 'parallel_proc_data_processing': 0, 'save_final_full_model': 1, 'skip_full_optimizer': 1, 'load_partial': 0, 'load_full': 0, 'logits_output': '', 'prescaled_batch': 1, 'max_context_width': 1024, 'vocab_size': 50400, 'hidden_width': 768, 'num_layers': 12, 'num_heads': 12, 'resid_pdrop': 0.1, 'embd_pdrop': 0.1, 'attn_pdrop': 0.1, 'summary_first_pdrop': 0.1, 'use_adamw': 0, 'tensor_parallel_degree': 4, 'pipeline_parallel_degree': 1, 'microbatches': 1, 'active_microbatches': None, 'optimize': 'speed', 'activation_strategy': 'each', 'shard_optimizer_state': 1, 'offload_activations': 0, 'fast_mode': 0, 'static_mode': 0, 'delayed_param': 0, 'same_partition_load': 0, 'attention_in_fp32': 0, 'placement_strategy': 'cluster', 'activation_loading_horizon': 4, 'skip_tracing': 0, 'query_key_layer_scaling': 1, 'fused_softmax': 1, 'fused_bias_gelu': 1, 'num_kept_checkpoints': 5, 'checkpoint_freq': 200, 'validation_freq': 1000, 'validation_batches': 10, 'manual_partition': 0, 'partition_assignment': '', 'match_weights': 0, 'preserve_np_state': 0, 'fast_validation': 1, 'gather_if_shard': 1, 'clean_cache': 0, 'use_fsx': 0, 'enable_memory_profiling': 0, 'lr': 0.0002, 'lr_decay_style': 'linear', 'lr_decay_iters': 125000, 'min_lr': 1e-05, 'warmup': 0.01, 'plateau': 0.4, 'ci': False, 'time_to_train': None, 'throughput': None, 'loss': None, 'save_or_verify_ckptsum': False}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Transformers version: 4.17.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:smdistributed.modelparallel version: 1.8.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:smdistributed config: {'ddp': True, 'tensor_parallel_degree': 4, 'pipeline_parallel_degree': 1, 'microbatches': 1, 'checkpoint_attentions': False, 'shard_optimizer_state': True, 'prescaled_batch': True, '_match_weights': False, 'fp16_params': True, 'offload_activations': False, 'optimize': 'speed', 'placement_strategy': 'cluster', 'activation_loading_horizon': 4, 'skip_tracing': False, 'auto_partition': True, 'default_partition': 0, '_fp32_grad_accumulation': False, 'static_mode': False, 'fast_mode': False}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[Warning] Note that save_final_full_model only saves the final model at the end of all steps. It does not save optimizer state. Optimizer state is only saved with partial models which are saved at checkpointing_freq during training. If you want to restart training you need partial checkpoints.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:# total parameters: 162465504\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Learning rate decay style: linear\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Creating val dataloader\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Created val dataloader\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:43.733: W smdistributed/modelparallel/backend/split.py:166] Non-splittable object of type <class 'fp16.fp16.FP16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:35:43.733: W smdistributed/modelparallel/backend/split.py:166] Non-splittable object of type <class 'fp16.fp16.FP16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:35:43.733: W smdistributed/modelparallel/backend/split.py:166] Non-splittable object of type <class 'fp16.fp16.FP16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:35:43.733: W smdistributed/modelparallel/backend/split.py:166] Non-splittable object of type <class 'fp16.fp16.FP16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:43.735: I smdistributed/modelparallel/torch/worker.py:296] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-04-21 13:35:43.807: W smdistributed/modelparallel/backend/split.py:166] Non-splittable object of type <class 'fp16.fp16.FP16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-04-21 13:35:43.808: W smdistributed/modelparallel/backend/split.py:166] Non-splittable object of type <class 'fp16.fp16.FP16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-04-21 13:35:43.808: W smdistributed/modelparallel/backend/split.py:166] Non-splittable object of type <class 'fp16.fp16.FP16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:WARNING: Will not set fp16 gradients to None since shard_optimizer_state is enabled.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-04-21 13:35:43.810: W smdistributed/modelparallel/backend/split.py:166] Non-splittable object of type <class 'fp16.fp16.FP16_Optimizer'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:45.975: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:45.988: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.440: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.443: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.446: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.449: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.452: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.455: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.458: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.460: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.463: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.466: W smdistributed/modelparallel/torch/nn/transformer.py:1469] Using fused softmax kernel in attention computation, which ignores the attention mask input. To use an attention mask that masks at least one token, disable the fused softmax kernel by passing fused_softmax=False into the smp.tensor_parallelism or smp.set_tensor_parallelism calls.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.661: I smdistributed/modelparallel/torch/model.py:493] Partition assignments:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.661: I smdistributed/modelparallel/torch/model.py:502] main: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.661: I smdistributed/modelparallel/torch/model.py:507] Tensor-parallel distributed modules:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.661: I smdistributed/modelparallel/torch/model.py:516] main/module/module/module\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.665: I smdistributed/modelparallel/torch/model.py:427] Number of parameters on partition 0 are 125. 125 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:46.742: I smdistributed/modelparallel/torch/model.py:548] Finished partitioning the model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:51.883: I smdistributed/modelparallel/torch/model.py:556] Broadcasted parameters and buffers for partition 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-04-21 13:35:55.765: I smdistributed/modelparallel/torch/ddp_model.py:630] [5] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:35:55.765: I smdistributed/modelparallel/torch/ddp_model.py:630] [2] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:35:55.765: I smdistributed/modelparallel/torch/ddp_model.py:630] [1] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:35:55.765: I smdistributed/modelparallel/torch/ddp_model.py:630] [3] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-04-21 13:35:55.765: I smdistributed/modelparallel/torch/ddp_model.py:630] [6] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-04-21 13:35:55.765: I smdistributed/modelparallel/torch/ddp_model.py:630] [7] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-04-21 13:35:55.766: I smdistributed/modelparallel/torch/ddp_model.py:630] [4] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:55.766: I smdistributed/modelparallel/torch/ddp_model.py:630] [0] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:35:55.766: I smdistributed/modelparallel/torch/ddp_model.py:630] [2] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-04-21 13:35:55.766: I smdistributed/modelparallel/torch/ddp_model.py:630] [4] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:35:55.766: I smdistributed/modelparallel/torch/ddp_model.py:630] [0] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-04-21 13:35:55.766: I smdistributed/modelparallel/torch/ddp_model.py:630] [5] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:35:55.766: I smdistributed/modelparallel/torch/ddp_model.py:630] [3] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-04-21 13:35:55.766: I smdistributed/modelparallel/torch/ddp_model.py:630] [7] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:35:55.766: I smdistributed/modelparallel/torch/ddp_model.py:630] [1] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-04-21 13:35:55.766: I smdistributed/modelparallel/torch/ddp_model.py:630] [6] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(17s), Batch 9 Loss: 10.96875, Speed: 126.64367629547294 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(18s), Batch 19 Loss: 10.96875, Speed: 114.3908240335593 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(19s), Batch 29 Loss: 10.8671875, Speed: 124.037017664161 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(21s), Batch 39 Loss: 10.65625, Speed: 120.85589972626423 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(22s), Batch 49 Loss: 10.3046875, Speed: 121.27041356603576 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(23s), Batch 59 Loss: 9.9375, Speed: 107.57849547861298 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(25s), Batch 69 Loss: 9.7421875, Speed: 121.67321910978153 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(26s), Batch 79 Loss: 9.5859375, Speed: 125.49013975997337 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(27s), Batch 89 Loss: 9.4453125, Speed: 125.32468813027565 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(29s), Batch 99 Loss: 9.234375, Speed: 127.8349299477108 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(30s), Batch 109 Loss: 9.21875, Speed: 111.21510734734801 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(32s), Batch 119 Loss: 9.1796875, Speed: 108.1771162274869 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(33s), Batch 129 Loss: 9.03125, Speed: 110.17980084881421 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(35s), Batch 139 Loss: 8.890625, Speed: 120.61154015510284 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(36s), Batch 149 Loss: 8.84375, Speed: 121.93032175541937 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(37s), Batch 159 Loss: 8.6484375, Speed: 116.64615170687617 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(39s), Batch 169 Loss: 8.6953125, Speed: 122.09781089892874 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(40s), Batch 179 Loss: 8.5546875, Speed: 123.63392243594822 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(41s), Batch 189 Loss: 8.4140625, Speed: 120.11416318691842 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(43s), Batch 199 Loss: 8.296875, Speed: 123.74061050826519 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:36:27.318: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [1] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:36:27.318: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [3] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:36:27.318: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [2] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:36:27.319: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [0] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-04-21 13:36:27.321: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [5] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-04-21 13:36:27.323: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [6] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-04-21 13:36:27.323: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [7] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-04-21 13:36:27.327: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [4] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Finished checkpointing after 200 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-200.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Finished checkpointing after 200 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-200.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Finished checkpointing after 200 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-200.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Finished checkpointing after 200 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-200.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Finished checkpointing after 200 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-200.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Finished checkpointing after 200 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-200.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Finished checkpointing after 200 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-200.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Finished checkpointing after 200 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-200.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(46s), Batch 209 Loss: 8.140625, Speed: 101.63697825165082 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(48s), Batch 219 Loss: 8.0625, Speed: 107.40459938254514 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(49s), Batch 229 Loss: 7.85546875, Speed: 111.90536491171316 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(51s), Batch 239 Loss: 7.80078125, Speed: 120.57123453307725 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(52s), Batch 249 Loss: 7.65234375, Speed: 122.42613261140036 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(54s), Batch 259 Loss: 7.68359375, Speed: 119.5320235826372 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(55s), Batch 269 Loss: 7.3828125, Speed: 120.0069813021273 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(56s), Batch 279 Loss: 7.27734375, Speed: 115.8871605006493 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(58s), Batch 289 Loss: 7.22265625, Speed: 117.39029829990221 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(59s), Batch 299 Loss: 7.09765625, Speed: 117.49779217368467 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(60s), Batch 309 Loss: 6.96875, Speed: 121.98728300583498 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(62s), Batch 319 Loss: 7.03125, Speed: 120.18622643604466 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(63s), Batch 329 Loss: 6.9921875, Speed: 116.29563957619217 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(65s), Batch 339 Loss: 6.7890625, Speed: 123.33059628256068 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(66s), Batch 349 Loss: 6.73828125, Speed: 114.85736975422742 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(67s), Batch 359 Loss: 6.7890625, Speed: 117.07246543678312 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(69s), Batch 369 Loss: 6.6796875, Speed: 118.73829184529613 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(70s), Batch 379 Loss: 6.50390625, Speed: 115.6542893284665 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(72s), Batch 389 Loss: 6.49609375, Speed: 117.28566559999162 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(73s), Batch 399 Loss: 6.484375, Speed: 117.0567414730805 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:36:57.466: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [3] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:36:57.469: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [0] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-04-21 13:36:57.472: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [6] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:36:57.474: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [2] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-04-21 13:36:57.484: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [4] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-04-21 13:36:57.490: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [5] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-04-21 13:36:57.494: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [7] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:36:57.496: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [1] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Finished checkpointing after 400 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-400.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Finished checkpointing after 400 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-400.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Finished checkpointing after 400 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-400.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Finished checkpointing after 400 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-400.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Finished checkpointing after 400 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-400.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Finished checkpointing after 400 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-400.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Finished checkpointing after 400 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-400.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Finished checkpointing after 400 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-400.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(76s), Batch 409 Loss: 6.51171875, Speed: 96.50369714208472 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(78s), Batch 419 Loss: 6.44140625, Speed: 102.58110097324531 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(79s), Batch 429 Loss: 6.3046875, Speed: 109.991991804958 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(81s), Batch 439 Loss: 6.25, Speed: 117.96166664322979 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(82s), Batch 449 Loss: 6.2890625, Speed: 113.29067458977649 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(83s), Batch 459 Loss: 6.23828125, Speed: 115.60567650534541 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(85s), Batch 469 Loss: 6.109375, Speed: 124.5041622681625 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(86s), Batch 479 Loss: 6.16796875, Speed: 120.04605152533156 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(87s), Batch 489 Loss: 6.1328125, Speed: 118.30478762600175 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(89s), Batch 499 Loss: 6.20703125, Speed: 118.75699264193214 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(90s), Batch 509 Loss: 6.015625, Speed: 107.23006519238145 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(92s), Batch 519 Loss: 6.0, Speed: 119.70109714485487 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(93s), Batch 529 Loss: 6.015625, Speed: 121.63308272857111 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(94s), Batch 539 Loss: 5.98828125, Speed: 119.47669354982285 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(96s), Batch 549 Loss: 5.9140625, Speed: 120.4424806079119 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(97s), Batch 559 Loss: 6.01171875, Speed: 118.91861087385661 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(99s), Batch 569 Loss: 6.0078125, Speed: 117.66631365590568 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(100s), Batch 579 Loss: 5.859375, Speed: 121.17581715160685 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(101s), Batch 589 Loss: 5.83203125, Speed: 118.25058765070114 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(103s), Batch 599 Loss: 5.91796875, Speed: 114.23368682656135 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:37:27.272: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [3] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:37:27.281: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [0] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-04-21 13:37:27.288: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [4] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:37:27.291: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [2] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-04-21 13:37:27.296: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [6] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-04-21 13:37:27.304: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [5] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:37:27.307: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [1] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-04-21 13:37:27.316: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [7] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Finished checkpointing after 600 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-600.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Finished checkpointing after 600 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-600.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Finished checkpointing after 600 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-600.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Finished checkpointing after 600 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-600.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Finished checkpointing after 600 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-600.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Finished checkpointing after 600 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-600.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Finished checkpointing after 600 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-600.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Finished checkpointing after 600 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-600.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(106s), Batch 609 Loss: 5.9140625, Speed: 92.0767689850666 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(108s), Batch 619 Loss: 5.7734375, Speed: 117.03061228048759 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(109s), Batch 629 Loss: 5.77734375, Speed: 110.63189338538335 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(110s), Batch 639 Loss: 5.78125, Speed: 115.99131649175204 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(112s), Batch 649 Loss: 5.84375, Speed: 106.60001048386201 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(113s), Batch 659 Loss: 5.80859375, Speed: 111.04727076566164 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(115s), Batch 669 Loss: 5.7265625, Speed: 103.02551655636256 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(116s), Batch 679 Loss: 5.6796875, Speed: 110.35246295212224 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(118s), Batch 689 Loss: 5.7578125, Speed: 111.4794811822241 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(119s), Batch 699 Loss: 5.734375, Speed: 121.1854452209401 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(121s), Batch 709 Loss: 5.64453125, Speed: 116.31700603867246 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(122s), Batch 719 Loss: 5.76171875, Speed: 105.1367050968117 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(123s), Batch 729 Loss: 5.7421875, Speed: 112.89463997927454 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(125s), Batch 739 Loss: 5.80859375, Speed: 123.74403304322172 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(126s), Batch 749 Loss: 5.66015625, Speed: 121.66571909264954 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(128s), Batch 759 Loss: 5.68359375, Speed: 120.33018113486317 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(129s), Batch 769 Loss: 5.703125, Speed: 121.02459486461824 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(130s), Batch 779 Loss: 5.69140625, Speed: 117.74352496148839 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(132s), Batch 789 Loss: 5.64453125, Speed: 117.93533919944608 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(133s), Batch 799 Loss: 5.72265625, Speed: 125.07056701107781 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:37:57.484: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [3] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:37:57.486: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [0] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:37:57.491: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [2] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-04-21 13:37:57.492: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [6] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-04-21 13:37:57.493: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [4] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-04-21 13:37:57.509: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [5] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-04-21 13:37:57.512: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [7] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:37:57.514: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [1] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Finished checkpointing after 800 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-800.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Finished checkpointing after 800 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-800.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Finished checkpointing after 800 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-800.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Finished checkpointing after 800 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-800.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Finished checkpointing after 800 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-800.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Finished checkpointing after 800 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-800.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Finished checkpointing after 800 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-800.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Finished checkpointing after 800 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-800.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(136s), Batch 809 Loss: 5.75, Speed: 112.81189630072923 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(138s), Batch 819 Loss: 5.625, Speed: 107.30053131530518 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(139s), Batch 829 Loss: 5.6171875, Speed: 109.00117757698511 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(141s), Batch 839 Loss: 5.73046875, Speed: 120.90859364584047 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(142s), Batch 849 Loss: 5.7578125, Speed: 115.44538485886902 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(143s), Batch 859 Loss: 5.6484375, Speed: 118.96983954488967 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(145s), Batch 869 Loss: 5.69140625, Speed: 120.45307348102891 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(146s), Batch 879 Loss: 5.69140625, Speed: 122.5781930963436 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(147s), Batch 889 Loss: 5.76171875, Speed: 123.43040437890153 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(149s), Batch 899 Loss: 5.76953125, Speed: 117.574370420724 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(150s), Batch 909 Loss: 5.69140625, Speed: 121.41456239540459 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(152s), Batch 919 Loss: 5.67578125, Speed: 110.51747625258554 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(153s), Batch 929 Loss: 5.73046875, Speed: 119.8180006784624 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(154s), Batch 939 Loss: 5.73828125, Speed: 114.0364664579956 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(156s), Batch 949 Loss: 5.640625, Speed: 112.18241024920931 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(157s), Batch 959 Loss: 5.78515625, Speed: 118.94938494806622 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Reading data from training path ['/opt/ml/input/data/train/training.json']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(159s), Batch 969 Loss: 5.77734375, Speed: 117.99464434852175 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(160s), Batch 979 Loss: 5.84765625, Speed: 121.46510647155178 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(161s), Batch 989 Loss: 5.71484375, Speed: 110.2297502172274 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(163s), Batch 999 Loss: 5.73046875, Speed: 101.58620668775828 samples/sec\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(163s) Batch 999 Validation loss: 6.26953125\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:(163s) Batch 999 Validation perplexity: 528.2297121579952\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:38:27.083: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [0] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:38:27.084: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [3] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-04-21 13:38:27.087: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [6] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:38:27.087: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [2] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-04-21 13:38:27.088: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [4] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-04-21 13:38:27.109: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [7] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-04-21 13:38:27.110: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [5] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:38:27.115: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [1] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-04-21 13:38:27.822: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [6] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-04-21 13:38:27.824: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [4] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-04-21 13:38:27.850: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [5] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:38:27.876: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [1] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-04-21 13:38:27.887: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [7] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:38:27.893: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [3] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:38:27.919: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [2] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:38:27.954: I smdistributed/modelparallel/torch/optimizers/optimizer.py:138] [0] rdp rank 0 is gathering optimizer state_dict from other rdp ranks with optimizer state sharding. To prevent hangs, please ensure optimizer.local_state_dict() (where optimizer is the object returned by the DistributedOptimizer wrapper) is called on all the ranks. Only rdp rank 0 will contain full optimizer state dict of that rdp group.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:With shard_optimizer_state=True, gather full fp32_from_fp16_groups for the rdp_group on rdp rank 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:time to train: 164.27577567100525\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:time to train: 164.30304884910583\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:time to train: 164.30298495292664\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:time to train: 164.3309588432312\u001b[0m\n",
      "\n",
      "2022-04-21 13:38:39 Uploading - Uploading generated training model\u001b[34m[1,mpirank:1,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:time to train: 165.42381286621094\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:time to train: 165.4411005973816\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/checkpoints/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:time to train: 165.49949479103088\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:time to train: 165.52106761932373\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:38:29.196: I smdistributed/modelparallel/torch/model.py:672] [1] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-04-21 13:38:29.196: W smdistributed/modelparallel/torch/model.py:682] [1] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:38:29.209: I smdistributed/modelparallel/torch/model.py:672] [2] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-04-21 13:38:29.209: W smdistributed/modelparallel/torch/model.py:682] [2] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:38:29.265: I smdistributed/modelparallel/torch/model.py:672] [0] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-04-21 13:38:29.265: W smdistributed/modelparallel/torch/model.py:682] [0] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:38:29.281: I smdistributed/modelparallel/torch/model.py:672] [3] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with rdp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-04-21 13:38:29.281: W smdistributed/modelparallel/torch/model.py:682] [3] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Skipping saving the final optimizer state\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/model/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Skipping saving the final optimizer state\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/model/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Skipping saving the final optimizer state\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/model/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Skipping saving the final optimizer state\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Finished checkpointing after 1000 steps: /opt/ml/model/trained_gpt_nparams-162465504_steps-1000.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:SMP training finished successfully\u001b[0m\n",
      "\u001b[34m2022-04-21 13:38:33,502 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-04-21 13:39:40 Completed - Training job completed\n",
      "Training seconds: 564\n",
      "Billable seconds: 564\n"
     ]
    }
   ],
   "source": [
    "smp_estimator.fit(\n",
    "    inputs=data_channels,\n",
    "    experiment_config={\n",
    "        \"ExperimentName\": experiment.experiment_name,\n",
    "        \"TrialName\": trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Training\",\n",
    "    },\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Training Logs\n",
    "\n",
    "You can access the training logs from [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html). Make sure to look at the logs of algo-1 as that is the master node whose output stream will have the training job logs.\n",
    "\n",
    "You can use CloudWatch to track SageMaker GPU and memory utilization during training and inference. To view the metrics and logs that SageMaker writes to CloudWatch, see *Processing Job, Training Job, Batch Transform Job, and Endpoint Instance Metrics* in [Monitor Amazon SageMaker with Amazon CloudWatch](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html).\n",
    "\n",
    "If you are a new user of CloudWatch, see [Getting Started with Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html). \n",
    "\n",
    "For additional information on monitoring and analyzing Amazon SageMaker training jobs, see [Monitor and Analyze Training Jobs Using Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html).\n",
    "\n",
    "# Deploying Trained Model for Inference\n",
    "\n",
    "In most cases the trained model can be deployed on a single device for inference, since inference has smaller memory requirements. You can use the SMP API to create a single, unified model after training. For TensorFlow, a SavedModel can be created using `smp.DistributedModel.save_model` API, and for PyTorch, `smp.save()` can be used.\n",
    "\n",
    "After you build and train your models, you can deploy them to get predictions in one of two ways:\n",
    "\n",
    "* To set up a persistent endpoint to get predictions from your models, use SageMaker hosting services. For an overview on deploying a single model or multiple models with SageMaker hosting services, see [Deploy a Model on SageMaker Hosting Services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html#how-it-works-hosting).\n",
    "* To get predictions for an entire dataset, use SageMaker batch transform. For an overview on deploying a model with SageMaker batch transform, see [Get Inferences for an Entire Dataset with Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html).\n",
    "\n",
    "To learn more about deploying models for inference using SageMaker, see [Deploy Models for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
