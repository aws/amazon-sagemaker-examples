{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker scikit-learn Bring Your Own Model\n",
    "_**Hosting a pre-trained scikit-learn Model in Amazon SageMaker scikit-learn Container**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "Amazon SageMaker includes functionality to support a hosted notebook environment, distributed, serverless training, and real-time hosting. We think it works best when all three of these services are used together, but they can also be used independently.  Some use cases may only require hosting.  Maybe the model was trained prior to Amazon SageMaker existing, in a different service.\n",
    "\n",
    "This notebook shows how to use a pre-trained scikit-learn model with the Amazon SageMaker scikit-learn container to quickly create a hosted endpoint for that model.\n",
    "We use the California Housing dataset, present in Scikit-Learn: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html. The California Housing dataset was originally published in:\n",
    "\n",
    "> Pace, R. Kelley, and Ronald Barry. \"Sparse spatial auto-regressions.\" Statistics & Probability Letters 33.3 (1997): 291-297.\n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "Ensure we have the latest verion of the SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.72.2)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.72.3.tar.gz (475 kB)\n",
      "     |████████████████████████████████| 475 kB 26.6 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from sagemaker) (19.3.0)\n",
      "Requirement already satisfied: boto3>=1.20.18 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.20.23)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.20.3)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.19.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.18->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.18->sagemaker) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.23 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.20.18->sagemaker) (1.23.23)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (2.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.20.18->sagemaker) (1.26.7)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.72.3-py2.py3-none-any.whl size=654043 sha256=87c03e2c1f8789c90a06faa1b2bd07c8bd42ac1ab677a664e9218a658de55094\n",
      "  Stored in directory: /root/.cache/pip/wheels/b0/af/a0/5c66d761bdb3fdaf0e10e9ab0c260d60f72098eeb3d778deac\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.72.2\n",
      "    Uninstalling sagemaker-2.72.2:\n",
      "      Successfully uninstalled sagemaker-2.72.2\n",
      "Successfully installed sagemaker-2.72.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by specifying:\n",
    "\n",
    "* AWS region.\n",
    "* The IAM role arn used to give learning and hosting access to your data.\n",
    "* The S3 bucket that you want to use for training and model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket: sagemaker-us-east-1-862774760132\n",
      "sagemaker version: 2.72.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role, image_uris, ModelPackage\n",
    "from sagemaker.sklearn.model import SKLearnModel, SKLearnPredictor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"sagemaker/DEMO-sklearn-byo-model\"\n",
    "\n",
    "print(f\"bucket: {bucket}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model inference\n",
    "\n",
    "We load the California housing dataset from sklearn, and will use it to invoke SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.6812</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.192201</td>\n",
       "      <td>1.022284</td>\n",
       "      <td>1392.0</td>\n",
       "      <td>3.877437</td>\n",
       "      <td>36.06</td>\n",
       "      <td>-119.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.5313</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.039384</td>\n",
       "      <td>1.193493</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>2.679795</td>\n",
       "      <td>35.14</td>\n",
       "      <td>-119.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.4801</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.977155</td>\n",
       "      <td>1.185877</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>1.360332</td>\n",
       "      <td>37.80</td>\n",
       "      <td>-122.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.7376</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.163636</td>\n",
       "      <td>1.020202</td>\n",
       "      <td>1705.0</td>\n",
       "      <td>3.444444</td>\n",
       "      <td>34.28</td>\n",
       "      <td>-118.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.7250</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.492991</td>\n",
       "      <td>1.028037</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>2.483645</td>\n",
       "      <td>36.62</td>\n",
       "      <td>-121.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.7147</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.251483</td>\n",
       "      <td>0.975089</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2.846975</td>\n",
       "      <td>34.08</td>\n",
       "      <td>-117.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0839</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6.221719</td>\n",
       "      <td>1.095023</td>\n",
       "      <td>670.0</td>\n",
       "      <td>3.031674</td>\n",
       "      <td>33.89</td>\n",
       "      <td>-118.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.6908</td>\n",
       "      <td>38.0</td>\n",
       "      <td>4.962825</td>\n",
       "      <td>1.048327</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>3.758364</td>\n",
       "      <td>33.92</td>\n",
       "      <td>-118.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.8036</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.924658</td>\n",
       "      <td>1.035959</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>1.797945</td>\n",
       "      <td>37.39</td>\n",
       "      <td>-122.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.1132</td>\n",
       "      <td>45.0</td>\n",
       "      <td>6.879056</td>\n",
       "      <td>1.011799</td>\n",
       "      <td>943.0</td>\n",
       "      <td>2.781711</td>\n",
       "      <td>34.18</td>\n",
       "      <td>-118.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  1.6812      25.0  4.192201   1.022284      1392.0  3.877437     36.06   \n",
       "1  2.5313      30.0  5.039384   1.193493      1565.0  2.679795     35.14   \n",
       "2  3.4801      52.0  3.977155   1.185877      1310.0  1.360332     37.80   \n",
       "3  5.7376      17.0  6.163636   1.020202      1705.0  3.444444     34.28   \n",
       "4  3.7250      34.0  5.492991   1.028037      1063.0  2.483645     36.62   \n",
       "5  4.7147      12.0  5.251483   0.975089      2400.0  2.846975     34.08   \n",
       "6  5.0839      36.0  6.221719   1.095023       670.0  3.031674     33.89   \n",
       "7  3.6908      38.0  4.962825   1.048327      1011.0  3.758364     33.92   \n",
       "8  4.8036       4.0  3.924658   1.035959      1050.0  1.797945     37.39   \n",
       "9  8.1132      45.0  6.879056   1.011799       943.0  2.781711     34.18   \n",
       "\n",
       "   Longitude  \n",
       "0    -119.01  \n",
       "1    -119.46  \n",
       "2    -122.44  \n",
       "3    -118.72  \n",
       "4    -121.93  \n",
       "5    -117.61  \n",
       "6    -118.02  \n",
       "7    -118.08  \n",
       "8    -122.08  \n",
       "9    -118.23  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fetch_california_housing()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# we don't train a model, so we will need only the testing data\n",
    "testX = pd.DataFrame(X_test, columns=data.feature_names)\n",
    "\n",
    "testX.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the pre-trained model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.7/site-packages (3.3.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from lightgbm) (0.34.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from lightgbm) (1.20.3)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.7/site-packages (from lightgbm) (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm) (0.14.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63346062 0.96964309 4.99226567 2.50300127 2.43031094]\n",
      "[0.477   0.458   5.00001 2.186   2.78   ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "model = lgb.LGBMRegressor()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.predict(X_test[:5, :]))\n",
    "print(y_test[:5])\n",
    "\n",
    "model_file_name = \"model.joblib\"\n",
    "\n",
    "joblib.dump(model, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Inference Script\n",
    "\n",
    "When using endpoints with the Amazon SageMaker managed `Scikit Learn` container, we need to provide an entry point script for inference that will **at least** load the saved model.\n",
    "\n",
    "After the SageMaker model server has loaded your model by calling `model_fn`, SageMaker will serve your model. Model serving is the process of responding to inference requests, received by SageMaker `InvokeEndpoint` API calls.\n",
    "\n",
    "\n",
    "We will implement also the `predict_fn()` function that takes the deserialized request object and performs inference against the loaded model.\n",
    "\n",
    "We will now create this script and call it `inference.py` and store it at the root of a directory called `code`.\n",
    "\n",
    "**Note:** You would modify the script below to implement your own inferencing logic.\n",
    "\n",
    "Additional information on model loading and model serving for scikit-learn on SageMaker can be found in the [SageMaker Scikit-learn Model Server documentation](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#deploy-a-scikit-learn-model)\n",
    "\n",
    "There are also several functions for hosting which we won't define,\n",
    " - `input_fn()` - Takes request data and deserializes the data into an object for prediction.\n",
    " - `output_fn()` - Takes the result of prediction and serializes this according to the response content type.\n",
    "\n",
    "These will take on their default values as described [SageMaker Scikit-learn Serve a Model documentation](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#serve-a-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code_path = \"./code\"\n",
    "model_code_inference = \"inference.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $model_code_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $model_code_path/$model_code_inference\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "\n",
    "def predict_fn(input_object, model):\n",
    "    ###########################################\n",
    "    # Do your custom preprocessing logic here #\n",
    "    ###########################################\n",
    "\n",
    "    print(\"calling model\")\n",
    "    predictions = model.predict(input_object)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    print(\"loading model.joblib from: {}\".format(model_dir))\n",
    "    loaded_model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing additional Python dependencies\n",
    "\n",
    "It also may be necessary to supply a `requirements.txt` file to ensure any necessary dependencies are installed in the container along with the script. For this script, in addition to the Python standard libraries, we showcase how to install the `boto3` and `requests` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $model_code_path/requirements.txt\n",
    "\n",
    "lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package the pre-trained model in `model.tar.gz` and upload it to S3\n",
    "The model file name must satisfy the regular expression pattern: `^[a-zA-Z0-9](-*[a-zA-Z0-9])*;`, and needs to be tar-zipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.joblib\n"
     ]
    }
   ],
   "source": [
    "model_tar_name = \"model.tar.gz\"\n",
    "!tar czvf $model_tar_name $model_file_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model data: s3://sagemaker-us-east-1-862774760132/sagemaker/DEMO-sklearn-byo-model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "key = os.path.join(prefix, model_tar_name)\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(model_tar_name, bucket, key)\n",
    "model_data = f\"s3://{bucket}/{key}\"\n",
    "print(f\"model data: {model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "Here we showcase the process of creating a model from s3 artifacts, that could be used to deploy a model that was trained in a different session or even out of SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SKLearnModel(\n",
    "    role=role,\n",
    "    model_data=model_data,\n",
    "    framework_version=\"0.23-1\",\n",
    "    py_version=\"py3\",\n",
    "    source_dir=model_code_path,\n",
    "    entry_point=model_code_inference,\n",
    "    sagemaker_session=sagemaker.Session(),  # Required for model.register().\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model version\n",
    "Create a model group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelPackageGroup Arn : arn:aws:sagemaker:us-east-1:862774760132:model-package-group/scikit-housing-prediction1641997896\n"
     ]
    }
   ],
   "source": [
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "model_package_group_name = \"scikit-housing-prediction\" + str(round(time.time()))\n",
    "model_package_group_input_dict = {\n",
    "    \"ModelPackageGroupName\": model_package_group_name,\n",
    "    \"ModelPackageGroupDescription\": \"For predicting ln(median house value)\",\n",
    "}\n",
    "\n",
    "create_model_pacakge_group_response = sm_client.create_model_package_group(\n",
    "    **model_package_group_input_dict\n",
    ")\n",
    "model_package_group_arn = create_model_pacakge_group_response[\"ModelPackageGroupArn\"]\n",
    "print(\"ModelPackageGroup Arn : {}\".format(model_package_group_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model to the model group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:862774760132:model-package/scikit-housing-prediction1641997896/1\n"
     ]
    }
   ],
   "source": [
    "model_package = model.register(\n",
    "    content_types=[\"text/csv\", \"application/json\"],\n",
    "    response_types=[\"text/csv\", \"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    description=\"Predict house values\",\n",
    "    approval_status=\"Approved\",\n",
    ")\n",
    "model_package_arn = model_package.model_package_arn\n",
    "print(model_package_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch transform data.\n",
    "\n",
    "Upload the test data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-862774760132/sagemaker/DEMO-sklearn-byo-model/input/X_test.csv\n",
      "s3://sagemaker-us-east-1-862774760132/sagemaker/DEMO-sklearn-byo-model/output\n"
     ]
    }
   ],
   "source": [
    "np.savetxt(\"X_test.csv\", X_test, delimiter=\",\")\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "X_test_prefix = f\"{prefix}/input/X_test.csv\"\n",
    "s3.upload_file(\"X_test.csv\", bucket, X_test_prefix)\n",
    "X_test_S3 = f\"s3://{bucket}/{X_test_prefix}\"\n",
    "output_path = f\"s3://{bucket}/{prefix}/output\"\n",
    "\n",
    "print(X_test_S3)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a transformer and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_package = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package_arn,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    ")\n",
    "\n",
    "transformer = model_from_package.transformer(\n",
    "    instance_count=1, instance_type=\"ml.m5.large\", output_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................\u001b[34m2022-01-12 14:36:20,962 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:20,965 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:20,966 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting lightgbm\n",
      "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (0.37.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (1.19.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn!=0.22.0 in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (0.23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /miniconda3/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->-r requirements.txt (line 2)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /miniconda3/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->-r requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\n",
      "  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3362 sha256=88a2dfb7321f64e11b3dd61d1363a3ab32df34b1b2bc85d5236f3d76738fc893\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-hd8i0dui/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built inference\u001b[0m\n",
      "\u001b[34mInstalling collected packages: lightgbm, inference\u001b[0m\n",
      "\u001b[34mSuccessfully installed inference-1.0.0 lightgbm-3.3.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[2022-01-12 14:36:23 +0000] [35] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2022-01-12 14:36:23 +0000] [35] [INFO] Listening at: unix:/tmp/gunicorn.sock (35)\u001b[0m\n",
      "\u001b[34m[2022-01-12 14:36:23 +0000] [35] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2022-01-12 14:36:23 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2022-01-12 14:36:23 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:28,255 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mloading model.joblib from: /opt/ml/model\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [12/Jan/2022:14:36:28 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [12/Jan/2022:14:36:28 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:28,986 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mloading model.joblib from: /opt/ml/model\u001b[0m\n",
      "\n",
      "\u001b[34mcalling model\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [12/Jan/2022:14:36:29 +0000] \"POST /invocations HTTP/1.1\" 200 101306 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mcalling model\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [12/Jan/2022:14:36:29 +0000] \"POST /invocations HTTP/1.1\" 200 101306 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2022-01-12T14:36:28.850:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:20,962 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:20,965 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:20,966 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m2022-01-12 14:36:20,962 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2022-01-12 14:36:20,965 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2022-01-12 14:36:20,966 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting lightgbm\n",
      "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (0.37.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (1.19.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn!=0.22.0 in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (0.23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /miniconda3/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->-r requirements.txt (line 2)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /miniconda3/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->-r requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\n",
      "  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3362 sha256=88a2dfb7321f64e11b3dd61d1363a3ab32df34b1b2bc85d5236f3d76738fc893\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-hd8i0dui/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built inference\u001b[0m\n",
      "\u001b[34mInstalling collected packages: lightgbm, inference\u001b[0m\n",
      "\u001b[34mSuccessfully installed inference-1.0.0 lightgbm-3.3.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[2022-01-12 14:36:23 +0000] [35] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2022-01-12 14:36:23 +0000] [35] [INFO] Listening at: unix:/tmp/gunicorn.sock (35)\u001b[0m\n",
      "\u001b[34m[2022-01-12 14:36:23 +0000] [35] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2022-01-12 14:36:23 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2022-01-12 14:36:23 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2022-01-12 14:36:21,146 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python3 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting lightgbm\n",
      "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wheel in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (0.37.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (1.19.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scikit-learn!=0.22.0 in /miniconda3/lib/python3.7/site-packages (from lightgbm->-r requirements.txt (line 2)) (0.23.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib>=0.11 in /miniconda3/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->-r requirements.txt (line 2)) (1.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: threadpoolctl>=2.0.0 in /miniconda3/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm->-r requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\n",
      "  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3362 sha256=88a2dfb7321f64e11b3dd61d1363a3ab32df34b1b2bc85d5236f3d76738fc893\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-hd8i0dui/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[35mSuccessfully built inference\u001b[0m\n",
      "\u001b[35mInstalling collected packages: lightgbm, inference\u001b[0m\n",
      "\u001b[35mSuccessfully installed inference-1.0.0 lightgbm-3.3.2\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[2022-01-12 14:36:23 +0000] [35] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2022-01-12 14:36:23 +0000] [35] [INFO] Listening at: unix:/tmp/gunicorn.sock (35)\u001b[0m\n",
      "\u001b[35m[2022-01-12 14:36:23 +0000] [35] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2022-01-12 14:36:23 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[35m[2022-01-12 14:36:23 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:28,255 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mloading model.joblib from: /opt/ml/model\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [12/Jan/2022:14:36:28 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [12/Jan/2022:14:36:28 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2022-01-12 14:36:28,986 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mloading model.joblib from: /opt/ml/model\u001b[0m\n",
      "\u001b[35m2022-01-12 14:36:28,255 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35mloading model.joblib from: /opt/ml/model\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [12/Jan/2022:14:36:28 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [12/Jan/2022:14:36:28 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2022-01-12 14:36:28,986 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35mloading model.joblib from: /opt/ml/model\u001b[0m\n",
      "\u001b[34mcalling model\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [12/Jan/2022:14:36:29 +0000] \"POST /invocations HTTP/1.1\" 200 101306 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mcalling model\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [12/Jan/2022:14:36:29 +0000] \"POST /invocations HTTP/1.1\" 200 101306 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2022-01-12T14:36:28.850:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "CPU times: user 561 ms, sys: 63.3 ms, total: 624 ms\n",
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "transformer.transform(X_test_S3, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the batch transform results and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6334606231537258, 0.9696430900210699, 4.992265674900781, 2.5030012731023397, 2.4303109395315596]\n",
      "[0.477   0.458   5.00001 2.186   2.78   ]\n"
     ]
    }
   ],
   "source": [
    "s3.download_file(bucket, f\"{prefix}/output/X_test.csv.out\", \"X_test.csv.out\")\n",
    "\n",
    "with open(\"X_test.csv.out\", \"r\") as f:\n",
    "    X_test_out = json.load(f)\n",
    "\n",
    "print(X_test_out[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a real-time endpoint\n",
    "Create an endpoint that serves up the model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes 5-10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!CPU times: user 158 ms, sys: 13 ms, total: 170 ms\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "endpoint_name = f\"scikit-housing-prediction-{str(round(time.time()))}\"\n",
    "model_from_package.deploy(\n",
    "    instance_type=\"ml.t2.medium\", initial_instance_count=1, endpoint_name=endpoint_name\n",
    ")\n",
    "predictor = SKLearnPredictor(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the prediction for the test data generated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63346062 0.96964309 4.99226567 2.50300127 2.43031094]\n",
      "[0.477   0.458   5.00001 2.186   2.78   ]\n"
     ]
    }
   ],
   "source": [
    "# the SKLearnPredictor does the serialization from pandas for us\n",
    "predictions = predictor.predict(testX[data.feature_names])\n",
    "print(predictions[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're ready to be done with this endpoint, please run the delete_endpoint line in the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
