{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c0ddb3",
   "metadata": {},
   "source": [
    "# Train a SKLearn Model using Script Mode\n",
    "\n",
    "The aim of this notebook to demonstrate how to train a scikit-learn model in Amazon SageMaker. The method we are going to use is called Script Mode, in which we write a script to train our model and submit it to the SageMaker Python SDK, which will provide the compute and infrasturcture to do so. For more information, feel free to read [Using Scikit-learn with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html).\n",
    "\n",
    "## Runtime\n",
    "This notebook takes approximately __ minutes to run.\n",
    "\n",
    "## Contents\n",
    "1. [Download data](#Download-data)\n",
    "1. [Prepare data](#Prepare-data)\n",
    "1. [Train model](#Train-model)\n",
    "1. [Deploy and test endpoint](#Deploy-and-test-endpoint)\n",
    "1. [Cleanup](#Cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f6e5a",
   "metadata": {},
   "source": [
    "## Download data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94305f91",
   "metadata": {},
   "source": [
    "Firstly, we download the [Iris Data Set](https://archive.ics.uci.edu/ml/datasets/iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(f\"sagemaker-sample-files\", \"datasets/tabular/iris/iris.data\", \"iris.data\")\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"iris.data\", header=None, names=[\"sepal_len\", \"sepal_wid\", \"petal_len\", \"petal_wid\", \"class\"]\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03329dbd",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d780b1",
   "metadata": {},
   "source": [
    "Next, we prepare the data for training by first converting the labels from string to integers. Then we split the data into a train dataset (80% of the data) and test dataset (the remaining 20% of the data) before saving them into CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the three classes from strings to integers in {0,1,2}\n",
    "df[\"class_cat\"] = df[\"class\"].astype(\"category\").cat.codes\n",
    "categories_map = dict(enumerate(df[\"class\"].astype(\"category\").cat.categories))\n",
    "print(categories_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60482c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 80-20 train-test split\n",
    "num_samples = df.shape[0]\n",
    "split = round(num_samples * 0.8)\n",
    "train = df.iloc[:split, :]\n",
    "test = df.iloc[split:, :]\n",
    "print(\"{} train, {} test\".format(split, num_samples - split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write train and test CSV files\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38880da",
   "metadata": {},
   "source": [
    "After processing and saving the data into CSV files, the next step is to upload the files to S3 where the SageMaker SDK can access and use them to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sagemaker session to upload data to S3\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Upload data to default S3 bucket\n",
    "prefix = \"DEMO-sklearn-iris\"\n",
    "training_input_path = sagemaker_session.upload_data(\"train.csv\", key_prefix=prefix + \"/training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b18753",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6aded1",
   "metadata": {},
   "source": [
    "The model will be trained using the SageMaker SDK's Estimator class. Firstly, get the execution role for training. This role will allow us to access the S3 bucket in the last step, where we uploaded our train and test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3faa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current execution role for training. It needs access to S3\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b137329",
   "metadata": {},
   "source": [
    "Then, it is time to define the SageMaker SDK Estimator class. We use an Estimator class specifically desgined to train scikit-learn models called `SKLearn`. In this estimator, we define the following parameters:\n",
    "1. The script that we want to use to train the model (i.e. `entry_point`). This is the heart of the Script Mode method. Additionally, set the `script_mode` parameter to `True`.\n",
    "1. The role which will allow us access to the S3 bucket containing the train and test data set (i.e. `role`)\n",
    "1. How many instances we want to use in training (i.e. `instance_count`) and what type of instance we want to use in training (i.e. `instance_type`)\n",
    "1. Which version of scikit-learn to use (i.e. `framework_version`)\n",
    "1. Training hyperparameters (i.e. `hyperparameters`)\n",
    "\n",
    "After setting these parameters, the `fit` function is then invoked to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef216809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docs: https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html\n",
    "\n",
    "from sagemaker.sklearn import SKLearn\n",
    "\n",
    "sk_estimator = SKLearn(\n",
    "    entry_point=\"train.py\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    py_version=\"py3\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    script_mode=True,\n",
    "    hyperparameters={\"estimators\": 20},\n",
    ")\n",
    "\n",
    "# Train the estimator\n",
    "sk_estimator.fit({\"train\": training_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59bf17d",
   "metadata": {},
   "source": [
    "## Deploy and test endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809365d8",
   "metadata": {},
   "source": [
    "After training the model, it is time to deploy it as an endpoint. To do so, we can invoke the `deploy` function within the scikit-learn estimator. As shown in the code below, one can define the number of instances (i.e. `initial_instance_count`) and instance type (i.e. `instance_type`) used to deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f78c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sk_endpoint_name = \"sklearn-rf-model\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sk_predictor = sk_estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.large\", endpoint_name=sk_endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253331a",
   "metadata": {},
   "source": [
    "After the endpoint has been completely deployed, it can be invoked using the [SageMaker Runtime Client](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html) (which is the method used in the code cell below) or [Scikit Learn Predictor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#scikit-learn-predictor). If you plan to use the latter method, make sure to use a [Serializer](https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html) to serialize your data properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c34f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "client = sagemaker_session.sagemaker_runtime_client\n",
    "\n",
    "request_body = {\"Input\": [[9.0, 3571, 1976, 0.525]]}\n",
    "data = json.loads(json.dumps(request_body))\n",
    "payload = json.dumps(data)\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=sk_endpoint_name, ContentType=\"application/json\", Body=payload\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"Body\"].read().decode())[\"Output\"]\n",
    "print(\"Predicted class category {} ({})\".format(result, categories_map[result]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842044f",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea4e98",
   "metadata": {},
   "source": [
    "If the model and endpoint are no longer in use, they should be deleted to save costs and free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_predictor.delete_model()\n",
    "sk_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
